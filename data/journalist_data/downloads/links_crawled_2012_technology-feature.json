[
{"file_id": "482257a", "url": "https://www.nature.com/articles/482257a", "year": 2012, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "As more mutations are found across the genome, geneticists are focusing on learning which ones are likely to cause human disease, and how. Even before the first draft of the human genome was complete, researchers knew that one genome wouldn't be enough. They needed sequence data from many individuals to reveal the mutations that make people different and sometimes make them ill. Now, tens of thousands of people have had their genomes fully or partially sequenced. Each person's genome contains an average of more than 3 million variants, or differences from the reference genome. A partial sequence, focusing on the 1.5% of the genome that codes for proteins, usually has about 20,000. For the most part, scientists don't know what those variants do. \u201cThe ultimate goal is to sequence a person's genome and make credible predictions just given the list of variants,\u201d says Greg Cooper, a genomicist at the HudsonAlpha Institute for Biotechnology in Huntsville, Alabama. \u201cWe're a really long way from that.\u201d Scientists have sorted through the most common variants, using genome-wide association studies to learn which occur more often in people with disease, but these variants tend to have small effects, with the biology behind those effects largely unknown. And as techniques that use sequencing to identify genetic variation become cheaper and more reliable, more rare variants are being uncovered. That is changing the questions that researchers are asking, says David Goldstein, director of the Center for Human Genome Variation at Duke University in Durham, North Carolina. \u201cThe field will transition from doing primarily association work to figuring out what implicated variants do biologically.\u201d Disparate strands of research are coming together to do exactly that. A host of increasingly sophisticated algorithms predict whether a mutation is likely to change the function of a protein, or alter its expression. Sequencing data from an increasing number of species and larger human populations are revealing which variants can be tolerated by evolution and exist in healthy individuals. Huge research projects are assigning putative functions to sequences throughout the genome and allowing researchers to improve their hypotheses about variants. And for regions with known function, new techniques can use yeast and bacteria to assess the effects of hundreds of potential mammalian variants in a single experiment. \n               Alignments and algorithms \n             Many bioinformatics tools rely on evolution to rate how likely a variant is to be harmful. Most focus on identifying the 'non-synonymous' mutations that alter the amino acids that make up the proteins for which genes code. It is expected that the more species have evolved with a certain amino acid in a certain place, the more likely a change is to be harmful. \u201cThe idea is that evolution has tested it and that's why you don't see that mutation,\u201d says Pauline Ng, a genomicist at the Genome Institute of Singapore. Ng co-wrote an algorithm called SIFT (sorting intolerant from tolerant;  http://sift-dna.org ), one of the first programs for predicting the effects of protein changes and still one of the most popular. It was originally designed to evaluate one gene at a time, but Ng has updated the protocol to accommodate genomic data files produced by sequencing analyses. The algorithm first identifies mutations that affect highly conserved amino acids, then predicts whether a particular change is likely to be harmful. To train it for such assessments, Ng used published data that assessed amino-acid changes in a well-studied bacterial protein. That showed how often a change from one particular amino acid to another altered protein function. When researchers run SIFT on their sequencing data, the algorithm uses evolutionary conservation and patterns inferred from that original data set to evaluate whether mutated human proteins are likely to behave in similar ways to their non-mutated counterparts. Another popular algorithm is PolyPhen (prediction of functional effects of human non-synonymous single-nucleotide polymorphisms;  http://genetics.bwh.harvard.edu/pph2 ), which was co-written by Shamil Sunyaev, a geneticist at Harvard Medical School in Boston, Massachusetts. This algorithm, too, uses evolutionary data in its predictions, but it also incorporates biochemical predictors of stability and spatial structure. Sunyaev trained it using single-gene mutations that are known to cause diseases, reasoning that they did so by disabling proteins. Stephanie Hicks and Marek Kimmel, statisticians at Rice University in Houston, Texas, were part of a team that evaluated 1  the abilities of 4 popular algorithms to predict the effects of 267 well-understood 'missense mutations', which swap one amino acid for another. The algorithms all had accuracies of about 80%. However, even when working from the same 'alignment data' \u2014 comparisons of protein sequences \u2014 the algorithms made different predictions about the same set of proteins. And Kimmel cautions that algorithms may perform less effectively with mutations that aren't well-known. Even if algorithms were 100% accurate, knowing that a variant causes a protein to lose function is a very long way from knowing whether it contributes to disease, says Sunyaev. The effects of loss-of-function mutations can be surprisingly minimal, buffered by redundancies in cellular machinery. Algorithms alone are certainly not good enough for clinical diagnostics, he says, and he frets that some clinicians are starting to take an interest in these scores. \u201cThis is how I lose sleep at night.\u201d \n               More than missense \n             Even if their predictions were perfect, algorithms that focus on protein sequences would miss many variants that potentially cause disease. Evolutionary analyses indicate that natural selection has conserved five times more base pairs that don't code for proteins than ones that do, which implies that these sequences have some sort of function, even if that is not yet obvious \u2014 and mutations in these genomic regions could therefore have a biological effect. Researchers have now introduced computational tools that use evolution to rank variants in non-coding regions 2 . These include GERP (genomic evolutionary rate profiling;  http://mendel.stanford.edu/SidowLab/downloads/gerp ) and phastCons (phylogenetic analysis with space/time models, conservationl;  http://compgen.bscb.cornell.edu/phast ). Like algorithms that assess protein-coding genes, they evaluate variants on the basis of how often the sequence changes between species. However, because non-coding regions evolve very quickly, sequences can be compared only among mammals. \u201cEven if you go to chickens, nearly all the non-coding stuff won't align,\u201d says Cooper, who co-wrote GERP. And it is not always clear what the rankings mean. Because non-coding regions do not have a corresponding protein, rules regarding amino-acid changes are irrelevant, and there are no data sets appropriate for training such algorithms. \u201cThe evolutionary data we do have are informative, but it's early days, so you have to take them with a grain of salt,\u201d says Arend Sidow, a genomicist at Stanford University in California, who co-wrote GERP and other predictive algorithms. But algorithms for non-coding sequences can provide evidence that a mutation has an impact by looking at conservation, says Sidow. For example, if a child with a rare disease has an unknown mutation not shared by his or her healthy parents, a score indicating that the mutation is in an evolutionarily conserved region would encourage researchers to examine it more carefully in follow-up experiments. Alternatively, researchers can consider the results of human-sequencing experiments. One algorithm, VAAST (variant annotation, analysis and search tool;  www.yandell-lab.org/software/vaast.html ), received a lot of attention last year when researchers used it 3  on just two newly sequenced genomes to pinpoint the mutation that causes Ogden syndrome, a fatal condition linked to the X chromosome in males. The algorithm was also able to re-identify single genes already known to cause some conditions and implicated in more complex diseases 4 . VAAST was developed by Mark Yandell, a geneticist at the University of Utah in Salt Lake City, and Martin Reese, chief executive of genetic-analysis company Omicia in Emeryville, California. It is different from other predictive algorithms that focus on protein-coding and non-coding regions, says Yandell. \u201cInstead of saying, is this conserved?' The algorithm asks, 'How often do we see humans with these variants?'\u201d Unlike many other algorithms, which score each variant as 'probably harmful' or 'probably benign', VAAST provides a ranked list of which variants are most likely to contribute to disease. The algorithm integrates many sources of information: whether a variant has been observed in healthy individuals; whether it occurs in a known functional region; and, for protein-coding variants, what its functional impact is expected to be. When working out whether a single gene is likely to contribute to a condition, it also looks at all the variants that occur in that gene throughout the surveyed population. \u201cYou dump all the variants for each gene into a bucket and then see which bucket has the most likely damaging variants. That goes to the top of the list,\u201d says Yandell. Future iterations of the algorithm, he says, will consider variants in genes that are associated with common biological pathways. VAAST is just one in a wave of algorithms to incorporate human-sequencing data. Another is ANNOVAR ( http://www.openbioinformatics.org/annovar ), which was developed at the Children's Hospital of Philadelphia in Pennsylvania. Knome, a genetic-analysis company in Cambridge, Massachusetts, provides informatics and services for interpreting genomes, and Softgenetics in State College, Pennsylvania, and GenomeQuest, in Westborough, Massachusetts, pluck out variants that might affect patients' health. But the results of such algorithms can't be trusted without further verification. Predictive algorithms can tell researchers which variants should be flagged up for follow-up studies, but not which ones cause disease, says Cooper. \u201cThe best we can do computationally is to prioritize things. It's still going to be a lot of work to nail it.\u201d And there are few ways to assess predictive algorithms, particularly those that go beyond evaluating missense mutations, says John Moult, a bioinformatician at the University of Maryland in Rockville. Moult is one of the co-organizers of the Critical Assessment of Genome Interpretation, a contest in which bioinformatics teams compete to predict a phenotype \u2014 an organism's characteristics \u2014 from genetic data. Of 13 teams that competed last year, only 2 tried to predict how nucleotide sequences might affect gene expression and splicing. But the field is still young, says Moult. For algorithms to improve, researchers will need more data \u2014 and the data are coming, he says. Not only are more genomes being sequenced, but researchers are working out protocols to share data without compromising patient privacy. Last year, the contest could provide data for only ten whole genomes. This year, Moult expects data for 500. \n               Experiments required \n             Laboratory experiments are essential for verifying the effects of variants, but with so many new variants cropping up, there is currently no way to test them all. \u201cWhat we need are functional approaches that have a bit of the feel of genomics,\u201d says Goldstein. \u201cThey need to be scalable; they need to be applied if not to every variant, at least to an awful lot of variants.\u201d In particular, Goldstein wants to know whether a variant associated with a gene affects RNA splicing or transcription rates. To find out, he is collecting genome-wide gene-expression data alongside sequencing data. That allows him to find out whether genetic variants correlate with changes in messenger RNA. \u201cIt's an affordable additional expense,\u201d he says. Other researchers are developing high-throughput techniques for testing protein variants. Just changing one amino acid at a time, a protein containing 1,000 amino acids would have 19,000 variants. In the past, variants had to be tested individually or in small batches, limiting assays to a few hundred. New methods allow the testing of hundreds of thousands at a time. Stan Fields, a molecular geneticist at the University of Washington in Seattle, is designing assays that exploit the basic principle of natural selection. He places many variants of a protein-coding gene into viruses or cells that depend on the protein variants that they produce to grow and reproduce, allowing him to interrogate characteristics such as the protein's stability, structure, enzymatic activity and interaction with other proteins. Sequencing can log which variants become more common and which become less so over several generations. \u201cYou can come up with all sorts of assays,\u201d says Fields, \u201cand the answer comes down to a simple sequence run.\u201d With his postdoc Doug Fowler, Fields has demonstrated 5  that this approach, called deep mutational scanning, can be used to assess the binding activity of hundreds of thousands of variants of the WW domain, a stretch of 40 amino acids that is found in many human proteins and is often important in protein\u2013protein interactions. Fields and Fowler are working out ways to analyse the residues that contribute to protein function, and so learn about general principles of protein design. Fowler is also using the technique to assess which mutations confer drug resistance on Src-kinase, an enzyme implicated in cancer. It should be possible eventually to assess all the single-amino-acid mutations that could occur in important genes, says Fields. \u201cThen if someone shows up with any mutation, you can say: 'Looking at that particular protein activity, we know what the mutation means.'\u201d Last year, Dan Bolon, a protein biochemist at the University of Massachusetts Medical School in Worcester, described 6  a similar approach, which he calls EMPIRIC (extremely methodical and parallel investigation of randomized individual codons). He and his colleagues used this technique to test every possible point mutation in a short stretch of Hsp90, a protein that is necessary for yeast growth. The team examined some 500 genetic changes that collectively encoded 180 protein variants. After growing yeast for several generations, Bolon could see which variants enabled the fastest growth, by measuring which showed up the most often in sequencing data. Previous approaches would have required one-by-one testing, but Bolon's method evaluated all the variants at once. An experiment that would normally have taken years was completed in days. Bolon found that about 15% of amino-acid substitutions that never occurred in evolution grew just as well in his experiments as the wild type, perhaps because effects of those substitutions were too small to matter over the tested time frame, or were irrelevant under the test conditions. Evolution eventually removes both lethal and slightly deleterious variants, but a variant that has an effect only over many generations might make little difference to an individual. As well as providing direct information on particular proteins, such subtle analyses could be used to train algorithms and improve their accuracy, says Peter Good, programme director for genome informatics at the US National Human Genome Research Institute (NHGRI) in Bethesda, Maryland. Both Bolon and Fields expect rapid increases in the number and complexity of variants that can be assessed. Bolon is able to vary 100 amino acids at once, the entire length of some small proteins. Already, he can imagine testing all protein variants within small viral genomes. \u201cThe ability to look at systematic libraries across an entire genome is just very exciting in terms of understanding the raw evolutionary basis for an entire organism,\u201d he says. Such sequencing approaches can also be applied to regulatory elements. Knowing that a mutation changes a transcription-factor binding site says nothing about how it will affect the binding of the gene-activating protein, says Gary Stormo, a molecular biologist at Washington University School of Medicine in St Louis, Missouri. The protein may bind just as well as without the mutation; hardly at all; slightly worse; or even slightly better. So Stormo has created experimental systems that link transcription-factor binding to cell proliferation. The cells that grow best are those that contain the best-binding DNA, and next-generation sequencing is allowing a more systematic exploration of more variants than ever before. Only two or three years ago, scientists would manually pick 20\u201350 of the fastest-growing colonies to examine, says Stormo. \u201cWe now just scrape the whole plate. You can get millions of examples in a single experiment.\u201d Even better, with that many samples, researchers can derive quantitative data, and so show how much better the best-binding sites are. However,  in vitro  results are far from perfect in predicting  in vivo  binding, says Stormo. \u201cSome of the best sites won't be bound, and there will be binding to other places that you wouldn't expect.\u201d The good thing is that differences observed between test tubes and living cells indicate interesting biology. \u201cThat tells you we're missing a lot of information, and that's what we want to Figure out,\u201d says Stormo (see  'Variants in context' ). \n               Decoding regulatory elements \n             Before they can work out what a variant might do, researchers need to learn whether it occurs in an active part of the genome. Several genome-wide studies are providing crucial clues. The NHGRI's ENCODE project (Encyclopedia of DNA Elements) hopes to map and annotate all functional elements in the genome, and the International Cancer Genome Consortium is mapping genomic changes in cancer. The International Human Epigenome Consortium and the US National Institutes of Health's Roadmap Epigenomics Mapping Consortium are studying features such as DNA methylation and other modifications across the genome in many types of cell, and so are showing which regions of the genome might be functional in particular tissues. Annotation alone will not demonstrate that a variant is pathogenic, but the information can help researchers to design the right experiment, says Good. \u201cThe question is knowing why it's pathogenic, that's where the annotation helps you. It's a big difference to say, 'this variant affects a protein-coding region or a promoter active in particular cell types.'\u201d In work 7  funded by these consortia, Manolis Kellis, a computational biologist at the Massachusetts Institute of Technology (MIT), along with Bradley Bernstein, a pathologist at Harvard Medical School, and their colleagues, mapped 'chromatin states' \u2014 sets of chemical modifications to DNA and DNA-binding proteins that distinguish genomic regions. The location of these states varies across different cell types and is correlated with gene expression. By comparing chromatin states on gene promoters, enhancers and other regulatory regions with data on gene expression, the researchers linked regulatory elements to target genes. The team then cross-referenced chromatin states with variants that had been associated with specific diseases. This revealed patterns that made sense: for example, variants that had been statistically associated with leukaemia occurred in what chromatin states revealed to be enhancer regions active in leukaemia cells. Similarly, variants thought to affect lipid and triglyceride levels in blood were found in regulatory elements active in liver cells. Other mapping projects rely on comparative genomics. Last year, researchers based at the Broad Institute of MIT and Harvard in Cambridge, Massachusetts, completed whole-genome sequencing of 20 mammalian species, then analysed 8  these sequences along with those of 9 other mammals that had already been sequenced. This revealed more than 3.5 million evolutionarily constrained elements in the human genome, up from a few hundred thousand that had been previously identified. Still, only about 60% of these could be assigned any putative function. Most of the new elements were located either between genes or in non-coding parts of genes. Furthermore, even nucleotides in protein-coding genes that would not alter amino acids were under evolutionary constraint, and further analysis suggests that these sites affect RNA-transcript processing, microRNA binding and how chromatin states are established 9 . \u201cWhen we are talking about synonymous changes, we can no longer think of them as neutral,\u201d says Kellis, who was part of the study. And more regulatory elements are being revealed. Scores of researchers have noticed that non-conserved areas of the genome have activities associated with function. Many such regions are transcribed; others host various DNA-binding proteins. One-half to one-third of 'biochemically active' elements are unique to humans, says Ewan Birney, a bioinformatician at the European Bioinformatics Institute in Hinxton, UK. When the number of these active, non-coding elements was first discovered, their activity was dismissed as an experimental artefact, then discounted as irrelevant noise. But unpublished work shows that many of these regions are in fact evolutionarily conserved in the human population, presumably because they have a function that helps individuals to survive and reproduce. Of course, changes to evolutionarily conserved sequences do not necessarily contribute to disease, says Birney. But researchers should start thinking about what variation in regulatory regions might do. Six months ago, the Variant Effect Predictor (VEP) tool went live on the Ensembl Genome Browser ( www.ensembl.org ), which brings together information from several databases, including human-sequencing projects and chromatin signatures across cell types. The tool shows, for example, whether a mutation affects a site that binds known transcription factors. Other tools are also coming online. Michael Snyder, a geneticist at Stanford, is developing RegulomeDB ( www.regulomedb.org ), which identifies binding sites and other elements in non-coding DNA. This January, Kellis introduced HaploReg ( www.broadinstitute.org/mammals/haploreg/haploreg.php ), which brings together data from chromatin-mapping and comparative-genomics studies. Researchers can enter common variants and see whether they fall in a highly conserved region, disrupt a regulatory motif or are associated with a regulatory element in a particular cell type. It provides the same information for common variants that tend to be inherited along with the ones entered. This is just the beginning of efforts to assign functions to the millions of DNA variants. In time, says Kellis, it will help researchers to pin down the mechanisms that cause disease. \u201cThe marriage of human genetics and functional genomics can deliver what the original plan of the human genome promised to medicine.\u201d \n                     Mutation-prediction software rewarded \n                   \n                     Software pinpoints cause of mystery genetic disorder \n                   \n                     Human genetics: Genomes on prescription \n                   \n                     Critical Assessment of Genome Interpretation \n                   Reprints and Permissions"},
{"file_id": "484271a", "url": "https://www.nature.com/articles/484271a", "year": 2012, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "As increasing numbers of protein\u2013protein interactions are identified, researchers are finding ways to interrogate these data and understand the interactions in a relevant context. Around the time that scientists celebrated the completion of the draft sequence of the human genome, papers from two separate groups described results of another project that tested all the possible pairings of thousands of yeast proteins to see whether they interact 1 , 2 . The importance of protein\u2013protein interactions is beyond dispute. Little happens in a cell without one protein 'touching' another. Whether a cell divides, secretes a hormone or triggers its own death, protein\u2013protein interactions make the event happen. Consequently, comprehensive maps showing which proteins came together in a yeast cell were much anticipated. But the results took scientists aback. Although the two research groups had explored the full collection of proteins in the same organism using the same yeast two-hybrid (Y2H) assay, the two papers found fewer than 150 interactions in common \u2014 only 10% of the findings that either team dubbed high quality. Most scientists regarded the results as so riddled with artefacts that they were useless. \u201cAs you can imagine, people were extremely critical. They just couldn't believe that you would get such different results when you were studying the same thing,\u201d recalls Peter Uetz, who studies protein interactions at the Center for the Study of Biological Complexity at Virginia Commonwealth University in Richmond, and was a co-author on one of the papers 1 . Even today, many researchers look askance at the Y2H assays used in the studies. But Marc Vidal, a systems biologist at the Dana-Farber Cancer Institute in Boston, Massachusetts, says that the technique has come a long way in a decade. Not only have researchers found ways to recognize and reduce false-positives, but gruelling follow-up studies show that the startlingly low overlap between the two reports was not because the assays found so many interactions that do not exist, but because they missed so many that do 3 . Understanding these interactions is as important as ever. Protein interactomes \u2014 maps of protein interactions \u2014 are raw fuel for systems biologists. Promising techniques to block protein\u2013protein interactions in cancer cells and for other diseases have launched a string of biotechnology deals. Considering disease in terms of protein\u2013protein interactions rather than individual genes and proteins could help to untangle jumbled observations. For example, mutations in the same protein could lead to different diseases by disrupting different interactions. Similarly, mutations in different proteins that disrupt the same interaction could lead to the same disease. A good reference map of interactions would be like completing the human genome sequence, says Vidal, and could spawn further efforts to study genetic variation and function. A validated network would give scientists a jumping off point for more experiments. \u201cMy guess is that as these networks grow, we will get more elaborate ways of understanding where these interactions take place, when and why,\u201d he says. \u201cWe are getting a sense of a cell's organizational self by doing this. \n               Screening systems \n             First described in 1989, the Y2H assay tests the interactivity of pairs of proteins by attaching them to two halves of a transcription factor 4 . If the proteins come together, the transcription factor is reformed, activating reporter genes and allowing the yeast to grow. Companies including Hybrigenics in Paris and Dualsystems Biotech in Zurich, Switzerland, run Y2H as a service. \u201cYeast two-hybrid has an enormous advantage, which might also be a disadvantage: it can detect low affinity,\u201d says Erich Wanker, a neuroproteomics researcher at the Max Delbr\u00fcck Center for Molecular Medicine in Berlin, and co-editor of a book on the topic 5 . In other words, the assay can identify weak, transient pairings such as those that perpetuate cell signalling. But it also detects proteins that randomly bump together. This bumping has led to almost philosophical discussions. \u201cAt what point do we really believe that it's an interaction?\u201d asks Wanker. Scientists have also found ways to detect and avoid many sorts of false positive. Artefacts from 'sticky' proteins, which bind non-specifically to other proteins, can be identified and excluded. Growth that is promoted by a single introduced protein rather than a reformed transcription factor can also be recognized. Precise systems also exist to make sure that all desired combinations are tested. Rather than transfecting the same yeast cell with genes for two potential interaction partners, yeast are transfected with individual genes, mated in pools and their progeny assayed for growth. Robotic systems mix yeast precisely and run multiple replicates of each assay. The number of times that the same interaction is seen becomes part of a quality score. \u201cOur view is that Y2H can give reliable and reproducible results,\u201d says Wanker. Still, some interactions will not be observed in Y2H. For example, the interacting proteins have to allow the two halves of the transcription factor to reunite, and the proteins must be able to reach the nucleus to activate the reporter gene. Thus, interactions with membrane- or organelle-specific proteins are invisible. Besides Y2H, lower-throughput tests in mammalian cells can be used to screen interactions; these tests include luminescence-based mammalian interactome (LUMIER), mammalian protein\u2013protein interaction trap (MAPPIT), protein arrays and protein-fragment complementation assay (PCA). Although these are orders of magnitude slower than Y2H, they can probe interactions in a more relevant context. MAPPIT is one of the highest-throughput mammalian screens. Instead of a yeast transcription factor, a mammalian cytokine receptor is split and becomes capable of cell signalling only when reconstituted. In 2009, Jan Tavernier, a network biologist at VIB, a life-sciences research institute in Ghent, Belgium, described a higher throughput version of MAPPIT in which plasmids encoding potential interaction partners linked to one cytokine receptor fragment can be individually spotted into wells and stored 6 . To begin the experiment, wells are filled with cells expressing the cytokine-receptor fragment linked with the selected 'bait'. When interactions occur, signalling activates the light-emitting enzyme luciferase. Using multiwell plates it costs about \u20ac2,000 (US$2,600) to screen one bait protein against the human ORFeome (a complete set of cloned protein-encoding open reading frames), says Tavernier, who hopes to describe techniques to run MAPPIT on microchips later this year. Miniaturized assays should reduce the cost to \u20ac100 and allow the ORFeome to be tested against 100 baits a week. At this throughput and cost, Tavernier says, new kinds of experiments become feasible. Instead of restricting screens to yeast cells, \u201cyou start mapping full interactomes in the appropriate species\u201d, he says. In addition, Tavernier plans to compare how interactomes change when cells are treated with agents such as drugs or toxic chemicals. He is hoping to commercialize the technology, and is working with Vidal and other scientists to map human protein interactions using both MAPPIT and Y2H assays. LUMIER assays are also relatively high-throughput and can be used to test whether particular interactions are affected by drugs, hormones or other additives. For these assays, cells are transiently transfected with two proteins. One protein is attached to a hydrophilic peptide called FLAG. Potential interaction partners are linked with luciferase. Cells are lysed, the FLAG-tagged proteins are captured and the presence of the interacting partners can be detected by the light they give off 7 . Protein-fragment complementation assays, which can be conducted in yeast as well as mammalian cells, rely on reconstituting a wide range of 'reporters', often enzymes or fluorescent proteins. Since the reporters can signal throughout the cell, interactions can be detected where they naturally occur. In a collection of articles published in January 2009, Vidal, Wanker and others described what Vidal terms an empirical framework for assessing protein interactions found in high-throughput screens 3 . In practice, this means repeating experiments using different types of assay and comparing the results with sets of controls. The positive controls are a reference set of about 100 well-established interactions carefully selected from the literature. The negative controls are some 100 randomly assigned pairs that have never been observed together. Conditions of the assays are adjusted to boost detection of positive controls without raising the detection of random interactions. As part of a framework put forth in  Nature Methods 8 , results from interaction studies should be confirmed in different types of assays. The more methods that find an interaction, the more confident researchers can be. Still, collectively, these assays detect only about 70% of the positive reference set (see  'Beyond binary interactions' ). High-throughput experiments are not the only way to identify protein\u2013protein interactions. Several databases, such as the Biological General Repository for Interaction Datasets (see  thebiogrid.org ) and IntAct (see  www.ebi.ac.uk/intact ), compile lists of interactions as they are published in the literature, culling from both small-scale and high-throughput experiments as well as predicted interactions inferred from other analyses. But this list is not even close to complete, says Sandra Orchard, a proteomics service coordinator at the European Bioinformatics Institute in Hinxton, UK, who helped to develop minimal information standards to help share and evaluate interaction data. \u201cWe will be lucky if as much as 30% of the yeast interactome has been observed,\u201d she says. For the human interactome, she estimates that the figure is less than 10%, including published results that are not captured in the databases. \n               When to believe \n             Biologists rely on interaction data in several ways. They often layer protein\u2013protein interaction networks onto other networks. After identifying transcription factors that regulate a gene, for example, they search databases and literature for transcription factors' interaction partners. Researchers also explore how sets of proteins are connected to each other, and then ask questions based on the structure of the network, such as classifying the proteins that have the most interaction partners. But not all interaction data are equal, warns Russell Finley, a network biologist at Wayne State University School of Medicine in Detroit, Michigan, who believes that incorporating quality measures could make the data substantially more powerful. At present, he says, savvy researchers filter out interactions unless they have been observed more than once through different methods, but these 'intuitive filters' can be biased. For example, the more often a protein is studied, the more interactions will be found. Finley says that a better approach would be to consider all the data available and assign a score reflecting the likelihood that an interaction is real. Computer analyses could then be used to consider more interactions, giving more weight to those with higher confidence scores. But an interaction can occur and have no actual consequences. \u201cThe real question is what interactions have meaning in the first place,\u201d says Stephen Michnick, a biochemist at the University of Montreal, Canada. \u201cAn interaction can be quite good, that is, reproducible in multiple assays, but not be biologically important.\u201d In other words, the interaction has no discernible effects: it does not start or stop a molecular machine, activate an enzyme or send another protein to destruction. Michnick came to these conclusions after conducting a comprehensive study that allowed protein interactions to be studied in a more natural context. In the protein-fragment complementation assays, interacting proteins reconstituted an enzyme that yeast needed to survive under culture conditions 9 . This identified about 3,000 new interactions, with many involving membrane and other proteins that cannot reach the cell nucleus. But thousands of other protein interactions were observed with less confidence. \u201cWe were surprised that there were known proteins that made too many interactions or made interactions that didn't make biological sense,\u201d Michnick recalls. \u201cWe thought we had the perfect method, and so we would get perfect results.\u201d \u201cSo we thought, if we are seeing junk interactions and other people are seeing junk, what is the junk?\u201d The answer, he believes, is that these are naturally occurring 'junk' interactions that, like sections of DNA that do not seem to have a function, simply exist. Michnick believes that perhaps as many as half of the interactions observed even in rigorous screens have no biological function. Abundant proteins should be treated with particular scepticism, but if the same pairs of proteins are consistently found together and not with other proteins then that interaction is more likely to be real, and the same is true of interactions identified across multiple species. \u201cThe parts that are functional have to be dissected from the rest of what's there,\u201d Michnick says. Trey Ideker, a network biologist at the University of California, San Diego, is more worried that such a small percentage has been observed at all. \u201cIt's not clear how you can shortcut to the functional interactions without some unbiased way of getting all the interactions,\u201d he says. \u201cWe have a flashlight illuminating 20% of the yard, but the other 80% is dark.\u201d In fact, no one yet knows how big the universe of interactions is, he says, \u201cbut everyone agrees that we are not even close to having mapped it\u201d. Nonetheless, more interactions have been identified than can be individually investigated. For Ideker, the best approach is to think in terms of databases. \u201cI have this big 'gamish' of interactions, how do I best query it?\u201d \n               Data combinations \n             One strategy is combining diverse data sets around focused questions. For example, Ideker decided to conduct a Y2H screen that would pull out interactions involved in the mitogen-activated protein kinase (MAPK) signalling cascade \u2014 an important drug target that regulates processes such as cell growth, differentiation and survival. Ideker and his colleagues picked 150 proteins associated with the pathway and hunted for their interaction partners using Y2H assays. This revealed more than 2,000 interactions among about 1,500 proteins. From these they selected a dozen or so proteins that had not previously been associated with the MAPK cascade and used RNA interference to knock down the expression of the identified interaction partners. In about one-third of the cases, RNA knockdown altered gene expression within the cascade, indicating that these interactions were functional. Follow-up studies provided the first experimental evidence that a protein called NHE-1 served as a MAPK scaffold 10 . By starting with the interactions and whittling them away with other data, the researchers can uncover new biology, says Ideker. \u201cIt's the superposition of biophysical and functional data that is really going to save the day here. Researchers can also glean insight from how proteins interact physically. This year, Haiyuan Yu and his colleagues at Cornell University, Ithaca, New York, showed how combining data about protein\u2013protein interactions and protein structure could suggest how certain mutations cause disease 11 . They combined several established data sets of protein\u2013protein interactions, the physical structure of those interactions, and genetic measurements to show that when mutations do not prevent proteins from being expressed but still cause disease, they are more likely to occur in the interface between interacting proteins than elsewhere. \u201cFor the past decade, biologists have been using this mathematical definition. Every protein is a mathematical dot. But we know that protein structure is fundamentally important for function,\u201d says Yu. Information about whether an interaction occurs in a specific cell type or under certain conditions could go a long way to revealing its function, says Anne-Claude Gavin, who studies protein complexes at the European Molecular Biology Laboratory in Heidelberg, Germany. \u201cInteractions have to be context-dependent; they have to start at one time and stop at another.\u201d But these studies are difficult and are rarely done. \u201cThis is a level of sophistication that we just don't understand,\u201d she says. To understand a protein\u2013protein interaction in context, researchers need to single them out for focused studies. Sometimes, screening techniques can be adapted to follow particular interactions in depth. For example, complementation assays with fluorescent proteins or luciferase can be used to follow interacting proteins. Because different coloured fluorescent proteins are so similar, one protein can be tested for interactions with two or more proteins in the same cell. One protein is labelled with a fragment of yellow fluorescent protein, a second with a fragment of cyan fluorescent protein and another interrogated protein carries a fragment common to both fluorescent proteins. This can show which protein interactions are occurring and where in the cell they occur. Complementation assays with luciferase can also be used with multiple colours of proteins and have the advantage that the enzyme easily breaks apart and reforms, allowing researchers to study how interactions can be disrupted. Imaging techniques such as bioluminescent resonance energy transfer and fluorescent resonance energy transfer can be used in living cells. They use genetically tagged proteins that emit light when proteins come into contact with each other, and so are used in a variety of assays. Other assays label each of two proteins and then monitor whether they move together in cells. Although slower and more expensive than large-scale screening efforts, one-at-a-time explorations of interactions are essential, says Uetz. \u201cEventually you want to drill down into the actual interactions.\u201d \n                     Protein\u2013protein interactions: Interactome under construction \n                   \n                     Protein;\u2013protein interaction inhibitors get into the groove \n                   Reprints and Permissions"},
{"file_id": "492293a", "url": "https://www.nature.com/articles/492293a", "year": 2012, "authors": [{"name": "Caitlin Smith"}], "parsed_as_year": "2006_or_before", "body": "Using two different kinds of imaging can give scientists a powerful combination of high specificity and detailed structural information. There are many types of microscopy, each providing a unique set of benefits. Light microscopy of cells that express fluorescently labelled molecules, for example, lets scientists observe the movements of specific molecules or protein complexes in live cells in real time, or in fixed samples. Scanning electron microscopy (EM) reveals tiny details of the cell surface, and transmission EM shows the detailed cytoarchitecture of sections through fixed tissue. Other aspects of the structure of cells and tissues can be explored using techniques such as ion microscopy, total internal reflection microscopy, atomic force microscopy and super-resolution microscopy. Each type provides different information, but using two sorts of microscopy simultaneously provides even more \u2014 and molecular tools have been developed to link them together. Researchers testing the waters of this 'correlative microscopy' are beginning to discover its challenges and rewards. Correlated light\u2013electron microscopy, for example, provides both the specificity and real-time observation of light microscopy with fluorescent labelling and the better structural resolution of EM. But such correlative microscopy has historically been tricky to use. Scientists who tried it might have needed to switch to EM halfway through an experiment, which meant moving the sample from one microscope to another that may be located in a different lab, or even in another building. Not only would this journey take time, but delicate samples could be damaged by the motion, or by changes in temperature or humidity. A second obstacle is the need to 'find back', which means searching the thin section prepared for EM to relocate the region observed under the light microscope. To help them find the sample, scientists could create fiducial markers, which act as landmarks to pinpoint the area of interest \u2014 they could be marks etched onto the sample dish, for example. But even then, the process is not straightforward, and correlative microscopy has faced a slow uptake. Instrument manufacturers have developed several solutions to address these problems. In the summer of 2010, Carl Zeiss, based in Jena, Germany, released the Shuttle & Find. This consists of a sample holder, enabling tissue sections to be transported safely, and a software module that connects to both microscopes and uses a coordinate system to find the region of interest. According to Kirk Czymmek, director of North American labs at Carl Zeiss Microscopy in Thornwood, New York, this approach offers a way of \u201crelocating the region of interest in different microscope systems within a matter of minutes \u2014 a task that until now had taken hours, and sometimes even days\u201d. Another way to make finding back easier is to use thicker tissue sections in EM. Cell biologist Judith Klumperman, director of the Cell Microscopy Center at the Utrecht University Medical Center in the Netherlands, says that she uses correlative light\u2013electron microscopy to characterize \u201cdistinct endosome populations by their dynamics, interactions, subcellular localization, cargo, protein composition and ultrastructural morphology\u201d. After observing the dynamics of fluorescently labelled lysosomal proteins with a light microscope, for example, her group uses EM to study their structural details. \u201cWhen using thin 80\u2013100-nanometre sections, these structures will appear in only one or two EM sections of the 20 to 40 that are generally obtained from one cell,\u201d says Klumperman. \u201cWith three-dimensional electron tomography, we can use 300\u2013400-nanometre sections, which increases the chance of finding back the region of interest.\u201d There is no need to switch between microscopes if one instrument integrates both fluorescence and electron microscopy. Furthermore, using the same stage reduces the chance of damaging the specimen and makes it easier to find back the region of interest. Zeiss's Merlin, for example, has all these features and also offers the option of an integrated atomic force microscope (see  'Multimodal microscopy' ) in the vacuum chamber of its scanning EM. A range of similar correlative microscopes is produced by FEI based in Hillsboro, Oregon. The ClairScope, produced by Japanese company JEOL, based in Tokyo, has a different solution to the same problems. It has an inverted scanning EM below the culture dish, which can be viewed at atmospheric pressure, and an optical microscope above it. A window coated with a silicon nitride film allows electrons to be projected from underneath while maintaining a vacuum between the EM and the sample dish. This set-up allows researchers to perform concurrent imaging of a sample in solution by both microscopes, says Donna Guarrera, assistant director in the SEM Division at JEOL USA. \u201cThere is no running from the optical microscope lab, then preparing the sample to be vacuum-compatible for SEM imaging.\u201d Despite recent improvements, technical difficulties still limit what scientists can accomplish with correlated light and electron microscopy. Jeffrey Caplan, associate director of the University of Delaware's bioimaging centre, finds that the speed of fixation is holding him back \u2014 he would like to study live-cell dynamics and then immobilize the cells rapidly for EM imaging. \u201cCurrent cryo-fixation methods take about 10 seconds to a couple of minutes,\u201d he says, \u201cbut we would like a tool that can stop movement of dynamic structures in less than one second if possible.\u201d This would make it easier to study dynamic events such as vesicle docking and trafficking, cytoskeletal remodelling and calcium signalling 1 . Immunogold labelling has historically been the gold standard for identifying subcellular structures in EM. However, the sheer bulk of gold-labelled antibodies can interfere with the identification of targets with single-molecule precision. Caplan's group is now developing an alternative to immunogold labelling for electron and super-resolution microscopy \u201cto ensure that single-molecule localizations are accurately mapped onto the EM image\u201d, he says. \n               Light\u2013ion microscopy \n             One drawback of correlative light\u2013electron microscopy is the fixed order of tasks: fluorescence microscopy must be carried out before EM, rather than after it, because the electron beam can destroy the fluorescence signal. It is therefore not possible to verify fluorescence microscopy results after EM. Scanning ion microscopy may provide the answer. Biomedical materials professor Molly Stevens and her colleagues at Imperial College London recently described correlated light\u2013ion microscopy (CLIM), in which fluorescence microscopy is correlated with ion microscopy 2 . Ion microscopy is similar to EM except that a beam of ions is used to scan the sample, instead of electrons. After imaging the fluorescent signal, Stevens and her colleagues fixed the sample and performed ion microscopy by scanning it with a beam of gallium ions. \u201cI believe that helium ions might also work,\u201d says Sergio Bertazzo, a postdoctoral researcher in Stevens' lab. \u201cOne concern we have is that helium ions might penetrate much more into the sample, damaging the fluorescence signal.\u201d Because the gallium ions did not damage it, the scientists could go back and forth between the two microscopy techniques. They could revisit the fluorescence in the sample after assessing its three-dimensional structure with scanning ion microscopy \u2014 several times for the same sample, in fact. \u201cWe have used this to study how cells interact with environments such as biomaterials, scaffolds for tissue engineering, and 2D micropatterned surfaces\u201d, which guide cell growth in specified geometries, Stevens says. Her group has also combined ion microscopy with total internal reflection fluorescence microscopy, a technique that can boost the resolving power to single-molecule resolution near the edges of the cell. The resulting sub-micrometre scale resolution allowed Stevens' group to see a migrating fibroblast in an intermediate step of the migration process, with one edge mobilized for travel and the opposite edge still firmly adhering to the substrate 2 . \n               Fluorescence\u2013atomic force microscopy \n             In another approach called atomic force microscopy (AFM), the sample is scanned by the tip of a sensitive cantilever probe, providing high-resolution, three-dimensional, structural information. Unlike electron microscopy, AFM can deliver three-dimensional images of live cells without requiring imaging agents such as fluorophores, and it can detect single molecules with nanometre resolution. There is a practical reason why microscopy companies sometimes correlate AFM and light microscopy. \u201cAFM can only visualize a very small area at a time, so you are essentially hunting around blind without correlation with light microscopy,\u201d says Ben Ohler, product manager for research atomic force microscopes at Bruker, based in Billerica, Massachusetts. \u201cSome basic correlation is simply a requirement to participate in the market.\u201d Nicholas Geisse, a bio-applications scientist at Asylum Research in Santa Barbara, California, which specializes in AFM, thinks that AFM needs to have faster imaging to keep up with millisecond-scale fluorescence microscopy measurements. Asylum's Cypher AFM was designed for faster scanning and data acquisition, he says. \u201cMany of Cypher's technical advancements, including the use of small cantilevers, have enabled high-speed scanning.\u201d Bruker's latest instruments are also designed with speed in mind, says Ohler. \u201cWhere AFM images have typically taken minutes per image, Dimension FastScan Bio now acquires images in seconds, or even several images per second,\u201d he says. He believes that such technology can lead to AFM measurements of dynamic biological events. A research group in Italy is also heading in that direction. Physicist Alberto Diaspro, director of the nanophysics department at the Italian Institute of Technology in Genoa, is correlating AFM with super-resolution stimulated emission depletion (STED) microscopy to develop tools for topographical imaging, nanomechanical imaging, and measurements of cell stiffness 3 . In STED, researchers intentionally deactivate some fluorophores in part of the sample, which enhances the resolution in that area. Diaspro is optimistic that combining correlative-microscopy techniques with STED's nanoscale resolution will reveal valuable information about cellular nanostructures. The rapid image acquisition possible with STED also holds promise for measuring fast events, such as intracellular vesicle dynamics during secretion or neurotransmitter release, or cytoskeletal remodelling during cell motility. \n               Fluorescent bridges \n             Researchers have long searched for a molecule that can bridge fluorescence and electron microscopy \u2014 a fluorescent molecule that also stains in EM images, for example. The ideal tool would be a genetically encodable molecular tag, small enough to penetrate fixed tissues better than awkward gold-labelled antibodies, but fluorescent and capable of delivering good contrast in EM. Two groups at the National Center for Microscopy and Imaging Research at the University of California, San Diego, are investigating a variety of molecules for use in more than one type of microscopy. Mark Ellisman, the centre's director, and biochemist Roger Tsien, who shared the 2008 Nobel Prize in Chemistry for developing the widely used green fluorescent protein (GFP) tag, often collaborate in what Ellisman calls \u201cmolecular painting\u201d in the search for a reliable, genetically encodable tag to label proteins in EM. \u201cWe were looking for the GFP of electron microscopy,\u201d says Ellisman. Last year, researchers from both groups revealed a molecular tool they had engineered called mini singlet oxygen generator (miniSOG), which is derived from a plant photoreceptor 4 . Singlet oxygen generator molecules are easy to see because they fluoresce in light microscopy and can be stained by diaminobenzidine for EM 5 . The group expressed proteins labelled with miniSOG to demonstrate its utility as a genetically encoded tag for protein targets in EM. \u201cMiniSOG is an extremely good singlet oxygen generator, so we believe this one will get to single-molecule sensitivity,\u201d says Ellisman. \u201cWe're working on that now.\u201d Meanwhile, miniSOG is already proving useful. Using a combination of microscopy techniques, molecular biologist Clodagh O'Shea and her colleagues at the Salk Institute for Biological Studies in La Jolla, California, are studying the puzzling observation that small viral oncoproteins seem to hijack cellular machinery to stimulate both viral and pathological cellular replication. According to O'Shea, her group wondered: \u201chow do small viral oncoproteins win?\u201d The group imaged infected cells by using miniSOG to label the adenovirus oncoprotein E4-ORF3 (ref.  6 ). Serial block-face scanning EM gave them reconstructed, three-dimensional views of infected cells. \u201cThe scanning electron microscope slices an infected cell from top to bottom in tiny 60-nm blocks,\u201d says O'Shea. The group also used electron tomography to make hundreds of 0.5-nm-thick computational slices through cells, and used specialized software to recreate cells from the computed slices. \u201cThree-dimensional reconstructions show that E4-ORF3 assembles into a remarkable network of cables that weaves through the nucleus,\u201d she says. The images showed that the weave physically separates viral DNA replication domains from cellular nucleoli. In other words, she says, the viral oncoprotein self-assembles into a trap for tumour suppressors. A cousin of miniSOG is also on the horizon. Ellisman and Alice Ting, now at the Massachusetts Institute of Technology in Cambridge, collaborated to create a peroxidase enzyme called APEX, which allows EM staining in relatively thick tissues 7 . Ellisman stresses the importance of genetically encoded tags for miniSOG and APEX. \u201cBoth are molecules we can introduce genetically, and both will result in contrast in EM,\u201d he says. Researchers continue to push the limits of correlative microscopy. \u201cWe are currently developing a methodology that combines CLIM with super-high-resolution fluorescence microscopy,\u201d says Stevens. \u201cIf successful, it will offer a new way of studying molecular interactions.\u201d Meanwhile, Ellisman and his colleagues are working on expressing two genetically encoded miniSOGs that have different colours. \u201cThis is what we call multicolour EM,\u201d he says. \u201cThe idea is that you would be able to do your dynamic light microscopy, then correlate a high-resolution subvolume down to the molecular scale. It's a hard project, but we know we'll succeed.\u201d Bridging light and electron microscopy with genetically encodable fluorescent tags may one day be as routine as labelling with GFP is today. High-throughput methods applied to tissue sections may make super-resolution microscopy faster and easier. When that happens, two \u2014 or perhaps more \u2014 microscopes will definitely be better than one. \n                     Correlative infrared\u2013electron nanoscopy reveals the local structure\u2013conductivity relationship in zinc oxide nanowires \n                   \n                     Protein localization in electron micrographs using fluorescence nanoscopy \n                   \n                     Fluorescent and photo-oxidizing TimeSTAMP tags track protein fates in light and electron microscopy \n                   \n                     Imaging three-dimensional tissue architectures by focused ion beam scanning electron microscopy \n                   \n                     Near-infrared branding efficiently correlates light and electron microscopy \n                   \n                     Correlative light and electron microscopy using cathodoluminescence from nanoparticles with distinguishable colours \n                   \n                     Induction of asymmetrical cell division to analyze spindle-dependent organelle partitioning using correlative microscopy techniques \n                   \n                     Correlative microscopy and electron tomography of GFP through photooxidation \n                   \n                     BIO-COSM Correlative Microscopy Center \n                   \n                     AFM correlated with fluorescence microscopy on live cells \n                   \n                     Correlative Microscopy Research Group at the Institute for Biological Interfaces \n                   \n                     Correlative Microscopy Beamline at the Australian Synchrotron \n                   Reprints and Permissions"},
{"file_id": "491143a", "url": "https://www.nature.com/articles/491143a", "year": 2012, "authors": [{"name": "Vivien Marx"}], "parsed_as_year": "2006_or_before", "body": "Transient changes to the genome make its code more complex to interpret but they still put a gleam in the eye of drug and technology developers. DNA is famous as the instruction manual of life \u2014 the multi-billion-base-pair data tape that directs how a fertilized egg turns into the specific cells, tissues and organs of, say, a sharp-eyed soccer pro who is musically inclined but who also battles depression. But DNA works with many partners, including 'epigenetic' factors, which influence gene expression in ways that don't involve changes to the underlying sequence (see ' Polygamous DNA '). An important example is methylation, in which methyl groups are tacked on to various locations along the double helix to control the activity of particular genes. Methylation also affects histones, the spool-like proteins around which DNA is tightly wound inside the nucleus: the chemical modifications help to control when this protein\u2013DNA complex, called chromatin, opens up so that the genetic instructions can be read. Figuring out when and how such epigenetic changes get made \u2014 or damaged \u2014 has become a crucial part of scientists' efforts to understand both the normal development of cells and their progression into cancer and other diseases. It can be painstaking work. Sometimes, says Andrew Feinberg, an epigeneticist at Johns Hopkins University in Baltimore, Maryland, the available techniques often pick up only \u201clittle biochemical shadows\u201d of events going on at a particular location, while the complete set of players and their mechanisms remain mysterious. And even when you can identify an epigenetic molecule, says Tony Kouzarides, a molecular biologist at the University of Cambridge, UK, \u201cyou have to work out why it is there, and what it is doing there\u201d. Nonetheless, epigeneticists have made remarkable progress over the past two decades. Their tool kit now includes advanced sequencing techniques, targeted antibodies and even laser cell sorting \u2014 and it should soon encompass ultrasensitive nanofluidic and nanopore sequencing methods. The community is also turning to advanced bioinformatics to cope with the sheer volume of data \u2014 especially the wealth of epigenetic information from the Encyclopedia of DNA Elements (ENCODE) project, which this year released more than 1,600 genome-wide data sets covering more than 100 cell types 1 . Technology development is now kicking into high gear as epigenetics researchers push to decipher the genome's many partners, and to deepen understanding of health and disease. \n               Beyond a precipitating headache \n             The standard method used to study epigenetic histone modifications is called chromatin immunoprecipitation (ChIP), coupled with sequencing 2 . The basic idea is to shear DNA while it is still wrapped around the histones, use antibodies to capture specific protein\u2013DNA complexes from the fragments, and then study which DNA sequences are attached to which proteins. The approach helps to unpick how the interactions are tuning genes \u2014 activating some, silencing others. The technique has its drawbacks, however. Sriharsa Pradhan, an RNA biologist at New England BioLabs in Ipswich, Massachusetts, says that he is often unable to reproduce work from published epigenetics studies. \u201cMost of the failures happen if the antibody is not good,\u201d Pradhan says. It might pick up too many DNA\u2013protein complexes \u2014 \u201cevery Tom, Dick and Harry\u201d in a sample \u2014 and so does not offer the resolution that scientists seek. Kouzarides agrees that the quality of the antibody matters greatly for ChIP and many other lab procedures. That's what led him to co-found Abcam, an antibody supplier with headquarters now in Cambridge, UK. The goal is exceptionally high quality, says Kouzarides, who is on Abcam's board of directors \u2014 but it is a constant struggle. \u201cYou are at the mercy of the rabbits,\u201d he says, referring to the animals used to generate the antibodies. \u201cSome generate good antibodies, some generate bad antibodies\u201d \u2014 and there is no predicting which is which. Monoclonal antibodies could offer more reliability, says Kouzarides, because they avoid the problem of batch-to-batch variability. But for reasons that still aren't clear, he says, some of them do not work well for ChIP. For now, the field has to use animals to generate the antibody mixes useful for ChIP. \u201cYou have to put up with the unreliable nature of antibodies because it's the only way to do such experiments at the moment,\u201d he says. Another drawback with standard ChIP is its bias, says Alan Tackett of the University of Arkansas for Medical Sciences in Little Rock. Although the technique lets scientists localize a specific protein acting on a genomic site, \u201cyou have to know what protein or histone modification you are targeting\u201d. And scientists need to have on hand an antibody that matches the protein of interest. So ChIP is not easily multiplexed to profile multiple areas of the genome at the same time. In response to this shortfall, Tackett and his Arkansas colleagues, along with scientists at the Johns Hopkins School of Medicine, have developed chromatin affinity purification with mass spectrometry (ChAP-MS) 3 . The approach involves cutting out a 1,000-base-pair region of a chromosome, purifying it and determining all the epigenetic changes that are present. The team has used the approach in yeast to detect different chromatin states, silenced genes and other regions in which genes are still active. And Tackett says that around ten other labs have begun exploring it, too. He is now readying the technique for use in human cell lines and tissues. \u201cWe are working on the mammalian version and anticipate having that complete within the year,\u201d he says. One challenge for ChAP-MS is that the analysis requires 10 7  to 10 10  cells, so Tackett and his colleagues are trying to lower that number. And Tackett is confident about the technology's promise. \u201cWe see this ultimately taking the place of ChIP in epigenetics labs,\u201d he says, with mass spectrometry being available through proteomics core facilities on campuses, he says. Other scientists are proposing different alternatives to ChIP, which \u201cis not a very efficient process\u201d, says Paul Soloway at Cornell University in Ithaca, New York. In addition to the challenges involved in sample processing, ChIP usually queries just one epigenetic mark at a time in a population of cells. That means that the results of multiple ChIP-seq experiments have to be aligned to determine if some cells have one mark and others have another, or if, perhaps, all cells have both. Soloway wants to offer scientists greater resolution for ChIP analysis. He also wants the approach to be scalable, delivering detail and screening for multiple epigenetic marks in a single experiment. His answer is a nanofluidic device based on a silica wafer that is in the prototype stage and which comes in two formats 4 . One of them quantifies the molecules with at least one epigenetic mark. The other, a branched nanofluidic device, sorts and quantifies the molecules. Using fluorescent labels and optics-based sorting, the molecules are shunted to one chamber or another for later analysis, such as DNA sequencing. \u201cBecause silica is clear and non-fluorescent, we can make measurements of individual molecules using highly sensitive optics,\u201d he says. Ultimately, Soloway would like to be able to go through whole genomes in a rapid, multiplexed way. He says that standard ChIP is still ahead of his technique because it can generate materials in the amounts needed for sequencing, whereas he still needs to get from single molecules to the pico- and nanograms needed. Soloway believes that his technology will find a home in drug development, helping researchers to quickly and quantitatively characterize how drug candidates affect epigenetic marks. Clinical applications could include helping to monitor how patients fare when treated with epigenomic drugs, and identify how epigenetic marks vary during the course of a disease such as cancer, he says. In January, together with the Cornell engineers Harold Craighead and Stephen Levy who worked on the technology, he founded Odyssey Molecular in Ithaca, to commercialize the device. \n               Finding other marks \n             DNA methylation has important roles in cells, including the regulation of genes during development and disease. One of several methods used to find these sections of the genome is methylated-DNA immunoprecipitation, which uses an antibody that locates 5-methylcytosine, a methylated form of the DNA base cytosine. A different approach targets methylated parts of the genome in 'CpG islands', which are characterized by a specific chemical bond between the DNA bases cytosine and guanine. In an analysis of methylation levels for 240,000 of the several million CpG islands in the ENCODE data, John Stamatoyannopoulos at the University of Washington in Seattle and his colleagues found a strong association between methylation and accessibility for genes to be read 5 . As Wendy Bickmore from the Medical Research Council Human Genetics Unit at the University of Edinburgh, UK, notes, the results support the idea that DNA methylation is blocked where the transcription factors that read DNA bind. This mechanism, she says, is relevant to the interpretation of disease-associated sites that show altered DNA methylation 6 . One widely used technique to determine DNA methylation patterns across a genome is bisulphite sequencing. The addition of bisulphite to DNA converts cytosine to uracil, but skips methylated cytosines, thereby allowing the methylation status of DNA segments to be determined through high-throughput sequencing. Many companies offer bisulphite conversion kits. \u201cIt's cheap enough now and there are statistical tools for understanding it, so there's no reason to use another method,\u201d says Feinberg. Yet detecting methylation is time-consuming, so scientists in academia and industry have been exploring ways to improve the approach. Some teams, including one at Osaka University in Japan and one at the University of Oxford, UK, are exploring the use of nanopores, tiny gates through which to run a DNA strand. And Pacific Biosciences, a sequencing firm in Menlo Park, California, is using tags to prepare single strands of DNA for high-throughput sequencing. At Washington University in St Louis, meanwhile, Rob Mitra is leading an effort to be more precise in capturing methylation data, because this information can, for example, be an early sign of tumour development. Mitra and his team, including graduate student Maximiliaan Schillebeeckx, have developed a technique that uses lasers to separate out the cells of interest. He calls the technique laser capture microdissection\u2013reduced representation bisulphite sequencing. Among the advantages, says Mitra, is that the technique covers \u201cthe majority of the CpG islands and it's relatively inexpensive\u201d. Reduced representation bisulphite sequencing is similar to whole-genome bisulphite sequencing, but sequences only the parts of the genome that include CpG-dense regions. The technique uses enzymes to cut up purified genomic DNA into fragments that contain CpG islands. The fragments are then processed, and those of a certain size are subjected to bisulphite conversion, amplified and then sequenced. The approach is geared to work on small amounts of DNA \u2014 perhaps even less than a nanogram \u2014 and in formalin-fixed, paraffin-embedded tissue, which is \u201ctypically not in as good shape as good fresh frozen DNA\u201d, Mitra says. This type of tissue fixation is typically used in biobank samples. His technique could be a tool for researchers who work with specific cell types or with complex tissues, such as neurological samples, in which it is hard to isolate the cell type of interest, he says. The method also avoids the need for multiple labour-intensive purifications. And, he says, \u201cat each point in space, you get a genome-wide profile of methylation, so now you can start to correlate methylation profiles spatially\u201d, Mitra says. A researcher can see, for example, if similar regions of complex tissue are methylated similarly. By coupling genome-wide methylation analysis with laser capture to isolate targeted cell populations, the tool can help researchers to address questions in these challenging tissues, he says. \n               Expanded reach \n             Along with the flood of data that ENCODE brought to epigenetics came data standards, quality metrics, software tools and ways to convey how experiments are done, allowing comparisons between labs. This development has heightened awareness about the \u201cgood technologies\u201d needed to study how the genetic code is put into action, says Adam Petterson, a senior scientist at Zymo Research in Irvine, California, which is one of many companies offering epigenetics services to academics as well as drug-discovery companies. Such awareness is going to become ever more important as epigenetics grows to encompass not just multiple cell types, but multiple species. The modENCODE project ( www.modencode.org ) is mapping regulatory patterns in two frequently used model organisms, the fruitfly  Drosophila melanogaster  and the nematode worm  Caenorhabditis elegans , and the Mouse ENCODE consortium is focusing on epigenomic mapping of the mouse. \u201cA huge way to understand function is by comparative epigenomics,\u201d says Feinberg, who would like to see efforts across many more species. These developments will inevitably require increased reliance on massive computation, says Kouzarides, who sees bioinformatics as a rate-limiting step in epigenetics. Researchers need ways to integrate and do global analyses of the emerging maps of epigenomic marks and their effects, as well as ways to do high-resolution analyses, preferably at the single-cell level (see  page 27 ). Without such computational tools, Kouzarides says, \u201cit's almost impossible to appreciate the complexity of the information\u201d. For scientists who would rather not dig into the data themselves, Michael Snyder and his team at Stanford University in California have developed Regulome-DB ( regulome.stanford.edu ), an automated tool to explore non-coding regions of the human genome. Manolis Kellis at the Massachusetts Institute of Technology in Cambridge and his group have set up Haplo-Reg ( www.broadinstitute.org/mammals/haploreg ), a tool that helps to link non-coding variant patterns to possible clinical conditions. \n               Transient drugs \n             The potential for clinical applications is an important motivator for epigenetics research. The transient nature of epigenetic changes gives drug developers and biomedical researchers reasons to dream about how their efforts might reverse changes that contribute to disease. \u201cThose sorts of things that are more malleable are likely the things that we can target,\u201d Feinberg says. Four drugs that act on epigenetic pathways have been approved by the US Food and Drug Administration (FDA), and the next wave of candidates is being readied in biotech and pharmaceutical companies. Kouzarides, for example, is looking at chromatin modifications and develops drug candidates that could right the wrongs in cancers in which, for example, epigenetic influences lead to the misregulation of cell pathways 7 . Targeting an aggressive form of leukaemia for which treatments are lacking, Kouzarides and his team have explored how to inhibit bromodomain and extraterminal (BET) proteins and remove them from chromatin. BET proteins belong to a class of epigenetic reader that targets histones, recruits multi-protein complexes to the spot where they attach and instructs cellular processes involved in reading genetic information. The journey from the lab to the clinic is not usually quick, Kouzarides says. In this case, however, a candidate under development for inflammation was found to be applicable for the leukaemia. Now, the small-molecule inhibitor of the BET protein is in clinical development at GlaxoSmithKline, headquartered in London. Kouzarides believes that chromatin-modification pathways are promising drug targets because they involve proteins interacting with other proteins. In the past, drugs have tended to target enzymes, and it has not been considered feasible to target protein\u2013protein interactions with small molecules. But his work 8 , along with that of others, has shown that it is possible to develop specific small molecules against the BET proteins that recognize a small epigenetic modification present on chromatin. Constellation Pharmaceuticals in Cambridge, Massachusetts, is also exploring the BET family, as well as other enzymes that modify chromatin 9 . These therapies are going to be part of the second-generation epigenetic drugs that target specific modifications with a role in disease, explains Keith Dionne, the company's president and chief executive. The past, more coarse scientific understanding of chromatin has shifted to an appreciation of the \u201csubtle distinctions\u201d between chromatin states, explains James Audia, the company's chief scientific officer. Earlier this year, Constellation and Genentech began collaborating on the development of inhibitors of epigenetic modifiers. Constellation also has its own programmes targeting inhibitors of BET proteins and another class of epigenetic modifier, the EZH2 chromatin-writers. These proteins seem to be part of a complex that represses gene expression; mutated versions have been linked to some cancers. As Patrick Trojer, director of biology at Constellation Pharmaceuticals explains, cancers use chromatin modification to gain an advantage, for example to inactivate a pathway that creates room for unhindered tumour growth. As part of the company's drug-discovery programme, he and his colleagues develop techniques to study the details of chromatin changes. The understanding of chromatin biology is one of the company's strong suits, he says. To support this application-based research, Trojer and his colleagues use a number of epigenetic techniques. ChIP-seq is a lab standard in which antibodies are \u201cthe key\u201d to the technique, he says. But the company has also made histone mass spectrometry a priority, because it allows the scientists to query the chromatin changes without using antibodies and to query a number of modifications at once. The company set up an in-house high-throughput facility to screen for potential compounds. Although other companies tend to outsource these tasks, the company wants to integrate findings about chromatin biology into drug discovery with an in-house suite of tools that includes mass spectrometry and biophysics analyses, Dionne explains. Another Cambridge-based epigenetics company, Epizyme, focuses on a family of proteins called histone methyltransferases. These epigenetic modifiers act on histones, by catalysing the transfer of methyl groups onto specific positions in the protein. The company has partnerships with the pharmaceutical companies GlaxoSmithKline; Celgene Corporation in Summit, New Jersey; and Eisai in Woodcliff Lake, New Jersey, as well as the Leukemia and Lymphoma Society in White Plains, New York, and the Multiple Myeloma Research Foundation in Norwalk, Connecticut. So far, 96 histone methyltransferases have been identified in humans, says Robert Copeland, Epizyme's chief scientific officer. \u201cWe believe there are at least 20 of those enzymes that are high-value targets for human cancers.\u201d The company's goal is to find a molecule that blocks an enzyme active in an epigenetic pathway but not its nearest neighbours, he says. It is a selectivity that has been difficult to come by in the development of biotherapeutic drugs. Copeland believes that epigenetics drugs fit into a trend of defining a cancer not by its anatomical location but by its molecular profile, which includes epigenetic signatures. Like many companies in this field, he and his colleagues mine the publicly available databases, noting that many genetic alterations in epigenetic pathways are found in human cancers. Kouzarides believes that many cancer cells will be very vulnerable to epigenetic drugs because they rely on only one or two epigenetic pathways, whereas normal cells draw on several pathways for their functions. At the same time, he believes that epigenetics researchers and technology developers will still want to develop and refine experimental methods, for example to explore the three-dimensional structure of epigenomic events, to see how chromatin is changing throughout the genome. \u201cIt's very difficult to look at chromatin itself,\u201d he says. \u201cTechnology still has to evolve to look at  in vivo  chromatin effects.\u201d The available epigenetic data are \u201cextensive, but still a very small snapshot\u201d of epigenetic changes, he says. They represent a situation at a specific time in a specific cell. Epigenetics might find its way into preventive medicine, too. Scanning the epigenome could be a way to detect disease well before symptoms arise. The blood pricked from the heels of newborn babies is one way to begin. In many countries, the blood spots are placed on Guthrie cards and stored indefinitely by hospitals and health-care systems. Scientists at Queen Mary, University of London are exploring how DNA methylation patterns change between newborns and in cells from the same children when they are three years old. Differences in epigenetic marks could be clues to health. If the sequencing companies are betting right, then genome sequencing could become commonplace for many patients, perhaps even part of an annual physical examination. An epigenetic read-out, updated at regular intervals, might be an important companion file to that genome sequence. But this type of progress depends on deeper understanding of epigenetic mechanisms and technology that has yet to evolve, Kouzarides says. Because epigenetic events change constantly in the cell, \u201cwhatever you see in one moment will change in the next\u201d, he says. \n                     An integrated encyclopedia of DNA elements in the human genome \n                   \n                     The accessible chromatin landscape of the human genome \n                   \n                     ENCODE: The human encyclopaedia \n                   \n                     Presenting ENCODE \n                   \n                     Scientific American: Hidden treasures in junk DNA \n                   \n                     ENCODE \n                   \n                     RegulomeDB \n                   \n                     HaploReg \n                   Reprints and Permissions"},
{"file_id": "490293a", "url": "https://www.nature.com/articles/490293a", "year": 2012, "authors": [{"name": "Vivien Marx"}], "parsed_as_year": "2006_or_before", "body": "The field of connectomics is pulling neuroscience into a speedy, high-throughput lane that is generating vast amounts of data. Researchers seeking to understand the brain want big data. And they are getting them. Just as geneticists have moved from genes to genomes to the interacting network of factors that regulate and modify the genome, neuroscientists are going from studying single neurons to tracing how vast neuronal networks connect and interact. \u201cI think this is a really exciting field,\u201d says neuroscientist Moritz Helmstaedter at the Max Planck Institute for Neurobiology in Martinsried, Germany, who is working to obtain a cell-level overview of the neuronal connections \u2014 the connectome \u2014 of the mammalian cortex. \u201cMany people are pretty ambitious about breaking the next barrier in understanding how the brain works by using this new field of connectomics.\u201d Sceptics argue that current methods lack the power to map the massively interconnected web of around 100 billion neurons in the human brain. Even if technology can rise to the challenge, they say, it is impossible to decipher so much data. Clay Reid, a neuroscientist at Harvard Medical School in Boston, Massachusetts, and recently appointed as a senior investigator at the Allen Institute for Brain Science in Seattle, Washington, counters detractors by pointing to recent progress in neuroscience. A few years ago, it was nearly impossible to collect data on networks of neurons. \u201cIt's not routine now but it's easier,\u201d he says. And, he adds, even without a map of the entire brain, charting just a fraction of a neural circuit is an important advance. The way the neuroscience community is embracing connectomics and its big-data harvest is part of a technical and cultural shift. \u201cWe're moving away from artisan cottage-industry science and going after bigger, harder, more complex problems,\u201d says geneticist Geoffrey Duyk, who is partner and managing director at the venture investment company TPG Biotech in San Francisco, California. Advances in high-throughput technology and automation are giving neuroscientists both speed and scale. By combining whole-brain magnetic resonance imaging and computational methods, some teams are mapping the major routes of traffic in the human brain. Other researchers are concentrating on animal models to chart the brain's neuronal circuitry on a much smaller scale, at the level of individual neurons and their projections of axons, dendrites and synaptic connections. The goal is to add the information gleaned to the animal and human connectomes, building them up as resources for understanding behaviour in health and disease. \n               Scaling up \n             Studying the neuronal web at every possible scale demands the integration of every available method. Old-fashioned staining techniques are used alongside new methods of tissue preparation and methods taken from genomics (the study of genomes), and traditional and new approaches are used in microscopy and image analysis. Automation pipelines and computational methods are essential to handling the data \u2014 but so is skilled, manual artistry. The brain is the only organ for which the number and types of cells it contains has not been determined. Just being able to differentiate these cells from one another under a microscope is an important advance. Jeff Lichtman and his colleagues at Harvard University's Center for Brain Science in Cambridge, Massachusetts, apply light and electron microscopy to study how neural circuits change over the course of development. Using a genomic technique called Brainbow, the group was able to make specific DNA modifications in transgenic mice to label neurons fluorescently in more than 90 colours 1 . The researchers could then distinguish individual neurons in the brain's dense tangles of otherwise identical neurons. Separately, the Brainstorm Consortium, which is composed of scientists from Harvard and the Massachusetts Institute of Technology (MIT), in Cambridge, Massachusetts, and from Stanford University, California, is also working on new methods for application in connectomics, in areas such as brain-tissue preparation, imaging and image analysis. Because sample preparation in neuroscience is labour and time intensive, several groups are working on improving it. Lichtman's team has developed the automatic tape-collection mechanism for ultramicrotomes (ATM), which automatically sections brain tissue and collects thousands of slices on a moving tape. The slices can then be stained and imaged on a scanning electron microscope. \u201cIt vastly improves sectioning methodology,\u201d says neuroscientist Scott Emmons, who has just set up an ATM in his lab at the Albert Einstein College of Medicine in New York. Classically, Emmons explains, sections are floated on water and then gathered manually. \u201cThey got lost, they get scrambled up, you cannot make long series,\u201d he says. At the moment, imaging and image analysis can be only partially automated. But, for connectomics researchers to obtain the massive amounts of imaging data they want, technology will need to be expanded. Lichtman and Winfried Denk of the Max Planck Institute for Medical Research in Heidelberg, Germany, note that this approach will necessitate \u201cthe automation, even industrialization of imaging\u201d 2 . High-throughput neuroscience reaches industrial scale at the Allen Institute for Brain Science. When a project is explored for feasibility, the institute looks for a 'brute force' method to convert it to managed, pipelined programmes with milestones and schedules, says Amy Bernard, director of structured science at the institute. This approach has been adapted from industry, says Chinh Dang, the institute's chief technology officer (see  'Neuroscience goes industrial' ). Large-scale efforts at a number of other labs take on circuits in big ways. At Harvard Medical School, Reid and his colleagues have been mapping and studying neural connections in the mouse visual cortex. To do this, they first looked at a brain region  in vivo , using two-photon microscopy \u2014 a technique that is good for imaging live tissue \u2014 and indicator molecules that show when neurons fire and release calcium. They then captured images from fixed tissue with a custom-built serial section transmission electron microscope camera array 3 . The team captured more than 3 million digital images of the mouse cortex and, in collaboration with the Pittsburgh Supercomputing Center in Pennsylvania, stitched them together into 1,200 montages comprising 10 billion pixels apiece, and aligned them in three dimensions. The computing support was key. \u201cWe would not have been able to do the work that we did last year without the help of that team,\u201d says Reid. Denk, Helmstaedter and Kevin Briggman from the National Institute of Neurological Disorders and Stroke in Bethesda, Maryland, used a labour-intensive approach in their recent study of a mouse retinal circuit. They, too, used two-photon imaging with calcium indicators to capture brain functional patterns  in vivo  and then imaged fixed tissue to obtain ultrastructure information 4 . But for that last step they used serial block-face scanning electron microscopy, an automated technique in which a block of tissue is imaged and then the top slice is shaved off to image the next slice. With their own visualization-annotation software tools, such as KNOSSOS ( www.knossostool.org ), they annotated the images to show how neurons branch and interconnect. \n               Reaching out to crowds \n             Both teams performed precise segmentation on the images they obtained: tracing the branching structure of neurons and the synapses between them. This approach has revealed links between structure and function \u2014 illuminating, for example, how the many types of neurons in the retinal circuit compute visual signals. \u201cThe way we got there was entirely segmentation by hand,\u201d says Reid, who says that the human eye remains best suited for the task, given our innate skill at recognizing patterns. Ultimately, however, researchers such as Sebastian Seung at MIT and Dmitri Chklovskii at the Howard Hughes Institute's Janelia Farm research campus in Ashburn, Virginia, hope to teach machines how to perform segmentation. \u201cImage processing is currently the bottleneck,\u201d says Viren Jain, a neuroscientist at Janelia Farm. One component of that bottleneck is the fact that circuits can be metres long. Contouring a path just one-third of a metre long would take a human annotator some 60,000 hours, or around 30 years assuming a normal work week. However, if manual reconstruction has its challenges, so too does automation, note Denk, Briggman and Helmstaedter 5 , 6 . Speeding up image analysis is hindered by stains that can emphasize the cell surface but obscure the visibility of synapses. And tracing a circuit is dogged by errors even with experts doing the work, which makes it hard to imagine how a computer program could do as well or better. Ultimately, new tools are needed to determine the 'volume' of neurons \u2014 that is, when different pathways touch each other and may connect. In the meantime, Reid's Harvard team continues to collect electron microscopy data and analyse them using open-source tools such as TrakEM2 ( go.nature.com/kgsalt ) and the Collaborative Annotation Toolkit for Massive Amounts of Image Data (CATMAID) ( www.catmaid.org ). \u201cOnce the computers take over,\u201d he says, wiring diagrams will capture more connections in a circuit. \u201cAll of us are convinced that we can get tens of thousands or maybe hundreds of thousands of connections, and then it will really become circuit science.\u201d Until then, one way of speeding up segmentation is to use crowds to sift through the data and then reconcile the many results. The software for tracing and annotating electron micrographs initially ran mainly on expensive lab computers, which limited access, says Helmstaedter. But in 2006, he and his colleagues converted KNOSSOS into distributable software and began recruiting students, of whom they now have about 200, to annotate electron micrographs at home or in the dorm. \u201cThis particular combination of being able to browse these extremely large data sets and still do it on a laptop is really so far unique,\u201d he says. The team uses different software, the redundant-skeleton consensus procedure (RESCOP), to reconcile the many annotations generated by the students, which overlap in some ways and conflict in others. Helmstaedter and his colleagues are now looking to expand the crowd working on their data to include members of the public. To entice participation, the team has hired gaming programmers to add an element of play. \u201cThe idea is basically to fly through the brain, it has the feel of a flight simulator,\u201d he says of the prototype. Similarly, scientists at MIT have launched Wired Differently ( www.wireddifferently.org ) to engage crowds in neuron tracing. In the long term, Helmstaedter hopes that automation will become so good that people are only needed for annotating complex brain regions. Having just launched his own lab, he is gearing up to pursue his goal of a cell-level connectome of the mouse cortex, which will involve tracing the connections between billions of neurons. \u201cThat is really an issue of scale,\u201d he says. \u201cWe need a factor of 100 in terms of annotation efficiency.\u201d \n               Small labs think big \n             High-throughput neuroanatomy might seem to be for large labs only, but the methods they use will enable smaller ones to follow in their path, Helmstaedter says. A few smaller labs are already scaling up, using new methods to pull more data out of optical and electron microscopy images than was possible a few years ago. In 2004, fresh from his postdoc at the University of Southern California in Los Angeles, where he studied connectivity in the rat brain, neuroscientist Hong-Wei Dong was among the first scientists to be recruited to the Allen Institute to create the Allen Brain Atlas, a map of gene expression for the entire mouse brain. \u201cI never thought science can be done on this kind of scale,\u201d says Dong, who has now left the Allen Institute to start his own lab at the University of California, Los Angeles (UCLA). \u201cIf I didn't have that kind of experience, I probably would have never thought of mapping the connectivity for the entire brain.\u201d At UCLA he launched iConnectome, a large-scale optical-imaging project, the aim of which is to create a three-dimensional connectome of the mouse brain. He uses optical microscopy and fluorescent markers to capture pathway information; the images are on a coarser scale than with electron microscopy but they show how brain regions interact. Dong's methods include classic neuroanatomical techniques of surgery, preparing and mounting tissue on slides, using tracers to show neuronal inputs and outputs, and classic staining techniques such as Nissl staining, which shows the architecture of brain regions. \u201cThat is all manual labour,\u201d he says. In neuroanatomy, tracers are usually injected into a brain region one at a time. In Dong's project, animals receive two injections in two sites, allowing scientists to examine input and output pathways concurrently, and yielding four times more data than in studies with single tracers. It also cuts down on the cost, time and number of animals needed, he says. In the decade since his postdoc, Dong notes that scanning tissue slides has sped up dramatically, a trend he expects to continue. Microscopes can automatically scan slides one by one, each of which holds 50\u2013100 tissue sections. Olympus, headquartered in Tokyo, Hamamatsu in Hamamatsu City, Japan, and Aperio, which was recently acquired by Leica Biosystems in Nussloch, Germany, all make high-throughput slide scanners. But speed is not everything, because analysis must be done with care. For example, Dong says that minor wrong-way tracer transport can lead to false conclusions. And not all image-processing steps gain speed through automation. For example, tissue slices are registered automatically to a corresponding image in the Allen Reference Brain Atlas, but contrast and brightness require manual adjustment. Even the process of automatic registration can be difficult, because slices differ slightly from one another, causing distortion. To match each image to the reference, Dong's team keeps enhancing the registration algorithm. \u201cThat is a difficult part, but a key part for the future if we want to evolve high-throughput analysis of the data,\u201d he says. It is also not easy to keep good computing staff, who often give in to the lure of Silicon Valley. The three-dimensional visualization in iConnectome allows users to navigate through the brain's pathways, and layers images of tracer-labelled neurons against the backdrop of the atlas reference images, giving them a geographical context. \u201cWe put this map behind each individual brain section, and as people change the opacity, the atlas can give them a reference,\u201d Dong says. The combination of high-throughput approaches and online presentation enables brain-structure information to be shared beyond neuroanatomy circles, he says, to researchers focusing on physiology or behaviour. While Dong finds ways to offer data-laden images to others, in his garage sit dozens of boxes of his own slides \u2014 around 6,000 of them, containing a total of 60,000 individual tissue slices from 200 rat brains. He hopes to image them so that he can share his work on the stria terminalis brain region with others. Brendan Brinkman, senior product manager of the Olympus America Scientific Equipment Group, based in Center Valley, Pennsylvania, has worked with Dong and other neuroanatomists who use slide-scanning and point-scanning confocal technology. To hasten data capture in microscopy further, the company is expanding its multi-point mapping software, Brinkman says. In scanning confocal imaging, a raster scan moves across a sample. To focus only on differences in fluorescence, the scanner can jump from one area to another and capture bursts of data. \u201cYou can adjust the scanning path to make it as fast as possible,\u201d he says. Olympus has also tailored the software to let scientists capture multiple-channel fluorescent signals. \u201cCertainly, neurobiology is the key group for this kind of technology,\u201d he says. And connectome projects are increasing sales of slide scanners, as researchers seek the quick generation of data from large sample sets. TPG Biotech's Duyk agrees that high-throughput approaches to neuroanatomy could be a commercial boon. \u201cIt certainly creates opportunities for the life-science tools companies to push the cutting edge of their technology,\u201d he says. For those lacking these tools, services are emerging. Earlier this year, for example, Renovo Neural, a spin-out from the Cleveland Clinic in Ohio, launched an electron microscopy service. Customers deliver samples, which the company sections with an ultramicrotome from Gatan in Pleasanton, California, then images with an automated serial block-face scanning microscope from Zeiss in Jena, Germany, to return hundreds of ultrastructure images. \n               Network success \n             The first complete connectome was obtained for the worm  Caenorhabditis elegans  in 1986. Sydney Brenner and his colleagues at the MRC Laboratory of Molecular Biology in Cambridge, UK, completed their wiring diagram of the hermaphrodite worm nervous system by tracing images of neurons on electron micrographs by hand 7 . The hermaphrodite is one of the animal's two genders; but the male worm has proven to be tougher to pin down, neuronally speaking. Teams tried to map the male's nervous system as Brenner's group had for the hermaphrodite, but assembling its branching structure by hand was too hard. \u201cThe field of connectomics kind of got stuck,\u201d Emmons says. Electron microscopist David Hall at the Albert Einstein College of Medicine is the custodian of material from the Brenner lab: notebooks, embedded tissue blocks, thin sections, negatives and the manually annotated  C. elegans  electron micrographs. This year, he, Emmons and colleagues revisited 5,000 of the historic images, analysing them in a new way 8 . They translated their analysis of the micrographs into a map of all the connections and their strengths in the male  C. elegans  posterior nervous system. Of the 170 neurons they studied, 144 were involved in the circuit controlling mating behaviour, allowing the team to make a link between connectivity at the cellular level and behaviour. Previous researchers had counted synapses but had not included synapse size, Emmons says, which the team now did. Then they applied a mathematical model to use size as a proxy for the functional strength of each neural connection. The result is a map of the neural network's connectivity that includes quantitative cell-biology information. Across the connectome, they found that interaction strengths between neurons varied more than 100-fold. Using the existing maps from Brenner's lab made neuron tracing easier. \u201cSomebody has already gone before you and put coloured numbers on them,\u201d Emmons says. Elegance ( go.nature.com/nvsfnn ), the group's software tool, accelerated neuron tracing by translating mouse clicks into map coordinates as the team scrutinized the digitized micrographs. Emmons thinks that his methods can speed up connectomics efforts in other small organisms. His group will expand its range in several ways; for example, by using their new ATM machine to automatically collect a series of slices for later imaging. Ultimately, says Emmons, using the ATM, \u201cwe're hoping for a 20,000 unbroken series, which would cover an entire worm, which has never been done before\u201d. Technology like this gives a small lab the tools to tackle large projects such as finishing the male worm's connectome, comparing it to the hermaphrodite or mapping the developing nervous system. The  C. elegans  community is \u201cback in the connectomics business\u201d, he says. Emmons will move on to analyse synaptic connections in the mouse brain, although, for now, his focus is on the worm. The ATM will deliver brain sections aplenty for imaging, but he is confident that his new scanning electron microscope with its bigger field of view is up to the task. \u201cWith one shot we can cover a whole worm cross-section,\u201d he says. Applying these techniques to mammalian brains takes more than automation. The human brain has over 80 billion neurons and the mouse brain has around 70 million. And both have more densely woven webs of neurons than in the worm. \u201cSo you can't just scale up from a little lab and make a big lab and do it,\u201d Emmons says. Big labs mapping large circuits on the single-neuron level are trying new scale-up approaches. Deciphering mammalian neural circuits is Reid's goal in his new position as senior investigator at the Allen Institute, to which he was appointed as part of the institute's ten-year US$300-million move to map connectomes and to use them to reach a broader understanding of brain function that integrates genes, circuits and behaviour. This project, called MindScope, is an attempt to go beyond the anatomy and wiring of the brain to how things are computed in the cortex, by having scientists work side by side to study cell types, neural coding, modelling analysis and theory. \u201cIt's a dream come true,\u201d says Reid. He will use transgenic mice to identify the different cell types in the cortex and thalamus, and will then focus on deciphering the neural coding in the visual parts of the brain using a combination of techniques: behavioural analysis, physiology, imaging with calcium indicators and electron microscopy. The results will be compiled into what he calls network anatomy, which is a wiring diagram with information piled onto it to map and understand the connectome's dizzying array of functionalities. As the wealth of data from using different imaging modalities and from integrated large-scale projects comes in and is collected and annotated, labs large and small will still need to put their heads and computing power together for data analysis. \u201cAstronomical amounts of connectomics data are being generated at an exponential rate; extracting meaning from it is the bottleneck that hasn't been broken,\u201d says Larry Swanson, neuroscientist at the University of Southern California and president elect of the Society for Neuroscience in Washington DC. \n                     Neuroscience: Towards functional connectomics \n                   \n                     Network anatomy and in vivo physiology of visual cortical neurons \n                   \n                     Wiring specificity in the direction-selectivity circuit of the retina \n                   \n                     A critical look at connectomics \n                   \n                     A technicolour approach to the connectome \n                   \n                     Neuroscience: The connected self \n                   \n                     From circuits to behavior: a bridge too far? \n                   \n                     Neuroscience: Making connections \n                   \n                     Blogpost: SfN Is My Nerd Disneyland \n                   \n                     KNOSSOS \n                   \n                     TrakEM2 \n                   \n                     CatMaid \n                   \n                     Wired Differently \n                   \n                     Mouse Connectome Project \n                   \n                     Elegance \n                   Reprints and Permissions"},
{"file_id": "486141a", "url": "https://www.nature.com/articles/486141a", "year": 2012, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "High-quality, data-rich samples are essential for future research. But obtaining and storing these samples is not as straightforward as many researchers think. Across the world, freezers and cabinet shelves are full of human samples. Biobanks \u2014 collections of biological material set aside for research \u2014 vary tremendously in size, scope and focus. Samples can be collected from the general population, from patients who have had surgery or a biopsy and from people who have recently died. Some collections date back decades. The Aboriginal genome, for instance, was sequenced from a lock of hair originally given to British ethnologist Alfred Cort Haddon in the 1920s; he crisscrossed the world gathering samples that are now housed at the University of Cambridge, UK. Most collections contain dried or frozen blood, but tissues such as eye, brain and nail are also held. Some biobanks address different questions from others: a population-based biobank that collects dried blood and health data may be used to determine the genetic risk factors for breast cancer, whereas a disease biobank that collects tumour samples might be used to reveal different molecular forms of breast cancer. The number of tissue samples in US banks alone was estimated at more than 300 million at the turn of the century and is increasing by 20 million a year, according to a report 1  from the research organization RAND Corporation in Santa Monica, California. Those numbers are probably an underestimate, says Allison Hubel, director of the Biopreservation Core Resource at the University of Minnesota in Minneapolis. But many scientists still say they cannot obtain enough samples. A 2011 survey 2  of more than 700 cancer researchers found that 47% had trouble finding samples of sufficient quality. Because of this, 81% have reported limiting the scope of their work, and 60% said they question the findings of their studies. Whereas researchers would once have inspected biological specimens under a microscope or measured only a handful of chemical constituents, or analytes, now they want to profile hundreds of molecules, including DNA, RNA, proteins and metabolites. The popularity of genome-wide association studies, in which researchers scan the genome to look for genetic markers, has trained scientists to go on statistical hunts that require both more quantitative measurements and greater numbers of samples. \u201cThe manner in which biomedical researchers use biospecimens has changed substantially over the past 20 years,\u201d says Stephen Hewitt, a clinical investigator at the National Cancer Institute in Bethesda, Maryland, and an expert on sample quality. \u201cOur knowledge of the factors that impact a biospecimen has not kept up, nor has the education of the users about how fragile a biospecimen is.\u201d \n               In the cold \n             \u201cIf you don't treat the sample properly, it can limit what you can do,\u201d says Kristin Ardlie, director of the Biological Samples Platform at the Broad Institute in Cambridge, Massachusetts. She recalls a project to isolate RNA from placenta samples, which are full of RNA-degrading enzymes. After several tries and no success, a bit of detective work revealed that a collaborator had put the samples in a \u221220 \u00b0C freezer to begin with, and only moved them to a \u221280 \u00b0C freezer several hours later. \u201cResearchers think 'freezing is freezing',\u201d says Ardlie, but a typical freezer is not cold enough to stop degradative enzymes. Except for DNA, few biomolecules are preserved well at \u221220 \u00b0C. Most samples can be stored at \u221280 \u00b0C, but certain specimens, such as live cells, need to be kept at temperatures close to \u2212200 \u00b0C, at which point enzymes are thought not to be able to function at all 3 . Worse than having nothing to analyse are analytes that change in unpredictable ways. One study 4  showed that the concentration of two cancer biomarkers seemed to increase by around 15% from the time that the serum samples were collected and frozen to when they were thawed and measured again about 10 years later. In another experiment 5 , designed to simulate long-term freezing, researchers examined how several cancer biomarkers changed in serum samples that were frozen and then thawed. Some protein biomarkers seemed to be stable for decades even with multiple freeze\u2013thaw cycles. However, vascular endothelial growth factor \u2014 an extensively studied biomarker implicated in diabetes, arthritis and cancer \u2014 was so unstable that the authors recommended that it should never be measured in samples that have been frozen. Not all biobanks document whether a sample has been thawed for analysis and then restocked, nor do they monitor freezer temperatures, says Daniel Simeon-Dubach, a biobanking consultant based in Switzerland. Even short-term fluctuations in temperature can allow sample-damaging ice crystals to form, but Simeon-Dubach says he has seen researchers hold freezer doors open for minutes at a time to show off their specimens. \u201cI think 'what are you doing? Show me a picture!'.\u201d \n               On the shelf \n             Many of the larger biobanks are buying sophisticated freezers to maintain a constant temperature. Systems from companies such as Hamilton Storage Technologies in Hopkinton, Massachusetts, start at around US$1 million and can hold between 250,000 and 10 million samples. Rather than opening freezer doors, researchers place sample tubes in a hatch, and a mechanical arm then moves them to interior shelves. Researchers can even use laboratory information-management systems to search for appropriate samples for a particular study, such as those from donors of a particular age or weight, and then transmit their request to be retrieved. The samples are deposited in a delivery hatch and an e-mail is sent when they are ready to be picked up. The \u221280 \u00b0C freezer also records how many times each sample is removed from frozen storage and for how long. Other companies, such as Brooks Automation in Chelmsford, Massachusetts, also sell automated freezers, and Freezerworks in Mountlake Terrace, Washington, sells data-management software for storing and tracking samples. Even without such sophisticated equipment, freezer storage can be expensive. A typical epidemiological study might have 100,000 samples from 10,000 patients that would fill five freezers, each of which costs $6,000 a year to maintain properly, says Jim Vaught, deputy director of the National Cancer Institute's Office of Biorepositories and Biospecimen Research (OBBR). And although freezing is considered the best way to preserve biomolecules and live cells, it can distort the appearance of tissues. To cut storage costs, most researchers study morphology by relying on a preservation technique that harks back more than a century. Tissue taken from a patient is soaked in the preservative formalin, and pieces of the 'fixed' tissue are then embedded in blocks of paraffin. The Joint Pathology Center in Silver Spring, Maryland, has some 28 million of these blocks, dating back to the First World War. The blocks allow a thin slice of tissue to be taken and stained for microscope slides, but biomolecules are not preserved as effectively. \u201cThe tissue actually drowns in fixative,\u201d explains Hewitt. Hypoxia in the dying cells degrades RNA and alters proteins; formalin crosslinks protein and DNA into complexes, and causes nicks in RNA and DNA. When researchers go to recover biomolecules, removing the paraffin can cause more damage. Although DNA and RNA have been extracted from paraffin-embedded samples, the quality varies and analysis is difficult. Mike Hogan is vice-president of research at IntegenX in Pleasanton, California, which sells products for storing DNA and RNA at room temperature. He believes that formalin fixation can be modified to preserve biomolecules. The main causes of biomolecular degradation are not due to the formalin directly but to hydrolysis and oxidation, he says. Freezing works because the chemical reactions driving degradation occur more slowly at low temperatures. Scientists at IntegenX and at the University of North Carolina, Chapel Hill, are working on techniques to slow hydrolysis and oxidation by removing water and reactive oxygen-containing molecules. If this technique works, it would allow researchers to study biomolecules but still maintain the morphology standards and staining protocols developed over decades of formalin fixation. Other approaches focus on removing formalin from the process. In 2009, Qiagen, based in Hilden, Germany, released a product called PAXgene Tissue, which uses a proprietary, alcohol-based fixative to preserve biomolecules and to allow tissue specimens to be embedded in paraffin. The tissue can be stored for up to seven days at room temperature, four weeks at 4 \u00b0C and months at \u221220 \u00b0C without compromising morphology or biomolecules, says Daniel Groelz, a senior scientist at Qiagen. Researchers there are working on ways in which pathologists can analyse more kinds of PAXgene-preserved histological samples. \u201cThere is a huge amount of specialized staining techniques that have been optimized for formalin,\u201d says Groelz. The company is working on adapting protocols for PAXgene, particularly antibody-based protocols to stain proteins, he explains. This preservative method is starting to be used in place of deep freezing. A pilot project for one of the most ambitious tissue-collection studies, which aims to correlate gene expression and common genetic variation within dozens of tissue types, examined more than 20 tissue types preserved using four different methods. The Genotype-Tissue Expression (GTEx) programme, a collaborative effort involving several groups at the US National Institutes of Health, as well as academic institutions ultimately chose PAXgene. Jeffrey Struewing, a programme director for the US National Human Genome Research Institute in Bethesda, Maryland \u2014 who works on the project \u2014 explains that not only did the technique preserve RNA, the logistics necessary to ship ultracold samples could have hampered collection. Struewing says that it is too early to know how PAXgene will work over many years or for biomolecules such as proteins. \u201cThere is no preservation method that is going to work for every analyte in every sample.\u201d \n               Quality collection \n             Some of the most intractable difficulties occur before preservation begins, says Carolyn Compton, the first director of the OBBR and chief executive of the Critical Path Institute in Tucson, Arizona \u2014 a not-for-profit organization for improving drug development. \u201cBiospecimens are parts of people's bodies that get removed from their setting. They are undergoing biological stresses they would never experience in your body.\u201d When cut off from a blood supply and exposed to abrupt changes in temperature, the cells' behaviour becomes hard to predict. Gene expression and protein phosphorylation fluctuate wildly and cellular self-destruct pathways may be activated. Researchers must ask themselves whether analyses of samples reflect the biology of the patients they come from, says Compton. \u201cYou can have an absolutely perfect test but still get the wrong answer.\u201d Even if tissue is preserved well, it may not tell the full biological story. \u201cThe problem is not just the post-mortem interval,\u201d says Hewitt. \u201cWhat's difficult is the pre-mortem lack of vitality.\u201d Tissues collected from patients who have been on ventilators may not resemble those from healthy patients. \u201cIf you took a biopsy of muscles in my arm after I went rowing in the morning, my RNA profile will look a lot different than if I've been sleeping for a while,\u201d Hewitt says. \u201cYou've got to interpret your data within the limit of what they can tell you.\u201d Blood, urine and saliva samples from non-hospitalized volunteers can be collected during scheduled appointments. But solid-tissue samples are usually collected in hospitals as part of more urgent procedures. Medication, the anaesthesia regime and how blood is shunted from the tissue being removed all affect the sample. So does the length of time the sample stays at room temperature before it is frozen, the time and type of fixative, the rate at which it is frozen and the size and shape of the aliquots. Medical staff will always be focused on the patient on the operating table, but a greater awareness of the impact that samples can have on medical research and patients' diagnoses is having an effect. At a conference organized by the OBBR this February, Gene Herbek, a pathologist at the Nebraska Methodist Hospital in Omaha, described working with surgical teams so that tissues reached pathologists' laboratories within an hour of excision. Biotechnology company Indivumed in Hamburg, Germany, collects samples within ten minutes of excision by having designated nurses on surgical teams. These nurses prepare for surgery along with the rest of the team, receiving information about a patient's treatment and condition. Once the tissue is removed, it is taken into a room next to the surgical suite, where it is sectioned into pieces that are then fixed and frozen. \u201cThe solution is not technology; it is process,\u201d says Helge Bastian, managing director of the company. \u201cThe rule of thumb for how long you have from taking a sample to starting to process it is 15 minutes,\u201d says Simeon-Dubach, adding that this is a very ambitious goal. Specifics vary by organ \u2014 gastrointestinal organs such as the stomach should be processed much faster. Speed is also important for tissues collected post-mortem. Staff collecting tissues for GTEx are expected to be ready around the clock so that they can begin work as soon as the team collecting donated organs has finished. More than half of the tissue samples in the programme have been collected and fixed within six hours of death, says Struewing. Tissues are shipped to the Broad Institute, also part of GTEx, for gene-expression profiling. Before beginning the profiling, Ardlie's team isolates the RNA and checks it for quantity and quality using a metric called the RNA integrity number; it is an imperfect measure, but excluding samples with low integrity numbers maintains consistency. \n               Assessing quality \n             Researchers need better biomarkers of sample quality both to prevent expensive experiments on inappropriate material and to reduce artefacts, says Hewitt, who is working with Ardlie to find measures of RNA quality that work in paraffin-embedded tissue. Scott Jewell, director of the programme for biospecimen science at the Van Andel Research Institute in Grand Rapids, Michigan, is evaluating markers of oxygen deprivation and various types of cell death (autophagy, apoptosis and necrosis). Documenting how samples are collected and maintained is important, but may be insufficient, he says. Researchers need specific recommendations. \u201cWe want markers that can say, 'This is a bad sample. This is a good sample.'.\u201d Such a process is important not only for choosing samples to include in a particular study, but also for understanding how best to preserve them. Many biobanking experts find that researchers give little thought to sample quality. An analysis of 125 biomarker discovery papers published in open-access journals between 2004 and 2009 found that more than half included no information about how specimens had been obtained, stored or processed 6 . Perhaps this is not surprising; biobanking practices have come under scrutiny only recently. The OBBR, established in 2005, released its first official set of best-practice guidelines in 2007, and last year released  Biospecimen Reporting for Improved Study Quality  to guide researchers on documenting how biospecimens are collected, processed and stored. The International Society of Biological and Environmental Repositories in Bethesda, Maryland, published its first edition of best practice in 2005, and a coding system \u2014 Standard PREanalytical Code \u2014 for describing what tissue had been collected and how in 2010. The European Union has funded a four-year programme called Standardisation and Improvement of Generic Pre-analytical Tools and Procedures for  In Vitro  Diagnostics, a multi-institution project coordinated by Qiagen with the aim of improving and standardizing sample handling for  in vitro  diagnostics. In addition, societies are advocating that journals request this information for peer-reviewed articles. The College of American Pathologists, a professional society based in Northfield, Illinois, has developed an accreditation programme for biorepositories. It began accepting applications this year and has so far had a good response. Facilities receive a checklist of required practices, such as whether they have tracked if samples have been thawed and refrozen, and whether they have installed freezer alarms. If facilities meet the list of requirements, they can apply for accreditation and schedule an inspection. \u201cI've been hearing for ten years that someone should step up and do this,\u201d says Nilsa Ramirez, director of the biopathology centre at Nationwide Children's Hospital in Columbus, Ohio, and co-chair of the accreditation working group. \u201cI think it will allow investigators to have a sense that what they are dealing with is the highest possible quality.\u201d One difficulty with these efforts is that published guidelines are generally based on researchers' impressions and experience, not dedicated experiments that test for the best ways to preserve samples, says Vaught, whose office is now awarding grants for assessing and developing storage technology, and is maintaining a hand-curated database of relevant peer-reviewed literature. Biobank professionals often develop their own practices after a few pilot studies but do not publish them, he says. \u201cThere are no international standards based on solid research.\u201d Indeed, resources for research and facilities for preserving biospecimen quality are in short supply. \u201cPeople want to allocate funds for the research project and analysis, not the infrastructure that supports it,\u201d says Compton. First-year start-up costs for a mere 50,000 samples will probably be between $3 million and $5 million, not including information systems. Ten-year operating costs 7  could be more than $10 million. Obtaining funds for ongoing expenses is also a challenge: academics are used to getting samples from their colleagues, rather than paying a repository for high-quality samples. \n               Preserving patient data \n             Several organizations exist to help researchers access the samples needed for their studies. The Cooperative Human Tissue Network \u2014 a network of divisions across the United States initiated by the US National Cancer Institute to improve access to human tissue \u2014 asks researchers to collect tissue and fluids from routine surgery and autopsies, as does the National Disease Research Interchange in Philadelphia, Pennsylvania, which specializes in rarer specimens, such as eyes. Several governments have initiated large biorepository projects (see  'A kingdom's worth of samples and data' ). The Biobanking and Biomolecular Resources Research Infrastructure (BBMRI), a network of European biobanks, is creating policies to allow researchers to share specimens and data. The Public Population Project in Genomics (P 3 G) based in Quebec, Canada, offers not only open-source software for documenting some aspects of informed consent, sample collection and processing, but also a database of biorepositories and their collections. Future research questions will require larger numbers of samples for rigorous statistical analysis, says Isabel Fortier, director of research and development at P 3 G and a researcher at McGill University Health Centre in Quebec. \u201cThere is no way to think that just one study, even a big study, will have enough samples,\u201d she says. \u201cWe need to give a second life to the data that we have already collected.\u201d Ongoing health information about a donor is increasingly desired by researchers, along with information on the preservation of any particular class of biomolecule. This has already prompted considerable reanalysis of appropriate informed consent and data policies 8 , as well as innovations in how data can be stored and mined. \u201cIt's really going to have to be the wave of the future for biobanking,\u201d says Jewell. \u201cWithout knowing how to manage the continuous flow of data, you'll be a static biobank. We want to be able to constantly update the clinical record.\u201d The more information that is available about a specimen, the more valuable it becomes to other researchers. Scientists studying the effects of a particular gene on a cancer pathway could save years of effort and thousands of dollars if they have ready access to a collection of tumour samples with mutations of interest. David Cox, a senior vice-president at drug-company Pfizer and a member of the BBMRI's scientific advisory board, believes that the way to get the most out of biological specimens is not prospectively banking samples but finding ways to reuse samples that researchers have already collected for their own questions. \u201cYou can't store everything. This concept that you're going to get all the samples and store them and then decide what to do is too expensive and it's hard to maintain the quality.\u201d At the same time, he says, individual negotiations with every group that collects specimens is also inefficient. He envisions loosely coordinated 'centres of excellence', in which researchers store samples and track clinical information for their own research questions, but also agree to a common structure for sharing samples and maintaining their quality. One problem is who pays for what, says Cox. \u201cPeople are trying to make money off of these individual pieces instead of trying to get them all together.\u201d Government funding is tight; pharmaceutical companies are willing to fund studies that can lead to new products, and individuals are generally willing to donate specimens and data for the public good, but not for corporate profit. One idea is that research would be conducted for pharmaceutical companies within the biobanking infrastructure, but that companies would not retain the exclusive rights to the data; however, it is too early to say whether this would be viable. There needs to be a way to link infrastructure and information \u201cin a precompetitive fashion, so we can understand the biology better, and we can make better medicines,\u201d says Cox. Perhaps the hardest problem of all will be establishing \u2014 and maintaining \u2014 investment. Reprints and Permissions"}
]