[
{"file_id": "505437a", "url": "https://www.nature.com/articles/505437a", "year": 2014, "authors": [{"name": "Vivien Marx"}], "parsed_as_year": "2006_or_before", "body": "Instruments for studying microbes under biological containment cannot be readily removed from labs for servicing. A US facility is finding ways around that problem. Some bacteria, viruses and toxins are deadly \u2014 natural threats to humans and the environment as well as potential bioweapons. To counter such hazards, laboratories that study these pathogens and substances must do so under high security. One such lab is opening this spring in Frederick, Maryland. It will be designated as biosafety-level-4 (BSL-4), the highest level of biological containment. The lab is part of the Integrated Research Facility (IRF), a complex operated by the US National Institute of Allergy and Infectious Diseases (NIAID) that has been opening in stages since 2008. Although some of the equipment in the 1,020-square-metre lab is standard for a BSL-4 environment, the degree of automation and integration is unprecedented, says Peter Jahrling, director of the IRF. For example, a complete imaging suite and clinical area lets researchers study and treat infected animals without having to remove them from containment. \u201cWe've basically built an intensive care unit for animals,\u201d says Jahrling. Likewise, the instruments used to screen and study blood and tissue samples have been customized to minimize the need for handling by people. In some cases, almost every step has been automated, with robots doing assays to check virus concentrations or assess immune responses. Until the lab receives its final BSL-4 approval, which is expected early this year, its staff is working on slightly less dangerous microbes, such as the viruses that cause cowpox and Middle East respiratory syndrome (MERS). But the researchers are already following BSL-4 procedures \u2014 wearing pressurized whole-body suits with air piped in through hoses, and taking disinfecting chemical showers in the suits when exiting the lab. \u201cIt allows our staff here to gain practice in working with a human pathogen in a BSL-4 environment,\u201d says Lisa Hensley, a microbiologist and the IRF's associate research director. Once the lab has final approval, Hensley and her team will move on to study the Ebola, Hendra and Nipah viruses. \u201cWe'll be able to see what's really happening in animals,\u201d she says, and to assess potential treatments. The lab's unique ability to image live specimens in the BSL-4 environment will help researchers to cut down on the number of animals killed, and will avoid the need for extensive and numerous post-mortem examinations, which increase the risk to staff. But installing robots and complex imaging equipment in the lab has called for careful engineering. \n               Clean imaging \n             In 2012, scientists at another NIAID BSL-4 lab, in Hamilton, Montana, showed that the infection that causes MERS in humans could be modelled in macaque monkeys (V. J. Munster  et al .  N. Engl. J. Med.   368 , 1560\u20131562; 2013). The scientists there could do blood work on these animals and take X-rays, says Hensley, but more detailed imaging data are crucial for identifying lung regions affected by the disease, pinpointing possible locations where inflammation begins and assessing possible treatments. \u201cYou can look to see if your countermeasure is working,\u201d says Hensley. That is why the new IRF lab has a hospital-quality imaging suite that includes an X-ray imager, a 3-tesla magnetic resonance imaging machine and scanners that combine computed tomography with positron emission tomography or single-photon emission computed tomography (see 'Hot and cold'). But each piece of equipment has many moving parts and electronic components that need regular servicing, says Jahrling. \u201cThere's no way you can put that in a BSL-4 lab and expect it to function for very long,\u201d he says. And in a lab with such high biosafety requirements, \u201cthe service people aren't coming in\u201d, says Hensley. The equipment lives in containment and can leave only if thoroughly decontaminated, yet no instrument can handle a chemical shower. The alternative \u2014 treatment with vaporous formaldehyde or hydrogen peroxide \u2014 can also cause damage. And even if the instrument were to survive, the process would be time-consuming. So before the IRF scientists installed imaging equipment in the BSL-4 lab, they collaborated with engineers at Philips Healthcare, which has its US headquarters in Andover, Massachusetts, to completely re-engineer the instruments. \u201cThat was a huge research and development effort,\u201d says Jahrling. The main sections of the imaging machines are installed on one side of a wall \u2014 the 'cold' or non-contained side. On the 'hot' side is a patient table onto which an anaesthetized animal is placed for imaging. The table is mounted on tracks and can be moved through a secure transparent tube that extends into the cold side of the lab but is sealed to prevent the escape of pathogens. Researchers then slide the table through the tube to the imaging apparatus on the cold side. This design allows service staff to maintain the instrument outside the contained lab. Four liquid-handling robots have been installed at the IRF, two of which are in the BSL-4 lab. The instruments are used to extract DNA and RNA from tissues and to perform assays that involve titration and staining. The robots cap and uncap tubes, weigh them and dispense various reagents. In plaque assays, used for calculating the concentration of virus in a sample, virus is added to a multiple-well plate that contains cells and culture medium. After a certain amount of time, visible holes or 'plaques' form where the virus has infected and killed cells. The assay requires multiple steps that include serial dilution, weighing samples and adding buffers. In neutralization assays, used for assessing immune responses, scientists can see how well antibodies interfere with an infectious agent. When robots tend to these assays, there is less variability than when a number of different staff members perform the task, says Hensley. Less handling of infectious and toxic agents is also safer for scientists, she says. \n               Simplified robots \n             Automation brings its own problems. For example, Jahrling says, \u201cthere's a concern that the repetitive manipulations that the robot performs might generate an aerosol\u201d \u2014 tiny, airborne droplets that can spread infectious agents. To reduce the risk, some instruments were enclosed and tested to ensure that performance was not impaired. Kenny Ung, an engineer at Tecan, a company based in M\u00e4nnedorf, Switzerland, and his colleagues worked with the IRF team to customize Tecan's Freedom EVO liquid handlers for use in the lab. The robots are enclosed by safety shields. Tecan customizes instruments for complex workflows in which robotic arms move between carousels, bar-code scanners, incubators, shakers, sealers and other devices. But the machines needed to be simplified and customized for use in this lab. Given the space constraints and difficulty servicing instruments, Ung says, \u201cyou don't want one of those types of systems inside BSL-4\u201d. But the IRF team wanted some degree of automation in their assays so that many samples can be prepared at once. \u201cA robot doesn't get bored, doesn't get tired,\u201d says Ung. In the IRF's BSL-4 lab, robots are programmed for sample preparation with fewer additional devices and with as little human intervention as possible (see 'A custom job'). A robotic arm will pick up tubes containing tissue or blood samples, move them to an automated balance, uncap and cap them and add buffer as needed. Another robot dispenses liquids for serial dilution of samples, which are then transferred to 96-well plates to be incubated and analysed. Tecan's engineers customized the liquid handler to deal with this succession of tasks, but kept the robot simple. They added an automated balance and a bar-code reader to scan and keep track of the sample tubes, but decided not to include a sample-mixing sonicator \u2014 instead, the IRF scientists use an external mixer. \u201cThe more things you add to a robot, the more things can go wrong,\u201d Ung says \u2014 which is true especially in a BSL-4 environment, where repair needs must be kept to a minimum. Ung knew that scientists would be operating the instrument while wearing protective suits, so he decided to simulate their situation. \u201cI wanted to feel what it's like to be the user inside the lab,\u201d he says. The IRF team sent him a safety suit to wear while he worked on the machine. One of Ung's tasks was to ensure that the equipment had no sharp edges that could puncture the suit. In addition to installing safety shields on the instrument's sides and tops, he filed and sanded the edges on the instrument and on its aluminium stand. He also elevated the instrument slightly and added a drip pan so that scientists could capture and clean spills easily. He and his colleagues added air ducts, adjusting the air flow to be strong enough to remove potentially pathogen-laden aerosols but not so strong as to disrupt liquid-dispensing or weighing. Increased air flow can prevent a balance from stabilizing, which would lead to flawed readings. Before installing the machine, the IRF team visited Tecan's labs to verify that it worked as specified. The instrument was then moved to the BSL-4 lab, where it was reassembled and tested. Tecan worked out a maintenance plan for the instrument. \u201cI'm not able to go in there, so they basically need to have skills on their side to remedy the problem,\u201d says Ung. Cindy Allan, a biomedical engineer at the IRF, has learned how to take the robot apart and put it back together. She also learned the tricky task of changing the pipette tips, diluters and syringes. Biothreat analysis calls for genetic and protein-based tests, says Amy Altman, who directs biodefence at Luminex, an instrument manufacturer in Austin, Texas. Antibody-based probes are used to detect proteins such as ricin or  Clostridium botulinum  toxin, for example. And some viruses can have very low concentrations in the blood, so detecting them requires sensitive genomic tests. \n               Fast flow \n             In the IRF's BSL-4 lab, researchers will be using Luminex's FlexMAP 3D, which is based on flow cytometry, a technique in which lasers are used to count cells. In this instrument, specific types of genetic material or protein are attached to beads coloured with multiple fluorescent dyes at different ratios. The beads move in single file past a laser that emits red light to excite the dyes. The emitted wavelength of light identifies the type of bead. A green laser then excites a 'reporter' dye in the bead that determines which protein or nucleic acid it is attached to. \u201cAs each bead gets interrogated by the laser, you can think of each bead as its own little test tube,\u201d says Altman. To screen a sample for ten strains of Ebola virus, for example, researchers use an array of beads that each carry snippets of DNA specific to one of the strains. A blood sample from a patient who succumbed to Ebola can therefore be tested to determine which strain caused that person's death. The platform works quickly and can distinguish between up to 500 genes or proteins simultaneously. Such 'multiplexing' means that scientists can check for many signatures of a possible infectious agent at once, Altman says; a 'true positive' for a pathogen would be when all signatures of the disease agent are present. Multiplexing platforms are used in biothreat monitoring, which sometimes entails \u201chigh-consequence, high-regret decisions\u201d, such as whether or not to shut down an airport, says Altman. To ensure that the hardware could operate in the IRF's BSL-4 lab, Luminex scientists tested whether the instrument could be safely decontaminated with vaporous hydrogen peroxide and paraformaldehyde. Neither substance is usually kind to instruments, but in the case of the Luminex machine, \u201cit made it through fine\u201d, Altman says. \n               Bacterial high alert \n             Scientists in the BSL-4 lab will also be using the BacT/ALERT 3D 60, an automated system made by bioM\u00e9rieux, based in Marcy-l'\u00c9toile, France, that tests samples of body fluid for fungi, mycobacteria or bacteria \u2014 such as  Yersinia pestis , which causes bubonic plague, or the anthrax culprit  Bacillus anthracis . To accomplish this task, the instrument has a built-in incubator that rocks sample bottles, swirling their contents to enhance microbial growth. At the bottom of each bottle is a dab of dried silicone that contains a dye and acts as a pH sensor. When the pH in a bottle changes because of microbial respiration, the sensor's colour changes from blue-green to yellow. A detection system illuminates the bottles periodically and alerts lab staff to any change in sensor colour, says Doug Matthews, product manager at bioM\u00e9rieux. \u201cEvery ten minutes, it takes a reading and will alert you if it detects growth,\u201d Matthews says. Speed is important in hospitals, where this instrument is often deployed, as well as in clinical-testing labs and the pharmaceutical and food industries. But in a BSL-4 lab, rapid detection is even more crucial, he says. After detection through this automated incubator, scientists can then confirm their findings with other steps. A sample will be grown in an agar dish to identify specific pathogens, for example, or to test how well an antimicrobial agent might work. Researchers might also extract genetic material for sequencing. The BacT/ALERT 3D 60 was not made specifically for BSL-4 labs, but a number of its design features help it to work in that environment, says Matthews. Despite its compact size, it can handle 60 samples. Its touch screen can be operated by heavily gloved hands. The instrument is designed to be hardy \u2014 company records of installed instruments show that the machines do not need major service or repair for an average of 1,000 days, he says. The sample bottles are made from nylon sandwiched between two layers of polycarbonate to resist shattering and spreading infectious cultures. \u201cKeeping glass out of a BSL lab is obviously a key safety feature,\u201d Matthews says. Later this year, bioM\u00e9rieux intends to launch a new model of the instrument, called the Virtuo, in Europe. The Virtuo handles more bottles than the BacT/ALERT 3D 60 and automatically places them in racks after scanning their bar codes. The instrument is now in clinical trials, says Matthews, and the company is slated to file for approval with the US Food and Drug Administration in 2015. \n               Cellular connection \n             The IRF's new BSL-4 lab is subdivided into rooms for different tasks, such as sample preparation, cell culture, animal care or imaging, and tests for clinical pathology, molecular biology and virology. But moving between those rooms is not simple, because scientists in protective gear are tethered by an air hose that stretches only so far. To venture farther, they have to detach from their air supply. \u201cIf you disconnect your air you could last for ten minutes,\u201d says Jahrling. \u201cYou just get used to disconnecting your air and walking ten feet down to the next air line and reconnecting.\u201d Given the multitude of tasks researchers do in the IRF lab, they need to confer often. Years of working in 'space suits' have made Hensley and Jahrling fluent in non-verbal communication \u2014 by a wave of a gloved hand, for example. For the new lab, they wanted to go one better \u2014 in part because they noted a cultural shift among younger scientists, who seemed to want to communicate more when in the BSL-4 lab. After testing various communication systems, they chose mobile phones and Bluetooth headsets. The phones are external to the suits; the headsets are internal. \u201cYou pick up the cell phone on the hot side of the suit,\u201d Hensley says. Communication and collaboration have been a goal for the lab since 2005, when construction of the IRF began as part of a plan to bolster US biodefence research. To enable collaboration, the IRF is located next to the US Army Medical Research Institute of Infectious Diseases (USAMRIID) and labs run by the US Department of Homeland Security. Both Jahrling and Hensley previously worked at the USAMRIID. Jahrling says that those neighbouring labs are the subject of much rumour and speculation, but that no secret research is done at the IRF. On the contrary, he has extended an open invitation to scientists from other biosafety research facilities to visit, so that they can see for themselves what it is like to battle the most noxious weapons from nature's arsenal in an up-and-running, next-generation BSL-4 lab. \n                     Progress stalled on coronavirus \n                   \n                     Biosafety lab passes disaster test \n                   \n                     Plans stall for biodefence lab \n                   \n                     Nature News blog: MERS surveillance in Saudi Arabia \n                   \n                     Nature News blog: Boston biocontainment lab \n                   Reprints and Permissions"},
{"file_id": "513445a", "url": "https://www.nature.com/articles/513445a", "year": 2014, "authors": [{"name": "Vivien Marx"}], "parsed_as_year": "2006_or_before", "body": "When healthy parents have a child with a genetic disorder, the cause is sometimes a new mutation. Tools are emerging to meet the challenge of finding such changes. When parents find that a child is not developing as expected, the protracted doctor visits, hospital stays and examinations only add to their distress \u2014 especially when no other family member has the condition and the standard tests on the child's blood and genes shed no light on the cause. The uncertainties, costs and anguish can be devastating to families, says Michael Friez, who directs the diagnostic laboratory at the Greenwood Genetic Center in South Carolina, a non-profit organization that analyses patients' genomes for clinicians. Every clinical geneticist has experienced the inability to identify the cause of a child's neurodevelopmental disorder, adds Roger Stevenson, a clinical geneticist also at the centre. In the early 2000s, he began seeing a family with a toddler that had severe developmental problems, including a smaller-than-average head and intellectual disability. It was more than a decade after their first visit before sequencing revealed that the boy had a mutation in a gene called  DYRK1A , which is thought to have a role in brain development. The finding later helped to diagnose 16 other children in the United States and Europe who had the same symptoms \u2014 and although the condition has no cure, Stevenson saw that identifying the gene comforted the boy's parents, as did knowing that there were other children like their son. \n               New mutations \n             What was notable about this child's case was that it involved a  de novo  mutation \u2014 one that neither parent carries in their regular complement of DNA.  De novo  mutations can occur early in the development of the embryo. They can be in parents' gametes. Around 80% of  de novo  mutations seem to occur in the father's sperm and 20% in the mother's egg, says Joris Veltman, a geneticist at Radboud University Medical Center in Nijmegen, the Netherlands, who in July published a study of  de novo  mutations in people with intellectual disabilities 1 . Disorder-causing  de novo  mutations are hard to detect \u2014 they have to be identified among a host of other, innocuous genetic changes. A number of software-based approaches are emerging to sift through sequenced genomes in search of such mutations. As sequencing instruments and databases of genetic information become increasingly available, tool-builders hope that their software contributions can become part of routine medical care. But sequencing and analysis are different from, say, a blood cholesterol test \u2014 samples have to be prepared for the instruments, which churn out the genome sequence in snippets that must be assembled and aligned to a reference genome, such as that curated by the Genome Reference Consortium. The results are not perfect. A patient's genome sequence can contain errors \u2014 caused by the machine misreading a letter of DNA, for example \u2014 that must be filtered out computationally. And even then, a huge number of possibilities remains. DNA bases might differ from the reference, sequences can be inserted or deleted and the number of copies of a gene can vary. Of thousands of such changes, only one might have a role in a disorder. The child's DNA is then compared with that of the parents. Again, not all differences between their genomes connect to the child's disorder. Researchers use software that includes statistical analyses to determine which changes are most likely to have a role. And the tools add information, such as published data about the links between genes and disease. These results help to create lists of genetic changes, or variants, ranked by likelihood of being linked to a disorder. But variant analysis is still an emerging science, and the software tools are still maturing. Despite this, in some cases the approach turns up a specific genetic change that is likely to be the cause of a disorder. \n               Assorted variants \n             Finding the probable genetic culprit does not mean a treatment is available. But such results help parents to cope with the situation, says Donald Conrad, a geneticist at Washington University in St Louis, Missouri. The results also inform parents about the risk that the condition might recur in their family and help them to plan future pregnancies. And some prospective parents might opt for genetic analysis as part of  in vitro  fertilization. Most newborns carry about 60\u2013100  de novo  variants, says Conrad \u2014 few of which cause any discernible problem. Software helps to sort these variants out. Conrad has developed DeNovoGear, which does statistical analysis to distinguish potentially important signals from background noise caused by experimental error 2 . The software also analyses the nature and frequency of sequencing errors. It then compares the genomes of parents, children and other family members to distinguish true  de novo  mutations from other types of genetic variation. To improve the odds of finding such mutations, the analysis takes into account the frequency of known variation at a given site in the genome. It does so by drawing on data from the 1000 Genomes Project, an international research consortium that catalogues human genetic variation. \u201cThere is no single magic trick that makes our method work well,\u201d Conrad says. \u201cIt is just the accumulation of many different attempts to squeeze out as much information as possible.\u201d \n               Raising the odds \n             The software must contend with the errors made by the sequencing instruments \u2014 reporting a 'C' as a 'T', for example. These mistakes are rare but hard to predict, says Conrad, and may explain many false-positive results in searches for  de novo  mutations. High-throughput sequencing instruments are more prone to error in some DNA regions \u2014 which also turn out to be where cells are more likely to make mistakes when copying and repairing the genome. These tricky places account for about 15% of the genome, Conrad notes, so current methods can reliably detect  de novo  mutations only in the other 85%. Even the best software tools come up with 2\u20133 times as many false positives as true positives when analysing whole-genome sequence. True positives have to be teased out with follow-up experiments \u2014 for example, by using the laborious but precise Sanger sequencing method to look at the genetic region in question. \u201cEach sequencing platform has its own idiosyncrasies,\u201d Conrad says, and the optimal method for detecting  de novo  mutations needs to incorporate the machine's quirks into its statistical models. Conrad is also developing statistical methods that take account of the frequency of various sequencing errors in different regions of the genome. Other software tools typically apply the same error estimates at all genomic sites. Other researchers are pursuing their own approaches. For the study published in July 1 , Veltman and his colleagues sequenced and analysed the genomes of 50 people with severe intellectual disabilities (see  'Better diagnosis' ). Working with Complete Genomics (CG) in Mountain View, California, a division of the genomics giant BGI in Shenzen, China, they identified  de novo  mutations by drawing on a number of resources. They used BGI's technology and software to analyse and compare genomes and whittle down the number of possible disease-causing candidates. They did not analyse the data with other tools such as DeNovoGear, so Veltman cannot compare the methods. But the advantage with BGI's analysis suite is that the software has been matched to the specifications of the sequencing technology, he says. All the patients in the study had previously undergone extensive testing. Protein-coding regions of their genomes had been analysed, and microarrays were used to analyse variations in gene-copy number, which can occur from person to person and also in some disorders. Veltman says that the software showed high sensitivity in detecting  de novo  mutations, which enabled a more accurate diagnosis of almost half of the patients. \n               Interpret carefully \n             In Veltman's view, interpreting mutations is now more possible for disorders such as intellectual disability than for diseases such as cancer or diabetes, because many cases of severe intellectual disability seem to be caused by a single mutation. But, he says, the few hundred genes that the scientific community has found to be implicated in intellectual disability form a still-incomplete list. Veltman stresses that the sequencing quality in the study was good, but says that even the best sequencing technology can miss or misidentify  de novo  mutations. To minimize errors, researchers need to seek out the highest-quality genome sequencing, he says. Beyond that, interpreting the many genetic variations that turn up when comparing genomes \u2014 and figuring out which ones are related to a disorder \u2014 is the field's major bottleneck. Researchers also need to find better ways to analyse  de novo  mutations in the genome's non-coding regions, which are still difficult to interpret. One suite of tools to analyse protein-coding and non-coding genome regions is FastQForward, which integrates the software programs VAAST 3 , pVAAST 4  and Phevor 5 . These tools were co-developed by Mark Yandell, a computational geneticist at the University of Utah in Salt Lake City who directs software development and computational analysis related to the Utah Genome Project. That project combines family histories from the Utah Population Database with medical records, which increasingly include DNA sequence information. The project includes family histories for more than 7 million people and medical records for around 4 million of them. Yandell and his team are using pVAAST to analyse family pedigrees in which there is a higher frequency of disease. pVAAST searches through many genomes in parallel to find alterations. The program addresses the statistical challenge presented by genomes from people who are related, he says. And it detects  de novo  mutations. Printed out, the large family pedigrees in the Utah database can span almost 2 metres. The ones Yandell is studying include multiple family members that have mental-health issues such as schizophrenia or depression. Mental illness has a large environmental component, but he hopes that these records can help to uncover genetic factors, he says. Studying families might offer advantages over the more typical 'cohort analyses' of unrelated people with similar conditions. In such cohorts, the causes of mental-health problems might be quite diverse. Yandell hopes that restricting the search to extended families will make it easier to identify gene variants involved. VAAST uses a similar approach to that of BLAST, a widely used search tool in genetics research 6 . With BLAST, a scientist can take a genetic sequence and search through many genomes to find high-probability matches to it. Similarly, VAAST compares variants in a person's genome to those collected in the 1000 Genomes Project. This comparison helps to determine the probability that a variant is causing a disease. pVAAST extends VAAST's capabilities to family-based sequence data. Yandell also uses Phevor, which taps into resources such as the Human Phenotype Ontology, which catalogues links between gene function and human disease symptoms. Phevor helped clinicians to diagnose a 12-year-old boy who had life-threatening diarrhoea and intestinal inflammation. Genetic analysis with VAAST had come up empty. By combining the analysis with Phevor, the researchers traced the boy's illness to a  de novo  mutation in  STAT1 , a gene involved in many intestinal disorders. The finding, which was confirmed with Sanger sequencing, enabled the boy's doctors to properly treat and stabilize his condition. Yandell hopes that genetic analysis will soon be a routine part of clinical diagnosis. Towards that aim, he and Martin Reese, a co-developer of VAAST, developed Opal, a platform that helps clinicians to interpret and use the results from software-based genetic analyses. Reese is chief scientific officer of Omicia, a company in Oakland, California, that offers genetic analysis using several tools, including Opal, VAAST, pVAAST and Phevor. Reese says that his company tries to fill the gap between tools developed in academia and the needs of clinicians. The VAAST algorithm does the hard-core maths to analyse the matches, score their probabilities and create a ranking, he says. The Opal software then searches for clinical and biological data about the candidate genes \u2014 added information that can help to determine which candidates are more likely to be causing the disease. In June, Omicia began working with Laboratory Corporation of America, a large medical-testing company based in Burlington, North Carolina. Omicia will interpret genomic data as part of clinical trials. Data analysis is Omicia's specialty. Unlike many other companies in the field, it does not do sequencing. \u201cWe're slicing and dicing the genome based on your clinical question,\u201d says Reese. His team first assesses the sequence quality and filters out typical sequencing errors before hunting for changes such as  de novo  mutations. \n               Future medicine \n             Eventually, clinical standards in this area will emerge, but for now service providers use the approaches they deem to be best for these complex analyses. Reese believes that many diseases, if not all of them, have contributions from  de novo  mutations. These contributions are hard to identify, he says, but whole-genome analysis raises the probability of finding them, as Veltman's study shows. Conrad says that detection of  de novo  mutations can be a standard medical test only when the genetic complexities of diseases they cause are better understood and tool developers have found ways to address them, and after the technical issues related to high-throughput sequencing have been resolved. Between 20% and 90% of the  de novo  mutations detected by software and with the help of whole-genome sequencing can be false positives. \u201cResearchers can accommodate this with extensive follow-up validation experiments, but this is just simply not practical for a routine diagnostic test,\u201d Conrad says. Better approaches are also needed for the tough-to-sequence regions of the genome, and the software has to cover the spectrum of mutations, from single-base changes to insertions or deletions. And researchers need to better understand changes such as large copy-number variations, regions of repetitive sequence and other types of DNA rearrangements, says Conrad. Greenwood Genetic Center, which uses genetic analysis to diagnose patients, does its own analysis and uses commercial services. Scientists and companies doing genetic analysis will soon have access to many of the same shared resources, and Friez says that he looks forward to seeing how that will help patients with neurodevelopmental disabilities. For now, patients, their families and clinicians all face the same issue: researchers' ability to identify mutations associated with disorders is not always matched by a medical understanding of these mutations, and therapies that might arise from knowing about them are far in the future. But genetics does deliver some answers for these patients and families, says Veltman. \u201cFrom what I hear from my clinical colleagues, these families are very happy to finally get an answer \u2014 it often means closure for them, they can give the disorder in their child a place and better accept it,\u201d he says. \u201cIn regards to therapy and treatment, unfortunately options are still quite limited, but progress is being made.\u201d \n                     Software pinpoints cause of mystery genetic disorder \n                   \n                     Rare-disease project has global ambitions \n                   \n                     Neurodevelopment: Unlocking the brain \n                   \n                     Neurodevelopmental disorders: Accelerating progress in autism through developmental research \n                   \n                     A de novo paradigm for mental retardation \n                   \n                     Biological insights from 108 schizophrenia-associated genetic loci \n                   \n                     Schizophrenia: Genesis of a complex disease \n                   \n                     American Association on Intellectual and Developmental Disabilities \n                   \n                     Inclusion Europe (Association for people with intellectual disabilities and their families)  \n                   \n                     Utah Genome Project \n                   \n                     World Health Organization on mental health \n                   \n                     1000 Genomes project \n                   \n                     Human Gene Mutation Database \n                   \n                     Online Mendelian Inheritance in Man  \n                   \n                     Open Biological and Biomedical Ontologies \n                   \n                     Phevor Web \n                   \n                     The Variant Annotation, Analysis and Search Tool \n                   \n                      Pedigree Variant Annotation, Analysis, and Search Tool  \n                   \n                     DeNovoGear \n                   \n                     Genome Reference Consortium \n                   Reprints and Permissions"},
{"file_id": "508133a", "url": "https://www.nature.com/articles/508133a", "year": 2014, "authors": [{"name": "Vivien Marx"}], "parsed_as_year": "2006_or_before", "body": "Beams of charged particles can treat cancer more safely and effectively than X-rays. Physicists and biomedical researchers are working to refine the technology for wider use. Clinicians attack cancer with many types of weapon, ranging from scalpels to physically remove all or most of a tumour to drugs that kill the tumour cells where they are. In about half of people with cancer, doctors go after the malignant cells with ionizing radiation. Classic radiation treatment involves mainly X-rays. But because these lose energy all along their path through the body \u2014 damaging healthy cells as they go \u2014 clinicians and researchers are increasingly paying attention to beams that use charged particles such as protons and carbon ions 1 . Charged particles can deposit most of their lethal energy mainly at the tumour site, largely sparing the healthy tissue. Protons are slightly more lethal to cancer cells than X-rays, and carbon ions seem to be around 2\u20133 times as deadly. Worldwide, around 100,000 people have received proton treatments for cancer. Japan, China, Germany and Italy have built ion-beam facilities that have treated some 12,000 patients with carbon ions, the majority in Japan and Germany (see 'Carbon count'). Carbon ions are heavier than protons, so the facilities to deliver them are pricier. The charged-particle facilities in Germany and Japan cost between US$130 million and $200 million each to build. Nonetheless, there has been a spike in research and clinical activity to use charged particles more broadly for cancer treatment, and a hope that as the technology evolves, the price will come down. Clinical trials and new types of radiobiology assay are under development to study the molecular effects of carbon ions as well as helium, lithium and oxygen ions, often referred to in cancer research as 'heavy' ions because they are heavier than protons. Advancing this approach to patient treatment will require further development of particle-accelerator and beam-delivery technology. An international community of clinicians, researchers and technology developers are working to make it happen. The United States began treating patients with protons in the 1950s and has funded research in this area 2 , but it has lagged behind Japan and Germany in advancing other ion therapies. The United States now has 14 facilities for proton treatment and none for carbon-ion treatment. In Japan, radiation oncologist Hirohiko Tsujii of the National Institute of Radiological Sciences (NIRS) in Chiba has devoted his career to advancing carbon-ion therapy. Chiba is home to the first of Japan's four carbon-ion facilities, all of which the NIRS oversees, and treated its first patient in 1994. In Germany, the Heidelberg Ion-beam Therapy Center (HIT) has been treating patients since 2009 with protons and carbon ions 3 . In both countries, the development and construction of these facilities was helped along by public funding for physics-based facilities, as well as investments from large companies such as Siemens Healthcare, based in Erlangen, Germany, and Hitachi, Mitsubishi, Sumitomo and Toshiba, all based in Tokyo. The published clinical results from patients in Germany and Japan are causing the United States to shift its stance, Tsujii says. The US National Cancer Institute (NCI) now wants to fund domestic research on carbon ions and other ion species, and to help fund an international clinical trial. \n               Fire away \n             Standard radiation treatment involves firing a barrage of X-rays at tumours in the hope of stunting their growth by breaking their cells' DNA. But some areas of tumours seem to be resistant to X-ray damage, and those that are not can often repair it. And many X-rays pass through tumour cells without hitting the DNA, leaving the cells unscathed. X-ray beams can also hit tissue adjacent to a tumour, which can set off molecular events leading to tumour formation. For example, children who survive cancer after radiation treatment face a heightened risk of secondary cancers later in life. This kind of collateral damage has been reduced by advances in radiation oncology, such as the ability to deliver beams from different angles and with varying intensities so that they converge on the tumour. But improvement is still needed. For example, when treating head and neck cancers with conventional radiation, \u201cwe do a reasonable job\u201d, says Stephen Hahn, a radiation oncologist at the Perelman School of Medicine at the University of Pennsylvania in Philadelphia \u2014 but the X-rays can harm healthy tissues nearby, such as the heart, oesophagus or lung. Charged particles, by contrast, can mostly avoid healthy tissue. Stripped of their electrons and accelerated to some 70% of the speed of light, they pass through healthy tissue without interacting strongly with the atoms there. Once their speed drops to a certain level, however, they abruptly deposit almost all of their energy in \u201ca dramatic bam\u201d, says Arnold Pompo\u0161, a physicist at the University of Texas Southwestern Medical Center in Dallas. This deposition of energy is known as the Bragg peak, named after physicist William Henry Bragg, who discovered the behaviour in 1903. Charged-particle beams can be tuned so that particles reach their Bragg peaks right at a tumour, where they do the maximum damage. \n               Ready, aim, scan \n             The researchers at the HIT use \u2014 and are continuing to develop \u2014 an approach called raster scanning, in which a source readies one or several types of ion that are then accelerated. Next, the beams are extracted slowly in bunches, each of which has hundreds of individual Bragg-peak positions tailored to deposit radiation throughout the tumour. The position, size and intensity of these collections of beams are measured around 100,000 times a second to ensure safety and precision. The beams fill the tumour's contours as a hand fills a glove, says Thomas Haberer, chief technology officer of the HIT, who developed the technology while working at the GSI Helmholtz Centre for Heavy Ion Research in Darmstadt, Germany. (The HIT is a spin-out of that centre.) \u201cClinically, we use protons and carbon-ion radiation as is needed from one patient to the next,\u201d says Haberer. He and his team are working to add oxygen, helium and other ions to the mix. The researchers also want to increase the number of ion species they use, because the heavier ones stay focused even when travelling deep into the body, as in the treatment of prostate cancer, he says. The heavier the ion, the greater the possible damage to solid tumours. For tumours that contain a mix of healthy and diseased cells, the HIT team chooses protons to avoid injuring the healthy ones. During particle-beam treatment, a patient lies on a couch wearing a stiff plastic garment that positions them with millimetre-scale precision in reference to a computed-tomography image of the tumour, says Haberer. A tumour can shift as the patient breathes, so the system must also adjust to account for this change. In collaboration with several research groups, Haberer and his colleagues are working on ways to use the raster-scanner approach to refine how a tumour can be tracked during treatment. \u201cIn the lab, the prototype is working well,\u201d he says. \u201cIt will take a while for it to reach clinical application, at least a few years.\u201d At the HIT, clinicians have applied carbon-ion therapy to tumours in the brain and at the base of the skull, and to head and neck cancers. And they have begun treating cancers of the liver and pancreas, recurrent rectal and prostate cancers, and paediatric bone cancer. Children are regularly treated with proton beams, and the team wants to launch clinical trials of carbon ions for paediatric cancers, says Haberer. When treating cancers of organs in the torso, clinicians use 'spacers', which can be made from a variety of materials, to physically shift healthy tissue adjacent to the tumour out of the way of the Bragg peak. For example, Tsujii says, in carbon-ion therapy for colon cancer, \u201cwe put a spacer between the tumour and the intestines\u201d. Tsujii points to the expanding range of tumour types that have been treated in Japan with carbon ions. When people with rectal cancer are treated with surgery, he notes, around 15% develop recurrence within 3\u20135 years. Another surgery is an option for only 10\u201340% of them. When these patients are treated with carbon ions, only 10% of them develop a second recurrence, compared with 30\u201370% of those treated with X-rays. Patients eventually succumb to metastases, but Tsujii nonetheless finds the results so far promising. Another study under way in Japan is looking at combined chemotherapy and carbon-ion therapy to treat people who have inoperable pancreatic tumours, and at pre-operative carbon-ion therapy for pancreatic cancer that can be removed surgically. Carbon-ion therapy may take less toll on the patient and reduce treatment times \u2014 for liver cancer and early-stage lung cancer, carbon ions are delivered in one or two sessions over as many days, compared with 10\u201330 sessions over many days or weeks for X-ray therapy. \n               Growing the technology \n             Protons hinder the growth of some tumours better than X-rays, says Herman Suit, a proton-therapy pioneer at Massachusetts General Hospital in Boston 4 . Some of the published results from carbon-ion treatment centres in Japan and Germany are \u201cimpressive\u201d, he adds, such as those for tumours at the base of the skull, renal cancer and mucosal melanoma of the head and neck. He would like to see clinical trials comparing protons and carbon ions. Carbon-ion therapy has so far been used mainly on tumours that are difficult to remove surgically and risky to treat with classic radiation. Tumours at the base of the skull, for example, are near nerves, brain tissue and the cochlea of the inner ear, where X-ray exposure could cause debilitating damage. Hahn says that some charged particles deliver \u201ca more powerful punch\u201d to tumours and that he and most of his colleagues now accept that carbon-ion therapy performs well on the most challenging tumours. The next step is to see whether charged particles are right for more common diseases such as lung cancer. \n               Comparing beams \n             According to the NCI, the first international clinical trial on charged particles is now being planned. Slated to last 3\u20135 years, the randomized phase III trial, which will examine efficacy and dose, will compare X-rays, protons and carbon ions in the treatment of cancers of the pancreas, liver, head and neck, as well as bone and soft-tissue tumours and recurrent rectal cancer. The NCI will contribute funding for the trial and is soliciting proposals from US institutions and from carbon-ion facilities in Japan, Germany, Italy and China. The trial presents many logistical challenges in addition to that of obtaining the necessary scientific review and approval. Facilities for charged-particle therapy are much less common than those for X-ray treatment, so patients will need to travel further to reach them, especially the carbon-ion centres. The NCI also wants to fund domestic research into charged-particle therapy. James Deye, a programme director for extramural radiation research at the NCI, and his colleagues are poring over project applications from institutions vying to set up the first US research centre for particle-beam radiation therapy. The NCI funds research, not construction, Deye says, so applicants must find the money to build, over the next 5\u201310 years, a research facility that can handle treatments with protons, carbon ions and other ion species. Grant recipients will be announced later this year. One of the applicants is the University of Texas Southwestern Medical Center, where radiation oncologist Hak Choy wants to put classic radiation, protons and heavy ions all under one roof. A new X-ray facility is set to open by 2016; a proton-therapy suite is slated for 2017; and Choy hopes to have a carbon-ion facility by 2021. The project is under review with the Texas state government, and Choy is hopeful about receiving support so that construction can begin. \n               Accelerate and deliver \n             Accelerators and beam-delivery systems are crucial components in directing charged particles at patient tumours, but their size and operation costs are some of the factors hindering their widespread use. To help to make the use of this technology more feasible, some scientists are trying to make these systems more compact. One overhaul researchers are targeting is of the gantry, a massive rotating platform that delivers ion beams to patients at any angle. To achieve different angles, the gantry directs the ions with huge, powerful magnets, which weigh it down and raise its electricity use. The gantry at the HIT is as tall as a commercial passenger aircraft and weighs around 600 tonnes. Because carbon ions have more momentum and charge than protons, a beam of them is around 2.5 times harder to bend than a proton beam, says Stephen Peggs, an accelerator physicist at Brookhaven National Laboratory in Upton, New York. Using stronger magnets on the gantry would be beneficial, he says. Superconducting magnets could be an option, he adds, because they are not limited to a magnetic strength of 1 Tesla, as conventional magnets are, and could have strengths closer to 4 or 5 Tesla. That boost could reduce the gantry size, because the greater the force the magnet exerts on the beam, the more readily the beam can be bent in a smaller radius, reducing the size of the system. Peggs and his team are also working to improve the accelerator itself. Charged-particle therapy accelerators could be more compact, he says, if they cycled more quickly. The Brookhaven scientists are building a prototype of a rapid-cycling accelerator, which extracts beams more often than the slow-cycling accelerators now used in carbon-ion treatment facilities 5 . In the new system, fewer ions travel around the accelerator track at any given time, and these doses are extracted cyclically and delivered to the patient. The use of fewer ions and more frequent extraction reduces the size of the beam pipe and the other system components, including the conventional magnets used on the accelerator to direct the beam. Such design changes also cut power use relative to that of slow-cycling systems. The Brookhaven researchers are testing this concept by building a fast-cycling accelerator to deliver multiple ion species, including lithium, neon, helium and carbon. It will be able to deliver a salvo of one ion species and then quickly switch energy levels and deliver another, Peggs says. The scientists are building the system components but want to assemble them at a location \u2014 not yet determined \u2014 where they can be applied to biomedicine. The prototype could be used as a radiobiology research facility to continue maturing the technology for charged-particle therapy, Peggs says. To help with technology transfer, the Brookhaven scientists have partnered with Best Medical, a company in Springfield, Virginia, that builds radiation facilities. Krishnan Suthanthiran, the company's chief executive, sees a market for a rapid-cycling system. His company has spent around $5 million on the partnership thus far and expects it will take another $10 million to $15 million to build a working system by 2016. This year, he is applying for approval from the US Food and Drug Administration to build a facility. He estimates that a therapeutic centre will cost between $30 million and $100 million, depending on how many treatment rooms it has. It may or may not need a gantry and will probably require less radiation shielding than current carbon-ion facilities, reducing cost and footprint. Haberer says that he likes Peggs' concept because of its robust, steady way of cycling, which is perhaps even more stable than that of a slow-cycling system. But until a rapid-cycling system is built and used in therapy, it will not be easy to compare systems, he says. Superconducting magnets would allow the facility's dimensions to be reduced, but they do not yet work quickly enough. \u201cAt present, these magnets are slow \u2014 which would mean fewer patients could be treated, impinging on the facilities' sustainability,\u201d Haberer says. And, he notes, a rapid-cycling accelerator is better able to extract the exact dose at the right time, but it could be difficult to monitor the beams extracted to ensure their dose and directionality. To further explore the potential uses and ideal dosages for charged-particle therapy, scientists want to expand basic radiobiology research. Scientists perform this work at charged-particle therapy centres and in facilities such as the NASA Space Radiation Biology Lab at Brookhaven, for example. Kathryn Held, a radiobiologist at Massachusetts General Hospital in Boston is working on ways to get molecular information about the effects of different ion species on tumour cells \u2014 information that could help researchers to develop dosage regimens. New approaches to studying cell survival, changes in cell cycle and cell death will help researchers to explore why charged particles have more tumour-killing power than X-rays. Measures such as standard 'clonogenic assays', in which scientists irradiate cells and then see whether they continue to grow and form colonies, suggest that protons are slightly more effective than X-rays at killing cells, whereas carbon ions are about 2\u20133 times more effective. Scientists have a number of hunches about why this is so. One is related to the fact that X-rays tend to be spread out, meaning that many of them pass through the cell without hitting DNA, says Held. Ions such as carbon are heavier and bigger than protons, have a greater positive charge and move more slowly through tissue or cells, creating a thicker track of ionization. This track seems to produce clusters of damage, such as breaks in one or both strands of DNA, and damage to neighbouring nucleotides (see 'Greatest hits'). When one strand of DNA breaks, repair enzymes use the sequence of the other, intact strand as a template to fix the helix's rails and rungs. But double-strand breaks are harder to repair accurately, because there is no intact template from which to reconstitute the DNA. A cluster of damage adds to the challenge. All of this helps to explain why carbon ions are more lethal, says Held. Indirect evidence \u2014 computer simulations or the use of antibodies to detect DNA-repair enzymes \u2014 helps to explain the type of injury that charged particles inflict on tumour cells. \u201cHowever, we do not have good assays to quantify, or identify the composition of, those clustered lesions,\u201d Held says. \u201cDNA assays that more specifically measure the various types of possible clustered DNA damages would be very useful.\u201d Another reason charged particles may pack more punch to cancerous cells is related to tumour physiology. As tumours grow, oxygen-poor regions develop, and these areas seem to be resistant to classic radiation treatment. That is because X-rays kill cells by producing free radicals \u2014 reactive molecules formed from the water in cells and tissues \u2014 which then react with DNA to produce other destructive radicals. More oxygen exacerbates the damage these radicals can cause, and less oxygen weakens their effect. Researchers think that charged particles such as carbon ions may not lose their destructive power in low-oxygen regions. Basic radiobiology research on charged particles can feed into clinical practices in current and future facilities. Radiation oncologist Anders Brahme at the Karolinska Institute in Stockholm, who has spent his career working on charged-particle therapy, says that he sees great potential for cancer treatment in such therapies and is excited that they are drawing global interest. In his view, charged particles offer the chance for radiation oncology to move from a cancer treatment to a cancer cure. \n                     Blogpost on proton treatment for prostate cancer \n                   \n                     European boost for particle therapy \n                   \n                     Charged particle therapy\u2014optimization, challenges and future directions \n                   \n                     Charged particles in radiation oncology \n                   \n                     Particle Therapy Co-Operative Group \n                   \n                     Heidelberg Ion Therapy Treatment Center \n                   \n                     National Association for Radiological Sciences in Japan \n                   \n                     Gunma Heavy Ion Medical Center in Japan \n                   \n                     National Association for Proton Therapy \n                   Reprints and Permissions"},
{"file_id": "509645a", "url": "https://www.nature.com/articles/509645a", "year": 2014, "authors": [{"name": "Vivien Marx"}], "parsed_as_year": "2006_or_before", "body": "The first draft of the complete human proteome has been more than a decade in the making. In the process, the effort has also delivered lessons about technology and biology. In 2003, two years after the Human Genome Project published a first-draft sequence of the roughly 20,000 genes that define  Homo sapiens , a group of Swedish researchers launched the Human Protein Atlas (HPA) \u2014 a large-scale effort to map where the proteins encoded by those genes are expressed in the body's tissues and cells. Few proteins had been localized in that way, explains Mathias Uhl\u00e9n, a protein researcher at the Royal Institute of Technology in Stockholm, and the HPA's principal investigator. A comprehensive atlas of the human 'proteome' would set the stage for more-sophisticated study of protein function, he says. It would reveal the array of membrane proteins, which ferry molecules in and out of cells, and expose the 'secretome' \u2014 the proteins secreted by cells in health and in disease. It would guide exploration into the physiological impact of genetic variation. And it would help drug developers to predict where a candidate drug might interact with a protein or cause side effects (see  'Uses for the Human Protein Atlas' ). The project's annual releases of preliminary maps and data have already yielded surprises, says Fredrik Pont\u00e9n, a pathologist at Uppsala University in Sweden and a co-founder of the HPA. For example, the maps show that some 3,500 genes encode proteins specific to one tissue type or a small group of tissues, and that the testis contains one-third of those proteins \u2014 more than in any other organ 1  (see 'Our proteins, ourselves'). In November, the team plans to upload data completing the first draft of the human proteome. In total, it includes nearly 15 million high-resolution micrographs of stained tissues and cells and presents data on about 80% of human proteins. This information was gleaned from 46 human cell lines and tissue samples from 360 people \u2014 44 normal tissue types from 144 people, and the 20 most common types of cancer, from 216 people. \u201cWhat's most exciting is the scale of it,\u201d says Pont\u00e9n. Another project \u2014 dubbed the subcellular protein atlas \u2014 will locate proteins within cells, and will be completed next year. The samples used for the HPA came from people who agreed to donate tissue while they were being treated for various conditions. In accordance with Swedish research-ethics guidelines, all samples have been anonymized so that they cannot be traced to their donors. The HPA is not the only protein-mapping venture. Other efforts include the Human Proteome Project, which is loosely affiliated with the HPA but uses some different strategies, and projects in individual labs or among groups of researchers. Two drafts of the human proteome, based on mass spectrometry, are presented in this issue of  Nature 2 , 3 . In this approach, tissues are processed and their proteins broken into fragments. The fragments are ionized and then separated according to their mass and charge, which helps to measure, sort and identify the proteins. The HPA is the only large-scale mapping effort based on antibody-profiling methods, in which chemical stains and antibodies are used to locate and identify proteins in tissues (see 'Express yourself'). The project's scientists used a 'brute force' approach that includes some automation but requires many manual steps. Applying their techniques to the many proteins in the body is daunting \u2014 the sum of human proteins exceeds the number of genes by far, and could run in the millions. With funding from the Knut and Alice Wallenberg Foundation in Stockholm, the HPA researchers \u2014 140 scientists in 13 groups working mainly in Stockholm, Uppsala or Mumbai, India \u2014 divided up the work. They scaled up standard proteomics approaches, such as immunohistochemistry, in which antibodies and stains are used to visualize proteins in tissue samples, and western-blot assays to check the specificity of antibodies. There is a need for such corroboration \u2014 often, different approaches yield contradictory results. The researchers have spent time, effort and resources to ensure that the antibodies they use work as expected, and to develop reliable ways to interpret and classify the stained samples, Uhl\u00e9n says. \n               Scaling up \n             The researchers have also struggled with the general lack of automation in proteomics, says Caroline Kampf, a proteomics researcher and director of the HPA's Uppsala site, who joined the project when it began. That situation has improved over the past decade. The task of locating proteins begins with preparation of tissue samples that Kampf and her team receive from Uppsala Biobank, which oversees management of the tissues. The scientists consult research literature and protein databases and also study the tissue's messenger RNA (mRNA), a molecule that carries the information used to manufacture proteins from a DNA sequence. In 2012, the researchers began using high-throughput mRNA sequencing, or 'RNA-seq', which has enabled them to more quickly obtain data about the set of genes expressed as proteins in a given tissue. This approach has helped to validate results from antibody-based tissue profiles, says Kampf. The Human Protein Atlas has delivered many lessons about tissue staining and large-scale projects. Tissue preparation involves preserving tissue samples in blocks of paraffin and then processing them into microarrays, groups of tiny tissue samples arranged in a grid to enable scientists to test for the presence of many proteins. Only a fraction of the tissue is used for the microarray; the rest is kept in a repository 4 . For the microarray, cylindrical 'cores' of tissue of around 1 millimetre in diameter are punched out of the paraffin blocks. These cores are embedded in rows and columns in another paraffin block, which is then sliced into thin sections and placed on a slide for staining. The researchers produce around 100 slides a day. \n               Tech support \n             Although production of tissue microarrays can be automated, says Kampf, plenty can go wrong, and skilled technicians are needed for troubleshooting and for their craftsmanship. For example, she says, a technician needs the right touch to know when to stop tissue punching, or when the punch is stuck. The degree of intervention required depends in part on the sample \u2014 differences in texture mean that some tissue types are more challenging than others. Skin, for example, is tougher than fatty breast tissue. To accelerate production, the scientists group similar tissue types for similar processing steps. Some steps still require manual labour. \u201cBut when punching cell lines or punching cancers, you can more easily let a machine do it because [the tissue has] a more homogeneous composition,\u201d Kampf says. Once a tissue sample is on the slide, it is treated with reagents that bind specifically to one protein and not another. Another challenging aspect of the procedure is that the concentration of a protein in a given tissue can vary within and between samples. In the body, protein abundance varies by as much as 1-million-fold. The abundant proteins can swamp out the more scarce ones and make them hard to detect. It is always easier to interpret your results if you spent time and effort validating reagents. There are many types of affinity reagents, both biological and synthetic. The HPA uses polyclonal antibodies, which recognize portions of specific proteins. They are produced by injection of an antigen into laboratory animals and later harvesting of the antibodies the animals produce in response. These antibodies are not identical to one another. The HPA teams have developed methods of antigen design and antibody purification that maximize the specificity with which an antibody latches on to a protein, raising the probability that it will locate one protein only. To date, researchers have validated 37,000 antibodies for the project. The antibodies developed in the course of the HPA project are available through Atlas Antibodies, an HPA spin-out in Stockholm. The company now sells 17,000 polyclonal antibodies developed by the HPA, and expects to add another 2,000 by November, says Marianne Hansson, the firm's chief executive officer. What is special about this collection of antibodies is that they are produced in a uniform fashion and are 'proteome-wide', says Henrik Wern\u00e9rus, chief scientific officer at Atlas. One validation test for antibodies is the tried-and-true western blot, in which proteins are propelled through a gel by an electric field and separated into fragments of different molecular weights. The fragments are then transferred to a material such as nitrocellulose paper and probed with a primary antibody, which binds the protein of interest, followed by a secondary antibody that binds the primary one and bears a fluorescent or enzymatic tag to enable detection of the protein. Each step in the mapping process takes time, and there are bottlenecks. For example, the project's tissue slides quickly piled up. An automated system cannot readily emulate a pathologist changing the focus on a microscope to study a tissue's protein-expression patterns. Since the HPA began, automated slide readers have become available and are now used to supplement the educated eyes of the project's pathologists. Kampf says that the first slide scanners her team used could handle around 10 slides a day. But she needed to scan more than 100 slides a day. Now, she says, a scientist can \u201cgo home, and when you are back in the morning, they are all there\u201d. But even as automated slide scanning emerged, Kampf and her staff still had to load the slides manually. And the first auto-loaders were far from perfect. Kampf recalls coming into the lab after a coffee break to find the floor covered in glass because an instrument's grippers had smashed the slides against the wall. She spent many an evening on the phone with companies in various time zones to address technical issues. Kampf says that scaling up is not just about data generation and analysis. \u201cYou encounter many other things that you haven't expected,\u201d she says. The staining patterns in tumour tissue, for example, can be difficult to interpret, in part because such tissues contain a mix of normal and diseased cells. Cancer cells also show variation \u2014 within a tumour as well as between tumour types and patients. Given this variability, data need to be carefully evaluated. \n               Object lessons \n             Pont\u00e9n says that the HPA has delivered many lessons about immunohistochemistry and large-scale projects. The scientists have found, for example, that RNA-seq data can help to shore up the results of immunohistochemistry. And they have grappled with differing results between immunohistochemistry and western blots. Whereas the proteins analysed in a western blot have been denatured into two-dimensional fragments, those probed in tissue samples are more likely to retain their three-dimensional (3D) structure. That distinction could lead to differing results, as some antibodies recognize proteins only in their 3D form. One solution has been to produce the antibodies in a different way, says Pont\u00e9n. The antigens used conventionally to create antibodies are peptides, short strings of amino acids. In their approach to making antigens, the HPA team designed longer protein fragments. The fragments are 50\u2013150 amino acids in length and are selected to have the highest likelihood of yielding unique proteins, and to contain two or three epitopes \u2014 the sites that antibodies recognize on a protein. This multiplicity increases the likelihood that the resulting antibodies will work in western blots, immunohistochemistry and other techniques, Pont\u00e9n says. The results of these assays depend not only on antibody affinity, but also on the relative abundance of a target protein, says Uhl\u00e9n. Because it can be tough to detect low-abundance proteins, results from antibody staining should be validated with a different antibody for the same protein, for example, or by checking the RNA-seq data to be sure that the RNA that gives rise to the protein in question is present in the tissue, he says. Antibody validation has \u201cturned out to be even more important than expected\u201d, says Emma Lundberg, who directs the subcellular protein atlas project, which shows where in a cell a particular protein is present. Antibodies can have affinity for proteins other than their targets, leading to cross-reactivity that can confuse protein-mapping results. \u201cIn the end, it is always easier to interpret your results if you spent some time and effort validating your reagents,\u201d she says. Kampf says that she has, over the years, received many e-mails from scientists asking when their favourite proteins will be published in the atlas. All she could tell them was to be patient. The process, from designing an antigen to having fully validated results ready to upload to the HPA website, takes 9\u201312 months on average. The researchers still have to refine the map, says Pont\u00e9n. There are many low-abundance proteins, among others, to find. And some proteins have been overlooked because they are expressed only at certain times during development, says Pont\u00e9n. Others have gone undetected because they are present in tissues that the HPA has only begun to collect, such as the retina. The team hopes that the research community will help with spotting inaccuracies in the map. Pont\u00e9n says that it will take at least another five years of curation to offer the research community HPA data that have been analysed and assessed to the level of 'textbook' authority. \n                     Europe: Swedish success story \n                   \n                     Antibodies: The generation game \n                   \n                     Next-generation proteomics: towards an integrative view of proteome dynamics \n                   \n                     Web Focus: Proteomics \n                   \n                     Nature Reviews Focus on Proteomics \n                   \n                     The Human Protein Atlas \n                   \n                     Human Proteome Project \n                   \n                     Atlas Antibodies \n                   Reprints and Permissions"},
{"file_id": "515293a", "url": "https://www.nature.com/articles/515293a", "year": 2014, "authors": [{"name": "Vivien Marx"}], "parsed_as_year": "2006_or_before", "body": "The processes behind neuronal communication have not yet been resolved in detail, but dyes, microscopy and protein analysis are beginning to fill in the gaps. Kiss-and-run sounds like a schoolyard prank, but it is also the informal name for one of four vigorously debated hypotheses about what happens in neurons in the brain before and after they transmit signals to one another at the cell-to-cell junctions called synapses. There are myriad ways in which this delicate messaging can be upset. Drugs such as cocaine or methamphetamine increase the release of the neurotransmitter dopamine, for example. Disorders such as Parkinson's disease involve damage or destruction of the neurons that release dopamine. And depression is linked to altered levels of neurotransmitters such as dopamine and serotonin. A better understanding of synaptic events is crucial to fighting a wide range of disorders, from drug addiction to mental illnesses \u2014 to say nothing of forging a better understanding of the brain. Scientists particularly want to see how the neurotransmitters travel within the synapse and how they are launched on their journey to the neuron on the other side. Investigators agree on the basics. A neuron packages its neurotransmitters inside vesicles, which are bubble-like entities around 50 nanometres in diameter. Vesicles are not unique to neurons \u2014 they shuttle molecules around in many of the body's cells. But a single neuron can have hundreds of thousands of vesicles, and some even have a few million. Researchers also know how these vesicles are called into action. When a neuron is activated, it fires an electrical pulse down its axon: a long fibre that behaves something like an electrical cable. When the pulse reaches the axon's tips, where most of the synapses lie, the vesicles there respond by moving to the synaptic membrane, merging with it and releasing their neurotransmitters. These messenger molecules then migrate to the neighbouring neuron across a gap called the synaptic cleft. Much less clear is exactly what happens next. \u201cIt is hard to get people to agree on much,\u201d says Silvio Rizzoli, a neurobiologist at the University Medical Center G\u00f6ttingen in Germany. The vesicles must reform and refill with neurotransmitters very quickly, he says, otherwise the neurons would lose their ability to communicate and we would be paralysed in a matter of minutes. Working out how that happens has drawn researchers from a wide range of disciplines, from cellular and structural biology to physiology, physics and microscopy. \u201cThere is so much more to find out about how the single synapse operates,\u201d says electrophysiologist Ege Kavalali of the University of Texas Southwestern Medical Center in Dallas. There are four major hypotheses. Kiss-and-run, in which the vesicle empties its cargo through a pore in the membrane, then retreats back into the neuron; full-collapse fusion, in which the vesicle melds with the membrane and a new one emerges with the help of a protein called clathrin; bulk endocytosis, in which numerous vesicles fuse with the membrane, which then forms a 'bleb', or bulge, that pinches off to form new vesicles; and ultrafast endocytosis, which is essentially a sped-up version of bulk endocytosis in which pinching off happens in milliseconds, rather than seconds (see 'Firing four'). There is evidence to support each hypothesis \u2014 which might not be mutually exclusive \u2014 and they are the topic of much debate. Bulk endocytosis is the easiest to study, says Erik Jorgensen, a neuroscientist at the University of Utah in Salt Lake City. But full-collapse fusion is the one for which there is the most experimental support. Almost all the molecules involved in this process have been identified, says Rizzoli, although it is not yet clear how the clathrin-mediated process is regulated. And there are possible technical flaws in experiments that support the kiss-and-run hypothesis. \u201cIt has been named a biophysicist's fantasy by at least one very prominent synapse investigator,\u201d Rizzoli says. Settling these differences is challenging \u2014 and each model may have a place. The number of molecules involved in the full-fusion process suggests that only around 10% of the vesicles at a synapse are releasing their cargo. Bulk endocytosis might be needed for the release of larger amounts of neurotransmitter. But it is not clear when larger numbers would be needed. Rizzoli's team looked at neurotransmitter release in locusts that had been eaten by frogs but quickly removed from the frog's stomach 1 . \u201cEven under the extreme stress of being chased and eaten, the locust's synapses did not use more than 5% of their vesicles at one time,\u201d he says. \u201cSo in an  in vivo  context, the protein numbers of the clathrin pathway are completely sufficient.\u201d \n               Tag trackers \n             Imaging can help to prove or disprove these hypotheses \u2014 and each type has both advantages and disadvantages. Electron microscopy has high resolution, but can work only on dead cells that have been chemically fixed. Fluorescence microscopy can image live cells, but will pick up components only if they can fluoresce under a particular wavelength of light \u2014 the rest of the neuron will remain dark. Researchers have been trying to get around these limitations by using dyes and protein analysis and by combining microscopy techniques. Styryl dyes, for instance, can latch on to the membrane of the vesicle and thus help to image the vesicle as it travels in the neuron. But they also stick to dying and dead cells, which clutters up the data, says Kavalali. As cells die, the lipids are moved around and this 'lipid scrambling' can lead to fluorescence not connected to vesicles. Scientists have therefore tried to tweak the structure of the dyes so that they bind only to lipids in the vesicle membrane, with little success. Another approach is to genetically engineer the neuron's proteins to express tags that fluoresce under certain circumstances, creating 'reporter' molecules that can be used to trace what a vesicle is doing and where it is. SynaptopHluorin is a well-known reporter used in vesicles 2 . It fluoresces green with a change in pH, which happens when the vesicle releases the neurotransmitter. Another pH-sensitive probe is pHTomato 3 , which shines red when the pH rises. \n               Technicolour glory \n             The availability of probes that fluoresce in different colours lets scientists monitor multiple proteins in the vesicle and neuronal membrane. But there are challenges: it can be unclear whether the movement is coming from the vesicle or just the protein, says Kavalali. To better understand synaptic dynamics, Rizzoli decided to identify the abundances and positions of neuronal proteins. He likens the pursuit to ecology research. A nineteenth-century scientist studying an area would note the plants, the deer that eat the plants and the tigers that hunt the deer. Understanding the ecosystem in depth requires more than just counting the plants, deer and tigers, says Rizzoli. Knowing their locations and interactions is important as well. Similarly, some proteins and their functions in the synapse are known, but until we map the organization of their mini-jungle we will not know how they all function together, he says. Around 20 years ago, Rizzoli's PhD adviser, physiologist William Betz, now at the University of Colorado in Aurora, developed dyes that can label living cells. But the cells needed to be chemically treated for the labels to identify the location of the proteins, and that made the dyes come off the membranes and get stuck elsewhere in the cell, resulting in poor quality images. To try to find something better, Rizzoli and his team tested every marker available, including lipid tags, protein-coupled labels and simple stains. Nothing worked, he says. So he set out to synthesize a better one. The result was membrane-binding fluorophore-cysteine-lysine-palmitoyl (mCLING) 4 . This marker clings to membranes and keeps clinging even after the neuron is treated with fixatives for imaging. In synapses, it labels mainly vesicles because there are few other membranes around, he says. Its chemical groups react more efficiently with fixatives than traditional probes and a long lipid tail anchors the probe in the membrane more strongly than the short tails of other commonly used probes. Using mCLING, Rizzoli can track individual vesicles and see the neuron's membrane fold inward and reform a vesicle after neurotransmitter release. Rizzoli has also scaled up his protein analysis. He and his team used data on protein types numbers and positions obtained from techniques such as staining, mass spectrometry, electron microscopy and super-resolution fluorescence microscopy (the technique responsible for this year's Nobel Prize in Chemistry), to build a three-dimensional computer model 5  of an average synapse down to a resolution of 40 nm. Rizzoli's team looked at the abundances of 62 different proteins in the synapse. The amounts differ wildly; there are 27,000 copies of one protein, for example, and 50 of another. \u201cTogether, they all sum to about 300,000 proteins in the model of the average synapse,\u201d he says. The researchers do not yet fully understand what they all do, but they hope that this grounding will set them on the right path. One thing Rizzoli can say, is that the role a protein has in synaptic vesicle trafficking correlates strongly with its abundance in the neuron. The proteins involved in fusing the synaptic vesicle with the neuronal membrane are up to three levels of magnitude more abundant than those involved in the recovery of vesicles from the membrane after fusion. This correlation can guide other researchers. A colleague recently contacted Rizzoli about a neuronal protein of unknown function. On the basis of its abundance, the pair concluded that it probably plays a part in the neuron's active zone, a region near the synaptic membrane where the neurotransmitter-filled vesicles wait to be called into action. But it is not plentiful enough to be involved in the retrieval of vesicle molecules, says Rizzoli, \u201cwhich gives my colleague a nice place to start his experiments\u201d. These data also suggest that scientists may have misstepped when they did experiments that involved boosting the expression of proteins. Such overexpression experiments \u201ccan be quite misleading\u201d, says Kavalali. \u201cYou don't know if what you are seeing are the genuine properties of this protein.\u201d \n               More microscopy \n             Rizzoli now wants to exceed the 40-nm resolution of his existing approach. He has been working for nearly a decade with physicist Stefan Hell at the University of G\u00f6ttingen, who has invented a type of super-resolution microscopy called stimulated emission depletion (STED) and is one of the three 2014 Nobel laureates. Owing to a fundamental limit imposed by the physics of light waves and the aperture of the lenses, fluorescence microscopy cannot discern objects closer to one another than 200 nm, which is almost the size of the synapse. Super-resolution microscopy finds various ways around this limit. For instance, STED starts with a laser that switches on fluorescent molecules in a 200-nm spot. A second laser then turns off all the molecules at the spot's edges. \u201cThe stronger the second laser, the more molecules it turns off,\u201d says Rizzoli, so the central area of fluorescence can be as small as 30 nm in a synapse. So, by scanning across a synapse one 30-nm region at a time, and measuring the fluorescence at each point, the STED system can build up an ultra-high-resolution image. \u201cThe procedure makes the initial blurry image far more clear,\u201d says Rizzoli (see 'Peek inside'). Rizzoli is also exploring isotopic microscopy, in which an ion beam plays the part of a light source in a traditional microscope and mass spectrometry detectors act as cameras. The instrument reconstructs an image of the sample as the beam burns off a sample's surface one atomic layer at a time then measures the atoms leaving the sample. \u201cThe resolution along the vertical axis is close to atomic size,\u201d he says. \u201cSo it really has some potential.\u201d \n               Flash and freeze \n             Jorgensen is also harnessing the high resolution of electron microscopy to home in on synaptic dynamics. In work published last December on neurotransmitter release in mouse neurons 6 , he and his team showed that the neuronal membrane folds inwards 50\u2013100 milliseconds after the neuron is stimulated. This work led to the newest hypothesis about synaptic dynamics, ultrafast endocytosis. It suggests that new vesicle form immediately after the vesicle fuses with the membrane and releases the neurotransmitters. For this work, Jorgensen and his team developed a technique nicknamed 'flash and freeze' electron microscopy. The researchers genetically engineer neurons so that a light beam causes the neuron to release neurotransmitters, then freeze different neurons at different time points between 15 milliseconds and 10 seconds after light stimulation and capture images of what is happening at the time. Jorgensen says that his device expands a technique used by researchers Thomas Reese and John Heuser, who did some of the first work on electron-microscopy-based synaptic imaging 7 . The pair developed a freeze slammer, in which neurons were stimulated then thrown against a metal block that had been cooled with liquid helium, freezing their molecules in place. Jorgensen and his team also freeze neurons, but using pressure rather than temperature, which has the advantage of leaving more of the specimen free of ice crystals. They also modified their electron-microscopy system, made by Leica Microsystems in Mannheim, Germany, to allow a path of light into the system's high-pressure freezers. Jorgensen says that they adapted the device out of necessity because \u201cthere is just no other way to look at fast events at the synapse\u201d. Inspired by Jorgensen's idea, Leica is now building a light path into its new high-pressure freezers, known as EM HPM100, says Cveta Tomova, who manages electron-microscopy sample preparation products at Leica. The approach can help researchers to image cellular processes at millisecond and nanometre resolution and to avoid some issues with ice crystals, she says. The researchers can switch on a gene or process with light, then use high-pressure freezing to image the sample at that moment. Freeze-slamming is done at ambient pressure and allows thin samples to be frozen well up to a thickness of around 15 \u03bcm. High-pressure freezing means that researchers can avoid ice crystals down to around 200 \u03bcm under the surface. Jorgensen is now working to tie in super-resolution microscopy techniques such as nano-resolution fluorescence electron microscopy (nanofEM) and super correlative light and electron microscopy (super CLEM), which can achieve resolutions of 20 nm. This gets close to the dimensions of proteins, which are 5\u201310 nm in diameter. His goal is to connect the ability to determine the location of proteins with the 1-nm resolution of electron microscopy. \u201cWe would then be able to characterize the molecular topography of the synapse,\u201d he says. In his most recent work 8 , also using flash-and-freeze electron microscopy, he and his team show how some of the hypotheses about synaptic dynamics might be linked. Using a technique called RNAi, in which small synthetic RNA molecules bind to \u2014 and block \u2014 the messenger RNAs that carry instructions for making proteins, they showed that the clathrin protein is an important component of ultrafast endocytosis, too. They also found that after neurotransmitter release, the bleb in the neuron's membrane separates completely to form what is known as an endosome. Synaptic vesicles then bud off from this endosome, rather than directly from the neuron's membrane. Rizzoli calls the new finding \u201cbulk endocytosis on steroids\u201d. What neuroscientists need, says Kavalali, is high-resolution images of living neurons stimulated to release neurotransmitters. He thinks highly of flash-and-freeze electron microscopy and has high hopes for super-resolution microscopy. These techniques will help researchers to map the synapse and track the events that occur before, during and after neurotransmitter release, and perhaps even to determine which hypothesis is the most accurate. \n                     Depressed mice have excitable neurons \n                   \n                     Ambitious plans for BRAIN project unveiled \n                   \n                     Through the nanoscope: A Nobel Prize gallery \n                   \n                     Neuroscience: New angles on the brain \n                   \n                     Blogpost for Lasker Award for neurotransmitter-release research \n                   \n                     Nature  Methods Focus: Mapping the Brain \n                   \n                     Nature  Insight: Neurotechniques \n                   \n                     Nobel prize for super-resolved fluorescence microscopy \n                   \n                     NIH BRAIN working group report \n                   Reprints and Permissions"},
{"file_id": "511493a", "url": "https://www.nature.com/articles/511493a", "year": 2014, "authors": [{"name": "Vivien Marx"}], "parsed_as_year": "2006_or_before", "body": "Bacteria can coat everything from thermal springs to teeth. Researchers are looking for antibiotics that can subvert the signalling that the microbes use to carve their niche. Bacteria are continually evolving ways to avoid the effects of antibiotics, and with the pipeline of new drugs drying up, infections are becoming more and more difficult to fight. As the need for innovative solutions grows, some microbiologists are teaming up with chemists and engineers to try to find ways to subvert the microbes by interfering with the signals they use to communicate. To undermine the microbes' language, scientists first need to work out what they are saying. Bacteria use chemical signals to synchronize behaviour across a population. That behaviour can help us \u2014 in the digestion of food, say \u2014 but it can also kill us. Such molecular coordination is thought to be central to the formation of biofilms \u2014 slimy mats of bacteria that spread across surfaces such as hospital catheters or water filtration systems. Some of the bacteria in a biofilm suspend their metabolism, explains microbiologist Peter Greenberg of the University of Washington in Seattle, making antibiotics less effective because they tend to target bacteria that are still growing. The bacteria can also cover themselves in an armour made of polysaccharides and proteins that antibiotics find difficult to penetrate, says microbiologist Bonnie Bassler of Princeton University in New Jersey. Such resistance to antibiotics can be treacherous, especially for people who have conditions such as cystic fibrosis that lead to long-term infections. Repeated treatments with broad-spectrum antibiotics heightens the risk that the bacteria will become resistant. Bacterial communication was first studied in the 1960s, and not long afterwards, researchers found that a marine bacterium known as  Vibrio fischeri  would start to shine brightly once its population reached a certain density 1 . The finding that bacteria will turn their light on synchronously under certain conditions suddenly rendered bacterial behaviour visible and measurable, says Bassler. But because most scientists believed that bacteria were incapable of \u201cfancy things\u201d such as signalling, she says, the collective behaviour was generally dismissed as a \u201cgoofy phenomenon of bacteria living in the ocean\u201d. Since then, researchers have observed this 'quorum-sensing' behaviour in many species 2 , 3 , 4  and have started to decipher the biochemistry and genetics of how it happens 5 . They have also been developing devices with which to characterize the messages that are transmitted and received. In general, quorum sensing is triggered when signalling molecules emitted by individual bacteria pass a certain threshold, at which point the molecules bind to receptors on the bacteria and cause the entire population to express specific genes at the same time. In the case of pathogenic bacteria, the synchronized behaviour can include the release of molecules known as virulence factors, which help bacteria to colonize and harm their host. It also allows bacteria to create biofilms. As the organisms adhere to a surface, they keep signalling to one another. Once they sense a quorum, genes are upregulated and sticky exopolysaccharides are produced that 'glue' the bacteria together. These findings initially led to excitement about the possibility of blocking infection by inhibiting bacterial communication. But the enthusiasm quickly waned when potential drugs failed in early-stage testing. Now, scientists are taking a more sophisticated approach. The problem with the early work turned out to be in the assumption that the communication required only a few molecules, says Herman Sintim, a chemical biologist at the University of Maryland in College Park. The reality is much more complex, he says. \u201cIn human cultures, we all know that it does not take just one word to silence a crowd and so we should not expect that from our distant cousins, bacteria.\u201d It has taken some time, but the research community in this field has grown and researchers have finally amassed enough knowledge about bacterial behaviour to start exploring how to stop the organisms from talking. \u201cWe are now getting there,\u201d says Bassler. Academics and companies are looking at fresh ways to study bacterial chatter and to create potential communication-disrupting drugs and agents for industrial and agricultural applications. \n               The language of bacteria \n             In developing drug candidates, researchers are sharpening their attack on infections beyond the broad-spectrum antibiotics currently in use. We need to talk to a specific bacterium \u201cin a language only it understands\u201d, says Martin Blaser, director of the Human Microbiome Program at New York University Langone Medical Center. Narrow-spectrum antibiotics are less likely to engender resistance because they put fewer species under selection pressure. They also cause less disruption to the body's community of microbes \u2014 its microbiome. Broad-spectrum antibiotics will also remain necessary, especially for people who are very ill. In general, they are assumed not to have lingering effects, but Blaser says that \u201cthere's more and more evidence that's just not true\u201d. They could even wipe out microbial communities involved in the developing metabolism of infants and children. It might take some time, but research on bacterial communication will \u201cwithout question\u201d deliver therapeutic opportunities, says Ronald Farquhar, who directs research at Cubist Pharmaceuticals in Lexington, Massachusetts. Regulatory agencies are particularly open to drug-firm suggestions that will meet the needs of people with chronic infections, he says. For example, someone who needs to use a urinary catheter for a long period of time could take a low-dose agent to stop bacteria from forming a biofilm on the device. Some drug candidates have already been identified. Microbiologists David Davies and Cl\u00e1udia Marques from Binghamton University in New York, for example, have found a chemical that some bacteria make to address overcrowding 6 . The bacteria continuously produce  cis -2-decenoic acid, a communication molecule. When the molecule reaches a critical threshold in a biofilm, a cascade of events is triggered, including changes in gene expression, prompting the bacteria to release themselves from the biofilm and disperse. Davies is now starting a company to commercialize a synthetic version of the acid for treating acne and disinfecting wounds. But Greenberg, among others, thinks that caution is in order before moving potential therapies towards the clinic. Dispersing a biofilm could end an infection, he says, but it might also distribute it. \u201cYou might be making more trouble than you had to start with,\u201d he says. Indeed, bacterial communication reveals ever more complexities. He has found, for example, that some bacteria in a community are cheats: they do not join the others in secreting enzymes in response to quorum-sensing signals, but still share in the benefits. \u201cThere are mixtures of cheats and cooperators in our laboratory experiments,\u201d Greenberg says, and a similar mix might be present in the infected lungs of a person with cystic fibrosis. Potential drugs could well be stymied by those cheats. Before developing therapies that disrupt communication, scientists need to know much more about quorum sensing and other bacterial behaviour, he says. Another complication is crosstalk between species and even across kingdoms. For example, Vanessa Sperandio, who studies bacterial communication at the University of Texas Southwestern in Dallas, has found that the stress hormones adrenaline and noradrenaline, which are present in the gut and elsewhere in the body, can amplify bacterial signalling and increase the virulence of  Escherichia coli  O157:H7 (ref.  7 ), a pathogen that causes bloody diarrhoea and can be fatal. \n               Other applications \n             To better understand the complexities of bacterial communication and how to use them against disease, the field is also turning to theoretical work, such as computational modelling and simulation, and to experiments with bacterial pathogens of plants. Greenberg and Lianhui Zhang, at the AStar Institute of Molecular and Cell Biology in Singapore, are working on a project funded by the Chinese government to use quorum-sensing inhibitors on crop pathogens. Such experiments could be proof-of-principle for biomedical applications, Greenberg says. Quorum-sensing inhibitors could well make it to market in agriculture before biomedicine, says Paul Williams, a chemical biologist and pharmacologist at the University of Nottingham, UK, a hub for bacterial-communication research. Scientists and companies are also testing communication inhibitors for industrial applications. For example, microorganisms are being used in bioreactors to degrade the pollutants in wastewater. The water is then passed through a filter, but a build-up of bacteria can clog the pores of the membrane. The reactor then has to be taken offline, flushed out and cleaned with harsh chemicals such as chlorine \u2014 an energy-intensive process that incurs more than half the cost of running a membrane bioreactor, says Chung-Hak Lee, a chemical engineer at Seoul National University. Lee has come up with a potential solution. His approach taps into a typical communication network found in biofilms, in which enzymes secreted by some species digest signalling molecules emitted by others. He and his team isolated such signal-quenching bacteria and placed them in beads that contain pores that keep the bacteria in, but let signalling molecules pass through. When placed near the filtration membrane in a bioreactor, the beads undermine bacterial communication and help to stop biofilms from forming (see 'Sludge fight'). In lab tests and in a pilot-scale wastewater treatment plant, Lee has found that the beads save almost half of the energy costs of a conventional membrane bioreactor. Several companies are exploring how to prevent biofilms for industrial and biomedical applications. Selenium, a spin-off company from Texas Tech University in Austin that is backed by the venture-capital firm Emergent Technologies, is developing selenium-containing coatings that could protect materials such as catheters, contact lenses and voice prostheses by producing reactive oxygen molecules that ward off bacteria. Another company, Curza, founded last year in Salt Lake City, Utah, is developing coatings that prevent biofilms from forming on hip and knee implants. Its research involves chemical synthesis, molecular genetics, mass spectrometry and scanning electron microscopy, as well as a proprietary flow cell assay that better represents physiological conditions by using liquid flow rather than stagnant broth assays. The company says that the assay can help to characterize whether a biofilm is prevented under real-life-like conditions and show what might happen as an antimicrobial compound dilutes away from a medical device's coating, for example. And Kane Biotech of Winnipeg in Canada is developing combinations of antimicrobials and biofilm inhibitors for coating biomedical devices, treating wounds and protecting teeth and skin. Sri Madhyastha, chief scientific officer, says that one of their products has been licensed by a medical-device company. Kane also sells products through veterinarians and distributors, including a water additive aimed at preventing plaque from forming on the teeth of pets. The company tried to obtain approval from the US Food and Drug Administration for an anti-biofilm enzyme in a wound-care product, but as a new chemical entity, it would require extensive testing. That route \u201cis too expensive and time-consuming\u201d, Madhyastha says, so the company has put this product on the back-burner. \n               Observation platforms \n             To test their potential products, Kane's researchers use confocal microscopy and an instrument called the CDC Biofilm Reactor: a 1-litre beaker containing 8 slim rods around which liquid moves. Dotting the length of the rods are 24 circular disks on which biofilms can be grown and tested. The reactor was built under a licence from the US Centers for Disease Control and Prevention by BioSurface Technologies of Bozeman, Montana, which sells several other types of vessel in which scientists can grow and disrupt biofilms in a controlled, standardized environment. At Fluxion Biosciences in South San Francisco, California, cell biologist Bryan Haines helps labs to set up the firm's BioFlux microfluidic platforms. The platforms allow scientists to do 24 biofilm experiments on one multiple-well plate. The temperature and gas content in the medium can be adjusted to suit the preferred growth conditions of the bacterium being studied. The wells are the reservoirs for reagents, potential antibiotics and bacteria; running underneath them are micrometre-scale channels in which a biofilm can grow. The plate is sealed at the top and users select the pressure with which to distribute fluids and cells through the channels, then observe the biofilm through an inverted microscope. But Sintim says that scientists need better assays if they are to study the subtleties of bacterial communication. Cells live in a three-dimensional architecture and respond to many cues. And biofilms contain multiple species, making a specific biofilm hard to culture using traditional approaches. \u201cMany systems that have been developed to date are reductionist systems,\u201d Sintim says, \u201cand it is not obvious to me if data obtained from these reductionist platforms have any biological meaning.\u201d Together with bioengineer William Bentley at his university, Sintim is developing a microfluidic system that will not just track cells moving through a three-dimensional space, but will also let experimenters perturb conditions and measure changes in appearance and behaviour. Their system uses a membrane to separate two types of bacteria. On one side of the membrane are bacteria they have engineered to fluoresce green under ultraviolet light. These bacteria secrete signalling molecules that can pass through the membrane. On the other side are bacteria engineered to fluoresce red only when they receive that signal. The device allows researchers to alter the environment of each side independently and to control the rates of flow of liquids across the device (see 'Just watch'). Scientists can then study the effect of different gradients in a setting that is more typical of, for example, the body. Thomas Bjarnsholt helps university-hospital physicians to diagnose infections and has a microbiology lab at the University of Copenhagen, where he is building a system for studying biofilms. Current assays do a poor job of showing how slowly a biofilm forms on a medical implant, he says, so he wants to develop an assay that more closely mimics the  in vivo  conditions. Also, only a few people develop infections when their hips or knees are replaced, so he hopes to determine what makes some luckier than others. In his view, a communication disrupter should be tested not just by adding it to a biofilm. In the chronically infected lung of a person with cystic fibrosis, antibiotics have to travel through the bloodstream, then diffuse through necrotic material, mucus and pus to get to the infection site. \u201cIt's all embedded in slime,\u201d he says. The slime also has anaerobic pockets, where antibiotics tend to fail. Just 40 micrometres of pus or mucus suffice to create such pockets. He is developing surfaces, gels and other media that mimic this kind of shielding and allow researchers to take this into account. Quorum-sensing inhibitors and other communication disrupters will eventually emerge, Bjarnsholt predicts 8 . An area of interest for him is dressings, especially for people with diabetes, who repeatedly develop wounds. At the moment, dressings often contain silver, which acts as an antibacterial treatment, but infections still develop, so new approaches are needed, he says. But new antibiotics will need more-expensive tests that require greater expertise to administer, says Sperandio. The standard way to test antibiotics is the minimal inhibitory concentration test, which measures the concentration at which a compound needs to be administered to stop bacteria from growing. Williams points out that this approach \u201cis obviously of no use\u201d for assessing compounds that disrupt communication. In fact, says Sperandio, the whole communications approach to curing infection is at odds with the long-held dogma that a cure means killing the microbes. Communication disruptors could prevent pathogenesis without killing the pathogen, for example. Except in rare cases, such as infections of heart valves, it is not necessary to kill every bacterium, says Blaser. Even conventional antibiotics do not sterilize an organ; they reduce replication rates and \u201cultimately it is the immune response in patients that clears the infection\u201d, he says. New antibiotics could battle bacteria in this way, too. The war on harmful bacteria is most definitely a war that humans need to win, says Blaser. But that does not mean we have to harm ourselves in the process.\u201cWe don't want a Pyrrhic victory,\u201d he says. \n                     WHO warns against 'post-antibiotic' era \n                   \n                     Antibiotic resistance: The last resort \n                   \n                     Report urges controversial 'delinkage' to foster new antibiotics \n                   Reprints and Permissions"}
]