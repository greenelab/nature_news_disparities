[
{"file_id": "4661137a", "url": "https://www.nature.com/articles/4661137a", "year": 2010, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "Long-term, live-cell imaging helps to settle long-running debates. Monya Baker investigates how the huge investment and time commitment is finally paying off. In the late 1980s, Sally Temple was studying neural development in mice at the University of Miami in Florida and needed a way to observe neural progenitor cells for days on end. At the time, no one had observed mammalian cells for more than a few hours, because the conditions that could be maintained under a microscope were too dry, cold and oxygen-rich to keep cells alive for long. Undeterred by the lack of precedent, Temple decided to build her own device that could monitor cells around the clock. Temple, a developmental neuroscientist now at the Neural Stem Cell Institute in Rensselaer, New York, credits her husband Jeffrey Stern with the inspiration for her apparatus. \u201cHe said, 'if the cells are living well in the incubator, you have to put the microscope in the incubator',\u201d she recalls. Although the idea was obvious to Stern \u2014 a vision researcher and co-founder (with Temple) of the Neural Stem Cell Institute \u2014 it seemed ludicrous to most cell biologists, who had long held the view that the humidity inside an incubator would ruin microscope optics. Temple was also sceptical. But she found an abandoned microscope and decided it was worth a try. She outfitted the ageing microscope with a red filter (a piece of broken glass taped onto a Petri dish) to minimize the cells' exposure to more damaging, higher-energy light during her extended experiment. She then attached a camera and drilled a hole through the incubator to connect the camera to a Panasonic tape deck, which could record image data several times an hour for up to seven days. Remarkably, the makeshift contraption worked. \u201cWe got some really neat data that showed to our surprise \u2014 I think to everyone's surprise \u2014 that the vertebrate brain had lineage trees that were similar to  Caenorhabditis elegans  and other invertebrates,\u201d says Temple 1 . But the discovery required as much luck as it did innovation. Temple's cobbled-together set-up could only keep track of what was under the microscope at a particular time, and the slide couldn't be moved around to find the most photogenic cells. If the few cells that were in the microscope's field of view had died or grown poorly, the whole project would never have panned out. All the major microscope manufacturers now offer a new generation of devices built for live-cell imaging, complete with computerized incubation chambers and microscope stages. Jochen Tham, global marketing and communications director at Carl Zeiss MicroImaging in Thornwood, New York, tracks what he describes as some of the major developments at his company: multilayered incubation, cooling and heating, computer control of environmental parameters, integration of microscope-controlling software and image-acquisition software, incubators for super-resolution and total internal reflection, and control over oxygen levels to mimic physiological conditions, to name just a few. Such systems are helping researchers come to a more complete understanding of how functional cells and tissues develop. \u201cUnless you can actually watch everything that is happening from the first cell up to the developing progeny, you have no idea how [cell development] actually plays out,\u201d says Michel Cayouette, a developmental neurobiologist at the Clinical Research Institute of Montr\u00e9al, Canada, whose work has revealed a way to predict how retinal progenitor cells divide 2 , information that could help to produce cells for treating blindness. Cayouette compares studying cell differentiation to watching ice hockey: how much could he learn about the game if all he knew was the final score? Scientists have valid reasons for avoiding long-term imaging experiments. They require expensive equipment that gets tied up for days or weeks at a time and are prone to time-consuming false starts. As an experiment runs its course, all aspects of keeping cells alive and in focus get harder: cells move, routine handling becomes disruptive and computer hard drives fill up. Any one of these unpredictable events can derail an experiment, says Cayouette. \u201cYou can't say, 'I'll try this and at the end of the week, I'll figure out what to do.'\u201d So researchers are planning ahead, and devising new systems for tracking cells in real time. \n               Getting answers \n             Beyond offering a lens onto new biology, long-term imaging studies are also beginning to resolve long-standing debates in developmental and cell biology. For example, Timm Schroeder, a stem-cell biologist at the Helmholtz Centre in Munich, Germany, led a team that used continuous imaging to distinguish between two competing hypotheses about the role of cytokines in blood development. One view held by some immunologists is that cytokines \u2014 regulatory proteins found in the immune system \u2014 cause cells to take on new fates; another theory is that cytokines help certain cell types but not others to survive. Both ideas would ultimately result in the same blood cells, but the path by which the cells got there would be radically different. As such, the two hypotheses would hold vastly different implications for treating diseases or generating blood in the laboratory. To tease apart the actual mechanism, Schroeder's group took pictures of mouse blood cells every two to three minutes for several days. Because the researchers did not observe extensive cell death, their time-lapse film firmly supported the active instruction over the passive-survival hypothesis 3 . Long-term images make \u201ca big, big difference\u201d, says Schroeder. \u201cYou can say, 'this is how it was', not 'this is how it probably was'.\u201d Researchers working in the fast-paced field of stem-cell reprogramming have also been keen to track how cells take on desired fates. Under most experimental set-ups, the early events are the hardest to follow. But a team led by George Daley and Thorsten Schlaeger, stem-cell biologists at the Children's Hospital Boston in Massachusetts, used long-term imaging to reveal the history of rare, reprogrammed cells. To identify the presence of expected molecules on cell surfaces, the researchers added fluorescently tagged antibodies to the culture media as the cells grew into the colonies characteristic of induced pluripotent stem (iPS) cells. Meanwhile, they set their microscope to scan the cells constantly, making a complete survey of the 4-square-centimetre area every two or three days for about two weeks. After assessing which colonies produced high-quality iPS cells, the team could go back to the images to identify the cell clusters that gave rise to fully reprogrammed cells, even though each group of iPS cells took up as little as 0.0003% of the scanned area 4 . Learning the hallmarks of iPS cells as they undergo reprogramming could not only yield better methods for growing patient-specific stem cells, but also prevent weeks of wasted effort (and costs) in animal experiments, says Schlaeger. \n               Come to light \n             These types of study are starting to shed light on hitherto unsolvable biological problems, such as why some patterns of cell division contribute to cancer and which progenitor cells give rise to blood, sperm, neurons or other tissue types. But researchers using live-cell imaging have to be careful not to shed too much light \u2014 quite literally \u2014 because illuminating cells for long durations can damage cells or alter their behaviour. Schroeder's advice to biologists is to take the worst image possible to get the necessary data. \u201cIf you're pushing the envelope, you should aim for having healthy cells rather than the best images,\u201d he says. Beautiful images, he notes, often make for unhealthy cells. So instead of continuous snapshots, researchers often rely on taking pictures at less frequent intervals. Many factors contribute to the imaging method used \u2014 the types of cell and how robust they are, what features need to be followed, even the size of the image files that will be collected (see ' A long-term live-cell commitment '). The most important decision is usually what type of light to use. Fluorescent tags can be linked to the expression of particular proteins to indicate specific biological activity or the production of an oncogene, for example. But too much fluorescence-activating light can trigger damage that prevents cells from growing \u2014 a phenomenon known as phototoxicity. Even with cells that resist phototoxicity, there are image-processing considerations. If viewed too often, fluorescent proteins can fade and become invisible. Overcoming such photobleaching requires the subtraction of background fluorescence and correcting for the fact that because the sample is not perfectly flat, levels of illumination vary across the field of view. Unlike fluorescence microscopy, phase-contrast imaging produces black and white pictures using less damaging wavelengths of light. But the technique reveals only the general cell shape, rather than the presence of a particular protein. Researchers often combine the two methods by, for example, taking a fluorescent image every hour and a phase-contrast image every ten minutes, but even that compromise must be planned carefully. Some instruments that are great for fluorescence microscopy perform less effectively at bright-field microscopy, explains Schlaeger. \n               Keeping watch \n             Keeping cells alive requires a much more delicate balancing act than finding the right mix or amount of photons. Even if cells aren't perturbed by the imaging set-up, they still need fresh culture media and the removal of waste products. Plus, any continuous imaging study that lasts for more than an hour will probably require a system with built-in environmental controls for temperature, humidity and gas concentration \u2014 and maintaining the right conditions for cells often requires special care for microscopes. For example, to avoid creating air currents that could blur an image when cells are kept at standard 37 \u00b0C, microscopes for studying live cells have to maintain the lens at the same temperature as the culture. Environmental chambers are available for all high-end microscopes, either from the microscope manufacturer or from third parties, but none works as well as an incubator, say researchers. Temple and her graduate students nicknamed an early model 'the Sahara' because it caused cells to dry out so quickly. The commercially available products have improved, says Jin-Wu Tsai, who studies cultured brain slices in Arnold Kriegstein's lab at the University of California, San Francisco. \u201cA few years ago, we built our own incubator on top of the microscope. Now you have lots of options,\u201d he says. With these newer commercial tools, \u201cwe can just keep the culture dish on the stage of the microscope, and the software allows us to take images every ten or fifteen minutes\u201d. Although he doesn't have to transfer samples, Tsai is still tethered to his microscope, keeping watch on its confocal image. The brain slices he studies flatten over time, making the image go out of focus, and it doesn't matter how healthy the cells are if the data collected from them aren't usable. Prototype microscopes with autofocus have been introduced. They might be reliable for flat culture, Tsai says, but he doesn't trust them to work in thick brain slices yet. In addition to keeping tabs on the brain slices themselves, Tsai has to monitor every facet of the experiment. If the conditions are just slightly off \u2014 say, a shift in pH, or a slight increase in carbon dioxide \u2014 the neurons stop growing. And there is still no way to change media under the microscope and maintain sterile conditions, he says. Nonetheless, Tsai and his colleagues have been able to keep brain slices alive for more than a week, after which time bacteria have started to grow. Conditions also need to be tightly controlled to minimize artefacts and experiment-ruining variability. Over the run of a protracted experiment, subtle differences in culture conditions can start to look like cell behaviour, notes Alfred Bahnson, a biologist at Kairos Instruments in Pittsburgh, Pennsylvania, which manufactures optically accessible environmental chambers and other long-term imaging products. Movement that appears to be cell migration, says Bahnson, might instead be cells moving downhill or following slight temperature gradients. \n               Coordinating accessories \n             Those imaging issues are typical, says Keith Bogdon, an adviser with consulting company Coalesce Corporation in Larkspur, California, who has researched live-cell imaging products. Often features to control temperature or pH conflict with features for positioning or focusing cells in the microscope, particularly if researchers want to compare several experimental conditions. \u201cThere are a lot of wires and tubes coming out of the plates,\u201d says Bogdon, and it is difficult to design a chamber so that cells can be both monitored and unperturbed. Even after the microscope, environmental chamber, and other equipment are brought together so that cells stay alive, the length of experiments can still be a problem. Most labs and core facilities aren't designed to accommodate experiments that tie up equipment for weeks at a time. If a researcher works out too late that some parameters need to be tweaked, it could take months to book the necessary time with the microscope again. And long-term imaging is costly in money as well as time. Renting equipment from a core facility at, say, US$20 an hour, 24 hours a day for a week or more could make even a single experiment pricey. \u201cIf you do this for half a year,\u201d says Schroeder, \u201cyou've paid more for renting than for buying.\u201d But buying equipment with a six-figure price tag is not always easy. \u201cIt's a classic chicken and egg scenario,\u201d says Bogdon. \u201cHow can researchers get into the area if they don't have the clout to justify funding?\u201d Still, researchers report that commercial offerings in software, cell incubation, and visual systems have expanded greatly. Microscope systems used for live-cell imaging for time periods of more than 24 hours are produced by companies including BD Biosciences in San Jose, California; Essen BioSciences in Ann Arbor, Michgian; GE Healthcare in Waukesha, Wisconsin; Leica in Wetzlar, Germany; Molecular Devices in Sunnyvale, California; Nikon in Melville, New York; Olympus in Center Valley, Pennsylvania; PerkinElmer in Waltham, Massachussetts; and Zeiss. Each system comes with its own proprietary software and storage options, and additional software is available, all of which tends to require considerable expertise ( see 'A software spot' ). Molecular Devices offers MetaMorph imaging analysis software; the imaging-processing software MatLab from MathWorks in Natick, Massachusetts, can be purchased with add-on image analysis tools. Volocity from PerkinElmer is popular. ImageJ from the US National Institutes of Health and Cell Profiler from the Broad Institute in Cambridge, Massachusetts, are both freely available and widely used. Several companies, such as Oko Lab in Ottaviano, Italy, Tokai Hit in Shizuoka-ken, Japan, and WaferGen Biosystems in Fremont, California, sell environmentally controlled microscope slides or other equipment to control conditions on the microscope stage. Still, a microscope accessory that solves one problem often creates another, says Schroeder. An incubator that fits to the stage of one microscope may not provide the kind of surface that a particular cell type grows on, or tubing that fits an incubator may not attach to a media pump. A computer program names files with a four-digit code, limiting experiments to less than ten-thousand images. And so on. Each problem is trivial individually, says Schroeder, but collectively they sap researchers' motivation and take up time that could be spent on experiments. Anyone who wants to conduct long-term live-cell experiments needs to be ready to spend a long time tinkering with equipment, he says. But the benefits are repeatedly proving worth the hassle, says Cayouette. \u201cThe technique is becoming increasingly user-friendly. More and more people are trying to do these long-term imaging studies.\u201d Temple predicts that the results of such studies will be profound. \u201cWe've just forgotten the fourth dimension in so many of these analyses,\u201d she says. \u201cNow that we've got time, we can finally start to understand.\u201d Reprints and Permissions"},
{"file_id": "4671138a", "url": "https://www.nature.com/articles/4671138a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Back to main article: The search for association \n             As genome-wide association studies (GWAS) get larger, the technical challenges pile up, and an onslaught of dense microarrays is compounding the issue by encouraging researchers to combine data sets. Genotyping a few-dozen single nucleotide polymorphisms (SNPs) in a sample is not much cheaper than genotyping hundreds of thousands, says Peter Donnelly, director of the Wellcome Trust Centre for Human Genetics in Oxford, UK. So rather than designing a targeted follow-up study on a handful of SNPs, researchers are more likely to try to replicate an association through meta-analysis, using samples that have been fully genotyped elsewhere. \u201cThat needs care,\u201d says Donnelly. Even in straightforward GWAS, everything that looks like a signal is probably an artefact, he says. Combining results typed on one platform in one lab and on another in a different lab creates more opportunities for artefacts. Even when cases and controls are processed by the same group, all the cases can be on one set of microarray plates and all the controls on another. This introduces potential for systemic error that sometimes leads to up to 30% of the data being discarded, says Christophe Lambert, chief executive of Golden Helix in Bozeman, Montana, which provides software and analytical services for genetic research. \u201cEveryone is running these experiments and asking the statisticians to fix the problems, when a simple block randomization at the beginning could have fixed it.\u201d Some problems occur before the sample is collected, says James Clough of Oxford Gene Technology, a genotyping-services firm. \u201cSamples will be collected in multiple centres and multiple countries.\u201d That can pose challenges when clinical standards vary. The best studies put more effort into collecting phenotypes than collecting samples, he says. Careful characterization of phenotype could make genetic signals more apparent, says Greg Gibson, director of the Center for Integrative Genomics at the Georgia Institute of Technology in Atlanta. Many aspects of phenotype are extremely variable, so longitudinal measurements of factors such as blood-lipid levels, body-mass index or toxin exposure could control for transient effects and effectively boost genetic signals. GWAS could be more successful at implicating genes if they concentrate on qualities more closely tied to genetics, such as lipid levels or endophenotypes, he says. \u201cJust mapping genotype to disease is several steps away from gene expression.\u201d \n               M.B. \n             Reprints and Permissions"},
{"file_id": "4661138a", "url": "https://www.nature.com/articles/4661138a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "The decision to undertake a long-term imaging project is not trivial. Experts suggest questions that researchers should ask themselves before starting out. \n               How frequently do you need to take an image? \n             Tracking individual cells often requires taking an image every few minutes. The more dense and mobile the cells are, the less time can elapse between images. For example, Michel Cayouette at the Clinical Research Institute of Montr\u00e9al, Canada, takes images of retinal progenitor cells every seven minutes until they develop into neurons, at which stage he slows the rate of image acquisition to roughly once an hour. \n               Can your cells survive the experiment? \n             Repeated imaging can harm cells, especially when the imaging requires high-energy light. But the tolerance of different cells for fluorescence varies widely. Blood-forming stem cells are generally more robust than neural stem cells, for example, and thus can be imaged more frequently without affecting cell behaviour, notes Tannishtha Reya, a stem-cell biologist at Duke University in Durham, North Carolina. \n               Can you keep calm? \n             Inexperienced researchers sometimes set up their long-term microscope systems in the middle of a heavily trafficked work station or worse, under ventilation systems. Such disturbances can easily overwhelm a system's ability to maintain stable conditions and can cause obfuscating artefacts, cautions Cayouette. \n               Can you follow your cells? \n             Following cells in culture gets complicated once cells start crawling under and over each other. To track individual cells at low densities, labelling nuclei with Hoeschst often works well, says Thorsten Schlaeger at the Children's Hospital Boston in Massachusetts, although he cautions that some cells stain poorly, and non-toxic genetic labels can work better. If cells must be grown at high density, consider mixing in a few labelled cells and tracking just these. \n               Are you computationally prepared? \n             Crunching through large data sets can easily go beyond the capacity of standard lab computers, and a single experiment can completely fill a computer's hard drive. Researchers need appropriate servers and back-up systems. A dedicated informatics set-up and the help of a programmer are \u201chighly desirable\u201d, says Schlaeger. \n               M.B. \n             See Also:  Cellular imaging: Taking a long, hard look Reprints and Permissions"},
{"file_id": "468851a", "url": "https://www.nature.com/articles/468851a", "year": 2010, "authors": [{"name": "Laura Bonetta"}], "parsed_as_year": "2006_or_before", "body": "Developing techniques are helping researchers to build the protein interaction networks that underlie all cell functions. An old adage says: \u201cShow me your friends, and I'll know who you are.\u201d In the same way, finding interaction partners for a protein can reveal its function. To that end, researchers are now building entire networks of protein\u2013protein interactions. Unlike biological pathways, which represent a sequence of molecular interactions leading to a final result \u2014 for example, a signalling cascade \u2014 networks are interlinked. Represented as starbursts of protein 'nodes' linked by interaction 'edges' to form intricate constellations, they provide insight into the mechanisms of cell functions. Furthermore, placing proteins encoded by disease genes into these networks will let researchers determine the best candidates for assessing disease risk and targeting with therapies.  \u201cThis is the next step after the Human Genome Project,\u201d says Trey Ideker, a systems biologist at the University of California, San Diego, and principal investigator at the National Resource for Network Biology, which provides open-source software for network visualization. \u201cThat effort identified 30,000 genes, but that is not the end goal. How the genes work in pathways and how these pathways function in disease states and development is the end goal. To accomplish this we will need to systematically map gene and protein interactions.\u201d Unlike the genome, the interactome \u2014 the set of protein-to-protein interactions that occurs in a cell \u2014 is dynamic. Many interactions are transient, and others occur only in certain cellular contexts or at particular times in development. The interactome may be tougher to solve than the genome, but the information, researchers say, is crucial for a complete understanding of biology. \n               The right partners \n             At any time, a human cell may contain about 130,000 binary interactions between proteins 1 . So far, a mere 33,943 unique human protein\u2013protein interactions are listed on BioGRID ( http://thebiogrid.org ), a database that stores interaction data. Clearly, there is work to do. There are two main approaches for detecting interacting proteins: techniques that measure direct physical interactions between protein pairs \u2014 binary approaches \u2014 and those that measure interactions among groups of proteins that may not form physical contacts \u2014 co-complex methods (see  'Tools for the search '). The most frequently used binary method is the yeast two-hybrid (Y2H) system 2 . It has variations involving different reagents, and has been adapted to high-throughput screening. The strategy interrogates two proteins, called bait and prey, coupled to two halves of a transcription factor and expressed in yeast. If the proteins make contact, they reconstitute a transcription factor that activates a reporter gene. Another method for identifying binary interactions is luminescence-based mammalian interactome mapping (LUMIER), a high-throughput approach developed by Jeff Wrana at the Samuel Lunefeld Research Institute in Toronto, Canada. This strategy fuses Renilla luciferaze (RL) enzyme, which catalyses light-emitting reactions, to a bait protein, which is expressed in a mammalian cell along with candidate protein partners tagged with a polypeptide called Flag. Researchers use a Flag antibody to immunoprecipitate all proteins with the Flag tag, along with any that interact with them. Interactions between the RL-fused bait and the Flag-tagged prey are detected when light is emitted. Other binary methods include the mammalian protein\u2013protein interaction trap and techniques based on proteome chips. The most common co-complex method is co-immunoprecipitation (coIP) coupled with mass spectrometry (MS). In this approach, a protein bait is tagged with a molecular marker. Several types of tags are commercially available; each requires a distinct biochemical technique to recognize the tag and fish the bait protein out of the cell lysate, bringing with it any interacting proteins. These are then identified by MS. In addition to these empirical methods, researchers have used computational techniques to predict interactions on the basis of factors such as amino-acid sequence and structural information. \u201cPeople ask 'Why are you predicting interactions when you can just do the experiment?'\u201d says Gary Bader, a bioinformatician at the University of Toronto. \u201cBut experimental techniques fail for some proteins.\u201d \n               False readings \n             Every step of a procedure to detect protein\u2013protein interactions \u2014 from the reagents used to the cell types and experimental conditions \u2014 influences the proteins that are identified. Two studies this year used similar methods to identify interacting proteins in transcription factors in embryonic stem cells 3 , 4 ; there was incomplete overlap between the resulting data sets. \u201cIf you use the same protocol you will get reproducible lists of proteins. But different labs use different protocols, which affects the end result,\u201d says Raymond Poot, a cell biologist at Erasmus MC hospital in Rotterdam, the Netherlands, and lead author of one of the studies. In his protocol, Poot pulled interacting proteins from cells using nuclear extracts expressing different Flag-tagged transcription factors. He added a nuclease to his reactions to remove DNA and eliminate possible artefacts caused by proteins binding to it. \u201cTranscription factors bind to DNA so you are likely to pull out DNA-binding factors that are not directly interacting,\u201d he explains. Purifying many different transcription factors with the same protocol also enabled the researchers to determine which interactions were most likely to be specific. For example, proteins that consistently co-purified with all transcription factors would be treated as unlikely to indicate a genuine interaction. Calling out false positives \u2014 reported interactions that don't actually occur \u2014 and false negatives \u2014 interactions that do occur but are not picked up by the experimental protocol or are discarded \u2014 is one of the main challenges in the field. \u201cNormally when you do a coIP followed by MS you will get hundreds of protein candidates interacting with any one bait,\u201d says Wade Harper, a cell biologist at Harvard Medical School in Boston, Massachusetts. \u201cWhen you weed out all the stochastic and non-specific interactions you end up with many fewer proteins. Some proteins in large complexes might have 30\u201350 partners, others only 4\u20135.\u201d One way in which researchers increase the accuracy of their results is to use more than one method (for example, Y2H plus LUMIER) to detect the interactions. But the definition of a 'real' interaction depends on the context. \u201cDoes a real interaction mean that two proteins interact if they are placed next to each other in a test tube, or that they must interact in a cell? Or does real mean that the interaction should have a biological function?\u201d asks Ideker. Researchers can home in on functional interactions by combining data on interactions with other types of biological information, such as genetic interactions, protein localizations or gene expression. For instance, proteins whose genes are co-expressed are likely to interact with each other or to be part of the same complex or pathway. Many tools are available on the web for integrating different types of information about a given protein or gene. One is GeneMANIA, developed by Bader's group in collaboration with Quaid Morris, a computational biologist also at the University of Toronto. A user enters the gene names into  GeneMANIA ; the program provides a list of genes that are functionally similar or have shared properties, such as similar expression or localization, and then displays a proposed interaction network, showing relationships among the genes and the type of data used to gather that information. The user can click on any node to obtain information about the gene and on any link to obtain information about their relationship (such as citations for any published studies or other sources of data). \u201cIt's like a Google for genetic and protein information,\u201d says Bader. Other web-based interfaces that predict gene functions include STRING ( http://string-db.org ) developed at the European Molecular Biology Laboratory in Heidelberg, Germany. It hunts for protein interactions on the basis of genomic context, high-throughput experiments, co-expression and data from the literature. \n               Keeping score \n             To select real protein\u2013protein interactions, Harper and some members of his lab, Matt Sowa and Eric Bennett, developed a software platform called CompPASS to assign confidence scores to an interaction detected by MS 5 . CompPASS takes data sets of interacting proteins (including those identified in experiments) and measures frequency, abundance and reproducibility of interactions to calculate the score. This year, Harper used CompPASS to identify interactions among proteins involved in autophagy, the process by which cellular proteins and organelles are engulfed into vesicles and delivered to the lysosome to be degraded. Starting with 32 proteins known to have a role in autophagy, they identified 2,553 interacting proteins using coIP\u2013MS. CompPASS then narrowed the list down to 409 high-confidence interacting proteins with 751 interactions 6 . Ideker's group used a different approach to map interactions among human mitogen-activated protein kinases (MAPKs), which respond to external stimuli and regulate cell function. Having used Y2H to identify more than 2,000 interactions among known MAPKs, Ideker used evidence including conservation of interactions among different species to winnow that down to a core network of 641 high-confidence interactions 7 .  For some of the proteins there was no previous evidence of interactions with MAPKs. Ideker and his colleagues knocked down the expression of these proteins using RNA interference, then looked for the effect of the knock-downs on proteins known to be activated by MAPKs. This allowed them to confirm that about one-third of their interactions had a role in MAPK signalling. These methods are helping to weed out false positives and provide associated confidence scores, but the problem of false negatives persists. \u201cWith these assays we try to get false positives down to zero. The hit you take is on false negatives. So now you can be highly confident of your data but you are probably probing only about 20% of the interactome,\u201d says Ideker. \u201cWe would like to get every interaction but we do not get even close with current technologies.\u201d New methods may become available to identify interactions that escape detection by current techniques (see  'Real-time analysis' ). In the meantime, one way to address the problem is to combine procedures for detecting interactions, each sampling a different portion of the interactome. The interaction data obtained in an experiment can also be combined with that available in public databases, thus providing a more complete picture, says Bader. \n               From data to networks \n             Protein\u2013protein interactions are only the raw material for networks. To build a network, researchers typically combine interaction data sets with other sources of data. Primary databases that contain protein\u2013protein interactions include DIP ( http://dip.doe-mbi.ucla.edu ), BioGRID, IntAct ( http://www.ebi.ac.uk/intact ) and MINT ( http://mint.bio.uniroma2.it ). These databases have committed to making records available through a common language called PSICQUIC, to maximize access. Other types of data that can be combined with protein\u2013protein interactions include information on gene expression, cellular co-localization of proteins (based on microscopy), genetic information, metabolic and signalling pathways, and data from high-throughput assays. \u201cOne challenge computationally is integrating heterogeneous data sets to build a network model,\u201d says Ilya Shmulevich, a professor at the Institute for Systems Biology in Seattle, Washington. The second challenge is to decide on a modelling approach. \u201cIt will depend on what kind of data you have available and how you will be using the model,\u201d says Shmulevich. Several bioinformatic tools have been developed to model and represent networks. The most widely used ones are associated with Cytoscape ( http://www.cytoscape.org ), an open-source program for visualizing networks and for integrating them networks with other types of data. Several Cytoscape plug-ins allow users to download and explore databases. Commercial packages with similar functions include MetaCore from GeneGO in St Joseph, Michigan; Pathway Analysis from Ingenuity Systems in Redwood City, California; and Pathway Studio from Ariadne Genomics in Rockville, Maryland. These can access public sources of data as well as the company's proprietary databases. \u201cOne of the unique features of Pathway Studio is the openness of our system and the ability to integrate many different kinds of data,\u201d says David Denny, director of marketing and product management at Ariadne. \n               Guilt by association \n             One reason for developing networks is to help assign functions to proteins through guilt by association. But \u201ca huge slice of the proteome consists of proteins that no one knows what they do or interact with\u201d, says Benjamin Cravatt, a chemical physiologist at the Scripps Research Institute in San Diego, California. For proteins not yet assigned to a portion of the human interaction network, Cravatt's group developed a technology for assigning protein functions by exploiting an interaction between enzymes and chemical reagents dubbed activity-based probes. These probes consist of a reactive group that binds the active sites of many members of an enzyme family, and a reporter tag that is used for the detection and identification of the probe-labelled enzymes 8 . Because these probes bind only to enzymes that are active, they can give insights into the enzymes' functions. For example, if a probe binds to a set of enzymes in a cancer cell but not in a normal cell, it means that these enzymes become more active in the cancer cell and so may have a role in cell growth. The activity probes can also serve as assays for the discovery of inhibitors for a particular enzyme, which may help researchers to understand the role of that enzyme. \u201cYou can develop an inhibitor for an enzyme before ever knowing what the actual substrate is,\u201d says Cravatt. This year, he developed another strategy that not only determines differences in enzyme activities in different cells, but also pinpoints where in the protein these differences occur, providing a more quantitative measure of the differences 9 . The activities of many families of enzymes are regulated or fine-tuned by cysteine modifications. By looking specifically for changes in cysteine modifications across the proteome, he found 'hyper-reactive' cysteine residues in several proteins of unknown function, which suggests that they probably have roles in signalling pathways. One challenge in defining protein\u2013protein interaction networks is that interactions vary depending on the type of cell and the cellular environment. For example, Wrana mapped the protein\u2013protein interaction network for TGF-\u03b2, a growth factor that regulates cell functions, and found that two proteins that pass on the signals from the factor inside the cell \u2014 Smad2 and Smad4 \u2014 interact with one another only when the cells are stimulated with TGF-\u03b2. If the cells are not stimulated, these two proteins don't come into contact 10 .  Bennett, Harper and Steven Gygi, a cell biologist also at Harvard Medical School, developed a proteomics platform centred around a technology called multiplex absolute quantification (AQUA) to look at dynamic changes in protein interaction networks. AQUA uses synthetic peptides that contain stable isotopes as internal standards for the native peptides that are produced when proteins from a cell lysate are digested. Using tandem MS, researchers can compare the levels of native and synthetic peptides in a cell to obtain a measure of the amount of native proteins present. Synthetic peptides can also be prepared with modifications, such as extra phosphate groups, to measure the number of post-translationally modified proteins. \u201cWe are pursuing the dynamics of protein networks by quantifying changes in the amount of proteins present in specific protein complexes,\u201d says Harper. \u201cTechniques such as AQUA provide an accurate and sensitive measure of how the stoichiometry of components within complexes that make up a network are altered in response to a stimulus.\u201d The team used the approach to describe the rearrangements that occur in the protein network of cullin-RING ubiquitin ligases, enzymes that regulate protein turnover, under various cellular conditions 11 .  \n               Developments in diagnostics \n             Changes in protein\u2013protein interaction networks may provide information about the mechanisms of disease. Last year, Wrana applied the network approach to the diagnosis of breast cancer. He used microarrays to measure genome-wide protein expression in the tumours of people with breast cancer, and then overlaid the expression data on the network diagram of the human interactome. Wrana had noted that 'hub' proteins, defined as those that interact with more than four others, can be grouped into two categories depending on whether they are expressed at the same time as the proteins with which they interact. When they looked at breast-cancer samples, Wrana and his colleagues found that certain hub proteins were in a different category in breast-cancer patients with a good prognosis than in those with a poor prognosis. Thus, by overlaying the expression pattern of a cancer cell from an individual patient onto the human interactome network, Wrana could predict a patient's prognosis. \u201cWe found that the detection of global changes in network organization is more predictive of outcome than is gene expression alone,\u201d says Wrana. \u201cWe have now applied this method to other tumour models and obtained similar results.\u201d KAYAK (kinase activity assay for protein profiling) is another approach to developing diagnostic tools for cancer on the basis of the functional consequences of the interaction between a protein, in this case a kinase, and its substrate. In this method, up to 90 peptide substrates for kinases are used to simultaneously measure the addition of phosphate groups to proteins in a cell lysate \u2014 in essence providing a 'phosphorylation signature' for that particular cell. \u201cThe readout is so sensitive and so quantitative that even small differences are teased out,\u201d says Gygi, who helped to develop the method 12 . According to Gygi, the biggest application of KAYAK might be in tumour classification. \u201cBiopsies or excised tissues can be profiled for kinase activities with pinpoint accuracy. These patterns could contribute towards personalized drug treatments based on dysregulated kinase pathways,\u201d he says. The combination of different types of data and technologies should continue to fill in the empty spaces of the current human interactome map. The picture may never be complete, but it will continue to provide insights into cellular mechanisms of health and disease. \u201cI think that the network we have is dense enough for us to start doing studies to classify disease states,\u201d says Wrana. \u201cAs the networks become better and coverage improves, the accuracy of diagnosis will also improve.\u201d See also:  Table of Suppliers \n                     The author file: Trey Ideker \n                   \n                     Cost-effective strategies for completing the interactome \n                   \n                     Maturing interactions \n                   Reprints and Permissions"},
{"file_id": "468855a", "url": "https://www.nature.com/articles/468855a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "468854a", "url": "https://www.nature.com/articles/468854a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Back to main article: Interactome under construction \n             In November, Pacific Biosciences of Menlo Park, California, commercially released its third-generation DNA-sequencing platform, based on its single-molecule, real-time (SMRT) technology. A single DNA polymerase bound to a DNA template is attached to a tiny chamber illuminated by lasers, and nucleotides labelled with coloured fluorophores are introduced to it. As the polymerase incorporates them, each base is held for a few microseconds, while the fluorophore emits coloured light corresponding to the base identity. SMRT technology could also be used to analyse biomolecules other than DNA, and could become a common tool for detecting protein interactions, with some unique features. \u201cThis technology can detect relatively weak interactions,\u201d says Jonas Korlach, a scientific fellow at Pacific Biosciences, adding that it could pick out interactions that happen so quickly that they can't be identified by current methods.  As a step towards such applications, Joseph Puglisi, a structural biologist at Stanford University School of Medicine in California, and his group, with scientists at Pacific Biosciences, observed transfer RNAs binding to single ribosomes in real time 13 . In an unpublished follow-up, Puglisi's group has used SMRT technology to watch interactions between transfer RNAs, ribosomes and protein factors to determine how the translation machinery synthesizes proteins. \u201cWe have just seen the tip of the iceberg in terms of applications,\u201d says Korlach. \n               L.B. \n             Reprints and Permissions"},
{"file_id": "4671139a", "url": "https://www.nature.com/articles/4671139a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "468852a", "url": "https://www.nature.com/articles/468852a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Back to main article: Interactome under construction \n             The two main methods for finding protein\u2013protein interactions are the yeast two-hybrid (Y2H) system and co-immunoprecipitation followed by mass spectrometry. Several companies sell reagents for both approaches. Invitrogen of Carlsbad, California, sells the ProQuest Two-Hybrid System with Gateway Technology. This is based on Y2H, with modifications to decrease false-positive results and allow rapid characterization, says the company. Other firms provide vectors used to produce proteins with affinity tags, which can easily be immunoprecipitated along with other interacting proteins. A polypeptide tag called Flag is popular among researchers, and Sigma Aldrich of St Louis, Missouri, provides several Flag-genes for purchase. Promega in Madison, Wisconsin, has the HaloTag technology, in which a protein of interest is expressed in fusion with a tag protein engineered from a bacterial enzyme. This tag can be used to purify the protein, and any interacting with it, by binding to a resin. The tag is cleaved off using a protease.  For researchers who don't have the time or infrastructure to do the experiments, companies such as Hybrigenics in Paris and Dualsystems Biotech of Schlieren, Switzerland, offer Y2H-based screening. \u201cWe have complex libraries with ten times more independent clones than most other libraries, which we screen to saturation. And rather than screening full-length proteins, we screen for interactions with domains,\u201d says Etienne Formstecher, director of scientific projects and sales at Hybrigenics. \u201cFull-length proteins can have some domains buried and not available to interact, at least in yeast where you may not have signals to unlock a closed protein conformation.\u201d A customer is given a list of proteins that interact with the protein of interest; it indicates which domains are making contact and provides a confidence score for each interaction. Innoprot in Derio, Spain, provides an interaction service using tag-based purification designed for high-throughput analysis. And Invitrogen's ProtoArray Protein\u2013Protein Interaction Service uses microarrays containing more than 9,000 human proteins to identify proteins that interact with any protein of interest. \n               L.B. \n             Reprints and Permissions"},
{"file_id": "4671136a", "url": "https://www.nature.com/articles/4671136a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Back to main article: The search for association \n             When single nucleotide polymorphism (SNP) studies failed to explain much of the heritability of diseases, researchers began pinning their hopes on a trickier source of variability: copy number variation (CNV). Whereas SNPs \u2014 changes of one DNA letter into another \u2014 are relatively easy for microarrays to detect and for databases to compile and sort, CNVs are a headache to identify and classify. Certain stretches of DNA are duplicated, inverted or repeated in some individuals and missing from others. \u201cIt's more complicated and the data will always be a little more dirty,\u201d says Stephen Scherer, director of the Centre for Applied Genomics at the Hospital for Sick Children in Toronto, Canada. In some cases, researchers can detect CNVs using microarrays designed for detecting SNPs. Others use products designed to identify CNVs directly, from companies such as Agilent Technologies in Santa Clara, California, and Roche Nimblegen in Madison, Wisconsin. One Agilent array, designed with the Wellcome Trust Case Control Consortium, detects about 11,000 common CNVs. Measuring whether a nucleotide at a particular spot is A or G is easier than detecting how many times a certain sequence occurs. That concerns Peter Donnelly, director of the Wellcome Trust Centre for Human Genetics in Oxford, UK. \u201cBecause there was a long history of GWA studies that didn't replicate, the field insists on strong criteria for declaring an association,\u201d he says. \u201cYet when it moves to CNVs, which are harder to measure, the standards the field requires are weaker.\u201d The jury is out on how much CNVs matter for common diseases. A study this year 8  profiled 3,423 CNVs, or perhaps half of all those larger than 500 base pairs. It found that most not only don't explain much disease, but are also so closely associated with common SNPs that they've already been explored, albeit indirectly. Scherer is not so sure. He was part of a team that resequenced a human genome and compared it to a reference. It found that the genome differed from the reference in only 0.1% of SNPs, but in 1.2% of CNVs. The analysis indicated that up to one-quarter of CNVs are not associated with SNPs, and so are likely to be missed by SNP studies 9 . As with SNPs, larger effects may be found in rarer and harder-to-measure variants. Scherer has done studies showing that people with autism-spectrum disorders carry more rare CNVs than do controls. To be certain that the CNVs were correctly typed, he and his colleagues ran subsets of samples through calling algorithms that convert an instrument's signals into a sequence of base pairs, and used two platforms (by Illumina, of San Diego, California, and Agilent) to identify them 10 . Scherer says that many research groups are still learning about CNVs and don't fully realize the need to validate their data. \u201cPeople are looking for low-hanging fruit; they see what they want to see and publish it,\u201d he says. The situation is improving, with the maturation of databases that collect diverse data on variation. \u201cNow that we have much better data sets to compare to, it's becoming more accurate.\u201d \n               M.B. \n             Reprints and Permissions"},
{"file_id": "4661139a", "url": "https://www.nature.com/articles/4661139a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "Computerized robotics are already easing lab-based wet tasks such as feeding cells and changing media. Several vendors now sell programs that can track cells in flat culture, keeping them in focus and in the field of view. The latest version of the Nikon BioStation CT can 'memorize' the positions of non-motile cells before a plate is removed for media exchange, and can then continue tracking them when the plate is replaced, avoiding the 'image jiggle' that would disrupt statistical analysis, says Ned Jastromb, a senior application manager at Nikon Instruments in Melville, New York. It also integrates a calendar function with a robotic system that slides culture plates in and out of an imaging area on schedule, allowing one instrument to run several long-term experiments. But software is poised to solve a wider range of problems. By combining a fast image-acquisition program with a noise-reducing algorithm that compares consecutive images, John Sedat at the University of California, San Francisco, and his colleagues decreased the amount of light needed to image yeast cell division by several orders of magnitude  5 . Advances in fully automated cell identification and tracking, and modern continuous cell-imaging techniques can outperform traditional manual methods 6 . Historically, software advances have spread slowly because programs designed to follow a particular cell type tend not to recognize other types, says Andrew Cohen, a computer engineer at the University of Wisconsin\u2013Milwaukee. More broadly, Cohen says he may be on the cusp of solving a problem that plagues many live-cell imaging experiments. Many software programs work only when cells are sparse. That limits the technology because some cells can grow only in dense cultures, and some cells divide many times before producing the desired cell types, in which case a single cell produces hundreds of daughters. By the time the most interesting cells appears, it is impossible to tell which cells they came from. Recently, Cohen found that an algorithm he originally wrote to follow hundreds of organelles within a single cell can be applied to trace neural stem-cell fate. \u201cOur ability to track very high-density image sequences is going to improve very rapidly,\u201d he says. Larger advances, however, may come less from improvement in technology than from biologists' awareness of what software can do, says Cohen. \u201cSometimes the biologists start out just wanting to characterize data, and they don't think about the big questions they can ask.\u201d \n               M.B. \n             See Also:  Cellular imaging: Taking a long, hard look Reprints and Permissions"},
{"file_id": "4671135a", "url": "https://www.nature.com/articles/4671135a", "year": 2010, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "The list of human genetic variations is expanding; but an understanding of how they contribute to disease is still patchy. Everyone carries dangerous genetic mutations. But only in the past five years or so have researchers begun to use genome-wide association studies (GWAS) to scour human genetic samples for the signals of individual variations. Typically, such studies assess hundreds of thousands of genetic variants in thousands of individuals sorted by traits: a certain height, perhaps, or asthma or obesity. Genetic variants that occur more frequently in one group than in another are subjected to stringent statistical analyses to determine whether associations between them and the traits are the result of biology or mere chance. As of 1 October, an online catalogue of GWAS contained nearly 700 publications linking some 3,000 variants to about 150 traits. The list of traits begins with abdominal aortic aneurysm and ends with YKL-40, a protein used as a biomarker for cancer. Other GWAS have identified correlations between genetic variants and smoking behaviour, sleep duration and general self-reported health. The catalogue is growing swiftly: 9 out of 16 research articles in the October issue of  Nature Genetics  report GWAS. In their current incarnation, GWAS are running into a problem of diminishing returns. By collecting ever-larger samples, researchers are able to find more and more variant\u2013trait associations, but these tend to have smaller and smaller effects. In fact, small effect sizes have been a hallmark of GWAS ever since the studies began. Originally, researchers hoped to find associations in which people carrying one variant would be several times more likely to have a trait than those carrying another. Instead, the effects found have been much more modest. An analysis published in June 2010 (ref.  1 ) pooled findings from several published GWAS that had each associated given traits with single nucleotide polymorphisms (SNPs) \u2014 the simplest and most common type of genetic variant, in which one DNA letter is changed to another. Extrapolating from previous findings, the researchers calculated that 201 SNPs associated with height could explain about 16% of genetic variance, 142 SNPs associated with Crohn's disease could explain about 20%, and 67 SNPs could explain about 17% of genetic variance in each of three common cancers. Although genetic variants with small effects can still help to uncover fundamental biology with therapeutic implications, researchers hunting for those with larger effects are pinning their hopes on several advances: an onslaught of newly discovered simple polymorphisms, the ability to assess more complicated variants ( see 'The tough new variants' ) and multiplying applications of sequencing. If the human genome were an archaeological site, these options would be equivalent to canvassing continents with metal detectors of varying convenience and reliability, or picking a handful of sites for a full excavation. \n               Rarer SNPs, bigger effects? \n             GWAS are only as good as the SNPs they sample. Rather than directly finding mutations responsible for an effect, the standard technique identifies SNPs that tend to co-occur with it. And the SNPs that have already been profiled in GWAS may be the ones least likely to be linked with large effects. Because the most common variants were the first to be catalogued, they were also the first that vendors put on genotyping microarrays. To make sure that SNP microarrays could identify variants in the greatest possible number of samples, vendors chose variants that occurred across several geographic populations. These tended to be the SNPs that evolved first, so natural selection has had time to weed out harmful mutations that might have occurred nearby in the genome. But multinational projects are now discovering and characterizing younger, rarer genetic variants. The 1000 Genomes Project aims to sequence 2,500 individuals, who represent an equal distribution from the continental regions of Africa, the Americas, Europe, and east and south Asia. The goal is to identify most of the variants that exist at frequencies of 1% or more in each of the populations studied, says David Altshuler, the project's co-leader and a human geneticist at the Broad Institute in Cambridge, Massachusetts. Similarly, the International HapMap Project has identified millions of SNPs and characterized their occurrence across populations. Sequencing data from ten geographic populations indicated that more than half of the identified genetic variants occur at frequencies of less than 5 per cent. More than one-third of newly discovered SNPs with frequencies of less than 0.5% were observed in only one population. Such discoveries mean that many more variants can be added to microarrays for assay, and so tested in GWAS, says David Bentley, chief scientific officer at Illumina, a genetics company in San Diego, California. \u201cThere is a new generation of GWAS that are fundamentally different from previous studies, because they capture a new fraction of variations that have previously been uncharted,\u201d he says. Illumina and other commercial vendors have been modifying their microarrays in response to releases of data. Illumina unveiled its HumanOmni2.5-Quad DNA Analysis BeadChip in June this year \u2014 letting researchers assay 2.5 million SNPs and other variants \u2014 and plans to launch the Omni5 next year, for 5 million SNPs. Using the Omni5, researchers will be able to combine one set of comprehensive SNPs with specialized sets tuned to emerging sequencing data. Illumina's competitor Affymetrix, in Santa Clara, California, has in its catalogue products geared towards Chinese, Japanese, European and African ethnicities. A new microarray design allows researchers to design custom arrays containing 50,000 up to a planned 5 million SNPs using a database stocked with proprietary and public SNP data. Nonetheless, it is not clear how effective adding to the available SNPs from healthy populations is going to be in finding SNPs associated with disease, says Christophe Lambert, chief executive of Golden Helix, a genetic-analysis company in Bozeman, Montana. This year, his company worked on an association study for Alzheimer's disease that failed to detect a signal from a variant known to boost risk for the condition. The variant, in the gene  APOE , wasn't included on the commercial assay used in the test. Although a custom-designed array found the variant's association with the disease to be extremely significant ( P  < 10 \u221260 ), the standard array did not pick up its signal. \u201cNone of the SNPs on the standard chip was correlated strongly enough with the risk variant to detect it,\u201d says Lambert. Even when Lambert's team used data from the 1000 Genomes Project to 'impute' the presence of one SNP by detecting another, the analysis did not pick up on the association. Sampling more individuals or using denser microarrays might have helped, but identifying variants in diseased individuals would produce the most-informative SNPs for genotyping across populations, says Lambert. Still, the ability to look more deeply within populations has intriguing possibilities. In a study published this September 3 , researchers at deCODE Genetics in Reykjavik found that the same SNP was associated with glaucoma risk in Chinese and Icelandic populations, but in the former it was much rarer and indicated a much higher risk. And if different susceptibility variants show up near the same gene in different populations, researchers will have independently implicated that genomic area in the disease. Working across populations and with rarer variants can get complicated, says Augustine Kong, head of statistics at deCODE. SNPs specific to a particular population could be difficult to replicate, and the lower the frequency of an allele, the larger the number of samples needed to detect an association. However, if rarer SNPs have stronger effects, larger sample sizes might not be necessary. Researchers are keen to find out whether a substantial number of the new variants discovered by genome-mapping projects will be associated with large effects. \u201cBefore, we just didn't have the technology to interrogate these low-frequency variants comprehensively,\u201d he says. \u201cIt gives you chances that you didn't have before to make discoveries.\u201d \n               Sequencing straight to causal variants \n             Some experts think that it is time to skip array-based GWAS that find SNPs associated with causative variants, and to hunt for contributing variants directly. Mary-Claire King is a geneticist at the University of Washington in Seattle, whose work in family studies identified the breast-cancer genes  BRCA1  and  BRCA2 . She says that even the rarer variants discovered by the 1000 Genomes Project are unlikely to be highly associated with disease. New variants are literally born every generation, she says, so a frequency rate of even 0.5% means that a variant has persisted for a while. \u201cThe question, is how common can an allele become if there is selection against it and none for it? Not very,\u201d says King. She advocates using sequencing within large families to find and track alleles that are inherited along with disease. Results from the 1000 Genomes Project will be useful, she says, not for finding SNPs to pursue but for filtering out variants that are not truly rare. For David Goldstein, director of the Center for Genome Variation at Duke University in Durham, North Carolina, the main limitation for SNP-based GWAS is that they usually don't allow identification of the precise causal variant that influences the trait, but instead implicate a genomic region within which the causal variants must reside. The priority for research now, he says, is to focus on identifying the precise variants that contribute to disease; doing so will provide much more information about the relevant biological processes. This year 4 , researchers reported the first use of massively parallel sequencing to identify the gene responsible for a Mendelian disease, one which is caused by mutations in a single gene. Researchers at the University of Washington took samples from just four individuals with the developmental disorder Miller syndrome, including two siblings, and sequenced all the coding regions of their genomes using a technique called whole-exome sequencing, which relies on genome-capture products from companies such as Agilent Technologies in Santa Clara, California, and Roche Nimblegen in Madison, Wisconsin, followed by next-generation sequencing. Although still labour-intensive, the method requires only about 5% as much sequencing as a whole genome. By filtering identified variants against publicly available SNPs and other human exome data, the researchers found that all four subjects carried previously unidentified mutations in a single gene involved in synthesizing nucleotides; follow-up studies in three further people with Miller syndrome revealed mutations in the same gene. Since then, those and other researchers have published a slew of papers tying a variety of other inherited conditions to variation in individual genes. The signals for diseases caused by multiple genes will not be so clear, but bioinformatics techniques borrowed from SNP-based GWAS are being applied to sequencing data. To enable this, many types of variants must be placed into distinct categories so that they can be subjected to statistical analyses. Collapsing or binning methods count how many of a particular kind of variant are found in a gene or a predetermined stretch of base pairs, and then compare frequencies in individuals with the disease to those without it. But researchers are still learning how to weed out artefacts from the sequencing data, says Goldstein. With sequencing, \u201cyou have to pick which variants to pursue, and that's prone to statistical abuse\u201d, he says. \u201cPeople could say the association is weak, but it makes sense.\u201d ( See 'Seeing more SNPs' .) One problem is that it is unclear how to classify different kinds of mutations. It might make sense, for example, to lump together different mutations in the same gene that stop translation early. But what about apparently silent mutations, or others whose effects on protein products seem minor? Another issue is working out the parameters for determining effective controls; because sequencing studies have smaller sets of controls, researchers need to be more rigorous to make sure that, for example, control sets don't include individuals with late-onset disease. But those problems won't send researchers back to SNP-based microarrays, says Goldstein. \u201cIf you were launching a new study now on a common disease, you'd turn to sequencing-based studies.\u201d \n               The common touch \n             First-generation GWAS looked at common genetic variants, but many initial-sequencing studies concentrate on finding individuals with extreme forms of disease, because cost limits them to small sample sizes. Prices are changing quickly and vary by sample and technology, but at the moment, genotyping a sample for millions of SNPs costs about US$400, whereas sequencing an entire genome costs around $10,000. Finding the genetic basis for an extreme form of disease can shed light on more common forms: in the 1980s, well before sequencing, family-based studies of severe cholesteraemia led to the discovery of the low-density lipoprotein receptor gene, common variants of which are now associated with high cholesterol levels 5 . But Altshuler thinks that studies linking rare variants with strong effects need to be followed up to understand how the relevant genes contribute to common forms of the disease. Trying to study common diseases as if they were single-gene disorders will be of limited use, he says. \u201cMendelian forms of type 2 diabetes explain less than 1% of heritability, but GWAS explain about 10%,\u201d he says. To fully understand both the inheritance and mechanisms of common diseases, he says, it will be necessary to study the diseases as they occur in the general population. Another problem with sequencing is that it is slow \u2014 one reason why Illumina's Bentley, whose company does both SNP genotyping and sequencing, says that he doesn't expect to see a decline in demand for microarrays any time soon. \u201cWith our best efforts at the moment we are sequencing one genome every three days; we can genotype more than 50 to 100 samples every three days,\u201d he says. Even if GWAS continue to find only small effect sizes, they can still have a large impact, says Peter Donnelly, director of the Wellcome Trust Centre for Human Genetics in Oxford, UK. \u201cThere is real value in working out the genetic architecture of a disease, regardless of what it turns out to be. For example, even if all the genetic components of a disease were based in very many common variants with small effects, it would be good to know that.\u201d And even if the effects of variants of a gene in a general population are small, those of modulating that gene with a drug can be large. For instance, variations in the gene encoding 3-hydroxy-3-methylglutaryl coenzyme A reductase have been connected in GWAS with small effects 6  on cholesterol levels, but the statin drugs that modulate that gene product are effective and very widely prescribed. Although statins were not inspired by GWAS, such studies have turned up surprising connections with therapeutic implications, such as the role of the immune system in age-related macular degeneration, or of cell-cycle regulators in type 2 diabetes. In fact, says Altshuler, such results could be useful for focusing sequencing studies. \u201cThe genome-wide association paradigm might be that you find the gene using GWAS, and then sequence to find the rarer variants.\u201d One of the biggest GWAS so far assessed samples from more than 100,000 individuals for more than 2 million SNPs, and identified 95 loci associated with variation in cholesterol and triglyceride levels in blood, 59 of which had never been reported before, and many of which were not near genes known to be associated with lipid metabolism 5 . Follow-up experiments in mice not only showed that some newly implicated genes had direct effects on plasmid lipid levels, but also identified a new cell-signalling pathway that could be targeted for therapeutic intervention. Another study 7  examined four genes that had been implicated by GWAS as contributing to high blood-triglyceride levels. Common variants explained less than 10% of observed variation, so researchers sequenced the genes to identify rare missense and nonsense variants \u2014 two categories of mutations likely to change protein function. Nearly twice as many of these were found in affected individuals than in controls. \n               Different strategies \n             The debate over the best approach for finding causal variants, says Altshuler, reflects researchers' various options for studying disease, and their limited funds. The decision whether to sequence a handful of samples or genotype thousands depends on whether researchers believe that a disease will be explained by a few rare variants or many common ones. The answer will vary by disease. Current GWAS, for example, explain more heritability for autoimmune disorders and late-onset diseases such as Alzheimer's and heart disease than for mental conditions such as schizophrenia and autism. Natural selection suggests ready explanations, although they are hard to prove. Almost by definition, late-onset diseases tend to affect individuals in their post-reproductive years, and so are less likely to be selected against. And some genetic variants that contribute to one disease might actually be protective against others, and so could be favoured by natural selection. Genetic variants for sickle-cell anaemia, for example, can help to prevent carriers from contracting malaria, and there are hints that genes causing predisposition to some autoimmune diseases also confer resistance to infection. In an effort to gather concrete evidence on which technologies are best suited to explaining the inheritance of common diseases, Altshuler has begun a study, with Mike Boehnke at the University of Michigan and Mark McCarthy at the University of Oxford, to compare the same population using several techniques. In this case, the study will compare what Altshuler calls \u201cextremes of risk\u201d: subjects who are at high risk for diabetes because of their age and weight but do not have the disease will be compared with slimmer, younger subjects who have been diagnosed with it. Presumably, individuals in the first group will carry relatively more protective variants, whereas those in the latter will have more susceptibility variants. About 2,600 people will be genotyped for 5 million SNPs, and be submitted to whole-exome and whole-genome sequencing. Altshuler says that the study should not only uncover important information about diabetes, but also offer empirical data to help researchers choose the most appropriate technology, or combination of technologies. \u201cWe want to know what each approach finds that the others don't,\u201d he says. \u201cRight now, no one actually knows which one is going to apply to which disease. Investigators have to take different bets.\u201d \n                     Finding the missing heritability of complex diseases \n                   \n                     A map of human genome variation from population-scale sequencing \n                   \n                     Genome studies: Genetics by numbers \n                   \n                     Genomics: In search of rare human variants \n                   \n                     US National Institutes of Health catalogue of published Genome-wide Association Studies \n                   Reprints and Permissions"},
{"file_id": "4661141a", "url": "https://www.nature.com/articles/4661141a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "463977a", "url": "https://www.nature.com/articles/463977a", "year": 2010, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "As the techniques for imaging whole animals become more sophisticated, researchers are able to get a clearer picture of what is going on inside. Monya Baker looks at the options available. Dead mice tell too few tales. Conventional animal imaging requires sacrificing multiple animals at numerous time points, then slicing and staining tissue to identify the location and state of particular molecules at a point in time. However, it would be easier and more instructive if researchers could image the whole body to follow the biological processes within a live animal, ideally with no need for surgery or other invasive techniques. There are many established technologies for human imaging that work in animals, but advances in optical imaging in particular are providing fresh opportunities to see inside smaller creatures.  Such advances are allowing researchers to follow disease progression and drug response more precisely than they can from dissecting organs, says David Piwnica-Worms, who directs the molecular imaging centre at Washington University in Saint Louis, Missouri. \u201cBecause you can use each animal as its own control, something that would have been lost in the noise becomes very, very clear.\u201d And whole-body imaging means not just better data, but also new types of data. For example, disease processes often start well before symptoms become evident. Last year, using new optical imaging technologies, researchers led by Stanley Prusiner at the University of California, San Francisco, detected the onset of a mouse equivalent of neurodegenerative conditions such as Creutzfeldt\u2013Jakob disease nearly two months before behaviour was affected 1 . Rather than comparing brains of animals killed at various stages of disease, the researchers could watch the prion disease spread through the brains of individual animals. Researchers can also monitor other processes: T cells travelling to inflamed sites, tumours spreading or shrinking, even enzymes catalysing reactions within cells. A diverse range of imaging techniques, or modalities, is now available (see  Table ). Magnetic resonance imaging (MRI), positron electron tomography (PET), computed tomography (CT) and single-photon emission computed tomography (SPECT) are all used routinely to scan patients in hospitals; smaller, less expensive versions have been produced for animal research. These techniques can penetrate deep into tissue, and sources of distortion are relatively few and largely understood. They are often used to survey whole bodies for disease and to do cross-sectional imaging, particularly for research on deep-seated organs.  Optical techniques are used to probe more-local processes and can readily make use of multiple labels, or signal-emitting tags that are attached to a molecule of interest, such as a fluorescent dye on an antibody. The downside is that light is scattered and absorbed quickly within the body, complicating attempts to quantify signals and limiting imaging ability to a couple of centimetres or so below the skin surface. \u201cNo single modality has the lock on being the best molecular strategy for whole-animal imaging under all circumstances. Different biological queries require different strategies,\u201d says Piwnica-Worms. \u201cThe different modalities have strengths that apply in one niche but not in another.\u201d As a field, whole-animal imaging has come a long way since the 1990s, when vendors began offering mouse-sized versions of their instruments to lure customers from research hospitals, and when those selling systems to read labelled gels started to adapt their systems to image mice. \n               Fast, cheap and multicoloured \n             Optical techniques have probably been the fastest growing way to image small animals. At the US National Cancer Institute's Small Animal Imaging Resource Program, twice as many funding recipients bought optical equipment as bought MRI, PET or SPECT machines for imaging small animals from 1999 to 2007. Programme director Barbara Croft says that much of this can be explained by the fact that optical instruments are perhaps a quarter of the cost of the other imaging technologies, and are also easy to use. \u201cPeople who don't know anything else about small-animal imaging are more likely to buy optical equipment than anything else,\u201d she says. There are two main approaches to optical imaging. In fluorescent techniques, labels introduced into an animal give off light of one wavelength when excited by light of another wavelength, so light must travel into the animal and back out again, getting scattered and absorbed in both directions. Bioluminescent techniques, however, rely on chemical reactions that produce light from within the animal, so light needs to travel in only one direction. However, almost all techniques rely on transgenic cells that express the enzyme luciferase, and the light produced by bioluminescent reactions tends to be in shorter wavelengths that are more readily scattered by tissue. In a typical experiment, mice with impaired immune systems are implanted with pathogens or cancer cells that have been genetically engineered to express luciferase, then injected with the appropriate substrate, resulting in a chemical reaction that emits light. Researchers can then create cohorts of animals or administer treatments according to how much a tumour has grown or an infection has spread. \u201cInvestigators like this because it allows you to eliminate some of the animal-to animal variation,\u201d says Stephen McAndrew of Taconic in Cranbury, New Jersey, which recently acquired the business of transgenic animals, cancer-cell lines and bacterial lines from Caliper Life Sciences in Hopkinton, Massachusetts. This can be a powerful way to show the efficacy of certain drug compounds and is regularly used in applications for the approval of investigational new drugs, he says. \u201cYou can get a lot more data points per group of animals using these technologies. My sense is that the Food and Drug Administration is very receptive.\u201d Taconic offers nearly four dozen light-producing mice, and plans to greatly expand the types of light-emitting cancer lines it offers. And bioluminescence can be used to follow not just engineered cells but also the molecular processes within them by means of proteins called split reporters. These are used to ensure that luciferase fragments come together only when certain proteins interact or when a connecting peptide is dephosphorylated; they can also be used to monitor cell signalling or kinase activity within a living animal. Like bioluminescent techniques, fluorescent techniques can be used for molecular imaging, such as tracking enzyme activity, or marking cells expressing a particular receptor. In this case, rather than genetically engineering cells to express a luciferase, researchers attach fluorescing dyes or nanoparticles to appropriate ligands in the laboratory, and the whole complex is then injected into the animal. One disadvantage of this method (as well as with similar labels in MRI, PET and SPECT) is that the imaging agents will also show up not just in the target site (for example, a tumour containing a particular surface receptor), but also in blood and in healthy organs. Such spurious signals can make it difficult to distinguish between, for instance, a highly vascularized site and the target. Moreover, strategies to avoid this problem can introduce other issues: increasing the dose of the imaging agent can be toxic, and waiting for the agent to clear the bloodstream and aggregate in the target zone decreases the time available for experiments. Hisataka Kobayashi at the US National Cancer Institute in Bethesda, Maryland, grew frustrated with this problem of 'always on' labels when working with MRI and PET. So he decided to work on new probes, designing 'activatable fluorophores', which emit signals only at the desired target. Activatable fluorophores can be made in many ways, but the goal is the same, says Piwnica-Worms, who has developed similar technologies. \u201cThe signal is silent while the fluorophore is circulating around the body. You build up signal at the target and only at the target. That idea has been out there for a while, but putting it into whole-animal models is what's coming along now.\u201d  One of the most common strategies for fine-tuning the signal is to combine a fluorophore with a transporter tag and a quencher molecule. The transporter means that the combined molecule will be taken up only by certain types of cells. The quencher prevents fluorescence until the fluorophore binds to a cell or interacts with intercellular proteases, allowing fluorescence to occur. Kobayashi recently published work on the fluorophore indocyanine green (or ICG, which, despite its name, gives off infrared light when it fluoresces). Although ICG has been clinically approved for years, it stops fluorescing when it conjugates (or covalently binds) to protein, making chemists reluctant to use it. Kobayashi reasoned that this silencing might be an advantage: he attached ICG to an antibody that both silenced the fluorescence and carried the fluorophore to the target of interest. Once the antibody was bound to the target and brought into the cell, ICG detached, becoming fluorescent 2 . The fact that this advantage was overlooked, says Kobayashi, is an example of how chemists' thinking can hold back development of activatable fluorophores; chemists work with such labels in solution and aren't trained to think about how fluorophores will act inside an animal's body, he says. \u201cWe pick up a lot of trash.\u201d Probes are the workhorses of the optical imaging system, and researchers such as Kobayashi and Piwnica-Worms are coming up with increasingly creative ways to get them into the right places and to detect them once they are there. Such advances in technology promise that researchers will soon be able to image more processes in more types of cells (see  'Probe progress' ). Partly as a result, expectations for optical approaches have been rising steadily, says William McLaughlin of Carestream Molecular Imaging in Woodbridge, Connecticut, which produces instruments for optical imaging. \u201cInitially, people were seeing things they hadn't seen before, so they just wanted to see them,\u201d he recalls. Now they want to go beyond pretty pictures and actually get quantitative data, he says. Instrument-makers have to calibrate everything: from how lamps change with age, to how light criss-crosses the machine \u2014 anything that could affect the measurement. Stephen Oldfield is the senior director of imaging marketing at Caliper, which sells the IVIS Spectrum, among other instruments. He thinks that three-dimensional (3D) imaging will be the next advance to become routine in optical imaging. \u201cEarly feedback was that 3D data just didn't smell right, but we can now show data that look like what you would expect from the anatomy,\u201d he says. \u201cI think that's going to become a standard. It offers more in-depth quantification of biological events, and the ability to co-register or integrate data with other clinical 3D modalities.\u201d \n               Multimodalities \n             \u201cCombining modalities is certainly where people have been looking most recently,\u201d says Croft. Companies such as Siemens in Berlin and Philips in Amsterdam now provide machines that combine CT with PET or SPECT. Researchers are also combining PET with MRI.  Bringing modalities together can be far from straightforward. The magnetic fields of MRI interfere with the radioactivity of PET and SPECT, for instance. Even though bioluminescence and fluorescence techniques both work by detecting light emitted from labels within the animal, fluorescent labels must first be activated by light and that must be accounted for. Moreover, the cameras need to be optimized for the different wavelengths emitted by the various labels. Samuel Achilefu, a radiologist at Washington University in Saint Louis, is creating imaging agents designed for multimodal studies. He and his group have created an imaging probe 3  that binds readily to  64 Cu \u2014 a radionuclide used in PET \u2014 and that contains a dye that fluoresces only after part of the probe has been cleaved by the enzyme caspase-3. This approach allows researchers to use PET to collect quantitative information about a disease tissue, then use the activatable fluorescent signal to detect a biomolecular event, says Achilefu, who is working to create monomolecular imaging agents that combine optical probes with probes for MRI, CT and ultrasound. An alternative way to combine modalities is to create a tray, or gantry, that enables an animal to be moved between instruments without repositioning it. Systems that perform both radioisotope and X-ray imaging, for example, have trays that slide sedated animals from one detector to another. Other approaches include moulds or tubes that keep the animal still and can be moved between machines depending on what data are desired.  Furthermore, time is a factor: X-rays and optical imaging can be completed in about a minute, so both can be done in one session relatively easily. An MRI scan, however, can take hours. Combining the modalities can therefore lower throughput and extend the time that animals need to be kept under anaesthesia or are subjected to radiation, potentially harming the animal or altering the disease process. Despite these issues, combining modalities can bring levels of clarity that had not been anticipated, says McLaughlin. Just a few years ago, researchers had to overlay optical images over an outline of the animal. False-colour splotches in a mouse-shaped shadow give only a general idea of where the signals are coming from, but overlaying this information with X-rays leads to a richer interpretation of the images. \u201cIt just makes it so much easier when you've got that anatomical background to be able to see where the signals are actually coming from,\u201d he says. McLaughlin recalls the reaction of researchers getting their first look at optical scans combined with X-rays about two years ago: \u201cPeople would point at the images and say 'yes, yes, I understand'.\u201d \n               Future images \n             New probes and technologies allow for more sophisticated, high-definition imaging in real time. But another trend needs to be taken into account: the influx of biologists into the field. \u201cThey don't know much about imaging, but they need images,\u201d says Michael Olive, vice-president of science and technology at LI-COR Biosciences in Lincoln, Nebraska, which sells optical instruments. This is a trend that Carestream has noticed, too: in the past, the company supplied machines to laboratories with radiologists or to other imaging experts. But recently, says McLaughlin, it has started to focus on \u201ccrossing the chasm\u201d\u2014 reaching basic-research labs in which the experience with imaging has been in microscopy rather than whole-body techniques. \u201cWe're looking at pulling some of the choices out of our system,\u201d he says. The goal is to allow researchers to study biology without needing to know what a steradian is. \u201cIt's really moving into the masses,\u201d McLaughlin says. Reprints and Permissions"},
{"file_id": "4641225a", "url": "https://www.nature.com/articles/4641225a", "year": 2010, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "The scientific community now seems convinced that small RNAs will become therapies, if new tools can help these large molecules to make it safely into cells. Monya Baker reports. Researchers in the field of small RNAs have been on a wild ride. Twenty years ago, RNA was considered a passive conveyer of information between DNA and protein. Now it is understood that RNA controls and coordinates almost everything that goes on in a cell. From being long unrecognized, then heralded as a convenient tool for screening gene function, small RNAs are now hotly pursued as therapies. Advocates of using small RNAs to treat disease like to tick off the hurdles that were anticipated but seem to have been cleared: that the computational techniques for predicting appropriate RNA sequences would be overwhelming, that synthetic oligonucleotides would be too expensive to manufacture, that the problems uncovered with earlier RNA drugs would crop up again, and that there would be no way of mitigating 'off-target' (or nonspecific) effects. The field has moved fast. The complex process by which small RNAs are able to silence genes by targeting complementary messenger RNA molecules for destruction was recognized and named RNA interference (RNAi) in 1998 (ref.  1 ). In 2004, the first RNAi-based experimental therapies entered clinical trials. For the past few years, though, researchers have been stalled by the challenge of delivering small RNA molecules  in vivo . \u201cIt's still the place where the most important innovations are being made,\u201d says Phillip Sharp, a researcher at the Massachusetts Institute of Technology (MIT) in Cambridge and one of the founders of Alnylam Pharmaceuticals in Cambridge, Massachusetts, the first company explicitly founded to harness RNAi. Delivery is also a stumbling block for researchers hoping to use small RNAs to carry out basic research into diseases in living mammals (see 'From tools to therapies'). \n               Getting in \n             For many experimental RNA therapies, synthetically produced oligonucleotides are somehow delivered into the desired cells in the body by way of targeting agents, chemical modifications or administration directly to the organ of interest. To test this approach, various clinical trials are in progress, involving many companies, including Alnylam, Santaris Pharma, headquartered in Hoersholm, Denmark, and Tekmira Pharmaceuticals in Burnaby, Canada. In another approach, genetic material is delivered not just into cells but into the nucleus to direct the production of small RNA precursors by the cell's gene-transcription machinery. Most techniques involve modified viral vectors, particularly lentiviruses, and clinical trials are under way for treating metastatic melanoma (at Duke University in Durham, North Carolina) and HIV infection (at the City of Hope's Helford Clinical Research Hospital in Duarte, California, with vectors supplied by biotechnology company Benitec of Melbourne, Australia). The optimal approach depends on both the tissue of interest and the genes targeted, says Mark Kay, who works on both types of delivery mechanism at Stanford University in California. Oligonucleotides need reach only the cytoplasm to have an effect, whereas genetic material that must be transcribed has to enter the nucleus. Genetic vectors have several advantages over oligonucleotides: the level of 'knockdown' of gene expression seems stronger; a treatment may need to be carried out only once, given that the dose is maintained by transcription (in contrast to oligonucleotides, which eventually degrade) and that the vectors are replicated during cell division; and the dose of small RNAs can remain constant even if cells are dividing rapidly. Most companies, however, have embraced synthetic oligonucleotides, which work more like conventional drugs. \u201cWith viral vectors, it's hard to control when they are on and off,\u201d Kay says. \u201cWith synthetic RNA, you just give the dose, and when it's gone, it's gone.\u201d But even getting enough RNA (or, in some cases, DNA or hybrid oligonucleotides) into the cytoplasm is a major challenge. Highly charged and 10 to 30 times heavier than typical small-molecule drugs, oligonucleotides do not readily enter cells. In addition, nucleases chop them into fragments, and they are quickly excreted in the urine. Some companies are exploring encapsulation technologies to solve these issues. In this approach, oligonucleotides are sequestered in various kinds of nanoparticle to further protect them from degradation and to direct them to the appropriate tissues. This March, researchers from the California Institute of Technology and Calando Pharmaceuticals (a subsidiary of Arrowhead Research), both in Pasadena, California, announced an important first for encapsulation technologies: proof of principle in a human patient. The small study showed that nanoparticles packed with short interfering RNAs (siRNAs) and injected into a patient's blood delivered their cargo to melanoma cells, silencing the targeted gene 2 . These nanoparticles, similar to those of other companies, have separate components for encasing RNA, targeting specific cell types, promoting stability and preventing aggregation, each of which is highly engineered and optimized. The list of encapsulation strategies being tested is long. RXi Pharmaceuticals in Worcester, Massachusetts, is using components of yeast cell walls. Tekmira is working on stable nucleic-acid\u2013lipid particles (SNALPs), which consist of a suite of different lipids encapsulating small RNAs. In March, the company announced a collaboration to evaluate SNALP technology for delivering siRNAs provided by Pfizer, headquartered in New York. The same month, Alnylam announced that siRNAs formulated with a lipid-based coating it had developed in collaboration with researchers at MIT could silence ten targeted genes simultaneously when delivered to rodents. Which approaches will be successful is still very much up in the air. \u201cWe believe there's no specific, single magic bullet,\u201d says John Maraganore, chief executive of Alnylam. \u201cWe've chosen to develop a range of strategies.\u201d \n               The right molecule \n             The downside of delivery vehicles is that they add complexity and manufacturing costs. Their components must be thoroughly vetted in terms of intellectual property and potential toxicity, and figuring out how to mix, measure and deliver both RNA molecules and nanoparticles is far from trivial. An alternative strategy is to pursue technologies that can bring molecules readily to the therapeutic site for a particular disease without needing such vehicles: injection into the eye or inhalation into the respiratory tract, for example. \u201cBecause you can design siRNAs to any sequence, it opens up so many possibilities that choosing a disease indication and even a target becomes very complicated,\u201d says Pamela Pavco, vice-president of pharmaceutical development at RXi. \u201cSo picking the right path forward becomes very important.\u201d Quark Pharmaceuticals in Fremont, California, for example, is the first company to start a clinical trial using siRNAs delivered into the bloodstream; its drug candidate, QPI-1002, is designed to prevent post-surgical acute injury of the kidneys, where RNA molecules are naturally taken up. \u201cWe go where the siRNAs go,\u201d says James Thompson, vice-president of pharmaceutical development. Several companies seeking a way to deliver small RNAs are pursuing the strategy of modifying the RNAs themselves so that they will reach their target. \u201cThe advantage is that you are working with one compound, so there is no formulation,\u201d says Anastasia Khvorova, chief scientific officer at RXi, of her company's most recent development programmes. \u201cWe are trying to bring all the components that you need onto the molecule.\u201d Almost all small RNAs that are introduced into cells or animals have been chemically modified. The modifications can protect the molecules against degradation by nucleases, minimize the off-target effects, increase the potency of the knockdown, and avoid the triggering of immune responses that normally protect against viruses. The types and combinations of such chemical modification are endless but often proprietary. Companies are investigating a variety of oligonucleotides: for example, shorter or longer than their naturally occurring small RNA counterparts; single-stranded, double-stranded or with gaps in the sequence; or alternating between DNA and RNA. Stanley Crooke, chief executive of Isis Pharmaceuticals in Carlsbad, California, thinks that his company might have made as many as 2,000 types of modification, studying them in animals in the 20 years since it began studying antisense drugs. Even so, there is a push to identify new and better modifications, and many companies are converging on similar ones. This year, Isis plans to pick clinical candidates with bicyclic additions to the sugars in the nucleotides. With these modifications, nucleotides are positioned such that base-pairing becomes more stable, and potency is boosted, Crooke says. MDRNA of Bothell, Washington, acquired the patents for a similar technology, called bridged nucleic acids (BNAs), from Valeant Pharmaceuticals, headquartered in Aliso Viejo, California, this March. Both kinds of modification are similar to Locked Nucleic Acids, produced by Exiqon of Vedbaek, Denmark, and used by Santaris Pharma, which introduce a ring at the 2\u2032 position of each sugar molecule. Such modifications increase the half-life by an order of magnitude compared with the types of oligonucleotide now in the clinic, says Santaris vice-president and chief scientific officer Henrik \u00d8rum. \u201cWe inject these into animals systemically as naked molecules. Once they are in the tissues, they will have a half-life of weeks to many weeks.\u201d Santaris also focuses on making its single-stranded molecules as small as possible, to aid their entry to cells. RXi has also used this strategy, designing 15-nucleotide double-stranded molecules. Another approach that is in development is to conjugate antibodies or other specific targeting agents directly to the RNA molecules. Biotechnology company Dicerna Pharmaceuticals in Watertown, Massachusetts, is working with 27-nucleotide sequences, based on their proprietary Dicer Substrate Technology, which enter the RNAi-processing pathway earlier than other synthetic counterparts. It has partnered with Kyowa Hakko Kirin, the biopharmaceutical arm of which is based in Princeton, New Jersey, to provide the delivery technology. But like many companies, Dicerna is interested in multiple delivery strategies. The rise of RNAi therapeutics companies has spurred the development of additional technology to track how oligonucleotides are processed by the body, but high-throughput purification and analysis technologies are not designed for such large molecules with a strong negative charge. Researchers who want to track nucleic-acid metabolites from humans and animals have a \u201chuge problem\u201d, says Michael McGinley, bioseparations product manager for Phenomenex in Torrance, California. \u201cYou have polar [charged] oligonucleotides floating around in serum that like to bind to things. How are you going to pull them and their metabolites out and separate them from everything else that's there?\u201d In September of last year, Phenomenex began selling kits and other products, such as its Clarity Oligo-MS and Clarity Oligo-RP chromatography columns, to researchers who are running preclinical and clinical studies. These kits provide high-throughput techniques to isolate synthetic oligonucleotides from biological fluids such as serum and urine, as well as from solid tissues, including liver and lung tissue. Although McGinley anticipates tweaking and improving the products over time, he says that they work well with all of the delivery devices, conjugates and modifications tested so far. \n               Potency potential \n             No matter what the approach, the delivery process is easier if researchers can achieve the same therapeutic effect with fewer molecules, so it is crucial to design small RNAs with high potencies. Most therapeutics companies are using a combination of open-access computer programs and their own algorithms to design siRNA sequences that will knock down a target mRNA. Indeed, some computer programs may generate thousands of sequences against a single mRNA. These are then assessed both by scientists and by bioinformatics programs. That's only the first step, says Khvorova of RXi. \u201cAll this computational screening is incredibly important, but then you need to do physical screening.\u201d There are significant differences between small-molecule screening and RNA screening, says Caroline Shamu, director of the ICCB-Longwood Screening Facility at Harvard Medical School in Boston, Massachusetts. Small-molecule drugs are often screened against proteins in solution, whereas RNAs are screened in cells. So the results of RNA screening are more variable and take much longer to obtain. The most important consideration, says Shamu, is the use of controls. Researchers have a plethora of protocols that are appropriate for screening small molecules against proteins, in which most compounds will show no activity, she says, but many of these strategies cannot be relied on for screening siRNAs. Different siRNA sequences targeted to the same gene will have not only varying potencies for knocking down the expression of this gene but also varying effects on other genes (or off-target effects), and because each cell type expresses a different set of genes, assessing these kinds of effect can be difficult. Researchers have only recently begun to realize that to resolve these issues requires multiple controls and the recharacterizing of controls for each new question and protocol, says Shamu. \u201cThere is no universal negative control for siRNA.\u201d \n               A move to microRNAs \n             This year, Shamu's centre has begun screening a kind of small RNA with even more complex biological effects: microRNA. Whereas siRNAs are designed to silence one gene completely, microRNAs moderate the expression of gene networks. So making siRNAs involves designing siRNAs against a known mRNA, but microRNA screening generally involves mimicking or blocking previously discovered microRNAs and looking for phenotypic effects. For companies working on microRNAs, such as Santaris and miRagen Therapeutics in Boulder, Colorado, screening also involves testing oligonucleotides with various chemical modifications or slight differences in sequence. Research on microRNAs has advanced at a furious pace. In 2001, only three research papers had been published on microRNAs. By September 2009, miRBase, a database of published microRNA sequences, held more than 10,000 sequences, about 700 of which are human. The rate at which new microRNAs are being discovered has slowed recently, but much about how microRNAs work remains to be uncovered. Libraries consisting of a few hundred microRNA sequences are supplied by several companies \u2014 including Thermo Fisher Scientific, headquartered in Waltham, Massachusetts, Ambion, part of Applied Biosystems in Austin, Texas, and Exiqon \u2014 but the tools are still new. \u201cBecause it's unclear how to interpret the results,\u201d says Shamu, \u201cit's unclear how to evaluate the quality of the libraries. We're learning as much about how microRNAs work as we are about how to use these reagents.\u201d People are taking microRNAs into living animals, as well as cells, says Devin Leake, director of research and development at Thermo Scientific Genomics, part of Thermo Fisher Scientific. Later this year, his company will provide lentiviral vectors for the delivery of microRNA mimics  in vivo , and Leake says that researchers are already using microRNAs  in vivo . Perhaps as many as one-third of his customers are using or planning to use the products in animals such as mice, rats and guinea pigs. And this March, researchers led by Robert Weinberg at MIT showed therapeutic proof of principle for targeting microRNAs, in a mouse model of metastasis 3 . Several companies are already developing microRNAs for treating disease, as well as for diagnosing disease (see 'MicroRNAs as biomarkers'). Established companies such as Santaris have brought microRNAs into their discovery and development programmes. In 2007, Alnylam and Isis pooled their expertise and intellectual property to launch Regulus Therapeutics in Carlsbad, California, another company devoted to microRNAs. Since then, Regulus has announced collaborations with drug giant GlaxoSmithKline, headquartered in Brentford, UK, to test microRNAs for treating inflammatory diseases and hepatitis C virus infection. The finding that different siRNA constructs diminish the activities of different sets of off-target genes was made by Aimee Jackson, when working at the now-defunct Rosetta Inpharmatics, a subsidiary of Merck that was based in Seattle, Washington. This effect is now attributed to siRNAs acting as microRNAs and is often controllable through specific chemical modifications. Although researchers are understandably cautious about pursuing constructs that can affect numerous gene products, Jackson is one of many scientists intrigued by the therapeutic potential of microRNAs, an interest that prompted her to move to Regulus as its director of drug discovery. The idea of using microRNAs to fight disease is similar to using combination therapies that block tumour-cell proliferation or HIV replication by targeting several proteins in a pathway, she says. In fact, compared with the sharp, single-target knockdown achieved with siRNA, she thinks that the broader, more moderate effects of microRNAs may be an advantage. \u201cIt's a combination that has been selected by evolution. It's a gentler way of bringing about a therapeutic benefit.\u201d Although microRNAs have complex biological effects, Jackson says that some aspects of using them are simpler. For siRNA, the targets are transcripts, mRNA molecules that could be several thousand nucleotides in length and part of populations that encompass several splice variants. For microRNA-based therapies, the goal is either to knock down or to replace a particular natural microRNA that is fewer than two dozen nucleotides long. The surprises have not stopped with the discovery of siRNAs and microRNAs. Some microRNAs isolated from cells, for example, display a heterogeneity in sequence length that Jackson thinks might reflect molecules with different functions encoded by the same microRNA transcript, requiring researchers to further investigate which is the best target. Such discoveries mean that companies must be vigilant in looking for unanticipated routes for toxicity in their preclinical and clinical testing. \u201cMy feeling is that you're never going to be able to predict all the off-target effects you might get,\u201d says John Rossi, one of the founders of therapeutics companies Dicerna and Calando and chair of molecular and cellular biology at the Beckman Research Institute of the City of Hope. But the flip side of this uncertainty is excitement: as is the case for siRNA, the discovery of worrisome off-target effects might be a window onto cellular machinery that could be harnessed for therapeutic effect. Rossi says that it's becoming routine to find RNAs doing surprising things in surprising places. One aspect that's changing fast is the numbers and types of small RNA being uncovered, says Leake, who develops tools for Thermo Fisher Scientific. He notes the emergence of molecular classes such as long non-coding RNAs and transfer-RNA-derived small RNAs. \u201cNon-coding RNA in general is becoming a widely studied area, so we firmly believe these tools will be applicable to other RNAs.\u201d Quark Pharmaceuticals' James Thompson also marvels at the changing directions in the field. A veteran of the RNA field, he is, like many, baffled at how long it took to notice the importance of tiny RNAs. \u201cThe RNA machinery is as abundant in cells as are ribosomes, and yet we never saw it. We didn't see this coming,\u201d he says. \u201cWho knows what's coming next.\u201d Reprints and Permissions"},
{"file_id": "4641225b", "url": "https://www.nature.com/articles/4641225b", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "It's not only clinical researchers who are looking for the best ways to get synthetic oligonucleotides into cells in living mammals. If the delivery problem were solved, says Phillip Sharp, who studies RNA interference (RNAi) at the Massachusetts Institute of Technology (MIT) in Cambridge, the ability to use animals to study physiology and disease would expand dramatically. \u201cYou can't make a knockout dog, but you could probe genetic function with RNAi.\u201d He is optimistic that in a few years researchers will have worked out ways to exploit RNAi in the immune system and the digestive system, at least. It's an open question whether all tissues will be accessible, he says. \u201cFormulating nanoparticles for certain tissue types is still a pretty sophisticated business.\u201d The challenge of delivering the molecules has not stopped companies from producing  in vivo  research tools. Those offering products for animal RNAi studies include Ambion, part of Applied Biosystems in Austin, Texas; Exiqon in Vedbaek, Denmark; Integrated DNA Technologies, headquartered in Coralville, Iowa; and Thermo Fisher Scientific, headquartered in Waltham, Massachusetts. These products are widely used in mice, but they lack some of the tissue or organ specificity that researchers desire. Companies are listening: in December last year, in what it announced as the first targeted delivery vehicle to hit the market, Bioo Scientific in Austin launched a kit containing a proprietary linking modality that allows researchers to conjugate RNA molecules of their choice to antibodies, in this way targeting them to specific cells. This is still a young field, however, and scientists may rightfully feel that the PubMed database is a better source of such technology than product catalogues. In fact, many of the best encapsulation tools are not readily available, given that the tools companies are now seeding therapeutics companies with technologies and expertise, says Mark Behlke, one of the founders of Dicerna Pharmaceuticals in Watertown, Massachusetts. Dicerna is pursuing RNA drugs built on the same platform as some of the research tools offered by Integrated DNA Technologies, of which he is chief scientific officer. Matt Winkler says that he decided to sell Ambion, the research tools company he founded, and launch a therapeutics-focused company after a chance discovery with enormous therapeutic potential. Ambion scientists who were characterizing small RNAs for the company catalogue realized that cancer tissues were surprisingly deficient in certain RNA molecules. There are other examples. William Marshall co-founded tools company Dharmacon (now owned by Thermo Fisher Scientific) before co-launching miRagen Therapeutics in Boulder, Colorado. Exiqon and Santaris Pharma, headquartered in Hoersholm, Denmark, are commercializing the tool and therapeutic aspects of similar nucleic-acid technologies. And scientists who a few years ago were developing tools are at other companies thinking about the best approaches for clinical trials. \n               M.B. \n             Reprints and Permissions"},
{"file_id": "465827a", "url": "https://www.nature.com/articles/465827a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "4641227a", "url": "https://www.nature.com/articles/4641227a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "In the 1990s, Carlo Croce, then director of the Kimmel Cancer Center in Philadelphia, Pennsylvania, was hunting for genes involved in chronic lymphocytic leukaemia. The disease was consistently associated with a lesion in chromosome 13, and so, back before the human genome was sequenced, his lab determined the identity of the nucleotides in an 800-kilobase stretch from the deleted region and began searching for protein-coding genes. \u201cWe failed, for six years,\u201d recalls Croce, now director of the Human Cancer Genetics Program at the Ohio State University in Columbus. His lead graduate student left science to go to business school. After another false start and a lucky conversation, Croce obtained cells from a patient who had leukaemia involving a very small translocation, only about 30 kilobases. Croce examined this region thoroughly enough to convince himself that it contained no genes. Then he read about microRNAs, which had just been discovered in mice. Further experiments quickly revealed that the region encoded two microRNAs, which, in 2002, were the first to be implicated in disease 4 . A few years later, researchers led by Todd Golub at the Broad Institute in Cambridge, Massachusetts, examined RNA molecules in tumours and reported that using just a small number of microRNAs, about 200, provided a better classification of tumours by type and source than using 16,000 messenger RNAs 5 . There's great potential, says Croce. \u201cThere is no doubt in my mind that microRNA can be used for diagnostic and prognostic purposes.\u201d Indeed, although insurance companies will not yet pay for them, at least two companies offer services that involve testing for microRNAs in biopsies from patients with cancer: Rosetta Genomics in Rehovot, Israel, and Asuragen in Austin, Texas. Such tests are possible because microRNAs are surprisingly stable both in the body and in paraffin blocks, says Muneesh Tewari, a researcher at the Fred Hutchinson Cancer Research Center in Seattle, Washington, and lead author on one of the first papers showing that microRNA can be extracted from plasma and serum 6 . \u201cThe hard part of this,\u201d he says, \u201cis working with small quantities of starting RNAs and applying this to the technologies\u201d that can identify them. Once extracted, known microRNAs can be identified by using microarrays and quantitative PCR; both known and unknown microRNAs can be identified by sequencing. But finding microRNAs in samples from patients is only the first step to identifying which microRNAs carry information about disease, Tewari says. \u201cVery little is known about the variation of microRNAs.\u201d The results of experiments can vary for reasons besides disease, says Jun Lu, a genetics researcher at Yale University in New Haven, Connecticut, and first author on the paper with Golub 5 . But, he says, \u201cif it's a strong discriminator, then you should get the same answer no matter what platform you're using\u201d. \n               M.B. \n             Reprints and Permissions"},
{"file_id": "465824a", "url": "https://www.nature.com/articles/465824a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "Getting proteins to form crystals is only one step for the structural biologist. The next step to sleuthing out a protein's structure involves placing the crystals in an intense beam of X-rays. This radiation bears little resemblance to the broad, diffuse X-rays used in medicine: the powerful X-rays that work best for protein crystallography are produced at giant facilities called synchrotrons, of which only a few dozen exist. At the Advanced Photon Source synchrotron at Argonne National Laboratory, Illinois, for example, electrons race around a 1.1-kilometre track at close to the speed of light. Radiation generated from the electrons is collected into a 70-metre beamline, which focuses X-rays into a 25-square-micrometre area where crystals can be positioned for analysis. Proteins in the crystal scatter the X-rays as they pass through, and researchers can decipher a protein's structure from the resulting diffraction pattern. To generate a complete pattern, the crystal must be rotated within the beam so that X-rays pass through in different orientations. The process requires precision: researchers have to collect enough data to solve a structure, while limiting radiation damage to the crystal.  Technologies for manipulating crystals and keeping them at temperatures below 0 \u00b0C to decrease radiation damage have got better, but the most dramatic improvement is that experiments can now be done using very small crystals or crystals with many poorly diffracting regions, says So Iwata, who heads the Human Receptor Crystallography Project at the Japan Science and Technology Agency in Kyoto. \u201cCrystals that would have been turned away ten years ago are welcome now,\u201d he says. Still, a crystal must be as wide as or wider than the beam passing through it to generate a reliable diffraction pattern. That's a problem, because crystals of membrane proteins tend to be small, says Robert Fischetti, a senior scientist at Argonne National Laboratory, which has produced data for crystal structures of several membrane proteins, including the \u03b2 2 -adrenergic receptor (S. G. Rasmussen  et al .  Nature   450 , 383\u2013387; 2007). Technologies such as lipidic cubic phase crystallization have helped researchers to grow crystals, he says, but these are often only 5\u201310 micrometres across, a tenth the size of most crystals submitted for analysis and much smaller than the X-ray beam used in crystallography studies. Researchers led by Fischetti have developed a new version of a collimator, a device that blocks most of the X-rays to produce a 'minibeam' of 5 micrometres or less. Collimators, essentially engineered strips of platinum, are placed about 3 centimetres from the sample and are much more than simple pinhole apertures, says Fischetti. \u201cWe started out with a single collimator with three parts \u2014 the beam-defining pinhole aperture, the capsule around the pinhole and a forward scatter guard tube.\u201d The first versions of the device caused X-rays to scatter in a way that interfered with the diffraction pattern, but his team has since engineered features, such as a layer of molybdenum, to overcome these problems. They also created double and triple collimators to let users pick the beam size. After one user damaged a collimator by spilling liquid nitrogen on it, they modified the design, eventually making a more robust version with four aperture settings but fewer parts. What has made the collimator most practical, says Fischetti, is automating the process of selecting the aperture to match the crystal. In the past, technicians had to refocus the beam manually to shrink its size, which took hours. Now, he says, \u201cwe can do it in seconds with just clicking\u201d. Besides allowing the study of smaller crystals, smaller beams let researchers identify the parts of the crystal that diffract better. \u201cIn the past, if you had a large crystal that was not homogeneous, you'd look at the crystal and say it was bad,\u201d says Fischetti. \u201cNow, people can find the region that is the best to look at.\u201d  \n               M.B. \n             Reprints and Permissions"},
{"file_id": "4641229a", "url": "https://www.nature.com/articles/4641229a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "465823a", "url": "https://www.nature.com/articles/465823a", "year": 2010, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "Proteins in cell membranes are notoriously hard to crystallize, but new techniques give scientists the means to map them. Monya Baker scouts out the tools for cracking the structure of membrane proteins. If a cell is a house and the cell membrane its walls, then proteins serve as the doors, windows and electricity and telephone lines. Membrane-bound proteins, anchored within the cell's lipid bilayer, regulate the influx and efflux of molecules and information. How and when these membrane proteins change shape determines essential processes, including whether a drug slows a racing heart, an eye detects light or a virus invades a cell. Yet scientists studying these proteins often know only the rough outlines of their shapes. Structures have been solved for fewer than 250 membrane proteins, and almost all of these are from microbes such as bacteria and yeast. Of the 7,000 human membrane proteins, researchers have found high-resolution structures for fewer than 12, and each structure captures only one of the protein's many possible forms. Functional protein assays are important, but are better suited to determining whether a protein performs a certain task than to explaining how or why it does so. Mapped-out structures can give researchers ideas about why a mutation changes a protein's behaviour or help researchers to design drugs. But without a structure, researchers can only speculate about the reasons for drug or mutation effects, says Brian Kobilka, a biochemist at Stanford University in Palo Alto, California. \u201cWhen you have a structure,\u201d he says, \u201cyou can begin to understand.\u201d  Researchers hoping to solve membrane-protein structures face a cruel paradox: to work on a protein's shape, they must remove it from the cell membrane, destabilizing it and disrupting the conformation. \u201cWhen you solubilize the protein, you are taking away the belt that holds it together,\u201d says Raymond Stevens, a biochemist at the Scripps Research Institute in La Jolla, California. Researchers have tended to avoid this added hassle by focusing on proteins that float free in water. According to So Iwata, head of the Human Receptor Crystallography Project at the Japan Science and Technology Agency in Kyoto, membrane structural biology is lagging 20\u201330 years behind the study of soluble proteins. But the field is catching up fast as researchers learn better ways to make, purify and crystallize membrane proteins. The first atomic-resolution crystal structure of a membrane protein, the reaction centre of a photosynthetic bacterium, was published in 1985 (ref.  1 ). It was important not just for the structure, but as proof that membrane proteins could be crystallized. Still, it was 13 years before crystal structures had been solved for 20 membrane proteins. Techniques have improved: in 2006 alone, 21 proteins were reported, mostly from  Escherichia coli  and other bacteria; this year, that number was surpassed by mid-May (see 'Protein progress'). But there's a long way to go. James Bowie, a structural biologist at the University of California, Los Angeles, estimates that representing 90% of structural families would need structures from around 1,700 membrane proteins 2 . His work indicates that most disease-causing mutations are likely to perturb the structure 3 , so the number of clinically relevant structures could be much greater. One sign that crystal structures are easier to solve is that pharmaceutical companies are looking for the structures of membrane proteins to target with drugs. \u201cBefore, they wouldn't spend money on it because it was too risky,\u201d says Iwata. \n               Added stability \n             The largest family of membrane proteins is among the most challenging for biologists. The G-protein-coupled receptors (GPCRs) consist of seven helices that twist and turn through the membrane. Flexible loops extending beyond the lipid bilayer interact with water and the inner helices are normally surrounded by lipids. Ligands that bind these receptors on the outside of the cell membrane cause conformational shifts on the inside that can trigger the cell to respond.  The roughly 800 different GPCRs control pretty much everything that happens in the body \u2014 smell, sight, even response to neuro-transmitters and immune signals. Many frequently prescribed drugs, from antihistamines to \u03b2-blockers, target this class of receptor, and researchers think that crystal structures can help to find more and better drugs more quickly. The first known GPCR structure, published in low resolution in 1993 (ref.  4 ) and in high resolution in 2000 (ref.  5 ), was of the bovine version of rhodopsin, the photoreceptor that enables vision in low light conditions. Rhodopsin, however, is an unusual GPCR because it is particularly stable and is expressed in high enough concentrations to be collected from natural sources \u2014 two characteristics not usually associated with this protein family. As such, \u201crhodopsin didn't tell us how to get structures for other GPCRs\u201d, says Kobilka. It wasn't until 2007 that researchers solved the structure of a second GPCR 6  \u2014 the human \u03b2 2 -adrenergic receptor, which is involved in cardiovascular and pulmonary function. Kobilka led a team that designed an antibody to bind two of the protein's helices, stabilizing the receptor and providing a polar surface that helped crystals to form 7 . In other work, Kobilka and the Scripps Research Institute's Stevens solved a high-resolution structure from another crystal, in which the protein was stabilized by replacing an intracellular loop with another protein 6 , 8 .  Researchers are also hunting for compounds that can boost proteins' stability without adding another protein, a strategy that has allowed Stevens to obtain a portrait of another important GPCR, the A 2A  adenosine receptor 9 , which is involved in many physiological processes and is blocked by caffeine. Receptos, a drug development company co-founded by Stevens and based in San Diego, California, used the same method to solve the structure for sphingosine-1-phosphate receptor subtype 1, a drug target for multiple sclerosis. This year, the company announced a clinical drug candidate designed with reference to this structure.  Another tactic is to stabilize GPCRs through targeted mutagenesis rather than through third-party agents such as antibodies and extra proteins. Researchers led by Chris Tate and Gebhard Schertler at the Medical Research Council's Laboratory of Molecular Biology in Cambridge, UK, for example, identified a handful of mutations in GPCRs that boost stability with no apparent effect on function 10 . In 2007, Tate co-founded Heptares Therapeutics in Welwyn Garden City, UK, which uses the stabilized GPCRs, known as StaRs, to inform drug design and has solved several crystal structures of GPCRs with bound ligands. \n               Sticking with a membrane \n             Techniques for purifying membrane proteins without denaturing them go beyond tool compounds and engineering. Anatrace in Maumee, Ohio, part of Affymetrix, based in Santa Clara, California, sells detergents and lipids used for solubilizing and stabilizing proteins, including Chobimalt, a water-soluble cholesterol derivative; A8-35, a polymer that wraps itself around the membrane protein; and tripod amphipiles, which limit protein mobility and interactions. In 2007, the company launched three or four new products for membrane proteins; last year, it rolled out twenty. The growing number of publications and tools has brought in scientists who previously restricted themselves to soluble proteins, says Ben Travis, the company's research and development manager. \u201cThey're finding the membrane-protein field more accessible,\u201d he says. Perhaps the biggest shift in the field is the ability to accommodate membrane proteins' structural need for fat. \u201cBefore, people tried to purify membrane proteins so they didn't have lipids associated with them, but now we know that it probably isn't a good idea,\u201d says Stephen White, a biophysicist at the University of California, Irvine. To address this, many structural biologists have turned to a technique called lipidic cubic phase (LCP) crystallization, also known as  in meso  crystallization. In this technique, proteins are dissolved in lipids to form membrane-like bilayers around water-filled cavities. This mixture feeds lipids and proteins into crystals as they grow. The idea that one could obtain crystals from a protein embedded in a bilayer was \u201cpretty radical\u201d, says Bowie, who has developed a variant of the technique using bicelles of lipid and detergent. \u201cA number of protein structures would remain unsolved without the LCP method.\u201d The LCP mixture, however, is incredibly difficult to work with. \u201cYou end up with something that looks and feels like very sticky toothpaste,\u201d says Martin Caffrey, a biochemist from Trinity College Dublin, Ireland, who is widely credited with popularizing the technique by inventing a way for researchers to homogenize the solutions. His method involves two syringes, one filled with protein, detergent and water, and the other with lipid. The syringes are coupled together so that each injects into the other; researchers mix the contents by pushing the plungers back and forth. Finally, the mixture is placed onto a crystallization plate along with a solution that promotes precipitation. \u201cAnd then,\u201d he says, \u201cyou pray for crystals.\u201d Although Caffrey says that researchers with access to a good machine shop should be able to build syringe-coupling devices themselves, Emerald Biosystems, a protein reagents and services firm in Bainbridge Island, Washington, makes a kit consisting of plates, a variety of precipitant solutions formulated for cubic phase and a syringe device prefilled with lipids. Formulatrix in Waltham, Massachusetts, sells crystal-imaging and other technologies that work with the LCP technique. Late last year, QIAGEN in Hilden, Germany, began offering a product that allows researchers to grow crystals using the LCP method while following many of the protocols for soluble proteins, such as vapour diffusion. The hardest part, says Frank Sch\u00e4fer, associate director of protein sciences at QIAGEN, was outfitting fluid-handling robots so that they could deal with such viscous material and dispense it into standard crystallization plates. Their success meant that researchers who buy the product don't have to work with the lipids at all. \u201cThis has the potential to be used by everybody. There is no special equipment,\u201d he says. Stevens, who switched to using LCP technology about five years ago, after Vadim Cherezov, who had previously worked with Caffrey, joined his lab, welcomes the development of commercial products for the technique, but believes that they need further refinement. \u201cThose kits have not advanced to the stage where it's routine,\u201d he says. Researchers familiar with LCP methods don't use the kits, and less-experienced labs have trouble using them. Time and communication should solve that problem, though. The National Institutes of Health Roadmap meeting on membrane-protein technologies this November includes a workshop, organized by Stevens and Cherezov, on LCP crystallization technologies.  \n               Computational boosts \n             Laboratory techniques will continue to improve, but some of the most important advances will happen  in silico . Computer programs are getting better at filling in gaps from incomplete or ambiguous data sets. In April, for example, Axel Brunger, professor of molecular and cellular physiology at Stanford University, described a technique for improving the accuracy of low-resolution structures 11 . The added precision provided by his software means that researchers can pinpoint specific amino acids, which should help with protein and drug engineering. So far, Brunger has reported using the algorithm only with soluble protein structures, but it should work for membrane proteins too, he says. But what if you don't have a structure to start with, even a crude one at poor resolution? David Baker at the University of Washington in Seattle is pursuing an approach that builds models of proteins using data that are too sparse to solve any structure at all. His computer program, Rosetta, evaluates possible protein conformations to find the most stable \u2014 and hence most likely \u2014 shape. Although the number of possibilities makes such calculations impractical for all but the smallest proteins, even sparse structural data can be used to refine the search by excluding improbable conformations. Baker compares his software to a vast team of explorers scouting Earth for the lowest possible point. If data showed that it was not in North America, say, then the explorers could search more effectively by focusing their efforts only on other continents, he says. Another program, MODELLER, by Andrej Sali at the University of California, San Francisco, is also very popular among structural biologists and modellers. Sali's software uses sequence homology to create three-dimensional best guesses for proteins of unknown structure. These and other computer programs will improve as more protein structures are solved \u2014 and will in turn allow more structures to be found. They could pave the way for researchers to get high-resolution structures using assorted data sources, including X-ray diffraction (see  'Crystal-clear images' ), cryoelectron microscopy (cryoEM) and nuclear magnetic resonance (NMR). \n               Structure without crystals \n             The prospect of refining coarse structures through computer modelling entices scientists such as Fred Sigworth at Yale University, New Haven, Connecticut, who studies ion channels using cryoEM, a type of electron microscopy performed at very low temperatures. CryoEM can't match the resolution of classic methods, but it addresses a nagging question: is this how the protein looks in the membrane or does the lack of the lipid bilayer distort it? Instead of extracting proteins and putting them through a crystallization matrix, cryo-EM involves embedding membrane proteins in artificial liposomes, which are then frozen and can yield thousands of pictures. In recent years, robots from companies including FEI in Hillsboro, Oregon, which sells the Vitrobot, and Gatan in Pleasanton, California, which sells the Cryoplunge, have helped to automate liposome preparation and freezing, preserving researchers' time and samples. With the cryoEM photo library, researchers can apply a technique called single-particle reconstruction to sort through all the two-dimensional images and calculate what kind of three-dimensional proteins could have generated them. Besides falling short of the resolution of most crystal structures, the technique has other drawbacks. For one, it works best on very large, rigid structures, such as ribosomes. Last year, Sigworth used the single-particle technique to solve the structure of a membrane protein \u2014 in this case, the human large-conductance calcium- and voltage-activated potassium channel \u2014 which, with a molecular mass of 0.5 megadaltons, is small for cryoEM 12 . Other important groups of proteins, such as the GPCRs, are flexible and just a fraction of that size, and thus are not amenable to cryo-EM studies. That hasn't dimmed Sigworth's enthusiasm, particularly for large membrane complexes. \u201cX-ray crystallography is very powerful,\u201d he says. \u201cCryoEM has the same kind of potential, it's just 30 years behind. That makes it really fun to be in because new methods are being invented every day.\u201d  Perhaps the most surprising technique to be applied to membrane structural biology is mass spectrometry. Proteins analysed by mass spectrometry are usually broken down and studied as fragments, but Carol Robinson, a chemist at the University of Oxford, UK, applies the technique to large protein complexes, under controlled conditions that cause subunits to separate from the main complex. In 2008, she and her colleagues showed that the technique could be used to study a membrane-protein transporter complex called BtuC 2 D 2  (ref.  13 ), which imports vitamin B12 into the cell. Since then, Robinson has applied mass spectrometry to four more membrane transport complexes, each with differing subunits. She is now applying the technique to larger complexes, with up to 20 subunits.  But mass spectrometry, cryoEM and X-ray crystallography share a problem: they cannot show the dynamics of protein movement. \u201cDrugs probably don't work by stabilizing a single state but an ensemble of states, and we need to understand what those are,\u201d says Kobilka. To gain this understanding, many researchers are turning to technologies borrowed, with a lot of tweaking, from studies on soluble proteins. Stevens subjects membrane proteins to a short burst of deuterium, then fragments the proteins and uses mass spectrometry to identify which peptides are most heavily deuterated, and thus are most mobile in solution. Kobilka is using fluorescence quenching to try to learn how far apart bits of the protein are in various drug-induced conformations. Researchers are also resorting to NMR, in solid state or in solution. The advantage of solid-state NMR is that it can be performed with proteins of any size, in conditions very similar to those of the cell membrane. However, solid-state NMR is technically challenging and expensive. It is often carried out at cold temperatures and so may not show how proteins move in living cells. Solution NMR is more commonly used, but it can follow only small proteins or parts of proteins. These proteins must be encased in protective groups called micelles, which add to the protein's weight and dampen the NMR signal. It does, however, offer the ability to look at different parts of the protein at once and show how they move in response to drugs. \u201cNMR can really show us the dynamics. We're beginning to appreciate the value of that kind of study,\u201d says Kobilka. \u201cIt was such a difficult task for so many years that many people were unwilling to undertake it. It isn't such a high-risk prospect any more.\u201d In fact, it's possible that the study of membrane-protein movement could follow a similar course to the one that membrane-protein structures are currently on: a series of tiny steps from impossible-to-get to essential-to-have. Reprints and Permissions"},
{"file_id": "463979a", "url": "https://www.nature.com/articles/463979a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "A big limitation of molecular imaging in whole animals is how hard it is to measure multiple signals, says Sanjiv Sam Gambhir, who directs the Molecular Imaging Program at Stanford in California. \u201cUnlike our colleagues who remove tissues or blood samples from animals and can analyse many, many things, we're very limited in how many signals we can get simultaneously,\u201d he says. \u201cIn this field, it's not so much about the instruments but about the abilities of the imaging agents.\u201d  These agents are not always quick to develop, says Michael Olive, vice-president of science and technology at LI-COR Biosciences in Lincoln, Nebraska, which sells optical instruments and agents. It's not simply a matter of attaching a fluorophore to a ligand, he says. Before trying a new probe in an animal, for example, LI-COR uses cell lines to see whether increasing the concentration of the unlabelled ligand displaces the labelled one; if not, the label is not binding specifically and can't be used for further studies. If so, the company does more tests, then an extensive autopsy to look for any traces of the label. Without this precaution, says Olive, unanticipated labelling could wreck an experiment. \u201cIt probably takes as much as 4\u20136 months to have confidence that the marker is really doing what you think it's doing.\u201d A problem for the optical techniques used to detect these probes is that most signals cannot travel very far through tissue. Some optical techniques are being used clinically, but they are restricted to organs such as the breast, bladder or stomach, where a detection device can be placed in or close to the site. By contrast, labels developed for PET or MRI for whole-body studies in patients can be readily used in animals. \u201cMost of the optical technologies are never going to be translated to humans,\u201d says Michael Welch at Washington University in Saint Louis, who uses PET to study glucose metabolism in mouse models of diabetes. Gambhir is doing his part to develop new labels. He began his career in imaging at the University of California, Los Angeles, as a teenage assistant to Michael Phelps, the co-inventor of PET. This year, he mutated luciferase to emit signals at a longer wavelength, allowing it to penetrate farther through tissues. He has also created imaging agents that produce light when acted on by particular cues, photoacoustic agents that convert light signals to sound, as well as nanoparticles that enhance Raman peaks \u2014 characteristic shifts in wavelength frequency \u2014 and so may allow detection of as many as ten spectral signals within an animal. Current versions of the particles are too large to enter cells, but Gambhir says that they can be made smaller. Nevertheless, he says, large size can be an advantage. \u201cDelivery is harder because they are bigger, but you also get more signal because they are bigger.\u201d Several groups are working on imaging agents that are activated by one kind of energy but emit another. Fluorescent probes, for example, must be activated by light to emit light. That creates a background signal that limits the resolution of the resulting image. Optimized dyes and software can minimize the problem, but start-up nanobiotechnology firm Zymera in San Jose, California, is working on quantum dots that can bypass it. These dots are activated by high-energy, short-wavelength bioluminescent light produced within the body by luciferase coating the nanoparticle, but, rather than emitting a similar wavelength, they emit the near-infrared light best able to penetrate tissues. The dots could be used in machines optimized for fluorescence. But getting a high-resolution image from light coming from deep within an animal, even a small one, is still a problem, says Vasilis Ntziachristos, who directs the molecular imaging department at the Technical University of Munich in Germany. Not only are many photons absorbed before they reach the charge-coupled devices that detect them, the way that tissues scatter light makes lost data extremely difficult to reconstruct. Ntziachristos is developing photoacoustic agents that create sound signals when they absorb light, and so allow signals to travel farther through tissues with less scattering. He has shown that organic dyes and fluorescent proteins can be detected with ultrasound and used for whole-body three-dimensional imaging of animals such as fruitflies, zebrafish and mice. The resolution is around 10\u2013100 micrometres, potentially orders of magnitude better than that achievable by light, he says. To create the signal, lasers transmit nanosecond pulses of light onto a chromophore, causing it to heat up and cool down. The resulting contraction and expansion generates a sound wave that can not only be detected but also traced to a particular depth. \u201cWhen taking a photograph of an animal, what you see is the light that has been reflected or scattered through the surface,\u201d says Ntziachristos. That makes for images with poor resolution, he explains. \u201cPhotoacoustic tomography gets the resolution back.\u201d But detecting sound rather than light is not a simple switch. Figuring out where to place the sound detectors is more complicated than setting up a home stereo system. Then there's the problem of a coupling medium, says Ntziachristos. His acoustics systems don't work as well in air, for instance. \u201cBut more importantly,\u201d he says, \u201cyou need mathematics.\u201d The data have to be processed, and variations induced by animal tissue and hardware must be accounted for. \u201cWithout the reconstruction software, you cannot produce volumetric images,\u201d he says. \n               M.B. \n             Reprints and Permissions"},
{"file_id": "463981a", "url": "https://www.nature.com/articles/463981a", "year": 2010, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"}
]