[
{"file_id": "456310a", "url": "https://www.nature.com/articles/456310a", "year": 2008, "authors": [{"name": "Henry Nicholls"}], "parsed_as_year": "2006_or_before", "body": "Evolution assumes that extinction is forever. Maybe not. Henry Nicholls asks what it would take to bring the woolly mammoth back from the dead. In 1990 the late Michael Crichton gave the idea of reviving extinct species a slickly plausible and enormously entertaining workout in his novel  Jurassic Park . At that time the longest genome that had ever been sequenced was that of a virus. The best part of 20 years on, hundreds of animal genome sequences have been published. This week, for the first time, the genome of something undoubtedly charismatic and definitively extinct joins the list: the mammoth ( Mammuthus primigenius ) 1 . If you want to bring a species back to life, the mammoth would be almost as dramatic as a dinosaur. And unlike  Tyrannosaurus rex , the mammoth has close living relatives to lend a hand. It is a fair bet that a complete genome and closely related species would make it easier to pull a Crichton on a mammoth than on a dinosaur. But easier is far from easy. To put flesh on the bones of the draft sequence \u2014 to go from a genome to a living, breathing beast \u2014 would require you to master, at the very least, the following steps: defining exactly the sequence or sequences you want for your creatures; synthesizing a full set of chromosomes from these sequences; engulfing them in a nuclear envelope; transferring that nucleus into an egg that would support it; and getting that egg into a womb that would carry it to term. None of those steps is currently possible. From getting a definitive sequence to harvesting eggs from an elephant there are all-but-insurmountable obstacles at every stage, and no evidence that anyone is going to work very hard to solve them. But do any of them actually make the dream impossible?  \n                The sequence  \n              The first stop in this mammoth challenge is to obtain a sequence good enough for us to contemplate using it as the basis for a living being. The sequencing of long-dead DNA such as that of mammoths uses fragments at various levels of degradation. To detect and correct the base-pair changes that can occur after death and to avoid the inevitable errors involved in assembling millions of these tiny fragments into a coherent sequence, it is necessary to sequence much more than a single genome's worth of DNA. \"If we want fewer than 1 error in 10,000 base pairs \u2014 a reasonable quality genome \u2014 we would need to sample in the order of 12-fold coverage,\" says Svante P\u00e4\u00e4bo, director of the genetics department at the Max Planck Institute for Evolutionary Anthropology in Leipzig, Germany, who has worked on Neanderthal genomes 2 . The genome published today has roughly 0.7-fold coverage. 'Reasonable quality' for science does not mean the sort of genome you would want to live with: in a human genome that error rate would mean 300,000 mutations. Coverage can be improved as long as there's the money to do it, but old samples offer particular challenges: a lot of contamination by bacterial, fungal and other species' DNA. Thirty-five-fold coverage, which P\u00e4\u00e4bo says is as good as it gets, would be \"extremely costly and extremely time-consuming\", according to Eske Willerslev, head of the Ancient DNA and Evolution Group at the University of Copenhagen. Ever-cheaper sequencing, however, and the possibility of better preserved and prepared samples, mean that those expenses of cost and time will shrink. Willerslev sees nothing to stop researchers from producing a mammoth genome as good as any genome today at some point in the future. Whether such a genome would be good enough for a living being remains a somewhat open question \u2014 but with time and effort, it's plausible that a sufficiently error-free genome can be arrived at. A sequence on its own, though, is not enough: researchers will need to work out exactly how it divides up into chromosomes. The obvious solution would be to tot up the number of chromosomes in an intact mammoth cell and sift through the genomic data looking for their beginnings and endings. But even the very best mammoth material falls short of this kind of preservation (see  'You need to do more than thaw' ). \"We have no idea \u2014 yet \u2014 how many chromosomes mammoths had,\" says Hendrik Poinar, a geneticist at McMaster University in Ontario, Canada. Kerstin Lindblad-Toh, co-director of the genome sequencing and analysis programme at the Broad Institute in Cambridge, Massachusetts, says that the institute will release a sequence of the African elephant ( Loxodonta africana ) to seven-fold coverage some time in 2009. When they do, the mammoth geneticists will be all over it (see  'Let's fake a mammoth' ). But it will take an immense amount of work to identify and locate all the chromosome changes, gene deletions, duplications and rearrangements that mammoths will have acquired since they diverged from their African ancestors 7.6 million years ago. A sequence for the Indian elephant ( Elephas maximus indicus ), which is more closely related to the mammoth, would be of further help. One chromosome offers particular problems. In mammals the Y chromosome is typically very small and hard to sort out, in part because it is remarkably repetitive. Researchers have sidestepped the issue in the elephant genome by sequencing a female. \"The X chromosome is hard enough to assemble and the Y chromosome is the hardest chromosome out there,\" says Lindblad-Toh. The dinosaurs in  Jurassic Park  were designed to all be female, to avoid unwanted breeding; cloned mammoths might all be females, too, at least for the first generation, just because it would be easier. There are other repetitive regions that will be hard to sequence with confidence, most notably the centromeres, which help chromosomes to get where they are meant to go in cell division. It is almost impossible to work out the centromeres' exact sequence, says Bill Earnshaw of the Wellcome Trust Centre for Cell Biology at the University of Edinburgh, UK. \"You just get hopelessly lost,\" he says. \"It's like being in a forest where all the trees look identical.\" But this need not be a sticking point, as artifice can make do for accuracy. Just this year Earnshaw and his colleagues created a human artificial chromosome that contained a synthetic but fully functional centromere 3 . In principle, that could work for a synthetic mammoth chromosome, says Earnshaw. Replacements for the shorter repetitive sequences at the end of chromosomes, called telomeres, are also doable. And although the sequences will need specific sites at which chromosome replication can start, too, Earnshaw says that \"any long enough strand of DNA will have sequences that can function as origins of replication\". Finally, there is the question of genetic variation. Most published mammalian genomes \u2014 the mammoth draft included \u2014 provide only a single version of all the genes and other sequences in the genome. But mammals have two versions of each gene \u2014 one from their mother and one from their father. Building a mammoth with chromosome pairs in which the two chromosomes were identical would be a recipe for trouble, as the effects of any bad gene would be felt to their fullest. Identifying different versions of genes would add yet more to the sequencers' to-do list, but it would be crucial to success.  \n                DNA synthesis \n              With the genome sequenced in painstaking detail, glitches corrected, chromosomes identified, key repeat sequences written in appropriately and genetic variation introduced, it's time to turn towards the DNA synthesis itself. The largest totally synthetic genome produced so far is that of the bacterium  Mycoplasma genitalium 4 . This contained 582,970 base pairs; the mammoth weighs in at 4.7 billion, half as many again as are found in the human genome. Assuming mammoths turn out to have the same number of chromosomes as elephants, the task would be broken down into making 56 separate chromosomes, each an average of some 160 million base pairs long. Short strings of bases that are made in the test tube can be assembled into respectable double-stranded stretches of DNA about 8,000 base pairs long without too much error, says Ralph Baric, a microbiologist at the Carolina Vaccine Institute in Chapel Hill, North Carolina; a range of companies will synthesise such sequences for less than a dollar a base pair, and the reagents cost much less. But as neighbouring stretches are joined together  in vitro , the DNA molecules become increasingly unstable. The team that put together the  M. genitalium  genome at the J. Craig Venter Institute in Rockville, Maryland, dealt with this by inserting the unstable chunks of DNA into 'bacterial artificial chromosomes'. The various components could then be stitched together in the relatively welcoming environment of  Escherichia coli . For the last steps, the researchers took the largest pieces assembled in  E. coli  and inserted them into yeast artificial chromosomes, which are larger. These were then recombined in living yeast cells to produce constructs that had the entire genome in them. This approach is impressive in terms of speed and cost, says Drew Endy of the Department of Bioengineering at Stanford University in California. But it's unlikely to be scaled up to accommodate mammoth-sized chromosomes in any straightforward way. After all, he says, at just over 12 million base pairs, the entire yeast genome is much smaller than a medium-sized elephant chromosome: \"I would wonder if yeast could handle so much exogenous DNA.\" This concern is echoed by Baric: \"The bigger it is, the quicker there are going to be nicks or breaks, allowing for degradation or deletions in essential genes.\" Even if it became possible to synthesize a stable mammoth chromosome in sufficient quantities, this would then have to be repeated for all the other chromosomes. \"I don't think we're going to be seeing any mammoths any time soon,\" concludes Baric. But it will not be long before synthetic biologists develop a certain confidence in synthesizing microbial genomes from scratch, he says. The technologies that are used to do so will, he predicts, give a good indication of whether it might one day be possible to reconstruct the genome of a large mammal such as a mammoth. It is worth remembering that genome synthesis is further developed today, in terms of the maximum lengths achieved, than genome sequencing was when Crichton wrote  Jurassic Park.  And look how sequencing has progressed since then.  \n                Wrapping it up \n              Once the chromosomes have been synthesized, they need to be put into a nucleus. Cells wrap chromosomes back up into nuclei whenever they divide, so this part of the process has been fairly well studied. In the 1980s, researchers discovered that naked DNA added to extracts from the eggs of frogs quickly becomes wrapped up in proteins that condense it into chromatin; then membrane fragments bind and fuse to form a functional nuclear envelope around the chromatin. \"Such artificial nuclei are capable of DNA replication and some DNA transcription,\" says Douglass Forbes, professor of cell and developmental biology at the University of California, San Diego. Forbes thinks that anyone trying to make a mammoth nucleus in the foreseeable future would be best advised to stick with frog extracts, rather than use fragments of the nucleus from some more similar creature such as an elephant. \"Mammalian eggs, which are fertilized internally, might have much less ability to assemble nuclei,\" she says. When later transferred into the cytoplasm of an elephant cell, elephant nuclear proteins might complement or replace the frog-specific counterparts used to make the mammoth pseudo-nucleus, she says, giving it a more properly mammalian look and feel. It would also be a challenge to ensure that the nuclear membrane engulfs all the mammoth chromosomes. \"You will somehow need to keep them together when you inject them,\" says Eric Schirmer at the Wellcome Trust Centre. If you don't, miniature nuclei may form that contain a random rabble of chromosomes. The way these chromosomes sit with respect to one another might also affect gene expression; how to engineer the correct configuration, nobody knows.  \n                Egg collection \n              It's almost time to contemplate the vagaries of nuclear transfer, but not before you have sourced your elephant eggs, and these are likely to be in pretty short supply. Female elephants ovulate on a 16-week cycle, although they regularly skip five or so years owing to gestation and lactation. Stopping these natural breaks in cycling would be both cruel and unproductive; cow elephants that don't gestate have a strong tendency to develop massive ovarian tumours. When they do ovulate, only one oocyte is released; a litter of little elephants would be a death sentence, and even twins are remarkably rare. On the positive side, though, elephants' infrequent ovulations are preceded by an early warning; uniquely among mammals that have been studied to date, elephant ovulation involves not one but two surges in luteinizing hormone, separated by 18\u201320 days. \"The first hormone peak dissolves the vaginal mucous and forwards just one follicle for development,\" says Thomas Hildebrandt of the Institute for Zoo and Wildlife Research in Berlin, Germany. \"The second peak stimulates ovulation.\" In other creatures it would be quite straightforward to get the follicle in which an egg is developing out of the ovary after this surge of harbinger hormones; you use ultrasound to guide a harvesting implement up the reproductive tract, or perform a laparoscopy, during which the abdominal cavity is inflated to make room for the job to be done surgically. Unfortunately, quirks of elephant biology rule out both these approaches. Whereas the entrance of the vagina is pretty simple to locate in most mammals, elephants have more than a metre of urogenital canal between the outside world and the hymen. This canal is as far as a bull-elephant's penis gets; the hymen remains intact even after copulation, which may be an evolutionary hangover from the elephant's aquatic past. Hildebrandt and his colleagues have developed a way to navigate an instrument up the canal, through the tiny aperture in the hymen that lets sperm in, along the vagina and into the uterus; they use it to perform artificial insemination with sex-selected sperm. But Hildebrandt says that even with such instruments threaded into the womb it would be almost impossible to locate a single mature follicle without some extra guidance, and the ovaries are too deep inside the abdominal cavity for the precise position of the follicle to be revealed by ultrasound. Laparoscopic surgery is also out of the question, as elephants have no pleural space between their body wall and lungs. \"Inflating the abdominal cavity during laparoscopy would compress the lungs and kill the animal,\" says Hildebrandt. There may, however, be an ingenious way out of this bind. Cryobiologists have in the past transplanted tissue from the ovaries of one animal into the ovaries of another. The procedure has worked even if the tissue had been frozen and thawed out between leaving the donor and being grafted into the recipient. Not only has this work led to the treatment of infertility after chemotherapy, it has also raised the possibility that ovarian tissue from endangered species could be transplanted into laboratory animals, making them a source of eggs-on-demand. For Hildebrandt, the only realistic way of getting a steady source of elephant eggs would be to collect tissue from the ovary of a recently deceased elephant and graft slivers of it into a laboratory mouse or rat with a suppressed immune system that won't reject them. This has in fact already been done. In the 1990s, frozen samples of ovarian tissue from African elephants culled in South Africa's Kruger National Park were thawed and transplanted into mice, and afterwards researchers saw what seemed to be mature follicles develop 5 . \"But we didn't have enough material to assess whether those eggs were competent,\" recalls John Critser, now professor of comparative medicine at the University of Missouri in Columbia, who led the study. Although the elephant work has gone no further, researchers have managed to produce live and apparently healthy offspring from eggs that have come from tissue transplanted into another species. Such successes, though, have been with closely related species \u2014 rats acting as hosts for mouse tissue 6 , for example. It's harder to see how elephant eggs would mature successfully in a mouse, not least because the mouse's oestrus cycle lasts just 4\u20136 days. It would probably be necessary to remove the mouse's pituitary gland and subject it to a hormonal cycle that approximates an elephant's with hormone treatments, says Critser. \"It's not something that you could use easily right now for the production of oocytes. But that's not to say that it couldn't be developed.\"  \n                Nuclear transfer \n             With a ready supply of eggs, it should not take long to perfect techniques for removing the native elephant nucleus to make space for the synthetic mammoth nucleus. But that might not be all the preparation needed for the nuclear transfer. The mitochondria \u2014 organelles that provide cells with energy through respiration \u2014 have their own genomes, specific to each species. Hybrids with a nucleus from one species and mitochondria from another can be viable \u2014 an embryo with sheep mitochondria but nuclear DNA from a mouflon developed into an animal, called Ombretta, in 2001 (ref.  7 ) \u2014 but there are risks of incompatibility. The close evolutionary relationship between elephants and mammoths reduces the chances of incompatibility, but Stefan Hiendleder, professor of animal science at the University of Adelaide in Australia, cautions that there would still be risks: human cells that have had their mitochondria replaced with those from other primates have failed to show proper respiration 8 . This means that researchers might need to rustle up some synthetic mammoth mitochondria and insert them into the elephant cytoplasm. With the mammoth mitochondrial genome already well sequenced 9 , this should be relatively simple. And all the elephant mitochondria would have to be cleaned out, in case they hybridize with the newcomers, warns Hiendleder. Nuclear transfer remains a fickle and inefficient way of producing new mammals even without such concerns. Only a few of the transfers result in embryos and not all of those embryos manage to establish a placenta. Of those that do, many abort spontaneously and the few successful live births frequently have developmental abnormalities. But it is reasonable to hope that things may get better, says Xiuchun Tian of the Center for Regenerative Biology and Department of Animal Science at the University of Connecticut in Storrs. The most likely explanation for the inefficiency of nuclear transfer is that the genes are not being expressed in a manner appropriate for embryonic development. This, says Tian, is probably down to errant 'epigenetic' signals \u2014 inherited patterns of DNA methylation, histone modification, microRNA presence and chromatin structure, all of which can have a crucial effect on gene expression[10]. The recent discovery that the nuclei of normal body cells can be induced into an embryonic-like state without the need for nuclear transfer will help researchers to understand the effect of these epigenetic signals and how to manipulate them, she says. Better still, there may be an alternative way to dress up a synthetic nucleus in suitably epigenetic garb. Even if an early embryo is doomed not to go all the way to term, it can still be used as a source of stem cells. These could then be introduced into normal elephant embryos to create chimaeras \u2014 in which some cells are mammoth and some elephant. Such chimaeras may stand a better chance of developing to term, and although they wouldn't be mammoths, they would be a way by which mammoths might then be made. If enough chimaeras are created in this way, some should end up with mammoth cells in their ovaries or testes, giving you elephants that produce mammoth eggs or sperm. 'Germ-line chimaeras' of this type have already been created in several species. In 2004, Japanese scientists[11] made salmon with trout cells in the testes that produced sperm capable of fertilizing trout eggs and producing bona fide baby trout. Gametes from germline chimaeras would be much more likely to be properly imprinted, says Tian \u2014 indeed, once you have a supply of mammoth eggs and mammoth sperm you might well consider the project close to bearing fruit.  \n                Embryo transfer \n              With a fertilized mammoth egg \u2014 either made through direct nuclear transfer or through the  in vitro  fertilization of mammoth eggs with mammoth sperm from chimaeras \u2014 the mammoth challenge comes to its final stage. Although the inaccessibility of elephant eggs means that nobody has ever performed elephant embryo transfer, Hildebrandt has thought about what it would take. First, he recommends inserting an arm up the urogenital canal to inject some sperm-free elephant ejaculate in the direction of the hymen on the day that hormones reveal the elephant is ovulating. \"We think the sperm-free component is needed to prepare the female's uterus to receive an embryo,\" he says. After that, the transfer of the cloned mammoth embryo into the uterus, a total distance of some 2.5 metres, should be possible using the same apparatus used for artificial insemination, says Hildebrandt. \"We are coming quite close to the oviduct, which would be the place to put a cloned embryo back into an elephant.\" An embryo at the four-cell stage would need to be transferred about two days about ovulation, he says. At that point, the last concern becomes whether a mammoth fetus would be suited to the uterus of its surrogate mother. Evidence from preserved mammoths suggests that size, at least, should not be a problem. An Indian elephant calf can weigh around 120 kilograms at term and stands around 1 metre tall. Dima \u2014 a famous mammoth calf unearthed in northeastern Siberia in 1977 \u2014 is estimated to have been about the same mass and height when he died aged 7\u20138 months. It's a similar story for Lyuba, a calf discovered in Russia's Yamal Peninsula last year. Although Lyuba's mass when she died has not yet been definitively ascertained, initial reports suggest she was only 90 centimetres tall and 4 months old, says Daniel Fisher, professor of ecology and evolutionary biology at the University of Michigan in Ann Arbor and one of an international team poring over her spectacularly preserved remains. Most evidence indicates that newborn woolly mammoths were about the same size as newborn elephants, he says. \"They could even have been smaller.\" Mercifully, we probably need not concern ourselves with how to incubate a preterm mammoth fetus.  \n                Birth and after \n             A single artificial mammoth would be a freak, not a species; once she was born others \u2014 including, ideally, males \u2014 would have to follow. Their genomes would have to be tweaked to ensure a certain diversity. A place for them to live would have to be found, as would an ecosystem into which they could integrate. It would be a huge undertaking \u2014 just as synthesizing mammoth chromosomes and reprogramming them into embryo-friendly nuclei would be. Perhaps the whole idea will remain too strange, too expensive, too impractical, even too unappealing for anyone to take seriously. But the fact that just 15 years ago cloning mammals was confidently ruled out by many as being impractical should give people pause before saying any such thing is impossible. On Darwin's 200th birthday in 2009, reoriginating extinct animal species will still be a fantasy. By 2059, who knows what may have returned, rebooted, to walk the Earth? Henry Nicholls is a science writer who lives in Greenwich, England. His most recent book is    Lonesome George .  See also  pages 281 ,  295 ,  330  and  387 , and  online \n                     Darwin 200 news special \n                   \n                     Mammoth site at Hot Springs, South Dakota \n                   \n                     Michael Crichton \n                   \n                     Svante P\u00e4\u00e4bo's research \n                   \n                     The Elephant's Guide to Sex \n                   \n                     Henry Nicholls \n                   Reprints and Permissions"},
{"file_id": "456436a", "url": "https://www.nature.com/articles/456436a", "year": 2008, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "A new generation of lithium-ion batteries, coupled with rising oil prices and the need to address climate change, has sparked a global race to electrify transportation. Jeff Tollefson investigates. \"We have had a massive shift in one of the biggest industries in the world,\" says Stephan Dolezalek, who leads the CleanTech group at the venture-capital firm VantagePoint Venture Partners in San Bruno, California. Dolezalek has been watching the global automobile sector embrace the idea of plug-in electric cars: \"In three years we've gone from thinking 'it can't be done' to not only 'it can be done' but 'we are all going to do it.'\" The shift is partly a story of technological innovation, which has produced rechargeable batteries that pack enough power to propel some of the basic passenger vehicles currently being designed further than 200 kilometres. Billions of dollars have poured into start-up companies that promise new batteries, and billions more have poured into fledgling electric-car manufacturers eager to take on the global automotive giants \u2014 every one of which is also developing electric vehicles. The shift is also a story of oil supplies, national security and global warming. Record-high oil prices have pushed consumers towards fuel-efficient vehicles and prompted many governments to consider electric transport as a way to escape their dependence on imported petroleum and to address climate change. Money currently spent abroad could instead be spent on domestic power generation from wind, solar and other low-carbon energy sources. And the shift is a story of a shared vision: developing the technology that would entice all drivers to plug in rather than fill up. Millions of battery-powered cars plugged into an increasingly green electric grid would not only save drivers money and reduce greenhouse-gas emissions, it would also provide the grid with a distributed, high-capacity storage system for electricity. Such a system would help to accommodate the variable and unpredictable nature of renewable electricity sources. And further out, it could allow power companies to store energy generated during times of low demand, then draw it back again to meet peak demand. The end result could be more a stable and efficient grid that might even lower home electricity bills. Getting there won't be easy. All these hopes hinge on battery technology that is only just emerging from the lab. A suite of technical challenges remains to be overcome, and it is not yet clear how much further the technology can be pushed. At the same time, the manufacturers who are arguably best able to bring about these changes \u2014 the global automotive giants \u2014 have been hammered by an energy crisis followed by an epic financial meltdown. None of them has abandoned the effort yet, in large part because they all believe that, despite the current lull, oil prices have nowhere to go but up. Moreover, batteries have leapt ahead of expensive hydrogen fuel cells as the technology of choice for getting beyond oil, at least for now. But the field is wide open in terms of bringing them to market. Dolezalek believes that major car companies might well perish in the face of versatile young upstarts, and he isn't alone. The automobile industry secured a place in this autumn's first round of economic bailouts from the US government with US$25 billion in loan guarantees for retooling its plants, and it is already seeking more. That has people such as Andrew Grove, former chairman of Intel, who has become a leading proponent of electric transportation, talking about the 'valley of death' that often accompanies a massive technological transformation. Grove says that car manufacturers have already begun their march through the valley, knowing that many won't make it through to the other side. \"The only time people make these moves [through the valley] is when things are rough, but they can't afford to make them when times are rough,\" Grove says. And that means that governments might have to step in. \"I just hope that it's going to be done in such a way that the government says, 'I'll give you some water and food to get through the valley of death, but don't turn back.'\"  \n                Building a better battery \n              Pioneers have turned back before, most notably General Motors. In 1996, the US company released the EV-1, the first all-electric car from a major manufacturer. The vehicle was expensive, rolled out in response to a California mandate, which was later rescinded, for 2% of all cars sold in the state to have zero-emissions by 1998. But its fate was ultimately sealed by one thing: its battery. \n               boxed-text \n             Building batteries has been an exercise in chemical compromise for more than two centuries. The idea is simple: chemical bonds can be used to trap ions in one electrode. When a battery is hooked up to a circuit, the ions flow through a separator to a second electrode; as the ions flow, they release electrons, generating an electric current. In rechargeable batteries, the chemical reaction can be reversed to store energy (see graphic, right). But the reality is complex: although scientists have produced numerous potential battery chemistries (see  _Nature_ 451, 652\u2013657; 2008 ), none of them performs well on all the crucial factors of cost, safety, durability, power and sheer capacity. The first-generation EV-1 deployed a lead-acid battery, still the technology of choice for conventional vehicles. Lead-acid batteries are safe, cheap, long-lived and reliable, but they are also big and heavy. They could push the car for about 150 kilometres per charge. A second-generation vehicle released in 1999 featured a nickel metal hydride battery, and travelled 50% farther on a charge, but General Motors cancelled them after the first year, saying that it could not sell enough to make them profitable. It was a decision that General Motors would come to regret. As it turned back to large and profitable vehicles such as the Hummer, its up-and-coming Japanese rival Toyota was digging into the new technology, using the same battery that General Motors had abandoned to produce hybrid cars that combined a standard combustion engine with an electric motor. Toyota has gone on to set the standard for hybrids: its third-generation Prius has been immensely popular, proving that consumers will adopt advanced battery technology in automobiles if it is done well. The Prius fortified Toyota's reputation, and helped it to surpass General Motors last year to become the largest automobile manufacturer in the world. But nickel metal hydride batteries can be developed only so far. These batteries pack more power than standard lead-acid ones but can be permanently damaged if allowed to discharge too far. To maintain an adequate safety margin, Toyota limited the Prius to using about 20% of its battery charge during normal operation. But although not using 80% of the capacity is acceptable if the battery is simply supplementing a petrol engine, it is a luxury that fully electric cars can't afford. Electric cars need all the charge they can get, and that means new chemistries. Lithium-ion batteries, which are compact and have a high capacity, are a natural place to start. Sony paved the way with the lithium cobalt oxide battery, which made its mass-market debut in a 1991 version of the firm's HandyCam video camera, and is now widely used in consumer electronics. Lithium is a light metal, and the lithium cobalt oxide lattice structure allows plenty of space for the give and take of ions. But scaling this chemistry up for vehicles is problematic. Cobalt is expensive and toxic, and the batteries have been known to show 'thermal runaway', battery lingo for fires or explosions. \"It has affected a tiny, tiny fraction of all of the batteries sold, but nonetheless, it's pretty freaky to think about a big fire in one of the vehicles,\" says Jeff Dahn, who works on advanced battery technology at Dalhousie University in Halifax, Canada. \"Safety really needs to be the focus for the research community.\" Many of the lithium batteries under development for vehicles replace cobalt oxides with manganese oxides and iron phosphates. Both are safer, but they do have their own problems, not least of which is a lower storage capacity for their size. Another challenge has been dealing with the physical expansion and contraction of the electrode material as the lithium ions flow back and forth during charge and discharge, which can lead to fractures. Researchers at multiple institutions have addressed the issue by adding carbon and other substances to the electrode material. They are also probing other chemistries \u2014 often at the nanoscale \u2014 based on silicon, fluorides and oxygen, which have a greater capacity. Others are looking at equipping the battery pack with capacitors, which can rapidly store and discharge electricity. Even in their current state, however, lithium-ion batteries are performing well enough to keep car manufacturers interested. Last year, General Motors inaugurated the race for mass-market electric vehicles when it announced plans to market its plug-in hybrid, the Chevrolet Volt.  \n                A break with the past \n              The Volt, now scheduled for a 2010 roll-out, is a radical shift in design. Hybrids such as the Prius are powered by petrol, and use a battery simply to improve fuel efficiency. The Volt hybrid will be the reverse: an electric car that uses petrol to extend its range. Only when the charge dies will a small petrol motor kick in to charge the battery, which then continues to power the vehicle. The goal is for Volt owners to plug in at night and then drive more than 60 kilometres a day on a single charge \u2014 before burning a single drop of petrol. Given that as many as 80% of US drivers commute less than that on an average day, such vehicles could eliminate a sizeable chunk of the nation's oil consumption. The Volt initiative could open the door to a new kind of transportation system \u2014 if the company can pull it off, both on time and at a cost that will tempt consumers. Many observers have their doubts. \"They are fundamentally redefining what a car is, but can they do it? I don't know,\" says Don Hillebrand, who heads the Center for Transportation Research at Argonne National Laboratory in Illinois. \"When the first generation of anything comes out, to a certain extent car manufacturers are rolling the dice, and this is the biggest roll of the dice anybody has ever made.\" Some say it is a long shot. With sales plummeting in the midst of a deepening recession, the company is facing possible bankruptcy, and has joined with the other major US car manufacturers in seeking an additional bailout from the government. But through it all, General Motors has continued to sink everything it can spare into the Volt, viewing it as a key technology that would allow the company to leapfrog its competitors. Toyota is taking a more measured approach with its plug-in hybrid, which is expected to roll out with a lithium-ion battery in 2009. John Hanson, a spokesman based at Toyota's US headquarters in Torrance, California, talks about managing customer expectations: the company is promising only that the vehicle will go \"at least\" 16 kilometres on an electric charge. After that, it will blend petrol and electric power in much the same way as the current Prius. That would leave General Motors in pole position, at least in terms of the electric range it is promising. But will the Volt succeed? The answer to that question depends on consumers. What will they want several years from now? And how much will they be willing to pay? General Motors expects to lose money in the beginning and has not yet announced a price for the vehicle, but the continued viability of the firm could depend on how fast it can sell the new cars and at what price. The company is banking on tax credits, enacted this year by Congress, to encourage people to buy plug-in hybrids, and high petrol prices would help as well. But the firm's chief economist Mustafa Mohatarem says that he can't help but wonder whether consumer demand for electric vehicles has been exaggerated. \"It is critically dependent on the battery,\" he says. \"Until you have a much better handle on the cost of this technology, to talk about demand is in a sense ridiculous.\"  \n                Have we forgotten something? \n              Others look at the market and see a different problem: a lack of batteries. Charles Gassenheimer, chief executive of Ener1 Group, a company in New York that produces lithium batteries, says that car manufacturers have collectively announced some 75 types of electric cars that are supposed to hit the road by 2013. But they have been slow to commit to orders, he says. And without orders, battery manufacturers can't invest the time and money necessary to ramp up production, a bottleneck that could delay the roll-out of new vehicles. Governments seeking to spur the electric-car market must look at battery manufacturing in addition to consumers and car manufacturers, Gassenheimer says. \"There needs to be some government intervention at this phase in the game. Otherwise it's going to be a chicken-and-egg problem that doesn't get solved.\" Gassenheimer also raises concerns that countries such as the United States will simply trade their dependence on Middle Eastern oil for a reliance on Asian batteries. He has a sizeable stake in the outcome, of course, but the issue has political resonance as governments look to spur new green jobs. Experts say that Ener1 and other Western companies have the technology, but Asian companies have a leg-up on the manufacturing side simply because Asia has such a lead in producing lithium-ion batteries for electronics. \"The United States is certainly not being blindsided at this time, but whether or not we really have the resources and critical mass to compete in the long term in automotive batteries is still very much an open question,\" says Yet-Ming Chiang, a materials scientist at the Massachusetts Institute of Technology in Cambridge, and founder of lithium-battery manufacturer A123Systems in Watertown, Massachusetts. \"The same thing goes for Europe.\" Others dismiss concerns about where the batteries are going to be made, citing a crucial difference between electronics and vehicles: electronics are by and large made in Asia, but cars are made in the West, too. Building batteries near automobile plants would not only save money, it would also get around complex international shipping regulations that put lithium-ion batteries in the 'dangerous goods' category. The market \"is driven by where the end product is\", says Khalil Amine, a battery researcher and one of Hillebrand's colleagues at Argonne. \"For electronics, we buy everything from Asia. For transportation, there is plenty of production here.\" General Motors tested lithium batteries from every manufacturer it could find and narrowed the decision down to two companies: A123Systems and LG Chem, a Korean giant that made its name in electronics. Only in late October did the contract reportedly go to LG Chem, which has a stronger base and a longer history on the manufacturing side. LG Chem has already partnered with the Korean car manufacturer Hyundai to supply 7,000\u201310,000 lithium-battery packs for a pair of hybrid vehicles that will begin rolling off the line in 2009. Soonho Ahn, LG Chem's vice-president for battery research and development, says that his company isn't expecting to make money on its automotive batteries for some time but wants to be ready when the market takes off. He notes that the battery market in Asia is in \"equilibrium\" after several years of stiff competition in the electronics sector. \"We have some time to look at the next mega-application, and the next mega-application is the automotive industry,\" he says. \"We're pretty sure that the market is coming.\"  \n                Making connections \n              So what will the market be like when it does come? Plug-in hybrids such as the Volt represent a leap beyond battery-augmented cars that merely make better use of petrol. They also give drivers the freedom to run on electricity for short trips while still making long trips, albeit guzzling gas on the way. But some car manufacturers say that the best path forward would be an all-electric vehicle, which could one day all but eliminate oil consumption in the transportation sector. Getting rid of the petrol motor greatly lessens costs and complexity and opens up space for more battery power. \"In terms of a solution, both from a carbon dioxide point of view and from a technical point of view, the hybrid and the plug-in hybrid do not provide the technical breakthrough that the electric vehicle could provide,\" says Serge Yoccoz, who is in charge of electric vehicles at Renault. \"And from what we've seen, the plug-in hybrid is definitely more expensive [than an electric car would be], even if you take into account the need to develop a charging infrastructure.\" So while researchers search for the technical breakthrough, entrepreneurs are trying to get around the high costs by rethinking the way we market cars, batteries and ultimately energy. \n               boxed-text \n             One such innovator, Better Place of Palo Alto, California, is aiming for nothing short of a wholesale conversion of the transportation sector. The company likens itself to a cell-phone network for all-electric cars: you buy the car from a Better Place partner and then sign up for one of its various user plans. Better Place then provides a network of charging spots \u2014 at home, work and retail outlets \u2014 as well as stations at which used battery packs could be swapped for recharged ones by a robotic arm in a matter of minutes (see graphic, right). But to accomplish all this, Better Place needs a computer system that can track electricity charges wherever they are incurred. It also needs to partner with governments and industry, including the automotive, battery and utility sectors. So far, Better Place has lined up partnerships with an alliance between Nissan and Renault to pursue electric cars, and the company plans to roll out its system in Israel, Denmark, Australia and California, with the first deployments scheduled for 2010. The scheme is ambitious, but Sidney Goodman, head of automotive alliances at Better Place, says that's the only way to do it. \"We don't believe we can do this on a small scale. It's one of these projects where either you do it big or you don't do it\". Better Place is aiming to provide family sedans that have a 160-kilometre range in an effort to attract all drivers, not just city commuters with an environmental bent. Goodman runs through some rough numbers \u2014 assuming that a battery costs US$15,000 (which is likely to be on the high end of the scale, he stresses), an electric vehicle would cost about 6 cents per kilometre to power. That compares with just under 12 cents per kilometre for conventional cars in the United States, and twice that in Europe. A Norwegian company called Th!nk is taking a similar route with its all-electric commuter car, which is due to hit roads in Norway, Denmark and Sweden in coming months. With an initial price tag of about 200,000 Norwegian krone ($30,000), the car will cost about 20% more than the same-sized petrol-powered car and will drive some 180 kilometres on a charge. Customers then pay a monthly lease to cover the cost of electricity and the battery. \"We'll get the costs of our car down to somewhat similar to the cost of a petrol-powered car, and then we'll have a very strong proposition going forwards,\" says Richard Canny, Th!nk's chief executive.  \n                Tapping the matrix \n              Utility firms are eager to cooperate. Although making electric vehicles a reality will require unprecedented cooperation between two industries that have until now had little in common, utilities actually see many more benefits than headaches. The fundamental fact is that most of the charging would take place at night, which creates a new source of revenue at a time when utilities typically have excess capacity. In the end, this should translate into substantial reductions in greenhouse-gas emissions, even in countries such as the United States that get much of their electricity from coal. A plug-in hybrid running on electricity generated entirely from coal is roughly equivalent to a conventional hybrid in terms of emissions, but utilities say that in the early years, electric vehicles will frequently draw power from spare generating capacity that uses cleaner-burning natural gas. Scaled up, millions of batteries \u2014 either in cars or in a future after-market for used batteries \u2014 could provide utilities with a flexible storage system that could soak up renewable power, particularly from wind turbines at night. Assuming that plug-in hybrids will make up 60% of the US automobile market in 2050, electric transport would consume as little as 8% of the nation's electricity, according to a joint modelling study conducted in 2007 by the Electric Power Research Institute \u2014 a non-profit research organization in Palo Alto \u2014 and the Natural Resources Defense Council, an environmental advocacy group based in New York. The resulting report,  The Power to Reduce CO 2  Emission s, predicts that the nation would use 15\u201320% less oil and reduce its greenhouse-gas emissions by 450 million tonnes, which is akin to pulling 82.5 million internal-combustion vehicles off the road. \"Our fundamental conclusion from this study is that the number one driver of benefits is really the number of vehicles,\" says Mark Duvall, programme manager of electric transportation at the Electric Power Research Institute. \"Don't worry about charging them from some perfect grid of the future \u2013 just get the cars out there. They don't have to be perfect.\" Utilities such as Southern California Edison in Rosemead are already thinking about how to integrate cars into the electricity system, allowing them to charge up at work or park in 'smart garages' that coordinate activities between the car and the grid. In the early days, advanced charging equipment would communicate with the utility to time the charging so that everybody's vehicle is fully juiced when it needs to be \u2014 but not necessarily before. That would help ensure that millions of vehicles don't create a sudden surge on the electricity system when people return from work, when they also tend to turn on lights and crank up their appliances. Further out, this process could be reversed, allowing batteries to provide power to the grid when it is needed most \u2014 so long as they are fully charged when it comes to time to drive. Levelling out the daily demand cycles would allow utilities to manage the grid more efficiently, potentially lowering costs to consumers. \"The more cars that come onto the energy system, the better off it is for the energy system,\" says Ed Kjaer, director of electric transportation for Southern California Edison. And Kjaer says that the vehicles will become cleaner over time as utilities expand their renewable electricity offerings. That kind of logic has convinced many researchers that electric cars are a must if the planet is to deal with global warming, even if they ultimately raise the stakes on efforts to produce carbon-free electricity. \"We've got to electrify the transportation system and then clean up the grid,\" says Timothy Lipman, research director at the University of California's Transportation Sustainability Research Center in Berkeley. \"It's the easiest path.\" \n                 See Editorial, \n                 page 421 \n                 See also Correspondence: \n                 Choosing between batteries or biomass to stay on the road \n               \n                     Course corrections \n                   \n                     GM's Volt \n                   \n                     Toyota's plug-in Prius \n                   \n                     Better Place \n                   \n                     Wired profile of Better Place founder Shai Agassi \n                   \n                     Th!nk web site \n                   \n                     EPRI Journal issue on plug-in hybrids \n                   Reprints and Permissions"},
{"file_id": "456696a", "url": "https://www.nature.com/articles/456696a", "year": 2008, "authors": [{"name": "Claire Ainsworth"}], "parsed_as_year": "2006_or_before", "body": "Squash them, pinch them, twist them, pull them \u2014 cells react to physical forces, finds Claire Ainsworth. The lecturer flattened a tangle of sticks and elastic on to the desk. He let go, and the structure pinged back into shape. It was a demonstration of 'tensegrity', a term constructed by the engineer Buckminster Fuller for situations in which push and pull have a 'win\u2013win relationship' with each other. Fuller mashed the word together \u2014 from the components tensional integrity \u2014 to describe the way sculptor Kenneth Snelson used taut wires and stiff poles to make strong yet flexible monuments. Among the students, Donald Ingber could see the ingenious engineering in the sculptures \u2014 but he also saw biology. Ingber was an undergraduate in molecular biophysics at Yale University; the course in three-dimensional design had seemed apt. But what he saw there changed the course of his professional life. At that time, in the late 1970s, researchers were publishing the first scientific papers describing how cells are propped up by an internal scaffolding, or cytoskeleton. \"I immediately thought: 'Oh, so cells must be tensegrity structures',\" Ingber says. On returning to the lab, he eagerly explained this idea to one of the postdocs, who was less than impressed. \"He told me: 'Just never mention that again',\" Ingber recalls. Then, as now, most cell biologists had little time for architecture and engineering. When they want to understand why a cell behaves the way that it does, they try to identify the genes, proteins and signalling molecules that are thought to exert control. But to Ingber there was an obvious gap between the dramatic events that mould a developing embryo and the molecular explanations that were given for them in his developmental-biology class. \"What I saw before my eyes was something that was incredibly physical, mechanical in nature: twisting, bending, folding,\" he says, \"and then I got into cell biology, and it was all chemical.\" Thirty years on, work from Ingber's group and many others has started to convince cell biologists to embrace the missing physics. Their findings are remarkable. Pull a stem cell in one way and it starts developing as a brain cell; stretch it in another, and a bone cell is its more likely fate. Change the mechanical stresses on cancer cells and they can start to behave more like healthy ones. Among this work's implications, few are more important than the consequences for cell therapy and tissue engineering, in which researchers hope to use new cells to repair damaged organs. If these cells encounter the 'wrong' kinds of mechanical stresses, they could conceivably end up doing more harm than good. The discoveries are giving biologists a fresh appreciation of the body's physical nature. Hearts pump, muscles stretch, blood surges, feet pound. And on the microscopic scale, fluids flow and cells jostle with their neighbours. When Ingber, now at Harvard University, talks about his ideas today, he doesn't get quite the same frosty reception that he once did. \"There's no doubt,\" he says, \"in the past five years it has exploded.\"  \n                Popular mechanics \n              Even in the 1970s, the ideas were not entirely new. The importance of mechanical forces was appreciated by embryologists in the 1800s and early 1900s, long before they had signalling proteins and chemical gradients to play with. Swiss biologist Wilhelm His, for example, experimented with metal, clay and rubber to try to mimic events in development, such as how the future brain starts forming as a roll of tissue on the back of a mammalian embryo. \"To think that heredity will build organic beings without mechanical means is a piece of unscientific mysticism,\" he wrote in 1888 (ref.  1 ). The molecular biology revolution of the 1960s onwards pushed much of this aside, as researchers focused on genes and proteins. But the mechanical ideas never fell completely out of favour. Physiologists know that astronauts' bones get thinner when they escape gravity, and that hefting weights inflicts physical damage on muscle cells that stimulates them to grow. But it was widely felt that the role of mechanical stress would be limited to these and other cell types that needed it in order to function normally. Then, in 1978, Judah Folkman and Anne Moscona at Harvard Medical School published one of the first studies to experimentally stretch mammalian cells \u2014 in this case, cells extracted from cow's blood vessels and other tissues. They coated plastic culture dishes with various concentrations of a sticky polymer and grew the cells on top. The stickier the substrate, the flatter the cells stretched out, and the more the cells stretched out, the more they divided. It was considered a landmark paper \u2014 one of the first to show that cell shape influences growth 2 . And it had a \"profound effect\" on Ingber when he read it that year. \"It resonated with my sense that physicality was critical to developmental control,\" he says. Ingber later contacted Folkman and eventually went on to complete a postdoc in his lab. Researchers now know that almost all human cells test the mechanical properties of their microenvironment in the body, and use it to adjust their growth. This is determined by the extracellular matrix, a lattice of proteins and other molecules to which cells in solid tissues anchor themselves like a horde of old canvas tents staked out at a rock festival. Instead of poles and ropes, cells have their internal cytoskeleton. This includes a mesh of fibres made up of actin protein that lines the cell's membrane, plus tough 'actomyosin bundles' in which actin combines with the protein myosin II. The tent pegs in this case are proteins called integrins that span the cell membrane, gripping the actomyosin filaments inside the cell and the extracellular matrix on the outside. In 2006, Ingber's team used a femtosecond laser to cut the actomyosin filaments and found that they immediately retract, revealing that they are under stress just like the tensed wires in Snelson's sculptures 3 . The role of the poles seems to fall to another part of the cytoskeleton, the microtubules, which buckle under severe stress. Bioengineers, meanwhile, have attempted to measure the elasticity of various tissues in the body. The simplest way involves hanging a weight from a hunk of tissue and measuring how much it stretches. More sophisticated methods include atomic force microscopy, which uses a cantilevered tip to lightly prod cells and measure their springiness. Last year, bioengineer Paul Janmey and his colleagues at the University of Pennsylvania, Philadelphia, used atomic force microscopy to show that connective tissue cells called fibroblasts can tune their internal stiffness to match that of the substrate on which they are growing 4 . \"That was actually quite a surprise,\" says Janmey. He thinks that this type of surveillance could help cells respond to change in the tissue around them. Imagine a cell that is normally pegged down on all sides in the skin \u2014 but whose moorings are weakened when the skin is cut. The physical change could be felt just as fast, if not faster, than chemical signals released by the wound. \"The cell already knows that mechanically, something has gone haywire and can respond to that,\" says Janmey. Ingber says that this idea is already being used in hospitals to promote healing, an idea he is working on in collaboration with Dennis Orgill, a plastic surgeon at Harvard's Brigham and Women's Hospital. Orgill seals a sponge-like device over hard-to-heal wounds such as those left by deep surgery, or diabetic foot ulcers. He then attaches this to an oscillating suction pump. The suction stretches the cells so that they divide, form new blood vessels and regenerate skin tissue, thus healing the wound 5  \u2014 the same type of behaviour that Folkman observed in his cultured cells 30 years ago. Ingber says the system works better (and is far cheaper) than artificial skin or synthetic growth factors.  \n                Eschewing growth factors \n              Monitoring the physical environment could also serve a purpose during development, by helping cells to detect where they are, and to migrate, divide or differentiate appropriately. This has required a particular change in thinking for many biologists. The field has long been dominated by the idea that developmental decision-making is directed by the chemical signals inherited from a cell's parent, or received from its neighbours and the environment. That way of thinking leads researchers to encourage stem cells to form heart muscle cells, neurons or other cell types simply by adding cocktails of proteins known as growth factors. In 2006, cell and molecular biophysicist Dennis Discher at the University of Pennsylvania reported he had done away with growth factors and allowed force to trigger a change in cell fate 6 . His team studied mesenchymal stem cells, a kind of cell that normally grows in the soft, fatty bone marrow where it generates bone progenitors, but that is also thought to move to other locations and give rise to a range of cell types including nerve and muscle. They grew the cells on gels made from polyacrylamide and collagen that mimicked the softness of bone marrow. By changing the degree of chemical crosslinking in the polyacrylamide, the team was able to alter the stiffness of the gel to be more like that of different body tissues. On a relatively soft base that resembled the sponginess of brain tissue, the stem cells began to form the precursors of neurons; on stiffer, muscle-like substrates the cells took steps towards forming muscle stem cells; and on still stiffer substrates resembling developing bone they started to become bone-forming cells. The team still had to add growth factors to get the cells to differentiate fully, but by the time the stem cells had become comfortable for three weeks on their soft, medium or hard mattresses, growth factors that would normally get stem cells to switch between one developmental pathway and another had little effect. Findings such as these have important implications for regenerative medicine, Discher says. A number of clinical trials are now under way in which researchers inject mesenchymal stem cells into the hearts of patients who have previously suffered a heart attack, in the hope that the cells will help repair the scarring caused by the attack. By using atomic force microscopy, Discher and his collaborators have found that in rats this kind of scar tissue is stiffer than normal heart muscle, resembling that of developing bone 7 . And last year, Bernd Fleischmann of the University of Bonn, Germany, and his colleagues found that mesenchymal stem cells formed bony spurs after they were injected into the scar tissue of damaged mouse hearts 8 . Although there are no published reports of human patients developing these problems, \"You have to think about the microenvironment that you put cells in,\" Discher says. The new appreciation of cells' mechanical environment is also a complication in the lab. A number of studies have revealed how poorly cell-culture conditions mimic those that a cell would encounter in real tissue. Mammalian cells grown on a typical, flat, glass or plastic tissue culture dish, for example, do not develop the directional 'polarity' that they do in real tissue, but they do when grown in three-dimensional gels 9 . Many groups are now developing sophisticated cell-culture methods that have more life-like mechanics. Melody Swartz, a bioengineer at the Swiss Federal Institute of Technology in Lausanne (EPFL), and her team are studying the effects of 'interstitial flow': the gentle current to which almost all human cells are exposed as the fluid that bathes them seeps into the lymphatic system, the body's drainage network. The team has found that fibroblasts align themselves perpendicular to this flow during wound healing and inflammation. Swartz thinks that interstitial flow distributes chemical signals through tissue, and that cells may be so accustomed to its mechanical presence that they are lost without it. \"They need the functional aspects \u2014 the mechanical environment \u2014 to act normally,\" she says. Her team has recently shown that the lymphatic cells themselves, which are notorious for losing many of their characteristic cell functions in culture, regain their character if she subjects them to flow. \"I don't think mechanobiology is a separate field from biology,\" says Swartz. \"It's something you can either choose to consider or not, but it is always present.\" Not all labs are considering it \u2014 those that have switched to three-dimensional culture systems are still a minority, partly because cells grown that way are harder to study with standard microscopy and other techniques. But if mechanical stress has yet to be fully appreciated by cell biologists, it is becoming hard to ignore among developmental ones. As Ingber observed, nowhere are physical forces more apparent than in the developing embryo where tissues twist, fold and writhe into the beginnings of adult tissues and organs.  \n                Let's twist again \n              In 2003, biophysicist and developmental biologist Emmanuel Farge at the Curie Institute in Paris gently squashed entire fruitfly embryos under a tiny sheet of glass, and found that they switched on a gene called  twist  in nearly all their cells, instead of the particular subsets where it should be expressed[10]. Farge has since shown that the normal compression of tissue that occurs as the embryo changes shape is required to switch on this gene in the nascent gut[11]. First he removed the cells that normally exert pressure on the gut as it grows, and showed that  twist  expression dropped. Then he devised a way to artificially exert a tiny force inside these embryos by injecting magnetic nanoparticles and tugging on them with an electromagnet. The expression of  twist  was restored. Such studies challenge the idea that an embryo's shape and patterning is driven only by a genetic program, suggesting instead that shape and patterning can also drive gene expression. This mechanical control could provide cells with feedback on their changing position in the growing embryo. \"The genome must be aware at key stages of the shape it is in charge of developing,\" says Farge. It also means that genes acting in two physically separate tissues \u2014 such as those that control tissue compression, and those that control  twist  \u2014 can still interact. This kind of action at a distance, Farge speculates, might be involved in an embryo-wide, \"global\" system that coordinates development. Given its importance in influencing cells in embryonic and adult tissues, it should come as no surprise that mechanical environments are now thought to contribute to disease. Even five years ago, says cell biologist Valerie Weaver, at the University of Pennsylvania, \"there was a lot of scepticism\" when she presented her work investigating links between tissue stiffness and cancer. \"I know there was a little bit of snicker.\" But now a whole body of work has shown that manipulating the extracellular matrix in the cancer's microenvironment can switch normally dividing cells into excessively dividing, cancerous ones and vice versa[12]. Weaver and her colleagues investigated how mechanical forces on the outside of the cell could be converted into cancer-promoting signals on the inside \u2014 one of the key questions in the field[13]. Her team used a machine known as an electromechanical indentor, which presses on tissue in culture, to show that cancerous mammary tissue is stiffer than healthy tissue. Then they grew normal mammary cells in gels stiffened to different extents with collagen, so that they resembled either healthy tissue or tumours. Mammary cells grown in soft gels organized themselves into structures characteristic of normal breast tissue, whereas cells grown in stiff 'cancerous' gels did not. They found that the stiff gels pulled more on the membrane-spanning integrins, and this boosted the activity of an integrin-controlled signalling pathway that regulates tension in the cytoskeleton.  \n                Crushing cancer \n              Weaver suggests that many oncogenes \u2014 genes which, when mutated, predispose a cell to becoming cancerous \u2014 can also activate biochemical pathways that increase internal tension. And in unpublished work, Weaver and her colleagues have shown that these changes may be some of the earliest events in cancer. They found that the extracellular matrix surrounding cells that harbour mutated oncogenes becomes stiffer even before those cells form invasive tumours, because the constitution of the extracellular matrix is changing. \"That microenvironment is changing dramatically long before you get a tumour,\" says Weaver. If her ideas are correct, then interfering with the mechanics of a cancer cell might override aberrations in its genes. And that's what Weaver found in her 2005 study: when she added chemicals that block integrin signalling to the cells grown in stiff gels, they grew into more normal-looking mammary-gland tissue. These signalling pathways might make a possible cancer drug target. And Ingber is pursuing the idea that cancer might be reversed by physically manipulating a tumour's microenvironment by, for example, implanting artificial materials that mimic the structural, mechanical and chemical properties of healthy tissue. Ingber is also becoming more and more convinced that cells behave like Snelson's deformable sculptures. When he used a laser to break a single actin fibre, the shape of the entire cell altered to accommodate the change 3 . So, just like other tensegrity structures, the cytoskeleton can transmit a force from one point through the entire assembly. These mechanical signals are transmitted faster than a chemical signal can diffuse across a cell[14], and Ingber thinks that such mechanical changes may physically deform other molecules attached to the cytoskeleton, including many associated with the cell's metabolism. This cascade of events could eventually cause a dramatic change in cell behaviour[15]. Ingber goes even further, suggesting that living creatures are a Russian doll of tensegrities. On the largest scale, muscles tense against bones. Inside the body, the forces in these and other tissues are picked up by integrins and relayed to the cytoskeleton. The cytoskeleton itself is made up of macromolecular structures that are themselves tensegrities at the molecular level. Such a hierarchy of systems, he says, could explain how mechanical signals such as gravity or movement are transmitted from the macro to the micro and nano scales. In the past two years, Ingber may have found the means to test his ideas. In 2007 he received seed money to establish a Harvard Institute for Biologically Inspired Engineering. And in October 2008, philanthropist Hansj\u00f6rg Wyss gave US$125 million towards it. Looking back, Ingber says, \"I was lucky to have my first 'Aha moment' as an undergraduate at a liberal arts college. In the United States, undergraduates can explore any field, and hence know no bounds between biology, chemistry, physics, architecture or art.\" It is a boundlessness he tries to convey to the students he teaches today. Those who visit his office find that it is filled with tensegrity models. He even has a small Snelson sculpture on the window sill, one that the artist gave him. And in the classroom, some Harvard undergraduates are already exploring how to develop expandable lightweight structures based on tensegrity and cell architecture. But these are not designs for beautiful sculptures \u2014 they are water carriers for the developing world.\n \n                 Claire Ainsworth is a freelance writer based in Southampton, UK. \n               \n                     Landmark papers in cell biology \n                   \n                     Cytoskeleton insight \n                   Reprints and Permissions"},
{"file_id": "456304a", "url": "https://www.nature.com/articles/456304a", "year": 2008, "authors": [{"name": "Simon Ings"}], "parsed_as_year": "2006_or_before", "body": "A celebration of one of evolution's crowning glories. \n               The online version of this Gallery Feature is available as a slideshow - click  \n               \n                   here \n                 \n                to see the pictures. \n             \"To suppose that the eye, with all its inimitable contrivances for adjusting the focus to different distances, for admitting different amounts of light, and for the correction of spherical and chromatic aberration, could have been formed by natural selection, seems, I freely confess, absurd in the highest possible degree. Yet reason tells me, that if numerous gradations from a perfect and complex eye to one very imperfect and simple, each grade being useful to its possessor, can be shown to exist; if further, the eye does vary ever so slightly, and the variations be inherited, which is certainly the case; and if any variation or modification in the organ be ever useful to an animal under changing conditions of life, then the difficulty of believing that a perfect and complex eye could be formed by natural selection, though insuperable by our imagination, can hardly be considered real.\"  \u2013 Charles Darwin,  _On the Origin of Species_ Charles Darwin was well aware that the eye \u2014 so obviously, so brilliantly 'designed' \u2014 might represent an impediment to the acceptance of natural selection. In fact, it has come to be seen as one of evolution's crowning glories. Across the animal kingdom, eyes have evolved in different ways, to different purposes, in exuberant diversity. Yet they are all sculpted by the lawfulness of light \u2014 and the imperatives of survival. This panoply of eyes sees the world in different ways, some concerned with colour, others with movement, others with acuity. Yet time and again, unrelated eyes hit upon common solutions to the problem of how to safely gather and focus information from a sunlit world. The eyes of the cuttlefish grow from invaginations of the skin; those of the human grow in part out of the front of the brain; each uses completely different receptors to pick up the light they focus. Yet cuttlefish and people see the world in the same way, through eyes whose similarities outweigh their deep differences. Eyes are largely built from building blocks designed for other things. The lenses of vertebrates use proteins that bacteria developed to deal with stress; the flexible guanine mirrors that make a cat's eye glow in the dark provide gas-proofing for the swim bladders of fish. And the components are put together in a bewildering number of ways. The brittlestar has an entire carapace pitted with optically tuned calcite crystals. The scallop eye uses a curved mirror to focus incoming light; the pram bug  Phronima  elongates each element of its eyes with a natural optical fibre. The dragonfly's compound eyes \u2014 each an array of up to 28,500 individual visual organs \u2014 may look ungainly, but they are extremely resistant to motion blur as it hunts on the wing. In dim conditions, an eye must gather what light it can, regardless of image clarity. By using box mirrors to focus light on a common point, the eyes of shrimp and lobsters enjoy more than 250 times the light-catching power of the human eye. The compound eye is a dead end in design terms; no matter how big you make it, it produces poor images. But it is well adapted to the zigzagging life of a hoverfly. The abrupt changes in the fly's direction of flight, which occur several times a second, help the eyes produce a fuller picture of the world. Human eyes are rare in their concern for producing images; most of the eyes in the animal kingdom are tuned to movement and colour. The band running across the eye of the mantis shrimp contains receptors tuned to 16 different wavelengths (humans typically have three), giving colour vision of extraordinary subtlety and complexity. Nor are two eyes a universal norm. The fish  Anableps anableps  effectively has four eyes in two sockets; each eye has one half for seeing above water and one half for seeing below. Another fish,  Bathylychnops exilis , has two pairs, one set to look up, which is how it sees its prey, and one to look down \u2014 but for what, nobody knows. The better your predators can see, the more need there is for disguise. For some animals, looking inedible is not enough: the unnervingly detailed mock eye on a moth wing is meant to convince predators that something big is looking right back at them. The principal high-resolution eyes of true spiders are as big and as powerful as the eyes of small rodents. Subsidiary eyes are used to spot movement in the periphery, and sometimes, by harnessing the polarization of sunlight, to navigate. \n                     Darwin 200 News Special \n                   \n                     On the Origin of Species \n                   Reprints and Permissions"},
{"file_id": "456441a", "url": "https://www.nature.com/articles/456441a", "year": 2008, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "Argentina's government has pledged to reverse a decades-long scientific brain drain. Rex Dalton reports. In September 2007, with a wind of change already in the air, about 40 people arrived at Argentina's New York consulate for an unusual meeting. Nearly all of them were Argentinian expatriate scientists, now conducting research abroad. The organizer, mathematician Adri\u00e1n Paenza, was a famous football journalist turned Buenos Aires talk-show host. The guest of honour was Cristina Fern\u00e1ndez de Kirchner, the woman almost assured of winning Argentina's presidential election the following month. For two and a half hours, Kirchner held court. She listened as the researchers suggested how to improve science in their homeland, and she peppered them with questions about black holes, applied mathematics and computer science. She had really done her homework, several of the attendees noted. Kirchner wanted to know what would be needed to entice researchers back to Argentina \u2014 and to recapture a level of scientific excellence that some say was last seen in 1947 when Bernardo Houssay became Argentina's first scientific Nobel laureate when he won the prize for physiology and medicine. From the 1960s until the end of the 1980s, Argentina's ruling parties viewed academia as a breeding ground of political dissent. Students and professors were targets for suppression and were among the tens of thousands who 'disappeared'. In the 1990s, when the country began to confront its ugly history, President Carlos Menem had no interest in research and one of his economic ministers, Domingo Cavallo, famously told scientists to go and \"wash the dishes\". During the Argentinian financial crisis that peaked in 2002, any scientists who had not already fled were hit by a peso devaluation that made it near impossible even to attend an international conference. Kirchner wants change. She says that she sees science as key to the nation's economic future, and the New York meeting helped to crystallize her ideas. After winning the election in October 2007, Kirchner set up a Ministry of Science, Technology and Productive Innovation and named Lino Bara\u00f1ao, a cell biologist and formerly head of the nation's main research granting agency, as the country's first science minister 1 . The move delighted researchers. Alberto Kornblihtt, a biochemist at the University of Buenos Aires, recalls the stunned response from his family: \"The president has appointed a biologist to her cabinet? Maybe Argentina is changing.\" Bara\u00f1ao and Kirchner hope to grow the country's investment in science and technology from 0.66% of gross domestic product (GDP) in 2007 to 1% in 2010. This would rival the 0.9% of GDP currently invested by Brazil, one of the biggest-spending Latin American countries. Bara\u00f1ao plans both to reverse the brain drain and to retain scientists. He has promised that in 2009 the government will increase the budget for competitive research grants by 40% over the current year, to around 700 million pesos (US$210 million). More is being pumped into professorial labs and salaries. In 2008, the national research council CONICET increased researchers' salaries by 30% to about $1,000 a month; a far cry from the $450 or so that a full professor made in 1990. Infrastructure expansion, with an initial budget of $50 million, is to start next year.  \n                Laying the foundations \n              In one of its most ambitious plans, the government is building a $50-million science complex called Polo Cient\u00edfico in the trendy Palermo area of Buenos Aires to house several new research institutes. One will be an international interdisciplinary research centre, focusing on biomedicine, information technology, nanotechnology and the social sciences. Another, a partnership with the Max Planck Society in Germany, will have 200 scientific staff working on molecular biology, neuroscience and cancer when it is completed in 2011. And at the University of Buenos Aires, an $8-million science building is being planned to house the centre where Kornblihtt works: the Institute for Physiology, Molecular Biology and Neurosciences, directed by Osvaldo Uchitel. But it may take more than money to bring scientists back. In the worst years of military rule, the 1976\u201383 Dirty War, an estimated 30,000 people disappeared. Subsequent investigations revealed that they were often dumped from military aircraft into the ocean. \"We all know someone who disappeared,\" Kornblihtt says. In the university building containing Kornblihtt's lab, there are floor-to-ceiling banners with names and photographs of the disappeared students \u2014 a daily reminder of the past. Kornblihtt was one of those who remained in Argentina through much of the troubled times, aided at times by philanthropic grants. In 2000, he was selected by the Antorchas Foundation of Buenos Aires to receive three consecutive three-year grants worth a total of $900,000. \"This changed the quality of my science,\" he says. The money has allowed him to equip his lab, nurture students and collaborate internationally in his work on alternative splicing 2 , the process by which messenger RNA is cut into different forms before coding for proteins. In 2002, and again in 2007, Kornblihtt received a five-year international fellowship of around $100,000 a year from the Howard Hughes Medical Institute (HHMI) in Chevy Chase, Maryland. The HHMI expanded its international programme to Latin America in 1997 and Argentina now has 14 HHMI-funded researchers. Only Canada receives more of the institute's international awards. \"We don't make grants because a nation has difficult economic times,\" says Jill Conley, director of the HHMI's international programme. \"We hold the bar high. But it is gratifying to see that this offers some folks a lifeline.\"  \n                Anonymous donors \n             As Argentina's economy has strengthened, philanthropy from national donors has also grown. By 2006, the Leloir Institute Foundation in Buenos Aires had amassed a $25-million endowment from private donations, of which about $3 million is now being used to build a new wing that will house core equipment for the institute's 25 principal investigators. The wing is set to open in 2009, when a symposium is planned around the institute's primary fields of neuroscience, cell biology, cancer and infectious disease. But the main donor wishes to remain anonymous. Maintaining such a low profile is common in Latin American countries, because the wealthy fear undesirable scrutiny or kidnapping. Located in central Buenos Aires, the Leloir Institute looks like a research centre in any major world capital, from its airy labs to the modern art on the lawn. The foundation was named after Luis Federico Leloir, who won Argentina's second scientific Nobel \u2014 for chemistry \u2014 in 1970. Institute director Fernando Goldbaum, one of five HHMI fellows there, studies brucellosis, a bacterial infection that has re-emerged in recent years in both humans and livestock. As two of the world's largest beef exporters, Argentina and its neighbour Brazil together lose an estimated $100 million annually to the disease. Last year, working with Roberto Bogomolni of the University of California, Santa Cruz, Goldbaum identified light-sensitive proteins that increase the virulence of brucellosis bacteria 3 . Goldbaum says that high-quality research of this type is already commonplace in Argentina, but often receives little international notice. Last year, Leloir scientists felt that Argentina was improperly lumped together with developing countries 4  in an issue of  Nature Medicine  about 'shoestring' science, which included an article about bamboo microscopes in India 5 . Neuroscientist Alejandro Schinder almost jumps from his seat at the mention of bamboo microscopes. Schinder did postdoctoral research with neuroscientist Fred Gage at the Salk Institute for Biological Sciences in La Jolla, California, and then returned in 2002 to establish his own laboratory at the Leloir Institute. In a recent paper, Schinder, Gage and their team showed that newly born neurons in the adult brain make connections with existing cells, resolving a controversy about whether such cells contribute to working neuronal circuits 6 . As part of this, Schinder's lab used a light-activated ion channel to show how the connections functioned. \"You don't publish research like this with bamboo microscopes,\" says Schinder. \"Not a single day do I regret my decision [to move back]. The impact of my science is much greater in Argentina than in the United States. We started a virgin field here.\" Goldbaum says that six new principal-investigator positions will be added at the institute in the next few years, funded by the endowment. \"We have a strong policy to reverse the brain drain,\" he says. Bara\u00f1ao estimates that about 850 of the 5,000 researchers in Argentina are of the world-class calibre that he particularly wants to retain and, to do so, the science ministry plans to match the money that Argentinian researchers receive from international grant givers, such as the HHMI. \"We want to reward the most innovative and productive,\" he says. \"So we are going to match their outside grants.\" Much of the money promised to competitive grants in the 2009 budget will go towards this. Juan Pablo Paz, a physicist at the University of Buenos Aires, sees these actions as vital to giving science a greater cultural and political presence in Argentina. \"Science has not been viewed as something that creates economic opportunity,\" he says. \"But now it is being seen as part of our economic policy. I am very encouraged.\" Outside the country though, there is still hesitancy. Oscar Bruno, a mathematician who attended Kirchner's New York meeting, left his homeland in 1986 to complete a doctorate in mathematics at New York University. Now he is a tenured professor at the California Institute of Technology in Pasadena. Bara\u00f1ao is creating an institute for applied mathematics, a public\u2013private partnership based in Buenos Aires. But although Bruno feels the cultural draw to return, he is not yet sure if the incentives and investment in Argentinian science are enough to persuade him to leave the US lifestyle he has adopted. \"I am watching and thinking,\" he says. \"I would like to help my country. But returning is a very difficult call.\" \n                     Immunology south of the equator in the Americas \n                   \n                     Army pall over Argentine science \n                   \n                     Leloir Institute \n                   \n                     HHMI international scholars \n                   Reprints and Permissions"},
{"file_id": "456690a", "url": "https://www.nature.com/articles/456690a", "year": 2008, "authors": [{"name": "Eric Hand"}], "parsed_as_year": "2006_or_before", "body": "Eric Hand reports on the short life and hard times of the little Mars lander that sort-of-could. By the standards of Tucson, Arizona \u2014 let alone the northern plains of Mars \u2014 Ithaca, New York, is lush in any season. But winter is coming on. The autumn foliage is already past its prime as Peter Smith strides up the steep hill towards the Cornell University campus. It is 14 October, and Smith, a professor at the University of Arizona, is scheduled to give a talk on the status of NASA's Phoenix lander at the annual meeting of the American Astronomical Society's Division of Planetary Science. Smith is Phoenix's principal investigator, and the only academic ever to have overall responsibility for running a mission on the surface of Mars. He has decided to walk from his downtown hotel to the conference centre, and the unseasonably sunny weather is exacting its toll. A patch of sweat spreads from the centre of his bright orange golf shirt towards the burning, beady-eyed bird emblazoned on his breast: the Phoenix mission badge. Smith is not the first speaker in the session on ongoing NASA missions; that is local hero Steve Squyres, the Cornell professor responsible for the science packages on Spirit and Opportunity, the rovers that have been trundling across the planet indefatigably since 2004. Squyres bounds up onto the stage in boots and blue jeans: \"I've only got 20 minutes for this, so hang onto your seats.\" After a whirlwind tour of hills climbed, craters visited and geological features studied, Squyres finishes with the latest ambition for Opportunity: a 12-kilometre trek to a crater 22 kilometres across. It could take up to two years to complete. Smith looks tired as he mounts the stage. He stands back from the podium and flashes a smile. \"You know, there's a big difference between Steve and me,\" he says. \"Steve's always moving. I stay in one place. I'm kinda a couch guy, ya know? So our missions are like that, too.\" But for all his self-depreciation, Smith talks with real pride about what his team of, at its peak, 300 people has done. It has delivered a comparatively cheap spacecraft to the surface of Mars, and set it down on a plain rich in near-surface ice. The analytical lab on board has found evidence of intriguing salts; Smith shows a picture of strange spots on one of the lander's legs that might, conceivably, be water droplets (see  'Strange brew' ). Another instrument has found evidence for carbonates, formed in the presence of water, and a weak signal that, Smith says, might just be due to organic molecules \u2014 something never detected before on Mars. So much to relish and pursue \u2014 if only there were more time. But there isn't. Whereas Squyre's rovers have had their mission stretched from its original 90 sols \u2014 the term used for a Martian day \u2014 to 1,700 and counting, nothing like that is possible for Phoenix. The plain it sits on is far to the north of the rovers, and the winter that is swiftly coming on is harsh enough to freeze the thin atmosphere onto Phoenix's body. The scientists running the various instruments are jockeying for a share of the ever-lower power levels as the days get shorter and the sun sinks lower; the end is in sight. The Cornell talk is on the mission's 138th sol. Smith knows that well before sol 200 the mission will be over, the lander dead on its darkling plain.  \n                Reversal of fortune \n              The mission has taken its toll on Smith, normally a gregarious, happy-go-lucky man. During the talk, he charms the audience with his humour. But over lunch he is uncharacteristically downbeat, even testy. \"It's not quite what I thought it would be at the beginning,\" he says, picking at his food. \"We're right there, next to a soil that has all these wonderful secrets locked into it. We've got the right instruments, we've got the right people involved, everything is perfect. And we can't quite get those bits of scientific knowledge out of our instruments. It's a frustration, I tell you. We're going right down to the wire.\" Phoenix was a child of misfortune. After the success of Mars Pathfinder in 1997, the first landing on Mars since those of the Vikings in 1976, NASA's Mars programme managers had taken up then-Administrator Dan Goldin's mantra of \"faster, better, cheaper\", trying to deliver two small, innovative spacecraft to the planet every two years. But in September 1999, Mars Climate Orbiter burned up in the planet's atmosphere because no-one had noticed a confusion between metric and imperial units in the navigation commands. Three months later Mars Polar Lander (MPL) was lost as it made its way to a site near the planet's South Pole, probably because its thrusters shut off while it was still 40 metres up in the air. Taking no more chances, in 2000 NASA cancelled a lander planned for 2001 that would have used the same design as MPL. And a proposed orbiter, the Mars 2003 Surveyor, lost its launch opportunity to Squyres' rovers. This put Smith in dire straits. Smith had run the main camera on Mars Pathfinder, and had MPL survived he would have done the same on that mission. He had also been working on providing and running cameras for both the cancelled missions, work now curtailed. \"I had no job. I lost 35 employees. I had no mission.\" Smith went to work for University of Arizona colleague Alfred McEwen, who was managing the Hi-Rise camera for the Mars Reconnaissance Orbiter, due to fly in 2005. But faster, better, cheaper was not quite dead. In 2002, NASA opened up a competition for a new 'Mars Scout' line of low-cost missions, modelled after its Discovery missions to the rest of the Solar System. Smith got back into the fray. He was the cameraman on six of the Scout proposals \u2014 and the principal investigator on a proposal of his own. \"No matter who won,\" he says, \"I would win too.\" Smith's proposal, Phoenix, was an ingenious one. It would use hardware from the abandoned 2001 lander to run a mission similar to the ill-fated MPL \u2014 but flipped from one pole to the other. Bill Boynton, a burly, balding colleague of Smith's at the University of Arizona, and also a veteran of MPL, was in charge of a \u03b3-ray spectrometer on Mars Odyssey, an orbiter launched in 2001. He was finding evidence of broad haloes of near-surface hydrogen extending out from the planet's polar ice caps. The implication was that there was water ice in the plains, and that it was close enough to the surface to be studied with little more than a trowel. The northern plains seemed to be as icy as MPL's target site in the south, but with the advantages of being flat, mostly boulder-free and at much lower elevation. A spacecraft landing there would have considerably more atmosphere to slow it down before reaching the surface. The Phoenix proposal suggested a landing on those northern plains with two specific goals: to study the history of water in the Martian arctic, and to assess the biological potential of the boundary between the ice and soil, in search of evidence of an environment that might be habitable. In 2003, to the surprise of many, Phoenix won the competition to become the first Mars Scout, beating a range of less risky missions. As on the Discovery missions, Smith would be in charge of everything. A project manager at the Jet Propulsion Laboratory (JPL) in Pasadena, California, would supervise the contracts and construction of the mission. But Smith's out-of-the-way university would be the operational home.  \n                So beautiful, so new \n             Smith's science operations centre is in a residential neighbourhood a few kilometres from the university campus in Tucson. It is a low-slung, stucco building amid the dun-coloured homes, anonymous but for a bright, almost garish mural showing Phoenix's descent to Mars. On 25 May \u2014 sol zero \u2014 hundreds of scientists, engineers and their families gathered there to munch on picnic food and watch the tracking information relayed to Earth via Odyssey and the Mars Reconnaissance Orbiter as Phoenix's interplanetary voyage came to an end. In what the team was calling the 'seven minutes of terror', Phoenix plunged through the thin air. The blackened heat shield was jettisoned. The parachute opened. Mission controllers called out altitudes. Twelve retro-rocket thrusters began a rapid fire dance of high-pressure hydrazine. After a final twist to orient itself to maximize the amount of sunlight its solar panels would absorb, Phoenix touched down. It was the first landing on Mars since the Vikings to use rockets, not air bags, to cushion the descent, and in the imaginations of the scientists and engineers back on Earth it had done so flawlessly. In Tucson, people cheered and clapped and cried. \"The landing was probably the biggest high of the whole mission,\" says Smith. \"We were told over and over and over again to expect it to fail.\" The influx of data was, at first, as smooth and splendid as the landing had been. As the scientists adapted their bodies to sols instead of days \u2014 an extra 40 minutes every 24 hours leads to something like permanent jet lag \u2014 Phoenix's main camera, mounted two metres above the plain, started on a panorama. Early shots revealed cracks in the soil that marked out polygons of different sizes. The cracks hinted at cycles of warming and cooling that had changed over time \u2014 in sync, perhaps, with oscillations in the tilt and orbit of Mars. The lander's robot arm reached out and scratched at the ground, and on sol 20 the camera saw some pale nuggets at the bottom of a trench a few centimetres deep. Four sols later they were gone; it was ice that had sublimed from solid to vapour after being exposed to the balmy warmth of the polar summer: \u221231 \u00b0C that day, according to the lander's meteorological station. The pictures were meant to be just an appetizer; the main course was to be soil samples fed into Phoenix's eight tiny bellies, each of them an oven that could bake its contents to 1,000 \u00b0C. Phoenix's nose, a mass spectrometer, would sniff the gases given off, detecting compounds down to concentrations as low as 10 parts per billion. The whole ovens-plus-spectrograph system was the Thermal and Evolved Gas Analyzer (TEGA), and it was crucial to the mission's goal of seeking out organic compounds in the soil. Viking had failed to find such compounds; but TEGA had hotter ovens and a landing site where, some argued, the iciness of the soil would preserve organic compounds well (see  _Nature_ 448, 742\u2013744; 2008 ). Such compounds might not be evidence of life \u2014 various apparently lifeless sites in the Solar System have complex carbon chemistry \u2014 but they would speak to Phoenix's goal of assessing habitability. TEGA, based on a similar instrument that had flown on MPL, was run by Boynton. The man who was detecting ice from orbit with one spectrometer would also melt it down on the Martian ground to sniff its isotopes with another. TEGA was at the heart of Phoenix's planned science, Boynton says. Its results were the most eagerly anticipated, and its care and feeding took up a great deal of the team's time. Impressively, the workhorse had been put together for just $13.6 million by a dozen university scientists and technicians. But TEGA was temperamental from the moment it was put to work.  \n                By this distant northern sea \n              In April 2007, a few days before TEGA was shipped from Tucson to Lockheed Martin in Denver, where it would be bolted onto the spacecraft, a short circuit was found in a filament inside the mass spectrometer. It was not a fatal flaw; there was time to clear up the problem, and the instrument had a second filament as backup. On sol 4, though, the backup filament was found to have a short circuit, leaving TEGA without a safety net and heightening the stress on Boynton's team. Towards the end of June, on sol 25, another short circuit was discovered, this time in one of the oven units, probably caused by the shaking the unit was subjected to as the spacecraft operators tried get some surprisingly sticky soil through a grating. Again, the short wasn't fatal \u2014 it affected only one of the ovens \u2014 but it added to the nervousness both in Tucson and at NASA headquarters. On missions led by principal investigators, such as the Discoveries and Scouts, NASA is supposed to defer to the scientist in charge on all matters of scientific operation. But Phoenix was high profile and some of its instruments a little erratic. At headquarters, everyone from Administrator Michael Griffin down was involved in daily reviews of the mission, says Doug McCuistion, Mars exploration programme chief at NASA. At the end of June, word came down that the Phoenix team was to treat its next TEGA sample as its last, and to go after a sample of rock-hard ice before it did anything else. The Tucson team had lost its autonomy. \"We stepped in, I'll be honest,\" says McCuistion. Boynton \u2014 a bit of a bulldog when it came to keeping control over his instrument \u2014 acknowledges the logic: \"NASA was really afraid \u2026 that if we never got the ice it would be embarrassing.\" But he and Smith still resent the way that the mission was taken over. \"That's not the way you do these things,\" says Smith. \"That's why we were pushed at the end.\" It took the team days to figure out how to get the scraps of ice, shredded with a rasp, into the arm's scoop. Then, when the scoop was turned over above one of TEGA's ovens, the ice refused to fall out. Even when the team figured out how to sprinkle some out, it was faced with what would become the most nagging of the mission's problems: the doors to the ovens only opened partway. Much of what came out of the scoop didn't make it into the ovens. What made the glitch most maddening was that the mistake had been caught before launch. One of the differences between the TEGA on Phoenix and that on MPL was a thin retracting cover to keep the instruments from being contaminated by any stowaway microbes from Earth. Boynton and his team had noticed, on a test version of TEGA, that the brackets at the bottom of this cover were just a hair's width too big, and as a result obstructed the doors. They sent revised designs for the cover to the manufacturer, Honeybee Robotics of New York. New parts were delivered and installed. But Honeybee had made the new parts using the original flawed designs \u2014 and nobody in Tucson checked them. \"They should've caught it and we should've caught it, but neither of us did,\" says Boynton, ruefully. (See \"Correction\":#Correction.) Boynton says that the problems with TEGA weren't lost capabilities, but lost time: \"the clock was running against us\". After three unsuccessful weeks attempting to get some pure ice into an oven, the NASA directive was relaxed, and the Phoenix team went for its preferred option: scrapings of icy soil. There was enough ice in the soil for TEGA to confirm that it was water, but not enough to measure its isotopic make-up \u2014 a measurement that could have provided insights into the history of the planet's water and atmosphere. In August and September, TEGA went on to cook up several more shallow soil samples, and found strong signals for calcium carbonate, which is typically found precipitated out of water. More intriguing to Boynton was a low-temperature signal in all the samples \u2014 the same signal that Smith had hinted at in Ithaca. It probably came from a different type of carbonate, but it could have been the trace of an organic molecule.  \n                The breath of the night wind \n              On sol 153 \u2014 a few days before Halloween, and a few weeks after Smith gave his talk at Cornell \u2014 Boynton celebrates his 64th birthday and convenes the final TEGA planning session in his office on campus. The mission scientists have long since vacated the science operation centre; only Smith and some support staff remain there. Boynton has a bandage on his left hand where a mishap with a coffee machine has left him with a third-degree burn; TEGA hasn't been much kinder. The previous evening Boynton had heard from mission engineers that Phoenix had entered a 'safe' mode \u2014 the \"do no harm\" response programmed into spacecraft as a way to deal with unexpected circumstances \u2014 and that commands to Phoenix to shut down its heaters, an attempt to conserve power, had not gone through. But there is still a chance that the TEGA team will get the power, and time, for one last experiment. With his good hand, Boynton wipes crumbs of celebratory chocolate and pumpkin cake from the table and he and his engineers sit down to go over blocks of programming code to be radioed up to Phoenix. They have done this hundreds of times before in the lifetime of the mission. Sunlight streams in through a large glass window, filled nearly to the edges of the frame by the crags of Mount Lemmon, which looms over Tucson. \"This is the last block I'm ever going to have to write,\" says one of the engineers. They pour over their packet of code \u2014 instructions to some valves to open and some to shut \u2014 looking for errors. The goal is to suck an atmospheric sample into TEGA and draw out all the carbon dioxide, leaving proportionally higher concentrations of trace gases. The mass spectrometer would then measure the isotopes of argon and other noble gases, which in turn would offer up information about the history of Mars's atmosphere. They know their best-laid plans may not come to pass, just as they have not in the past. They are ready for the end, which will bring both relief and grief. \"It's really has been a lot more emotional than I expected it to be. We wanted it to do a little bit more,\" says Heather Enos, the TEGA instrument manager. \"But isn't that always human nature?\". In many ways, the trials and tribulations of TEGA are representative of those of the entire mission. At $428 million, Phoenix was a 'low-cost' mission that took on significant risk. According to Boynton, there is no way that one of NASA's traditional operation centres, such as JPL, could ever have operated it for less than half a billion dollars, or built an instrument like TEGA for $13.6 million. \"JPL would just say you can't do it, you gotta do it for this much or not at all. We figured there was a shot we could do it. But we did cut corners.\" A similar, but much fancier, instrument on JPL's vastly overbudget $2-billion Mars Science Laboratory, the launch of which was last week put off for two years to 2011, has cost about $80 million. It is being assembled by 50 scientists and engineers. Gentry Lee, JPL's chief engineer for Solar System exploration, says that Phoenix will be remembered for getting \"remarkable bang for the buck\". The lander performed science on almost every sol of its 157-sol life. It baked samples in five of eight of its ovens, and used three of its four wet chemistry beakers; it never tested the ice, but it tested icy soil. It took more than 25,000 pictures. Its atomic force microscope saw plate-like particles 100 times smaller than those spotted in the best resolution images from the rovers' microscopic imagers. Its main camera saw snow falling from overhead clouds, and frost gathering on the ground. Boynton says that they didn't reach the optimum point on the science per dollar curve, and he wishes, especially, that they had more money for post-mission analysis, given that the scientists had so little time to work with data during the mission. He is still working on Earth-based controls that might help to decipher that enigmatic low-temperature, probably carbonate signal from TEGA. But he doesn't regret the way things turned out one bit. They stayed within their cost caps. And they delivered. Maybe not to the doorstep, but at least to the front yard. \"You can't expect the toast to always fall butter-side up.\"  \n                Like a land of dreams \n              The next night, when Phoenix finally takes leave of this world, Peter Smith sleeps through its slipping away. At seven the following morning, he answers the door to his home in NASA-labelled athletic shorts, having just woken up to an e-mail with the subject 'Not so good news'. \"You picked a heckuva day to come,\" he says. The sun, still low, has yet to bake off the desert's night-time chill. Despite efforts to turn off power-hungry heaters, the lander, with its batteries drained, has gone beyond its safe mode to 'Lazarus mode', an autonomous state in which it tries to operate fixed programs but no longer takes new commands on board. Engineers will monitor weak signals for three more days but never regain control. The science mission has ended. But the time to grieve has not yet come, because the dog must be walked. Smith puts on his running shoes and grabs the leash. Happy, his exuberant one-year-old mutt \u2014 \"some sled dog,\" Smith mutters, still bleary \u2014 is already bouncing. Happy charges past the barrel cactus in the front yard and Smith, tugged along, wrestles for the words that describe the end that he knew would come. \"There's some relief when it's over. On the other hand, it seems over too soon.\" Lee, who has seen generations and missions come and go, says that there is a place for Mars missions of all styles and sizes. Big, expensive flagships for the jobs that cannot fail; smaller, cheaper missions for the ones that can; and, occasionally, medium-sized ones that take dead parts and make them live again. Phoenix has shed light on the results of the Viking experiments, and it is already influencing the way that scientists practice with the scoop on the Mars Science Laboratory. Mars missions are mutually indebted in a way missions to more seldom visited places cannot be \u2014 it is part of the luxury of a destination less than a year's rocket ride away. An exploration programme committed to staggered missions, built on what came before, provides some solace in the face of disappointment. But it's hard to deny the pathos of a mission that goes from landing to ending in five months, with many hopes unrealized. Tucson is getting hotter by the minute. In a desert 374 million kilometres away, Phoenix is chilled to the bone, waiting for the solid CO 2  that will soon freeze from the sky to entomb it. Peter Smith, 61, will soon be out of his job as a principal investigator, and he will never run a spacecraft again. Happy noses in the dirt, sniffing for something that's probably organic.\n See Editorial,  page 675  See also Correspondence:  Finding of unusual soil on Mars could stem from tools used \n                     Phoenix landing blog \n                   \n                     Phoenix slideshow \n                   \n                     Mars Rover Web Focus \n                   \n                     Mars Insight (2001) \n                   \n                     University of Arizona Phoenix Website \n                   \n                     Nasa Mars Exploration Programme \n                   \n                     2008 Division for Planetary Sciences meeting at Cornell University \n                   \n                     Dover Beach \n                   Reprints and Permissions"},
{"file_id": "456295a", "url": "https://www.nature.com/articles/456295a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "Celebrating the man and the book. Charles Darwin did not merely open a new chapter in the story of biology; he opened a new book. The publication of  On the Origin of Species  in 1859 was an event without any parallel, the sudden and powerful exposition of a new view of life as mutable, competitive and shaped by its environment laid out in the form of one long argument. As we enter the year of Darwin's 200th anniversary, this week's  Nature  celebrates the impact of  On the Origin of Species  by looking back and looking forward. Janet Browne offers a guide to how the 50th and 100th anniversaries of its publication were marked, and Marek Kohn looks at the long-running debate over how, and to what extent, selection can work on groups rather than individuals.  Nature  readers tell us of their ambitions for the coming year of Darwin celebrations, and we provide a guide to when and where some of those celebrations are taking place. We also look forward to the promise of new research into how natural selection shapes not just the forms creatures take but also the remarkably diverse ways they reach those forms. And we ask what stands in the way of re-originating species by looking at the technology and know-how that would be needed to turn the genome of the woolly mammoth into a living thing. The ways that evolutionary biology has developed, and the range of areas into which it offers insight, are now almost as diverse as the unending carnival of natural forms that Darwin sought to explain. But they can all be traced back to a single place: the origin. \n               Editorial \n               Beyond the origin \n             \n               Gallery Feature \n               An eye for the eye \n               Simon Ings \n             \n               News Feature \n               The needs of the many \n               Marek Kohn \n             \n               News Feature \n               Beneath the surface \n               Tanguy Chouard \n             \n               News Feature \n               Let's make a mammoth \n               Henry Nicholls \n             \n               Commentary \n               Great expectations \n               Patricia Adair Gowaty, Ismail Serageldin, Per-Edvin Persson, Niles Eldredge, Michael Lynch, Masatoshi Nei, Ulrich Kutschera, Mustafa Akyol, Randolph Nesse & Mel Greaves \n             \n               Books & Arts \n               Colonies that conquer \n               Manfred Milinski \n             \n               Books & Arts \n               Darwin: heading to a town near you \n               Joanne Baker \n             \n               Books & Arts \n               Books in brief: A Down House bookshelf \n             \n               Essays \n               Birthdays to remember \n               Janet Browne \n             \n               News & Views \n               Mammoth genomics \n               Michael Hofreiter \n             \n               Letter \n               Sequencing the nuclear genome of the extinct woolly mammoth \n               Webb Miller et al. \n             \n               Letter \n               Mechanism of phototaxis in marine zooplankton \n               G\u00e1sp\u00e1r J\u00e9kely et al. \n             \n                   Podcast \n                  Listen to: Simon Ings and G\u00e1sp\u00e1r J\u00e9kely on the eye; Marek Kohn on group selection; and Henry Nicholls and Stephan Schuster on the making of mammoths.  For more online see our  Darwin 200 News Special \n                     Darwin 200 News Special \n                   Reprints and Permissions"},
{"file_id": "456563a", "url": "https://www.nature.com/articles/456563a", "year": 2008, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "The current crisis in worldwide food prices reinforces the need for more productive agriculture. Emma Marris meets five ambitious scientists determined to stop the world from going hungry. [image 11 center] The rust hunter   \n                Peter Dodds \n             \n                Molecular biologist at the Commonwealth Scientific & Industrial Research Organization Plant Industry in Canberra, Australia \n             \n               Timescale for change: 10 years \n             It was only when his supervisor showed him a picture of a tractor crossing a rust-infected field of wheat that Peter Dodds really understood what he was up against. Behind the tractor bloomed an orange plume of spores several times higher than the vehicle itself. \"It is just amazing the amount of spores that get released in an infected wheat field,\" says Dodds. \"It is like looking at Mount Everest.\" The cloud of fungal spores revealed a terrifying strength in numbers. You might think that if a mutation that will overcome wheat's resistance to a strain of rust is a one in a million fluke it would not be worth worrying about. But there in that one picture were thousands of billions of spores, obscuring the sky. And there are millions of fields. Since the early days of its domestication, wheat has been plagued with various strains of rust. The fungus's spores infiltrate the stomata through which the plant takes in carbon dioxide from the atmosphere, and poke tentacle-like haustoria into the wheat's cells, extracting their nutrients. If the conditions are to its liking, stem rust ( Puccinia graminis ) can kill 50\u201370% of the wheat in an area. These losses can be prevented if the wheat is resistant to the rust. Resistant plants can identify an invasion early and sacrifice the invaded cell, stopping the rust in its tracks. But domesticated wheat, which is bred for yield, has a limited gene pool from which to draw the genetic variations that might offer resistance. So whenever a strain of fungus overcomes the resistance genes, researchers need to scour wild relatives of wheat for new ones that can be introduced to the crop by cross breeding. \n               boxed-text \n             In 1999, the stem-rust-resistance gene  Sr31  \u2014 hitherto an unbeaten champion and relied on by wheat farmers throughout the developing world \u2014 succumbed to a new strain of rust from Uganda. The Ug99 rust has since spread through Kenya and Ethiopia, crossed the Red Sea and reached Iran (see map, right). Experts estimate that 19% of global wheat production grows in the potential migration path of the rust. If it covers the entire zone at risk, losses are estimated to be tens of millions of tonnes, and thus billions of dollars, per year. Dodds might be the man to stop it in its tracks. A laconic Australian, originally from Melbourne, Dodds wants to understand how rust invades cells so that he can engineer resistance proteins from scratch. That would remove the need to find hardy relatives in their wild redoubts. Making the leap from crossbreeding to genetic engineering would also remove the problem of incorporating large chromosome segments that reduce yield along with the resistance genes. Wheat with  Sr31 , for example, is unfit for making yeast-based bread thanks to some undesirable genes that come along with its top-of-the-range resistance. Dodds and his team in Canberra use flax and flax rust as a model; they rarely see a commercial wheat field, and never an infected one \u2014 hence Dodds's amazement at the ominous orange wake in that photo. His team's work centres on substances that the rust secretes to gain entrance to plant cells, or perhaps to manipulate their metabolism \u2014 substances the plant can use to recognize that is it being invaded, and thus get its countermeasures up and running. These compounds, says Dodds, \"make [the rust] vulnerable to recognition by the plant, but the rust can't do without them\". The hope is that once the interaction between these substances and the wheat immune system is worked out, all a bioengineer will need to do is to look at how the rust strain has evolved to identify effective resistance genes. Such an advance would change the arms race between man and rust. \"Initially it will make the catch-up several times faster,\" says Dodds. Eventually, he adds, varieties engineered to express several new resistance genes might be too complex for rust to adapt to for years and years. After that it will just require occasional tweaking to keep ahead of the pathogen. But will countries in East Africa, where rust often takes hold, open their arms to genetically modified wheat when it is ready for use? \"That is a question that I have difficulty answering,\" says Dodds. \"When any part of the world is going to be ready to accept that is hard to know, but having the things available puts forth a strong argument.\" \"Dodds took the analysis of the flax\u2013rust interaction to another level,\" says Jonathan Jones, a specialist in disease resistance at the John Innes Centre in Norwich, UK. Jones says that Dodds is \"the best young guy coming through. He's an outstanding young scientist, but also the work that he is doing is of profound importance\". Many molecular biologists, says Jones, wouldn't be interested in such a difficult problem. \"The pay off is too long term,\" he says. Dodds seems to have the long-term gene. He has been working on rust for 10 years and says he still finds it as interesting as ever. Ug99 has sharpened his focus. \"There is certainly a higher level of urgency when you see that there is a problem that needs a solution rapidly. Every now and then you get a really bad epidemic like this and it requires a response.\" \n               The perennial optimist \n             \n                Jerry Glover \n             \n                Agroecologist at the Land Institute in Salina, Kansas \n             \n               Timescale for change: 30 years \n             He didn't want to stay on the Colorado farm he grew up on; he wanted to become a philosopher. But a summer job in landscaping led Jerry Glover to a community-college course in soil science. He had always loved the loamy smell of the freshly turned land at ploughing time, and the soil drew him back. He got a PhD at Washington State University in Pullman and took a job at the Land Institute, which focuses on sustainable agriculture. There he works on a soil-improvement project that he acknowledges might not be finished in his lifetime. Glover and his collaborators around the world are in the midst of a decades-long attempt to breed wheat into a perennial plant. About 85% of Earth's cultivated land is planted with annual crops (see world map). That means that every year they must be planted anew from seed. On most farms that means ploughs, and ploughs mean carbon loss \u2014 those rich loamy smells \u2014 and erosion. One way round this is 'no-till' farming, in which seeds are inserted into unploughed land. But although this technique has its advantages, when the crops involved are annuals, they never manage to get their roots as deep into the soil as they might if they had longer to grow. \n               boxed-text \n             If crops such as wheat could instead be made to persist from year to year \u2014 to become perennial \u2014 they would require less fertilizer and fewer passes of heavy machinery, and would have more growing seasons. The world's arable lands would revert to something more like the prairies and savannahs that agriculture has replaced, and that would bring many benefits. Some perennial grasslands can be harvested time and again with few or no human inputs of fertilizer without depleting the soil of nutrients 1 . With roots like the beards of grand old men, perennials control erosion in ways that the spindly whippersnapper whiskers of annuals simply cannot. They also improve the quality of the soil and pump more organic matter into it. Glover's particular concern is nitrogen. \"We need a lot for good plant yield,\" he says, \"and we often need to remove a lot for our needs.\" All the nitrogen removed from a farm in the form of protein in crops must somehow be returned to the soil if long-term fertility is to be assured. Most farmers who can afford to do so use synthetic nitrogen fertilizers for the job (see graph). But these fertilizers are expensive to make, and are often applied in such a way that a fair amount flows unused into rivers, disrupting the ecosystems downstream. As the roots of perennials can tap into naturally occurring nitrogen resources that annual crops can't reach, they could make more harvestable protein with less added nitrogen. Also, perennial plants have more opportunities and more root space to interact with the mycorrhizal fungi and bacterial populations that fix nitrogen for them. According to Glover's experiments, perennials, as represented by tall-grass prairie meadows, require just 8% of the energy that a typical high-input annual wheat field needs to make the same amount of harvestable nitrogen. Glover lives across the road from some of his experimental plots, a few kilometres out of town, with his wife and triplet sons, who are still too young too start running through the test fields after the bugs and birds that find the perennial plots more congenial than normal fields. Cats and dogs lounge around the property. Comparing his spread to the working farm he grew up on, he says, with a laugh, \"I feel more like a gentleman farmer on five acres in the country.\" Glover's graduate adviser, John Reganold, says that Glover is motivated by thoughts of the future. \"I don't think he would take on a project that would not feed the planet and also be good for the environment,\" he says. Glover was \"the best graduate student I've ever had, and one of the best that I've ever seen\", he adds. \"Agriculture is one, if not the largest, single threat to biodiversity in terms of human behaviour,\" says Glover. \"People have to eat \u2014 but what can they eat without destroying the environment?\" Perhaps they will someday do that by eating food from farms that run more like the natural landscapes they replaced, acting like a healthy ecosystem and a farm all at once. \n               The thriving peasant \n             \n                Zhang Jianhua \n             \n                Plant physiologist at Hong Kong Baptist University \n             \n               Timescale for change: now \n             Unlike most successful agricultural scientists, Zhang Jianhua knows what it is to be hungry. And it was an observation sharpened by hunger that provided the motivation for much of his research. Zhang, 52, grew up on a farm in the Chinese countryside where every day was hard work in the fields, just as it is for the best part of a billion of his compatriots today. His father, a schoolteacher, was imprisoned in a re-education camp for three years starting in 1957, one of tens of thousands branded as 'rightists' after responding to the call for constructive criticism in Chairman Mao Zedong's 'Hundred Flowers Campaign'. Later, Zhang's father was assigned to teach far from his family, and rarely saw them. Zhang, his siblings and his mother worked on a collective farm, living in a hut with an earth wall and a rice thatched roof. Each household in the collective also had a small plot for its personal use. Zhang vividly remembers the year \"when I was supposed to be in high school\" when he noticed his family plot, which sat a little higher than the others the irrigation channels served, was drying out a bit just as the rice crop was producing its grains in mid-autumn. \"I worried a lot that my rice's yield would suffer,\" he says. \"But at harvesting time, I found that the kernel weight was actually heavier than average. I always remembered that.\" Zhang's abilities as a farmer saw him promoted to a post as a technical expert in his collective. He went on to a local agricultural college, where he studied crop production and breeding, and taught himself English in part by reading works by Charles Dickens with a dictionary in his left hand. \"I spoke with a very strange accent,\" he says, laughing. In 1985, the Chinese government offered him an opportunity to work abroad and he wound up in Bill Davies's plant-physiology lab at Lancaster University, UK. \"There was something pretty compelling about his letter,\" remembers Davies. \"He was the first Chinese person to join the lab.\" Worried about his English, Zhang avoided answering the phone, but started pumping out publications. \"He did the definitive work 2  showing that roots can signal to shoots using a particular plant hormone \u2014 abscisic acid \u2014 in response to drought stress,\" says Davies. One idea Zhang worked on, as remembered from his rice patch back at home, was that drought stress can, at times, convince a plant to throw all its resources into reproduction, as death by drought might be imminent. As a result, nutrients throughout the plant are rushed to the grains. Management techniques derived from this insight, which Zhang is now studying in molecular and genetic detail, go by the name of 'deficit irrigation'. Zhang didn't invent deficit irrigation, but he is now one of its most influential scientific proponents. \"As the result of his work,\" says Davies, \"it became a much more focused effort.\" These days, Zhang's team is looking at the gene functions that are induced by water stress. The team has found, for example, that when deprived of water, plants protect themselves by producing more enzymes that scavenge reactive oxygen species. The group is now busy untangling the details of that regulation. On the management side, one of Zhang's specialities is a technique called 'partial root zone drying', in which some roots are watered and others are not. The idea is that the plant gets both the water it needs and the focus-your-efforts, put-it-all-into-seeds-for-tomorrow-we-die hormone signals. This technique has been enthusiastically taken up by wine growers in Australia, among others. Zhang moved to Hong Kong after several years at Lancaster and now travels throughout China talking about research on deficit irrigation as a way to improve agricultural efficiency. He had always intended to bring his skills home. \"It is very, very unusual to find someone who is so unselfish,\" says Davies of his former prot\u00e9g\u00e9. Thanks to Zhang and others, farmers in northern China have learned to use less water. In northwest China, for example, the amount of water used for irrigation has almost halved from what it was a decade ago, according to Zhang. \"This is a huge and significant achievement and means that we are using less underground water, which has been depleted rapidly in recent years,\" he says. \"In north China where most of the country's wheat is produced, irrigation times have been reduced from traditionally 4\u20135 times per crop to today's 1\u20132 times per crop.\" \"Life for peasants in the countryside is so hard,\" Zhang says. \"We suffered a lot.\" He and his parents are happier now, but many of his childhood friends are still in poverty. \"If you ask how much has changed in the farmers' lives, it is very little,\" he says. \"My dream is to see the farmers have a better life and to have equal rights as city people. I always want to do something for the village people, for the peasants. I am one of them.\" \n               The biotech humanitarian \n             \n                Richard Sayre \n             \n                Director of the Enterprise Rent-A-Car Institute for Renewable Fuels at the Donald Danforth Plant Science Center in St Louis, Missouri \n             \n               Timescale for change: 6 years \n             \"I like being challenged,\" says Richard Sayre. \"I couldn't do one project; I would get bored.\" Sayre recently left Ohio State University in Columbus with a caravan of lab members and their families to direct the Enterprise Rent-A-Car Institute for Renewable Fuels at the Donald Danforth Plant Science Center in Missouri. In that position he will be working on turning algae into biofuel. He has also got a hand in a start-up firm called Phycal, based in Cleveland, Ohio, that works on renewable energy. And he is the head of the BioCassava Plus collaboration. Funded by the Bill & Melinda Gates Foundation, BioCassava Plus is a US$12-million, five-year effort to turn cassava, a South American plant widely cultivated in Africa (see map), into a super-food that provides extra protein and a wide range of micronutrients. In his cassava role, Sayre coordinates 19 investigators from five continents and works on getting the intellectual rights to use the relevant genes as well as the tools needed to get the modified root into the public domain. He's still active in the lab, too, mostly as a troubleshooter. \"Instruments break down and protocols go wrong and I'll go in and fix them,\" he says. \"I'll go under the centrifuge and fix the brushes.\" Cassava is a staple food eaten by some 250 million sub-Saharan Africans. It grows well in very poor soils and requires very little labour, so even people sick with AIDS can grow it. It also tolerates drought better than maize (corn) and other crops do. Unfortunately, it is not a very rich source of nutrients, and many varieties are tainted with bitter compounds that turn to toxic cyanide during digestion. These varieties must be soaked for a long time or ground into flour and then cooked to remove the toxins. Although this makes the food much more labour intensive, Sayre says that many farmers prefer the more-toxic varieties because thieves will not bother with roots that need such laborious preparation. BioCassava Plus aims to improve Cassava both as a crop and as a foodstuff; so far it has managed to hit almost all the ambitious targets the collaboration promised the Gates Foundation when the programme was set up in 2005. The consortium has developed a number of genetic modifications aimed at getting the levels of specific nutrients high enough that 500 grams of cassava would contain the minimum daily allowance. These target nutrients include protein, vitamin A, vitamin E, bioavailable zinc and iron. Another engineered strain now resists some viral infections, and yet another has many fewer cyanogens. Many of these lines are in field trials in Puerto Rico. Next, the group will try to squeeze several or all of these traits into one, farmer-preferred variety of cassava. Sayre says they can physically deliver four genes at a time to the cassava genome, but he is not yet sure if they will all be properly expressed. Eventually, they may have to smuggle 15 genes into one line, a complex undertaking. \"All the collaborators in the cassava project are committed to changing the world, and we think we can,\" says Sayre. He insists that genetic manipulation is essential, despite the resistance of many African countries. \"We cannot get to target levels of iron or zinc without transgenics,\" he says. Sayre is working to manage the resistance in his target markets by building local involvement into the scheme. \"Our strategy is to have gene introgression happen in Africa by Africans,\" he says. \"We think it is a critical element in adoption of the programme.\" Field trials are scheduled for Nigeria for 2009 and are being developed for Kenya. Sayre's interest in cassava can be traced back to Offiong Mkpong, who now teaches at Palm Beach Community College in Lake Worth, Florida. In the late 1980s, Sayre heard about a Nigerian student who needed a job. \"I hired this guy as a dishwasher,\" he says. Sayre had a policy at the time that he would support any student or technician who wanted to work on an independent research project. \"After about three months he said he would like to remove the cyanide from cassava,\" remembers Sayre. \"He grew up in the Niger Delta; during the Biafran civil war, cassava kept his family alive.\" Mkpong and Sayre worked together on the project, starting with enzyme kinetics. When Mkpong moved on, Sayre kept plugging away, eventually engineering strains 3  of the plant that produce 99% less cyanogen than typical cultivars. With so much on his plate and an avowed love of stress, Sayre sounds like he just might be the uptight type. Not at all, according to Washington State University's John Fellman, one of the investigators at BioCassava Plus. \"That's the paradox,\" says Fellman. \"Most of these people who are such hard-chargers don't have time for you, but he always does.\" Fellman has a story that illustrates Sayre's generosity and what Fellman calls his \"California laid-back\" personality. \"When we were coming back from a meeting in Kampala, Uganda, we had to do some time in Amsterdam waiting for flights,\" says Fellman. \"One of the marketing guys for the Danforth Plant Center had a world club membership for KLM. So we were sitting around drinking Heinekens at 7:30 in the morning. I was complaining I didn't have a lecture ready for my class. He said, 'here, just take this summary lecture I gave at the meeting, and this could be your lecture for Monday'. He gave me his whole presentation \u2014 I put it on my thumb drive \u2014 and every one of the slides had a specific credit on it. He wasn't saying this is my work. He was saying this is our work.\" \n               The rice transformer \n             \n                Julian Hibberd \n              [image 10 left]h2. Molecular biologist at the University of Cambridge, UK \n               Timescale for change: 15\u201320 years \n             If there was a moment when Julian Hibberd's pure science got a mission, it was halfway through a lap of the guest-house pool of the International Rice Research Institute (IRRI) in Laguna in the Philippines. It was 2006. Hibberd was attending a meeting of a dozen scientists who study photosynthesis in rice. They had been gathered there by IRRI scientist John Sheehy, who had a very ambitious plan and wanted them on board. Some plants, especially various grasses growing in hot climates, have evolved a way to supercharge photosynthesis. They do this by fixing CO 2  into a four-carbon sugar before feeding it into the cells doing the photosynthesizing, thus increasing the concentration of CO 2  and the efficiency of the photosynthesis. The process is known as C4 photosynthesis because of the four carbons in that sugar. Sheehy's idea was to engineer the C4 supercharger into rice. In one stroke, he thought, he could increase yield by up to 50%. Hibberd had been asked along because his basic investigations into the still-imperfectly understood marvel of photosynthesis had more or less accidentally taken him near the heart of the C4 adaptation. While looking at photosynthesis in cells far away from the stomata that let CO 2  into a plant's leaves, Hibberd had found that they were using a number of the proteins that C4 plants use. And so he found himself, before dinner one night, in the pool with \"all the big names of C4\". He remembers \"swimming up and down thinking about C4 rice\", with the vast experimental rice fields of the institute in the distance. \"I think the most exciting thing was just to feel that the pure science that I was doing might have a route out into agriculture and make a difference to people.\" Hibberd returned to Cambridge with a mission. After a lean couple of years, the Bill & Melinda Gates Foundation has recently given Sheehy's project $11 million over three years, according to the IRRI. Meanwhile, Hibberd has been moving forward, examining which genes change as C4 metabolism evolves \u2014 as it has done on many independent occasions. \"C4 leaves have modifications in biochemistry, anatomy and organelle structure,\" says Hibberd. \"That is why it is really complicated.\" On just the gross anatomy level, many more veins must be made to grow in the leaves. But there is hope. The fact that the C4 process evolved independently dozens of times means that \"there is a biological precedent to say there is some relatively easy or tractable route to get these changes\", says Hibberd. In particular, Hibberd's lab is looking at integrating genes from maize \u2014 which benefits naturally from the C4 pathway \u2014 into rice. Luckily, they often seem to express themselves in the right cells. \"We almost certainly need to use genes from other plants,\" say Hibberd. \"To our knowledge, there just isn't enough variation in rice to get there through conventional breeding.\" Hibberd, whom fellow Cambridge plant-man David Baulcombe calls \"a very modest guy\", seems undaunted by the scale of the project. \"When people say to me, 'don't you think that's ridiculous, to make C4 rice?' I say that I've got 30 years before I retire. It would be defeatist of me to think I can't understand the pathway pretty well. The projected benefits of having C4 rice are huge. If C4 can have 50% more yield, it would impact billions of people.\" \n                     GM Crops: time to choose \n                   \n                     Environmental Impact of GM crops \n                   \n                     Food and the future \n                   \n                     International Rice Research Institute \n                   \n                     The Land Institute \n                   \n                     Hong Kong Baptist University \n                   \n                     The Donald Danforth Plant Science Center \n                   \n                     Commonwealth Scientific and Industrial Research Organisation \n                   \n                     The Food and Agriculture Organization \n                   Reprints and Permissions"},
{"file_id": "456862a", "url": "https://www.nature.com/articles/456862a", "year": 2008, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "He did more than anyone to build the Large Hadron Collider. This year he saw it finished -- and then break down. Geoff Brumfiel profiles the LHC's project leader,  Nature 's newsmaker of the year. Lyndon Rees Evans  gets up from his desk and crosses his sparsely furnished office to a shelf filled with notebooks. He pauses before choosing one and bringing it to the table. He opens it as fondly as if it were a family scrapbook, flipping through pages crowded with diagrams, budgets and the business cards of mid-level government bureaucrats. Finally he gets to what he was looking for: a photocopied drawing of a conference table. Most of the writing on the diagram is in Japanese, but around the table's edge someone has written names, including Evans's, in English. The date at the top is also in English: 2 March 1995. \"This was it, this was the key meeting,\" he says. He points to a Japanese character written in a corner. \"They even showed where the flowers were.\" Most laboratory notebooks \u2014 like most family scrapbooks \u2014 don't record the place settings at meetings with Japanese parliamentarians. But this is the laboratory notebook, or rather one of many notebooks, of the largest scientific experiment ever constructed: the Large Hadron Collider (LHC), a particle accelerator at CERN, the European high-energy physics laboratory near Geneva, Switzerland. The LHC represents a level of ambition never before seen in physics, an ambition so monumental that its realization required it to become the first truly global experimental undertaking. It consists of hundreds of thousands of tonnes of extremely powerful machinery looped round a tunnel 27 kilometres long. Much of this hardware is chilled to within two degrees of absolute zero by a liquid helium system much larger than any seen before. For most physicists, the LHC story \u2014 a new chapter in their discipline's history \u2014 has yet to begin. For Lyn Evans it is almost over. After a decade and a half of daily devotion to getting the machine built, he is due to retire at roughly the same time that the LHC will start to generate data. At 63, he is still never more than a quick recap away from being up to speed on every detail of its design and engineering. At one point he more or less redesigned it single-handedly on his kitchen table. Since he joined the project in 1993 he has built relationships with the world's physicists, politicians and industrialists to ensure its support; he has travelled to factories in Europe, North America and Japan to see its components forged; and he has tackled enormous technical challenges \u2014 the most recent and dramatic being a galling accident just weeks after the machine was first turned on. \"He's the guy who made the LHC,\" says Chris Llewellyn Smith, a friend and former director-general of CERN. \"That is what's going to be on his tombstone.\"  \n                Rise and fall \n              When Evans left his house in Versonnex, France, on 10 September 2008, he was heading for the biggest scientific media event of the year. But that sort of slipped his mind. He didn't even mention to his wife that he would be on television a little later on. He was entirely focused on the experiment. At the CERN campus, which sits on the Swiss border about ten kilometres away, the car park by the main entrance was crammed with broadcast trucks and satellite dishes. Soon, the LHC's normally hushed control room was packed with news anchors, past laboratory directors and distinguished scientists. It was 'beam day' \u2014 the day on which protons would circulate around the LHC for the first time. \"I have rarely been in such a stressful situation,\" Evans says later; he has a lilting Welsh accent that he has kept during four decades of living in France and Switzerland, and a stutter that becomes more pronounced when he is nervous. The machine's eight sectors turn on one by one. There's a cheer each time the beam first passes through a sector; but it is the last sector \u2014 the one that completes the loop \u2014 that really counts. The cameras focus in on Evans's round face as he stares at a wall of flat-screen monitors. \"So it will be on the next cycle \u2014 in about one minute,\" he says haltingly to the crowd. He completes the countdown in French: \"_Trois_ \u2026  deux  \u2026  un  \u2026  et z\u00e9ro .\" Suddenly, two bright dots appear on a screen, a sign that protons have made it all the way round. The control room bursts into applause. Evans's face slackens into a broad smile. He moves around the consoles, shaking hands with the individual members of his team. \"_Merci_,\" he tells them all. \"Well done.\" The circulation of that beam puts the LHC on to more front pages the next day than any scientific event since the sequencing of the human genome. Just circulating the beam, though, is not enough. A second, counter-rotating beam has to be put through the machine. Then the current has to be upped, the acceleration increased, the beams focused to the point at which, when they cross inside vast buried detectors, the head-on collisions between protons are frequent enough to offer up a wealth of data. All that will take weeks, maybe months. Then, nine days into the process, disaster strikes. On an otherwise quiet Friday morning, physicists in the LHC's control room were slowly pumping up the current in the magnets in the region called sector 3\u20134 to see whether the magnets would work as designed. Suddenly, their screens lit up with alarms. According to the computer, more than 100 magnets inside the machine had 'quenched' or lost their superconductivity. Uncontrolled quenches can be disastrous. Currents of many thousands of amperes can be handled quite safely by superconductors, but they will turn magnets to slag if they encounter even a little electrical resistance. Those in the control room knew that what had happened in sector 3\u20134 was serious even before a second set of alarms warned of falling oxygen levels along a subsection of the tunnel. Then, the entire sector's power cut out. The operators were literally in the dark about what was happening five kilometres away, more than a hundred metres below the French countryside. \"I was over in the personnel department haggling about some recruitment problem when I got a call,\" Evans says. He rushed across the lab to the control room. At first, he says, he couldn't understand how so many magnets would quench simultaneously. But he and others soon realized that the quench was by design. The LHC had automatically brought the magnets out of their superconducting state to protect them. At the same time \u2014 and probably for the same reason \u2014 it had sprung a leak. The oxygen alarms had been triggered by helium escaping onto the tunnel. Two months later, sporting jeans and a red Geneva Squash Club sweatshirt, Evans introduces me to the culprit behind both quench and leak. The tops of the Jura mountains to the west are obscured by cloud, the Alps to the south nowhere to be seen. We are in a blocky industrial building on the edge of the campus, where four of the LHC's superconducting magnets are scattered about in various stages of disassembly amid shelves of vacuum flanges and copper tubing. A handful of technicians move between them. From afar, the magnets look like 15-metre sections of what might be a Franco\u2013Swiss oil pipeline. Their long steel tubing is painted an evening-sky blue, and some have \"LHC dipole\" in white lettering on their side. Running through each dipole are the two pipes that carry the colliding proton beams in opposite directions around the machine. \"This is the famous busbar that burned,\" says Evans, pointing at a small ribbon of niobium\u2013titanium alloy, no wider than his own thick index finger, sticking out of the end of a dipole magnet. To an untrained eye, it is almost indistinguishable from the dozens of other superconducting wires that supply the magnets with current. The busbar, however, is the mainstay of the machine's electrical system. It distributes the primary current between all the magnets in a given sector. During installation, the busbar ribbon leading from each magnet must be connected manually to the next by brazing. Evans and his team now think a bad braze lay behind the accident. When it came undone, 8,700 amps of current vaporized a half-metre length of the ribbon and punctured the LHC's heavy vacuum insulation. The liquid helium it protected boiled instantaneously. The outer shell gave the gas boiling off nowhere to go; in places the pressure built up to 20 atmospheres, prying magnets loose of their steel anchors. All told some 53 magnets were affected. Now it all needs to be fixed: the damaged magnets must be replaced, the underlying causes sorted out. If a busbar braze bursts again the team has to be sure it won't fill up the system with high-pressure helium. To that end, relief valves need to be added all through the machine. Before the accident, Evans had hoped to provide the world's waiting physicists with the LHC's first collisions by November 2008. Now, the machine will not start running again until summer 2009, and probably not do any serious physics until 2010. The frustration is all the worse because the team got so close to getting things to work first time. \"This was the last circuit on the last sector, so it was a bitch,\" Evans says. \"Fortunately, I've had some hard problems in the past.\"  \n                The antimatter opportunity \n              The hard problems that established Evans's reputation at CERN came about when the lab took a gamble on an utterly new technique in the 1970s. There was a revolution under way in particle physics, and part of its promise was to confirm a prediction that the electromagnetic force and the weak nuclear force were aspects of the same underlying process. This unification called for new particles, called the  Z  and  W  bosons, which were suddenly highly sought after. Unfortunately, no accelerator had remotely enough oomph to produce any of them. The big machines \u2014 the Main Ring at the Fermi National Accelerator Laboratory (Fermilab) in Batavia, Illinois (see  'Making it big' ), and the Super Proton Synchrotron (SPS) at CERN \u2014 were designed to shine their protons on to fixed targets. In fixed-target physics, only a small fraction of the energy in any of those protons is used to make new particles, and neither the Main Ring nor the SPS had any chance of making the new bosons. In machines that contained two counter-rotating beams it would be possible to arrange head-on collisions between particles \u2014 collisions in which a large fraction of the energy released went into producing new stuff. They could make  W s and  Z s. The problem was that proton colliders were hard to build; there had only ever been one, a CERN machine called the Intersecting Storage Rings, or ISR, and it operated at much too low an energy to make  W s and  Z s. There seemed no way forward until Carlo Rubbia, a gifted and monumentally ambitious Italian physicist, started touting a cunning plan: turn one of the big proton machines into a collider by putting a beam of antiprotons into the same tube as the protons circulating in the opposite direction. Fermilab baulked at this outrageous idea, but CERN \u2014 a previously rather humdrum lab keen to up its game \u2014 gave it a shot. The project required all CERN's technical tenacity. Antiprotons had to be made from scratch (contrary to the novels of Dan Brown, CERN does not have stocks of antimatter just lying around) and accumulated over weeks, carefully sequestered from regular matter and 'cooled' into a tight beam. Evans took on the complex calculations needed to describe how the particles in such beams would ricochet off one another as they travelled together around the SPS's ring. He then helped to design a set of magnets that could keep the beam tightly packed. \"There were several clever people, but he was really exceptional,\" says Walter Scandale, a long-time CERN physicist who has worked with Evans throughout his career. \"I'm speaking of the top 5% at CERN.\" Before the SPS, Evans had not necessarily been committed to a career at CERN. A few years after he arrived in the 1970s he was offered a job at the Joint European Torus, a European fusion experiment at Culham, near Oxford, UK. \"I brought the offer home and my wife, for reasons known only to her, decided she wanted to stay here,\" he says. He was unhappy with the decision. \"I didn't see where the next step was going in accelerators.\" After the proton\u2013antiproton work he could see the next step clearly. The challenge of marshalling beams of powerful particles had bewitched him, and CERN was the best place to put it to work on a truly epic scale. This speciality of Evans's, accelerator physics, is an odd one. It is scientifically demanding, but to an outsider it sounds like a secondary, almost menial, role: the exciting theory-overturning physics is what comes out the other end. Insiders, though, know that accelerator physicists are absolutely crucial, and CERN's culture reflects that. As Philippe Lebrun, who directs the lab's accelerator technology department, puts it: \"The accelerator theorists are a kind of aristocracy.\" \n               boxed-text \n             In his childhood, few would have taken Evans for any sort of aristocrat. He was born in 1945 in the Welsh mining town of Aberdare. His father worked down the pit and died of the lung disease silicosis when Evans was just 11. Evans's mother took a job at a school canteen, struggling to support Lyn and his younger brother Peter. It is not a time that Evans is keen to talk about; but he does say that, although it was difficult, he was \"perfectly happy\". \"I played for the town football team, I played rugby, I had a good social network.\" He got a place in the local grammar school, but describes himself as an undisciplined student until, around the age of 16, he started to get enthusiastic about science. Trying to remember the appeal, he thinks of newspaper stories of hydrogen bombs and fusion power that was too cheap to meter. \"All these things were happening, and that just sort of gelled with my own interests and my abilities,\" he says. \"I found science easy.\" He received top grades in his A-level exams, and went to the University of Wales in Swansea. He started in chemistry but soon switched to physics, staying at Swansea to complete a PhD on fusion energy. On a weekend home in Aberdare he met his wife Lynda, and the couple married in 1967. His adviser, Colyn Grey Morgan, had connections to CERN and convinced the young physicist to apply for a fellowship at the laboratory. Evans had no knowledge of how particle accelerators worked, but his fusion work had focused on stripping gas molecules of their electrons, so he started there, turning hydrogen atoms into protons in a way that suited the accelerator he was working on. \"That was my baptism,\" he says. \"I've come all the way from the hydrogen bottle.\" The hydrogen bottle led to a linear accelerator, the fixed-target version of the SPS and finally the antiproton work that turned the SPS into a collider. Thanks to Evans and others like him, the gamble that CERN had taken paid off spectacularly. The first  W  and  Z  bosons were found in 1983. A year later, Rubbia got a Nobel prize for the discovery \u2014 as did Simon van der Meer, the accelerator physicist whose cooling technique had made it possible. For Evans the real significance of the project was what it suggested might be possible in the future. \"For me it was the essential prototype,\" he says. \"Although the LHC was far over the horizon in those days, this is where we learned the physics of intense stored particle beams.\" The next machine at CERN, though, was to be different; it would work with electrons, not protons. The Large Electron\u2013Positron collider (LEP) was in some ways technically quite simple; it could manage quite well without superconducting dipoles and the like. But at the same time it was, as the name made clear, large. To be big enough to further the study of  Z s and  W s, it had to have much more tunnel than the SPS. And that was a good thing. Rubbia, who became head of CERN in 1989, knew that he would never get the money to build a big proton machine from scratch. Somewhere down the line, though, putting a proton machine into a tunnel that had already been dug might be quite feasible. Before LEP was even finished, Rubbia was forcefully advocating the idea of a similarly vast proton collider as a follow on: the LHC. CERN started to put more effort into the LHC's design. But many European politicians had doubts about actually building it. The United States, stung by its failure to discover the  W s and  Zs , was building a much bigger 87-kilometre superconducting supercollider, which would achieve energies three times higher than the proposed LHC. How could CERN's machine be anything other than an also-ran?  \n                Collide and conquer \n              In 1993, Llewellyn Smith arrived to replace Rubbia at the same time that Giorgio Brianti, who was leading the LHC project, was planning retirement. Llewellyn Smith needed to appoint a replacement. \"It became pretty clear to me that Lyn was the right guy to do the job,\" he recalls. In 20 years Evans had worked on practically every accelerator at the laboratory, which would be useful knowledge; several of them would have to be run together into a cascade of pre-accelerators to provide the LHC with its beam. And for all his CERN experience, Evans was only 47 years old. He would be able to see the project through to its completion. Evans accepted the position immediately, and although he was not officially scheduled to take over until 1994, he began a vigorous redesign of the machine. \"Lyn started working like hell on the accelerator,\" Llewellyn Smith recalls. \"And he made quite a lot of changes.\" In October 1993, as Evans was in the midst of reworking the LHC design, the US Congress dropped a bombshell. Furious with projected cost overruns and poor management at the US supercollider project, it withdrew its funding for the accelerator, even though construction had already begun. The supercollider's cancellation put the LHC back in the game. In fact, it made it the only game in town. But it could still prove too rich a game for the blood of CERN's member states. Evans trimmed hundreds of millions of Swiss francs from the budget. However, in June 1994, Britain and Germany, two of the most powerful members of CERN's council, baulked at the price tag of 2,350 million Swiss francs (US$1,590 million). Evans and Llewellyn Smith hatched plan B: a design in which the LHC would initially be built with only two-thirds of its superconducting magnets. In terms of physics it was unthinkable \u2014 it would cripple the machine's capabilities \u2014 but politically they knew that it might be the only way to get the CERN council's support. Once they had that support, Evans and Llewellyn Smith could go and get the rest of the money from elsewhere \u2014 from Japan, which had been a reluctant partner in the superconducting supercollider, and perhaps also from the humbled Americans. So Evans found himself the Sunday before a crucial trip to Bonn working furiously at his kitchen table to finalize a design that, he believed, could never produce ground-breaking science. \"It was a totally crazy scenario,\" he says. \"But one that was swallowed by the German government.\" The project won approval from the council in December 1994, and Evans and Llewellyn Smith were given two years to find the money for the full-power machine. Suddenly the shy physicist, who had spent his entire career cloistered at CERN, was thrust into the unexpected role of international diplomat \u2014 a role he would find fascinating. \"I was mystified at first,\" he recalls of his early visits to Japan. \"We would have ten courtesy visits in the morning, just in and out, covering all the bases. We drank so much green tea.\" But Evans is in many ways a natural negotiator. He is quiet and deferential, but also confident and engaging, especially when speaking one-on-one. \"He never says what doesn't need to be said,\" says Peter Limon, a high-energy physicist from Fermilab who worked with Evans on bringing the United States into the project. \"That's sort of the definition of a diplomat.\" The efforts paid off. In June 1995, some \u00a55 billion (US$50 million) appeared for the project, wrapped up with emergency spending in the wake of the Kobe earthquake. \"With no warning at all, we had the announcement of a contribution,\" Evans says. \"That started the ball rolling.\" In the cash-strapped Russian Federation, a complex arrangement was worked out whereby CERN and the Russian government each paid for a third of parts supplied. \"The third third, nobody knew where it came from,\" he says. \"It was just cheap labour.\" By the end of 1996, CERN had secured contributions from Japan, India, Canada and Russia, and had a strong signal of interest from the US government. Then came the 'kick in the teeth': Britain and Germany announced at the last minute that they would not meet their 1994 commitments. But the council allowed CERN to make up the shortfall with loans from the European Investment Bank. By 1996, construction work on the project had begun \u2014 and Evans's role changed yet again, from designer to diplomat to something more like a foreman.  \n                Organized chaos \n              You might expect a laboratory based on the border of France and Switzerland that measures the fundamental symmetries of existence to be a supremely orderly and Cartesian sort of place. You would be wrong; it is in fact decidedly higgledy piggledy. Its gently decaying post-war buildings, to take one example, are identified in such a way that numbers 30, 112 and 376 sit side-by-side \u2014 half a kilometre away from building 31. Either through force of habit or tradition, the thousands of physicists and engineers from dozens of nations who work at the lab organize themselves in the manner of academics: they set up small and often loose collaborations; when absolutely necessary they will put together a committee to decide on a course of action. Paperwork can meet with something approaching deliberate disregard. Evans's style is much in line with that culture. Understandable, as he has worked there all his life; necessary, too, as imposing any other style might prove impossible; and probably wise, as CERN has brought in groundbreaking mega projects one after the other. True to CERN culture he oversaw the LHC project with a group of around half-a-dozen advisers. The bulk of the daily work was carried out by teams that resemble academic lab groups, each responsible for a subsystem or a sub-subsystem, but overall scheduling remained the preserve of Evans's core team alone. It was an approach seen by some as aloof. Still, when someone ran into a technical problem, Evans would be one of the first on the scene to help. He would get his hands dirty literally and metaphorically; he would take the hours needed to understand a cooling system in the most painstaking detail. Some at the lab, particularly the engineers, see a downside to Evans's loose, confederate approach. At the start of the project he didn't have all the stringent, quality-control procedures that were needed, says Gerard Bachy, a mechanical engineer who has since retired from the laboratory. \"It's not just paperwork at all,\" he says. \"You have to have full traceability for each bit and piece \u2014 it's a very complicated machine.\" And Evans's readiness to mingle on the floor of the machine shop, although often appreciated, left some middle managers feeling out of the loop. There were frayed egos and crossed wires. A consequence of what one engineer describes as Evans's \"soft control\" of the project became apparent towards the end of 2000. Six years after the LHC's initial approval, CERN's council wanted a new cost estimate. As auditors looked in more detail at what had already been spent, they realized that the project was likely to run over budget by about 20%. Evans, now the accomplished diplomat, pushed hard for the lab's director, an Italian particle theorist named Luciano Maiani, to break the news to the member states gently. Instead, Maiani presented the cost overrun to an open council meeting in September 2001, along with a bald request for the inflated budget needed to cover it. The council members were furious. Robert Aymar, a French physicist with a background in nuclear fusion and a long history on large projects was called in to mount a full review of the lab's management. Evans was an obvious and possibly politically expedient target. \"Many people were shooting at Lyn,\" recalls Lucio Rossi, who oversaw the making of the LHC's magnets. But Aymar did not fire Evans. He was more worried about the generally fragmented culture at CERN, which made it hard for the lab to focus and prioritize, than about Evans's specific handling of the LHC. Aymar says that although he recognized Evans's faults, there were many more reasons to keep him on than to fire him: he was popular with the staff and he was willing to work to keep a better track of future costs. Above all he really understood the machine, and could manage the complex relations between its millions of components and the thousands of people and organizations responsible for them. Evans repaid the confidence. Aymar took over as director-general of CERN in 2004, and soon thereafter the machine was faced with a major crisis: the system that fed liquid helium to the magnets proved too poorly manufactured to be used. After that it was a design flaw in some of the magnets. Then there were problems with the connections between sections of beamline. Every time, Evans's calm, low-key approach and overall grasp of the project let him reorder the responsibilities of CERN staff and outside contractors, keep the council calm and find the workaround. \"He is the one who is able to control the technical part of the project, which is by far the most important,\" says Aymar today.  \n                The Last Hadron Collider? \n              Although the setbacks were weathered, there were costs to doing so: the problems with the liquid-helium system, for example, delayed completion by nearly a year. The lost year now facing the machine is thus not unprecedented. But it has come about in a much more public way. Sitting in a kitchen that adjoins the LHC's control room, Evans maintains his usual upbeat attitude. \"When the damn thing works, it will be the scientific tool for really crossing the frontier into new knowledge,\" he says. Throughout the winter \u2014 during which the LHC would normally be shut down anyway, as electricity costs a lot more \u2014 engineers are working double shifts to replace dozens of magnets and to repair those damaged in the incident. Meanwhile, new diagnostic tools are being developed to spot faulty connections before they can cripple the machine. The first collisions cannot come soon enough for the lab's frustrated detector physicists, many of whom have had PhD theses and sabbaticals derailed by the accident. Evans has found himself the subject of more than one ad hominem attack in physics chat rooms and blogs; he knows because he Googles to find out. If Evans is confident that the LHC can get through this just as it got through earlier slip-ups and crises, he is less certain about CERN's long-term future. Each generation of accelerators becomes larger and more expensive. The next machine the community wants to build will be an electron machine, like LEP, but linear rather than a loop. If its 30-kilometre rifle barrel is ever built, it is unlikely to be in the suburbs and farms around CERN. There are, currently, no plans for any further proton machines after the LHC, although the LHC itself could be upgraded. The first proton collider was built at CERN: the last one may have been, too. The CERN that Evans came to in the 1970s, about to occupy the commanding heights of particle physics for the rest of the century, is nearing the end of its time. Quite a few of Evans's younger colleagues have moved from the LHC to the new international fusion project in France, ITER. Evans says that if he were starting now, he might head in the same direction. But he sees no rush for CERN to diversify out of particle physics. \"The flagship for the next 20 years at CERN will be the LHC.\" Due to retire in 2010, Evans plans to split the subsequent years between the local golf courses and some part-time work on the upgrade plans. \"I kid myself that it's not going to leave a void,\" he says, knowing that it will. \"The ability to step away is something that I'm going to have to test myself on.\" Building the LHC has taken his days, his evenings and his weekends. He has so much unused holiday from his years devoted to the machine that he had originally been planning to quit the project next October, months shy of retirement, to use it up. Now he has to face the faint worry that even then it might not be working. It won't be for lack of effort by him and his team \u2014 but it will mean a change of plan. \"If it still isn't working,\" he says matter-of-factly, \"I certainly will not take my vacation.\" See Editorial,  page 837 \n                     LHC Special \n                   \n                     LHC Insight \n                   \n                     CERN \n                   Reprints and Permissions"},
{"file_id": "456860a", "url": "https://www.nature.com/articles/456860a", "year": 2008, "authors": [{"name": "Ashley Yeager"}], "parsed_as_year": "2006_or_before", "body": "Medals, cash and fame rained down on the heads of prominent scientists in 2008. Ashley Yeager rounds up some of them. \n               Robert Langer: Bringing home the medals \n             Chemical and biomedical engineer Robert Langer, of the Massachusetts Institute of Technology in Cambridge, racked up more major science prizes than any other researcher this year \u2014 the Millennium Technology Prize and the Max Planck Research Award, along with a share of the Prince of Asturias Award for Technical and Scientific Research. Total haul: about US$2 million.  \n                What will you do with the money you've won? \n              One of the awards, the Max Planck, can be used for unrestricted research. I plan to do materials and nanotechnology research, in part with people from Germany. As for the rest, my wife and I haven't figured out yet what we will use it for. We will probably use it for our children's education. Another top priority involves science education. We might use some of the money to help improve science education at the high-school or college level.  \n                What is your recipe for success? \n              To the extent that I've been successful in research, it may be because I've often tried to do high-risk projects. When I was done with my PhD in chemical engineering in 1974, my colleagues were all going into petroleum engineering and chemical engineering. I decided to work in a surgery lab, the lab of Judah Folkman at the Children's Hospital Boston and at Harvard Medical School. It's like Robert Frost's  The Road Not Taken . I didn't take the road most chemical engineers took.  \n                Ever thought about going back to petrochemicals? \n              Never. I struggled hard to learn the biology I needed. I never thought about other areas of chemical engineering. All I thought about was that I wanted to make a difference in people's lives, whether it was through education or health. \n               Beatriz Barbuy: Rising star \n             Brazilian astrophysicist Beatriz Barbuy won one of the two Trieste Science Prizes given out this year by TWAS, the academy of sciences for the developing world. Barbuy, of the University of S\u00e3o Paulo, was honoured for her work on the evolution of the chemical composition of stars and the formation of the Milky Way.  \n                How important is it to have a prize that recognizes researchers from developing countries? \n              It is indeed very important. There is a group of people like me who devote a lot of effort to doing good work, and we have been fighting for better infrastructure. This recognition will help going ahead with having better conditions for doing astronomy in Brazil.  \n                How did you react when you learned you had won? \n              I knew that I was submitted as a candidate, but I was surprised with being chosen. This is my first prize.  \n                How do you think astronomy can grow in Brazil? \n              We really need better observing conditions. The way to go with the expensive instruments nowadays clearly is through consortia involving other countries. \n               Sumio Iijima: Big prizes for small work \n             Sumio Iijima, a physicist at Japan's Meijo University who is widely credited as the discoverer of carbon nanotubes, has racked up numerous awards. In 2008, among other prizes, he won the the inaugural Kavli Prize in Nanoscience and shared the Prince of Asturias Award for Technical and Scientific Research.  \n                What does winning the inaugural Kavli Prize mean to you? \n              I feel a kind of responsibility; I have to behave well as a recipient since people may pay me special attention.  \n                How does it feel to win two major prizes? \n              I am extremely happy and glad to have these opportunities in a single year. Only one thing that I am not satisfied with is that both awards have not been known much in Japanese society.  \n                *What will you do with the money? \n              I will keep it for myself and my family. Partly because I don't have a good pension programme. However, according to Japanese tax law, nearly one-third of it will be taken as tax. It is too bad! \n                What advice do you have for other researchers hoping to be as successful as you? \n              My suggestion is you should do what you think best. You should believe in yourself. If you are an experimentalist, you should train yourself as the best technician in your field. I like the word challenge. \n                 For more stories from some of the prizewinners, see \n                 http://tinyurl.com/5o9g3m. \n               Reprints and Permissions"},
{"file_id": "4551171a", "url": "https://www.nature.com/articles/4551171a", "year": 2008, "authors": [{"name": "David Lindley"}], "parsed_as_year": "2006_or_before", "body": "Electronic voting machines were supposed to vanquish unreliable counts. They did not -- but David Lindley finds that other technologies present their own problems. In the US mid-term election of 7 November 2006, the balloting in Sarasota County, Florida, was decidedly high-tech. Voters recorded their choices on electronic touch-screen machines that had been installed following the debacle of Florida's 2000 presidential race between Republican George W. Bush and Democrat Al Gore. Then, recounts and legal actions left the United States uncertain of its next leader for more than a month before Bush was eventually declared winner of the state \u2014 and the country \u2014 by little more than 500 votes out of almost 6 million. The devices promised to prevent a repeat of that event, memorable for its images of officials solemnly peering at hanging, pregnant and dimpled chads on punch-card ballots, trying to decide which of them to count as true votes. Surely with electronic voting, any such ambiguity would be impossible? Evidently not. By the morning of 8 November, election officials were grappling with complaints about machine glitches, unrecorded votes and 'flipped' votes allocated to the wrong candidate. Somehow 13% of voters apparently failed to register any choice at all in the closely fought congressional race between Republican Vern Buchanan and Democrat Christine Jennings. By the time the protests and the lawsuits were over, and Buchanan was officially declared the winner by 369 votes, the conclusion was painfully clear: one set of problems had been traded for another. The effort to reform US voting technology has not been successful. Florida, for example, threw out its old mechanical and paper-based voting technology after 2000, and switched to electronic systems, aided in part by funds allocated by the 2002 Help America Vote Act (HAVA). Critics of the move were quick to say that electronic voting machines were vulnerable to hacking and other attacks that could allow people to change the election results. And then experiences in Sarasota and elsewhere seemed to suggest that electronic machines could produce untrustworthy counts in many other ways. As a result, Florida, New Mexico, Iowa and several other states have now rejected electronic voting and gone back to paper ballots that are marked by hand, typically by filling in a blank oval, and scanned by machine. But the cure may be worse than the disease \u2014 especially given the long history of paper ballots being lost, stolen, faked and stuffed into ballot boxes on the sly. \"It's the most insecure medium there is,\" says Paul Herrnson, a political scientist at the University of Maryland in College Park. Meanwhile, some worry that arguments over the actual and alleged flaws of electronic voting systems have overshadowed more immediate concerns about usability and reliability. During the Florida 2000 election, for example, Palm Beach County's infamous 'butterfly ballot' \u2014 a split-page design with punch-card slots running down the middle \u2014 may have led a few thousand Gore supporters to accidentally vote for Reform candidate Pat Buchanan. Misleading ballot design, it seems, may also have caused problems in Sarasota. Adding to the confusion is the fact that in the United States, conducting elections is the responsibility of state and local governments. The result is a patchwork in which nothing is standard or uniform, something that the HAVA legislation did little to change. Voters heading to the polls on 4 November could face any number of glitches and anomalies, and the possibility that, once again, the outcome of a close presidential race could be shrouded in uncertainty.  \n                Universal problem \n              Controversies over electronic voting are not unique to the United States. The Netherlands embraced electronic voting in the 1970s, in part to deal with the complexity of the nationwide system of proportional representation by which members of parliament are elected. The Dutch system came into being before hacker culture was widespread, says Doug Jones, a computer scientist at the University of Iowa in Iowa City who has observed elections in the Netherlands. But in light of recent concerns, the country has decided to return to paper balloting. Brazil, by contrast, introduced electronic voting beginning in 1996, and now has a fully electronic system. In the initial stages, as many as 7% of voters trying the new system were unable to record their choices electronically, but that figure fell to less than 0.2% by 2000, and the country remains committed to its voting technology. Lingering memories of the 2000 election, along with the chance of a close contest on 4 November, make questions about voting technology especially urgent in the United States. The debate has been particularly contentious when it comes to direct recording electronic (DRE) devices, in which a voter's choice is translated immediately into electronic data. Sarasota's iVotronic voting machines \u2014made by Election Systems & Software based in Omaha, Nebraska \u2014 generated no record of each vote other than the electronic data. (To satisfy a HAVA audit requirement, each one did print out a paper copy of its tally once the election was finished.) In Sarasota, therefore, there was \"basically no evidence that could be examined after the fact that would explain what went wrong\", says Jones. Concerns that DRE machines are vulnerable to undetectable tampering took off in 2003, when a group of computer scientists including Avi Rubin of Johns Hopkins University in Baltimore, Maryland, and Dan Wallach of Rice University in Houston, Texas, published a report (T. Kohno  et al. IEEE Symposium on Security and Privacy  9\u201312 May 2004) pointing out serious security flaws in the software running the AccuVote-TS, a DRE system made by Diebold based in North Canton, Ohio. (A copy of the software had been posted on the Internet some months earlier.) As a safeguard against malfunction or tampering, the authors recommended that DRE systems should generate voter-verified paper audit trails (VVPAT) \u2014 literally, a paper display on which a voter could check that his or her vote had been cast correctly, and which could be kept as a record of the vote should any irregularities come to light. Such a system was put in place in the May 2006 primary elections in Ohio, with printers being added to existing DRE systems. It was not a great success. A study commissioned by Ohio's Cuyahoga County found that almost 10% of the paper records were useless or absent, because, for example, the printers jammed, ran out of paper or overprinted. Ted Selker, who until June was at the Massachusetts Institute of Technology (MIT) in Cambridge as co-director of the California Institute of Technology/MIT Voting Technology Project, says that he saw similar problems when he observed elections in Nevada in 2004, one of the first occasions when a VVPAT system was added to electronic voting. So a DRE device will record many legitimate ballots for which no paper record, or an illegible one, exists. That creates a further problem, says Michael Shamos, a computer scientist at the Carnegie Mellon University in Pittsburgh, Pennsylvania, who is also a lawyer. Election law generally states that the paper records, not the electronic data, constitute the legal ballot. Although Rubin originally thought that adding VVPAT technology would make DRE systems workable, he now says that experience with paper-trail systems and poor implementation by the vendors has killed even that hope. And in any case, he says, VVPAT systems do nothing to solve the fundamental problem with electronic voting, which is that it depends on the reliability of software. Rubin says he is less concerned about external hacking or tampering than about software going wrong, or the risk of malicious actions from within voting-machine manufacturers. He is quick to add that he has never claimed that any manufacturer is corrupt, but thinks it's foolish to use systems that depend on them not being corrupt.  \n                Lost in the system \n              Mundane programming errors may present the main danger. In the March 2008 Ohio primary election, for example, poll workers found that votes were sporadically lost during the transfer of data from individual voting-machine memory cards to the central system; after discovering the problem, election officials were able to re-read the memory cards and establish the correct totals. At the time, Premier Election Solutions \u2014 the name under which the voting machine division of Diebold now operates \u2014 claimed that the data loss resulted from a software clash with an antivirus program on the central system. On 19 August, however, Premier acknowledged in a letter to Ohio secretary of state Jennifer Brunner that it had found an internal software bug that could cause votes to be dropped when data from two memory cards were being read at the same time. Election officials have now circulated guidelines to help poll workers to circumvent the problem without altering the voting equipment. But even if Premier devised a software patch that repaired the glitch, says Shamos, the company couldn't install it without losing certification of its system for use in federal elections. The HAVA legislation vested authority for such certification in a new Election Assistance Commission (EAC), which issued revised guidelines for the conduct of elections in December 2005 and took over the federal certification procedure in January 2007. That certification is voluntary \u2014 not every state complies \u2014 but when they do, it is not a quick process. Of the eight voting systems currently under evaluation, none has yet received EAC certification. Systems now in use were accredited under the old procedure, which was overseen by the National Association of State Election Directors. Shamos argues for an accelerated process to certify small changes, \"especially when it prevents a much more serious problem\". But Rubin thinks that is unrealistic. He says that even large software companies, despite all the testing and checking they do, keep releasing new versions of software as bugs and problems are fixed, and then the fixes give rise to new problems that require additional fixes. A quick turnaround is \"an opportunity to really mess things up if you're making changes at the eleventh hour\", he says. The certification process also impinges on the question of software disclosure. Both critics and advocates of DRE systems generally argue that companies should make public the software they use, which would give them greater incentive to resolve security issues. Vendors can still protect themselves by patenting their ideas and holding copyright to their code, Wallach says, and disclosure should not generate security issues for well designed software. \"Only if their systems are built like garbage does disclosure become a security problem.\" But then, as Rubin points out, voting-machine design \"needs to be set in concrete in order to be certified\". He imagines the case of a software security flaw or bug being discovered just before an election. \"If you try to fix it, you will now be using an uncertified voting system and that would be an avenue for someone to tamper with the votes,\" he says, but if you don't fix it \"you're going to use a system that you know is bad\".  \n                Design errors \n              Another concern about voting machines is that they all too often neglect basic human factors. For example, use of a printed paper record tacitly assumes that a voter confronted with an incorrect paper ballot will reject it and ask to start again. But Selker says that in practice not many voters pay attention to the paper printout, or understand its purpose. That conclusion is supported by Sarah Everett's PhD thesis at Rice University. Everett conducted mock elections to compare the performance of various voting technologies, including punch cards, paper ballots and a touch-screen interface designed by Wallach and his colleagues. In some experiments, deliberate errors \u2014 omitting an election race, or flipping a voter's selection \u2014 were introduced on the electronic system's final review screen, which subjects were asked to check before they cast their ballot. Almost all participants said that the review screen gave them more confidence that their votes had been correctly registered. Only about one-third of them, however, actually noticed that their votes had been changed. Of course, Jones says, if even just a few people verify their vote, such review systems can still provide warning of systematic problems, as long as election officials act. In the Sarasota case, there was \"ample evidence that something was wrong before polls officially opened\". Difficulties with the machines were flagged by early voters, who under Florida law can vote ahead of election day, but their complaints brought no change in voting procedure. The Sarasota 'undervote' came almost entirely from the 120,000 people who voted electronically, of whom almost 18,000 failed to register a choice in the Buchanan\u2013Jennings congressional race. Among the roughly 20,000 absentee voters who sent in optical scan ballots, the undervote was a more typical 2.5%. Post-election testing by Florida officials and later by the Government Accountability Office produced no conclusive evidence of machine malfunctioning, although Wallach says that the tests did not address the full range of troubles that voters reported. \"We have all kinds of evidence of touch-screen malfunction and miscalibration,\" he says. Touch screens must be calibrated so that they correctly connect the place where a voter touches the screen with the corresponding spot on the displayed ballot. The  Sarasota Herald-Tribune , however, suggested poor ballot design as another cause of the undervote. The Buchanan\u2013Jennings race, in which there were no other candidates, appeared at the top of a screen, separated from the governor/lieutenant governor race by a blue banner. The suggestion was that a voter's eye would be drawn to the banner and the large race following it, and could easily overlook the smaller race at the top. Selker tested this by running mock elections using the Sarasota ballot screens. He found that more than 16% of the test subjects missed the congressional race. When he put the race at the bottom of the previous page instead, almost 19% missed it. Selker also has an explanation for the claims of vote flipping. He cites a study by Sarah Sled of the Caltech/MIT Voting Technology Project of the 2003 election that made Arnold Schwarzenegger governor of California. Of 135 candidates, only Schwarzenegger and two others got a substantial number of votes. The six candidates immediately above or below one of the three major candidates on the ballot did slightly but significantly better than the other minor candidates. Sled's conclusion was that about 0.4% of voters inadvertently voted for a candidate adjacent to their true choice. Claims of vote flipping arise, Selker maintains, when voters mistakenly choose the wrong candidate and then, when they see a confirmation screen, assume that the machine has erred. But that's a point in favour of well-designed DRE devices, he argues: they give voters an immediate opportunity to review and correct their choices. DRE systems are also easy to use. Herrnson and his colleagues conducted experiments in which volunteers worked through a moderately complex mock election on a variety of electronic systems, and with a hand-marked paper ballot. Volunteers rated the systems on a number of criteria, including ease of use, ability to correct a mistake and confidence that their choices were accurately recorded. Two of the three touch-screen systems got most of the high marks; the third scored less well largely because it jumped to the next screen without waiting for voters to signal that they were ready to proceed.  \n                Judged too early? \n              Shamos points out other advantages of DRE systems. They can more easily guide voters through complex ballots, can provide the ballot in a variety of languages, can adjust font sizes for voters with visual difficulties and are generally better for disabled voters. He says that the alarm raised over software security \"was a perfectly good message, but it turned into a campaign of incredible vituperation against the concept of a computer being used in voting\". As a safeguard against the manipulation of electronic votes, Shamos says that the data representing the touch-screen image could flow directly into a non-rewritable recording medium. That record could be used to reconstruct the voter's operations on the touch screen. Jones, however, is sceptical. Independent data capture has promise, he says, but \"can you really show to me that it is independent?\" What these systems aim to achieve, he says, is the electronic equivalent of a \"camera over the shoulder\", directly recording the voter's actions, but such techniques require data transfer and storage, and often end up being hackable themselves. Apart from their technical challenges, proposals to improve security and provide trustworthy data back-up increase the cost of voting systems and make life more difficult for volunteer poll workers, whose training is often perfunctory. In addition, says Jones, election officials themselves are intimidated by novel equipment, and leave instruction and trouble-shooting to representatives of the vendor. Concerns over the expense of voting equipment, along with the uncertain legislative atmosphere, combine to make \"a treacherous marketplace\", says Peter Lichtenheld, a spokesman for Hart InterCivic in Austin, Texas, which makes both DRE and optical-scan voting systems. There is still no sign of resolution. Critics of DRE systems concede that no voting system is perfect \u2014 but insist that electronic voting is uniquely susceptible to failures with catastrophic, system-wide consequences. Wallach argues, for example, that defrauding a paper ballot is labour-intensive, whereas for a hacker who has the ingenuity, and the means, to mount an attack on electronic systems, \"the cost to throw an election is dirt cheap\". But Herrnson counters that that distinction is not as stark as it might seem. Anyone wishing to throw an election the old-fashioned way would pick on close races and would know where tampering with a few ballot boxes would swing the result. \"Some of the critics of DRE voting machines uncritically recommend paper-ballot systems without understanding that they too have shortcomings,\" says Jones. Even so, he thinks that optically scanned paper ballots represent the best compromise. Paper ballots are easy to understand, he says, and although they are far from immune to fraud, \"the frauds are understandable and the defences against the frauds are understandable\". For now, Rubin endorses the same system, but \"not because of some love for paper systems\", he says. \"It's just that I think DRE systems have too many inherent problems.\" Florida has now embraced optically scanned paper ballots. But on 26 August, the  Sarasota Herald-Tribune  reported that a Premier scanner being used to read absentee ballots could not upload its data to the central tallying system, and vote totals had to be entered manually. And in the District of Columbia, which uses optical scanning technology from Sequoia Voting Systems, based in San Leandro, California, early vote counts in a 9 September primary election for the city council were inflated by thousands of phantom votes that disappeared in subsequent election reports. The cause of the problem has not been definitively identified. Moreover, Herrnson warns, the return to paper ballots inevitably raises the spectre of a repeat of Florida 2000, with a close result spawning unresolvable arguments over what does and does not constitute a legitimate mark. In place of the relentless focus on the flaws of electronic voting, he says, what is needed is a dispassionate comparison of voting systems taking security, reliability and usability into account. \"The debate would benefit from a panel of election administrators and computer people who aren't bound to one position,\" he says. What's also needed, says Jones, is patience. In recent years, the discovery of electoral flaws has led to \"irresponsible and unrealistic\" expectations that new and better equipment can be put into place in a matter of months. Vendors need two years to develop voting systems to meet new standards, he says, and election officials need another two years to get the systems working reliably. That kind of time might be available \u2014 as long as this November's election doesn't end in contention and acrimony. See Editorial,  page 1149  David Lindley is a freelance science writer based in Alexandria, Virginia. \n                     US election special \n                   \n                     Election Assistance Commission \n                   \n                     Full text of HAVA \n                   \n                     Caltech/MIT Voting Technology Project \n                   \n                     ACCURATE (NSF-funded voting project) \n                   \n                     Verified Voting (advocating paper ballots) \n                   Reprints and Permissions"},
{"file_id": "4551164a", "url": "https://www.nature.com/articles/4551164a", "year": 2008, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Where should the drug industry go to find new ideas? In the first of two features, Alison Abbott asks if the future lies in systems biology -- a field that attempts to piece together 'everything'. In the  second , David Cyranoski looks at drug companies' attraction to China. If it is hard to make something from nothing, then it can be just as hard to make something from everything. But that, in essence, is what many pharmaceutical companies are trying to do as they seek new drug targets by integrating the massive sprawl of biological information now available. The puzzle is like a jigsaw with an unknown number of pieces and, as yet, no edges. So when Cristiano Migliorini of the drug giant Roche saw a promising drug target for type 2 diabetes emerge from hundreds of millions of data points, he felt a celebration was in order. He picked up the phone and invited his 20 or so collaborators from research institutions all over Switzerland to a party. The celebration \u2014 a good dinner, drinks and some lively pub games \u2014 might have been considered premature. It is a long and tortuous route from an interesting protein to a new and effective drug. But since the party, this protein has been put through its paces at Roche's drug-development labs in Basel, and candidate drugs that bind to it are already lined up for testing. The target could still fail, but in Migliorini's mind, the results have already shown that the mega-data-crunching approach of 'systems biology' \u2014 new, modish and increasingly adopted by pharmaceutical firms \u2014 can piece together a meaningful picture from this colossal biological puzzle. \"And that,\" says Migliorini, \"is worth at least a good meal and a game of pool\". The crisis-ridden pharmaceutical industry desperately wants something to celebrate. The glory days of the blockbuster drug seem to be over. When genomics matured at the turn of the century, much of the industry was convinced that individual genes would emerge as the new drug targets. But that reductionist bubble soon burst: the more that geneticists and molecular biologists have discovered, the more complicated most diseases have become. As individual genes have fallen out of favour, 'systems' \u2014 multitudes of genes, proteins and other molecules interacting in an almost infinite number of ways \u2014 have come into vogue. Systems biology is an attempt to make sense of all these data. Some researchers and analysts were cynical when systems biology was hyped as the saviour of the failing research and development pipelines. \"Rightly so, perhaps,\" says Giulio Superti-Furga, who studies complex protein interactions and is head of the Research Center for Molecular Medicine in Vienna. \"A few years ago, systems-biology proponents were writing reviews that triggered unreasonable expectations for advances in medicine.\" Many companies have therefore been cautious about making a big investment. Like Roche, most are testing the water with defined projects, usually in collaboration with top academics or small computational companies. Although systems biology is far from proven as a drug-discovery tool, it has many advocates and its wider adoption seems to be inevitable. If leading biologists are concluding that disease can only be understood by tackling the entire, messed-up system, pharmaceutical companies arguably have little choice but to keep up. \"Industry really needs to dive in,\" Superti-Furga says.  \n                Never-ending networks \n              At its heart, systems biology is about gathering unprecedented amounts of data from cells, then making sense of it through mathematical models. At its most sophisticated, it might involve the high-throughput collection of molecular data, such as DNA sequences, RNA molecules, proteins and metabolites as well as more descriptive data such as clinical diagnoses and drug responses. These data are then assimilated into computational models of cellular processes which, as molecules change location and function every microsecond, must also accommodate the dimensions of time and space. It is not enough to mimic what's known about the cell \u2014 such models must also predict what is unknown so that scientists can test their hypotheses. A model of a liver cell, for example, might reveal which of 100 interesting drug candidates will prove beneficial and which will be toxic. Knowing when to pull a doomed drug from the pipeline could spare a company a lot of expensive work. \"The biggest value of systems biology immediately is as an aid to decision-making,\" says David de Graaf, who has worked in systems biology programmes at Pfizer, AstraZeneca and now at Boehringer Ingelheim. \"And a drug company is a decision-making machine.\" Although systems biologists want it all, for now they cannot get it. \"We'll not have everything a hundred years from now,\" says Colin Hill, chief executive of the biotech company Gene Network Sciences in Cambridge, Massachusetts, which uses supercomputers to model complex biology for drug companies. \"Even after two decades of gathering and archiving molecular data, barely 5% of the total circuitry inside the cell is known.\" So systems biologists must choose between two approaches when developing their models. In one, they scale back their ambition, paring down the number of biochemical pathways being modelled to just a few well-known ones. In the second, they bluff the bits they do not know, trying to model 'everything', and using sophisticated computational tricks to compensate for absent data. At the end of the day, the choice of approach depends on what question researchers want to ask of their models. It's a bit like the conundrum in Douglas Adams's  Hitchhiker's Guide to the Galaxy , where the computer Deep Thought spends 7.5 million years computing an answer to the most horribly imprecise \"ultimate question of life, the Universe and everything\" \u2014 only to come up with the answer \"42\". If the question is narrow \u2014 'which surface proteins are affected when insulin activates receptors on the pancreas cell?' \u2014 then it may be sufficient to model only those networks known to be involved in insulin signalling. If the question is broad \u2014 'what would happen to an animal fed drug X?' \u2014 then the 'everything' approach is more appropriate to see what falls out, be it stronger muscles or liver failure.  \n                The meaning of life \n              Drug companies are divided on which of the two approaches is better, and Migliorini is clear about which he prefers. \"We take modelling in small doses \u2014 a defined number of well-understood pathways,\" he says. \"We have a pragmatic approach \u2014 we can't say '42' to management.\" Cell biologist Willy Krek, an academic partner of Miglorini's at one of Switzerland's foremost universities, the ETH in Zurich, adds: \"It is strategically better to get a rich data set on a limited network than to take measurements for the rest of your life, always saying they are still not enough.\" Even the scaled-back approach to systems biology takes massive effort. Migliorini's collaboration began in 2006 as part of Switzerland's big systems-biology research programme now known as SystemsX.ch. Migliorini calls it \"an industry\u2013academic collaboration beyond clich\u00e9\", referring to what is in fact the largest such partnership between pharmaceutical companies and academia in biology that Switzerland has ever seen. It involves several top Swiss research groups, plus academic and industrial partners elsewhere, at a cost of 9 million Swiss francs (US$8 million) over three years as well as undisclosed internal spending. In searching for its diabetes drug, the consortium has built a model focused on the 500 or so proteins \u2014 called the 'surface proteome' \u2014 on the coat of pancreatic \u03b2 cells, the cells that secrete insulin when blood glucose concentrations rise. Migliorini and his co-workers are working on the concept that some people who overeat develop type 2 diabetes because their \u03b2 cells cannot keep up with the body's demands. The team is seeking to identify surface proteins that stop responding to environmental signals such as glucose metabolites, as these may be responsible for transforming the overworked cells into diseased cells. The scientists use almost every high-tech method imaginable to gather data on proteomes and the cellular networks they link into. Consortium member Rudi Aebersold from the ETH developed one such method using mass spectrometry that separates out glycoproteins \u2014 the proteins involved in key processes such as protein folding or cell\u2013cell signalling \u2014 from the background of other proteins 1 . Importantly, the technique quantifies as well as identifies proteins, providing a molecular fingerprint for a cell in a particular stage of health or disease. Quantification is essential to systems biology, whereas in the past, molecular biologists were happy just to see whether or not a protein was present. To flesh out the proteomic information, the researchers also generate reams of other data, either experimentally or by pulling them out from public and commercial databases, to feed into their computational model. They then work out which surface proteins converge on the same intracellular pathways. At the end, they have a massive, moving, knitting-ball of a network in which every protein is linked to every other protein it interacts with. The hope is that this can point to the 'nodes' \u2014 the key points in a network that have disproportionate influence. Then researchers can ask questions of the model. For example, if the team suspects that the disease is being caused by a particular aberrant glycoprotein, they strip that protein from the model. If correct, the model should then mimic the knock-on effects in the circuitry that are known to occur. That glycoprotein then becomes a potential drug target or, perhaps, a biological marker of disease progression.  \n                Quick results \n              At the start, not everyone at Roche expected the systems approach to succeed within the three years the collaboration was initially given. In fact, Migliorini's first hit, last autumn, came within a year of his project starting. \"It was a great feeling to see our engineering-style approach \u2014 multiple inputs, each comprising amounts of data that make you sweat \u2014 spit out a clear single target,\" he says. Here, luck intervened: the protein in question (its identity is still guarded) had been investigated in relation to an entirely unrelated disease, so the tools and compounds for studying it were already available. \"The whole process went lightning-fast,\" says Ren\u00e9 Imhof, director of research for Roche in Basel. Roche's academic partners are also happy, because the collaboration was formed at exactly the time they were starting to feel the limitations of the traditional genes-will-explain-disease reductionism. Across the Atlantic, in the Boston area of Massachusetts, Peter Sorger and Douglas Lauffenburger have been feeling similar constraints. Sorger says that he used to regularly raise hackles with his outspoken criticism of traditional genomics. As the region is home to the Whitehead Institute for Biomedical Research, one of the most important sequencing hubs of the Human Genome Project, those hackles were being raised on some particularly high-level necks. Sorger and Lauffenburger now operate complementary systems-biology labs \u2014 Sorger at Harvard Medical School in Boston and Lauffenburger at the Massachusetts Institute of Technology in Cambridge \u2014 and have been asked to collaborate with numerous pharmaceutical companies, usually in the area of cancer. Both have worked with AstraZeneca, for instance, to understand the action of the company's anticancer drug gefitinib (Iressa). Gefitinib was a much-heralded drug designed to hit the ERBB1 receptor when it is mutated, as it frequently is in various cancers. But few patients responded to the drug, and genetic sequencing showed that mutations in the receptor could not predict who these patients would be. \"By focusing down on just the gene, the genomic approach had taken the target out of its physiological context,\" says Sorger, \"and the physiology needed to be put back with systems biology.\" Lauffenburger attempted to do this by looking at the whole life cycle of the errant receptor and how it connected with biochemical pathways inside the cell. Like many membrane receptors, ERBB1 switches off its signalling capacity by removing itself from the surface of the cell, and Lauffenburger theorized that sensitivity to gefitinib could depend on how well this process works. His team collected data on genes, transcription, protein phosphorylation and the internalization of the receptor from cells that were either sensitive or insensitive to gefitinib and built a model to mimic the circuitry associated with the receptor. The model suggested that mutations in ERBB1 were almost irrelevant: gefitinib would work only in cells that had inefficient internalization of ERBB1 (ref.  2 ). So far this has not generated new biomarkers for the company's clinical trials, but it has shown that complicated systems biology may be needed to select patients for drug treatments. Like Migliorini and many others in systems biology, Lauffenburger has an engineering background. Others, such as Hill, have swept in from theoretical physics. Hill turned to biology after a stint at the interdisciplinary Santa Fe Institute in New Mexico, where he realized that physicists like himself had become comfortable contemplating the vastness of the Universe or the minuscule nature of the subatomic world, but were distinctly uncomfortable with a mid-sized affair such as life. \"Life was always too complicated a subject for us,\" he recalls. Hill's type have brought with them the sort of Bayesian, or probabilistic, analyses that have been applied in other areas that have huge and incomplete data sets, such as astronomy. They use these techniques to tackle the second approach to systems biology, relying on computer models to fill in the gaps between relatively sparse data points. Researchers then put real data into the model and modify it to fit the data better, repeating these adjustments until all possible permutations have been explored. This type of modelling requires a scale of computing effort analogous to that required to predict weather and understand global warming. The number of ways in which one gene or protein can link to another in the tangled web of intracellular pathways and networks is astronomical. \"If you take 22 variables, such as gene, transcript or clinical measurements, they may each affect an average of maybe 250 points in a network \u2014 that means there could be 250 22  possible combinations,\" says Hill. \"We can only do this with supercomputing.\" Hill's company is now cranking out potential drug targets, which it patents and then peddles to pharmaceutical companies. Industry giant Merck is known for its bold move to incorporate the 'everything' approach into systems biology into its drug-development programmes. In 2001, it acquired the Seattle-based genomics company Rosetta Inpharmatics to develop systems genomics in house. \"The scale of the biological information we have today is unprecedented,\" says Eric Schadt, Rosetta's science director and a former physicist. \"Fifty thousand or so transcripts, millions of mutations, hundreds of clinical endpoints \u2014 a high-performing computing environment is essential and we have one of the world's biggest.\"  \n                Growth factors \n              Earlier this year, the group used the model to show that its systems approach had identified key disease-causing gene networks in metabolic diseases 3 . The paper was published back-to-back with another that had used a systems approach to identify three obesity-related genes that had not sprung out of genomics data alone 4 . \"The surprise here was to find that not just a handful of genes were causally implicated in the disease, but hundreds \u2014 whole pathways were involved,\" says Schadt. \"So the challenge is to hit the nodes in networks where implicated pathways overlap.\" The company says that up to 30% of targets for various diseases in its early pipeline have been identified through Schadt's techniques, and that a few are in early-phase clinical trials. But some people in industrial and academic circles remain hesitant about systems biology. No one can be sure that it will really increase the number of targets or biomarkers that make it through clinical testing. Still, it is a gamble that almost all companies seem willing to make, even if investment levels are sometimes small. In a report published earlier this year, industry analysts at PriceWaterhouseCoopers argue that the pharmaceutical industry needs to rely much more on systems biology if it is to survive the failing-pipeline crisis and predicts that the approach will have become more prevalent by 2020. Stephen Friend, Merck's vice-president for oncology, thinks that any hesitancy will be overcome when the modelling becomes so predictive that the toxicity and efficacy of a potential drug can be forecast very accurately even before an experimental animal is brought out if its cage. \"The next three to five years will provide a couple such landmark predictions and wake everyone up,\" he says. The challenge for systems biologists is that the system is being stretched at all ends of the imagination. There will inevitably be new categories of biological information to collect and feed into models. Measurements will be made on finer and finer scales so that individual molecules can be identified and counted in single cells with greater speed and efficiency. This information will be integrated into grander models, although hopefully not ones that take 7.5 million years to compute answers. At a meeting in Japan in February, scientists signed up to a declaration calling for a grand challenge to create a virtual representation of the physiology of the entire human within three decades. To get that far that fast will require formidable advances in technology development, and even greater computational power and confidence. But for the time being, most systems biologists are happy to have their questions answered, or at least considered, by drawing boundaries around 'everything' such that it is contained within the walls of a virtual cell. \"In fact, I don't think we have a choice,\" Migliorini says. Life, the Universe and almost everything is about the most that biologists can handle for now. Alison Abbott is  Nature 's senior European correspondent. \n                     Roche \n                   \n                     AstraZeneca \n                   \n                     Merck \n                   \n                     SystemsX \n                   \n                     Gene Networks Systems \n                   \n                     Harvard systems biology programme \n                   Reprints and Permissions"},
{"file_id": "456018a", "url": "https://www.nature.com/articles/456018a", "year": 2008, "authors": [{"name": "Brendan Maher"}], "parsed_as_year": "2006_or_before", "body": "When scientists opened up the human genome, they expected to find the genetic components of common traits and diseases. But they were nowhere to be seen. Brendan Maher shines a light on six places where the missing loot could be stashed away. If you want to predict how tall your children might one day be, a good bet would be to look in the mirror, and at your mate. Studies going back almost a century have estimated that height is 80\u201390% heritable. So if 29 centimetres separate the tallest 5% of a population from the shortest, then genetics would account for as many as 27 of them 1 . This year, three groups of researchers 2 ,   3 ,   4  scoured the genomes of huge populations (the largest study 4  looked at more than 30,000 people) for genetic variants associated with the height differences. More than 40 turned up. But there was a problem: the variants had tiny effects. Altogether, they accounted for little more than 5% of height's heritability \u2014 just 6 centimetres by the calculations above. Even though these genome-wide association studies (GWAS) turned up dozens of variants, they did \"very little of the prediction that you would do just by asking people how tall their parents are\", says Joel Hirschhorn at the Broad Institute in Cambridge, Massachusetts, who led one of the studies 3 . Height isn't the only trait in which genes have gone missing, nor is it the most important. Studies looking at similarities between identical and fraternal twins estimate heritability at more than 90% for autism 5  and more than 80% for schizophrenia 6 . And genetics makes a major contribution to disorders such as obesity, diabetes and heart disease. GWAS, one of the most celebrated techniques of the past five years, promised to deliver many of the genes involved (see  'Where's the reward?' ). And to some extent they have, identifying more than 400 genetic variants that contribute to a variety of traits and common diseases. But even when dozens of genes have been linked to a trait, both the individual and cumulative effects are disappointingly small and nowhere near enough to explain earlier estimates of heritability. \"It is the big topic in the genetics of common disease right now,\" says Francis Collins, former head of the National Human Genome Research Institute (NHGRI) in Bethesda, Maryland. The unexpected results left researchers at a point \"where we all had to scratch our heads and say, 'Huh?'\", he says. Although flummoxed by this missing heritability, geneticists remain optimistic that they can find more of it. \"These are very early days, and there are things that are doable in the next year or two that may well explain another sizeable chunk of heritability,\" says Hirschhorn. So where might it be hiding?  \n                Right under everyone's noses \n              The inability to find some genes could be explained by the limitations of GWAS. These studies have identified numerous one-letter variations in DNA called single nucleotide polymorphisms (SNPs) that co-occur with a disease or other trait in thousands of people. But a given SNP represents a much bigger block of genetic material. So, for example, if two people share one of these variants at a key location, both may be scored as having the same version of any height-related gene in that area, even though one person actually has a relatively rare mutation that has a huge effect on height. The association study might identify a variant responsible for the height difference, says Teri Manolio, director of the Office of Population Genomics at the NHGRI, but averaging across hundreds of people could give the appearance that its effects are pretty wimpy. \"It's going to be diluted,\" she says. Finding this type of missing heritability is conceptually easy, because it involves closer scrutiny of the genes already in hand. \"Just exploring, in a very dense way, genetic variation at the loci that have been discovered is probably going to [explain] another increment of missing heritability,\" Hirschhorn says. Researchers will need to sequence candidate genes and their surrounding regions in thousands of people if they are to unearth more associations with the disease. Helen Hobbs and Jonathan Cohen of the University of Texas Southwestern Medical Center in Dallas did this in an attempt to capture all the variation in  ANGPTL4 , a gene their studies had linked to cholesterol and triglyceride concentrations. They sequenced the gene in around 3,500 individuals from the Dallas Heart Study and found that some previously unknown variants had dramatic effects on the concentration of these lipids in the blood 7 . Mark McCarthy of Britain's Oxford Centre for Diabetes, Endocrinology and Metabolism says that such studies could reveal much of the missing heritability, but not a lot of people have had the enthusiasm to do them. This could change as the cost of sequencing falls.  \n                Out of sight \n              Other variants, for which GWAS haven't even begun to provide clues, will prove even harder to find. In the past, conventional genetic studies for inherited diseases such as cystic fibrosis identified rare, mutated genes that have a high penetrance, meaning that the gene has an effect in almost everyone who carries it. But it quickly became apparent that high-penetrance variants would not underlie most common diseases because evolution largely keeps them in check. What powered the push into genome-wide association was a hypothesis that common diseases would be caused by common, low-penetrance variants when enough of them showed up in the same unlucky person. Now that hypothesis is being questioned. \"A lot of people are recognizing that screening for common variation has delivered less than we had hoped,\" says David Goldstein, professor of genetics at Duke University in Durham, North Carolina. But between those variants that stick out like a sore thumb, and those common enough to be dredged up by the wide net of GWAS, there is a potential middle ground of variants that are moderately penetrant but are rare enough that they are missed by the net. There's also the possibility that there are many more-frequent variants that have such a low penetrance that GWAS can't statistically link them to a disease. These very-low-penetrance variants pose some problems, says Leonid Kruglyak professor of ecology and evolutionary biology at Princeton University in New Jersey. \"You're talking about thousands of variants that you would have to invoke to get near 80% or 90% heritability.\" Taken to the extreme, practically every gene in the genome could have a variant that affects height, for example. \"You don't like to think about models like that,\" Kruglyak says. If rare, moderately penetrant or common, weakly penetrant variants are the culprits, then bumping up the number of people in existing association studies could help find previously missed genetic associations. Peter Visscher of the Queensland Institute of Medical Research in Brisbane, Australia, says that a meta-analysis of height studies covering roughly 100,000 people is in the works. Lowering the stringency with which an association is made could drag up more, but confidence in the hits would drop. At some point it might make sense to stop using SNPs, and start sequencing whole genomes. Collins suggests that the NHGRI's 1,000 genomes project, which aims to sequence the genomes of at least 1,000 people from all over the world, could go a long way towards finding hidden heritability, and many more genomes may become possible as the price of sequencing falls. Not everyone supports an all-out sequencing onslaught. Goldstein warns against continuing to \"turn the crank\" without devising a more rational approach, such as sequencing the genomes of people who exhibit extreme manifestations of diseases. \"I'm not really sold on doing the sequencing version of what we did with [GWAS],\" he says. \"It's a big enough, costly enough job, that I think we want to think a little bit harder about exactly who gets re-sequenced.\"  \n                In the architecture \n              Some researchers are now homing in on copy-number variations (CNVs), stretches of DNA tens or hundreds of base pairs long that are deleted or duplicated between individuals. Variations in these features could begin to explain missing heritability in disorders such as schizophrenia and autism, for which GWAS have turned up almost nothing. Two recent studies looked at hundreds of CNVs in normal people and in those with schizophrenia, and found strong associations between the disease and several CNVs 8 ,   9 . They commonly arise  de novo   \u2014 in an individual without any family history of the mutation. These structural variants might account for a lot of the genetic variability from person to person and could account for some of those rare 'out-of-sight' mutations with moderate penetrance that GWAS can't pick up. Many CNVs go undetected because they don't alter SNP sequences. Duplicated regions can also be difficult to sequence. A standard technology for uncovering CNVs is array comparative genomic hybridization, in which scientists examine how genetic material from different individuals hybridizes to a microarray. If certain spots on an array pick up more or less DNA, it could indicate that there's a CNV. This and several other techniques are being tested by a consortium called the Copy Number Variation Project, run out of the Wellcome Trust Sanger Institute in Cambridge, UK. The consortium is dedicated to characterizing as many CNVs as possible so that associations can be made between them and diseases. McCarthy says that the role hidden CNVs have in heritability \"should play out in the next six months to a year\". But Goldstein argues that current technologies will miss many of the smaller CNVs, from 50 base pairs down to repeats of just two bases. \"All we'll have verification of is the big whopping CNVs that are identifiable, and they clearly do not account for much of the missing heritability.\"  \n                In underground networks \n              Most genes work together with close partners, and it is possible that the effects of one on heritability cannot be found without knowing the effects of the others. This is an example of epistasis, in which one gene masks the effect of another, or where several genes work together. Two genes may each add a centimetre to height on their own, for example, but together they could add five. GWAS don't cope with epistasis very well, and efforts to find these interactions usually require good up-front guesses about the interacting partners. Joseph Nadeau, a geneticist at Case Western Reserve University in Cleveland, Ohio, says that 'modifier' genes act even in some straightforward single-gene diseases. \"That's a simple kind of epistasis,\" he says. Cystic fibrosis, for example, is usually caused by mutations in one gene,  CFTR,   yet can vary greatly in symptoms and severity. The suspicion has been that modifier genes are one cause of this variability. But despite the years of study, researchers still struggle to pin down these genes. \"People haven't modelled truly the effect of epistasis,\" says population geneticist Sarah Tishkoff at the University of Pennsylvania in Philadelphia. It's no surprise that genetics is more complicated than one gene, one phenotype, or even several genes, one phenotype, but it's humbling to realize how much more complex things are starting to look. In a now classic study 10 , Kruglyak and his colleagues found that expression of most yeast genes is controlled by several variants, often more than five. To fill in all the heritability blanks, researchers may need better and more varied models of the entire network of genes and regulatory sequences, and of how they act together to produce a phenotype. At some point this process starts to look more like systems biology, and researchers are already applying systems methods to humans and other organisms (see  page 26 ). \"What we're learning from these studies is that we need to think about the more complex of the complex models rather than the more simple of the complex models,\" Kruglyak says.  \n                The great beyond \n              What if heritability estimates were wrong in the first place? Heritability of height was initially measured by taking the mean height of parents and comparing that value to the adult height of their offspring. As the average heights of parents increase, researchers found, so too does the average height of their children, hence the calculated 80\u201390% heritability. Environment, especially factors such as nutrients or toxins present during important growth phases, can affect the mean height of a population considerably \u2014 but researchers have controlled for environment in estimates of heritability by, for example, comparing genetically identical twins raised together with those raised apart. Most researchers are confident that the heritability estimates are sound. \"I don't think anyone's going to say that the heritability of height is 10% and let environment get you closer to the answer,\" Kruglyak says. \"I don't think you can explain it away.\" But there are lingering doubts about how precisely environment has been accounted for in heritability studies. Adverse experiences  in utero   could lead to lifelong health disparities, according to David Barker from the University of Southampton, UK, and yet a shared womb is an aspect of the environment that would not be factored into such studies. \"Heritability estimates are basically what clusters in families, and environment clusters in families,\" says Manolio. Epigenetics, changes in gene expression that are inherited but not caused by changes in genetic sequence, confuses things further. Feeding a mouse a certain diet, for example, can alter the coat colour not only in its children, but also in its children's children 11 . Here, the expression of a coat-colour gene is controlled by a type of DNA modification called methylation, but it's not completely clear how that methylation pattern is 'remembered' by the next generation. The idea that grandma's environment could affect future generations is controversial \u2014 and such effects would have been included in the heritability normally attributed to genes. \"This complicates everything,\" says Nadeau. \"How do we sort out what great-grandfather and great-grandmother were exposed to when they were young and having children?\" Model organisms might help. Nadeau has investigated testicular germ-cell tumours in mice that are analogous to a highly heritable cancer in humans. His group found that the effects of one weak, cancer-promoting gene,  Dnd1 Ter  , are greatly enhanced by several other gene variants, and the boosted effects are passed on even if the genes that cause them are not 12 . \"It's presumably transmitting its presence in some epigenetic way,\" says Nadeau. The mechanisms by which epigenetic inheritance might work are still disputed, though; marks such as methylation that direct gene expression during someone's life seem to be wiped clean in a new embryo. One possible explanation for Nadeau's observation, he says, is that RNA is being inherited alongside DNA through sperm or eggs. Collins is not convinced that epigenetics will play a big part in missing heritability in humans. \"It just doesn't look likely outside of one or two examples to suggest that this is the case.\" Nadeau disagrees. \"It's hard to imagine that every other organism works one way and humans are the exception,\" he says.  \n                Lost in diagnosis \n              There is a nagging worry as researchers hunt for heritability: that common diseases might not, in fact, be common. Medicine tries hard to lump together a complex collection of symptoms and call it a disease. But if thousands of rare genetic variants contribute to a single disease, and the genetic underpinnings can vary radically for different people, how common is it? Are these, in fact, different diseases? GWAS could actually be proving so difficult because researchers are seeking shared susceptibility genes in a group of people who may share few, if any. And yet without a more refined understanding of genetics, it could be impossible to categorize them any better. \"It may be rare variants, common disease. And that's kind of scary to people because it's much, much harder to find those,\" says Tishkoff. There could be scarier and more intractable reasons for unaccounted-for heritability that are not even being discussed. \"It's a possibility that there's something we just don't fundamentally understand,\" Kruglyak says. \"That it's so different from what we're thinking about that we're not thinking about it yet.\" Still the mystery continues to draw its sleuths, for Kruglyak as for many other basic-research scientists. \"You have this clear, tangible phenomenon in which children resemble their parents,\" he says. \"Despite what students get told in elementary-school science, we just don't know how that works.\" See Editorial,  page 1 , and News Feature,  page 26 . \n                     A Catalog of Published Genome-Wide Association Studies \n                   \n                     The Copy Number Variation (CNV) Project FAQ \n                   Reprints and Permissions"},
{"file_id": "456023a", "url": "https://www.nature.com/articles/456023a", "year": 2008, "authors": [{"name": "Katharine Sanderson"}], "parsed_as_year": "2006_or_before", "body": "Could the next generation of genetic sequencing machines be built from a collection of minuscule holes? Katharine Sanderson reports. DNA sequencing is a technology on the move. In April, 454 Life Science, based in Branford, Connecticut, sequenced the entire genome of James Watson in two months for less than US$1 million 1 . In this issue, Illumina, based in San Diego, California, reports the sequence of a human genome obtained for a quarter of that price and in eight weeks 2 . Companies are positioning themselves aggressively to go further, faster and cheaper. Many consider the ideal technology to be 'single molecule' sequencing, which reads from individual DNA fragments without the need for amplification and the risk of introducing errors. Pacific Biosciences, based in Menlo Park, California, has placed itself centre stage, promising to deliver such a service by watching enzymes build DNA base by fluorescently tagged base. But the single-molecule technology that the US National Human Genome Research Institute (NHGRI) in Bethesda, Maryland, has invested most in is nanopore sequencing, in which DNA is read as it threads through a tiny hole. The technique has received $40 million of a total of $68 million spent in the institute's drive to generate human genomes for $1,000. $4.2 million of that went to Hagan Bayley, a chemical biologist at the University of Oxford, UK, to back research that forms the basis of Oxford Nanopore Technologies, the company he founded, and the one that is closest to making a working nanopore sequencer. Jeffrey Schloss, NHGRI programme director of technology development, says that nanopore sequencing is the only method the institute has supported so far that has the potential to sequence DNA directly from cells without amplification, modification or use of expensive reagents such as fluorescent tags. Oxford Nanopore Technologies's chief executive Gordon Sanghera says that he would like his technology to \"dominate the world, ultimately\". But Sanghera faces stiff competition. Pacific Biosciences, and Complete Genomics in Mountain View, California, are just two of the companies that have announced their ambition to become the chief provider of genetic sequencing. There is still scepticism in the scientific community about whether nanopore sequencing can deliver, says Schloss, and there is a simple reason: \"Pacific Biosciences and Complete Genomics have both sequenced some DNA. Nanopores have not.\" One of the first suggestions that nanopores could form the basis for DNA sequencing came in 1996, when a team led by Daniel Branton, a biophysicist at Harvard University, showed that the presence of DNA could be detected as it passed through a pore by the interruption in the flow of ions through the aperture 3 . The pores, made from a ring of seven \u03b1-haemolysin membrane proteins, are the same as those that the infectious bacterium  Staphylococcus aureus  pushes into the membranes of other cells in order to create damaging holes. Branton's result suggested that the identity of each of the four bases traversing the hole might be revealed by distinctive changes in ion flow, which can be read as an electrical signal.  \n                From small beginnings \n             Bayley and Sanghera founded the company in 2005 to develop nanopores as sensor systems for DNA and other molecules, but the company quickly decided to focus on DNA sequencing. Bayley provided 20 years of experience studying nanopores and Sanghera, who had previously worked for Abbott Laboratories, the business know-how. Of the $35 million that has been raised to finance the company, all from private and institutional investors, $20 million came in a financing round in March this year. In 2006, Bayley showed that the distinction between bases could be made when each was held in place in the nanopore for long enough 4 . \"The breakthrough is that one free nucleotide gives a distinguishable signal,\" says Tim Harris, from the applied physics and instrumentation group at the Howard Hughes Medical Institute's Janelia Farm Research Campus in Ashburn, Virginia. DNA cannot, for now, be run continuously through the nanopore, partly because of the need to hold each base in the pore long enough to disrupt the flow of ions. So, to do their sequence detection, Bayley's group has used genetic engineering and chemistry to make two alterations. At the pore's mouth, the team placed an exonuclease, an enzyme that grabs the ends of a DNA molecule from a solution running over the top. The enzyme then severs each base and directs it into the hole (see  graphic ). At the other end of the pore, the group inserted a cyclodextrin plug, a ring-shaped molecule that narrows the neck. The passing bases have to squeeze through this plug and, as they do so, a phosphate group on the nucleotide briefly binds the cyclodextrin and blocks the pore. Because the bases are different sizes, they sit within the cyclodextrin for different lengths of time, and fill it to different extents, giving characteristic readouts for each base. \n               boxed-text \n             \"The advantage of this technique is, first of all, it's a single-molecule technique, so you don't have to amplify or clone your DNA,\" says Bayley. There are no fluorescent tags and, in theory, minimal sample preparation. \"Also you're directly sequencing the genomic DNA, so, in principle, as well as just getting the four bases you should be able to get modified bases,\" Bayley adds. Oxford Nanopore Technologies says it has unpublished data showing that the system can better discriminate between the four bases and detect 5-methylcytosine, a chemically altered version of cytosine that is commonly involved in gene regulation. In May this year, the company decided that its technology had advanced far enough to announce its intention to develop a next-generation sequencing system. The company had also been quietly vacuuming up the intellectual-property rights from some of the leading nanopore research teams, signing licensing deals with leaders in the field such as Branton, and David Deamer and Mark Akeson at the University of California, Santa Cruz. \"They're eliminating their competition,\" says Harold Swerdlow, head of sequencing technology at the Wellcome Trust Sanger Institute in Cambridge, UK.  \n                Key questions \n              The part of the project that the company is reluctant to talk about is the bit that everyone most wants to know: how this will be scaled up into a working, multichannel sequencer. How many working pores could be used in parallel, and how quickly would it sequence a DNA strand? And crucially, when will sequencing data be made available? Early prototypes in the company's lab look far from complete. A ten-square-centimetre chip, capable of holding 128 pores that will sequence different DNA fragments, sprouts plastic tubing that delivers the samples and naked wiring that connects to an electronics box. But those at the company are tight-lipped about the details of the final product, how it might work and when. They say that they do not want to oversell themselves by making a specific prediction that they will do  X  in  Y  time, and then disappointing or surpassing those expectations. \"There's a danger for a company like this to come out too soon,\" Sanghera says. \"It's a very difficult commercial strategy.\" (see  'ACGT spells hype' ). Swerdlow is talking with all of the new companies. \"It's quite difficult to decide who's telling the truth,\" he says, \"It's all hearsay to some extent.\" He remains optimistic but unconvinced about Oxford Nanopore Technologies. His concern is whether the reagents needed to run a sequence might break down the biological pore in some way. \"I do think that there is some scepticism about direct nanopore sequencing,\" says Barrett Bready, chief executive of sequencing start-up NABsys in Providence, Rhode Island. He says this scepticism is based on the inherent difficulty of the problem. \"The four bases actually differ by only a few atoms. These differences must be detected in the face of noise from various sources.\" NABsys, formed in 2004 by Xinsheng Sean Ling, a physicist at Brown University in Providence, is also pursuing nanopore sequencing, but seems to be further from a working machine than its Oxford rival. In 2007, Ling and John Oliver, another NABsys scientist, received two NHGRI sequencing grants worth $1.32 million in total. The method is based on a silicon chip dotted with synthetic nanopores. Through these pores pass 100,000-base-long fragments of genomic DNA that have six-base-long probes attached to them at intervals. The method uses a library of probes, each having a different, but known, sequence. As the DNA passes through the pore, the points at which a probe is attached can be detected from the current in the chip. The time gaps between those current readings allows the location of the probes to be determined. Once lots of fragments are probed in this way, a picture of the entire genome can be put together from these sequences. But Bayley is dubious. \"You can engineer [proteins] with angstrom precision, which you simply can't do with a pore in plastic or silicon nitride at this point.\" And Harris says that NABsys's sample preparation, which involves reengineering the DNA, is clunky. \"This seems like an improbably gymnastic sample process for something that has to be fast, and essentially free.\" George Church, a molecular geneticist at Harvard University, whose work has also been licensed by Oxford Nanopore Technologies, thinks that the sequencing race will be won by whichever company has the lowest instrument cost and the highest throughput per instrument. Sequencing methods that rely on a digital camera to record colour changes from fluoresently tagged bases \u2014 such as Pacific Biosciences' technology \u2014 are winning that race over nanopores, he says. \"Digital cameras are capable of collecting millions of bits of information at close to the maximum data-flow rate that a PC can handle.\" The cost of these cameras has dropped because of huge consumer use, says Church. \"It does not seem to be a similar case for massively parallel ion-channel monitors.\" Schloss says that the NHGRI views nanopore sequencing as a long-term goal. \"We expected, when we launched the programme in 2004, that it might well take ten years to achieve the goal of using nanopores for sequencing DNA.\" Sanghera has no such reservations. \"Our products are going to be so good that we're just going to let the technical data speak for itself. All things will flow from that.\" \n                 See Editorial, \n                 page 1 \n               \n                     Nature Genetics \n                   \n                     Nature Biotechnology \n                   \n                     Nature Reviews Genetics \n                   \n                     Oxford Nanopore Technologies \n                   \n                     Pacific Biosciences \n                   \n                     The Sanger Centre \n                   \n                     Complete Genomics \n                   Reprints and Permissions"},
{"file_id": "457018a", "url": "https://www.nature.com/articles/457018a", "year": 2008, "authors": [{"name": "Jeff Kanipe"}, {"name": "Lynette Cook"}], "parsed_as_year": "2006_or_before", "body": "The next 40 years will see telescopes that far outstrip any ever seen before. Jeff Kanipe profiles four of them; illustrations by Lynette Cook. The armillary and astrolabe are now seldom seen outside museums and antique shops; but the telescope, which joined them in the observatories of early modern Europe 400 years ago, is still at the centre of the astronomical world. In optical precision, in the wavelengths that are used and in their sheer size, they have changed almost beyond recognition (see  page 28 ). After two centuries in which they left no records other than the users' sketches, and a century in which their visions were recorded on photographic plates, they have in the past decades become entirely electronic. And they are now stationed everywhere \u2014 oceans, deserts, mountain tops and all kinds of orbit. But the job is still the same: collecting and focusing whatever information the Universe sends our way. Yet for all its glorious 400-year history, the astronomical telescope's best days may still be to come. Telescopes currently in development show an unprecedented degree of technical ambition as they seek to provide near-definitive answers to questions that, a generation or two ago, it was hard to even imagine investigating. To answer these questions, the telescopes profiled here (see  slideshow ) will often work in complementary ways. The infrared capabilities of the James Webb Space Telescope and the radio acuity of the Square Kilometre Array will both be used to probe the Universe at the time of its own 'first light' \u2014 the birth of the first stars and galaxies. The radio array will map the large-scale structure of the Universe, elucidating the role in that structure of 'dark matter' and 'dark energy', as will studies of the faintest galaxies by the Large Synoptic Survey Telescope and European Extremely Large Telescope. That behemoth and the orbiting Webb will, in turn, complement each other in their attempts to characterize planets around other stars with unprecedented detail. This quartet, for all its ambition and expense, does not exhaust the possibilities being explored and wished for. The Atacama Large Millimeter/Submillimeter Array will soon revolutionize astronomy at its chosen wavelengths. Other projects are planned throughout the electromagnetic spectrum and beyond into the new realms of gravitational waves and neutrinos. These instruments are all being designed with specific scientific challenges in mind. But at the same time, all concerned hope devoutly to discover something as strange and unlooked for as Galileo's mountains on the Moon \u2014 or spots on the face of the Sun. \n                The James Webb Space Telescope \n             Collection area:  33 square metres  First light:  2013  Cost:  US$4.5 billion all in  Unique selling point:  The best infrared possible Like the Hubble Space Telescope, to which it is in some ways the successor, the James Webb Space Telescope (JWST) will be the orbital flagship of its generation. But whereas the Hubble sees mainly in the visible and ultraviolet, JWST is optimized for the infrared. That means it can see things hidden from the Hubble and its like by dust, and peer into the high-red-shift epoch just after the Big Bang at objects indiscernible at visible wavelengths \u2014 such as the first stars. Astronomers at the Space Telescope Science Institute in Baltimore, Maryland, started their first plans for a follow-on instrument in 1989 \u2014 a year before the Hubble itself was launched. It should finally make it to the launch pad 24 years later. Although its design and cost have changed a few times over the past two decades (see  _Nature_ 440, 140\u2013143; 2006 ), its main mission remains simple and visionary \u2014 to study unseen aspects of every phase of the history of the Universe. To do so, the telescope will make use of several innovative technologies, such as ultra-lightweight optical systems made from beryllium, extremely sensitive infrared detectors and a cryocooler that can maintain the mid-infrared detectors at a frosty 7 kelvin indefinitely. The most striking of the new technologies, though, affects the very heart of the telescope. JWST's designers wanted a mirror that would have been too large to fit into the payload fairing of any rocket available. So they designed one in segments, a mirror that could be launched folded up and then deployed to its full 6.5-metre diameter once the telescope settles into its final orbit, 1.5 million kilometres from Earth. That distance gives the telescope much more sky to look at than the Hubble gets, and keeps it cooler, too. But it has its downside: as yet there is no way to get there to fix any problems so, unlike, the Hubble, JWST has to work perfectly from the start. At the moment, says John Mather, Nobel laureate and senior project scientist for JWST, the telescope is designed to last for at least five years, but longer may be possible. It will carry ten years' worth of fuel, and the presence of the cryocooler means that, unlike earlier infrared missions, its lifetime is not limited by a fixed supply of coolant. \"If we are lucky and clever we hope to conserve fuel and perhaps run much longer,\" says Mather. \"But we can't promise that.\" What Mather thinks he can promise is discovery. \"We do not know which came first, black holes or galaxies, and we do not know how it happens that there is a massive black hole at the centre of almost every massive galaxy. If there are any surprises about the early Universe, I am guessing that they will be in these areas.\" JWST is not just about deep space and distant epochs, though; it will also scrutinize the shrouded origins of objects closer to home \u2014 such as nascent solar systems, coalescing stars and star clusters amassing within dusty nebulae, says Matt Mountain, director of the Space Telescope Science Institute. But where the telescope will really stand out will be its ability to probe the very early Universe. \"JWST is so sensitive,\" says Mountain, \"that we can take actual spectra of the very earliest objects you can just barely detect with Hubble.\" \n               A deep, moving image \n             \n                The Large Synoptic Survey Telescope \n             Collection area:  35 square metres  First light:  2015  Cost:  $390 million to first light  USP:  All of space, in real time Sometimes telescopes see double not because of any aberration, but because that is the way the Universe works. The bending of light by intervening masses \u2014 called gravitational lensing \u2014 means that some galaxies are seen by Earthly observers in more than one place. By adding together survey image after survey image, and so measuring things that no individual image would show, the designers of the Large Synoptic Survey Telescope (LSST) hope to find a significant fraction of the 10,000 or so such images in every square degree of sky. They also hope to open up a neglected dimension in astronomy: time. As well as adding together images of the same part of space taken again and again to reveal new depth, they will compare those images to spot any differences, turning up a wealth of supernovae, asteroids and Kuiper belt objects on the fringe of the Solar System that would otherwise be missed. The telescope's proponents call it celestial cinematography. The telescope will suck in celestial data by the terabyte every night, surveying almost all of the sky visible from Cerro Pach\u00f3n, Chile, every week. Such coverage is made possible by an 8.4-metre primary mirror, which will be ground so as to provide a field of view of 10 square degrees. That's 49 times the size of the full Moon, and more than 300 times the field of view of the Gemini telescopes, which have mirrors of similar size optimized for staring in a single spot. Over ten years, says \u017deljko Ivezi\u010d, of the University of Washington in Seattle, the LSST system will look at everything in its field of view about 1,000 times. A massive amount of computing power will be used to correlate, compare and catalogue the torrent of data \u2014 and to make them all available on the Internet. Anyone with a computer \u2014 students, and amateur and professional astronomers \u2014 will be able to participate in the process of scientific discovery. Studies of objects that have been gravitationally lensed should reveal huge amounts about the structure of the Universe in general, and the distribution of dark matter and the effects of dark energy in particular. At the same time, though, LSST will mount a virtual space patrol, looking for potentially hazardous near-Earth asteroids. Astronomers already know where most of the big, killing-off-species-wholesale asteroids are. LSST will be one of the tools that catalogues the vast majority of lesser asteroids still capable of smashing a city. But with a sensitivity to faint, transient events 1,000 times greater than ever previously achieved, the telescope will not restrict itself to the 'vermin of the skies' in Earth's backyard. It will observe vast distant cataclysms, such as collisions between neutron stars, and is all but sure of discovering whole new categories of transient events. The project is overseen by the LSST Corporation, comprising more than 100 scientists and two dozen laboratories, universities and institutes based mainly in the United States. Although the project's design is still being worked out, the main mirror has already been cast. Astronomers with the corporation are hopeful that construction will begin as planned in 2011 and that first light will occur in 2015. In the subsequent ten-year survey, LSST will take stock of every object in the Universe, known, unknown and newly discovered. \"For the first time in history,\" says Ivezi\u010d, \"we will catalogue and study more celestial objects than there are people on Earth.\"  \n                Follow up around the world \n              LSST will be able to send out alerts on the Internet within a minute of seeing something untoward in the sky. The Las Cumbres Observatory Global Telescope will be eager to hear them. When completed, this will be a network of robotic telescopes distributed in two rings, one circling each hemisphere. Currently the network \u2014 a privately funded, non-profit organization \u2014 has just two telescopes up and running, one on Haleakala, Hawaii, and another in the Siding Spring Observatory in New South Wales, Australia. More are planned for sites in Mexico, the Canary Islands, Chile, South Africa and Australia. The final goal is to have two dozen or so 0.4-metre telescopes and a similar number of 1-metre telescopes. The smaller ones will be skewed towards educational use, the larger ones towards science, but there will be a lot of overlap. Once fully operational, the network will be able to keep a constant eye on new-found objects, such as supernovae and new asteroids, for days or weeks. \n               Supersizing the heavens \n             \n                The European Extra Large Telescope \n             Collection area:  1,385 square metres  First light:  2010s  Cost:  $1.4 billion to first light  USP:  The clue is in the name Space-based observatories have much to recommend them; they see in wavelengths forever hidden from observers on Earth's surface, and with the crystal clarity of an airless sky. But they are hard put to compete in one of the most important aspects of telescope design: size. Whether you want a telescope for a backyard or a mountain top, the larger the light-gathering surface, the more photons you will catch. With a primary mirror measuring 42 metres across, the European Extremely Large Telescope (E-ELT) is designed to pick out features only hinted at in the best images made by orbiting telescopes, detecting objects as faint as the planets around other stars. Its backers think that such a telescope could put satellites out of the optical-and-near-infrared business until similar sized behemoths can be assembled in orbit \u2014 or on the Moon. Today E-ELT, a project of the European Southern Observatory (ESO), is halfway through a 3.5-year, \u20ac57.2-million ($72.5-million) detailed design phase. Construction could start as soon as mid-2010. It is not the only such giant planned: the Thirty Meter Telescope is being built by a public\u2013private consortium including universities in North America, and the Giant Magellan Telescope is a collaboration between several US and Australian universities and research institutes (see  _Nature_ 452, 142\u2013145; 2008 ). But E-ELT is the most audacious in scope and, with a budget of \u20ac950 million, the most expensive. That said, the current E-ELT design is a scaled-down version. When originally discussed as the OverWhelmingly Large Telescope (OWL), it was to have had a mirror 100 metres in diameter. The current 42 metres is more pragmatic. But if adaptive optics that constantly correct the mirror's shape to offset atmospheric turbulence work for a system of this size, as its designers hope, E-ELT should still have a spatial resolution 18 times better than that of the Hubble Space Telescope. Today's largest mirrors are those of the twin 10-metre Keck telescopes on Mauna Kea; E-ELT would have a light-gathering area almost nine times larger than both of them put together. When it comes to thinking big, E-ELT's designers have the advantage, not shared by astronomers in the United States, of steady annual funding. ESO's management hopes to build the giant without asking the organization's 13 member states for more money, or having to hunt around for new partners. That could still change though: the telescope's optical design is not finalized, nor is its ultimate location. The Canary Islands, Morocco, Argentina and two sites in Chile are all under discussion. All this is just background noise to Roberto Gilmozzi, E-ELT's principal investigator, who already has his eye on the telescope's potential discoveries. \"One of the key goals of the telescope will be the quest for Earth-like planets around other stars,\" he says, \"both by indirect methods \u2014 such as measuring radial velocity variations in the parent star \u2014 and by direct imaging and spectroscopy of their atmospheres.\" It will also aim to study proto-planetary systems around young stars in unprecedented detail. \"E-ELT will provide a 'quantum jump' in sensitivity over the present generation of telescopes \u2014 comparable to that of Galileo's telescope over the naked eye,\" says Gilmozzi, and that will affect astronomy at all scales. Galaxies that are now blurs will turn into fields of distinct stars. And on the largest of scales the telescopes will carry out a \"physics experiment\" that would not be possible with any other telescope, Gilmozzi says. By comparing ultra-precise velocity measurements of very distant objects taken several years apart E-ELT will make the first direct observations of how fast the Universe is expanding.  \n                Other ways to other planets \n              The international COROT mission, launched in 2006, has spent two years trying to detect dips in the brightness of stars caused by intervening planets. Later this year it will be joined by NASA's Kepler mission, and a follow-on European mission, PLATO, is planned for the middle of the next decade. All aim to discover Earth-sized and smaller planets, particularly in or near the 'habitable zones' around stars where liquid water is possible on the surface. E-ELT might be used to confirm and follow up the findings of such missions \u2014 perhaps making spectroscopic measurements of a newly discovered planet's atmosphere, for instance. The combination of detection from space with such satellites and study from Earth with a new generation of very large telescopes such as E-ELT means that astronomers will be able to start putting together profiles of other planetary systems, some of which may be similar to the Sun's, some of which will doubtless prove very different. \n               Looking everywhere \n             \n                The Square Kilometre Array \n             Collecting area:  Up to 1 million square metres  First light:  2020s  Cost:  $1.4 billion or more  USP:  All things radio, better It hasn't yet got a site; it hasn't yet got funding. But what the Square Kilometre Array (SKA) lacks in these pragmatic matters it more than makes up for in ambition: it will be the largest telescope of any kind in the world. Today's biggest single-aperture radio telescope, the Arecibo Observatory in Puerto Rico, has just 73,000 square metres of collecting area. SKA is aiming for a million. This collecting area will be distributed between thousands of small dishes, some grouped together in a central core, some trailing out along a vast crop-circle-like, multi-armed spiral to provide a total radius of a couple of thousand kilometres. That vast radius will give the system a resolution to match the incredible sensitivity provided by all that collecting area. The result is a telescope that works over a frequency range of from 70 megahertz to 10 gigahertz, and boasts a huge field of view \u2014 200 square degrees at frequencies of under 1 gigahertz, 1 square degree or so at higher frequencies. The telescope's final design will incorporate dishes for higher frequencies and flat 'phased-array' panels for the lower frequencies. An advantage of the phased-array approach is that observers can study different parts of the sky simultaneously. This design will allow the instrument to survey the sky 10,000 times faster than any radio telescope producing similar images. Research topics will cover more or less the whole reach of astronomy, from galaxy evolution and the cosmic jets spewed out by quasars to the study of pulsars and supernova remnants. The array is a truly international collaboration between institutions in 19 countries, including much of Europe, Argentina, Australia, Brazil, Canada, China, India, South Africa, Sweden and the United States. The consortium members hope to see it completed in 2020, and to keep producing science for 50 years. Taking into account the all-but-inevitable delay, later upgrades and the like, SKA could well last long enough to be the first great telescope of the twenty-second century. According to James Cordes, chair of the US SKA Consortium, SKA will revolutionize understanding of the first galaxies, and of how the Universe developed its 'metals' (astronomer speak for all elements heavier than helium). Previous work has shown that just half a billion years after the Big Bang, galaxies were awash in carbon monoxide gas that could only have come from earlier stars. The array will be able to map that spread, and to measure CO molecules in the interstellar media of 'pregalactic objects' \u2014 the seeds of galaxies. But like everyone with a big telescope that they have fallen in love with, Cordes is also keen to stress the capacity for surprise inherent in the object of his affection. SKA, he says, will be a discovery machine. Who knows what it will see that lesser eyes on the sky have missed: \"Perhaps extrasolar planets and flare stars of new types. Perhaps even exotica \u2014 such as evaporating black holes and extraterrestrial civilizations.\"  \n                Back to space \n              Earth-based telescopes find it relatively easy to grow to a giant size; in space things are more constrained. But astronomers at the Space Telescope Science Institute in Baltimore, Maryland, who are looking for the next big thing are thinking that it should be exactly that: big. The Advanced Technology Large-Aperture Space (ATLAS) telescope is a proposed telescope that could only be launched on NASA's planned Ares V heavy-lift launch vehicle, the humans-to-the-Moon launcher that will be the largest rocket since the days of Apollo. ATLAS would have a larger mirror than the largest Earth-based telescopes today, with a diameter of perhaps 16 metres and a sensitivity some 40 times that of the Hubble telescope. It would provide yet another set of perspectives on dark matter and galactic evolution. Perhaps most exciting, though, would be its role in characterizing habitable planets \u2014 perhaps even inhabited ones \u2014 around other stars.\n Jeff Kanipe is a science writer based in Maryland. He is the author of  The Cosmic Connection  Lynette Cook is an artist and illustrator based in California.See Editorial,  page 7 , and  online  for more. \n                     Focus on the year of astronomy \n                   \n                     The James Webb Space Telscope \n                   \n                     The Large Synoptic Survey Telescope \n                   \n                     The European Extremely Large Telescope \n                   \n                     The Square Kilometre Array \n                   \n                     Las Cumbres Observatory Global Telescope \n                   \n                     The COROT mission \n                   \n                     The Kepler mission \n                   \n                     Thirty Meter Telescope \n                   \n                     Giant Magellan Telescope \n                   \n                     The Atacama Large Millimetre/Submillimetre Array \n                   \n                     Laser Interferometer Space Antenna \n                   \n                     IceCube Neutrino Observatory \n                   \n                     The Cosmic Connection \n                   Reprints and Permissions"},
{"file_id": "456026a", "url": "https://www.nature.com/articles/456026a", "year": 2008, "authors": [{"name": "Bryn Nelson"}], "parsed_as_year": "2006_or_before", "body": "Eric Schadt revels in making people uncomfortable with his science. Bryn Nelson reports how the bioinformatics rabble-rouser hopes to charge ahead in the face of his company's disintegration. In need of an escape from the mental gymnastics of hardcore genome analysis, Eric Schadt, executive scientific director of research genetics for Rosetta Inpharmatics, is clear about what works for him \u2014 careering down a steep mountainside on a snowboard. \"You can't sort of ease your way down a hill,\" he says over breakfast near the company's headquarters in Seattle, Washington. \"It's either all, or nothing.\" That fearless approach may be tested after a bombshell announcement last month. Rosetta, which has been on a head-turning run for most of the past decade, now finds itself in mid-air, hoping to make a landing that could be very tricky indeed. Merck & Co., which bought the biotech company in 2001 and operated it as a subsidiary, announced on 22 October that it will close down most of the Seattle operations by December 2009, transferring Schadt and a number of his team to its Boston research centre. The move comes as part of a global reorganization in the face of slumping drug sales, and includes cutting more than 7,200 jobs worldwide. It could be worse. Reni Benjamin, a senior biotechnology analyst and managing director for investment bank Rodman & Renshaw in New York, says that given the current economic climate for pharmaceuticals, subsidiaries such as Rosetta have no guarantee of survival. \"They could just cut the entire thing and call it a day if they didn't think it was important.\" What's likely to save Rosetta from complete oblivion is Schadt's trend-setting science of integrated genomics, which uncovers disease mechanisms by revealing vast networks of gene interactions. Genome-wide association studies, which have been a favoured technology for finding disease-linked genes over the past several years, seek out associations between a disease state and a genetic variant. Although the studies have turned up hundreds of variants associated with disease, they can detect only independent effects of individual genes, which means they might miss a lot (see  page 18 ). Like genome-wide association studies, Schadt's approach tries to correlate variations in DNA with some observable complex trait, such as a disease, in a population. But Schadt and his group add a third factor: gene-expression levels, as measured by microarrays. They then use the data to build models of how the three factors \u2014 variations in the genotype, variations in the disease, and variations in gene expression \u2014 are related. Some relations are straightforward: a gene variant has a direct effect on expression, and that has a direct effect on the disease trait. But it is also possible for a particular genetic variation to be linked to a disease trait that, in turn, alters the expression of some other gene. And then there are cases in which a genetic variation is linked to both a disease and an unrelated change in gene expression. The complexity of all this goes through the roof when genes start interacting: the models explode into networks of interconnected elements. But these network models allows Schadt and his colleagues to identify and validate associations between genes and disease \u2014 and to predict how perturbing the system, with a drug or genetic mutation, will affect expression and disease.  \n                Success story? \n              So far, company officials say, the strategy has worked well. Of the 52 compounds in Merck's clinical-trial pipeline in 2006, ten entered through Rosetta's efforts. Now, according to Stephen Friend, a senior vice-president at Merck and Rosetta's co-founder, the approach is so integrated, it's hard to tell what originated from Schadt's team. Something that important is not going to be axed just to control costs, says Benjamin. \"If 20 to 25% of a company's pipeline is being generated from one particular platform, you would have more significant clues than 'streamlining operations' if they didn't like what they were seeing.\" On Rosetta's fourth floor, there's all sorts of stuff that may not survive the move east. A cheeky monument built from lab equipment including discarded flasks, racks and pipettes, entitled 'Don't Mess With Us, We're Scientists', sits near display cases showing off the company's early innovations such as its microarray spotter prototypes. One recent acquisition is likely to stay on the packing list. The Hamilton MicroLab Star, a custom-designed platform hosting three interconnected robots, represents the next generation for gene-expression profiling, offering a nearly seamless start-to-finish automated process. Upstairs, John Dey, the company's UNIX operations manager, lifts a floor panel to reveal a sea of blue cables and a rough visual gauge of the computing power housed within the company's nerve centre. The cluster began with eight computer nodes in 1998. Now it boasts 1,000 connected by more than 16 kilometres of cable. The computational muscle in Boston will have to be bigger still. Dey shakes his head when asked whether he's confident about keeping up with storage demands. \"Oh, it's going to explode,\" he says. Schadt's team needs this state-of-the-art equipment for what Schadt calls one of the biggest looming bottlenecks for biology in the next ten years: understanding the effects of many genes interacting at once. For genome-wide association studies, the question is fairly straightforward from a computational perspective. But testing biology's dizzying network of interactions in a holistic way, Schadt says, requires the computational power generally reserved for climate modelling and astrophysics. This aggressive approach rightfully has made specialists uneasy. Friend says integrated genomics met with an initial 'show me'-style distrust. With any new technology, he says, an evangelical wave first oversells it and turns almost everyone into a sceptic. Schadt clearly revels in proving the critics wrong. In May, Rosetta led a study of gene expression in the human liver and found more than 6,000 associations between single nucleotide polymorphism genotypes and gene expression traits 1 . Although many of the expressed genes were already implicated in human disease, other connections were newly exposed as suspect. The study's integrated data, for example, pointed to  RPS26  instead of the previously favoured  ERBB3  as the most likely susceptibility gene in a genomic region associated with type 1 diabetes. The layering approach by which Rosetta constructs a complex network has garnered its own share of detractors. \"To some of these naysayers, you have a big magic black box where you pour in everything and out comes a drug target, and that doesn't sound like science,\" says Trey Ideker, an associate professor of bioengineering at the University of California, San Diego, who is collaborating with Schadt on a review detailing the use of systems and network modelling for drug development and health care. \"But if you look under the hood, what Eric is doing is absolutely sound \u2014 it's the sheer complexity that is overwhelming.\"  \n                Friends reunited \n              Schadt often winds his critics up, of course. \"The network stuff still makes a lot of people uneasy,\" he says. \"People don't want things to be that complicated.\" He smiles as he recalls his admittedly \"inflammatory\" talks a few years ago, in which he basically told other scientists: forget genes and focus on what genetic perturbations are doing to the whole system. \"Human geneticists just hated it when I would say stuff like that,\" he says. \"You know, their whole lives were, 'What the heck is that gene?' and my whole point was, 'Well, first of all, most of those genes aren't even druggable'.\" Most of Rosetta's methods have all but ignored the question of gene identity initially and instead tracked disease-associated hiccups in a genetic network. The approach led to the 2005 publication in  Nature Genetics  of a study laying out the general integrative strategy \u2014 something Schadt counts among his proudest accomplishments 2 . \"It wasn't just the intellectual satisfaction, it was that everybody \u2014 nearly everybody \u2014 was saying, 'Nah, it's not going to work',\" he recalls. He looks back almost wistfully on those earlier fights. \"Because our work has got so successful now, I don't feel that people push back enough,\" he says. \"It's almost too accepting.\" The troubled economic landscape could provide plenty of pushback to Schadt's resource-intensive approach, but he seems unfazed by unanticipated changes. Unplanned course corrections have defined his past. Raised in rural Michigan by a conservative family that placed little value on education, Schadt joined the US Air Force after high school and enrolled in its elite pararescue programme, sometimes called 'superman school' because of its gruelling training regimen. During one exercise, Schadt dislocated his shoulder so badly he required surgery, ending his rescue career. His superiors, though, noticed his high test scores and asked about college. It had never occurred to him. With financial assistance from the military, he gravitated towards mathematics and computer science at California Polytechnic State University in San Luis Obispo. \"It just opened up a brand new world.\" It wasn't until he earned his master's degree in pure mathematics from the University of California, Davis, that biology first caught his eye. Schadt began attending biology seminars and met Kenneth Lange, now chairman of the human-genetics department at the University of California, Los Angeles. Lange encouraged Schadt to pursue a curriculum in biomathematics and eventually became his PhD adviser, guiding him to a degree that integrated molecular biology and applied mathematics. \"It just all made sense,\" Schadt says. By the time Merck began courting Rosetta in 2001, Schadt had become a key member of the team behind a seminal annotation of the human genome 3 . Annotation aided by gene-expression microarrays was fast gaining attention, Schadt says, \"and we just thought we were going to make it big\". The company, in fact, was on the verge of agreeing to a lucrative contract to expand on its annotation work. \"The day we were supposed to sign that deal, Stephen Friend comes in and says, 'We're not going to sign',\" Schadt recalls. \"So I said, 'Do you have a better $20-million deal?'\" Merck ended up buying Rosetta in a stock swap valued at about $620 million. Next year, Schadt and Friend will be reunited in Boston, where Merck is hoping for a better synergy by co-locating its molecular-profiling and integrative-genomics research with Friend's oncology group, as well as the researchers working on bone and respiratory diseases, arthritis and analgesia. \"By providing a more comprehensive view of the numbers of genes that may be causally related to disease,\" Schadt says, \"we can actually use the networks to prioritize those targets.\" Identifying and halting a drug programme headed for trouble can be just as important, and Schadt cites the  Ppm1l  gene as a perfect example. In March, his team found an obesity connection for  Ppm1l  \u2014 which encodes a newly discovered but poorly characterized protein phosphatase \u2014 and two other previously unlinked genes. But the group found that  Ppm1l  also modulated traits for diabetes and high blood pressure 4 . \"What we showed is when you knock the gene down, you improve your diabetes profile or you become less insulin resistant, but you also gain weight.\" Even worse, knocking down the gene also increased blood pressure. Making someone heavier and with higher blood pressure in exchange for a lower diabetes risk, the company concluded, wasn't worth the trade-off \u2014 something it might have missed by taking a narrower focus. Thomas Gingeras, head of functional genomics at Cold Spring Harbor Laboratory in New York, says Schadt should be commended for embracing a systematic approach to teasing out the molecular networks in cells. But he worries about the initial downside to such an ambitious plan. \"The concern I have is whether the information we have to be able to put together this network is patchy and, in some cases, unreliable; and, even if it is accurate, whether we have the right interpretation of those data,\" he says. By focusing on the low-hanging fruit within the network, might researchers be losing sight of more important non-coding elements that aren't yet within reach?  \n                A legacy lives on \n             Schadt is confident that his team's models can adapt as a wider range of information comes online, including forthcoming studies that will incorporate data from genome-wide screens of small-molecule metabolites and protein\u2013protein interactions. He's particularly enthusiastic about several big pilot projects that are resequencing entire transcriptomes for hundreds of mice and humans. They offer a way to ask whether largely unknown non-coding RNAs may be central players in the protein-coding network. Ritsert Jansen, a bioinformatics expert at the University of Groningen in the Netherlands, says Schadt's ability to use high-throughput screens and work with large patient populations is a big step closer to explaining the causality of complex traits. Jansen, who works on molecular networks in the  Arabidopsis  plant model, says Schadt's work has so far suggested that a few very influential DNA variations are critical for linking genotype with phenotype. The expanding repertoire of 'omics-based studies, including epigenomics, should lead to more dynamic models that zero in on the most important perturbations, he says. One of the next phases in that progression will add in more clinical information from Merck collaborators such as the Moffitt Cancer Center and Research Institute in Tampa, Florida. Every cancer patient giving informed consent will have multiple tissue samples collected and analysed, with an eye towards charting tissue-to-tissue communication networks. And then what? Rosetta's legacy may be in providing Merck with a mastery of biological information and superior predictive models of disease risk, drug development and patient response. But ultimately, Schadt says, consumer genome-testing products, such as those provided by 23andMe in Mountain View, California, Navigenics in Redwood Shores, California, and deCODEme of Reykjavik, Iceland, will take the lead in solving perhaps the problem of the century: translating all that information for the people who need to know what it means, be they doctors or parents. \"The next ten years are going to be an amazing ride to see how this all plays out,\" he says. Despite Schadt's sentiment that \"change is good from time to time\", he concedes that keeping his team together and focused on its research during the move east will be challenging. But his overall mission, he says, remains unchanged. In an e-mail to  Nature  shortly after Merck announced it would close Rosetta and two other research units in Japan and Italy, Schadt wrote: \"It is very gratifying that our work is so highly valued within Merck and that as a result it will become even more integrated within Merck's research.\" The bottom line: \"everything is continuing, we will just be doing it in Boston instead of Seattle\". \n                 See Editorial,  \n                 \n                     page 1 \n                   \n                 , and News Feature,  \n                 \n                     page 18 \n                   \n                  Bryn Nelson is a freelance science writer based in Seattle, Washington. \n               \n                     Nature Network: Applying Systems Biology for Advancing Human Health \n                   \n                     Nature Reviews Cell Biology Web Focus: Systems Biology a User's Guide \n                   \n                     Nature Insight Human Genomics and Medicine \n                   \n                     Rosetta Inpharmatics \n                   Reprints and Permissions"},
{"file_id": "4551023a", "url": "https://www.nature.com/articles/4551023a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "If you want to start an argument, ask the person who just said 'paradigm shift' what it really means. Or 'epigenetic'.  Nature   goes in search of the terms that get scientists most worked up.   This article is best viewed as a  \n                   PDF \n                 .  To a great extent, science is about arriving at definitions. What is a man? What is a number? Questions such as these require substantial inquiry. But where science is supposed to be precise and measured, definitions can be frustratingly vague and variable. Here,  Nature   looks at some of the most difficult definitions in science. Some are stipulative definitions, created by scientists for their convenience, but on which the community has not found consensus. Popular though they are, not everyone agrees on what is meant by 'paradigm shift' or 'tipping point'. Essential definitions \u2014 those that get at the question of what makes a thing a thing \u2014 can be just as troublesome. What is race, or consciousness? And does it even matter if there is no agreed-on meaning? The good news is that for every troublesome term there are thousands used every day with no problems. Scientists are competent, if unconscious wielders of definition, says Anil Gupta, a philosopher of science at the University of Pittsburgh in Pennsylvania, \"just as one can walk quite happily without having a complete account of walking\".   Paradigm shift   ( noun. )  \n                Emma Marris \n             Paradigm shift has a definite origin and originator: Thomas Kuhn, writing in his 1962 book  The Structure of Scientific Revolutions , argued against the then prevalent view of science as an incremental endeavour marching ever truthwards. Instead, said Kuhn, most science is \"normal science\", which fills in the details of a generally accepted, shared conceptual framework. Troublesome anomalies build up, however, and eventually some new science comes along and overturns the previous consensus. Voil\u00e0, a paradigm shift. The classic example, Kuhn said, is the Copernican revolution, in which Ptolemaic theory was swept away by putting the Sun at the centre of the Solar System. Post-shift, all previous observations had to be reinterpreted. Kuhn's theory about how science works was arguably a paradigm shift of its own, by changing the way that academics think about science. And scientists have been using the phrase ever since. In a postscript to the second edition of his book, Kuhn explained that he used the word 'paradigm' in at least two ways (noting that one \"sympathetic reader\" had found 22 uses of the term). In its broad form, it encompasses the \"entire constellation of beliefs, values, techniques and so on shared by the members of a given community\". More specifically it refers to \"the concrete puzzle-solutions\" that are used as models for normal science post-shift. Scientists who use the term today don't usually mean that their field has undergone a Copernican-scale revolution, to the undying annoyance of many who hew to Kuhn's narrower definition. But their usage might qualify under his broader one. And so usage becomes a matter of opinion and, perhaps, vanity. The use of the term in titles and abstracts of leading journals jumped from 30 papers in 1991 to 124 in 1998, yet very few of these papers garnered more than 10 citations apiece 1 . Several scientists contacted for this article who had used paradigm shift said that, in retrospect, they were having second thoughts. In 2002, Stuart Calderwood, an oncologist at Harvard Medical School in Boston, Massachusetts, used it to describe the discovery that 'heat shock proteins', crucial to cell survival, could work outside the cell as well as in 2 . \"If you work in a field for a long time and everything changes, it does seem like a revolution,\" he says. But now he says he may have misused the phrase because the discovery was adding to, rather than overturning, previous knowledge in the field. Arvid Carlsson, of the University of Gothenburg in Sweden stands by his use of the phrase. \"Until a certain time, the paradigm was that cells communicate almost entirely by electrical signals,\" says Carlsson. \"In the 1960s and '70s, this changed. They do so predominantly by chemical signals. In my opinion, this is dramatic enough to deserve the term paradigm shift.\" Few would disagree: base assumptions were overturned in this case, and Carlsson's own work on the chemical neurotransmitter dopamine (which was instrumental in this particular shift) earned him the 2000 Nobel Prize in Physiology or Medicine Unless a Nobel prize is in the offing, it might be wise for scientists to adopt the caution of contemporary historians of science and think twice before using a phrase with a complex meaning and a whiff of self promotion. \"Scientists all want to be the scientists that generate a new revolution,\" says Kuhn's biographer, Alexander Bird, a philosopher at the University of Bristol, UK. \"But if Kuhn is right, most science is normal science and most people can't perform that role.\" (See  Don't get us started .)   Epigenetic   ( adjective .)  \n                Helen Pearson \n              No one denies that epigenetics is fashionable: its usage in PubMed papers increased by more than tenfold between 1997 and 2007. And few deny that epigenetics is important. What they do disagree on is what it is. \"The idea is that there is a clear meaning and that it's being violated by people like me who use it more loosely,\" says Adrian Bird at the University of Edinburgh, UK. Last year he suggested this as a definition: \"the structural adaptation of chromosomal regions so as to register, signal or perpetuate altered activity states\" 3 . But this wide-ranging proposal, which takes on-board pretty much every physical indicator of a gene's activity is \"preposterously dumb\", says Mark Ptashne of Memorial Sloan\u2013Kettering Cancer Center in New York, who has published his own take on the word's usage 4 . \"I've grown to be very careful about using the term,\" says Bing Ren, who studies gene regulation at the University of California, San Diego. According to the 'traditional' definition that Ptashne favours, epigenetics describes \"a change in the state of expression of a gene that does not involve a mutation, but that is nevertheless inherited in the absence of the signal or event that initiated the change\". The classic example is found in a bacteriophage called Lambda, which stays dormant in the genome of generations of cells through inheritance of a regulatory protein. These sort of processes are basic to some of the most pressing questions in biology today: such mechanisms are needed to explain how a single-celled embryo can generate cells that are genetically identical, but express a different array of genes and hence take on different jobs in blood, brain or muscle for generation after generation. Over the past few years, however, all kinds of processes associated with gene control have been subsumed under the moniker. For example, 'epigenetic' is often used to refer to the chemical modification of histones \u2014 proteins that DNA winds around \u2014 which is involved in gene regulation. This infuriates those who learned the classical definition; they say it puts these modifications at the heart of development and disease despite scant evidence that they are inherited. \"Why did histone marks become epigenetic?\" says Kevin Struhl at Harvard Medical School in Boston. \"People decided that if they call them that it makes them interesting.\" Others say that it is not about making things sound important, it is more the lack of any other phrase with which to collectively refer to this type of work. The word had dual meanings long before the current debate. In the 1940s, Conrad Waddington used it to describe how the genetic information in a 'genotype' manifests itself as a set of characteristics, or 'phenotype'. In 1958, David Nanney at the University of Michigan, Ann Arbor, borrowed the term to describe \"messy\" inherited phenomena that could not be explained by conventional genetics 5 . \"It was controversial in 1958 and everything died down and it has come alive again,\" says Nanney. \"It took 40 years for epigenetics to become a major word in the vocabulary of modern biology.\" A lot of money can ride on whether a researcher is, or is not, studying epigenetics: the US National Institutes of Health (NIH) this month started handing out US$190 million as part of its epigenomics initiative and other countries are pouring funding into the area. (The NIH is careful to define the epigenetics it is paying for as including both heritable and non-heritable changes in gene activity, something that Ptashne describes as \"a complete joke\".) Bird says he remains in favour of a relaxed usage. \"Epigenetics is a useful word if you don't know what's going on \u2014 if you do, you use something else,\" he says.   Complexity   ( noun. )  \n                M. Mitchell Waldrop \n             In his book  Programming the Universe , engineer Seth Lloyd of the Massachusetts Institute of Technology in Cambridge describes how he once compiled 42 definitions of complexity \u2014 none of which encompasses everything people mean by that word. Researchers in the many institutes and programmes formed to study 'complexity' are still searching for the right way to describe their discipline. \"If we're a university centre, we should be able to say what we care about,\" says Carl Simon, director of the Center for the Study of Complex Systems at the University of Michigan. The quest for a rigorous definition reached a particularly intense pitch in the early 1990s, when some of the more visionary researchers at the Santa Fe Institute in New Mexico held out the hope of a universal theory of complexity \u2014 a mathematically precise set of equations that would hold for all complex systems in much the same way that the second law of thermodynamics holds for all physical systems. James Crutchfield, head of the Complexity Sciences Center at the University of California, Davis, says that this created a problem. \"New people would come into the field and start using the word 'complexity' as if it was a unitary thing\" \u2014 which, as became increasingly clear, it was not. No all-encompassing theory emerged. Even within the precise world of binary code and bit strings, there was computational complexity, which describes how much memory and processing is required to carry out a calculation; algorithmic complexity, which is related to how much a digital description of something can be compressed; and any number of combinations and variations. \"So my bottom line is, add an adjective to 'complexity',\" Crutchfield says. Researchers have found plenty of undeniably complex systems to study, such as economies, ecosystems, urban traffic and brains (see 'Consciousness'). And in a qualitative sense, at least, these systems do have certain features in common that might serve as a definition. They are, for instance, all composed of many independent 'agents' (consumers, species, vehicles, neurons) that are constantly interacting with, and adapting to, one another. They all display a rich array of nonlinear feedback loops among the agents, which means that small changes can have a big effect. And they never quite settle down into static equilibrium. The effort to understand complex systems has led researchers to develop new analytical tools such as network theory, agent-based modelling and genetic algorithms. These tools, combined with the exponential growth in computational power, have allowed researchers to build ever more complex models of complex systems \u2014 and study the subtle but powerful phenomenon of 'emergence,' in which multiple agents exhibit collective behaviour that is a great deal more than the sum of its parts. So even though the field seems little closer to defining its subject, says Lloyd, \"in places where people can apply these conceptual and computational tools, we've made huge progress in understanding complex systems\". But in a world where we are constructing ever more complex artefacts \u2014 technologies, economies, organizations and societies \u2014 even better tools are needed to keep pace.   Race   ( noun. )  \n                Erika Check Hayden \n              If biologists had a list of four-letter words to avoid, then 'race' would be higher up than anything more conventionally vulgar. It is controversial, it lacks a clear definition and the more that genetics reveals about race, the more biologically meaningless the term seems. Race was long used to imply a shared, distinct ancestry, as in a 1936 definition of the term in  Nature : \"It has two main connotations, one being community of descent, the other distinctness from other races.\" But in 1972, Harvard geneticist Richard Lewontin showed that the concept of race starts to dissolve under genetic scrutiny. He found that the vast majority of human genetic variation, which he measured in 16 genes, is found within, not between, what he called the 'classical racial groupings' 6 . Since then, studies examining hundreds or even thousands of genetic markers have confirmed Lewontin's findings 7 ,   8 . A consensus now exists across the social and biological sciences: regardless of appearance or heritage, groups of human beings are overwhelmingly more genetically similar to each other than different. This doesn't mean race does not exist or is meaningless in society \u2014 far from it. But it does mean that an individual's race is not a particularly useful or predictive indicator of biological traits or medical vulnerabilities. Race is \"the social interpretation of how we look, in a race-conscious society\", says Camara Phyllis Jones, the research director on the Social Determinants of Health and Equity programme at the US Centers for Disease Control and Prevention in Atlanta, Georgia. Lewontin says that assigned races are essentially arbitrary. \"It means essentially a group of related people, and where you draw the line depends on where you are in history.\" Some argue that severing biology from the definition of race risks jettisoning medically meaningful information. Patterns of genetic variation can be used to classify people from different geographical regions into clusters that sometimes mimic the classical racial groupings, and geneticists say that members of these groups seem to have distinctive disease prevalences and drug metabolism. So race could serve as a cheap, albeit imperfect, surrogate for defining groups for clinical trials or medical interventions. But genetics is turning up ever more examples of how race obscures relevant information. A study published in April showed that a mutation found in 40% of African Americans acts like an endogenous beta blocker to protect patients with heart failure from death 9 . It also suggested why previous research had found conflicting evidence about the response of African Americans to beta blockers: those studies had lumped all African Americans into one group, obscuring the effects of mutations that confer protection or vulnerability. A person's perception of his or her race can still serve to capture life experiences relevant to behavioural and clinical research, such as the stress of lifelong discrimination that may contribute to health disparities. But in other contexts researchers are abandoning the term in favour of other ways to group humans, by 'population,' genetic ancestry' or 'geographic ancestry'.   Tipping point   ( noun. )  \n                Quirin Schiermeier \n             In July 2006, scientists running the RealClimate blog ironically headlined one of their posts 'Runaway tipping points of no return'. The post laments that usage of the phrase 'tipping point' in climate-change and ecosystem discussions had reached, well \u2014 a tipping point. It's not the frequency of the word that bothers researchers. It's the lack of one clear definition and the confusing way in which the concept is being used, among scientists and in the public discourse, often to imply that global warming-induced changes will propel Earth into irreversible and catastrophic climate change. \"There is no convincing theoretical argument or model that at some point the planet as a whole will snap into a second state of system,\" says Timothy Lenton, an Earth scientist at the University of East Anglia, UK. The term was originally coined in 1958 by sociologist Morton Grodzins in the context of studies on the racial makeup of US neighbourhoods. He found that when the migration of African-Americans into traditionally white neighbourhoods had reached a certain level, whites began to move out. In the 1970s, epidemiologists adopted tipping point to describe the threshold at which, mathematically, an infectious disease's 'reproductive rate' goes above one. This means that each infected person infects more than one other and the disease starts growing into an epidemic. The phrase reached its own tipping point in 2000 when Malcolm Gladwell, a staff writer at  The New Yorker , published his successful book  The Tipping Point: How Little Things Can Make a Big Difference . It also acquired a worrisome \u2014 some say alarmist \u2014 flavour courtesy of its frequent usage in the context of climate change. Regarding climate, the term is commonly defined as the critical threshold at which a slow gradual change qualitatively alters the state of an entire system. This is different to a 'point of no return' which is, by definition, irreversible. Only if internal forcing will cause a runaway effect is a tipping point also a point of no return. The idea that positive feedbacks \u2014 such as the melting of polar ice reducing surface reflectivity, thereby causing yet more solar absorption, warming and melting \u2014 could amplify climate change to a point of fundamentally altering the global system has been around for decades. The debate now is about where those tipping points lie, and what will happen when they are crossed. In a paper published in February, a team led by Lenton looked at 15 potential tipping 'elements' (things that could reach tipping points) in Earth's climate system 10 . Arctic sea-ice and the Greenland ice sheet were those most at risk from 'tipping' within the twenty-first century, the authors concluded. But researchers accept that most known tipping points seem to be reversible on human timescales. Melting of the complete Arctic summer ice sheet, for example, could probably be reversed within a few years or so in a cooler world. Melting of the extremely thick Greenland and Antarctic ice sheets are a possible exception because, once melted, new ice would have to form at lower, warmer altitudes with less snowfall. Claims that global warming could reach an irreversible tipping point by 2016, as made last year by James Hansen, director of NASA's Goddard Institute for Space Studies in New York, refer to the trajectory of greenhouse-gas emissions, not to changes in the climate system. Even if greenhouse-gas concentrations reach a point at which they cannot be restored to pre-industrial levels, it will not inevitably push the world's climate over a catastrophic tipping point.   Stem cell   ( noun. )  \n                Monya Baker \n             Ask a group of stem-cell biologists to define stem cell, and they'll say roughly the same thing: a cell that can, long term, divide to make more copies of itself as well as cells with more specialized identities. Ask the same scientists to list the most disputed terms in the field, however, and 'stem cell' will be top of that list. The problem here is an operational one: reasonable people disagree on which cells qualify under the definition. \"It's not unusual to pick up a paper and see someone call something a stem cell and the evidence that it is, is just not there,\" says Lawrence Goldstein, who directs the stem-cell research programme at the University of California, San Diego. Alleged 'stem cells' can fail to meet the definition on many counts. Stem cells should persist long term, yet many 'stem cells' exist only in the fetus. Multipotency \u2014 the ability to generate multiple cell types \u2014 is a criterion for a haematopoietic, or blood-forming, stem cell, but spermatagonial stem cells only produce sperm. Stem cells specific to tissue such as cartilage, the kidney and the cornea have been reported, with varying degrees of acceptance. The quest for a 'stemness signature', a collection of markers common to all stem cells, has been met with frustration. Debate erupts most commonly over whether a particular cell should be considered a stem cell, which can divide indefinitely, or a progenitor cell, which can differentiate into fewer cell types and is thought to burn itself out after a certain number of divisions. The only way to be really sure of what a cell can, and cannot do, is to observe it, but it is difficult to study cells  in vivo , and putting them in a dish might change their behaviour. Haematopoietic stem cells were the first to be identified and have, to some extent, set default standards. Putative stem cells are isolated, then placed into animals whose own haematopoietic stem cells have been destroyed by radiation. If the blood-forming system is restored, the transplant is assumed to have contained stem cells. But such an assay is impossible when working with other cell types, such as neural stem cells, which are harder to transplant and assess in disease models. And it is difficult to pin the label to one cell type, when studies commonly involve a mixed population. \"It is perhaps not realistic to come up with a generally applicable definition of an adult stem cell,\" says Thomas Graf of the Centre for Genomic Regulation in Barcelona. Some researchers are side-stepping the debate by referring in their papers to 'stem/progenitor cells'. Fully understanding what each cell can do is more important than knowing what to call them all, says Goldstein. \"Some of this just breaks down,\" he says. \"That's biology. It wasn't designed to fit the language.\"   Significant   ( adjective. )  \n                Geoff Brumfiel \n             Few words in the scientific lexicon are as confusing, or as loaded, as 'significant'. Statisticians wring their hands over its cavalier use to describe scientific validity. And backed by statistics or not, researchers commonly employ the word to illustrate the importance of their latest finding. The very definition of statistical significance is misunderstood by most scientists, says Steven Goodman, a biostatistician at the Johns Hopkins School of Medicine in Baltimore, Maryland, and associate editor on  Annals of Internal Medicine . Typically, researchers take a result to be statistically significant based on  'p -values'. This parameter is used, for example, to reveal whether a drug lowers cholesterol based on promising data collected in a clinical trial. According to the common interpretation, a 'significant' result with a  p -value of 0.05 or less means that there is a 5% or less chance that the drug is ineffective. According to the statistically accurate definition, there is a 5% or less chance of seeing the observed data even though the drug is, indeed, ineffective. Rhetorically, the difference may seem imperceptible; mathematically, say statisticians, it is crucial. In situations in which the data is somewhat ambiguous, there is a chance that results can be misinterpreted. \"It's diabolically tricky,\" Goodman says. Most statisticians resign themselves to abuse of the term's strict definition. But more grievous trespasses abound. \"Statistical significance is neither a necessary nor a sufficient condition for proving a scientific result,\" says Stephen Ziliak, an economist at Roosevelt University in Chicago, Illinois, and co-author of  The Cult of Statistical Significance .  P -values are often used to emphasize the certainty of data, but they are only a passive read-out of a statistical test and do not take into account how well an experiment was designed. A  p -value would not reveal, for example, that everyone was taking different doses of that cholesterol drug. In many experiments, Ziliak says, \"there are so many different errors that they tend to swamp the  p -value errors\". Even if a result is a genuinely statistically significant one, it can be virtually meaningless in the real world. A new cancer treatment may 'significantly' extend life by a month, but many terminally ill patients would not consider that outcome significant. A scientific finding may be 'significant' without having any major impact on a field; conversely, the significance of a discovery might not become apparent until years after it is made. \"One has to reserve for history the judgement of whether something is significant with a capital S,\" says Steven Block, a biophysicist at Stanford University in California. In some situations other statistical methods can substitute, but Goodman believes that trying to use them in the scientific literature would be like \"talking Swahili in Louisiana\". He says he and other editors do their best to keep the term out of  Annals   though. \"We ask them to use words like 'statistically detectable' or 'statistically discernable,'\" he says.   Consciousness   ( noun. )  \n                Heidi Ledford \n             Psychologists, philosophers, neurobiologists and doctors all grapple with the term consciousness. For clinicians, the definition is of life or death importance; for some others, it is a matter of determining how the brain's interconnecting tissues collectively create a sense of self. \"How can this three-pound piece of meat inside my head give rise to something like being me?\" sums up Gerald Edelman, director of the Neurosciences Institute in La Jolla, California. In 2006, neuroscientist Adrian Owen, at the Medical Research Council Cognition and Brain Sciences Unit in Cambridge, UK, reported that a woman who had been diagnosed as being in a vegetative state had shown signs of brain activity associated with consciousness 11 . The activity was picked up with functional magnetic resonance imaging (fMRI), which can reveal changes in brain blood flow. The finding rattled the clinical definition of consciousness, which is determined by using a series of behavioural tests to see if the patient can make voluntary movements in response to commands. The outcome can determine whether a patient needs pain medication, or whether it is time to switch off life support, but clinicians readily acknowledge that the tests break down when patients are unable to move. Doctors now find themselves in an uncomfortable limbo, because it is not clear whether cortical activity measured on fMRI is enough to redefine those decision points. \"What do we do as a community as long as this method is not yet validated?\" asks Steven Laureys, a neurologist with the Coma Science Group at the University of Li\u00e9ge in Belgium. The French philosopher Ren\u00e9 Descartes declared that consciousness was a fundamental property that fell beyond the rules of the physical world. Most scientists, says Edelman, are not satisfied with that answer. \"There must be some physical basis for consciousness,\" he says. \"The difficulty is, how does that arise? Philosophers David Chalmers of the Australian National University in Canberra, explored what he called the \"hard problem\" of consciousness by pondering'qualia', the subjective properties of experiences. Scientists and philosophers alike have struggled to explain how the physical properties of the world around us \u2014 such as colour and temperature \u2014 give rise to the experiences of 'red', or 'warm'. Chalmers has argued that the functional organization of the brain rather than its chemical or molecular properties makes these experiences possible. Many definitions of consciousness include the ability to sort through the relentless onslaught of incoming data to create and respond to an internal model of the external world. And some believe that simply gathering data about neurons and behaviours will not be enough. \"What we need is a 'theory of consciousness'. Then we'll be in a better position to define it,\" says professor of biology and engineering Christof Koch of the California Institute of Technology in Pasadena. Koch thinks that information theory could provide the solution by determining whether consciousness might be an inherent by-product of a system as enormously complex as the brain (see 'Complexity'). Michael Gazzaniga, a neuroscientist at the University of California, Santa Barbara, argues that researchers need only develop a working definition to explore consciousness, not a precise one. \"You don't waste your time defining the thing,\" he says. \"You just go out there and study it.\" See Essay,  page 1040 . See also Correspondence:  'Subspecies' and 'race' should not be used as synonyms \n                     Nature Reports Stem Cells \n                   \n                     Nature Reports Climate Change \n                   \n                     Regenerative Medicine Focus \n                   \n                     Human Genome collection \n                   \n                     NIH Epigenomics \n                   \n                     Santa Fe Institute \n                   \n                     RealClimate \n                   Reprints and Permissions"},
{"file_id": "4551168a", "url": "https://www.nature.com/articles/4551168a", "year": 2008, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "Where should the drug industry go to find new ideas? In the  first  of two features, Alison Abbott asked if the future lies in systems biology -- a field that attempts to piece together 'everything'. In this, the second feature, David Cyranoski looks at drug companies' attraction to China. From the outside, China may not seem an obvious place to look for a pharmaceutical breakthrough. Last year, the head of the State Food and Drug Agency was executed for taking bribes. This year, more than 80 people died in the United States alone after taking a contaminated version of the blood-thinner heparin, the main ingredient for which had been imported from China. The number of domestically discovered drugs that the Chinese pharmaceutical sector has turned into internationally successful products? Zero. But these are desperate times. Pharmaceutical companies around the world face shrinking incomes as their leading drugs go off-patent and generics come online. The cost and complexity of new drug discovery and clinical testing is soaring and drug firms are taking a machete to their research and development (R&D) programmes. Looked at in this light, certain features of China begin to look attractive. A spate of returnees, often with experience from Western pharmaceutical companies, has put talent on the ground. A vibrant contract research organization (CRO) industry \u2014 with more than 400 companies in Shanghai alone \u2014 allows for speedy changes of direction with minimal investment. Clinical trials can cost half or even one-fifth of what they would in the United States. The government is showing that it is serious about meeting international standards for drug development with a 6-billion renminbi (US$900 million) investment aimed at good practices in laboratory and clinical practice (see  _Nature_ 455, 142; 2008 ). And, perhaps most importantly, multinational companies recognize that a research branch in China helps build connections with regulatory agencies and gain access to a market that some analysts predict will be the world's second largest after the United States by 2020. The combination of desperation outside China and promise within has convinced almost every big pharmaceutical player, including Roche, Novartis, GlaxoSmithKline, Eli Lilly and Pfizer, to collectively invest hundreds of millions of US dollars into research operations there over the past two to three years. The companies are somewhat cagey about how much they are investing at a time when they are laying off employees elsewhere, and when there is no guarantee of a return. But Kenneth Chien, an expert in cardiovascular medicine and an adviser to several large drug and biotechnology firms working in China, calls it \"Basel on steroids\", referring to the throng of pharmaceutical companies in Switzerland. Expansion into China is an experiment, allowing companies to try out new models for seeking drugs, often relying heavily on outsourcing to CROs. \"Every one is doing different things. Eventually we'll figure it out,\" says Li Chen, head of the Roche R&D centre in Shanghai. But there is no proven leadership, points out Mingwei Wang, director of China's National Center for Drug Screening and a pharmacologist at the Shanghai Institute of Materia Medica, the drug-research institute of the Shanghai Institutes for Biological Sciences. \"Pharmaceutical companies in China have all the problems of the United States plus the lack of experience. There are no success stories, nobody has built an Amgen or Genentech in China.\"  \n                Hi-tech high-rises \n              Most of Shanghai's pharmaceutical boom is happening in the Zhangjiang Hi-Tech Park in Pudong, east of the Huangpu river. It is in the 'new' part of Shanghai, symbolized by the Oriental Pearl Tower, with wide streets, spacious new residential complexes and golf courses. Zhangjiang boasts companies selling almost every service a pharmaceutical or biotech company could want, including the production of active ingredients, genomics, analytical and combinatorial chemistry, and preclinical toxicology testing. It is a natural place for a drug company to want to set up in. But not GlaxoSmithKline (GSK). GSK plans to go to Puxi, the area west of the Huangpu, to build a massive permanent home for its research branch, launched in May 2007. Puxi has  shikumen  stone-gate dwellings and fashionable restaurants, bars, and cafes tucked into alleyways. Puxi is also home to Shanghai's, and probably China's, most prized biological sciences research institute, the Shanghai Institutes for Biological Sciences, in addition to strong research hospitals, such as the Ruijin and Zhongshan hospitals, and the medical schools of Fudan University and Jiao Tong University. \"We were looking for a scientific environment,\" says Jingwu Zang, who heads GSK's new outpost, called R&D China. \"Zhangjiang is for services. We want hard-core science.\" R&D China's motto is 'Discovered in China', and it is emblazoned over an image of the Oriental Pearl Tower on the wall of the company's temporary office in Zhangjiang. Zang hopes that the phrase will replace the ubiquitous 'made in China' that brands the mostly low-margin goods made there. The unit will be the global centre for GSK's new neurodegenerative and neuroinflammatory drug-development programmes, focusing on multiple sclerosis, Parkinson's disease and Alzheimer's disease. The plan is to employ 1,000 people over the next 10 years. The company declined to give the full cost of the enterprise, but such a unit would be three to four times larger than those of other drug companies investing in the country. GSK's decision to build such a large and permanent facility, rather than rent space as most companies do, suggests great confidence in what can be accomplished in China. It is particularly striking considering that GSK announced plans last autumn to lay off 5,000 people in other branches. Zang says that the company was drawn to China by several factors, and that cost-saving is not one of them. One attraction is the growing talent pool of scientists who trained overseas and have already returned or are willing to return. Almost half of R&D China's scientists are people who have returned from working in the United States or Europe, says Zang. Researchers are being lured back by government-supported research institutes, university positions and the opportunity to start their own government-supported businesses (see  'Started in China' ). He says the company also has in mind the benefits of being able to transition drugs to the growing Chinese market more easily. Zang rates the \"overall interactive scientific environment\" in Puxi as comparable to that in the Boston area of Massachusetts and hopes to thrive in it. Over the past decade, GSK has collaborated with the Shanghai Institute of Materia Medica and it plans to do more with the Shanghai Institutes for Biological Sciences and the Shanghai Institute of Organic Chemistry. China also offers opportunities to try an Asian angle on some intractable research problems. For example, Zang plans to investigate how multiple sclerosis differs between Asian and white populations, drawing on patients from Ruijin, Zhongshan and other hospitals. Until now, most researchers studying multiple sclerosis, such as those in the Consortium of Multiple Sclerosis Centers, of which GSK is a member, have mainly studied white people. \"In the United States we hit a wall,\" says Zang, who was the research director of the Baylor College of Medicine Multiple Sclerosis Comprehensive Care Center in Houston, Texas, for almost 10 years. Asians seem to have a lower incidence of the disease and there is at least one clue as to why \u2014 susceptibility genes in the major histocompatibility complex sometimes differ in patients grouped by race. \"We will look at the genetics very closely,\" he says.  \n                Business base \n              Eli Lilly is also pouring millions of dollars into China \u2014 US$100 million over the next 5 years \u2014 but it couldn't be spending it more differently. Its rented office in Zhangjiang Hi-Tech Park employs ten people, and none does scientific research. Instead, the experienced research scientists effectively figure out how to pay other companies to do drug discovery for them, focusing initially on diabetes and oncology. They subcontract different stages of the drug-development process to Zhangjiang's myriad CROs, in which some 300 staff members are dedicated to Lilly's work. The company's China unit, which officially opened its doors on 15 October, is calling itself a \"fully integrated pharmaceutical network\". \"In a smaller organization, you know the bottom line much better and you know whether you made the right decision much more quickly,\" says Tony Zhang, head of Eli Lilly's drug-discovery unit. \"You can consider what we are doing in China an experiment. We don't have to own everything.\" And it is an experiment he believes will work. \"There is a large pool of scientific talent, vibrant entrepreneurs, and good infrastructure,\" he says. A similar experiment is being planned by Pfizer. The firm already has a China Research and Development Center that focuses on drug development and clinical trials, but it is planning to launch a similar 'virtual R&D network' that will work through partnerships with academic institutions and CROs. Such efforts by Pfizer, Roche and other companies are like \"extensions of global operations\", says Wang. By keeping the in-house operation small, \"it's easier to open and easier to close if things don't go well\". Being able to open, close or change direction quickly is a moneysaver. Beatrijs Van Liedekerke, an associate director at PriceWaterhouseCoopers, Beijing, in charge of the business advisory group for the biotechnology, pharmaceuticals and medical-device industries, estimates that outsourcing in Chinese pharmaceutical R&D saves some 30% in costs. \"They outsource because they don't have the expertise, they don't want to invest, or they just have too many things going on,\" she says. Gerald Chan, a co-founder of the Morningside, an investment company active in China, is blunter. \"Drug companies had to do something to please Wall Street,\" he says, \"and that meant outsourcing.\" Roche, which in 2004 became one of the first multinationals to establish a research unit in Shanghai, is somewhere between the Eli Lilly and GSK models. Having spent some US$70 million to $80 million over the past four years, it now employs around 90 people in house. But it also relies heavily on CROs and partnerships with universities that, taken together, give it a total of about 250 research staff, says Chen. \"You can build a drug-design chain without leaving Zhangjiang,\" he says. The role of Roche's Chinese branches is partly to adapt pre-existing drugs by verifying the appropriate dose for Chinese people \u2014 essential work if the company wants to gain regulatory approval for drugs it has developed abroad. But the company also wants to make this a two-way flow, by developing a programme to identify and evaluate new drug candidates in China as far as phase II clinical trials, and pass them to Roche's global portfolio. Roche's China branch already has two cancer drugs in the preclinical stage that are ready for human trials next year. Chen agrees that outsourcing speeds up progress. \"You can do several steps in parallel, and flexible resources will reduce the waiting time when you have numerous projects with multiple compounds,\" he says. \"By picking different CROs to do the same services in parallel, whether it's drug metabolism and pharmacokinetic studies, drug safety or oncology animal studies, pharmaceutical companies can avoid a backlog at any given point in drug development.\" Wang cautions that there are limitations on just how much can be outsourced. Screening drug candidates for biological activity, for instance, requires experience that CROs are short on and this could be a concern when it comes to generating high-quality data. Very few of Zhangjiang's company's offer these types of screen. \"CROs are passive. If you say 4 weeks, they'll do it. But they won't spot early signs of deviation. It's a recipe for disaster. To interpret the data, you must be experienced.\" Van Liedekerke, who has been based in China for the past two and a half years, says that China has a lot going for it, but it won't be a \"deus ex machina that solves all of pharma's problems\". It might solve some of them though. She gives an off-the-cuff estimate of 3\u20135 years before drugs discovered in China start making their way into health systems around the world. \"It's just a matter of time,\" she says. Drug discovery and development is becoming more complicated both scientifically and managerially, she says, and requires people with expertise in many fields to come together. \"The people that can communicate the best will win out.\" And whoever wins, less money will probably have been spent trying out new models in China than elsewhere. Chan estimates that a full-time chemist in the United States costs US$225,000 in salary and research costs per year compared with $70,000 in China. But the price is rising quickly owing to competition and rising prices. \"Shanghai used to be the most desirable spot. Now we wouldn't dare touch it,\" he says. \"Prices in China are going up,\" Chien agrees. \"It will not be a bargain for much longer.\" David Cyranoski is  Nature 's Asia Pacific correspondent. \n                     China Special \n                   \n                     Nature Reviews Drug Discovery \n                   \n                     Nature Biotechnology \n                   \n                     GlaxoSmithKline \n                   \n                     Roche \n                   \n                     Eli Lilly \n                   Reprints and Permissions"},
{"file_id": "456161a", "url": "https://www.nature.com/articles/456161a", "year": 2008, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Neuroscientists are pretty sure they know what causes Alzheimer's disease, but their theory has not yet given rise to effective drugs. Alison Abbott asks what's wrong. It is sad and bewildering to watch a parent slowly fade away, the years of authority dissolving into childlike moodiness. But it is an experience that more and more people will face as the number of elderly people with Alzheimer's disease climbs to the 80 million or so expected by 2040 1 . If a strong hypothesis were enough to help, then treatments should be on the shelf. For years researchers have been confident that the disease is caused by protein 'plaques' and 'tangles' that eat away at the brain \u2014 and pharmaceutical companies have spent untold millions identifying and testing therapies that interfere with these structures. But medical need and mechanism are not, it seems, enough. It has come as an acute disappointment that the first clinical trials of therapies designed to deal with the plaques have failed to help patients. In July this year, participants at the annual Alzheimer's Association meeting in Chicago, Illinois, heard that two major trials, one a phase III including 1,600 people, the other a phase II involving 234 people, failed to improve the mental state of patients even though they had looked promising in earlier studies. Another phase III trial was abandoned in 2007 when it was similarly unsuccessful. Something seems in need of a rethink, but what? The presumed biological mechanism \u2014 or the trials themselves? Researchers say that they remain confident about their hypothesis. Their concerns lie in the way that the trials have been done. Because diagnosis is relatively rudimentary, some of the recruited patients may have had a different sort of dementia from Alzheimer's, and all of them may have had such advanced disease that their brains were irreversibly damaged. \"We diagnose people when they already show dementia, not at a more helpful earlier stage,\" says Bruno Dubois, a neurologist at the H\u00f4pital de la Salp\u00eatri\u00e8re in Paris. \"It is like diagnosing someone with Parkinson's disease when they are bedridden, not when they develop a tremor.\" This means that the trial results are not so much negative as uninterpretable. Researchers and pharmaceutical companies had anticipated these problems, but hoped that the trials would show some therapeutic effect anyway. They pushed ahead because of the pressing medical need and the potential therapeutic market. At the same time, scientists have started work to improve the next round of trials by launching major international research programmes to identify biological markers of disease progression, such as those that expose plaques in living brains (pictured above). Such biomarkers might eventually reveal invisible equivalents to the Parkinson's tremor so that the disease can be recognized before dementia sets in and so that researchers can observe directly whether a drug is doing the biological work expected of it. Researchers are already incorporating the most promising biomarkers into many of the clinical trials now under way. Only with biomarkers in hand will scientists be able to seriously test the theory of Alzheimer's on which they have staked so much. \"The theory is not proven, and we have to be open to the fact that it could be wrong,\" says Paul Aisen, a neurologist at the University of California, San Diego, who runs the US Alzheimer's Disease Co-operative Study for the National Institute of Aging in La Jolla, California. \"But the trials so far have simply not tested the hypothesis.\"  \n                Deep roots \n              The hypothesis in question has roots that can be traced back more than a century to Alois Alzheimer, a physician and neuropathologist in Munich, Germany, who followed the five-year mental decline of one of his patients, Auguste Deter. When Deter died in 1906, Alzheimer scrutinized her brain using a staining technique developed by his colleague Franz Nissl. He saw a mess of clumped proteins in the form of plaques, and tangles of fibrils like snarled-up thread. These characteristic features, which until recently could be seen only at autopsy, still provide the most secure diagnosis for the disease, together with neuropsychiatric assessments of memory, language and other cognitive functions. When molecular biologists started to dissect the structures that Alzheimer observed, they found that plaques are made up of small, sticky protein fragments, or peptides. These peptides, the most common of which is called \u03b2-amyloid, are cleaved from a larger amyloid precursor protein (APP) by the enzymes \u03b2-secretase and \u03b3-secretase. Neurofibrillary tangles, inside neurons, are now known to be made of an insoluble protein called tau. The prevailing theory holds that Alzheimer's disease is initiated when APP \u2014 the normal function of which is unknown \u2014 is converted to toxic amyloid peptides that are taken up and released by plaques. This stresses cells in a way that promotes so much phosphorylation of tau that it forms tangles. The process is known as the amyloid cascade. As the plaques and tangles spread from the hippocampus into the cerebral cortex, the people affected gradually lose their memory, their logic and eventually the ability to speak and move. In this weakened state, patients typically die of pneumonia. The only drugs available for Alzheimer's patients aim to treat symptoms, trying to chemically stimulate areas of the brain that are already damaged by the time dementia sets in. They are marginally effective at best. So in the past five years, academic groups and pharmaceutical companies have focused on preventing the plaques and tangles from forming, or dismantling those already there. So far, these strategies have generated a string of failures. Aisen was principal investigator on the North American trial of a drug called tramiprosate (Alzhemed) being developed by Neurochem, a biopharmaceutical company based in Montreal, Canada. It was the first trial based on the amyloid-cascade hypothesis to reach this advanced stage of testing and as such was greatly anticipated by the community. Tramiprosate is a small chemical that inhibits the formation of plaques in animal tests by binding to \u03b2-amyloid. Aisen's team followed up more than 1,000 patients for longer than 18 months but did not detect any change to their mental states and Neurochem has since pulled out of further clinical testing. The two trials that failed this year used different approaches to attack amyloid, but with no greater success. Myriad Genetics' compound tarenflurbil (Flurizan) was designed to prevent the build-up of amyloid plaques by inhibiting \u03b3-secretase. Myriad says that the drug might not have reached appropriate concentrations in the brain and the company abandoned it when it failed to show benefit. Elan Pharmaceuticals' bapineuzumab, a humanized monoclonal antibody that binds directly to \u03b2-amyloid and mimics an immune system attack on plaques, fared little better. Perhaps the cruellest blow came when Clive Holmes of the Moorgreen Hospital in Southampton, UK, reported this July 2  on patients six years after they received an experimental amyloid peptide vaccine developed by Elan. The vaccine was designed to teach the immune system to recognize amyloid as being foreign so that it would launch an attack against the protein. Post-mortem analyses showed that almost all the patients had stripped-down amyloid plaques, despite most of them having progressed to severe dementia before they died. Some researchers were left wondering whether too much emphasis had been placed on removing the plaques. \"It was only a small number of patients,\" says Karen Duff, an Alzheimer's researcher at Columbia University in New York, \"but one has to question the amyloid-cascade theory, which dictates that clearance of the plaques should be correlated with disease improvement.\" She notes that tau tangles were still present in the autopsied brains. \"This, for me, showed that far too much attention has been given to amyloid proteins, whereas other targets such as tau itself and other pathological features have been neglected.\"  \n                Protein principles \n              Researchers have very strong reasons for believing that plaques and tangles are the cause of Alzheimer's symptoms, rather than just markers of it. Mutated versions of APP can cause an early-onset, hereditary form of the disease in which amyloid peptide has a greater propensity to accumulate in the brain years before symptoms develop 3 . Mice engineered to produce mutant APP that increases production of \u03b2-amyloid develop Alzheimer's-like symptoms that can be reversed by reducing tau expression 4 . And even if plaques and tangles do cause the symptoms, it does not mean that removing plaques and tangles will treat them. These structures may damage the brain in ways that cannot be repaired by simply dissolving the offending protein accumulations. \"It could well be that there has been too much focus on early events of the amyloid cascade,\" admits Simon Lovestone, a psychiatrist and neuroscientist at London's Institute of Psychiatry. \"But let's not throw the baby out with the bath water \u2014 there is just too much convincing preclinical research that supports the amyloid-cascade hypothesis.\" Rather than rethinking the hypothesis, researchers are rethinking the trials. Aisen says that a major problem for the tramiprosate trial \"was the huge variability in rates of decline among patients\". In retrospect, he estimates that one-fifth of the patients recruited might not have had Alzheimer's at all, and so could not have been helped by therapy geared to disrupt the amyloid build-up typical of the disease. Only 50\u201370% of dementia cases are the Alzheimer's type; the rest have dementias that have other pathological indicators and neuropsychiatric profiles. Aisen also says that 18 months may not have been long enough to pick up any clinical effect on the memory or behaviour of patients. \"One of the huge difficulties of disease-modifying treatments in general is that they aim to slow the rate of progression of the disease and this is what trials have to measure,\" he says. With a disease that advances slowly, as Alzheimer's does, there may be only a small difference in how fast it progresses between patients in the treatment group and those in the control group. Furthermore, any difference may take a long time to become apparent. That means that trials need to recruit large numbers of patients and follow them for a long time \u2014 and that costs a lot of money.  \n                Early start \n              Yet another problem is that disease-modifying drugs are best deployed in the early stages of disease, and it is not currently possible to identify Alzheimer's before dementia sets in. Some of the promising preclinical results are based on mouse models of Alzheimer's that are treated before symptoms develop. By the time patients are recruited into trials, they are already taking such an array of therapies, including memory-enhancing drugs and antidepressants, that variability in both subject and control groups confounds analysis further. The only way round these problems, everyone agrees, is to develop reliable biomarkers to help follow the progress of disease and treatment. In fact, the search has been on for several years. There are two main biomarker approaches. One is to image the brain to determine its structure, its activity and how much amyloid it contains. The other involves measuring concentrations of amyloid and tau peptides in the cerebrospinal fluid and perhaps even in the blood. These techniques could be used to select those with true Alzheimer's for clinical trials. They might also help neurologists to follow the disease as it advances and monitor whether a test drug is having a biological effect (such as removing amyloid) even if the patient has no measurable change in mental state. The major challenge for researchers now is to validate the proposed biomarkers by showing that the detection methods are consistent and reliable in thousands of patients from different clinical centres and that the measurements correlate with biological or clinical endpoints. The community's perceived need for Alzheimer's biomarkers is reflected in the amount being spent to pursue them. Lovestone is principal investigator of one of the largest consortia of academics and pharmaceutical companies. Known as AddNeuroMed, this \u20ac8.6-million (US$11-million) five-year scheme is one of only two pilot projects supported by the European Union's \u20ac2-billion Innovative Medicines Initiative, planned to remove research bottlenecks in drug-development processes. In the United States, Alzheimer's biomarkers are being sought as part of the Biomarkers Consortium launched in October 2006, and by the $60-million Alzheimer's Disease Neuroimaging Initiative. The neuroimaging initiative began in 2004 and is following the neuropsychiatric progression of 200 healthy elderly people, 400 patients with mild cognitive impairment and 200 with Alzheimer's disease using various test biomarkers. Both consortia are supported by the National Institutes of Health and by several pharmaceutical companies. These and various other initiatives under way in Japan, Australia and China, coordinate with each other carefully, sharing protocols and data and having regular conference calls, not least because those doing clinical trials are keen for drug regulatory agencies such as the US Food and Drug Administration and the European Medicines Agency to accept their validity. Magnetic resonance imaging is already widely used to monitor key affected parts of the brain, such as the medial temporal lobe, as they shrink in nearly all patients with Alzheimer's disease, but in fewer than 30% of the normal ageing population. Changes in brain volume are believed to correlate well with memory performance. Positron-emission tomography (PET) imaging, used to measure the rate of metabolism in different brain areas, can also be used to detect Alzheimer's, although there is large variability between centres. A new PET technique developed by Steven DeKosky from the University of Pittsburgh in Pennsylvania and his colleagues uses special radioactive ligands that bind to amyloid peptides 5  to visualize plaques directly, allowing them to be tracked as the disease progresses, and as treatment interferes with them. Concentrations of amyloid peptides, particularly one called amyloid-\u04311-42, are low in the cerebrospinal fluid of patients with Alzheimer's disease compared with healthy controls because the plaques are thought to suck them out of circulation, and concentrations of tau protein and phosphorylated tau are high. The concentrations and ratios of these three proteins can be useful indicators of whether a test drug is having a biological effect in the brain, and are already being used in some of the 30 or so trials that are now under way for treatments based on the amyloid-cascade hypothesis. These include a phase III study by Eli Lilly in Indianopolis, Indiana, of a \u03b2-secretase inhibitor and further studies of bapineuzumab. None of these methods is as yet sensitive or specific enough to act as a biomarker for Alzheimer's on their own. In a paper published last year 6 , 19 authors representing centres in Europe, North America and Japan, proposed that diagnosis of Alzheimer's should include at least one of the biomarkers under development in addition to new memory assessment techniques that can differentiate more precisely between different dementias. The authors hope that this approach might be applied routinely to diagnose a condition known as 'mild cognitive impairment', which often precedes full-blown Alzheimer's. If it were possible to identify patients with mild cognitive impairment who were highly likely to progress to Alzheimer's, they could form the basis of much more useful cohorts in clinical trials, says Dubois, who was lead author on the paper. Eventually, and this is the trickiest part, a set of proven biomarkers may be able to act as surrogates for the disease, just as CD4-positive cells are used as a surrogate for HIV progression. When antivirals were first tested against HIV, their efficacy was measured based on the number of days a patient spent in hospital, or on time until death, says Michael Weiner, director of the Center for Imaging of Neurodegenerative Diseases at the Veteran Administration Medical Center campus of the University of California, San Francisco. Once CD4 was shown to correlate with clinical outcome, \"they didn't have to wait for people to die any more \u2014 and this is what we want in Alzheimer's\". Surrogate endpoints might allow clinical trials to be shorter and cheaper.  \n                Surrogate scepticism \n              But regulatory agencies need a lot of persuasion to accept a new surrogate. In the past year, the validity of even very established biomarkers \u2014 such as blood cholesterol for cardiovascular health and blood glucose for diabetes \u2014 have been called into question when trials showed they were poor indicators of morbidity or mortality 7 . Marisa Papaluca-Amiti, a biomarkers expert at the European Medicines Agency, says that different centres still show a high level of variability for Alzheimer's biomarkers, especially those performing imaging. \"We'll be persuaded when the scientific community gives systematic validation data that are reproducible everywhere,\" she says. \"We are not going to see a surrogate marker for Alzheimer's anytime soon.\" Most pharmaceutical companies seem to be unwavering in their pursuit of the amyloid target, with several beginning new trials and few giving up on those that are running. Even though Elan's early-phase bapineuzumab trials did not perform well across the board, post-hoc analysis indicated some improvement in a sub-group of patients who, counterintuitively, did not carry a risk gene for the disease called  APOE4 . That was more than enough to give the company confidence to carry on, says Dale Schenk, Elan's chief scientific officer. He says that the company has set up four phase III trials, involving some 4,000 patients, of which two trials will evaluate those who carry an  APOE4  mutation, and two will evaluate patients without it. Schenk believes that biomarkers will also make all the difference. Some patients in the bapineuzumab studies are being evaluated with the new PET technique for imaging plaques in living brains. \"Biomarkers are really changing our approach to studying and, hopefully, treating the disease,\" Schenk says. \"These technologies are promising in their ability to show us whether amyloid is being cleared from the patient's brain while the treatment is going on.\" Some serious attention is now being given to tau as a clinical target. One clinical trial under way by the Singapore-based company TauRx Therapeutics is testing whether methylthioninium, commonly known as methylene blue, can clear tangles. \"I believe that eventually we'll find we need different therapeutic strategies for different stages of the disease, including symptomatic treatments as well as disease-targeted treatments,\" says Bob Holland, vice-president for neuroscience at AstraZeneca in Wilmington, Delaware. \"But amyloid-targeted therapies are highly like to be a big part of it.\" Those with Alzheimer's, and the people caring for them, face difficult tests every day. But the biggest tests of the amyloid hypothesis are yet to come. Alison Abbott is  Nature 's senior European correspondent. \n                     Neuroscience \n                   \n                     Focus on Neurodegeneration \n                   \n                     EMEA guidelines on Alzheimer's disease \n                   \n                     The EU Alzheimer Biomarker programme (AddNeuroMed) \n                   \n                     FDA Critical Path \n                   \n                     Alzheimer's Disease Cooperative Study \n                   \n                     Alzheimer's Disease Neuroimaging Initiative \n                   Reprints and Permissions"},
{"file_id": "456300a", "url": "https://www.nature.com/articles/456300a", "year": 2008, "authors": [{"name": "Tanguy Chouard"}], "parsed_as_year": "2006_or_before", "body": "You might think that once evolution has found one way to get something done, it will stick with it. But similar physical forms can hide radically different wiring, finds Tanguy Chouard. \"It is not birth, marriage or death, but gastrulation, which is truly the most important time in your life,\" British embryologist Lewis Wolpert famously said. That's when the primordial sheets of embryonic cells are instructed to fold inwards on their way to becoming more specialized tissues. The process is governed in part by a group of cells called the Spemann organizer, and developmental biologists, echoing Wolpert's view, thought that you couldn't mess with something as important as the organizer. So when, in the early 2000s, those biologists tried to find the Spemann organizer in tunicates they were in for a big surprise. Tunicates \u2014 also known as sea squirts \u2014 are humans' closest invertebrate cousins. They have tadpole-like larvae that closely resemble miniature vertebrate embryos and so were expected to build their bodies in the same way. But they don't. Most of the 'organizer genes' are there in the tunicate genome, but they are expressed elsewhere in the embryo and do dramatically different things 1 . It's as if you had found a car in which components of the engine were scattered all over the back seat \u2014 but the car still worked. Many biologists, consciously or not, tend to see living systems as optimally tuned. If one species has a complex solution to a difficult problem (such as a Spemann organizer for building a swimming tadpole), it is often assumed that the way genes are orchestrating that process will be the same in related species. That idea has only been strengthened by the discovery that many genes, and the proteins they encode, display a stunning degree of conservation across hundreds of millions of years of evolutionary time. What is becoming clear, however, is that although most genes might be conserved, the regulatory connections that control their expression might not be. Closely related species can connect up their genes in very different regulatory networks, while keeping the end result deceptively unchanged. \"Problems with many solutions are the rule rather than the exception in living systems,\" says Andreas Wagner, a bioinformatics expert at the University of Zurich, Switzerland. Now researchers are trying to understand how evolution finds the solutions it does, and why. Some think that this 'underground' variation was selected for. Some think that it appeared by chance. And regardless of how it arose in the first place, some believe that a variety of regulatory networks may offer an evolutionary advantage in the future. Trying out many different designs 'underground' could provide a hidden source of evolutionary innovation, variation on which organisms might draw when faced with new challenges. All these ideas present a major challenge to those who study 'evo-devo', the evolution of developmental processes. Researchers can no longer conclude that two organisms are built in the same way by considering one gene or even one signalling pathway at a time. They must consider the entire system with its inputs, outputs and the connections in between. Where evo-devo has met systems biology, a new discipline \u2014 systems evo-devo \u2014 has emerged. \"I've never been too fond of any of those buzzwords,\" says Patrick Lemaire at the Developmental Biology Institute of Marseilles in France, \"but if this means bringing more rigour to evo-devo, making it less descriptive and more mechanistic, then, yes, a systems framework seems useful.\"  \n                Conservative ideology \n              It is hard to shake the belief that conservation runs deep. From the 1980s onwards, DNA sequencing and gene-knockout technology revealed the extent to which genes and their functions were conserved. These techniques showed that many of the genes that determine the animal body plan are virtually identical in both structure and function in creatures that, on the outside, have little in common. The expression patterns of  Hox  genes, for example, specify the same positional values in the head-to-toe body axis of fruitflies and mice. Such discoveries strengthened the intuitive idea that when gene sequence and organization are conserved, so too is gene function \u2014 and that conserved function implies conserved genetics. But in the past few years it has become apparent that the gene\u2013function relationship is not so clear cut. A given function can result from diverse combinations of the same genes, or different genes, even in very closely related species. Molecular dissections of the extremely conserved body plan of insects \u2014 head, thorax and abdomen \u2014 provide a striking example of how different genes and wiring can lead to very similar endpoints. In the fruitfly  Drosophila , the  Hox -related gene  Bicoid  is essential to establishing this form. Work starting in the 1980s has shown that messenger RNA coding for the Bicoid protein is glued to the front end of the egg. Here, the protein is translated after fertilization, diffuses from one end of the embryo to another and orchestrates the activity of several other key genes involved in distinguishing head from tail. In fact, Bicoid does so many wonderful things that it was widely assumed to be fundamental to insect development. Not so. Starting in the late 1990s, sequence data made clear that the  Bicoid  gene, although crucial in  Drosophila , is absent from most other insects. This apparent paradox is resolved only when considering gene regulatory networks as a whole. The labs of Reinhard Schroeder, of the University of T\u00fcbingen, Germany, and Claude Desplan, of New York University, for example, found that several genes that are activated by Bicoid after fertilization in fruitflies are deposited into the egg before fertilization in the flour beetle 2  and a parasitic wasp 3 . The lack of  Bicoid  is compensated by minor changes in the action of several other genes in the developmental network. So whether you think about the Spemann organizer or the  Bicoid  gene, the same rule seems to apply: there are many combinations of contributing factors that can reach the same outcome. \"You can't understand any of this if you think at the single-gene level,\" says Lemaire. The information that determines biological function lies at a higher, more abstract level, in the entire network of genes, proteins and other factors that each act on the others in a series of nonlinear feedback loops. The body plan or feature that results is what scientists who study complex systems call an 'emergent property' \u2014 one that is more than the sum of its parts. It all feels very counterintuitive. Maybe it is because man-made machines are sensitive to individual component failure, that biologists tend to assume that genetic networks are similarly constrained, and that changing their wiring piecemeal would mess up the whole system. So one of systems evo-devo's key questions is how the genes and their interactions can change while keeping the output of the system as good as, or better than, it was before. To find out, researchers need an experimental system simple enough that many species could be analysed in the finest molecular details. Sexual differentiation in yeast offers such a system, one conserved across species that diverged up to about a billion years ago. Most yeasts have two mating types: 'a' cells express 'a' genes, and '\u03b1' cells express '\u03b1' genes. In the human pathogen  Candida albicans , a-specific genes must be actively turned on by a DNA-binding protein called a2 before they are expressed. But in the brewer's and baker's yeast  Saccharomyces cerevisiae , a-specific genes have to be turned off by a repressor protein \u03b12 \u2014 their default setting is on. The circuitry works in completely different ways and  S. cerevisiae  doesn't even have a copy of the a2 gene vital to  C. albicans . But these two systems didn't just fall from the sky, they must have evolved from a common ancestral system. To find out how, Alexander Johnson and his team at the University of California, San Francisco, teased out the way that a-genes are regulated in 16 yeast species whose genomes had been sequenced and could thus be reliably ordered on a phylogenetic tree 4 . They identified the regulatory DNA controlling many of the a-genes and compared the binding sequences for a2, \u03b12 and a third DNA-binding protein called MCM1, which associates with either a2 or \u03b12 depending on the yeast species. They also looked for changes in the way the proteins interact with each other. They reasoned that changes in the proteins and their binding sites might reveal how one regulatory system incrementally changes to give rise to the other. And they did.  Kluyveromyces lactis  \u2014 a yeast used in the cheese industry \u2014 carries what is likely to have been the transition state between the ancestral  C. albicans  logic and the  S. cerevisiae  scheme. Some of its a-genes bind a2, some bind \u03b12 and some bind both. At the same time, MCM1 is able to bind both a2 and \u03b12. \"This is absolutely terrific work,\" says Sean Carroll, a developmental biologist at the University of Wisconsin in Madison. \"They took enough snapshots to show us the whole movie of the evolutionary process.\" The key to this overhaul is functional redundancy: by possessing more than one regulatory pathway to achieve the same function, it is possible for one to change while the other keeps the system running smoothly. It's like moving around a boat in rough weather: you always keep at least one of two clips hooked to the lifeline. Johnson's yeast studies revealed how an organism might switch from one regulatory network to another without losing fitness in between. But such experimental studies can only deal with a couple of genes at once. So Wagner and two French collaborators turned to computer simulation to ask how far successive small steps can take a large gene-regulatory network away from its starting point 5 . Wagner's model consists of a number of genes, each of which codes for a regulatory protein that activates or represses one or more of the other genes. In the virtual embryo with which the model starts, every gene has a given level of expression. The model then calculates what that level of expression, for each gene, will do to all the other genes, and makes the necessary changes. That gives a new pattern of expression \u2014 and again the model sees what that means in terms of regulating each gene, until the system settles down to a steady state \u2014 the end point of the development of the 'organism'. Each 'genotype', the set of instructions dictating which gene does what to which other gene, results in a different final state, or 'phenotype'. (Systems that never settle down are discarded as unviable.) This type of model was originally developed by the group of John Reinitz, of Stony Brook University, New York, and, despite its level of abstraction, it has described early  Drosophila  development well enough to predict mutant phenotypes. Wagner and his colleagues then asked a question of their model: how many genotypes can generate the same phenotype? This is equivalent to asking how many different species' developmental programs can generate a perfect insect body plan. They explored this by allowing a given genotype to evolve by small steps (modifying one gene interaction at a time), but keeping only the genotypes that produced the same phenotype. By repeating such 'neutral' evolutionary steps hundreds of times they could measure the 'neutral space' of genotypes that all produce the same phenotype \u2014 itself a fraction of the total space of all possible genotypes. Even with a relatively small number of genes, says Wagner, \"the number of genotypes with any given phenotype is astronomically large\". Not only large, but spread out; the team found that it was possible to get from their starting genotype to something completely different \u2014 to move from one end of that imaginary space to the other \u2014 while keeping the corresponding phenotype unchanged. If the real world works like the model, regulatory networks can undergo dramatic reshuffles with no outward sign of what is going on at all. And Wagner's studies hint that being able to build the same edifice on such different \u2014 and changing \u2014 foundations may confer an evolutionary advantage. The team altered its algorithm to look at novel phenotypes that are just one mutation away from a starting genotype. So imagine genotypes in two far-flung parts of that imaginary neutral space, creating the same phenotype. Make a small mutation to one of those and it will produce a second phenotype; make a small change to the other and it will produce something very different. Wagner and his colleagues were not trying to get their model to innovate; but because the genotypes originally explored such a range of the possibilities without changing phenotype, they could later mutate in a very wide range of directions. So it may be that invisible variation in networks better prepares organisms for new challenges to come, by arming them with more possible solutions.  \n                The real deal \n              This year, a group led by Mark Isalan, of the Center for Genomic Regulation in Barcelona, Spain, rewired a real genetic network 6 . Isalan picked out 22 DNA-binding regulatory proteins in  Escherichia coli  and then created close to 600 genetic sequences in which the DNA coding for a given protein was put under the regulatory control of another. They then introduced each one of these synthetic genes into normal bacteria. In essence, they added a handful of new links to  E. coli 's endogenous regulatory network. The group found that roughly 95% of the rewired bacteria survived: the bacterial regulatory network seems remarkably robust to arbitrary rewiring. And some of these engineered bacteria could out-compete the original bacteria in their ability to survive culture conditions such as 'heat-shock'. If genetic systems were like the computer code that humans build, this would be like dumping arbitrary 'Go-To' instructions in the middle of some air-traffic-control software and finding that not only did it cause very few crashes, but it also got more planes to their destination on time. The researchers concluded that, for  E. coli  at least, new links in regulatory networks are rarely a problem, and that they can even confer improved fitness in some environments. But some biologists question the evolutionary significance of lab experiments, in which organisms are subject to less challenging conditions than in the wild. Systematic gene deletion in yeast, for example, has similarly shown that close to 80% of all protein-coding genes are not essential in normal lab culture. And yet their persistence as functional sequences in the genome is a strong indication that they contribute to the organism's fitness in the wild. So even if computer simulations or lab experiments suggest that network rewiring can be innocuous or even advantageous, the situation in real life might be different. Come back to the yeast mating type: although the various genetic logics of  S. cerevisiae ,  C. albicans  and  K. lactis  seem to have identical outputs, in that they all generate two mating types, these may provide the various organisms with some unknown adaptive features that are of benefit in their different ecological niches in humans, breweries or cheese factories. \"We don't know what hidden advantages if any might be associated with the various mating-type circuits found in those yeast species,\" Johnson says. Sea-urchin development is one case in which alternative strategies for building the same adult body plan seem to have offered advantages in lifestyle. The two species in the genus  Heliocidaris , for example, still share the shallow waters of southeastern Australia; their last common ancestor lived around the same time as the last common ancestor of chimps and humans. But they have dramatically different means of development. The  H. tuberculata  embryo develops first into a swimming and feeding larva called the 'pluteus', a form of indirect development that is shared by most of the 1,000 known species of sea urchins and probably represents the primitive mode for the clade. But  H. erythrogramma  develops direct from egg to adult in just a few days. Its entire developmental program has been completely scrambled to bring this about: its egg is 100 times bigger and the fates of the various cells that the egg divides into are completely different to those seen in  H. tuberculata 7 . Even Wolpert's sacrosanct gastrulation is rearranged. It is as if, in a brief 4 million years, chimps had evolved to lay hard-shelled eggs while still developing into chimp-like creatures in the end.  \n                Solving multiple problems \n              The sea urchins' developmental modes may represent equivalent solutions to one problem \u2014 how to build a spiny adult body that sticks to a rock \u2014 and yet be non-equivalent solutions to another \u2014 how to survive a youth in open water. Sea urchins with swimming and feeding larvae produce many small eggs, have broader geographic distributions, and show high mortality rates due to predators. Sea urchins that develop direct and fast produce fewer, bigger eggs and tend to stay put in remote niches; they have higher survival rates per embryo. In this case, changes that are 'neutral' with respect to one aspect of an organism's phenotype seem to present advantages in others. Many are unconvinced by these evo-devo studies. The common view that \"the guiding hand of natural selection\" is behind the evolution of all biological sophistication is greatly misleading, says evolutionary biologist Michael Lynch of Indiana University in Bloomington. Lynch thinks that most biologists underestimate the influence of chance, particularly in small populations in which neutral genetic changes can become the norm without any selective pressure by the process of 'genetic drift'. \"Nothing in evolution makes sense except in the light of population genetics,\" Lynch says. This is particularly true for gene regulatory networks, which can evolve by chance much faster than protein-coding genes because they can tolerate changes by point mutation, duplication or deletion with little effect on overall function. New binding sites for regulatory proteins, for example, can appear or disappear and the change can quickly become fixed in a population without loss of fitness 8 . Such constant reshuffling makes regulatory networks more complex than they need to be. It also creates redundancy \u2014 several parts of the network can perform the same task \u2014 and this allows further reshuffling to eventually bring about dramatic change in the way that things are run. Lynch thinks that Johnson's yeast studies are a particularly ele gant demonstration of this. Such \"seemingly baroque architecture\" of biological networks, as Lynch puts it, leaves developmental biologists with a problem: how to understand whole systems while relying on single-gene experiments. Traditionally, they have knocked out a gene in a model organism and drawn conclusions from that about the gene's function in a range of species in which that gene's sequence is conserved. The evolutionary biologist Stephen Jay Gould long challenged this type of reductionist method, in which biological function of a gene is discussed 'all-other-things-being-equal' \u2014 the  ceteris paribus  paradigm 9 . Systems evo-devo makes it clear that all other things are not equal: the function of any gene cannot be defined outside its species-specific context. Developmental biologists have to find a new way to describe the commonalities between organisms that were once ascribed to genes. The fact that tunicates and vertebrates both develop through a tadpole-like stage suggests that there must still be shared biological 'rules' governing the process. But if genes and regulatory links aren't conserved, then what exactly do they have in common? The shared features may actually lie in some other property of the regulatory networks. It is this problem that Lemaire and others now hope to address in tunicates. Because of the relatively simple body plan and the ease with which tunicate genes can be manipulated, researchers have come closer to building a complete regulatory network for tunicates than for any other model organism, one that shows how each regulatory gene interacts with every other, in every cell, and at every stage of development[10]. Lemaire is hoping to extract a 'network signature' that would characterize the process of gastrulation, perhaps a mathematical expression that describes the density of links between genes or the types of feedback loops used. Then researchers can try to see whether the same features are shared by gene regulatory networks in humans and other species that do rely on a Spemann organizer to gastrulate. If Wolpert is right, and gastrulation is truly the most important event in your life, then that seems an answer worth knowing. See pages   281 ,  and   295 ,  and the Darwin special at   http://www.nature.com/darwin. Tanguy Chouard is a senior editor at    Nature . \n                     Evo\u2013devo: extending the evolutionary synthesis \n                   \n                     Darwin 200 News Special \n                   Reprints and Permissions"},
{"file_id": "455858a", "url": "https://www.nature.com/articles/455858a", "year": 2008, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "Native Americans want to claim fossil resources found on their lands. Rex Dalton looks at how tribes and palaeontologists are working together to avoid bitter ownership disputes. In the heart of America's dinosaur country, the relationship between Native Americans and outside palaeontologists has always been tense. In the 1890s, the battle between white settlers and Native Americans was barely over when legendary fossil-hunter Edward Cope arrived to prospect for bones in the grassy hills that make up the Standing Rock Sioux Reservation in North and South Dakota. A century later an amateur palaeontologist digging in the neighbouring Cheyenne River reservation unearthed the largest and best-preserved  Tyrannosaurus rex  known, setting off an epic legal battle involving the research institute she worked for, the Sioux rancher who owned the land on which the fossil was found, and local tribes. But that long-standing tension was nowhere to be seen this July. On the Thunder Hawk Ranch on the Standing Rock reservation, near the sacred grounds where Chief Sitting Bull was born and died, Native American students came together for what is believed to be the nation's first palaeontological field school conducted by a tribe. Led by Swiss-born palaeontologist Gerald Grellet-Tinner, the students worked just as Cope did a century ago, unearthing fossils under a punishing sun broken only by torrential rain. If this field school is any indication, Native Americans could be on the verge of instituting sweeping changes that will resonate throughout the palaeontological community. Some tribal members, such as those who ran the dig site this summer, want to focus on the history and fossil resources of their land. Others, though, are seeking ways to protect their palaeontological heritage more aggressively. Two decades ago, Native Americans pressed for new laws covering ancient human remains. The resulting legislation, the 1990 Native American Graves Protection and Repatriation Act (NAGPRA), transformed US archaeology. NAGPRA required that any remains be returned to the custody of tribes who could prove they had a 'cultural affiliation' with them. Thousands of items have since been repatriated, prompting archaeologists to complain that their museum collections are being depleted of specimens for scientific study.  \n                Land rights \n              Now, some tribes are looking to extend that concept to fossils. Next week, a committee in the Nebraska legislature will hold a hearing about whether new legislation is needed to protect palaeontological resources on tribal lands. Nebraska's decision is important because the state contains large expanses of reservation land \u2014 and as such it often pioneers laws that involve Native Americans. For instance, it enacted a law protecting Native American human remains even before the federal NAGPRA was passed. The Nebraska drive was spurred by Lawrence Bradley, a doctoral student in geography at the University of Nebraska in Lincoln, who was raised by the Lakota people. Bradley openly acknowledges that he would like to see the repatriation of fossils taken by major East Coast museums a century ago 1 . Others are simply trying to improve the often-rocky relationships of today. In Standing Rock, for instance, the Dakota/Lakota people are now seeking the return of specimens taken from tribal lands by fossil hunters (see  'Fossil fray' ). And in New Mexico, leaders of the Pueblo of Jemez community charge that a scientist from a state museum tricked them into getting access to the reservation, but didn't train a Native American student as they expected in return. In 2004, tribe member Kevin Madalena agreed to help Spencer Lucas, both at the New Mexico Museum of Natural History and Science in Albuquerque, get access to the Jemez reservation to collect geological samples. But Madalena says that Lucas gave him very little instruction, cutting him out of the possibility of co-authoring the resulting article 2 . Lucas, who has been criticized before for the way he deals with students 3 , says that he is surprised to hear of the criticism. But for the young Madalena, the real lost opportunity was for his tribe and its resources. \"It is imperative to educate native communities and non-natives about the importance of guarding palaeontological resources,\" he says. At Standing Rock, they are working on that bond. Last year, the governing tribal council approved a 24-page Paleontological Resource Code, which set out provisions for a palaeontology office, defines agency responsibilities, created permitting requirements, and affirmed tribal ownership rights of specimens. The tribe is thought to be the first in the United States and Canada to adopt such a code, authorities say. The strict rules have taken a while to be taken up by the reservation, which encompasses nearly a million hectares on the west side of the Missouri River. For years, the tribe tried to curb looting by tightly controlling access to the reservation; tribal members were also told to stop picking up any fossils they might find. But in 2004, Gale Bishop, then director of the geology museum at the South Dakota School of Mines and Technology in Rapid City, helped the US Bureau of Indian Affairs and the tribe to collaborate with his museum and the Indian Affairs bureau to assess and develop the palaeontological resources. After Bishop retired in 2006, however, the university and the tribe began to differ over how to complete the survey. Then Grellet-Tinner arrived. The lanky outdoorsman came from Switzerland by way of Texas, Los Angeles and Brazil. Trained as a gemologist, he later turned to palaeontology and explored rich fossil beds in China and Argentina. By the time he got to South Dakota's School of Mines and Technology, his experience of working with different cultures helped him smooth over the troubles between the tribe and the university. Grellet-Tinner straightened out the paperwork and contracted with the tribe to help develop its palaeontology programme. \"The tribe has a tremendous opportunity here \u2014 fantastic palaeontological resources, eager students and the financial support of its leaders,\" he says. The Standing Rock council spent US$50,000 to run the field school this summer, including a second week-long session for half-a-dozen tourists. Tribal council member Henry Harrison credits Grellet-Tinner with turning the palaeontology programme around. The Standing Rock tribe now runs the programme from its headquarters in Fort Yates, North Dakota. The 6,000-member tribe is also building a fossil-preparation laboratory at its Sitting Bull College and hopes to build a palaeontological museum to bring not only tourists and economic development, but also scientists to study specimens found on the reservation. The land contains a wealth of material. The field site is on a working cattle ranch, which is located a 30-kilometre drive down gravel roads from the main highway. Rocks here expose both the Hell Creek Formation \u2014 one of the most famous sedimentary beds for dinosaur fossils, dated at just over 65 million years old \u2014 and the slightly older Fox Hills Formation, at 68 million years old. Both sites record how the great Cretaceous seaway that once divided North America was receding at the time. \"What is now Montana was exposed before the Dakotas,\" says Grellet-Tinner. \"When Montana was a delta, the Dakotas were under water. When Montana was a plain, the Dakotas were becoming a delta.\" With each rainstorm and snow melt, fossils erode out by the hundreds. \"I found sites that were literally bone beds, with so many fossils you couldn't walk on the ground without stepping on fragments,\" says Bishop, who is now at Georgia Southern University in Stateboro. \"There are several world-class deposits on that reservation.\"  \n                Bone bounty \n              This July, students recovered and casted about 40 substantial specimens, which are now at the lab at Sitting Bull College. They include a possible new crocodilian species, along with sea shells that act as markers of the climatic conditions as the great seaway was receding. \"I think this material will lead to some quality publications,\" says Grellett-Tinner. To the budding scientists, the chance of contributing to a scientific article was more than they dreamed of in their camping tents. \"I never imagined it would be like this; it was awesome,\" says Vida Dogskin, a Dakota/Lakota from Sitting Bull College. A single parent with four children, Dogskin is an environmental-science major now following an interest in dinosaurs. \"I jumped in to learn as much as I can,\" she says. Dogskin picked up the trade so quickly that she was hired as a technician apprentice to help store and inventory the summer's finds. For Matthew Wood, a member of the Seminole tribe originally from Oklahoma, the field school was a chance to fulfil a lifelong desire to study dinosaurs. A junior information-science major at Sitting Bull College, he is now looking at a career in palaeontology or a related field. \"I thought I might learn a little, but I learned much more than I expected,\" says Wood. \"It was all hands-on \u2014 an incredible experience.\" The summer's finds are already heading for analysis elsewhere. Grellet-Tinner plans to send ammonite fossil samples from the Fox Hills Formation to the University of Claude Bernard Lyon I in France. Researchers there are preparing the specimens to be analysed in a mass spectrometer to measure their ratio of oxygen-16 to oxygen-18, which can indicate the temperature and depth of the water in which they formed. Grellet-Tinner says the team will also be analysing dinosaur bones \u2014 in part to try to determine whether dinosaurs were warm- or cold-blooded, which is still under debate 4 . Will students who participated in the field-school research be included in the article? \"Of course,\" he says. And one day this may be the model for a new era of paleontological research on tribal lands \u2014 in which Native Americans collaborate with outside experts and share in the scientific analysis and credit. If so, it will be a long way from the days of the wars over bones on the Great Plains. \n                     Standing Rock Tribe \n                   \n                     Standing Rock field school \n                   \n                     Concordia College \n                   \n                     Adventure Safaris \n                   Reprints and Permissions"},
{"file_id": "455277a", "url": "https://www.nature.com/articles/455277a", "year": 2008, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "Bia\u0142owie\u017ca is one of the best-preserved woodlands in Europe. But is it a good reference point for what Europe looked like 5,000 years ago? Emma Marris goes deep into the forest to find out. In dappled light, the trunks of lindens, Norway spruces, and oaks prise the undergrowth and the canopy apart. Quite a few are dead, and draped with beards of moss. Birds sing in the cool, green-smelling air. It is very still, and at first feels almost timeless. But time is here \u2014 time past and time present. Hornbeam limbs, once weighed down with snow, are still bowed at oblique angles in the summer. Red deer bones on the ground mark an old kill; saplings clamber like children on the body of their recently fallen parent. As the wind ruffles the canopy, patches of sun traipse to and fro on the forest floor. Fat, desultory mosquitoes drift through the air. Behind a shrub stands a European bison, or wisent, in the prime of his life, with curled black horns, a high woolly shoulder and a ridged backbone. He is browsing noiselessly. In a few minutes he will glide away as noiselessly as if he were slipping back 1,000 years to when his kind roamed forests like these over most of Europe. The Bia\u0142owie\u017ca forest in eastern Poland is a place apart, and of the past. Grey wolves hunt red deer; woodpeckers \u2014 all ten of Europe's species live here \u2014 hunt for beetles in 500-year-old oaks; stripy young boar splash through alder bogs blooming with yellow irises. The forest spreads over some 146,000 hectares in Poland and Belarus, various parts of it are managed in various ways, and have been cleared at various times. But it is said that the 4,438 hectares at its Polish core have never been cleared. Many of the forest's ecological processes have run uninterrupted since the last ice age, and the trees have never been selected for wood that can be sawn into long, knotless boards. It is thus a reference point, showing how humans have changed forest composition and processes elsewhere. It is a model for sylviculturalists interested in conservation \u2014 or in commerce, as tree plantations that mimic nature may produce more wood. The University of Warsaw's Geobotanical Station and the Polish Academy of Sciences' Mammal Research Institute, both in Bia\u0142owie\u017ca village, study the surrounding forest to get an idea of how things once were. The Mammal Research Institute is most famous for the discovery by one of its staff of the Dehnel effect, in which shrews actually shrink their skeletons to cope with reduced food supply in winter. The forest is indubitably a good place to study species in conditions most like those to which they were adapted. Ornithologists, in particular, love the forest and have noted that bird life there is diverse, but not dense; the reverse of what's seen in agricultural landscapes. But to what extent is Bia\u0142owie\u017ca, as often claimed, Europe's last primeval broadleaf forest? \n               boxed-text \n             In his 1996 book  Natural Woodland  (Cambridge Univ. Press), the British naturalist George Peterken defines three types of naturalness. He contrasts \"original-naturalness\" \u2014 \"the state which existed before people became a significant ecological factor\" \u2014 with \"present-naturalness\", \"the state which would prevail now if people had not become a significant factor\". To see the difference, remember that forests are constantly shaped by fire and storm. In the clearing, seedlings compete for light. Trees fill in the gaps. Soil is depleted and fertilized; climate alters. A forest that has travelled thousands of years down a human-free road will change on the way, as original-naturalness becomes present-naturalness. And both are distinct from \"future-naturalness\", \"the state which would eventually develop if people's influence were completely and permanently removed\". Few environmentalists have adopted Peterken's distinctions. They prefer terms such as 'virgin' and 'old growth' forests, seeing the human factor as more crucial than any other. The problem with this, Peterken notes, is that if human use is taken in its broadest sense \u2014 not just logging \u2014 there is no more virgin forest in North America than there is in Europe. Native Americans managed their woodlands for game and maintained open spaces using fire.  \n                Royal playground \n              Bia\u0142owie\u017ca cannot aspire to present-naturalness. It owes its existence to the Lithuanian dukes, Polish kings, Russian tsars and German Nazis who kept it to themselves for hunting, first to feed their armies, later for sport. The forest ungulates were fed winter hay. Competing predators were killed; there are no more bears in Bia\u0142owie\u017ca. The European mink that once flourished are also gone, their place now taken \u2014 apparently without much ecological disruption \u2014 by Asian raccoon dogs and American mink escaped from fur farms. Before they went extinct in the early seventeenth century, aurochs, the ancestor to domestic cattle, probably shared the forest with wisent and red and roe deer. An unknown poacher shot the forest's last wild bison in April 1919; today's are descended from 13 zoo bison, the descendants of which were reintroduced into the wild in the 1950s. This history of intervention makes the forest's ecology hard to read. Take the recent reduction in Norway spruce. Their share of the woodland, says Bogdan Jaroszewicz, head of the Geobotanical Station, has halved since 1950. Some believe the spruce trees are disappearing because of climate change; the park is at the southern limit of the spruce's range. Others blame the spruce bark beetle, although its effects may be synergistic with, not alternative to, those of climate change. But Jaroszewicz points out that a century ago Russian royalty were breeding so much game in the woods that very few saplings survived to maturity. \"There was almost no regeneration,\" he says. This intense browsing might have had a greater impact on seedlings more succulent than those of spruce. No one can guess whether the spruce would be declining without previous human intervention. What then of the possibility of original-naturalness \u2014 the primeval state preserved? This is certainly the pitch of Bia\u0142owie\u017ca's growing tourist trade. Definitions of 'natural' or 'pristine' or 'primeval' forest typically include trees of many different ages coexisting and the presence of marker species dependent on old or dead wood \u2014 often mosses, lichens and fungi. Bia\u0142owie\u017ca has all these characteristics. Trees age, die and rot where they fall, fuelling the life cycles of innumerable beetles, fungi, woodpeckers and, eventually, the next generation of trees. In Bia\u0142owie\u017ca's core up to one quarter, by volume, of the wood above ground in each hectare is dead, compared with 2% in commercial stands.  \n                Strange forces \n              People have interacted with Bia\u0142owie\u017ca for millennia, but not necessarily as agents of change \u2014 their needs remained constant. Hunting has affected the numbers of large animals, but pollen records suggest that in terms of the species that make up the forest's main constituents, Bia\u0142owie\u017ca has stayed fairly constant for millennia. Tomasz Samojlik, who grew up near the forest, has returned to study its environmental history at the Polish Academy of Sciences' Mammal Research Institute in Bia\u0142owie\u017ca village. He has investigated royal hunts, beekeeping in trees, ore-smelting operations deep in the forest, and more. \"After five or six years I have come to the conclusion that humans have always been connected to the forest,\" he says. For humans, he adds, the forest could be a matter of life and death: \"They were part of it and could be killed by it easily.\" But for the forest, humans were one factor among many \u2014 they shaped it in some ways, left it alone in others, and were no threat to it at all. That was then. \"Now we are some strange force from an outside world,\" says Samojlik. All concerned want to limit that force's impact. The humans who now manage Bia\u0142owie\u017ca struggle to do so in a way that leaves no fingerprints, aspiring, in Peterken's terms, to something close to the future-natural (see  'Flora v. Fauna' ). Tourists may enter only when accompanied by guides. A fence, built in the 1930s to exclude domestic animals, now stands as a barrier between the domains of humans and the woods. Passing through sunny fields to reach a shaded gate into the forest, researchers mark the separation of realms by anointing themselves with mosquito repellent. But future-natural takes more than a fence. The Geobotanical Station's records show rum goings-on over the past 50 years that look like the work of climate: plants flowering earlier, changes in annual distribution of precipitation, changes in snowmelt timing. In a world where today's saplings will experience an atmosphere containing twice as much carbon dioxide as their parents' generation, a human-free future-naturalness is not an option.  A local approximation to future-naturalness may sometimes be possible. Unfortunately, though, Bia\u0142owie\u017ca faces a further handicap in this regard. To deal with change, an ecosystem needs space \u2014 to provide refuges for the species that will recolonize fire and wind clearings, room for ranges to change, and so on. Bia\u0142owie\u017ca is quite small \u2014 only big enough, for example, for the 100-square-kilometre range of a single male lynx. \"We have three or four, because ranges overlap, but hardly enough to sustain a population,\" says Jaroszewicz. The minimum size of a functioning forest probably varies greatly, but a calculation for Canadian boreal forest suggested about 650,000 hectares as the minimum size (S. J. Leroux  et al. Biol. Conserv.   138,  464\u2013473; 2007).  \n                Here and now \n             But if neither present-natural nor original-natural, and with poor odds for becoming future-natural, Bia\u0142owie\u017ca is still the best we have. For researchers looking for a proxy for European forests unchanged by modern man, Bia\u0142owie\u017ca is a starting point; they still have to comb out the possible effects of human activity; to sort through tangled puzzles such as the spruce decline; to account for the small size of the forest and the changes in megafauna; and to race against the clock as climate change rearranges relationships. In the end, it is the biological relationships which constitute the forest that are important to the science of ecology, not what history says about 'touched' and 'untouched'. History is uncertain: easily lost, written by victors, disproportionately concerned with the urban rich. Biology is available now, changing but measurable, and Bia\u0142owie\u017ca has it in abundance. \"Even if we estimate that 5,000 papers have been written about it, we still only know a little about it,\" says Jaroszewicz. \"We still don't know what it will be in 100 years.\" The number of species, the complex interactions researchers track from year to year, and the individual trees, among the oldest and tallest in Europe, have a value beyond any question of whether they have been interfered with or not. Wonder \u2014 formalized in science and experienced directly \u2014 trumps purity when it comes to feeling natural. Bia\u0142owie\u017ca has long been human \u2014 but in a special and timeless way. Once, Samojlik found a sickle knife dropped by a previous visitor to the wood lying just under the tangled grass; it had been there for 1,500 years. Moss-hung and teeming with life, Bia\u0142owie\u017ca retains one of the essential features of the European forest, at least as popularly imagined. It is still profoundly mysterious, even if it is no longer trackless or boundless. See Editorial,  page 263 \n                     Nature Reports Climate Change \n                   \n                     Evolution & Ecology \n                   \n                     Polish Academy of Sciences Mammal Research Institute \n                   \n                     University of Warsaw Geobotanical Center \n                   Reprints and Permissions"},
{"file_id": "455446a", "url": "https://www.nature.com/articles/455446a", "year": 2008, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "Barack Obama accepted  Nature 's invitation to answer 18 science-related questions in writing; John McCain's campaign declined. Obama's answers to many of the questions are printed here; answers to additional questions (on topics including biosecurity, the nuclear weapons laboratories and US participation in international projects) can be found at  http://www.nature.com/uselection.  Wherever possible,  Nature  has noted what McCain has said at other times on these topics. \n               What will be your highest priority in the science and technology arena? \n             Barack Obama:  I am committed to strengthening US leadership in science, technology and innovation, and doing so will be a central priority for my administration. Our talent for innovation is still the envy of the world, but we face unprecedented challenges that demand new approaches. For example, the United States annually imports $53 billion more in advanced technology products than we export. China is now the world's number one high-technology exporter. This competitive situation may only worsen over time because the number of US students pursuing technical careers is declining. The United States ranks 17th among developed nations in the proportion of college students receiving degrees in science or engineering; we were in third place thirty years ago. Federally supported basic research, aimed at understanding many features of nature \u2014 from the size of the Universe to subatomic particles, from the chemical reactions that support a living cell to interactions that sustain ecosystems \u2014 has been an essential feature of American life for over fifty years. While the outcomes of specific projects are never predictable, basic research has been a reliable source of new knowledge that has fuelled important developments in fields ranging from telecommunications to medicine, yielding remarkable rates of economic return and ensuring American leadership in industry, military power and higher education. I believe that continued investment in fundamental research is essential for ensuring healthier lives, better sources of energy, superior military capacity, and high-wage jobs for our nation's future. Yet, today, we are clearly under-investing in research across the spectrum of scientific and engineering disciplines. Federal support for the physical sciences and engineering has been declining as a fraction of gross domestic product for decades, and, after a period of growth of the life sciences, the National Institutes of Health (NIH) budget has been steadily losing buying power for the past six years. As a result, our science agencies are often able to support no more than one in ten proposals that they receive, arresting the careers of our young scientists and blocking our ability to pursue many remarkable recent advances. Furthermore, in this environment, scientists are less likely to pursue the risky research that may lead to the most important breakthroughs. Finally, we are reducing support for science at a time when many other nations are increasing it, a situation that already threatens our leadership in many critical areas of science. This situation is unacceptable. As president, I will increase funding for basic research in physical and life sciences, mathematics and engineering at a rate that would double basic research budgets over the next decade to support our scientists and restore US scientific leadership. \n               John McCain \n               has similarly promised to fight for increased funding at some of the country's leading science agencies, but without specific targets in terms of dollars or time frames. \"Under a McCain administration, science and research will have a very high priority,\" says Jay Khosla, who advises the campaign on health policy issues. \"He will do everything it takes to ensure we will continue to be leaders, especially in the field of innovation.\" One potentially major stumbling block to those looking for more research funding: McCain has said he would freeze domestic discretionary spending, which includes science money, for one year if elected in order to help trim overall spending levels. \n             \n               In general, McCain has stressed less government control and more business- and technology-oriented approaches to spurring innovation, such as a $300 million prize for advanced battery technology. On the campaign trail he touches on many of the same themes as Obama, such as the perceived need to educate more American scientists and engineers; McCain would, for instance, provide bonuses for high-performing teachers in subjects such as maths and science, and support education programmes at science agencies such as the National Science Foundation and the Department of Energy. \n             \n               Biomedical innovation is expensive and very slow; it takes $1 billion and the better part of a decade to develop a new drug. What would your administration do to make it easier to turn research into cures? \n             Obama:  Americans have good reasons to be proud of the extraordinary role that medical science has had in combating disease, here and throughout the world, over the past century. Work sponsored by the NIH, other government agencies and our pharmaceutical and biotechnology industries has produced many vaccines, drugs and hormones that have improved the quality of life, extended life expectancy and reduced the dire consequences of many serious illnesses and disabilities. While it may never be easy to \"turn research into cures\", I understand that biomedical scientists are seeing enhanced opportunities to use their science to improve health. I will encourage the development of biological markers of disease that might simplify the evaluation of new therapies, the use of genetic information to select patients most likely to benefit from new treatments, and the multi-disciplinary efforts that are now possible at many research centres. In addition, I will support increased attention to research that focuses on prevention, early detection and improved management of disease. Furthermore, I believe there is more that we can do to ensure new treatments are developed and made available to the public more efficiently. I believe that we must increase funding for the NIH to reverse the funding trends that have left our nation's scientists with fewer resources as research costs escalate. We must also do a better job of providing resources to the Food and Drug Administration (FDA), the Centers for Disease Control and Prevention (CDC), and other federal agencies that help ensure that when these medical advances are turned into exciting new treatments, we are able to ensure that they are swiftly and safely considered for widespread usage. Additionally, we must prioritize removing barriers both between federal agencies and across public, private and nonprofit organizations to ensure better and more efficient collaboration on new innovations. \n               McCain \n               has also said he would strongly support funding for the NIH. \"McCain wants to make sure we are doing everything possible to give the young scientists of today the resources they need to go out and bring research to cures,\" says Khosla. The health-care platforms of both candidates tend to focus on how to make health insurance more affordable and accessible to Americans, and McCain has talked about technologies that could benefit public health \u2014 such as telemedicine \u2014 to bring the latest medical knowledge to many more patients. \n             \n               Many scientists are bitter about what they see as years of political interference in scientific decisions at federal agencies. What would you do to help restore impartial scientific advice in government? \n             Obama:  Scientific and technological information is of growing importance to a range of issues. I believe such information must be expert and uncoloured by ideology. I will restore the basic principle that government decisions should be based on the best-available, scientifically valid evidence and not on the ideological predispositions of agency officials or political appointees. More broadly, I am committed to creating a transparent and connected democracy, using cutting-edge technologies to provide a new level of transparency, accountability and participation for America's citizens. Policies must be determined using a process that builds on the long tradition of open debate that has characterized progress in science, including review by individuals who might bring new information or contrasting views. I have already established an impressive team of science advisers, including several Nobel laureates, who are helping me to shape a robust science agenda for my administration. In addition I will: \u2022 Appoint individuals with strong science and technology backgrounds and reputations for integrity and objectivity to the growing number of senior management positions in which decisions must incorporate science and technology advice. These positions will be filled promptly with ethical, highly qualified individuals on a non-partisan basis; \u2022 Establish the nation's first Chief Technology Officer (CTO) to ensure that our government and all its agencies have the right infrastructure, policies and services for the twenty-first century. The CTO will lead an interagency effort on best-in-class technologies, sharing of best practices and safeguarding of our networks; \u2022 Strengthen the role of the President's Council of Advisors on Science and Technology (PCAST) by appointing experts who are charged to provide independent advice on critical issues in science and technology. The PCAST will once again be advisory to the president; and \u2022 Restore the science integrity of government and restore transparency of decision-making by issuing an Executive Order establishing clear guidelines for the review and release of government publications, guaranteeing that results are released in a timely manner and not distorted by the ideological biases of political appointees. I will strengthen protection for 'whistle blowers' who report abuses of these processes. \n               McCain \n               has similarly pledged to fill key technical positions in his administration with qualified scientists and engineers, including having a science adviser working directly with the president. \"McCain will seek to restore the credibility of scientific research\" in the federal government, says campaign adviser Floyd DesChamps. McCain has argued that taxpayers' investment in scientific research should be repaid with the untarnished results of that work. \n             \n               What role does nuclear power have in your vision for the US energy supply, and how would you address the problem of nuclear waste? \n             Obama:  Nuclear power represents an important part of our current energy mix. Nuclear also represents 70% of our non-carbon generated electricity. It is unlikely that we can meet our aggressive climate goals if we eliminate nuclear power as an option. However, before an expansion of nuclear power is considered, key issues must be addressed, including security of nuclear fuel and waste, waste storage and proliferation. The nuclear waste disposal efforts at Yucca Mountain [in Nevada] have been an expensive failure and should be abandoned. I will work with the industry and governors to develop a way to store nuclear waste safely while we pursue long-term solutions. \n               McCain \n               has proposed building 45 new nuclear power plants by 2030, with an eventual goal of a total of 100. McCain has not addressed where the nuclear waste from these and current reactors would go, and he has supported the Yucca Mountain storage project in the past. \n             \n               You support a cap-and-trade system for regulating greenhouse-gas emissions; what lessons from the European emissions-trading system would you implement? \n             Obama:  I will implement a market-based cap-and-trade system to reduce carbon emissions by the amount scientists say is necessary: 80% below 1990 levels by 2050. While Europe has had important successes with its system, it also has made mistakes that we should learn from. Unlike the European system, my plan would aim to cover virtually all greenhouse-gas emissions, would auction off all of the permits instead of giving them away, and would make sure there was stability in the market for permits and their price. My plan would use the proceeds of the auction for investments in a clean-energy future, habitat protection and rebates and other transition relief for families. \n               McCain \n               has described his own vision of a cap-and-trade system, but with a different target; the McCain plan calls for reductions of emissions by 60% below 1990 levels by 2050. McCain would initially give away emissions permits instead of auctioning them. McCain would also allow emissions allowances to be 'banked' or 'borrowed' for different time periods, as well as establish a national 'strategic carbon reserve' that could release permits during difficult economic times. He would also allow unlimited offsets, from both domestic and international sources, to ease into a newly set up cap-and-trade system. \n             \n               Does your stance on tapping domestic oil reserves stand at odds with your goals for reducing national emissions and combating climate change? How will you balance the two? \n             Obama:  With 3% of the world's oil reserves, the United States cannot drill its way to energy security. But US oil and gas production plays an important role in our domestic economy and remains critical to prevent global energy prices from climbing even higher. There are several key opportunities to support increased US production of oil and gas that do not require opening up currently protected areas. Increasing domestic oil and gas production in the ways I propose in no way lessens my commitment to combating climate change, one of the great challenges of our time. I am committed to implementing a market-based cap-and-trade system to reduce carbon emissions 80% below 1990 levels by 2050, and I will start reducing emissions immediately by establishing strong annual reduction targets with an intermediate goal of reducing emissions to 1990 levels by 2020. \n               McCain \n               currently favours a more aggressive offshore-drilling policy than Obama; both candidates, like the Democratic-led Congress, have changed their earlier stances opposing such drilling in the face of rising oil prices and public pressure to do something about it. However, McCain sees climate change as a national security issue, and maintains that it is a major priority for him. He emphasizes developing new emissions-reducing technologies with minimum costs in order to soften any blow to the national economy. McCain's intermediate goal for emission reductions is also 1990 levels by 2020. \n             \n               Do you believe that evolution by means of natural selection is a sufficient explanation for the variety and complexity of life on Earth? Should intelligent design, or some derivative thereof, be taught in science class in public schools? \n             Obama:  I believe in evolution, and I support the strong consensus of the scientific community that evolution is scientifically validated. I do not believe it is helpful to our students to cloud discussions of science with non-scientific theories like intelligent design that are not subject to experimental scrutiny. \n               McCain \n               said last year, in a Republican primary debate: \"I believe in evolution. But I also believe, when I hike the Grand Canyon and see it at sunset, that the hand of God is there also.\" In 2005, he told the Arizona Daily Star that he thought \"all points of view\" should be available to students studying the origins of humanity. But the next year a Colorado paper reported him saying that such viewpoints should not be taught in science class. \n             \n               Would you lift President Bush's ban on federal funding for research on human embryonic stem-cell lines derived after 9 August 2001? Under what conditions do you find it acceptable to create a human embryonic stem-cell line? \n             Obama:  Stem-cell research holds the promise of improving our lives in at least three ways \u2014 by substituting normal cells for damaged cells to treat diabetes, Parkinson's disease, spinal-cord injury, heart failure and other disorders; by providing scientists with safe and convenient models of disease for drug development; and by helping to understand fundamental aspects of normal development and cell dysfunction. For these reasons, I strongly support expanding research on stem cells. I believe that the restrictions that President Bush has placed on the funding of human embryonic stem-cell research have handcuffed our scientists and hindered our ability to compete with other nations. As president, I will lift the current administration's ban on federal funding of research on embryonic stem-cell lines created after 9 August 2001 through executive order, and I will ensure that all research on stem cells is conducted ethically and with rigorous oversight. I recognize that some people object to government support of research that requires cells to be harvested from human embryos. However, hundreds of thousands of embryos stored in the United States in  in vitro  fertilization clinics will not be used for reproductive purposes, and will eventually be destroyed. I believe that it is ethical to use these extra embryos for research that could save lives when they are freely donated for that express purpose. I am also aware that there have been suggestions that human stem cells of various types, derived from sources other than embryos, make the use of embryonic stem cells unnecessary. I don't agree. While adult stem cells, such as those harvested from blood or bone marrow, are already used for treatment of some diseases, they do not have the versatility of embryonic stem cells and cannot replace them. Recent discoveries indicate that adult skin cells can be reprogrammed to behave like stem cells; these are exciting findings that might in the future lead to an alternate source of highly versatile stem cells. However, embryonic stem cells remain the 'gold standard', and studies of all types of stem cells should continue in parallel for the foreseeable future. Rather than restrict the funding of such research, I favour responsible oversight of it, in accordance with recent reports from the National Research Council (NRC). Recommendations from the NRC reports are already being followed by institutions that conduct human embryonic stem-cell research with funds from a variety of sources. An expanded, federally supported stem-cell research programme will encourage talented US scientists to engage in this important new field, will allow more effective oversight, and will signal to other countries our commitment to compete in this exciting area of medical research. McCain 's  stance on embryonic stem-cell research has been the subject of much speculation among researchers. He has voted twice before to lift President Bush's funding restrictions on such work, but his running mate Sarah Palin opposes the work. His public position is perhaps best summarized in his response to questionnaires from advocacy groups such as Research!America last year and ScienceDebate2008 this year: \"While I support federal funding for embryonic stem-cell research, I believe clear lines should be drawn that reflect a refusal to sacrifice moral values and ethical principles for the sake of scientific progress. Moreover, I believe that recent scientific breakthroughs raise the hope that one day this debate will be rendered academic. I also support funding for other research programmes, including amniotic fluid and adult stem-cell research which hold much scientific promise and do not involve the use of embryos. I oppose the intentional creation of human embryos for research purposes and I voted to ban the practice of 'fetal farming', making it a federal crime for researchers to use cells or fetal tissue from an embryo created for research purposes.\" \n               Do you see astronauts on the Moon as a worthy goal for the country? \n             Obama:  I believe that the United States needs a strong space programme to help maintain its superiority not only in space, but also here on earth in the realms of education, science, technology, the environment and national security. Technology developed for space missions has been applied to improve everything from computers and medical technology to baby formula and automobiles. As president, I will establish a robust and balanced civilian space programme. In achieving this vision, I will reach out to include international partners and to engage the private sector to amplify NASA's reach. I believe that a revitalized NASA can help America maintain its innovation edge and contribute to American economic growth. I will re-establish the National Aeronautics and Space Council, which will coordinate civilian, military, commercial and national security space activities and report to the president. This council will oversee a comprehensive and integrated strategy and policy dealing with all aspects of the government's space-related programmes, including those being managed by NASA, the Department of Defense, the National Reconnaissance Office, the Department of Commerce, the Department of Transportation and other federal agencies. It will solicit public participation, engage the international community and work toward a twenty-first-century vision of space that constantly pushes the envelope on new technologies as it pursues a balanced national portfolio that expands our reach into the heavens and improves life here on Earth. Human spaceflight is important to America's political, economic, technological and scientific leadership. I will support renewed human exploration beyond low Earth orbit. I endorse the goal of sending human missions to the Moon by 2020, as a precursor in an orderly progression to missions to more distant destinations, including Mars. \n               McCain \n               has released an extensive space platform, including calling space exploration a \"top priority\" for the country and manned space flight \"a reflection of national power and pride\". Unlike Obama, McCain has explicitly committed to funding the Constellation programme to replace the space-shuttle fleet (although without details on how he would accomplish that). He also says he would maintain the nation's space infrastructure, including the related workforce, and focus on maximizing the research possibilities of the International Space Station. He would maintain investments in aeronautics research as well as the infrastructure for Earth-monitoring satellites. \n             \n               Would it make sense for more overseas students who receive PhDs at American universities to stay in the country and contribute to its research base and its wealth? What immigration reforms would you support? \n             Obama:  I believe that we must enact comprehensive immigration reform to restore our economic strength, relieve local governments of unfair burdens stemming from an inefficient federal immigration system, ensure that our country and borders remain secure and allow a path to citizenship for the 12 million undocumented immigrants who are willing to pay a fine, pay taxes, and learn English. A critical part of comprehensive immigration reform is turning back misguided policies that since 9/11 have turned away the world's best and brightest from America. As president, I will improve our legal permanent resident visa programmes and temporary programmes to attract some of the world's most talented people to America. McCain ,  as a senator from Arizona, has long been involved in immigration issues, mainly through strengthening federal security at land border crossings. He supports immigration reforms to allow more highly skilled workers to stay and work in the United States after graduation. For more on the US election, see  www.nature.com/uselection  See Editorial,  page 431 \n                     US elections special \n                   \n                     John McCain \n                   \n                     Barack Obama \n                   \n                     Scientists and Engineers for America page on the candidates \n                   \n                     Science Debate 2008 \n                   \n                     AAAS page on the candidates \n                   Reprints and Permissions"},
{"file_id": "455281a", "url": "https://www.nature.com/articles/455281a", "year": 2008, "authors": [{"name": "John Whitfield"}], "parsed_as_year": "2006_or_before", "body": "This summer a group of high-profile researchers met in Altenberg, Austria, to try and plot the future course of evolutionary theory. John Whitfield was there. \"Oh my gosh,\" says Massimo Pigliucci, \"maybe I shouldn't use that term.\" Pigliucci, responding to comments on his talk about how living things respond to their environment, and what it means for evolution, has just let slip the p-word. Later the same day, G\u00fcnter Wagner, an evolutionary theorist at Yale University in New Haven, Connecticut, puts up a slide bearing the words 'Postmodern Synthesis'. Pigliucci is moved to make an editorial suggestion from the floor: \"I'd really rather we didn't use that term.\" Wagner says the slide was intended to be tongue-in-cheek, but Pigliucci is worried about the impression the word creates: \"If there's one thing we don't want, it's for people to get the idea that there's a bunch of evolutionary theories out there, and that they're all equal.\" A lot of scientists loathe what they take to be postmodernism's intellectual relativism, and shy away from using the word. But doing so puts Pigliucci in something of a bind. An evolutionary ecologist at the State University of New York in Stony Brook, Pigliucci is one of the conveners of this small meeting on the future of evolutionary thought taking place at the Konrad Lorenz Institute for Evolution and Cognition Research in Altenberg, Austria. The meeting has received a fair amount of hype \u2014 in the blogosphere it was dubbed 'The Woodstock of Evolution'. Its agenda is, pretty explicitly, to go beyond the 'modern synthesis' that has held sway in evolutionary theory since the middle of the twentieth century. And in everyday speech, it is pretty clear what comes after the modern. What's more, some of this work sounds as though it fits the term quite nicely. Over dinner at the meeting's end, Pigliucci expresses his hope of \"moving from a gene-centric view of causality in evolution to a pluralist, multilevel causality\". Postmodernists in the humanities call this 'decentering', and they are all for it. Over the course of the meeting, it's fairly clear that the means to this pluralist end are being sought through mixing and matching neglected ideas and old problems from biology's past with the latest experimental and analytical techniques. Apply that sort of bricolage to architecture and you get the sort of brutalist-right-angle here, classical-column-there, swirling-titanium-ceiling-above-it-all look that is normally pigeonholed, for better or worse, as postmodern.  \n                Evolution of ideas \n              Leaving aside the troublesome adjective, what is the modernism that the Altenburg meeting is meant to move beyond \u2014 or to use Pigliucci's preferred term, 'extend' 1 ? Between about 1920 and 1940, researchers such as the American Sewall Wright and the Englishmen Ronald Fisher and J. B. S. Haldane took Charles Darwin's ideas about natural selection and Gregor Mendel's insights into how traits pass from parents to offspring \u2014 which many biologists of the time believed antithetical \u2014 and fused them into a mathematical description of the genetic makeup of populations and how it changes. That fusion was the modern synthesis. It treats an organism's form, or phenotype, as a readout of its hereditary information, or genotype. Change is explained as one version of a gene being replaced by another. Natural selection acts by changing the frequency of genes in the next generation according to the fitness of phenotypes in this one. In this world view, the gene is a black box, its relationship to phenotype is a one-way street, and the environment, both cellular and external, is a selective filter imposed on the readout of the genes, rather than something that can influence an organism's form directly. What's wrong with this picture, say the would-be extenders at Altenberg and elsewhere, is what it leaves out. Molecular biology, cell biology and genomics have provided a much richer picture of how genotypes make phenotypes. The extenders claim that enough insights have now come from this and other research for it to be time to re-examine problems that the modern synthesis doesn't address. These problems include some of the key turning points in evolution: the patterns and changes seen in the fossil record as new branches spring from the tree of life and new anatomies \u2014 skeletons, limbs, brains \u2014 come into being. \"When the public thinks about evolution, they think about the origin of wings and the invasion of the land,\" says Graham Budd, a palaeobiologist at the University of Uppsala, Sweden. \"But these are things that evolutionary theory has told us little about.\"  \n                Bring on the kangaroos \n              The question of how form changes in individuals is the province of developmental biology, and genetic studies have now revealed a lot about how the mechanisms of development have evolved. Many see the evolutionary developmental biology \u2014 'evo-devo' \u2014 that is emerging from this work as the key ingredient needed to extend or surpass the modern synthesis. \"Evolution needs a theory of body construction and change, as well as population construction and change,\" says Scott Gilbert, an evo-devo researcher at Swarthmore College in Pennsylvania, who was not in Altenberg but who is writing a book on extending the evolutionary synthesis in similar directions. \"The modern synthesis is remarkably good at modelling the survival of the fittest, but not good at modelling the arrival of the fittest.\" To explain the production of novel features, such as limbs and feathers, Gilbert and like-minded biologists want a theory in which the environment is defined broadly enough to include the developing body, which is the primary context in which the genes are expressed. Genes shape this developing environment, but the dynamic environment also shapes the expression of the genes. And it does so directly, rather than through some later selection. \"The gene will continue to be centre stage,\" says Gilbert, \"but it will be seen as both active and acted upon. It's not going to be the unmoved mover.\" The importance of the environment acting on the genome can be seen in plasticity, the ability of the same genes to give rise to radically different phenotypes in different conditions \u2014 as studied by several of the Altenberg group. Pigliucci, who works on invasive plant species, gave the example of species that lie low in a new environment for several years before becoming a problem. He puts this down to plasticity and the Baldwin effect. In 1896 James Baldwin, an American psychologist, suggested that over the generations, tricks that at first have to be learned can become hard-wired as genes fix variations caused by the environment. \"It could be that the plants arrive in a new environment and hang on thanks to plasticity \u2014 it gains time for natural selection to kick in,\" says Pigliucci. To begin with, the genes follow adaptation rather than leading it, as \"bookkeepers of what's happening\". Once the genes have caught up, and the immigrant can take adaptation to the environment as read, it is able to become dominant. Plasticity also allows organisms to make the most of their mutations. \"The myopic view \u2014 that we don't need to worry about phenotypic variation, that it is abundant, always small and that it goes in all possible directions \u2014 doesn't correspond to the conservation we've seen in developmental systems,\" Marc Kirschner, a systems biologist at Harvard University in Cambridge, Massachusetts, told the Altenberg meeting. To grow a limb you don't need mutations in every gene involved in limb building; life can use the facts that muscle cells naturally align with bone, nerve cells stabilize when they plug into muscles, and blood vessels grow towards areas low in oxygen to leverage a small genetic change into an important difference. Again, the changing environment within the developing body is part of the process by which the gene is expressed: Kirschner calls it facilitated variation 2 . As an example, he points to the discovery that the narrow, tweezer-like beak of an insect-eating finch can become the fat, nutcracking beak of a seed-eater by increasing the activity of a single gene involved in bone formation 3 . \"Because developmental systems are so integrated and self-regulating, you can make a large functional change without a large genetic change,\" says Kirschner. Pigliucci gave a more speculative example of the possible evolutionary consequences of such changes, showing a slide juxtaposing a kangaroo and a dog that had been born without forelimbs but learnt to walk on its hind legs. \"It's hard to imagine that this kind of change doesn't have anything to do with the evolution of bipedalism,\" he told the meeting.  \n                Self-organizing cells \n              Pigliucci and Kirschner think that the capacity of small genetic changes to trigger large shifts results in waves of innovation separated by seeming lulls in which evolution stablizes and integrates the new arrangements. This matches some aspects of the fossil record, where bursts of innovation and diversification are interspersed by much longer periods of stasis \u2014 a pattern known as punctuated equilibrium, first described by the late Stephen Jay Gould and Niles Eldredge of the American Museum of Natural History in the 1970s. Gilbert, who studies turtles, sees something similar: \"Turtle biologists joke that one Tuesday in the late Triassic there weren't any turtles, and by the weekend the world was full of turtles. One reason why might be that it's not all that hard to make a shell \u2014 all the genes are probably there already, and it doesn't take many changes to get a shell.\" Stuart Newman, a developmental biologist at New York Medical College, takes such ideas further than most, arguing that the abilities that cells have to self-organize into complex structures can lead to major evolutionary innovations such as the origin of the vertebrate limb \u2014 a problem on which he collaborates with Altenberg's other organizer, evo-devo researcher Gerd Muller of the University of Vienna, Austria 4  \u2014 with perhaps little or no genetic change. \"You can't deny the force of selection in genetic evolution,\" says Newman, \"but in my view this is stabilizing and fine-tuning forms that originate due to other processes.\" The same process might have given rise to animals themselves. The further you turn back the clock through geological time, Newman believes, the weaker genetic regulation of development becomes relative to plasticity and self-organization. The development of the most basic features of multicellular organisms some 600 million years ago, in the late Proterozoic, might have been the rapid and spontaneous result of molecules already present on unicellular organisms doing new jobs when cells stick together 5 . \"You don't need incremental change under gradual selection regimes to get attributes such as segmented, hollow or multilayered bodies,\" says Newman. \"You can get it all with thermodynamics and self-organization.\" The problem is testing such ideas. Newman suggests that knocking out the genes that stabilize development in model laboratory organisms might provide insights, but extrapolating back from modern organisms to their distant ancestors is fraught with problems. It is difficult to see how such an approach can get beyond the theoretical, says Budd, adding that what evidence there is weighs against Newman's hypothesis. \"Clearly there are physical and chemical processes that affect cells,\" says Budd. \"But I don't think there is any evidence at all for the idea that development was more permissive and plastic [in the Proterozoic] and that body plans could spontaneously emerge. The fossil record shows that body plans appeared sequentially in a series of innovations, not in a misty way at the bottom of the tree.\" Confusing what can happen and what did happen is a common criticism of the ideas raised at Altenberg. For example, some lab studies lend support to the Baldwin effect: experiments with fruitflies show that following up an environmental stress with selective breeding can produce animals that show the phenotypic response to that stress without having experienced it 6 . But there is little evidence so far that genetic change in wild populations takes this course, says Wagner. \"The idea that environmentally induced changes are the path-breaker for genetic fixation is an old one, but I'm not yet convinced that's how it works in real populations,\" he says. \"These notions haven't forced us to change the neo-darwinian paradigm,\" says Jerry Coyne, an evolutionary geneticist at the University of Chicago. Coyne has little time for \"evo-devotees\" 7  who think that the discipline will cause a revolution in biology. Researchers coming at evolution from population genetics are particularly resistant to any attempt to displace natural selection from the place at the heart of evolutionary theory that the modern synthesis provided it with. \"The whole thing about natural selection being an insufficient paradigm seems grossly overblown,\" says Coyne. \"There are a lot of interesting new things coming out that will change our view of evolution. But to say the modern synthesis is incomplete or fatally flawed is fatuous.\" And it is worth noting that you can work in evo-devo and not subscribe to such ideas. Sean Carroll of the University of Wisconsin in Madison sees things in terms of bridge-building, not replacement. \"What did population genetics and palaeontology have to do with each other for the past 80 years? Nothing. The modern synthesis describes evolution within populations \u2014 it's agnostic or silent about the cumulative effect of that process,\" he says. By revealing the genetic basis of development, and showing how genetics relates to morphology, evo-devo \"sits right in the middle\" of the two disciplines, says Carroll. The true message of evo-devo, Carroll says, is that developmental processes have evolved in a way that allows small aspects of form to be tweaked without affecting the whole organism \u2014 something which tends to reinforce the modern synthesis's view of evolution as incremental 8 . \"Because we can get large effects when we manipulate genes in development, the spectre that these things have happened in history is out there,\" says Carroll. \"But just because we can make freaky-looking animals in one step, I'm unwilling to say that evolution works that way.\" Wagner and his colleagues have recently shown that altering many genes in mice produces only a small effect 9 , countering the idea that most individual genes have such a wide-ranging influence that changing them would be fatal. The differences of opinion suggest that, although evo-devo may once have looked as if it would unify population genetics and development, so far it has done more to give new voice to important problems that had been pushed to the margin \u2014 this was a strong note at Altenberg, making the meeting as much about revivalism as revolution. \"Originally, the idea was that evo-devo was going to be the synthesis between evolution and development \u2014 now it is part of what needs to be done to get there,\" says Alan Love, a philosopher of science at the University of Minnesota in Minneapolis who attended Altenberg. \"There is still a lot of outstanding work to do on fitting the pieces together, but no consensus on how to go about that right now.\" Nevertheless, he says, that's no cause for alarm. \"What is needed is to incorporate empirical findings into the bigger picture. It took populations genetics 25 years to do that and make the modern synthesis. As far as evo-devo goes, I'd say we're smack dab in the middle of that process.\"  \n                Preaching to the converted? \n              David Krakauer, an evolutionary theorist at the Santa Fe Institute in New Mexico who was not at Altenberg, agrees. \"It's a matter of finally unifying two areas that haven't spoken to one another,\" he says. \"To tackle any modern problem in evolutionary biology, you'll have to use development and the dynamics of the genes that underlie it.\" He's quite enthusiastic about the possibility of bringing together mathematical theories of pattern formation, of the kind favoured by Newman, and the large body of theory on genetic change between generations used by population geneticists such as Coyne. But at the same time, he can see forces beyond the content of the theories that may keep them apart: \"It's not about totally incompatible world views, it's about who holds the torch \u2014 who are the legitimate heirs to the Darwinian intellectual estate.\" Love saw the Altenberg meeting as an attempt to bridge the divide, but one that, by avoiding conflict (partly through invitations being declined), ended up a little one-sided. \"Altenberg was an attempt to pull people together; the hard part was that it didn't pull in people who were less than sympathetic towards one another,\" he says. \"It could have been a much more eraser-throwing meeting, but there is no reward for organizing that \u2014 you don't get another grant by trying to get people in the same room, you just have to take time away from the lab or fieldwork.\" And there are forces at play beyond jockeying for disciplinary prestige. Never mind what can happen and what did happen. What should happen? It's a fight that evolutionary theory \u2014 rooted as it is in a world view shaped by Victorian capitalism \u2014 has always found itself dragged into. To give one example, the championing of 'punctuated equilibrium' in the fossil record by Gould and Eldredge was easily construed by participants on both sides of the debate in the 1970s as an attack from the political left \u2014 part of a broader rising of hackles at the arrival of sociobiology, selfish genes and the like. Evolutionary ideas and political metaphors still seem to seek each other out \u2014 in an extended synthesis, says Gilbert, \"the gene will be a much more constitutional monarch, taking instructions from the cell and environment\". Eva Jablonka of the University of Tel Aviv, Israel, is explicit about a political side to her work. She advocates the importance of epigenetic inheritance \u2014 traits that can be passed on without changes to DNA sequence. These can be induced by environmental stressors such as temperature, diet or environmental chemicals. Such mechanisms, and insults, may be behind some inherited diseases, she says, in which case we have a responsibility to curb and reverse them. \"There are social implications to our approach,\" says Jablonka. \"Our way of looking at heredity and evolution counters genetic determinism and its political implications.\" Jablonka is one of the Altenberg attendees most comfortable with the term 'postmodern'. Yet there was no sense at Altenberg of a desire to attack evolutionary theory from the left. Quite the reverse \u2014 the dominant political concern was a fear of attack from fundamentalists. As Gould discovered, creationists seize on any hint of splits in evolutionary theory or dissatisfaction with Darwinism. In the past couple of decades, everyone has become keenly aware of this, regardless of their satisfaction or otherwise with the modern synthesis. \"You always feel like you're trying to cover your rear,\" says Love. \"If you criticize, it's like handing ammunition to these folks.\" So don't criticize in a grandstanding way, says Coyne: \"People shouldn't suppress their differences to placate creationists, but to suggest that neo-Darwinism has reached some kind of crisis point plays into creationists' hands,\" he says. It is tempting to say that it's not just genes that express themselves in an environment that responds and reshapes itself around them, feeding back and complicating matters beyond simple cause and effect; the same applies to ideas. And if that seems a bit self-referential \u2014 well, that's postmodernism. John Whitfield is the author of  In the Beat of a Heart: Life, Energy, and the Unity of Nature . There is a  Correction  associated with this article: we should have identified Susan Mazur as the first to use the term 'The Woodstock of Evolution' to describe the Altenberg meeting. \n                     Heredity journal \n                   \n                     Konrad Lorenz Institute workshop homepage \n                   \n                     Altenberg 2008: What happened? \n                   \n                     Extended synthesis workshop \n                   \n                     Massimo Pigliucci's blog \n                   Reprints and Permissions"},
{"file_id": "455451a", "url": "https://www.nature.com/articles/455451a", "year": 2008, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "A new president could bring radical shifts to America's major research entities.  Nature  profiles some of the agencies in need of a makeover. \n                Going for the stars \u2014 or going broke? \n             When  NASA's  rockets shot astronauts to the Moon, the Apollo programme had the backing of a nation \u2014 and its chequebook. At its peak in 1966, NASA was spending just under $6 billion a year. That was 4% of the federal budget at the time, and nearly $38 billion in today's money. Some things are similar today. Four years ago, President George W. Bush revived the notion of a Moon shot with his Vision for Space Exploration. And although it's not quite the space race of the 1960s, international competition is heating up again, thanks to heightened tensions with Russia over the Georgia conflict plus nascent competition with China, which plans to conduct its first spacewalk shortly. But the main difference is money. Today, NASA's budget is $17.3 billion, less than 0.6% of federal spending. NASA is now in a squeeze as it tries to develop its next Moon rockets, a programme called Constellation that is scheduled to lift astronauts into orbit by 2015, and to the Moon by 2020. When either John McCain or Barack Obama takes office next year, he will have simple alternatives, says Len Fisk of the University of Michigan in Ann Arbor. \"You've got two choices: more money or less programme,\" says Fisk, a former NASA science chief and former chair of the National Academies' Space Studies Board. NASA administrator Mike Griffin has tried to find more money for Constellation by holding firm to a 2010 retirement for the space shuttle fleet, which costs more than $3 billion annually. From 2010 until 2015, when the Constellation rocket would theoretically be ready, NASA would have to buy rides aboard Russia's Soyuz spacecraft to ferry astronauts to the International Space Station. The gap \u2014 already deemed \"unseemly\" by Griffin \u2014 has become even more prominent in an election year in which Florida, a crucial swing state, has thousands of shuttle-related jobs at stake. Obama has said he would support additional shuttle flights, and a $2-billion boost to NASA's budget to that end. McCain, along with two other senators, recently sent a letter to the White House imploring the Bush administration to leave open the option of extending shuttle flights beyond 2010. But if campaign promises don't materialize in congressional spending bills, the agency will have to find other ways to squeeze more out of a budget that has remained roughly flat in real terms for the past two decades. One way would be to improve accountability in its contracts, suggests planetary scientist Alan Stern, who left NASA in March after a one-year stint as science chief. Within the science mission directorate alone, which receives roughly a quarter of NASA's budget, 10 missions had collectively been delayed by 85 months and run up $430 million in unanticipated costs within the past two years. \"We have weather satellites that cost $3.5 billion. This is absurd. We have Mars missions that have tripled in cost and no one blinks an eye,\" says Stern. \"You reward those who do not show discipline at the expense of those who did.\" \n               Eric Hand \n             \n                Curing the country \n              The 2008 presidential election comes at a critical juncture for the  National Institutes of Health  (NIH). From 1998 to 2003, the biomedical agency's budget doubled to $27 billion; in the five years since, its purchasing power has eroded as congressional increases have failed to keep pace with inflation. The transition from rapid growth to freeze has shaken the nation's biomedical enterprise, causing senior people to face odds as low as one in ten of getting funding. Elias Zerhouni, the NIH director, this month singled out funding difficulties as hampering the agency's long-term planning. Like all federal agencies, the NIH gets its money through annual congressional appropriations, and the numbers cannot be depended on from year to year. \"The way the process works does not allow us to do good medium- and long-term capital investments,\" Zerhouni told a congressional subcommittee. A new president won't be able to change the way Congress funds agencies, but he might have something to say about increasing the total bottom line for the NIH. Both leading candidates have come out in favour of biomedical research spending, with subtle but possibly important differences between the two. In a 2007 questionnaire by advocacy group Research!America, Barack Obama wrote: \"I strongly support increasing funding for the NIH.\" John McCain responded to the same question with the ever so slightly different words: \"I strongly support funding for the NIH.\" To NIH supporters, the writing on the wall seems, if not crystal clear, at least legible. \"It's clear that Senator Obama will act to increase funding for the NIH. We do not see that in the McCain response,\" says Mary Woolley, president of Research!America, which is based in Alexandria, Virginia. Woolley nonetheless believes that the force of public opinion is so strongly in favour of medical research that McCain will deliver on an NIH increase. Other NIH advocates are similarly positive. \"We are very optimistic that a new administration, regardless of which party is elected, will support renewed investment in the NIH and other science agencies,\" says Richard Marchase, president of the Federation of American Societies for Experimental Biology and vice-president of research at the University of Alabama, Birmingham. That may be wishful thinking in an era of blooming US budget deficits and growing pressure on Congress to curb spending. Even with presidential backing, cooperative lawmakers \u2014 including NIH advocates and purse-string holders Senator Tom Harkin (Democrat, Iowa) and Representative David Obey (Democrat, Wisconsin) \u2014 may not be able to deliver NIH increases in the face of multiple, competing priorities. And McCain has floated the idea of a one-year spending freeze across the board for all agencies to help cope with the ballooning federal deficit. Adding to the morose budgetary atmosphere is a ban on federal funding for research done on human embryonic stem cells lines generated after 9 August 2001. US researchers have watched their international colleagues push ahead in what is arguably one of the most promising areas of biomedical research. In the most recent fiscal year, only $42 million of the $650 million spent by the NIH on stem-cell research went to work on human embryonic lines. And although many expect the ban, put in place by President Bush, to be lifted no matter who is elected \u2014 McCain and Obama have both voted in favour of lifting the ban in the past \u2014 the choice of conservative Sarah Palin as McCain's running mate has some researchers wondering what that might signal (see  page 442 ). \n               Meredith Wadman \n             \n                Cleaning up the mess \n              Environmentalists and scientists alike are hoping that the next president will set the  Environmental Protection Agency  (EPA) on a new course. Under President Bush, the agency has struggled against accusations of political interference in scientific decisions dating back to 2002 when the White House reportedly meddled with the EPA's assessments on global warming. By July this year, things at the agency had deteriorated to the point that leading Senate Democrats were calling on its chief, Stephen Johnson, to resign. The main fight is over global warming. In 2007, the US Supreme Court ruled that the EPA had authority to regulate carbon dioxide pollution from vehicles. Johnson, though, denied California its application to separately regulate CO 2 , saying the problem should be addressed at the federal, not local, level. In that decision he overruled his staff, reportedly after receiving pressure from the White House to do so. He then followed up in July by delaying action on the agency's own regulatory proposal, insisting the issue of regulating greenhouse gases was better left to Congress. Many experts agree that Congress is the only real forum for a deal between environmental and industrial interests, but environmentalists and some politicians say that the EPA should take action while that debate is under way. Indeed, some see EPA regulations as a tool to force action in Congress, because industry interests would rather have an open debate about legislation. Moving forward, the question is how either presidential candidate would work with Congress, the legislative body, and the EPA, the regulatory body, to set up a greenhouse-gas programme. Obama's advisers are quick to suggest their candidate would use both legislative and regulatory approaches to help control emissions. \"If the circumstances are not right to get that legislation passed quickly, then I think the regulatory authority would come to the fore,\" says Robert Sussman, a Democratic adviser who served as deputy EPA administrator under Bill Clinton. McCain's campaign has been quiet on the issue, but many experts say the Republican would be less likely to take the regulatory route. Regardless, the next EPA administrator may find his or her hand forced by lawsuits. Just last month, New York and 11 other states sued the EPA for failing to regulate CO 2  emissions from oil refineries. But the alleged problems run much deeper than any given policy position. Reports of political interference in agency decisions are widespread, and many say they have contributed to morale problems throughout the agency. Union leaders representing EPA scientists blasted Johnson earlier this year, criticizing the agency's proposal to close several libraries as well as regulatory decisions regarding pesticides, drinking-water standards and mercury emissions from power plants. Jeff Ruch, executive director of Public Employees for Environmental Responsibility in Washington DC, says it's not yet clear what kind of management style either Obama or McCain would bring to the agency. \"The question is the extent to which a president will pledge to let the scientists talk,\" he says. \"I have not yet heard that pledge by either candidate.\" For more on the US election, see  www.nature.com/uselection \n                     US election special \n                   \n                     NASA \n                   \n                     National Institutes of Health \n                   \n                     Environmental Protection Agency \n                   Reprints and Permissions"},
{"file_id": "455586a", "url": "https://www.nature.com/articles/455586a", "year": 2008, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "Cell therapies are as much about the patients as they are about the cells. Monya Baker meets two stem-cell scientists who have decided to put people first. The first time Christine Mummery encountered a human heart, she didn't like what she saw. The vessels were brittle with atherosclerosis and the tissue was mottled like a bruise. But the biggest shock was that this diseased heart looked so different from that of a healthy mouse. How could she expect the stem-cell therapy she'd been testing on rodents to work in this alien-looking organ? This is the type of thinking that led Mummery, a stem-cell scientist, to move to a hospital in May this year. In the six years since she saw the heart, she had become convinced that closer ties to clinicians and better access to human samples would make her research more applicable to patients. So she packed up her lab at the Hubrecht Institute in Utrecht, the Netherlands, and relocated to Leiden University Medical Center. \"I thought we'd be better off in a clinical environment,\" she says. \"It's so much easier if you can speak to clinicians over lunch.\" Viviane Tabar also talks medicine at the table. A practicing neurosurgeon at Memorial Sloan-Kettering Cancer Center in New York, she spends two to three days a week caring for patients with brain tumours. But she thinks that surgery and the other tools that she has to help them aren't enough, so she is investigating the cells that might make brain therapies of the future. \"The patient's perspective is often different from what a scientist might think,\" Tabar says. \"They want to know what a technology can do for them on a very practical basis. So you learn to think much more pragmatically and to ask: would this really be helpful?\" Both Mummery and Tabar have veered from conventional career paths in hopes of making stem cells more useful therapeutically. They work in different countries and on different diseases. Mummery is well-established and Tabar is still early in her career. Yet both believe that the best way to ensure that stem-cell therapies are 'translated' into patients is to be as close as possible to the patients themselves. \"Some scientists could really benefit from just one or two days in the clinic,\" says Tabar. Patients haven't always been the strongest focus for stem-cell researchers. Getting enough of the right cells to transplant was seen as the biggest stumbling block. That is changing with the derivation of human embryonic stem-cell lines, along with new techniques to make induced pluripotent stem (iPS) cells from patient skin biopsies. Researchers can now generate potentially limitless supplies of cells and use improved methods to grow them into the specialized types they want. The possibilities of human trials are edging nearer, and another problem is moving into focus: healing potential depends not just on the cells going into a patient, but on what they'll encounter inside. Transplanted cells will be going into diseased bodies, says Marie Csete, scientific director of the California Institute for Regenerative Medicine (CIRM) in San Francisco, and it is the body as much as the cells that needs to be studied if researchers hope to \"predict cell behaviour when we put them into a distinctly pathological environment\". But few people are well versed with both stem cells and patients. \"We've had a crisis in embodying someone who really understands clinical medicine and who really understands basic science in one person,\" says Csete. There is no question that the field needs more people like Mummery and Tabar to chart a course between an interesting concept and ready-to-test therapy, says Fred Gage, who studies neural stem cells at the Salk Institute for Biological Studies in San Diego, California. Mummery conveys a sense of purpose. She is tall, sometimes wears her short hair spiked up, and her speech and movements are brisk. She trained as a physicist at the University of Nottingham, UK, and when Mummery later moved into biology she thought that its practitioners should \"measure something rather than just look\". Mummery's quantitative approach has shown that, in some cases, stem-cell therapy may not live up to its initial promise even in animals. As a postdoc at the Hubrecht Institute she developed culturing techniques that induce embryonic stem cells to generate plentiful cardiomyocytes, the muscle cells that power the heart. But her later work has highlighted how difficult it could be to use these cells therapeutically. A common way to track cells transplanted into a mouse, for example, is to engineer them to express a green fluorescent protein and then look for the tell-tale signal. But Mummery discovered that scar tissue within the heart also emits green fluorescence under certain conditions, and posited that at least some transplantation studies were mistaking scar tissue for successful engraftment 1 . She has also shown that cell-therapy results can be frustratingly short-lived. Other researchers showed that one month after derived cardiomyocytes are transfused into injured mouse hearts, the hearts pump significantly more blood 2 . Mummery showed that the effects disappear after three months 3 . \"What her work has uncovered is that we need to pay attention to graft survival, appropriate alignment, and integration,\" says Ken Chien, a cardiologist and stem-cell biologist at Massachusetts General Hospital, Boston, who has collaborated with Mummery. Around the time she glimpsed her first human heart in 2002, Mummery began to question the relevance of her work to humans. She realized that the way her lab was simulating heart attacks in healthy mice was a poor mimic for the real thing. As soon as a heart attack is diagnosed, heart surgeons work quickly to place stents in blocked vessels, opening them up again to allow oxygenated blood to perfuse the tissue. In mice, Mummery's group tied off blood vessels permanently (partly to comply with animal husbandry laws), making for simpler, cleaner wounds. She recalls her discouragement: \"You're already a year and hundreds of mice down the line, and the cardiologist would say 'that's not really it'.\" Mummery was also becoming aware that clinicians are less concerned about the heart attack itself, and more about the ensuing heart failure. \"I felt we needed to know more about the most relevant clinical problems,\" she says. She received recruitment offers from various institutions, including Harvard University. But in August 2007, she was offered the position at the hospital in Leiden \u2014 and she took it.  \n                Converging paths \n             Whereas Mummery has gone from basic research to medicine, Tabar has been diverted from medicine to the lab. In 1995, as a third-year neurosurgery resident at Memorial Sloan-Kettering, she found herself intrigued by a talk given by Ron McKay from the National Institute of Neurological Disorders and Stroke in Bethesda, Maryland, describing evidence for neural stem cells. \"I had been taught that the brain is a post-mitotic organ; that everything dies and nothing regenerates,\" she says. \"And his work was saying 'maybe not'.\" Her clinical programme required a year of research, and she went to McKay's lab to study how to coax embryonic stem cells into forming dopaminergic neurons, the kind that die in Parkinson's disease. Tabar finished her medical training and in 2002 she joined the medical faculty at Memorial Sloan-Kettering, where she specializes in brain cancers. But while seeing patients, she also took a postdoctoral fellowship in the laboratory of her husband, stem-cell biologist Lorenz Studer. The two had met collaborating on experiments in McKay's lab, and when Tabar went to complete her training, Studer established a lab at Sloan-Kettering studying how embryonic stem cells can create different sorts of neural cells in culture. Tabar wanted to develop animal models to test whether neurons differentiated from embryonic stem cells could restore function, and she wanted her own lab to do so. She established this in 2005. Researchers sometimes worry that clinicians will rush to act on preliminary results: \"Some clinicians want to try an idea tomorrow, rather than the day after,\" Mummery says. But Tabar says the opposite can be true: being a physician sometimes makes even the best research publications seem less exciting because they will rarely make a concrete difference in a patient's life. McKay recalls visiting Tabar's lab in New York, and her questioning the treatment implications of a recent paper on glioblastoma. \"Within five minutes, we were in another room, looking at patient records, talking about specific cases.\" It is precisely this \"ability to dance on both sides of the aisle\" that makes people like Tabar able to ferret out the valuable experimental approaches, he says. Some of Tabar and Studer's most recent work has addressed a pressing question: how closely cell transplants into the brain must be immunologically matched to the recipient. It has not been clear how much tissue rejection and inflammation could present a problem, because brain tissue is better protected from immune attack than that of other organs. In a paper published this year, Tabar and Studer tackled this question in a mouse model of Parkinson's disease 4 . Drawing on the expertise of colleagues at the RIKEN Center for Developmental Biology in Kobe, Japan, they used therapeutic cloning to create embryonic stem-cell lines from 24 mice, differentiated these into dopamine-producing neurons, and transplanted them back into the animals. Mice that received cell transplants derived from their own cells improved; mice that received cells derived from other mice did not. The foreign transplants seemed to trigger an immune response allowing only a few cells to survive, suggesting that human cell transplants would need to be closely matched immunologically and perhaps even derived from the patients themselves. The results highlighted the difficulties with such therapies, but it was the paper's success that made headlines: it was the first time that cells made by therapeutic cloning were used to treat exactly the same animal from which they were derived. Tabar says it is difficult to gain recognition for the \"unglamorous\" work necessary to move from proof-of-principle research to a clinical application. \"You could perhaps come up with a paradigm that works beautifully in animals in the lab and that puts together all these biological concepts,\" she says. \"You can get it published, but bringing it to the patient will require a lot of mining through the details.\"  \n                The hard way \n              Academia does not tend to reward this type of detailed investigation; recognition is based on experimental 'firsts' and high-profile publications. If a scientist-clinician is not generating a stream of prestigious papers, then pressure increases to see patients, the activity that, after all, generates revenue for a physician's institution. Does Tabar ever consider how much simpler her life would be if she switched to all clinical or all scientific work? \"Every day,\" she responds, laughing. But, she adds, she cannot imagine giving up either pursuit. Tabar is now studying how cell replacement might help patients whose brains have been damaged by radiation therapy, as they would be after treatment for a brain tumour. Her lab administers various radiation regimes to rats, and then supplies cells \u2014 derived from embryonic stem cells \u2014 at various stages of differentiation and at various time points, trying to find the best combinations. She insists on measuring any benefits of these treatments using behavioural tests, rather than tissue integrity alone. \"It is the clinical problem that I want to address rather than the simple histological or radiographic repair of the brain.\" Stem-cell researchers are not alone in finding clinical research challenging: scientists in almost every biomedical discipline are struggling to translate basic results into ones that can benefit patients. Stem-cell researchers are under particular pressure though. They must justify the massive investment made in them by funding agencies, philanthropies, and patients, such as the US$3 billion of California taxpayer's money distributed by the CIRM. Getting cell therapies to patients is also daunting because there is no established path to clinical approval. No treatments based on pluripotent stem cells have been approved for testing in humans, and earlier this year the US Food and Drug Administration halted plans by Californian biotech company Geron for the first trial in human patients of cells derived from embryonic stem cells. The risks for such therapies are almost impossible to assess, but the worry is that even a differentiated cell product could be unpredictable when administered to a patient, and might proliferate or transform into unwanted tissues. Csete says that the best approach to translational research is funding collaborations or facilitating other practices that bring disease experts, clinicians and cell researchers together. The CIRM plans to announce several large, multi-year grants for such 'disease teams' later this year and has already funded just under two dozen 'planning grants' at around $50,000 apiece to help collaborators at different institutions hammer out proposals. The collaborative strategy seems to be working for Mummery, whose lab building is adjacent to the hospital at Leiden. There, she has access to fetal and adult human tissue and this has enabled her to try a new line of research: comparing human and mouse cardiac cells. She had good collaborations in hospitals before, she says, but people seem more willing to help colleagues within a hospital than at an unrelated institution. Mummery also benefits from the clinicians' established procedures for gathering informed consent for tissue collection. Mummery is using the human samples to work out which molecular markers are expressed when and where in developing hearts, and she has already found that rodents can sometimes lead scientists astray. In the mouse embryo, cardiac cells express distinct markers depending on whether they will form the atrial chambers that receive blood or the ventricular chambers that pump it out. But when Mummery's team probed human fetal tissue, they found that human ventricular cells \u2014 the type that weaken in heart failure and that cell therapy might replace \u2014 actually express the markers used to identify mouse atrial cells. \"That made me realize that we are trying to make heart cells from human embryonic stem cells without really knowing what the cells are.\" Even as she has moved closer to the clinicians who could do cell transplants, Mummery now feels that the timelines for those transplants are lengthening. She no longer thinks that pursuing cell transplants is the most productive use of her time. Instead, she is using them to screen for drugs that can change how the cells beat  in vitro , in order to understand the cause and control of cardiac arrhythmias. The patients are just next door, but Mummery is a bit less intent on putting cells into them. For now, she's decided, there is plenty that the cells can teach her in a dish. Monya Baker is the editor of Nature Reports Stem Cells. \n                     Translational research news special \n                   \n                     Nature Reports Stem Cells \n                   \n                     California Institute for Regenerative Medicine (CIRM) \n                   Reprints and Permissions"},
{"file_id": "455583a", "url": "https://www.nature.com/articles/455583a", "year": 2008, "authors": [{"name": "Sharon Weinberger"}], "parsed_as_year": "2006_or_before", "body": "What began several years ago as an attempt to recruit social scientists to help the military has sparked a broader debate about militarizing academia. Sharon Weinberger reports. It is a story that repeats with grim monotony: US forces in Iraq detain a suspected insurgent after they find in his home what they think is jihadist literature and an illegal weapon. These detentions \u2014 often based on mistaken assumptions or poor intelligence \u2014 can easily escalate into major conflicts with the local community. But in one recent case, researchers helped defuse a potential conflict. Analysts working for a 'human-terrain team' informed a US commander that the 'jihadist' literature discovered in the village of Banat al Hassan, about 30 kilometres northwest of Baghdad, was ordinary religious teaching material, and the weapon \u2014 a riflescope \u2014 was for a pellet gun that beekeepers in the area use for shooting birds. The suspect was promptly released, and his family ended up helping US forces by revealing the location of a large improvised explosive device. This upbeat anecdote is \"a story about how a little respect, culture and compassion can save human life\", says Montgomery McFate, an anthropologist at the Institute for Defense Analyses in Alexandria, Virginia, and senior adviser to the Pentagon's human-terrain programme. But it also underscores some of the complexities and controversies surrounding the Pentagon's quest for 'cultural knowledge'. What if, for example, the literature had indeed been jihadist literature? Would the human-terrain teams, which include civilian social scientists, then be helping the military to target insurgents? Last year, the Pentagon provided almost $60 million for the Human Terrain System, a Department of Defense programme that represents the latest incarnation of the military's long, troubled relationship with social science (see  'Lessons from the past' , overleaf). It includes deployed teams that directly advise military commanders in the field, specialized software for cultural 'mapping' plus personnel based in the United States conducting research. According to official figures provided by the army, there are now sixteen five-person Human Terrain Teams (HTTs) deployed in Iraq and five in Afghanistan, along with about 40 people in 'research reachback cells' in the United States. The teams are supposed to provide deployed military forces with \"direct social-science support in the form of ethnographic and social research, cultural information research, and social data analysis\". But the effort is not without its problems; two social scientists have been killed in the field, one in Afghanistan and one in Iraq. And critics fear that this sort of work poses ethical problems, particularly if it's telling the military who is, or isn't, a potential enemy. Last November, the executive board of the American Anthropological Association (AAA) condemned the effort, saying it \"creates conditions which are likely to place anthropologists in positions in which their work will be in violation of the AAA code of ethics\", as well as endanger other anthropologists by bringing suspicion on their activities. The association is also proposing changes to its rules of ethics that would tighten restrictions on secret research. Beyond the AAA, a number of researchers in 2007 founded the Network of Concerned Anthropologists, which asks colleagues to sign a pledge committing them to \"refrain from directly assisting the US military in combat, be it through torture, interrogation, or tactical advice\". Though there hasn't been any known case of that happening with the HTTs, historical precedents exist. During the Second World War, for instance, anthropologists helped raise guerilla armies, passed information used to plan bombing raids and theorized about race-specific bioweapons. Critics say the current work flies in the face of everything anthropology represents, from transparency of research to informed consent (for example, the social scientists on the HTTs do not submit their research to an institutional review board, as would be normally required for human research). \"I don't think there's a place for embedded anthropologists with combat missions,\" says Roberto Gonzalez, an anthropologist at the University of California, Berkeley, who is working on a book about the Human Terrain System. \"It runs completely counter to anthropology's ethical framework, something that's come about over a long, bitter period that goes back to the First World War.\"  \n                Militarizing anthropology? \n             McFate has emerged as the most public face of the Pentagon's military anthropology work. She got her PhD in anthropology from Yale University, focusing on the British counterinsurgency in Northern Ireland, and by 2005 she had co-authored an article in a military journal outlining a plan for deploying social science advisers with troops ( M. McFate and A. Jackson  Military Rev . July/Aug, 18\u201321; 2005 ). For her, the issue is unabashedly about moving anthropology toward an applied discipline that can aid the military. \"Why should anthropology be some leftist religion?\" she asks. \"I mean, it's supposed to be a science; it's not supposed to be a political platform, a substitute for the Peace Corps, or a cult.\" The Pentagon, however, has had a hard time recruiting and keeping qualified anthropologists. Of 35 social scientists based in Iraq and Afghanistan, only about half have PhDs, and only seven of those deployed are anthropologists. One social scientist hired to work on a HTT was identified during screening as a convicted criminal (and dismissed prior to deployment), another was found medically unfit, two were let go because of security clearance issues, and two were fired for performance issues. The company responsible for hiring the researchers is BAE Systems, a major Pentagon contractor, and some have criticized its focus on recruiting through intelligence and military-focused websites, as opposed to academic venues. One of those fired was Zenia Helbig, a PhD candidate in religious studies, who says she was let go by BAE after a joking comment she made over drinks with colleagues about switching sides if the United States attacked Iran. Helbig, who travelled to Iran as a graduate student, had even met Iranian President Mahmoud Ahmadinejad. Now back at the University of Virginia, in Charlottesville, to complete her degree, Helbig describes a programme in disarray, in which social scientists \u2014 few of whom have regional or linguistic expertise \u2014 sat around for weeks at Fort Leavenworth in Kansas, with little in the way of region-specific training. Matt Tompkins, Helbig's fianc\u00e9 and another human-terrain participant, describes other problems. As a PhD student in political science with a military background, he was assigned as a team leader in Baghdad; but the social scientist on his team had no relevant field-research experience, he says, and their de facto translator was a Moroccan who barely spoke English. As for the military commander they were supposed to be supporting, Tomkins says, \"I didn't get the inclination that he was particularly interested in what we were doing.\" McFate disputes the recruitment problems, although she says some academics have told her they fear being blackballed professionally if they work for the programme. Other supporters note that experiences of different teams have varied widely. Adam Silverman, a political scientist who works on a HTT outside of Baghdad, says he believes such work is valuable. \"The programme is new, so it isn't perfect,\" he says. \"It has growing pains.\" Working from what he describes as a mix of \"unstructured interviews, casual discussions with members of the population, academic sources and the Internet\", Silverman has provided advice on everything from local funeral rites to agriculture. Although he is now working on oral histories, he acknowledges that his field research has been difficult to conduct. \"We don't interview anyone per se \u2014 we do try to talk with anyone who will talk with us,\" he says. \"I've had conversations with fish farmers, brickmakers, government officials and tribal leaders.\" However, it is not clear whether academic social scientists are even the key feature in successful human-terrain teams. McFate's story about a team defusing the situation in Banat al Hassan was confirmed by Major Philip Carlson, who led the team in question. But the recommendation to let the man go wasn't from a social scientist; it came from Carlson and an Iraqi\u2013American analyst. There wasn't even a social scientist on that team at the time. McFate says that \"smart, competent, well-trained people on a team\" can be successful, as in this case, but that social scientists are needed to achieve the programme's broader goals. But few, if any, definitive numbers exist by which to measure the programme's effectiveness. Earlier this year, Colonel Martin Schweitzer, a military officer working in Afghanistan, testified before Congress that HTTs helped to reduce the number of operations involving military force in his region by 60\u201370%. Sceptical of those numbers, David Price, an anthropologist at Saint Martin's University in Lacey, Washington, filed a Freedom of Information Act (FOIA) request to look at the report. Price says that what he got back was merely a correspondence stating the numbers; there was no actual report. \"When I got my FOIA reply I learned that there was no study out there substantiating any of this,\" he says. Even with the doubts surrounding the Human Terrain System, the Pentagon made another foray into the social sciences this April when Defense Secretary Robert Gates announced a broader military initiative. Called Project Minerva, it would fund work at universities that do research ranging from looking at Chinese military technology to Islamic radicalism. Anthropologists critical of the Human Terrain System didn't welcome Minerva either. In a 28 May letter to the White House's Office of Management and Budget, the president of the AAA outlined a number of concerns, including the notion that having the Pentagon run such research creates a \"potential conflict of interest\". Partly in response, the Pentagon forged a relationship with the National Science Foundation (NSF), which culminated earlier this year in the signing of a formal agreement. That, however, created new confusion, as many presumed that the foundation was cooperating on Minerva. Mark Weiss, director of the NSF's behavioural and cognitive sciences division, insists that is not the case. \"It is a Memorandum of Understanding that would allow for a number of different interactions that . . . would help enhance the flow of information from the social and behavioural sciences to the Department of Defense,\" he says.  \n                Shopping for knowledge \n             One question concerns who would oversee the peer-review process for selecting grantees: the defence department or the NSF. Thomas Mahnken, the deputy assistant secretary of defense for policy planning, says that Minerva is budgeted for approximately $100 million over five years, and that half that money would go through the NSF. The other half would go through the Pentagon, which he insists also has a well-tested peer-review process. \"The two paths are complementary,\" says Mahnken. \"NSF certainly gives us access to a different pool of scholars.\" Critics of the programme, particularly anthropologists, point to a number of pitfalls associated with Minerva. One social scientist who works with the military warns of 'ScamTechs' \u2014 firms that are adept at getting defence department funding, regardless of the subject. And Hugh Gusterson, an anthropologist at George Mason University in Fairfax, Virginia, notes that a government contractor recently contacted several colleagues, \"shopping\" for an anthropologist so that they could bid on Minerva, which requires university participation. Another concern is that the Pentagon's largesse could ultimately shift anthropologists away from their traditional role as advocates for the people and cultures they study. \"Anthropologists ought to be involved [in the national security debate], but my fear is what makes anthropology appealing will be undercut and deformed if anthropologists are directly answerable to the Pentagon in that conversation,\" Gusterson says. \"Anthropology will thrive more as a discipline if the funding is not directly from the national security state.\" Both the Pentagon and the NSF downplay any concerns that the defence department could flood the field with military funding. Weiss notes that NSF's total annual budget for the behavioural and cognitive division is already about $220 million. The Pentagon money, he says, is \"not going to put us into a stratospheric level of funding\". Meanwhile, researchers in other countries are grappling with some of the same issues. Two years ago, Britain's Economic and Social Research Council was criticized for circumventing normal open academic competition by funding counterterrorism studies. Jeremy Keenan, a UK-based anthropologist and North Africa expert, says that the UK Foreign Office gave itself a respectable academic veneer by rerouting money quietly through the council. By contrast, \"if one looks at the US military programme, it's been very overt,\" he says. Other militaries have not yet developed an exact equivalent to the Human Terrain System, but they do have, on a smaller scale, social scientists providing advice to armed forces \u2014 they work in psychological operations units and provide training and education. And McFate says that some NATO allies have also expressed interest in setting up human-terrain-like programmes. Whether other countries will be engulfed in the same controversy remains to be seen. McFate, for her part, puts the criticism down to a small but vocal group. \"It's just a very small segment of the anthropology community,\" she says of the critics. \"We're not going to draft them.\" Sharon Weinberger is a freelance writer. \n                     Human Terrain System \n                   \n                     Network of Concerned Anthropologists \n                   \n                     American Anthropological Association \n                   \n                     NSF call for solicitations for Project Minerva \n                   \n                     Hugh Gusterson article on Minerva \n                   Reprints and Permissions"},
{"file_id": "455726a", "url": "https://www.nature.com/articles/455726a", "year": 2008, "authors": [{"name": "Erik Vance"}], "parsed_as_year": "2006_or_before", "body": "Jacek Koziel is a master of odours. On a pig farm in Iowa, he shows Erik Vance some of the peaks and troughs of life as a human detector. Hooking yourself up to Jacek Koziel's olfactometer is a little like having a photo album of your childhood shoved in your face. It can be offensive, sometimes painful, yet distinctly nostalgic. Seated in Koziel's Iowa State University laboratory in Ames \u2014 with my eyes closed, and my ears covered by soundproof headphones to isolate the sense of smell \u2014 a glass cup over my nose delivers an olfactory slide show. The scents come at regular intervals. Some are strong, some less so. My job is to categorize them on a screen in front of me. For the untrained, identifying the odours is surprisingly hard. Most of the time I get a memory rather than a name. One has a hint of lemonade and summer afternoons. Another smells like summer camp. A third reminds me of a high-school girlfriend's lip gloss. For each one, I click a tab from the computer list in front of me with labels such as 'floral', 'cardboard' and 'rancid'. Then one hits me like an angry bull. I recoil from the machine, regain my composure, and click 'fecal'. My reaction is probably not surprising because the sample came from the gas given off by swine manure. Koziel is an analytical chemist in the Department of Agricultural & Biosystems Engineering, where he specializes in finding and identifying trace volatile organic compounds responsible for odour. His lab teases apart all kinds of odours, including maize (corn), wine, and the bitter fluids that insects use to defend themselves. But his \"bread and butter\" is livestock leavings, an incredibly complex chemical matrix. Koziel's lab is a leading authority, having catalogued almost 300 ingredients in swine manure \u2014 some of which exist only at concentrations of a few parts per million. His work is part of an international effort to understand and remove these compounds from daily life. It is a dirty job, but one that may have growing economic importance as residential areas increasingly encroach on farmland, and demand for food grows. Koziel lives and works in a world of odour. A sign on the wall of the lab urges \"good personal hygiene\" while warning against wearing perfumes or scented deodorants. At the base of every whiteboard Koziel keeps scented markers to help researchers 'build awareness' of odour. And if a fume hood is accidentally left up, one quickly wonders who has broken wind. Most people in the small, dedicated community of agricultural-odour research casually refer to odour as air pollution. But technically speaking, the United States regulates only two of the smellier ingredients of manure \u2014 ammonia and hydrogen sulphide \u2014 for their other negative effects on soils and water sources. They are also the easiest smells to single out. \"There is a tendency to go after gases that are easy to measure and historically that has been ammonia or hydrogen sulphide,\" says Koziel. Yet government regulations are not nearly as likely to shut down a factory farm as neighbours' complaints, which spring from a wide suite of smelly chemicals. That is because odour is the only 'air pollutant' that almost every person living nearby has state-of-the-art detection technology for: the human nose. It is this technology that allows Koziel to catalogue manure's most offensive components. He uses a gas chromatography mass spectrometry olfactometer (GC-MS-O). Gas chromatography separates gas samples taken from the air just above manure into different groups, based on molecular weight. Then it runs each group into two simultaneous ports. One goes to a mass spectrometer that prints out a detailed chemical analysis of the compounds. The other comes wafting out of a 'sniff port' into a researcher's waiting nose. This blending of quantitative analysis with qualitative description allows Koziel to create detailed chemical maps and find the smallest traces of compounds. Take 2-isopropyl-3-methoxypyrazine, a compound Koziel discovered in ladybirds that accumulated on the window sill of his old office. A mass spectrometer barely registers the compound in the cocktail of chemicals found in ladybirds. But it is unbelievably bitter stuff, which the human nose can detect in the parts per trillion. Found in wine when ladybirds get crushed with the grapes, it adds a flavour often associated with a bell pepper ( Capsicum) . Using human noses, Koziel pinpointed the offensive region of the chemical fingerprint and was able to isolate the noxious ingredient by essentially zooming in on that region (a technique called multi-dimensional GC-MS-O).  \n                The collector \n              Koziel shows me a sample of the 2-isopropyl-3-methoxypyrazine along with a few dozen other dark bottles in a small cabinet covered with colourful warning stickers. It is housed in a sealed container, within another sealed container, within another sealed container, and is part of a frightening collection. They are the foulest odours that Koziel has come across in his work \u2014 and suffice to say, it is not a place one would want to see anything spill \u2014 just opening the wrong bottle could make the lab stink for days. In the complex matrix of animal manure, strong but scarce compounds such as these are almost impossible to spot, yet minimizing them may cut down on odour. Koziel's use of GC-MS-O has allowed him to create a very comprehensive view of manure that includes pungent, bitter compounds, as well as one that smells like buttered popcorn and another described as 'taco shell'. Koziel calls himself a \"smelling weirdo\". Nearly anywhere he goes, he can recognize at least a few chemicals in the air. He says thatdistinctive flavours of body odour, red wine and mushrooms can all be found in manure. \"You become more aware of the smells around you and you say 'wow, this smell smells like this chemical'. So you start making associations,\" he says. \"Early on it was exciting, and that's when I made a lot of faux pas at home.\" He once casually compared a meal his wife had made to manure. And there are the times when a family member breaks wind and he ventures to guess what they ate. Ironically, his wife says that one of the things that first attracted her to Koziel was that he smelled like the outdoors. Koziel came to the United States to climb Mount McKinley and stayed to help his climbing partner raise money for hospital bills after he lost both feet to frostbite in the attempt. It is odd to think of a mountain climber with ascents in the Andes, Indonesia and the Himalayas doing research on an Iowa pig farm, but Iowa \"is a good place for odour\", he says. \"I have different mountains to climb, in a way.\"  \n                Mountains of manure \n              The next day, we visit one of these mountains in the cornfields of central Iowa \u2014 a pig-birthing facility where Koziel monitors ammonia, hydrogen sulphide and dust. It is part of a nationwide detection study to quantify livestock-facility emissions. The head of that study \u2014 the National Air Emission Monitoring Study \u2014 is Al Heber. He says you cannot separate odour from air pollution. And although stink isn't a federal air pollutant, it is often the most crucial element in air-pollution debates. \"It affects proposed facilities. If that producer wants to put another farm a mile away down the road on his other property, then the odour is going to be the issue as to why he might find it difficult to locate his farm there,\" says Heber. Tracking ammonia and hydrogen sulphide is a good first step, but even measuring two abundant compounds is more difficult than it sounds. For one thing, where do you sample? Next to the farm, or next to the houses two miles away? And because odours are subject to wind and temperature, when are the most representative times to sample? And assuming you find the best time and place to sample, what is the best way to do it? On the drive to the pig farm, not only does the smell become stronger, but it also becomes sweeter \u2014 almost like a Doppler shift of colour. That, Koziel explains, is because different odours diffuse at different rates. Because technology cannot detect odour's reach, the most common way to measure it is dilution olfactometry \u2014 essentially, letting people smell a bit of air and asking them if it stinks. Koziel's colleague at Iowa State University and regular co-author, Steve Hoff, is an expert in this field. He collects samples of air in specialized bags, dilutes it with odourless gas until a panel of paid sniffers cannot detect the scent. This is the 'odour threshold'. Techniques such as this are the primary tools for quantifying odour. The problem is that often his data are essentially subjective, because one man's stink is another man's farmyard aroma. At a modern industrial factory farm, however, there are no nice ways to describe the stench. Inside the large, squat building that is Koziel's research site, 3,000 sows are either nursing piglets, having piglets or waiting to have piglets. Under the slats at my feet is one of two pits, each the size of a football field, five metres deep with manure. The sweet scent I sensed outside is now overpowered by a cacophony of others that at times make my eyes water. Koziel on the other hand, is perfectly at ease, and sets about showing off a few of his hundred instruments in the building. \"This is the toughest air sampling that you can imagine. Climbing stacks or sampling smelters is okay, but this is a different ball game. You have so many variables that you have to think about,\" Koziel says with a rare touch of visible pride. \"There is no one chimney where you just stick in your probe.\" Then there's the dust. Animal dander and other particulates pump out of a pig farm at a couple of grams per cubic metre (double that for poultry). Over time this builds up, covering Koziel's detectors and every flat space in the feedlot with a millimetre or two of dust. The dust contributes to the odour's spread by carrying stowaway chemicals on the wind, but no one knows for sure to what extent.  \n                Sticky mess \n              Two showers later, I get a sense of what the dust experiences. Many of the chemicals breezing about an industrial hog farm are what Koziel calls 'sticky'. That means that they are heavy molecules with a low vapour pressure that are often electrically charged. When they encounter skin, cotton or anything else with pores and crevices, they stay there. The worst will even stick to plastic or glass, making them nearly impossible to collect and measure. These are not the easy-to-spot sulphides, but volatile fatty acids, indoles and phenols that are not a part of Heber's study. Heber says he hopes to approximate the concentration of these chemicals by measuring things such as dust and ammonia. However, in other parts of the world it's not clear that ammonia and odour are necessarily linked. \"It has an odorous component, but I don't think ammonia is really important for odour,\" says Nico Ogink, a farm odour scientist at the Wageningen University and Research Centre in the Netherlands where Koziel will be taking a sabbatical. \"It is only important to me because of its effects on natural areas.\" Ogink says that although ammonia is much more damaging to the environment, rarely does it have the ability to reach noses far from its source. This is a crucial point in the Netherlands, which has almost as many pigs as people, and where more than half of the substantial nitrogen pollution comes from livestock. In one study, 11% of the Dutch population reported being annoyed by livestock odour \u2014 more than by industrial activities and traffic. It is difficult to say how big an effect stink has on the wallets of farmers, and industry representatives are loath to dwell on it, but several European countries such as the Netherlands, Denmark and Germany are investing heavily in odour mitigation through a series of odour-specific regulations. In Europe, the answer is often some kind of technological filtration system. These may be bio-scrubbers, bio-filters or acid scrubbers, similar to the ones that many countries use to cut sulphur dioxide from coal-plant emissions. The scrubbers work by exposing the air to water, which will absorb dust and suspended pollutants. European scientists say modern filters can cut odour emissions by 30\u201390%, depending on the conditions and whether they use several techniques together. But acid scrubbers add about \u20ac5 (US$7) to the cost of each pig. In the United States, where odour regulations are administered locally, mitigation may be a slurry pond out back (a practice that has led to problems in flood-prone areas) or a storage silo. Koziel's lab is working on its own mitigation techniques, most notably one that attempts to use ultraviolet light to break smells into their component parts. The process is similar to the photochemical reactions that change car exhaust into smog, only instead of creating ozone, it creates smaller, less smelly chemicals. He has also researched changes in livestock feed to try and cut the stink. Similar studies are widespread on both sides of the Atlantic Ocean. But Koziel is hesitant to suggest that US farmers adopt expensive mitigation approaches. \"We would be laughed at,\" he says. \"These people work on very small margins of profit. They make money because they have a lot of animals.\" When I suggest it to long-time pig farmer Chad Pierce at the Iowa pig farm, he doesn't laugh, but it is clear that's because he is trying to be polite. In good years, he can expect just $5\u201310 per pig in profit. Last year, with higher feed prices, the birthing facility actually ran at a deficit. He wouldn't be opposed to smell-reducing feed, he says, but it's not a very high priority. However, Pierce acknowledges that odour regulations may be on their way. And not just in the United States. One of the researchers in Koziel's lab, Lingshuang Cai, is originally from China. She says that as cities expand and citizens develop more of a taste for meats such as pork, the Chinese government is increasingly interested in odour mitigation. Although their commitment is generally ranked even lower than that of the United States, Cai says she has been offered numerous jobs in China working with manure. But for now she is content to be in Iowa with Koziel and his mountains of smell. Stop by sometime if you are in the area. But whatever you do, don't go nosing about in the little cabinet just past the smelly fume hood \u2014 the one with the colourful warning stickers. Erik Vance is a freelance science writer in Berkeley, California. \n                     On the ladybirds\u2019 stink  \n                   \n                     Jacek Koziel\u2019s website \n                   \n                     Pig industry websites \n                   \n                     Nico Ogink\u2019s website \n                   Reprints and Permissions"},
{"file_id": "455724a", "url": "https://www.nature.com/articles/455724a", "year": 2008, "authors": [{"name": "Amber Dance"}], "parsed_as_year": "2006_or_before", "body": "More creatures live in soil than any other environment on Earth. But what are they all doing there? Amber Dance reports on the world's widest biodiversity. Ecosystems aren't green; they are black and brown, at least in the colour palette favoured by Diana Wall. Wall, a soil ecologist at Colorado State University in Fort Collins, spends her days digging into the world's underground ecosystems. These beiges, ochres and charcoals reflect a three-dimensional mosaic of micro-environments, each with its unique set of inhabitants. But very little is known about these inhabitants. Understanding soil is a matter of rising urgency. A July report from the US National Research Council listed soil quality as the biggest barrier to higher crop yields for farmers in sub-Saharan Africa and south Asia. And knowing what myriad organisms live in the soil, and how they interact, is crucial to creating a healthy ecosystem. For those scientists who are willing to crouch down and dig, the diversity of soil denizens beats any above-ground system, even that of a tropical rainforest. A handful of soil from one spot may house a very different community from soil just a metre away, because of variations in the availability of water or nutrients. For example, the ground under a decaying plant or animal is a different environment from soil lacking such enrichment. And around plant roots, specialized organisms inhabit the rhizosphere, a thin layer where roots and soil organisms interact in myriad ways. Large animals such as moles contribute, changing and aerating the underground landscape by tunnelling. Even a small clump of soil has a gradient of oxygen from its edges to the centre, and each oxygen concentration may make the perfect habitat for different kinds of creatures. \"It is the most incredible zoo,\" says Wall. Take that view to a larger scale, and it is possible to appreciate just how complicated the world's soil ecosystems are. In one ongoing study, not yet published, Wall and her colleagues scooped soil cores from two sites in Alaska, one in the tundra and one in the taiga forest. Although the sites were only 400 kilometres apart, the species living there were radically different: only 18 invertebrate taxa out of an estimated 1,300 appeared in both locations. \"That just blew me away,\" says Wall. And that's just looking at invertebrates, not including microbes. \"As far as I know, there is no environment on Earth that is more biologically diverse, per unit area, than soil,\" says Eric Triplett, a microbiologist at the University of Florida in Gainesville. Thanks to faster, cheaper DNA sequencing, scientists are now getting a grip on what is down there and what those organisms might be doing. That information, in turn, could help improve soil management for agriculture and forest management for conservation. At this point, scientists don't even agree on how many creatures they are looking for. The first DNA-based estimate of soil microbial biodiversity, published in 1990, counted about 4,000 different bacterial genomes per gram of soil 1 . Since then, various studies and models have pushed the number up as high as 830,000 species per gram 2 , down to 2,000 (ref.  3 ), and back up again. Most recently, Triplett and his colleagues ran 139,000 individual sequences \u2014 more than other studies have used \u2014 and came up with an estimate of 10,000 to 50,000 species per gram of soil 4 . Complicating the matter is the fact that, because so few of these species have been described, researchers have to group similar organisms within 'operational taxonomic units', which correspond roughly but not precisely to species designations.  \n                Valuable species \n             Quantifying such diversity illustrates just how much remains to be discovered, and soil scientists are teaming up to tackle the challenge. The Tropical Soil Biology and Fertility (TSBF) Institute, run by the International Center for Tropical Agriculture and headquartered in Nairobi, has united more than 300 scientists in seven countries to survey soil organisms. The project, which began in 2002, aims to identify living indicators for fertile or poor soil, and has already identified some novel organisms that could be useful to humans. In the Veracruz rainforest, for instance, Mexican scientists have discovered  Acaulospora , a mycorrhizal fungus that entwines with lily roots and provides water and mineral nutrients. Last spring the researchers injected  Acaulospora  into the soil of test lily plots in Benigno Mendoza, a community in Veracruz where lily bulbs are an important cash crop. As a result, this year's harvest consists of big, first-quality bulbs that match the yields gained through using inorganic fertilizer with none of the downsides of chemical treatments. Isabelle Barois, a soil ecologist at the Institute of Ecology in Xalapa and coordinator of the TSBF Mexican team, says that the fungus could eventually help replace the expensive nitrogen fertilizer and harsh agrochemicals that farmers apply to their land five or six times a year. Global soils contain a bounty of unusual and potentially useful organisms such as  Acaulospora  \u2014 more, theoretically, than they should. Although some species are common, there are also countless taxa found in vanishingly small numbers. Many species also seem to be redundant, eating the same foods and fulfilling the same ecosystem jobs, so scientists don't quite understand why they're there at all. \"There is some debate about how many species need to be present in the soil to make an ecosystem,\" says Wim van der Putten, an ecologist at the Netherlands Institute of Ecology in Heteren. Heikki Set\u00e4l\u00e4, an ecologist at the University of Helsinki, took on this question with experiments in which he controlled the number of animal or microbial species in artificial ecosystems. In one study 5 , he set up soil microcosms in glass jars and added fungal species: only one in some jars, and up to 43 in others. Diverse systems decomposed more organic matter \u2014 demonstrated by higher carbon dioxide production \u2014 and produced more nitrogen compounds in the soil. But that relationship held true only at the lower end of the spectrum. Six species were better than one, but 43 weren't any better than six. \"It was kind of a bummer,\" Set\u00e4l\u00e4 says. \"It would be nice to tell the audience that we need all the species to make the planet green and sustain it.\" The explanation for the wealth of soil biodiversity, then, remains an open question. Maybe the multitudinous creatures are simply adapted for niches that humans don't yet understand. Alternatively, they could literally be waiting for a rainy day; some organisms spring into action after a storm, fire or other disturbance, and so make the ecosystem more resilient. Or perhaps those organisms are truly redundant. \"We know virtually nothing about what controls the diversity of soil communities,\" says soil ecologist Richard Bardgett of Lancaster University, UK. Triplett disagrees. \"I don't think it's a vast unknown,\" he says. \"I think there are some dominant genera out there that we could learn about pretty fast.\" In a follow-up to his soil biodiversity survey 6 , Triplett and his colleagues found that up to around 65% of the DNA samples from soil microbes fell into known genera, which makes those genera prime candidates for further study. For example,  Chitinophaga  was prevalent in the four distinct soils tested, from Canada, Illinois, Florida and Brazil. But a PubMed search for the genus finds only ten papers on the genus (and one of those is Triplett's), highlighting the lack of work that has been done in this area. \"My dream for the future would be that you would just take a DNA sample from the soil, and then explain what species are there, and what benefits,\" van der Putten says. But this kind of quick DNA test is years in the future.  \n                Setting microbes to work \n              For some scientists, just defining the diversity isn't enough. Triplett, for instance, wants to alter it. He envisions harnessing the nitrogen-fixing power of bacteria that form nodules on the roots of some plants, such as legumes, and convert nitrogen from the air into a form the plants can use. He thinks he could insert some of the nitrogen-fixing (nif) genes from the bacteria into agricultural crops \u2014 which could then collect their own nitrogen from the atmosphere and eliminate the use of artificial nitrogen fertilizer. It has already been shown that some nif genes can function in plants 7 . A nitrogen-fixing plant would require at least ten new genes, making the task difficult, Triplett says, but not impossible. Policy-makers are slowly starting to pay attention to the problem of soils. In 2006 the European Union agreed that soils need protection from erosion, landslides and salinisation, but has not yet finalized the laws that would ensure this happens. Some countries, including France, would prefer to see individual countries regulate soil. \"I'm pretty confident that the politicians will swallow the hook sooner or later,\" Set\u00e4l\u00e4 says. Avoiding that hook comes with a price tag: one estimate valued the free services provided by the world's soil biota at US$1.5 trillion or more each year 8 . Soils are also important as a carbon sink; soil stockpiles 1,500 gigatonnes of organic carbon, more than Earth's atmosphere and all the plants on the planet, according to the United Nations Food and Agriculture Organization. If soils remain degraded and their many denizens disappear, the world might lose access to organisms that improve crop yields, degrade toxins, or make useful by-products such as drugs \u2014 before they're even discovered. Amber Dance is a freelance science writer based in the Los Angeles area, and a former News intern with  Nature . \n                     FAO/AGL Soil Biodiversity Portal \n                   \n                     Natural Resource Ecology Laboratory at Colorado State \n                   \n                     Smithsonian Soils Exhibition \n                   \n                     Tropical Soil Biology and Fertility Institute \n                   \n                     International Union of Soil Sciences \n                   Reprints and Permissions"},
{"file_id": "455455a", "url": "https://www.nature.com/articles/455455a", "year": 2008, "authors": [{"name": "Kendall Powell"}], "parsed_as_year": "2006_or_before", "body": "What is it like to be labelled a genius? Kendall Powell follows the paths of four MacArthur Fellows \u2014 and finds they lead to rutting elephant bulls, climate-change champions, hybrid sunflowers and robotic hands. In what is perhaps the most secretive talent contest ever, the MacArthur Foundation recognizes and rewards people who demonstrate exceptional creativity. Just this week, the foundation awarded 25 disbelieving recipients in the arts, humanities and sciences with its illustrious MacArthur Fellowships or, as the media has labelled them, 'genius grants'. The fellowships don't have quite the intellectual and financial cachet of a 10-million Swedish krona (US$1.5-million) Nobel prize. But they do bring with them the label of 'genius', and half a million dollars to spend however the recipient decides. Now in its 28th year, the programme has awarded more than $360 million to 781 US citizens and residents aged from 18 to 82. The MacArthur Foundation, based in Chicago, Illinois, does not give the awards for specific accomplishments. Through a highly secretive nomination process \u2014 files are allowed to 'ripen' over several years \u2014 the programme identifies extremely talented and creative people who are likely to make breakthroughs and make a lasting contribution to society. \"We get the best information we can and consult with the wisest people we know,\" says Mark Fitzsimmons, associate director of the MacArthur Fellows programme and the programme's in-house scientist. \"We think it is a good investment to build up the portfolio of young people who can change their field significantly.\" It is also an investment with no guaranteed return: the foundation makes no attempt to measure the productivity of fellows or even what career path they follow. So do the geniuses go on to scale new intellectual heights, or do they struggle with the weight of expectation?  Nature  catches up with four individuals who got 'the call' 1, 5, 10 and 20 years ago and who have used their awards to push forward the frontiers of science.  \n                Class of 1988: Philip Kahl \n             When Philip Kahl won a MacArthur award in 1988, he was already known as a zoologist who followed his own path. He had spent 35 years studying bird behaviour and he had done it all as a freelancer. Even though colleagues pressured him to settle into a 'real job', Kahl had pieced together grants from the National Science Foundation, National Geographic and the National Audubon Society to travel to exotic field sites. He also funded his work by selling his wildlife photography, including a 1969  National Geographic  cover of an elephant in the Kenyan savannah. \"I was having a helluva lot more fun than if I had been teaching at a university,\" says Kahl, now 73, from his home in Sedona, Arizona. With the MacArthur money, Kahl's income \"quadrupled overnight\". Through investment and simple living, his fellowship (US$320,000 in 1988) has grown into an endowment that has largely sustained him and his work ever since. It also gave him the opportunity to switch his studies to the elephants he had been photographing. \"The birds were great and very photogenic \u2014 but you can't get attached to birds, they are too reptilian,\" he says. \"Elephants you can really grow to love and still stay objective.\" Kahl says that no one else would have given him a grant to study a topic that, on paper at least, he knew little about. He used some of the money to spend six seasons in the 1990s studying African elephants' visual communication displays in Zimbabwe's Hwange National Park. Since then, Kahl has been working at home analysing 225 hours of digital videotape frame by frame, and writing a book that catalogues the 100 or so visual displays elephants exhibit in the wild. He does not toil in obscurity. Elephant researchers around the world recognize his extreme dedication and deep knowledge of elephant displays. He collaborates by e-mail and phone and because he no longer travels, colleagues go to visit him and his library of 3,800 scholarly papers on elephants in about a dozen filing cabinets in his home office. \"I can only take a week at a time with Phil,\" says Bob Dale, who studies animal behaviour at Butler University and the Indianapolis Zoo in Indiana. \"Because it's breakfast, then work on elephants, lunch, work on elephants, dinner, more elephants.\" This focus has helped Kahl make observations about the animals that had been missed before. He discovered that 'rutting' African musth bulls can be identified by a wrinkle about two-thirds of the way down their trunks. He also documented how only older musth bulls mate during the peak rainy season; younger bulls must wait until the off season 1 . His observations of these and other displays are valuable for conservationists and zookeepers who want to better understand elephants in the wild and in captivity. (Kahl makes no secret that he identifies with these testosterone-driven males \u2014 his e-mail moniker is 'musthbull'). \"Phil has no time for the pretensions of the academic society,\" says Dale. \"But he admires an intellect wherever he finds it.\" \"Many people don't think it's possible to be a freelance researcher,\" says Kahl. He says that if he had ever got a real job, he'd most likely be retired now and \"playing golf or some other useless pursuit\". Instead, he works everyday with the goal of leaving a legacy of some knowledge about elephants that will benefit elephants. \"I'm quite happy I never got a job. A lot of people who get the fellowships don't do diddly, they spend the money on a house or something. I would like to think I did a lot more than diddly. I just hope I don't kick off before the book is done.\"  \n                Class of 1998: Benjamin Santer \n             Benjamin Santer listens to National Public Radio (NPR) on his morning drive to work. And every day, when the announcer says, \"Support for NPR is provided by the John D. and Catherine T. MacArthur Foundation,\" he lets out a little \"Woohoo!\" Ten years ago, Santer was at an utterly gloomy point, both professionally and personally. The call from the MacArthur Foundation, \"was like a miracle, it changed my life\", he says. Santer was in the middle of a bitter legal fight for custody of his four-year-old son and the fees were forcing him into debt. To remain near his son, he was seriously considering resigning from the Lawrence Livermore National Laboratory in California, where he did statistical analysis of climate models. He was also on the stand at work. He had spent the past two years defending a single sentence that he had penned in the 1995 Intergovernmental Panel on Climate Change (IPCC) Second Assessment Report. Those 12 historic words \u2014 \"The balance of evidence suggests a discernible human influence on global climate\" \u2014 became one of the report's major conclusions and unleashed a firestorm around Santer, including calls for his dismissal and editorials in national papers denouncing his science. Energy, transportation and industrial producers of greenhouse gases were keen to avoid blame and emissions curbs. Although 35 contributors agreed to the wording, Santer was singled out because he was the lead author of the relevant chapter and much of his own research had helped to build the case that some global warming must be attributed to human activities. One of his key studies showed that an anthropogenic 'fingerprint' on climate change predicted by computer models could be detected in weather-balloon data 2 , and was cited as \"the most convincing demonstration yet\" of a human contribution to changing global air temperatures 3 . \"Every day I was getting up and putting on this battle armour,\" recalls Santer. \"The recognition that the MacArthur brought showed me that I wasn't alone, that there were others who thought this battle to preserve the integrity of science against these powerful interests was worth fighting.\" The award money allowed him to pay his legal fees, refocus his energy on science and helped secure a major grant from the Department of Energy to pursue his academic studies. (In 1999, he won custody of his son.) Santer says that the MacArthur prestige also generated its own pressure that has followed him for the rest of his career. \"People expect you to do extraordinary things and for pearls of wisdom to come out every time you open your mouth,\" he says. Santer went on to show that the human fingerprint was evident in many other parts of the climate system, such as atmospheric water vapour, the height of the tropopause (the upper limit of the weather-containing layer of atmosphere) and ocean surface temperatures in regions where hurricanes form. He and his colleagues also tackled another of the field's controversies \u2014 why a satellite data set developed at the University of Alabama in Huntsville showed cooling temperatures in the atmosphere in the tropics, whereas computer models invariably predicted warming in response to greenhouse gases. The apparent discrepancy was a hot issue for more than a decade because it allowed climate-change sceptics to question the validity of the various climate models. In 2005, scientists at Remote Sensing Systems in Santa Rosa, California, found that the cooling results were due to an error in correcting the Alabama data for the effects of satellite orbital drift and created a new data set that was adjusted correctly. Led by Santer, a large team including many high-profile climate scientists, then showed that the new data set was in good agreement with computer models 4 . Only the most extreme sceptics now doubt that humans have contributed to the global climate. \"I have felt the responsibility to continue to do science that helps us understand the magnitude of the problem,\" says Santer, who has continued to contribute to subsequent IPCC reports. Karl Taylor, a climate scientist and long-time collaborator with Santer at Lawrence Livermore, says that Santer's 'genius' lies in the ability to coordinate group efforts to address important questions. \"He's a driver of work and he has the confidence of the scientific community. When he asks, people are willing to jump in and help,\" he says. Like most other people awarded with a MacArthur fellowship, Santer is uncomfortable with the label of 'genius'. \"I don't think of myself as a genius,\" he says. \"If I ever develop any kind of ego or maniacal characteristics, my friends and family should feel free to take the nearest blunt object and whack me on the head.\"  \n                Class of 2003: Loren Rieseberg \n             Loren Rieseberg isn't a science superstar. He has no fancy educational pedigree. He doesn't exhibit eccentricities or ego. \"On the outside, he is exceedingly calm, very soft-spoken, and kind,\" says Gerry Gastony, a plant systematist working with Rieseberg at Indiana University in Bloomington. \"All that belies the intensity within. He has a laser-like focus.\" As an undergraduate, Rieseberg attended Southern Adventist University in Collegedale, Tennessee, where the teaching of evolution was banned. To get around the restrictions, the biology department taught a class called 'speciation' because as Rieseberg puts it, even Bible college types \"don't mind a little microevolution\". Captivated by the subject, Rieseberg pursued a PhD in plant speciation at Washington State University in Pullman. \"I was canny enough to realize even at that early stage that grant funding would be easier if I was working on a wild relative of a crop plant \u2014 and sunflowers are the one of very few crop plants to be domesticated in North America,\" says Rieseberg. \"Plus, I like the way they look.\" Rieseberg turned down posts at Harvard University in Cambridge and at the University of Michigan in Ann Arbor for a job at Rancho Santa Ana Botanic Garden in Claremont, California, where, he says \"there were no committees to sit on\". It was there in the early 1990s that he started the work that most likely opened his MacArthur file, showing that new species of sunflowers can arise from hybridization. Hybrids are often considered an evolutionary dead-end, because they tend to have low or no fertility: think mules, the generally sterile offspring of male donkeys and female horses. Rieseberg investigated how new, viable species could arise by crossing together two US sunflower species to make experimental hybrids and then using nearly 200 genetic markers to roughly map their genomes. The work showed that the greenhouse-made hybrids had a similar genetic make-up to an ancient, natural sunflower hybrid species, suggesting that selection rather than chance determines which genetic combinations survive as hybrid species 5 . The genetic mapping was a technical tour de force, taking almost 8 years and 100,000 manual polymerase chain reactions. And the study changed the way that botanists and zoologists think about hybridization, says Jeannette Whitton, who was a postdoc with Rieseberg and is now an evolutionary biologist at University of British Columbia in Vancouver. \"When you form a hybrid, yes, the first generation's genome is 50\u201350 between the parental species. But after that, all bets are off \u2014 selection pulls advantageous genes through to the following generations and disadvantageous genes get left behind,\" says Whitton Rieseberg says that winning the MacArthur fellowship \"has made everything easier\". He says the biggest impact was in greasing the wheels for the next grants \u2014 a comment repeated by other winners. It helped him expand his group, which at around 20 people is arguably the largest plant evolutionary biology lab in North America; in the five years since his MacArthur fellowship, Reiseberg has published 50 original research papers. The influx of cash allowed Rieseberg to maintain his laboratory at Indiana and served as a 'piggybank' to set up another at the University of British Columbia, which is close to where he grew up. Since the MacArthur award, Rieseberg and his group have solved a long-running debate by showing that sunflowers were first domesticated on the east coast of the United States rather than in Mexico, as some had thought. They also showed that introducing a genetic modification for pest resistance is highly advantageous and likely to spread from crop sunflowers to wild relatives 6 . Rieseberg thinks that the aura surrounding the MacArthur award, rather than the money itself, has made the biggest difference to him in his career. He suspects that the MacArthur Foundation earned its reputation by selecting people from all ages and disciplines and by rewarding people early in their careers. \"There are a handful of MacArthur Fellows who are truly head and shoulders above everyone in their field,\" he says. \"But a lot more of us are lucky.\"  \n                Class of 2007: Yoky Matsuoka \n             All the MacArthur award recipients remember the moment they got the call. When neuroscientist Yoky Matsuoka picked up the phone last September, she assumed the caller was a prankster. \"I'm going to tell you something very shocking,\" said the voice. \"Are you sitting down? If you are holding anything precious, like a baby, please put it down.\" Matsuoka was indeed holding a baby: she'd given birth eight days earlier. The caller, programme director Daniel Socolow, promised to try again in half an hour. When he did, he told her she had been named a MacArthur Foundation Fellow, that she would receive half a million dollars with no further obligation and did she have any questions? \"I'll never forget in my life how it happened,\" says Matsuoka. \"And one year later, it still doesn't feel real.\" For Matsuoka, who works in robotics at the University of Washington in Seattle, the sum of money could be dwarfed by larger research budgets. \"If I just let it sit, it could easily get absorbed into daily things,\" she says. Instead, Matsuoka has spent the past year pondering how to do something special with her fellowship. \"In academic life, it's more about buying time than anything else,\" she says. And time is at a premium for this 37-year-old whose stated lab and life motto is 'work hard, play hard'. While in graduate school at the Massachusetts Institute of Technology in Cambridge, Matsuoka became frustrated by the lack of neuroscience in the field of robotics and went on to combine the two in the emerging field called neurobotics. Her laboratory's main goal is to produce the Anatomically Correct Testbed Hand \u2014 a prosthetic that resembles and functions like a human hand, including synthetic parts for the bones, tendons, muscles and skin. It would be controlled by the same brain signals that control a natural hand. The aim is to provide rehabilitation and assistance to people with disabilities, and to understand how all the moving parts and neural signals generate such dexterity. So far, Matsuoka has developed a robotic hand with three of the fingers moved by several motors, representing the muscles that give the human finger its many ranges of motion 7 . Current prosthetics, by contrast, typically offer only one range of motion, opening and closing the fingers. Her group has also tested whether 'molecular wires', polymers designed to conduct electricity across cell membranes, can be inserted into neuron membranes to connect a silicon prosthesis directly to existing nerve cells 8 . The team has succeeded in artificial cell membranes but has yet to translate their findings to real neurons. \"Yoky can do pretty much whatever she wants,\" says Matt O'Donnell, the dean of engineering who helped recruit Matsuoka to the University of Washington in 2006. He says the synthesis of living tissue engineering with non-biological components in the future is likely to overtake a purely robotic project such as the testbed hand, but that if anyone can do this type of synthesis, it's Matsuoka. Fitzsimmons says that, like many recipients, Matsuoka was chosen because there was evidence, including testimony from her peers, of her creativity and diligence. She was selected \"with the confidence that she can and will make important strides in whatever direction she pursues\", he says. Since receiving the call, Matsuoka has decided to use the MacArthur money to make a more immediate difference in the lives of people with disabilities. Her inspiration was a man she had met about five years ago who complained that the umbrella attachment for his wheelchair only allowed for a vertical position, whereas rain comes down in all directions. She immediately recognized this as a simple engineering problem \u2014 one easily solved by undergraduate or graduate students. She figured there must be many such ways that engineering expertise could improve the quality of life for people with disabilities, but how would she organize such an effort on top of her already demanding duties? Enter Rayna Liekweg, a former manager for IBM with a background in mathematics, who had spent the past decade raising her children in the Seattle suburb of Kirkland. Liekweg read about Matsuoka being tapped as a MacArthur Fellow in the newspaper and, fascinated by her neurobotic work, cold-called her to see if there was a way she could help as a volunteer. \"There is a whole community of women out there not unlike me,\" says Liekweg. \"We are college-educated, have good skills and are looking for something meaningful and intellectually challenging to contribute to.\" Matsuoka is in the process of forming a non-profit organization to draw on this community of women, making them into project managers who would connect a problem encountered by people with disabilities with engineering students who could build solutions. As a mother of three children under three-years-old, Matsuoka says she is passionate about helping and encouraging women who are balancing work and family or who gave up their careers. She also wants to change the image of science and maths for young girls so that being captain of the National Science Bowl team becomes as hip as being captain of the girls' soccer team. \"Now that I'm in my thirties, it's time for me to be disseminating what I did wrong and what I did right to those in their teens and twenties.\" Matsuoka admits that she was worried at first about getting stuck with the label 'genius' and how it might fuel jealousy. She had heard the sniping comments colleagues made behind the back of a researcher who had won another prestigious award. \"But after the first three months I let that go. People are going to talk. I have to do the productive things I can do because of it. Then they can say, 'Maybe she's not a genius, but at least she's done something with it.\" Kendall Powell is a freelance science writer based in Broomfield, Colorado. Read our  news story  about the MacArthur Fellows of 2008. \n                     MacArthur Foundation \n                   \n                     Lawrence Livermore National Laboratory \n                   \n                     Loren Rieseberg's Lab page \n                   \n                     Yoky Matsuoka's Lab page \n                   Reprints and Permissions"},
{"file_id": "455720a", "url": "https://www.nature.com/articles/455720a", "year": 2008, "authors": [{"name": "John Whitfield"}], "parsed_as_year": "2006_or_before", "body": "What makes a successful team? John Whitfield looks at research that uses massive online databases and network analysis to come up with some rules of thumb for productive collaborations. Flip through any recent issue of  Nature , including this one, and the story is there in black and white: almost all original research papers have multiple authors. So far this year, in fact,  Nature  has published only six single-author papers, out of a total of some 700 reports. And the proportions would be much the same in any other leading research journal. Of course, there is nothing new about this: the scholars who study the folkways of science have been tracking the decline of the single-author paper for decades now. And they have followed the parallel growth of 'invisible colleges' of researchers who are separated by geography yet united in interest. But what is new is how their studies have been turbo-charged by the availability of online databases containing millions of papers, as well as analytical tools from network science \u2014 the discipline that maps the structure and dynamics of all kinds of interlinked systems, from food webs to websites. The result is a clearer picture of science's increasingly collaborative nature, and of the factors that determine a team's success. Funding agencies are not using this work to decide where the money goes \u2014 yet. But the researchers behind the analyses are willing to give tentative tips on what their work reveals. They also think that their studies point to rules of thumb that apply very broadly, whether you're looking for a gene or putting on a show. The first question a researcher might ask him- or herself is: should I collaborate at all? Perhaps the rarity of single-author papers would translate into higher impact? To answer this question, sociologist Brian Uzzi of Northwestern University in Evanston, Illinois, and his colleagues analysed more than 2 million patents 1 , along with nearly 20 million papers published since 1955. They found that in the early 1950s, the most cited paper in any year was more likely to have been written by a single author than a team, but this pattern reversed decades ago. And the citation gap continues to widen. \"The image of the scientist alone at the workbench, plucking ideas out of the ether was true up to about the end of the Second World War,\" says Uzzi, \"but not any more.\" Uzzi doesn't know what drives this trend. It is not just a product of science's increasing technical complexity: the same pattern is seen in pencil-and-paper disciplines such as mathematics and the humanities. It is not just the Internet: author teams began to swell long before the online age, and the dawn of e-mail hardly affected that growth. And it is not just that large teams create many opportunities for future self-promotion: the pattern remains when self-citation is removed. Uzzi speculates that the increasing specialization of all fields plays a part, as may changing social norms. Researchers have always swapped ideas and criticism, but when fields were small, authorship was not such an important mark of achievement. Reputation travelled by word of mouth, and everyone knew who had contributed the good ideas. Now, however, academia is too vast for that kind of informal credit assignment to work. So people need to get their ideas and their names into print, as well as on each other's lips. So if lone wolves go hungry, who should researchers hunt with? Someone in their own discipline, or someone in another field? Should they build long-term relationships, or should they keep changing the people they work with? Research is now revealing that these questions need to be answered with a careful weighing up of costs and benefits, rather than a list of absolute dos and don'ts: teams are most successful when they contain the right mix of specialism and diversity, and familiarity and freshness. And researchers are starting to find hints of how to strike this balance. Uzzi and his team, for instance, looked at a sample of 4.2 million papers published between 1975 and 2005. Dividing universities into tiers based on the number of citations their researchers achieved, they found that teaming up with someone from another institution of the same or higher tier reliably produced more highly cited work than teaming up with someone down the corridor. \"There's something about between-school collaboration that's associated with the production of better science,\" Uzzi told participants at a meeting of network scientists in Norwich, UK, in June. At the same meeting Pietro Panzarasa, from Queen Mary University of London, presented an analysis of 9,325 papers written by 8,360 authors submitted to the 2001 UK Research Assessment Exercise in business and management studies. He too found that between-institution collaborations had a higher average impact than did those within institutions.  \n                Middle ground \n              As well as looking at where people worked, Panzarasa looked at how specialized they were. First he assigned researchers to disciplines by analysing the keywords in their papers, and then he measured each author's breadth of experience by looking at the fields of their co-workers. Social scientists are divided over whether specialization is the best strategy, he says. \"It is beneficial for productivity and earning, but there is also evidence from banking and academia that being a generalist pays off.\" Panzarasa's data show that the most highly cited papers were written either by authors who worked mostly with others in their own field or by those who worked with people in a wide range of other disciplines. But between these peaks lay a trough: papers that had authors from an intermediate number of disciplines were the most poorly cited. \"Being extremely specialized allows you to exploit the benefits of being embedded in your discipline, such as reputation, consensus building and controlling the flow of knowledge,\" says Panzarasa. \"When you go to the other extreme you can take advantage of all the information coming from different pools of knowledge. But if you're somewhere in the middle, you have less success \u2014 unless you feel you can manage very high levels of interdisciplinarity, it might be better to stay in your discipline.\" The most successful interdisciplinary authors, Panzarasa found, work with people who have independent authorship connections with each other, creating a tight social network. Panzarasa suspects that when these backup connections between colleagues are missing, the person in the middle can flounder as they try to process too many information streams. But his analysis also found that highly specialized workers who broaden their focus slightly produce more highly cited papers, as do those that exploit what social scientists call brokerage: bridging communication gaps between researchers who don't otherwise interact, and acting as a conduit for transferring knowledge from one field to another. Specialist brokers produced the most highly cited papers of any in his sample. The lesson of these studies might seem to be that if you do decide to take the leap across disciplinary boundaries, then the more addresses and subjects you can cram onto an author list, the better. But not necessarily. All these surveys have looked for co-authorship patterns in the published literature, which means that they have a built-in bias: they look only at the collaborations that actually result in publication. In fact, teams can also fail if they spread themselves too thinly. Jonathon Cummings, of Duke University's Fuqua School of Business in Durham, North Carolina, is monitoring more than 500 projects funded by the US National Science Foundation's information technology research programme, which creates cross-disciplinary teams of natural, social and computer scientists. He found that the most diverse teams were, on average, the least productive 2 . \"Projects that had more universities involved were at a greater risk of not publishing,\" says Cummings, as were those that covered multiple disciplines. \n               Click here for larger image \n               This apparent discrepancy is resolved by thinking of interdisciplinary research as a high-risk, high-reward business, explains Sam Arbesman, a mathematician at Harvard Medical School in Cambridge, Massachusetts, who has studied authorship networks. \"A more diverse team isn't always better \u2014 it might be that you get more really good or really bad research,\" he says. Still, there are ways to reduce the risks that the work won't be publishable. Cummings found that if the principal investigators had a previous history of collaboration, their project was much more likely to be successful than if they had never written a paper together before. Such teams will have already paid the start-up costs of getting everyone familiar with one another's approaches and languages; new teams should invest in travel and seminars, he says. \"Familiarity adds a lot of value.\"  \n                Talent spotting \n              \"We can spot projects that have been patched together at the last minute in response to the latest call for proposals,\" says Suzanne Iacono, who directs the information technology research programme. \"Reviewers say, 'These people have never produced a paper before, and we're going to give them $15 million?'\" The programme currently requires researchers to include plans for team-building in their proposals, but Iacono wants more than that. \"I'd like to understand better the point at which bringing in more disciplines leads to a decline in knowledge production,\" she says. But it is a fine line between a collaboration that has found its groove and one that has fallen into a rut. And it's not a line that people spot easily, because mature groups gravitate towards common ground and avoid areas of disagreement. Network scientists call this an echo chamber: a situation in which everyone tells everyone else what they want to hear, and a group that thinks it is performing well is really just mired in consensus. To avoid stagnating, scientists think that teams need a stream of fresh input. And the optimum rate of turnover seems to depend on the size of the team. In a paper published in  Nature  last year 3 , physicist Gergely Palla of the Hungarian Academy of Sciences in Budapest and his colleagues analysed networks of authorship on physics papers posted to the arXiv preprint server. They showed that teams with around 20 members had a better chance of surviving for a long period if they had a high rate of arrival and departure. For a team of three or four to persist, however, the opposite was true \u2014 they needed stability. Palla speculates that it's easy to find two people you like well enough to form a long-term working relationship; in a big team, fall-outs are inevitable, but the whole can persist if the comings and goings are constant and low-level. Endurance is not the same as quality of output, of course, but, as Pallas says: \"It's hard to imagine that you would publish rubbish for a long time.\" But even small groups benefit from some turnover. Looking at a data set of nearly 90,000 papers published between 1955 and 2004 by 115,000 authors in 32 journals spread across the fields of social psychology, economics, ecology and astronomy, Luis Amaral, a network scientist at Northwestern, and his colleagues measured the proportion of authors who had worked with each other before 4 . Papers in high-impact journals showed a strikingly lower proportion of these repeated interactions than did papers in low-impact journals. \"The patterns with repeat collaboration are very different and dramatic,\" says Amaral. \"In low-impact journals, people repeat collaborations almost all the time.\" When people choose collaborators, says Uzzi, who also worked on this analysis, they look for two opposing things: high-status individuals with a proven record and good resources, and newcomers who have lots of time and energy to devote to a project. The trick is to find the balance. \"If you had to give people a rule of thumb, you might want 60\u201370% of a team to be incumbents, and 50\u201360% repeat relationships,\" Uzzi says. \"That gets you into the bliss point across four very different scientific fields.\" And this is not just in science \u2014 the same, they found, goes for Broadway musicals. It typically takes six specialists to create and put on a musical: one each to write the music, lyrics and dialogue, plus a choreographer, director and producer. The most critically and financially successful musicals have an intermediate level of turnover within the creative team 5 . Amaral thinks there may be group properties that influence outcomes across all kinds of collective effort \u2014 \"but we'll need a lot of data to figure them out\", he says. Uzzi has been mobbed by organizations seeking to locate their bliss points. \"The president of a university called me up to ask how he can tell what areas he should be investing in,\" he says. Corporations have been asking for tips on assembling work groups; venture capitalists want to know how to spot the next hot field; a delegation from the US National Institutes of Health (NIH) is interested in whether the work can help make funding decisions; and Uzzi has been invited to the offices of  Nature  and  Science , as both journals strain after ways to detect the highest-impact papers.  \n                First come, most cited? \n              Another issue is the opaque relationship between a paper's citations and its science. A known trend is that the more a paper is cited, the more citations it attracts, which stretches small gaps in quality into chasms in citation count. The process can also reward novelty above merit \u2014 in a preprint posted online this September, physicist Mark Newman of the University of Michigan in Ann Arbor models and measures the effects of 'first-mover advantage' on citations, showing that it has no relation to the quality of the research. Those who are the first to publish in a new field are likely to garner more citations than those who publish later 6 . \"Were we wearing our cynical hat today,\" he writes, \"we might say that the scientist who wants to become famous is better off writing a modest paper in next year's hottest field than an outstanding paper in this year's.\" There are also other networks to consider: analysing every paper published in the  Proceedings of the National Academy of Sciences  between 1982 and 2001, Katy B\u00f6rner, who studies networks and information visualization at Indiana University in Bloomington, found that US authors are more likely to cite papers by workers at nearby institutions than from those on the other side of the country 7 . \"People read widely,\" she says, \"but when it comes to filling the slot at the end of the paper, they also consider who they have to face again in the hallway or at the next conference.\" Such factors make some urge caution about using network analysis. At present, no one should be using such techniques to judge a collaboration's likely performance, says Deborah Duran, head of the systemic assessments branch of the Office of Portfolio Analysis and Strategic Initiatives at the NIH. \"We can see a pattern, but we don't know what the pattern means,\" she says. Louis Gross, a theoretical ecologist at the University of Tennessee in Knoxville, agrees. \"It is very difficult to account for the effects of social networking in evaluating metrics of citations. Network analysis definitely has potential, but an awful lot of social science needs to be integrated with these analyses to ensure that they are applied in an equitable way,\" he says. Gross has reviewed grant proposals for the European Commission; one risk in Europe, he says, is that if granting agencies place too much emphasis on encouraging international collaborations they will stunt development within institutions and nations. But Duran does expect network studies to be an important part of what she calls \"the emerging science of science management\". The NIH already uses data-mining tools devised by the company Discovery Logic, based in Rockville, Maryland, to see how grants connect to papers, citations, patents and products. Duran suggests that in the future, network analysis could be used to track the spread of new ideas, work out the best ways to disseminate information or to target particularly well-connected individuals to work on emerging issues. \"I think, hope and believe that this will become useful,\" she says. So, can a scientist looking to make the most of his or her talent really exploit these findings? Amaral says that network analysis might actually help young researchers to look beyond citation counts, which are dominated by a field's obvious stars, and find other groups with a healthy mix of rookies and veterans and a productive rate of turnover. At present, a do-it-yourself approach would be difficult: mapping the networks and measuring scientific success requires access to subscription databases such as ISI and computing resources that are beyond the reach of the average web-surfing graduate student. But this is about to change: B\u00f6rner and her colleagues are soon to release an open-access tool for analysing scholarly networks. This will allow researchers to map connections using free sources such as Google Scholar, as well as Indiana University's database of 20 million publications, patents and grants and even its own bibliography files. But however finely honed scientists' team-building strategies become, there will always be room for the solo effort. In 1963, Derek de Solla Price, the father of authorship-network studies, noted that if the trends of that time persisted, single-author papers in chemistry would be extinct by 1980 8 . In fact, many branches of science seem destined to get ever closer to that point but never reach it 9 . And whatever the pay-off in citations might be, there's still a pleasure to be had in seeing just your name on a paper, says Matt Friedman, a palaeontology graduate student at the University of Chicago and a member of  Nature 's sextet of singleton authors[10]. \"With any piece of scientific work there are people who help you along the way,\" he says. \"But knowing that you developed a project from start to finish largely under your own direction is gratifying. It's a nice validation of my ability to do science.\" \n                     Brian Uzzi \n                   \n                     Cyberinfrastructure for network science \n                   \n                     NetSci 08 \n                   Reprints and Permissions"},
{"file_id": "455442a", "url": "https://www.nature.com/articles/455442a", "year": 2008, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "The leading US presidential candidates are not trying to woo voters with science issues. But the senator who wins will help shape the world's most influential research agenda. Alexandra Witze looks at how John McCain and Barack Obama have developed their thoughts on science and technology, and where each of them might take the country if elected. The United States' scientific establishment has been doing its best to get this year's presidential contenders to pay attention to it. In April, a group of supporters calling itself Science Debate 2008 reserved a large hall at the Franklin Institute in Philadelphia, hoping the whole field of candidates would show up to take questions. None accepted. In fact, it took evangelical pastor Rick Warren to get the two leading candidates \u2014 Republican John McCain and Democrat Barack Obama \u2014 together for the first time, at a forum at a California church in August. It is no surprise that the campaign has revolved more around the war and the economy, and of course personality, than science. It would be a mistake, however, to assume that neither McCain nor Obama cares about science and technology. In fact, both have a long history of engaging with core science issues \u2014 from McCain running a Senate committee that oversaw science, among many topics, to Obama working on technological approaches to fighting poverty. During the course of the campaign both candidates have laid out where they stand on scientific and technical issues (see  page 446 ). Major changes are in store no matter who wins. Many scientists argue that the research environment will be better off during the next four years than it has been under President George W. Bush's two terms. The next president will probably play a leading part in initiating the first mandatory greenhouse-gas regulations in the United States. He may also lift the restrictions on research with human embryonic stem cells that limit federal funding to cell lines that are at least seven years old. New people will move into critical positions such as the presidential science adviser (see  page 453 ) and heads of the various science-related agencies (see  page 451 ). This will be a breath of fresh air to researchers who charge that the Bush administration has manipulated science to political ends, from climate-change reports to endangered-species listings. Bush aside, though, the question remains as to how McCain or Obama might influence research. What the candidates say about science will normally be broad-brush; no one expects a president to rattle off details of the confinement of quarks or the dynamics of proteins. More pertinent is how they think about science as a process, where they get their scientific advice from \u2014 and how they might incorporate such advice into where they would lead the country. Obama's views on how science and technology affect the nation's growth trace back several years. Working with disadvantaged children on Chicago's South Side, \"he saw the role and importance of technology in the future of those schoolchildren\", says Alec Ross, a technology entrepreneur and campaign adviser. Early in his campaign, Obama released details on how he would expand broadband access as a way to shrink the 'digital divide' between rich and poor, and described plans to appoint the first-ever national 'chief technology officer' to improve infrastructure. The theme of how technology investment can drive economic growth comes up again and again in Obama's speeches. His advisers say that he spends a lot of time thinking about how science and technology can be used to address specific national aims, such as health care and climate change. To this end, Obama voted in the Senate in favour of the 2007 America COMPETES Act, which authorized among other things a doubling in the physical-sciences research budgets at the National Science Foundation, the Department of Energy's Office of Science and the National Institute of Standards and Technology. It was a politically popular act that sailed through the Senate \u2014 also garnering McCain's support \u2014 perhaps because it was simply an expression of support for funding in these areas and not an actual allocation of dollars.  \n                Expert advice \n              Much of Obama's advice on economic matters comes from a group of advisers that includes Austan Goolsbee of the University of Chicago in Illinois. Obama also maintains a select group of science advisers headed by Nobel laureate Harold Varmus, head of the Memorial Sloan-Kettering Cancer Center in New York and former director of the National Institutes of Health. Varmus endorsed Obama back in February and has been his linchpin for science advice since; even so, the most face time Varmus has had with the candidate was a two-hour summit on competitiveness issues at Carnegie Mellon University, in Pittsburgh, Pennsylvania, in June. Varmus has strong local support in Don Lamb, an astrophysicist at the University of Chicago who has known Obama since he represented Lamb in the state senate of Illinois. Lamb serves as a sort of secretary for the advisory group, maintaining e-mail lists and helping coordinate discussions and the flow of advice back into the Obama campaign. Other members of the team include Henry Kelly, president of the Federation of American Scientists in Washington DC, who served as assistant director for technology in Bill Clinton's advisory office; Gil Omenn, a professor of internal medicine and human genetics at the University of Michigan in Ann Arbor who held several key policy positions in the Carter administration; and Sharon Long, a biologist at Stanford University in Palo Alto, California. Also loosely connected are a host of Nobel laureates including Bob Horvitz of the Massachusetts Institute of Technology; Peter Agre of the Johns Hopkins Malaria Research Institute in Baltimore, Maryland; and Burton Richter of the Stanford Linear Accelerator Center in Menlo Park, California. The team also includes Tom Kalil, special assistant to the chancellor for science and technology at the University of California, Berkeley, and deputy assistant to President Clinton for technology and economic policy. Kalil was the main architect of Hillary Clinton's science platform, generally regarded as the most well-developed set of policies on science for any of the presidential candidates until she dropped out of the race. Varmus's team helps to write material that serves as the basis for developing policy platforms, and otherwise throws general science advice into the churning maw that is any candidate's advisory system. A related group funnels climate and energy advice up the same chain. This group includes energy researcher Dan Kammen of the University of California, Berkeley, and energy policy expert Jason Grumet, founder and president of the Bipartisan Policy Center in Washington DC. Academics are notoriously left-leaning \u2014 a national study done last year by sociologists Neil Gross, from Harvard University in Cambridge, Massachusetts, and Solon Simmons, from George Mason University in Fairfax, Virginia, found that half of them identified themselves as Democrats, and just 14% as Republicans. So perhaps it's not surprising that McCain's science and technical advice comes more from corporate and political leaders than from the academic world. Much of his campaign's day-to-day work on technical matters comes from experienced staff members who have worked with McCain for years. These include Floyd DesChamps of the Senate Committee on Commerce, Science and Transportation, which McCain headed for much of the time between 1997 and 2001. The committee has a broad remit but includes, for instance, NASA, where McCain was involved in following up on the 2003  Columbia  space-shuttle disaster. Other technical advice to the McCain campaign comes from business leaders such as Carly Fiorina, the former chief executive of Hewlett-Packard, and Meg Whitman, the former chief executive of eBay. McCain's most detailed positions on science issues come in the field of energy. According to James Woolsey, director of central intelligence under Bill Clinton, the campaign's top policy advisers request specific advice on energy from a small group that includes Woolsey; Ronald Reagan's national security adviser Robert McFarlane (now a renewable-energy advocate); and James Schlesinger, secretary of defence under Richard Nixon and Gerald Ford and secretary of energy under Jimmy Carter. Since July, the McCain campaign has also relied on chief strategist Steve Schmidt \u2014 an adviser to former White House deputy chief of staff Karl Rove and to Vice-President Dick Cheney \u2014 who helped develop energy and climate policies when he masterminded the successful 2006 re-election campaign of California governor Arnold Schwarzenegger. Environmental groups have not been big fans of McCain in recent years \u2014 yet in some respects, McCain has adopted politically risky energy policies. He has proposed scrapping subsidies and tariffs for biofuels, an approach favoured by Obama that plays well with voters in America's corn-growing heartland, but that a growing consensus among energy and environmental experts rejects. Most strikingly, McCain is a long-time supporter of a cap-and-trade approach to regulating greenhouse-gas emissions, in which the government sets an overall cap on pollution and then issues permits to each polluter; those that reduce their emissions cheaply can sell the extra permits to anybody who needs them. Whereas McCain's plan for such a system would distribute the allowances and allow businesses to sell any unused emissions for cash, Obama's version of the scheme would auction off the allowances from the start. Obama has also pledged $150 billion over the next ten years towards energy technologies.  \n                Environmental back-up \n              Five years ago, McCain introduced the first version of a significant emissions-regulating bill with Joseph Lieberman, the formerly Democratic, now Independent, senator from Connecticut. Introduced three times into the Senate, the McCain\u2013Lieberman bill never garnered enough votes to pass, but is seen as the prototype and inspiration for much of what followed. Tim Profeta, the former Lieberman staff member who worked with McCain aides to write the bill, says he thinks McCain will continue to fight for a cap-and-trade strategy if elected as president. \"He may have to make some deals, but I don't see any indication from him that he really is wavering in his dedication to the topic. At this point, I have to take him at his word,\" says Profeta, now director of the Nicholas School of the Environment at Duke University in Durham, North Carolina. If the presidential candidates' views on scientific matters are of only minor interest to voters, those of the vice-presidential candidates are usually even less so. This year, though, that might not hold true. Obama's choice of Joe Biden had much more to do with Biden's experience in foreign policy than anything he might have to say about offshore oil drilling. Biden, a senator from Delaware, hews to traditional Democratic lines on most technical issues. Despite his personal belief that human lives begin at conception, Biden supports abortion rights and human embryonic stem-cell research. Biden has never been shy about speaking his mind; in an appearance last month on Bill Maher's liberal television talk show, he spoke out against teaching intelligent design in science class, saying \"I refuse to believe the majority of people believe this malarkey.\" McCain's choice of Sarah Palin as a vice-president is much more challenging to parse. Far more than McCain himself, Palin is truly representative of the evangelical Christian right. Also convinced in the embryo's full personhood, she opposes human embryonic stem-cell research, as well as abortion for any reason other than saving the mother's life. Biden even got into a dust-up with Palin this month by asking why she doesn't support stem-cell research when she has a child with Down syndrome. When running for governor of Alaska in 2006, Palin said that creationism should be taught in school on the basis that \"healthy debate is so important\". And in her only major interview to date, Palin engaged in a back-and-forth with ABC's television news anchor Charlie Gibson over whether she had ever challenged humanity's role in climate change. This summer, the conservative news outlet Newsmax reported her as saying she did not think global warming was \"man-made\". In her ABC interview, though, Palin said: \"Regardless, though, of the reason for climate change, whether it's entirely or wholly caused by man's activities or is part of the cyclical nature of our planet \u2014 the warming and the cooling trends \u2014 regardless of that, John McCain and I agree that we gotta do something about it.\" How \u2014 or indeed if \u2014 McCain and Palin will resolve their large differences on science topics remains to be seen. In some areas the two have been moving closer; McCain has, for instance, come out in favour of offshore drilling for more oil and gas, a stance long favoured by Palin, as by most Alaskans. Stem cells, however, could turn out to be more of a stumbling block. As a senator, McCain twice voted to lift the federal restrictions on funding of human embryonic stem-cell research, but he has also spoken of adult stem-cell research and other new techniques, such as induced pluripotent stem cells, as an addition to funding the work on embryonic stem cells. In answer to a questionnaire submitted by the advocacy group Research!America, he wrote: \"I also believe that clear lines should be drawn to reflect a refusal to sacrifice moral values and ethical principles for the sake of scientific progress.\" Last week, at a forum on health policy organized by the advocacy group Scientists and Engineers for America, McCain health-policy adviser Jay Khosla would not explicitly answer a question on the ban, reiterating McCain's prior voting record and interest in alternative approaches to human embryonic stem cells. \"His real hope is that we can continue to develop new technologies,\" says Khosla. Biomedical research advocates are starting to wonder what a McCain\u2013Palin administration would mean for stem cells. \"Although he has historically been somewhat independent from the White House on issues such as stem cells, I think with his vice-presidential pick, he would definitely not lift the ban,\" says Frankie Trull, president of the Foundation for Biomedical Research in Washington DC.  \n                Top priorities \n              McCain advisers have been quick to point out that McCain, not Palin, is the presidential nominee, and as such his priorities take precedence. And it's possible that intense media focus on Palin since her nomination has exaggerated her possible influence on McCain's positions. She was not, after all, chosen for her background and expertise in policy. But her addition to the ticket is perhaps the most significant factor in recent months that could influence where science goes in the next administration. As the campaign grinds its way towards its 4 November end, the science advisers for each campaign are still working to flesh out various platforms. Last month, for instance, McCain and Obama released detailed policies on space for the first time. Such policies are needed because of the electoral college votes up for grabs in the swing state of Florida, which stands to lose thousands of jobs when the space shuttle is retired in 2010, as current Bush policy holds. Clinton official Lori Garver helped to develop Obama's space platform, which calls for sending humans to Mars, whereas astronaut Walt Cunningham has represented the McCain camp at times on its space policies. As they scramble to put together last-minute science advice, campaign representatives on both sides are also looking forward to life after November. One high priority is to draw up lists of possible names for appointments to top science positions in a new administration. The changeover from the Bush administration to a McCain or Obama one will be handled by a 'transition team', often incorporating some of the main campaign advisers and using them to help populate the new scene in Washington.\"The most important thing about the transition is the president-elect's selection of the right team,\" says Neal Lane of Rice University in Houston, Texas, who served as a science adviser to Bill Clinton. The next question will be how that new administration will work with the newly elected Congress; all 435 seats in the House of Representatives are up for grabs in November, along with one-third of the Senate seats. Still benefiting from the voter anger that swept them into power in 2006, the Democrats are widely expected to pick up seats in both the House of Representatives and the Senate. This could be crucial in the Senate, where Democrats govern by a narrow 51-49 majority, thanks to the fact that a pair of Independents has chosen to caucus with them. The delicate balance of Congressional power could be particularly important when it comes to climate-change legislation. In fact, Republican minorities in both the House and the Senate might be more inclined to work with a Republican president McCain on the issue. On the other hand, \"if it's President Obama\", says a Senate aide familiar with the discussions, \"then many Republicans on the Hill will see it as their job to obstruct climate legislation simply because it is the priority of the president, who is a Democrat.\" It's a salutory reminder that, whatever the next president's positions, their realization will depend on a lot of practical politics. And their implementation will depend on factors beyond even the president's power \u2014 from world food prices to medical breakthroughs to climate-linked catastrophes. For the next six weeks, though, it's all about who gets to face those uncertainties. Additional reporting by Jeff Tollefson and Meredith Wadman. For more on the US election, see  www.nature.com/uselection \n                     US election special \n                   \n                     John McCain \n                   \n                     Barack Obama \n                   \n                     Scientists and Engineers for America page on the candidates \n                   \n                     ScienceDebate 2008 \n                   \n                     AAAS page on the candidates \n                   Reprints and Permissions"},
{"file_id": "4541046a", "url": "https://www.nature.com/articles/4541046a", "year": 2008, "authors": [{"name": "Patrick Goymer"}], "parsed_as_year": "2006_or_before", "body": "Cancer cells vary; they compete; the fittest survive. Patrick Goymer reports on how evolutionary biology can be applied to cancer \u2014 and what good it might do. The oncology clinic isn't a field site where one might expect to find an evolutionary biologist. But within the complex ecosystem that is the human body, tumours grow, mutate and face diverse selective pressures as they change and react to their environment. Over hundreds of generations, cells can acquire mutations that promote their errant growth and survival. This makes for diversity both between cancer types and within an individual tumour. But just as species have evolved convergent similarities, cancers too have common themes and steps along their developmental paths. If properly directed with evolutionary theory in mind, treatments might become more effective (see  'Targeting what isn't there' ). Tony Green of the University of Cambridge, UK, and his colleagues have looked at evolutionary processes in myeloproliferative disorders \u2014 overgrowths of blood-producing bone-marrow cells that can become cancerous. Changes to the  JAK2  gene play an initiating role in these disorders, allowing the cells to bypass their growth-control mechanisms. Green and his colleagues began to study these mutations as the disorders progressed, in some cases, towards a cancer of the white blood cells called acute myeloid leukaemia, or AML. As expected, the  JAK2  mutation arises often and early in myeloproliferative disorders because of the growth advantage it confers on cells. But three of four individuals who went on to develop AML no longer had the mutation 1 . \"This was a surprise,\" says Green. \"An initiating mutation was not present in the more evolved state.\" Did cancer cells that had acquired  JAK2  mutations lose them over time as other mutations and physiological changes took over the controls of the disease? Or were the  JAK2  mutants outcompeted by other cells taking advantage of the changing environment within the cancer-afflicted individuals? Green stumbled across this evolutionary parallel, but some scientists specialize in comparing the similarities between changes to a cell in the body and the evolution of organisms within an ecosystem. As more information about cancer genetics accrues, the importance and usefulness of this evolutionary analogy is becoming clear. Science has been looking for commonalities in cancer, and several large-scale projects aimed at sequencing the genetic changes in different cancers have in their earliest stages revealed what many feared. The main feature of cancer, says Bert Vogelstein of Johns Hopkins University in Baltimore, Maryland, is its complexity and heterogeneity. Most mutations found in cancer are rare. \"There are a few genes that are commonly mutated \u2014 we call these the mountains \u2014 but the landscape is dominated by hills,\" says Vogelstein. Evolutionary theory, in conjunction with the sequencing of cancer genomes, could help map that countryside more quickly.  \n                Diversity breeds success \n             Peter Nowell of the University of Pennsylvania in Philadelphia first developed the idea of cancer as a Darwinian process in 1976 (ref.  2 ). Cancer is known to occur because of the stepwise accumulation of mutations in certain cells of the body. Nowell added to this the population-genetics idea of clonal expansion, in which cells that have a mutation to make them grow faster or survive better produce more offspring than surrounding cells without the mutation. Carlo Maley of the Wistar Institute in Philadelphia sees the diversity of cancer cells as key to understanding their resistance to drug treatment. \"One thing that is surprising is that the multidrug therapies in cancer haven't worked nearly as well as they have in HIV,\" he says. \"That seems to me to be a basic evolutionary question that should be addressed and is at the heart of why we haven't been able to cure cancer.\" Maley has been applying evolutionary theory to a condition called Barrett's oesophagus, which can progress to become cancer. As surgical treatment for Barrett's oesophagus is extremely risky, standard medical practice is to monitor cells in the oesophagus for signs that they have started to progress towards cancer. Maley uses biopsy samples to track the evolution of the disorder, testing each biopsy for changes in specific genes such as  CDKN2A  and  p53 . His group has found that in the early stage of the disorder, individuals with diverse populations of cells harbouring different mutations are more likely to develop cancer 3 . This might be because the body is struggling to defend itself against more kinds of attacks. Maley uses methods borrowed from ecology to measure the diversity and make predictions about progression.  \n                Quick and easy \n              Perhaps the most important advance in cancer biology has been cheap and fast DNA sequencing. The technology that allows researchers to sequence the genomes of hundreds of species, and of individual humans, is now being applied to the genomes of tumours. Knowing the genome sequence of a cancer cell allows scientists to look in detail at how a tumour has evolved from the normal cells of the body \u2014 which genes have mutated, how much of the original genome has been lost or duplicated, and whether the evolutionary process has unfolded similarly in each individual case. Several large-scale projects are taking this approach, including the Cancer Genome Project, which is sequencing protein-coding genes in cancer cells to look for mutations; the Cancer Genome Anatomy Project, which is looking at levels of gene expression in cancer cells; and the Cancer Genome Atlas, which is looking at various types of genomic alteration in specific cancer samples. But cancer genome sequences aren't by themselves going to explain the evolutionary process of tumour development. In fact, Maley and Green point out that the sequences provide only 'snapshots' of the evolutionary process, so further work is needed to fill in the gaps, such as the order in which the mutations appear. And current technology means that the genome sequences are actually an 'average' sequence taken from a heterogeneous collection of tumour cells, whereas much of the interesting detail is in the differences between individual cells within a tumour \u2014 after all, variation is the basic stuff on which natural selection acts. The need for a sophisticated evolutionary understanding of cancer led Vogelstein to team up with biologist and mathematician Martin Nowak from Harvard University in Cambridge, Massachusetts. Nowak has applied his modelling ideas to problems as diverse as the evolution of HIV, altruism and the politics of climate change. Cancer, he says, \"is just like any other evolutionary process, but it's even simpler. Because of this we can ask much more complicated questions.\" Sequencing the genomes of cancer cells, says Nowak, can \"help us get quantitative data to calibrate our evolutionary models\". From Vogelstein's data on sequence variation between individual colorectal cancers, Nowak could predict when malignant tumours would arise from benign ones and when they would metastasize, or spread to other parts of the body 4 . He found that malignant tumours do not mutate more frequently than normal cells, as is often thought. Instead, it is the evolutionary context in which these mutations occur that matters. Nowak's former student, Franziska Michor, now at the Memorial Sloan-Kettering Cancer Center in New York, is interested in developing her mentor's approach of modelling the process of cancer evolution. The roots of this approach go back half a century. In the 1950s, Richard Doll at the University of Oxford, UK, found that solving equations containing terms for growth and mutation rates allowed him to predict the number of mutations that are required for a tumour to evolve 5 . Doll developed a model in which the time taken for cancer to arise depends on the probabilities of each of the mutations needed to cause the cancer actually occurring, and he fitted the model to real incidence statistics. But Michor says that this approach fails to take into account population-genetics theory. Doll's models look at single cells, ignoring the fact that if the first mutation increases the evolutionary fitness of that cell, then the mutation will expand into many cells, increasing the probability that subsequent mutations will occur. Understanding this population effect will be hugely important in overcoming evolved drug resistance. One way to deal with this is to use combinations of treatments that tackle different aspects of the disease. \"We can try to come up with treatment strategies if we understand how many mutations are needed for resistance,\" says Michor. \"We can actually write down the equations that predict what the risk of resistance is depending on how many drugs you use.\" A crucial goal will be to make these models predict what is going on in systems with complicated evolutionary trajectories, such as AML.  \n                Pushing the parallels \n              So how far can the evolutionary idea be extended? The small number of cell divisions within a cancer compared with that in the evolution of species is an obvious limitation. But there are still plenty of evolutionary ideas to be explored in cancer, several of which come from thinking about the whole ecosystem of the disease. As with 'real' ecosystems, these involve not just the species in question, but also its competitors, predators and symbionts. The evolving cancer cell not only needs to outcompete the normal body cells, it must also evade attack by the immune system and, if it is to reach the advanced stages of cancer, it needs to cooperate with other cells and then migrate and colonize other parts of the body. For example, once a tumour reaches a certain critical mass its cells require a blood supply to keep them oxygenated. This means that it needs to co-opt the body's system for creating blood vessels. How these processes take place is ripe ground for evolutionary biologists and ecologists to investigate, something that Maley and his colleague John Pepper, of the University of Arizona in Tucson, were keen to encourage when they organized a recent workshop on the topic at the Santa Fe Institute in New Mexico. The fact that a similar workshop was organized by the National Cancer Institute, a major funding body, suggests that the money might follow. Maley certainly hopes so. \"I see my role,\" he says, \"as attempting to bring evolutionary biologists into cancer biology and advocating the need for evolutionary biologists as part of our interdisciplinary teams.\" Whether such interdisciplinary research will entice evolutionary biologists to shift their field study to the clinic is yet to be seen, but for investigations of variation and selection, cancers unfortunately continue to produce ample material for study.\n Patrick Goymer is associate editor of  Nature Reviews  Genetics. \n                     Cancerevo blog on Nature Network \n                   \n                     Nature Insight: Cell Division and Cancer \n                   \n                     Cancer Genome Project \n                   \n                     The Cancer Genome Anatomy Project \n                   \n                     The Cancer Genome Atlas \n                   Reprints and Permissions"},
{"file_id": "455022a", "url": "https://www.nature.com/articles/455022a", "year": 2008, "authors": [{"name": "Mitch Waldrop"}], "parsed_as_year": "2006_or_before", "body": "Pioneering biologists are trying to use wiki-type web pages to manage and interpret data, reports Mitch Waldrop. But will the wider research community go along with the experiment? Alexander Pico remembers just when the idea hit him. In January 2007, he and his boss, Bruce Conklin, were discussing how to push their software tool for visualizing intracellular signalling pathways to the next level of interactivity \u2014 when Pico blurted out, \"What we really need is a wiki!\" Well, it was an original thought at the time, says Pico, a software engineer in Conklin's laboratory in the Gladstone Institute of Cardiovascular Disease at the University of California, San Francisco. In retrospect, it was one of those ideas that strikes everywhere at once. As soon as he and his colleagues started giving talks about 'WikiPathways', as they called their project, someone in the audience would invariably say, \"Ah \u2014 we had the exact same idea.\" Scientist-edited interactive 'wiki'-type websites have proliferated over the past year or so (see ), to the point where researchers have begun to joke about the new science of 'wikiomics'. All the sites are modelled on the popular user-edited, online encyclopedia Wikipedia, and all aim to help biologists turn the data flooding into the large public gene and protein databases into useful knowledge. The flood is going to rise even faster, says Amos Bairoch, executive director of the Swiss Institute for Bioinformatics in Geneva and creator of Swiss-Prot, a predecessor to the international protein sequence database UniProt: \"As the price keeps going down, we're reaching the point where every genome that can be sequenced, will be sequenced,\" he says. Ultimately, that could mean the genomes of most of Earth's 1.8 million named species, along with individual variants produced by projects such as the '1000 Genomes' programme for humans. And there's all the rest of the quantifiable information about life on Earth \u2014 data on protein structure and function, biomolecular interactions, signalling and metabolic pathways, and much more. The challenge is to make sense of the deluge. Teams of scientist-annotators at the data repositories make valiant efforts to keep up, and bioinformatics programmers devise increasingly sophisticated annotation algorithms to help. Scientists write review articles and textbooks to make sense of it all. But it's still not enough. Hence the proliferation of wikis, which have the potential to vastly multiply the number of annotators and bring in the most interested expertise: \"The best people to do annotation are the researchers in the laboratories, the people who are producing this knowledge in the first place,\" says bioinformatician Barend Mons at the University of Rotterdam in the Netherlands. Mons is one of the prime movers behind WikiProfessional Life Sciences, a site that links publications on a given topic and enables users to add their own annotations. But will the bench scientists participate? \"This business of trying to capture data from the community has been around ever since there have been biological databases,\" says Ewan Birney of the European Bioinformatics Institute in Hinxton, UK. And the efforts always seem to fizzle out. Founders enthusiastically put up a lot of information on the site, but the 'community' \u2014 either too busy or too secretive to cooperate \u2014 never materializes. So how do the wiki proponents know that this time around will be different? They don't. \"This is an experiment,\" says Pico, echoing just about everyone in the wiki movement. He is optimistic, however. This June he attended a workshop at the University of California, San Diego, on new communication channels in biology. \"Many of the people had come to this from prior attempts,\" he says, \"and were very sober about the challenges.\" From ensuring usability to ensuring users, these challenges go beyond the technical. As the developers of WikiPathways and several others have found, a truly cooperative web-based community requires a change in thinking \u2014 a shift in the way scientists work and in the way they get credit for that work.  \n                Take but no give \n              Conklin's original idea for software to help biologists visualize and draw pathways grew from his research exploring how hormones and their receptors direct tissue development. Pathway diagrams are flow-chart representations of the interactions between genes, proteins or metabolites involved in a particular cellular function, such as the response to an external signal. They enable researchers to interpret the biochemical functions of individual molecules in the broader cell-biological context. One protein might have a very limited function, marking another protein for destruction, for example. But seeing its place in a pathway gives a clue to the physiological significance of that tiny action and offers clues to the functions of similar-looking proteins. Better still, says Conklin, pathways help make sense of DNA microarray data on gene expression. If administering a drug enhances the expression of a set of genes all involved in the same pathway, say one causing cell death, then that's an important clue to what is going on. So, back in 1999, Conklin's lab began to develop software that would make it easier to visualize and modify cellular signalling pathways. Known as the Gene Map Annotator and Pathway Profiler (GenMAPP), it offered free, downloadable software that could turn a database of interactions into a pathway diagram, and also enabled the user to add a new entry to the database simply by sketching in a new reaction. GenMAPP also offered the capability to match microarray gene-expression data against an extensive library of known pathways and identify the most likely matches. To get the library started, says Conklin, \"I went to Amazon.com and bought $900 worth of textbooks. Then my students and I flipped through and redrew the pathways we found there by hand, making electronic versions.\" They figured the tedium was worth it. Their library would grow fast, as soon as researchers who downloaded the drawing tool began uploading their own pathways. But the team was overly optimistic. The GenMAPP drawing tool proved popular, and in the nine years since the launch, it's been downloaded 17,000 times. But when it came to giving back to the library \u2014 the rate wasn't so great. Only about 30% of the 557 pathways in the current GenMAPP library have come from outside the developers' own labs. There were some enthusiasts. The group run by Chris Evelo, head of the department of bioinformatics at the University of Maastricht in the Netherlands, was such an active contributor that it became a formal collaborator on GenMAPP in 2003. \"But it was frustratingly slow,\" says Conklin. \"We'd see publications with pathways created using our software, but half the time people wouldn't submit them back to us.\"  \n                Make it easy \n              Two things broke the impasse, says Conklin. In 2005, the lab was approached by the developers of Cytoscape, an open-source software platform for very powerful, very high-end network analysis, much used in systems biology. They liked GenMAPP's layout, with its easy-to-use sketching capability, which they wanted to incorporate into Cytoscape, where the pathway drawings were abstract and mathematically elegant, but hard for the uninitiated to understand. Conklin and his group were happy to oblige. \"Cytoscape turned out to be supported by a very robust open-source community, which we didn't have,\" says Conklin. \"Here were people coming out of the walls, offering us all kinds of software solutions.\" The GenMAPP team became active participants \u2014 Conklin now sits on the Cytoscape board \u2014 and soon decided to revamp their own drawing tool entirely; the next GenMAPP release, due out in 2009, will essentially be a slightly specialized version of Cytoscape. That involvement led to the second innovation, says Pico. \"The Cytoscape team was using a wiki to coordinate their work,\" he says. \"And that was my first experience with the idea.\" So he decided to install a wiki in Conklin's lab for internal use. \"These were mostly wet-lab biologists, and what impressed me was that even the least technically inclined people in the group picked it right up,\" says Pico. \"Even biologists who would never add to a website would add to the wiki \u2014 it was easy and fun.\"  \n                Sketching the idea \n              So the next big idea was almost inevitable \u2014 a public wiki interface for GenMAPP to make it easier for researchers to contribute their new pathways. As inspiration hit on that January day, Pico sketched out his idea. It would need an online version of the GenMAPP drawing tool, instead of a separate piece of software to download, and a one-click submission of a finished pathway to the library instead of a separate uploading process. When he e-mailed Evelo with the idea, two of Evelo's graduate students, Martijn van Iersel and Thomas Kelder, replied. Surprise \u2014 they'd had the same idea. Kelder and van Iersel in Maastricht and Pico and Kristina Hanspers at the University of San Francisco became the design group for WikiPathways. A top priority was to make the site very easy for bench scientists to use. Like most of the other wiki-inspired biosites, WikiPathways does this by using the open-source MediaWiki software that underlies Wiki-pedia. As EcoliWiki creator James Hu of Texas A&M University in College Station puts it, \"We didn't want to ask young scientists who were already editing Wikipedia to learn a new interface.\" The WikiPathways team did need tools not available on Wikipedia itself. \"We completely gutted the MediaWiki text-editing functionality and replaced it with new applets that would represent pathway information graphically,\" says Pico. The diagram is linked behind the scenes to a structured database of biochemical interactions, he says, but the goal is to make drawing the pathway on screen as easy as drawing it on a napkin. \"And then once you're done, it's immediately available to you \u2014 or to the world. You can e-mail the link and do collaborative editing with biologists globally, which is impossible with GenMAPP or any other tool that's on your personal machine,\" says Pico. \"The wiki can put all of you on the same drawing board.\" A prototype WikiPathways was up and running by spring 2007. By autumn the team felt confident enough to promote the site more widely. And in January 2008 they got their first pathway contributed by a researcher they didn't know directly. \"I consider that the birthday,\" says Pico. By mid-summer 2008, WikiPathways had some 350 registered users, of whom 50 or so had made changes to at least one pathway. \"It's already more contributors than we'd gotten over the past nine years,\" says Pico. And for several weeks after July 2008, when they published a description of WikiPathways in the journal  PLoS Biology  (see  A. R. Pico  et al .  PLoS Biol.   6,  e184; 2008 ), the average of one new pathway contributed per month jumped to a new pathway every other day. The hope is that at some point soon, says Pico, \"we'll reach a tipping point, a critical mass, where people from areas of biology we know nothing about will start participating in the whole cycle of revision and correction while involving us less and less \u2014 and it will become self-sustaining\".  \n                Critical mass \n              WikiPathways is a stand-alone site, but a few of the new bio-wiki sites are tapped into Wikipedia directly. Earlier this summer, a team led by Andrew Su at the Genomics Institute of the Novartis Research Foundation in La Jolla, California, launched a software 'robot' that systematically goes through Wikipedia creating or amending entries for every human gene that has been studied to any significant degree \u2014 some 9,000 in all. The result is Gene Wiki: a collection of Wikipedia pages in a standard format, populated with an integrated suite of information culled from the National Center for Biotechnology Information's Entrez Gene, together with links to data repositories and publications, and to Wikipedia's rich resource of pages on diseases and physiology. Gene Wiki entries are already showing up on the first page of Google search results for particular genes, says Su. \"And our hope is that some number of readers will actually stay to make an edit,\" he says. \"It could be as trivial as fixing a typo, or as substantive as summarizing a new paper in the literature. But it will start a positive feedback loop by making the page that much more useful.\" Building critical mass \u2014 that's the real challenge in the wiki game, as everyone is acutely aware. It's also a mysterious process that requires timing and luck just as much as skill. Wikipedia, for example, didn't become the largest collaborative site on the planet by being the first. That honour goes to a programmers' idea exchange called WikiWikiWeb, which was developed in 1995 by American programmer Ward Cunningham. (He named it 'wiki' after the Hawaiian word for 'quick.') But Wikipedia, founded in 2001 by developers Jimmy Wales and Larry Sanger, was among the first to offer a free service \u2014 knowledge aggregation \u2014 that was useful to essentially any literate person on the planet. It proceeded to grow exponentially, to the point where it now claims more than 10 million articles in more than 250 languages \u2014 roughly a quarter of them in English. Wikipedia has also acquired the classic 'long tail' of contributors, with a comparative handful of people making lots of edits, and a multitude who make only a few. The science wikis face a tougher challenge in building critical mass, if only because they're aiming at a much smaller audience. One obvious strategy is to avoid fragmenting that audience. As Evelo points out, \"biologists aren't going to work on a dozen wikis to see which will survive\". They are going to want the various wikis to be interoperable and mutually supporting, so that the data they enter in one can be easily ported to another \u2014 or will even flow to all the appropriate sites automatically. It should help that so many of the sites are based on the same MediaWiki software. That gives them the potential to act as one big open-source community, sharing code and improvements. And it's not just potential, adds Pico: \"We've been in close contact with Jim Hu and the Ecolihub folks about making our wikis interoperable.\" Ecolihub is the 'parent' website of EcoliWiki, providing access to vast amounts of information on the bacterium  Escherichia coli . Also critical to interoperability will be a standard language that can be understood by all the databases. In the realm of pathways, says Pico, the closest to that right now is BioPAX, an XML-based standard for the exchange of pathway and interaction information. \"We're planning on converting our system to it.\" Interoperability is only part of the equation, however. Few scientists will contribute to these sites out of altruism. They need tangible incentives \u2014 starting with a real benefit to their day-to-day research. Giving them that is definitely a work in progress, says van Iersel. The wiki architecture offers some possibilities. \"For example, you can sign up to be e-mailed whenever a change is made to a page you're interested in,\" he says. So a researcher could immediately be alerted to any new findings in an area he or she is working on, not to mention the existence of potential collaborators (or rivals), without having to wait for a paper to come out. There will always be some hypercompetitive fields in which people will keep their work under wraps for fear of getting scooped. But the hope is that for most researchers, the win\u2013win dynamics of real-time data sharing will prevail. \"Community annotation supports the natural process in which people form intellectual networks around topics,\" says Mons. \"The system tells me, 'Hey \u2014 if you're interested in ABC, you'd better look at XYZ, as well.' And that will become part of the workflow of a scientist's life.\"  \n                Due credit \n              Academic culture being what it is, however, the wiki sites will have to crack the credit-assignment problem, and provide some way for scientists' efforts there to be identified, recognized, cited and shown to funding agencies and tenure committees. Without a solution, says Hu, wiki-based community annotation will get nowhere. \"Everybody gets excited by the idea,\" he says, \"but then it always falls off the table, because it's not one of the things that pays the rent.\" Pico couldn't agree more. At the San Diego workshop, \"we had break-out sessions, lunches, lots of brainstorming trying to think of metrics for scientists to quantify their activity at these sites,\" he says. Perhaps the most thoroughly worked out demonstration of how credit assignment could function in a wiki context is WikiGenes (not to be confused with Gene Wiki), created by Massachusetts Institute of Technology computer scientist Robert Hoffmann (see  R. Hoffmann  Nature Genet.   40,  1047\u20131051; 2008 ). Like Wikipedia, the WikiGenes site consists of articles that are collaboratively written and edited by the users. Unlike Wikipedia, however, WikiGenes links every piece of text directly to its author. (In principle, a user could find that information on Wikipedia by tracking back through every previous version of an article, but in practice this rapidly becomes unworkable.) A single click leads to an automatically constructed page for that author, which lists all his or her contributions. Registered users have the option to do a one-click rating of each contribution, thus providing a fine-grained community peer review. For such non-traditional measures of merit to be accepted by promotion and tenure committees, or by the wider community, will require a substantial shift in academic culture. But, says Hu, \"cultural factors are not immutable. If we can promote various incremental changes, then eventually this will take off.\" In the meantime, the wikis still have a lot of challenges to face \u2014 not least the need to prove to funding agencies that they are worthy of long-term support. And that is why it is all very much still an experiment. \"Community intelligence is a new concept for biology \u2014 and in broader society \u2014 and we certainly don't claim to have the final answer,\" says Su. Still, the more mechanisms for harnessing community intelligence, the better: \"The community will essentially vote on which model will be the most useful, and the beauty is that they will vote with their participation,\" he says. \"The only question is which model will resonate best.\" Mitch Waldrop is Editorials and Features editor for  Nature . See also Editorial,  page 1 \n                     Big data special \n                   \n                     Cytoscape \n                   \n                     EcoliWiki \n                   \n                     Gene Wiki \n                   \n                     OpenWetWare \n                   \n                     PDBWiki \n                   \n                     Proteopedia \n                   \n                     Topsan \n                   \n                     WikiGenes \n                   \n                     WikiPathways \n                   \n                     Wiki Professional Life Sciences \n                   Reprints and Permissions"},
{"file_id": "455160a", "url": "https://www.nature.com/articles/455160a", "year": 2008, "authors": [{"name": "Helen Pearson"}], "parsed_as_year": "2006_or_before", "body": "Proteins with 'zinc fingers' designed to bind almost any DNA sequence will soon be available to any lab that wants them \u2014 from two very different sources. Helen Pearson reports on a revolution in designer biology. \"There is only one word that matters in biology,\" says pioneering molecular biologist Aaron Klug, \"and that is specificity. The truth is in the details, not the broad sweeps.\" This neatly explains the importance of proteins equipped with structures called zinc fingers, on which Klug's team at the MRC Laboratory of Molecular Biology in Cambridge, UK, did pioneering work in the 1980s and 90s. The human body contains more than 700 different zinc-finger proteins, which bind to specific DNA sequences to switch genes on and off. For almost 20 years, would-be protein engineers have dreamed of taking the DNA-recognition ability of these zinc fingers and making it universal \u2014 of designing zinc-finger proteins targeted at any DNA sequence that catches their fancy. The world is about to reap the benefits of a decade of hard work realizing those dreams. From the middle of September, designer zinc-finger proteins will be available to anyone with an idea, an Internet connection and US$25,000. The reagents company Sigma-Aldrich, based in St Louis, Missouri, has a splashy launch planned for CompoZr, a service that will provide its customers with zinc-finger proteins aimed at whatever DNA sequences they want. The service uses techniques developed by Sangamo Biosciences of Richmond, California. Zinc-finger technology has previously been the preserve of a select few, mostly working with Sangamo; with Sigma's cut-and-paste offering, Sangamo hopes to make zinc fingers far more widely used in commercial research and in academia. The specificity zinc fingers offer could find a million uses in the lab, revolutionizing the techniques researchers use to work out what genes do. This summer two studies, one from Sangamo 1  and one from Scot Wolfe's lab 2  at the University of Massachusetts, Worcester, showed that bespoke zinc-finger proteins that can cut the DNA they recognize \u2014 zinc-finger nucleases \u2014 could, in principle, knock out any gene in zebrafish. This is something that researchers working with most model organisms have so far been unable to do with other methods. In an accompanying article 3 , Ian Woods and Alexander Schier of Harvard University wrote that such tools might become \"the major technology for genome manipulation\". \"It could have a monstrous impact,\" says Wolfe. The technology could also prove valuable in the clinic. Monoclonal antibodies, a previous breakthrough in biological specificity, went through a phase of being just lab tools. Now they are the basis of a therapeutic market worth over $20 billion, and treat tens of thousands of patients annually. Sangamo hopes that therapies that use zinc fingers to turn genes on and off \u2014 or indeed to edit them \u2014 have similar potential. Earlier this year, in an illustration of what might be possible, the company built a zinc-finger protein that disables a protein by means of which HIV gains access to human cells. When applied in mice, the zinc-finger protein locked the virus out 4 . Exploiting that sort of capability, Sangamo plans first to open up and then dominate a whole new class of therapeutics. On the wall of his office looking towards San Francisco Bay, chief executive Edward Lanphier has framed copies of inspirational headlines from biotech history \u2014 such as the  San Francisco Examiner 's 1980 \"Genentech Jolts Wall St\", commemorating the stock-market debut of the company on the other side of the bay that has done as much as any to capitalize on the specificity of antibodies. Asked whether he entertains such visions, though, Lanphier demurs: \"My hubris is significant, but not that significant.\" The CompoZr offering is intended in part to appease researchers who have expressed frustration at not being able to access Sangamo's proprietary technology. But following developments a couple of months ago, those academics may end up spoilt for choice. In July, a powerful new methodology for do-it-yourself zinc-finger design was published in  Molecular Cell  by a consortium of scientists led by Keith Joung of the Massachusetts General Hospital in Boston 5 . Joung hopes the consortium's protocol \u2014 the culmination of ten year's work on his part \u2014 will allow any lab, anywhere, to make zinc-finger proteins at a fraction of Sigma's price.  \n                Two ways forward \n              Joung says that he and the consortium are not seeking to undercut Sangamo, but rather to expand the zinc-finger universe. Still, some will inevitably see the two as rivals \u2014 or even as a new round in the struggle between academics devoted to open systems and companies built on the defence of intellectual property. \"It's pretty unusual to have an academic research group as committed to an open platform in clear opposition to a firm,\" says Arti Rai, an expert in patent law and the biopharmaceutical industry at Duke University in Durham, North Carolina, who has studied Sangamo. Michael Eisen, at the Lawrence Berkeley National Laboratory in California, is blunter: \"It's nice there's a reservoir of rebellion and people saying 'screw it, we're not going to be held back'.\" Although the first zinc-finger protein was discovered in 1985 (ref.  6 ), the protein-engineering possibilities only really came to the fore in a May 1991 paper in  Science 7  by Carl Pabo and Nikola Pavletich, then both at Johns Hopkins University in Baltimore, Maryland. This paper revealed the X-ray structure of three zinc-finger domains bound to a piece of DNA. The zinc fingers \u2014 each a chain of 30 amino acids folded back on itself and stabilized by a zinc ion \u2014 nestle in the major groove of the DNA molecule, touching three of the base pairs that provide the DNA with its sequence. The fingers lie one after the other in the groove, with three fingers recognizing a sequence of nine bases overall. \"Everyone who saw that structure had the same thought,\" says Joung. \"It might make a very nice scaffold for making designer DNA-binding proteins.\" Understand the relationship between the amino acids and the bases and you might design a protein that recognized any sequence of bases you chose, threading zinc fingers together as simply as beads on a string. Lanphier, though, saw more than an engineering opportunity in the fingers. He saw a business break. Lanphier had spent his career in the business and strategic side of the pharmaceutical and biotechnology industries. In the 1990s he was head of commercial development at a California company called Somatix Therapy, which was developing the vectors that deliver genes into tissues for gene therapy. But although the company controlled intellectual property (IP) surrounding its vectors, others owned the genes that the company might want to put into them. \"We were frustrated with the fact that we couldn't access proprietary genes,\" Lanphier says. Zinc-finger proteins offered the opportunity to create those genes \u2014 newly written genes to describe newly imagined proteins. \"If you have a motif that can be engineered, by definition those different proteins will be encoded by different genes,\" Lanphier says. \"So you might have a technology platform here to generate an infinite number of genes.\" One of those designer genes, delivered to a cell, would make a zinc-finger protein that could control one of the cell's naturally occurring genes. With that in mind, Lanphier flew out to talk to some of the leading academic groups in the field, including Pabo's and Klug's. Bolstered by those meetings and half a million dollars in venture capital, Lanphier started Sangamo in 1995. He hardly looks the part of a cut-throat monopolist as he pads around his office in hiking socks and Birkenstocks, but he went about developing the company with a fierce focus: he wanted an unassailable IP position. \"By a very aggressive, creative and smart licensing strategy they've swept up most of the IP,\" Rai says. \"It's unusual to have this vision.\" Both inside and outside Sangamo, early attempts to engineer zinc-finger proteins were based on two attractive and sometimes complementary ideas. One was the  a priori  approach: work out the rules specifying which amino acids direct a finger to which of DNA's 'letters' (the bases A, G, C and T) and rationally design new fingers. The other was the 'look-for-what-works' approach: design or try to identify zinc-finger domains that bind to each of the 64 possible three-letter sequences, and then mix and match them like Lego blocks to fit whichever base sequence is wanted. If only the science of life were so simple. Researchers soon realized that there was no simple code, and that 'modular assembly' was not as easy as it looked. For one thing, the sequences bound by a series of zinc fingers turned out not to be completely distinct from each other but instead overlapped: the same DNA triplet can be touched by amino acids from more than one finger (see 'In the groove'). Almost everyone in the field came to believe that each finger had to be chosen in the context of the fingers next door. \"You have to take into account the overlap or the failure rate is very high,\" says Mark Isalan of the EMBL/CRG Systems Biology Research Unit in Barcelona, Spain, who worked in Klug's lab and devised a way to design proteins that took context into account 8 . He took the technology forward at Gendaq, a company co-founded by Klug. In April 2000 \u2014 amid a frenzy of anticipation over the human genome sequence \u2014 Sangamo took its stock public, raising $50 million in a day when the exuberant market valued the company at $375 million. In 2001, it bought Gendaq, and Isalan's approach has formed the basis of the company's protein building ever since. Flush with new money and science, Sangamo started to hit its stride.  \n                Mix and match \n              Joung first got to grips with zinc fingers in 1998, when he joined Pabo's lab at the Massachusetts Institute of Technology (MIT) in Cambridge as a postdoc inspired by the idea of designing proteins  de novo . Then \u2014 as now \u2014 he set about the task by a process of selection: \"evolution in a tube\", he says. Joung builds libraries of zinc-finger combinations, gets them mass produced in bacteria, and then identifies those that are best at binding a specific sequence. The strength of this approach is that if you start off with combinations of zinc fingers, rather than looking at them one at a time, the issue of context gets dealt with. The problem is that to cover all the possible combinations of key amino acids in a three-finger protein you would have to make 8 \u00d7 10 24  different proteins. Joung calculates that working with such a library in bacteria would require an agar plate around 100 times the size of North America. By the end of his postdoc his library-building and selection method worked \u2014 but only for finding a single finger, not a set 9 . In September 2001, Pabo told his lab that he was moving to California to work for Sangamo. Joung says there was some discussion of him moving to Sangamo, too \u2014 the company had assessed his technology and licensed it, although it didn't end up using it \u2014 but he wanted an academic position, and one became available at Massachusetts General Hospital. Isalan, as it happens, made a similar decision: \"[Sangamo] offered me a job but I'm really a scientist, and by that stage it was a production line,\" he says. In the development of that production line, the company has built an extensive library of mainly two-finger proteins, designed so as to take into account interactions between neighbours. When the company's scientists want to design a protein for a new target gene, they feed the gene sequence into a computer program, which tells them various ways to piece together two-finger modules into a four-finger protein that might serve, and predicts how well each option will work. A robot then assembles the DNA for the new zinc fingers, picking out genetic fragments encoding the various modules from a standardized set of 384-well plates. \"We've spent a lot of time and energy developing this kit of pieces, information about how these pieces function and then the software that knows best how to use them,\" says Jeff Miller, a one-time lab-mate of Joung's who made the transition from MIT to Sangamo with Pabo. And the amount of time and energy Miller and his colleagues spent on honing their approach, not to mention the $230 million the company spent on this and other R&D, is far beyond what an academic group could hope to repeat. Another company might, but although no one can really know the strength of an IP position until it is tested in court, Sangamo's seems to have been strong enough to scare off most competition. It's possible that this is good for the company but bad for the field. Richard Jefferson, founder of CAMBIA, a non-profit organization based in Canberra, Australia, that supports open access to life-sciences technology, claims that if a company had pursued monoclonal antibody or DNA sequencing IP as diligently as Sangamo has pursued zinc-finger IP \"it would have set [those fields] back a long way\". Lanphier rejects the idea: \"On the contrary, I would argue that our patents have allowed us to access the funding that we have used to advance zinc-finger technology.\" Sangamo has not just been amassing IP and developing its design processes: it has also started showing what zinc fingers might do. Much of the recent excitement in the field has focused on the zinc-finger nucleases, originally discovered in 1996 (ref. [10]). In 2005, working with Matthew Porteus of the University of Texas Southwestern Medical Center in Dallas, scientists at Sangamo showed that a zinc-finger nuclease aimed at a mutation in a gene called  IL2R\u03b3  erased the mutation in over 18% of cells, converting the gene into one that worked properly[11]. The mutation in question causes X-linked severe combined immune deficiency (SCID), a fatal genetic disease; the results looked all the more promising because gene-therapy trials for SCID a few years previously \u2014 in which a working copy of the gene was introduced into cells \u2014 had been successful in compensating for the defect but resulted in several children developing leukaemia. \"People were very excited,\" Porteus says. The efficiency was good enough that if it could be made to work  in vivo  it might actually cure people.  \n                Spreading the message \n              Sangamo has been racking up further scientific and commercial testaments to the potential applications of mutation-correcting zinc-finger nucleases. Plant biologists are drooling with delight over the enzymes, because they should enable precise genetic modifications to be made in plants to genes that have been impossible to target precisely with conventional techniques \u2014 opening up new possibilities for adding to, knocking out or overwriting genes associated with yield, nutritional content, pest resistance and other useful traits. The company has an agreement with Dow Agrosciences, based in Indianapolis, Indiana, to develop the technology for plants. \"It's revolutionizing plant breeding \u2014 that's not too strong a word for it,\" says Klug, who serves on Sangamo's scientific advisory board. Together with drug giant Pfizer and Genentech, Sangamo is also engineering cell lines to improve their protein productivity. And the company has an agreement with Genentech's corporate parent Roche for the creation of transgenic cells and animals. But as Sangamo went from strength to strength, progress in the field as a whole was not so inspiring. Only a handful of groups, such as Joung's and Wolfe's, were able to make proteins using their own methods. \"Many people have got excited by the technology and try it and it doesn't work and they give up,\" Isalan says. \"It's been a real problem.\" Sangamo thus became a target of envy. \"I think there has been some frustration,\" says Porteus, who no longer collaborates with the company. \"They publish and present at conferences as if they've invented sliced bread, but they are very closed about helping anyone else do it.\" Joung \u2014 who, were he not devoted to the lab, would make a first-rate diplomat \u2014 is less critical: \"I think Sangamo has done a really amazing job of moving the field forward.\" The problem he saw was not that Sangamo had too much technical ability \u2014 it was that the rest of the world had too little. \"For many years I'd felt academics needed to develop a technology of their own,\" he says. \"Sangamo was putting so much time and resources into this, there was no way that one lab was going to be able to keep up with them, so if we were to stay relevant to the field we had to band together.\" In the summer of 2005, Joung got in touch with another zinc-finger veteran, Dan Voytas, then at Iowa State University. In 2000, Voytas had started a small company called Phytodyne to develop zinc-finger nucleases for plants. Voytas says that the company folded in 2004 after failing to come to a deal with Sangamo over access to the company's proteins. \"We were bumped out of business. Their pockets had more cash in them than ours.\" (Sangamo says it would have loved to collaborate, but Phytodyne's lack of funding made it too unstable to partner with.) The zinc-finger consortium that grew out of a dinner Voytas and Joung had later that year now has 14 member labs, including many big names in the field, but most of its protein engineering has been done in Joung's lab. Joung's current technique uses a battery of small libraries, or what he calls 'pools', that each contains just less than 100 proteins selected to bind a target sequence. One pool, for example, is full of protein domains that bind well to the sequence GAA, but only when they are the first finger in a three-finger protein. Another pool contains domains that bind to GAA when they are the second finger, and so on. To create a three-finger protein that binds at a specific sequence, three pools are combined and subjected to a second round of selection, from which the best-binding proteins are pulled out. \"This is an advanced form of mix and match,\" Joung says. Sixty-eight of 192 possible pools now sit behind the ice-caked door of a \u221280 \u00b0C freezer off his lab. Joung doesn't have a robot \u2014 but two years ago he hired a young technician called Morgan Maeder and she has been spending near-robotic hours honing the technique. The first time she showed that the pools had generated a zinc-finger nuclease that corrected a gene in human cells \"was the most exciting thing ever\", she says. \"Even Keith was giddy; he said 'give me a high five' and we were jumping up and down, it was so exciting. For me it's two years' worth of work \u2014 for him it's a whole career.\" On 24 July, the consortium published Joung's methods for zinc-finger protein engineering 5 , along with work from other members of the consortium showing that the proteins accurately cut genes in human and plant cells. They named the technique OPEN, for Oligomerized Pool Engineering. Joung is now arranging to make the pools available to other labs; he estimates the set will cost around $5,000. But the technique is hardly workaday. Maeder says her protocol runs to 20 pages. Porteus says he would have liked to be a fly on the wall in Richmond when Lanphier's crew first saw the OPEN paper. Whatever such a fly might have heard, though, in public Sangamo has no problem. The company is much more focused on its first clinical results. Later this year, phase II trials will reveal whether a zinc-finger protein designed to activate a growth factor called VEGFA has promoted the recovery of damaged nerves in diabetics: the first chance for zinc fingers to show beneficial effects in humans. And in the next few months the company also plans to file two investigational new drug applications with the US Food and Drug Administration to test two zinc-finger nucleases \u2014 including one that disrupts the HIV receptor CCR5 \u2014 in humans for the first time. Philip Gregory, Sangamo's vice-president of research, does not think that OPEN will be much competition for the click-and-go convenience of ordering from Sigma. \"If you're a biologist and want to knock out a gene you might not want to set up as a zinc-finger lab to answer that one question. So with labs less fiscally constrained the advantage of working with a quicker product, and what's probably a better product anyway, will be convincing.\" But convenience does not always win out. Eisen sees parallels with the stand-off in the 1990s between Affymetrix of San Jose, California, and academics, including Eisen, who thought the cost of the company's DNA chips was restricting research and published their own system for manufacturing microarrays that did the same thing. \"Just the mindset \u2014 the transition from thinking it's inaccessible to something people could use \u2014 was important,\" he says. \"It allowed arrays to be integrated into the fabric of research in a way that they wouldn't have if they had remained a commercial product only, and I can see exactly the same here.\" An integrated, improvable, hands-on experience might give OPEN an edge in some academic settings. But that could suit Sangamo fine. Success for the consortium will increase the total amount of zinc-finger research, and that may matter more to Sangamo than what reagents are being used. Lanphier says that one of the original motivations for the CompoZr service came from Sangamo's management asking themselves why biotech companies focused on RNA interference were gaining value while theirs wasn't. They concluded that RNAi was a hot technology in part because it was widespread. \"RNAi was ubiquitously available and easy to use, high-school kids were using it,\" recalls Lanphier. \"So we said: 'how can we get this in the hands of every lab in the country or the world that wants to use it?'.\" The more zinc-finger research is done, in this analysis, the more exciting Sangamo looks. And the more commercial possibilities it might have. Sangamo is confident that when that research gets to the stage of generating products, its hard-bought IP position will win through. \"Ultimately, from a commercialization perspective, you'd run into Sangamo's IP at some point,\" says Gregory. If Sangamo's patent position is as strong as the company claims, then \"all the consortium is doing is loading up Sangamo's pipeline\", says Richard Jefferson. Since the OPEN paper came out, Joung says he has had some 15 requests for the pools. Now that researchers have the proteins in their hands, he says, they can start to tackle all kinds of other pressing questions, such as how to deliver them efficiently to cells, and whether they might make unwanted cuts in the genome. \"Unless you were willing to use [Sangamo's] proteins there was nothing to work with \u2014 now we can ask those questions,\" says Joung. But for all the possible applications, his personal fascination remains the motif itself. \"I look at the sequences as a way to see that they worked, but I don't really care what they are,\" says Maeder. \"But he loves it. Every time it works, it's like further proof that the past ten years of his career have been worthwhile.\" Joung and some of his colleagues still think it's possible that there is some kind of code \u2014 an algorithm for writing the best amino-acid sequence with which to target a piece of DNA from first principles. It's even conceivable that the tools with which to crack that code are sitting in the databases that are now filling up with successful designs, awaiting release through bioinformatic wizardry. Still, complex though it is, there's something satisfying about the current process \u2014 at least to Joung. \"You start with 200 million different things and you end up with this very small number that are very similar,\" he says. \"It's exciting and gratifying to me that the system actually works and that you don't always get the same finger depending on which context it was selected in. I had the thought nearly ten years ago, so yes, it's pretty cool to see it come out this way.\" To come out in all its glorious specificity \u2014 which is what matters in biology. There is a  Correction  associated with this article: the name of Matthew Porteus of the University of Texas was mispelled. This has now been corrected online. \n                     Biotechnology@nature.com \n                   \n                     Sangamo BioSciences \n                   \n                     Zinc Finger Consortium \n                   \n                     Keith Joung \n                   Reprints and Permissions"},
{"file_id": "455156a", "url": "https://www.nature.com/articles/455156a", "year": 2008, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "The Large Hadron Collider is the latest attempt to move fundamental physics past the frustratingly successful 'standard model'. But it is not the only way to do it. Geoff Brumfiel surveys the contenders attempting to capture the prize before the collider gets up to speed. It is powerful; it is galling; it is doomed. The incredibly successful mathematical machine that physicists call the 'standard model' is a set of equations that describes every known form of matter, from individual atoms to the farthest galaxies. It describes three of the four fundamental forces in nature: the strong, weak and electromagnetic interactions. It predicts the outcome of one experiment after another with unprecedented accuracy. And yet, as powerful as it is, the standard model is far from perfect. Its mathematical structure is arbitrary. It is littered with numerical constants that seem equally ad hoc. And perhaps most disturbingly, it has resisted every attempt to incorporate the last fundamental force: gravity. So physicists have been trying to get beyond the standard model ever since it was put together in the 1970s. In effect, they will have to shatter the model with experimental data that contradict its near-perfect equations. And then, from its fragments, they must build a newer, better theory. The Large Hadron Collider (LHC), a giant particle accelerator at CERN, Europe's particle-physics laboratory near Geneva, Switzerland, is the latest attempt to break the standard model \u2014 and one that many see as all but assured of success. The prodigious energy it generates will force particles into realms where the standard model cannot follow. In the race to move beyond the status quo, \"the LHC is by far the favourite\", says Frank Wilczek, a theorist at the Massachusetts Institute of Technology in Cambridge who won the 2004 Nobel Prize in Physics for his work underpinning the standard model. But the LHC is not the only game in town. For decades physicists have tried to get beyond the standard model in all sorts of ways, sometimes with accelerators, sometimes with precision measurements of breathtakingly rare events, sometimes with observation of outer space. And in the time it takes for the LHC to get fully up to speed \u2014 its first results aren't expected until at least next summer (see  'The unstoppable collider' ) \u2014 some of those experimental groups think that they have a fighting chance of seizing the prize first. Their task will be hard: the standard model is a formidable piece of work that has resisted all the easy and obvious attacks. To crack it, experiments will need unprecedented sensitivity, a multitude of data, and more than a little luck. Here's a rundown of the heroic few who feel up to the task.  \n                Tevatron \n             While the LHC gets its protons up to speed, the world's other heavyweight particle-accelerator is racing to break the standard model first. Since 2001, the Tevatron, located at Fermilab in Batavia, Illinois, has been accelerating protons and antiprotons at an energy of around 1 tera electron volt. That's only a seventh of the eventual top energy of the LHC, but total energy isn't everything in the hunt for new physics. Collisions that would generate new particles outside the standard model are extremely rare, which means that the longer an accelerator runs and the more data it accumulates, the better its chances of finding something. So for a while, at least, the Tevatron will continue to have a data lead over the LHC. Even by the summer of 2009, the Tevatron will have several times more total data than its new competitor. And already those data are showing some tantalizing, if tentative, hints of something beyond the standard model. One deviation comes in measurements of a particle known as the strange B (B s ) meson. The B s  is made of a strange quark and an anti-bottom quark, and it is among the heaviest of all mesons. Under a rule known as charge-parity symmetry, the standard model predicts the B s  will decay in the same way as its antiparticle (made of an anti-strange and a bottom quark). But measurements of the two are hinting at a difference in their decays. According to Dmitri Denisov, a spokesperson for the D-Zero experiment at the Tevatron, that difference could be an important clue in the quest for discoveries. It might signal the existence of new, exotic particles, or of previously unknown principles. In any case, says Denisov, \"it's an exciting measurement\". The B s  anomaly is not the only oddity showing up at the accelerator, adds Robert Roser, a spokesperson for the Tevatron's other major experiment, the Collision Detector at Fermilab, or CDF. An unusual feature in the decays of pairs of top and anti-top quarks has him intrigued. Again, he admits, it's far from certain. But some of these signals may turn out to be important, Roser says. \"As you add data, one of [these anomalies] may become real.\" Perhaps not surprisingly, a more sceptical view comes from John Ellis, a theorist at CERN. Yes, the Tevatron could provide some tantalizing hints, says Ellis, but it is unlikely to make a definitive find before the LHC comes on strong. In the world of particle physics, he points out, nothing constitutes a discovery until it is measured to five \u03c3 (five standard deviations from the mean), the equivalent of 99.99994267% accuracy. Much more data than the Tevatron has accumulated so far will be needed to reach that exacting standard, and the detector is unlikely to make those big gains before it is overtaken by its new rival. \"I think its going to be very, very tough for the Tevatron,\" Ellis says. \"I just don't see them getting it before the LHC starts going gangbusters.\"  \n                Cosmos \n             While the high-energy physicists gather in their machine's control rooms, another group of physicists is looking to the heavens. There they hope to find something that shatters the standard model \u2014 if the Universe cooperates. The main thing that their spacecraft will look for are indications of dark matter, the ghostly substance that could make up as much as 85% of the matter in the Universe. Astronomers know that dark matter exists only because of its gravitational pull on galaxies and its influence on the Universe's shape; it seems to pass right through the kind of ordinary matter found in stars, planets and people. Presumably, dark matter is a haze of particles that rarely, if ever, react with the ordinary variety. But nobody is quite sure what those particles might be \u2014 except that they are not accounted for in the standard model. One candidate comes from the 'supersymmetry' theory, which predicts that every particle in the standard model has another, heavier partner lying outside the model. The lightest of these supersymmetric partners is called the neutralino, and is predicted to have just the right properties to be dark matter. Neutralinos themselves wouldn't be seen by telescopes, orbiting or otherwise. But periodically, two neutralinos could collide and annihilate \u2014 creating a shower of more mundane particles that orbiting detectors might pick up. The PAMELA (Payload for Antimatter Matter Exploration and Light-nuclei Astrophysics) experiment has already seen an intriguing clue. The satellite-borne instrument has unofficially reported a surplus of anti-electrons that may have been generated by dark-matter annihilations (see  Nature  454,  808;  2008). \"It's a beautiful result,\" says Graciela Gelmini, a physicist at the University of California, Los Angeles, who has seen PAMELA's data. Still, she adds, the complexities of the measurement require caution. A second, recently launched satellite may also be able to spot the untimely demise of the neutralino. The Fermi Gamma-ray Space Telescope is a US$690-million space instrument designed to scan the entire sky for ultra-high-energy photons. It is possible that such \u03b3-rays could be created by neutralino collisions, in which case they would show up as a ubiquitous haze in the orbiting detector's sky-map. \"That would be a stunning, stunning signature,\" says Steven Ritz, the telescope's project scientist at NASA's Goddard Space Flight Centre in Greenbelt, Maryland. Such signatures, if they're spotted and confirmed in time, definitely have a chance to beat the LHC in the quest to break the standard model, says Michael Turner, a cosmologist at the University of Chicago in Illinois. But Ritz points out that although astrophysics could technically be the first to make such a discovery, they can't do much more than that. Anti-electrons, \u03b3-rays and other such signatures could provide physicists with only a rough mass range for the new particles, and would say nothing about how supersymmetry might work. For those reasons \"there would still be a large number of essential question marks\", says Ritz \u2014 questions that would have to be resolved at the LHC.  \n                The dark \n             Other physicists have chosen darkness over light. From their lairs inside disused mines and traffic tunnels, they are watching a number of highly sensitive detectors that could find direct signatures of dark matter, including supersymmetric neutralinos (see  Nature  448,  240;  2007). There are around half-a-dozen different schemes for such detectors, but they all follow the same basic concept. Take some stuff you think could respond to dark matter, place it deep underground to protect it from cosmic rays and other disruptive influences, and wait for something to happen. \"It's like watching grass grow,\" says Wilczek. Although they are perhaps not the most exciting way to beat the LHC, these detectors are making impressive progress. One experiment, the Cryogenic Dark Matter Search II, or CDMS II, is currently accumulating data in the Soudan Mine deep beneath Minnesota. Its operators aim to treble its current sensitivity by the end of the year. Another experiment called XENON100, located in a tunnel under Italy's Gran Sasso Mountain, stands a chance to have its first results out before the LHC's detectors can finish processing their findings. \"The field is going so fast and there's so much competition, that it's not easy to survive at the moment,\" says Elena Aprile, the principal investigator for XENON100 at Columbia University in New York. \"It's an amazing time.\" And on top of these prospects, one group claims that it has already seen dark matter in its detector. Earlier this year, the DAMA/LIBRA (Dark Matter Large Sodium Iodide Bulk for Rare Processes) experiment, also at the Gran Sasso National Laboratory, announced that it had seen a signal in its latest generation of detector (see  Nature  452,  918;  2008). But their finding has the other groups stumped, says Aprile, whose experiment sits in a vault next to that of DAMA/LIBRA. No one else has yet been able to confirm the signal, and in fact, the findings from other teams seem contradictory, she says. \"We are definitely not consistent.\" Although these detectors seem to be improving in leaps and bounds, they have an Achilles heel: they only work if the so-far unseen dark-matter particles interact, at least occasionally, with regular matter. There's no guarantee that that is the case, says Ellis. And as far as he's concerned, that makes these experiments \"shots in the dark\". Still, Ellis concedes that there is a chance that these esoteric searches might manage to see something before the LHC can. \"I think the dark-matter guys are the jokers in the pack,\" he says.  \n                Neutrino \n             The next few months will be a caffeine-fuelled blur for most of those scientists racing to beat the LHC. But neutrino physicists can take it easy: they've already broken new ground, and they did it a decade ago. Neutrinos are the neutral members of the 'lepton' family of particles, the group that includes the electron. The original version of the standard model predicted that neutrinos should be completely massless, but experimentalists suspected otherwise. For years they saw fewer neutrinos from the Sun than theorists predicted. One possible explanation for the deficit was that solar neutrinos could be switching from one type to another. But that switching would be possible only if neutrinos had mass. In 1998, a Japanese experiment in Hida called Super-Kamiokande saw the neutrino switch in action, and that result is the first \u2014 and to date the only \u2014 firm finding that defies the standard model. Unfortunately, says Ellis, the neutrino's mass can be accommodated within the standard model by making just a few simple modifications to the equations. \"It's possible to add something in relatively easily,\" he says. And consequently, although neutrino physicists can arguably claim the prize, their discovery hasn't helped theorists in their search for new models of physics. But neutrinos may not be finished just yet. Experiments in the United States, Europe and Japan are now firing beams of neutrinos at their detectors to try to learn more about how the neutrinos switch from one kind to another. The precise details of this switching may help narrow the field of possible new theoretical models, says Lisa Randall, a theorist at Harvard University in Cambridge, Massachusetts. And two new detectors could go further still. A European collaboration is now running the Astronomy with a Neutrino Telescope and Abyss Environmental Research (ANTARES) detector under the Mediterranean Sea off the coast of Toulon, France, and a team of Americans is installing IceCube beneath the ice of Antarctica. Both use strings of detectors to see high-energy neutrinos from cosmic sources striking water or ice. ANTARES was completed earlier this summer, whereas IceCube has about half of its 70 strings of detectors installed. But already IceCube is five times more sensitive than Super-Kamiokande, according to Francis Halzen, IceCube's principal investigator at the University of Wisconsin, Madison. \"It's not inconceivable we'll find something,\" he says. Just what that something might be is up for debate. One possibility would be neutrinos produced by dark-matter particles trapped in the Sun's core. But again, Halzen says, anything seen by the neutrino experiments would almost certainly require follow-up by the LHC. \"I think these experiments are complementary,\" he says. \"But if you give me a choice, I'd rather see it first.\"  \n                Success? \n              So can any of these projects best the standard model? Wilczek is sceptical. \"I'm not on the edge of my seat,\" he says. Looking the track record, it seems that, \"the standard model always wins\". He believes that only the LHC stands a real chance of breaking the existing paradigm. And there's no guarantee that even the giant collider will find something new. \"Super symmetry could show up anytime between mid-2009 and never,\" says Ellis. If never is the date, he says, physicists will face \"the maximum conceivable horror scenario\". \"What will we do next?\" he asks. But Turner takes a different view. Ultimately, these experiments and the LHC are fighting the battle together. He is confident that by combining their data with the LHC's, the standard model can be bested, and that new physics will be discovered. \"We're on the verge of a major revolution,\" he says. To read more about the LHC start-up, visit the  Nature News  special at  http://tinyurl.com/5usrfl \n                     LHC Special \n                   \n                     LHC Insight \n                   \n                     CERN \n                   \n                     LHC UK \n                   \n                     Fermilab \n                   Reprints and Permissions"},
{"file_id": "455153a", "url": "https://www.nature.com/articles/455153a", "year": 2008, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "Palaeontologists in Argentina are exploring a trove of fossils that is rewriting evolutionary history. Rex Dalton reports. In the shadow of Cerro C\u00f3ndor, a 600-metre-high limestone bluff in Patagonia, two young palaeontologists gaze over waves of mountain ridges running west towards the Andes. Diego Pol and Ignacio Escapa, from the Egidio Feruglio Palaeontological Museum in Trelew, Argentina, have spent years trekking the winding gravel trails here in the Chubut River valley, meeting only wandering guanacos, rheas and sheep. Already, their team has hand-dug half a dozen quarries in nearby canyons that have yielded globally important fossils. But many prizes remain among the uncharted sediments of the Middle Jurassic, a geological epoch spanning 160 million to 180 million years ago, when dinosaurs, plants and early mammals were all undergoing key evolutionary changes. This time period holds crucial clues to the explosion of evolutionary diversity in both dinosaurs and mammals. The oldest known dinosaur remains, for instance, are around 230 million years old; the oldest known fossil mammals have been dated at 193 million years ago 1 . Both groups diversified to an enormous extent during the Middle Jurassic 2 , yet relatively few sediments of that age have been studied. That makes Chubut province in southern Argentina a rare opportunity. \"This has the potential to be a global landmark for the Middle Jurassic,\" says Pol. \"For the Southern Hemisphere, it already is.\" The Argentine finds may open a little-understood palaeontological window, just as China's rich fossil beds have illuminated the early history of mammals, dinosaurs, reptiles and birds. Chubut is \"an amazing region because you get fairly complete skeletal material, which allows you to answer many evolutionary questions\", says Peter Makovicky, a palaeontologist at the Field Museum in Chicago who has explored much of Argentina. \"The discoveries from the Middle Jurassic of Argentina are no ordinary field finds,\" adds Zhe-Xi Luo, a curator at the Carnegie Museum of Natural History in Pittsburgh, Pennsylvania, who has published on the earliest mammals from China. \"They are of such a significant nature that the whole early mammalian evolutionary paradigm must be changed.\" Within the past decade, for instance, Argentine fossils have helped rewrite conventional wisdom on the evolution of tribosphenic mammals, so named for having molars of a mortar-and-pestle design that can both grind up plant material and shear meat. Palaeontologists had thought that tribosphenic mammals evolved only on the ancient northern land mass of Laurasia, which included what is now Asia, Europe and North America. But in 2001, within sight of Cerro C\u00f3ndor, Pablo Puerta of the Trelew museum unearthed a tiny jaw of a shrew-like tribosphenic mammal,  Asfaltomylos patagonicus 3 . This confirmed that tribosphenic mammals had evolved on the southern supercontinent, Gondwana, before it began splitting from Laurasia about 180 million years ago. Another team had found one southern Middle Jurassic tribosphenic mammal, in 1999 on Madagascar 4  \u2014 but \"the Argentine discovery was overwhelming evidence there were multiple evolutions of the same innovation\", says Luo. More tribosphenic discoveries may await in the preparatory laboratory at Trelew, where Puerta and his colleagues are close to removing the matrix from about 20 small mammalian specimens. These include parts of skulls and skeletons \u2014 rare finds for creatures usually represented by jawbone fossils. \"We need to more fully prepare them to know how many new species we have,\" says Argentine palaeontologist Guillermo Rougier, a mammal specialist at the University of Louisville in Kentucky. \"But we will greatly increase what we know of mammals from that time.\" Another type of Middle Jurassic mammal has emerged from the Cerro C\u00f3ndor quarries \u2014 South America's first example of the proto-mammals known as triconodonts, and dubbed  Argentoconodon fariasorum 5 . In this animal, Rougier and his colleagues see characteristics similar to triconodonts found in North America and Morocco, including teeth like those in modern seals and some other fish-eating mammals. And last year, Rougier and his colleagues also reported the discovery of a mammal they named  Henosferus molus , on the basis of three jawbones. Each bone had a lateral groove, marking where cartilage had attached three or four little bones to the back of the jaw 6 . Such bones, which are typically lost during the fossilization process, are the predecessors of the ear bones of later mammals. Rougier believes  A. patagonicus ,  Ar. fariasorum  and  H. molus  are part of the ancestral lineage leading to monotremes \u2014 animals that, like the platypus, lay eggs like a reptile but nourish their young with milk.  \n                Dinosaur heaven \n             Along with its mammals, the Chubut Valley offers new windows on dinosaur evolution. In 2005, Argentine researchers and colleagues in Germany reported the discovery in Late Jurassic rocks of a short-necked sauropod dinosaur 7 .  Brachytrachelopan mesai  had vertebrae that were shorter in length than those of long-necked sauropods. It had clearly evolved to browse on lower-growing plants \u2014 and its ancestors, deep in the Middle Jurassic sediments of Chubut, may help to explain how and why. Much of the time the palaeontologists aren't sure what treasures they have until they get a block of fossils into the lab at the Trelew museum \u2014 a decade-old building now a centerpiece of the city's historic downtown, in a frontier village that played host a century ago to outlaws Butch Cassidy and the Sundance Kid. Technicians at the museum are currently removing rock from a large pod of fossils dug up last year. The size of a small convertible, the cache weighs several tonnes. Fossils stick out at various points from its plaster jacket. \"We believe it holds a new type of theropod,\" a two-legged, mainly meat-eating dinosaur, says Pol. \"There could be more than one. But we won't know until it is prepared, hopefully by later this year.\" Plant fossils from Chubut are also promising. \"I wouldn't be surprised if the first flowering plants come from Middle Jurassic sediments like those in Chubut,\" says Mark Norell, curator of vertebrate palaeontology at the American Museum of Natural History in New York. Escapa, the palaeobotanist at Trelew, sighs at the thought, saying \"that would be great, but I make no predictions\". For now, Escapa is happy describing types of cypress never identified in the Southern Hemisphere. Fossils of the new species, called  Austrohamia minuta , show incredible plant detail, complete with fossilized cones 8 . Despite such potential, Argentine palaeontology has blossomed only in the past decade, due to a national political climate for science that previously varied from deadly to uninterested. When authoritarian generals ruled in the late 1970s, students and scientists were among the tens of thousands of Argentines killed for purportedly having left-leaning political beliefs. During the decade-long term of right-wing President Carlos Menem that ended in 1999, there was little support for any type of research. But by the early 2000s, papers by Argentine palaeontologists began appearing in major journals, typically from the programme at the Argentine Museum of Natural Sciences in Buenos Aires, run by the now-retired Jose Bonaparte. A technician without a doctorate, Bonaparte prowled the countryside for four decades making seminal discoveries. His prot\u00e9g\u00e9s \u2014 including Luis Chiappe, now curator of vertebrate palaeontology at the Los Angeles County Natural History Museum, and Rodolfo Coria, former director of the Carmen Funes Museum in Plaza Huincul in Argentina \u2014 subsequently discovered dinosaur eggs and revealed dinosaur nesting behaviour at Auca Mahuevo (see map), 100 kilometres or so north of Plaza Huincul. These studies produced detailed analyses of dinosaur eggs, nests and the discovery of a giant predatory dinosaur, from around 80 million years ago 9 .  \n                Emerging from the shadows \n              Around 1980, Bonaparte briefly studied the Cerro C\u00f3ndor sediments, which had been discovered in 1949 by Italian-born palaeobotanist Joaquin Frenguelli. But with many other sites available to work, he moved on. Now the Middle Jurassic sediments are coming under scrutiny from researchers such as Puerta, Rougier and a German collaborator, Oliver Rauhut of the Bavarian State Collection for Palaeontology and Geology in Munich. They have been joined by Argentine researchers who returned with doctorates from abroad, like Pol, who completed his Columbia University studies in 2004 at the American Museum of Natural History in New York before coming to Trelew in 2006 as curator of vertebrate palaeontology. There, he and colleagues have benefited from a 15-year-old Argentine policy to establish museums in provinces to exhibit and study specimens. A coastal town founded by Welsh immigrants nearly 125 years ago, Trelew \u2014 Welsh for 'town of Lewis', named for its founder \u2014 is a prominent stop for tourists heading to Patagonia and beyond. With about 110,000 visitors a year bringing in revenue, the museum has developed a healthy research programme. Its palaeontological exhibit area will soon be doubled, at cost of US$5 million. \"They have the best laboratory equipment in Argentina,\" says Makovicky. Local residents know it. Unlike in China, where farmers often scavenge fossil sites for specimens for sale[10], digging in Argentina is controlled by national policy and private landowners. Near Cerro C\u00f3ndor, in a village of a half-dozen homes, the community has embraced its palaeontological history, setting up a mini-museum to educate children and naming its traditional annual fiesta as the dinosaur holiday. When Pol and Escapa visit, locals show them fossils they have found when riding after their livestock. But more often than not, the most promising new localities come from the palaeontologists' own kilometres of hiking. As they survey the barren ground, Escapa looks for dark or black rocks \u2014 indicating carbonaceous material that was sealed ages ago, limiting oxygen so that a plant fossil can form. The first finds are often conifers, such as the cypress. Once a promising fossil is noted, the hikers stop, break rocks and dig. Frog and amphibian fossils usually are the first indication of vertebrates. With luck, those fossils lead to larger vertebrates. On a brisk autumn day in June this year, Pol and Escapa checked such a site, where they had earlier found a metre-long dinosaur specimen, including the brain case, a valued find for any new species. They had hoped to be able to extract the fossil that day, but logistical problems caused by ash drifting from a Chilean volcano delayed its removal. Extracting fossils can be onerous. For the large pod back at the lab, it took five years from the day in 2002 when technician Leandro Canessa discovered the fossils jutting from the hillside. First, the fossil clump had to be covered with casting material; then arrangements were made with a construction crew to bring a bulldozer to cut a road up a steep canyon. Finally, a crane was driven in to winch the fossil cluster into a truck. \"This was really a project,\" says Pol, giving credit to Puerta. But the hard work pays off in providing the exact geological context for a fossil. This stands in stark contrast to China, where palaeontologists often have to reconstruct the geology of a purloined fossil long after it has been removed from the ground. With each new find, the Cerro C\u00f3ndor scientists divide up the research based on their specialities. Beginning in 2000, Rauhut worked on dinosaurs, with Pol joining him later. Escapa takes the plants. There's no shortage of work to go around. The Middle Jurassic, says Rauhut, is \"the least-known part of dinosaur history. And the area around Cerro C\u00f3ndor is incredibly rich.\" Since first coming as a postdoc to Trelew in 2000, Rauhut has been on 13 field campaigns. New finds that have yet to be described include a sauropod and an ornithischian dinosaur, a bird-hipped creature. \"The ornithischia is the one I am excited about,\" says Pol. \"We have a skull, a lower jaw and about 50% of the skeleton. There is nothing known about Middle Jurassic ornithischia from this region.\" And perhaps the biggest challenge is just the processing time to analyse and study all the new fossils. With a new three-year grant of \u20ac90,000 (US$130,000), Rauhut and his colleagues will be heading out to Cerro C\u00f3ndor again later this year, hoping to bring as many specimens back as possible from the field. \"There is a lot more to come,\" he says. Rex Dalton is a reporter for  Nature  based in San Diego. \n                     Trelew Paleontological Museum \n                   \n                     Latin American Congress of Vertebrate Paleontology \n                   \n                     Trelew Museum \n                   \n                     Argentine Museum of Natural Sciences \n                   Reprints and Permissions"},
{"file_id": "455850a", "url": "https://www.nature.com/articles/455850a", "year": 2008, "authors": [{"name": "Jane Qiu"}], "parsed_as_year": "2006_or_before", "body": "In an effort to avoid a food crisis as the population grows, China is putting its weight behind genetically modified strains of the country's staple food crop. Jane Qiu explores the reasons for the unprecedented push. In a paddy field 30 kilometres south of Fuzhou, the capital of China's Fujian province, Wang Feng is surveying a massive green and yellow chessboard before him. Wang, a rice researcher at the Fujian Academy of Agricultural Sciences, and his colleagues have been developing genetically modified (GM) rice strains to resist pest infestation, and have been testing in these plots for a decade. Two strains from Wang's team are now awaiting regulatory approval by the agricultural ministry for commercial growth. It could represent the largest commercialization of a GM foodcrop. Rice is a staple for most of the country's 1.3 billion people and a primary source of calories for more than half the world's population. China's population is set to top 1.45 billion by 2020, and the country needs to increase grain production by about 25%, a daunting task in the face of increasing urbanization, industrialization, farmland reduction and the efflux of rural workers to the cities. The Chinese government has latched on to transgenic plants as a solution, rolling out a major research and development initiative on GM crops for the next 12 years, including a sizeable investment of 25 billion yuan (US$3.7 billion) from the central government and additional matched funding from its provincial counterparts.  \n                The bigger picture \n              Like GM initiatives elsewhere, such as in the United States, the move has drawn its share of criticism, with concerns being raised about the practicality and safety of such a push. Scientists warn that a single-minded focus on genetic engineering to enhance pest resistance misses the bigger picture of how to address agricultural production. China is the world's largest rice producer, weighing in with nearly 200 million tonnes, and several observers fear that introducing GM rice could endanger the food supply and the environment. \"The consequences would be unthinkable if large-scale cultivation of GM rice were not properly regulated,\" says Xue Dayuan, chief scientist on biodiversity at the Nanjing Institute of Environmental Sciences. But in a country where policies are rarely a matter of open debate, government officials warn that the scale of the impending food shortage makes further delays an unaffordable luxury. \"This is the only way to meet the growing food demand in China,\" says Huang Dafang, former director of the Biotechnology Research Institute of the Chinese Academy of Agricultural Sciences (CAAS) in Beijing. Wang is optimistic that his group's pest-resistant GM rice will help lead the way. In April, the team planted alternating squares of conventional rice crops and crops genetically modified to produce an insecticidal toxin made by the bacterium  Bacillus thuringiensis   ( Bt)   gene and the cowpea trypsin inhibitor ( CpTI ) gene. In the absence of chemical pesticides,  Bt / CpTI   rice thrived, whereas the conventional plants withered, resulting in the chessboard pattern of alternating colours. Wang pulls the top from one of the non-transgenic plants. Unrolling its leaves and splitting its stem, he reveals the insecticides' primary target, the stem borer. Wang says stem borers can affect 3.3 million hectares of rice fields, resulting in a 5% loss in yields at a cost of 10 billion yuan a year. According to Zhu Zhen, a geneticist at the Beijing-based Institute of Genetics and Developmental Biology, Chinese Academy of Sciences (CAS), who developed the rice strains with  Bt   and  CpTI   genes, there are no naturally occurring strains that can confer such resistance. After ten years of field testing at a dozen locations, the researchers are confident that farmers would use less pesticide with GM rice strains 1 .  \n                Plagued by pests \n              But David Andow, an entomologist at the University of Minnesota in St Paul, says he is unconvinced. In the past few decades, the stem borer has been overtaken by another pest, the brown planthopper ( Nilaparvata lugens ), which wreaks havoc every spring and has become the main concern of farmers in Asia.  Bt   and  CpTI   toxins have no effect on the insect. Moreover, many simply see GM approaches as ham-fisted in the face of complex ecologies. Kong Luen Heong, an entomologist at the International Rice Research Institute in Los Ba\u00f1os, the Philippines, calls pest-resistant GM crops a short-term fix for long-term problems caused by crop monoculture and overuse of broad-spectrum pesticides. \"Pests thrive where biodiversity is at peril,\" says Heong. \"Instead of genetic engineering, why don't we engineer the ecology by increasing biodiversity?\" Indeed, such ecological engineering has proved beneficial. Zhu Youyong, president of the Yunnan Agricultural University in Kunming, and his colleagues have found that growing a mixture of rice varieties across thousands of farms in China could greatly limit the development of rice blast \u2014 a fungal rice disease \u2014 and boost the yield 2 . They have also tested similar practices using different crops and found beneficial effects. Although GM crops are, in principle, compatible with such an ecological approach, it requires management that has proved hard to achieve within China's agricultural system, which is based on small-scale cultivation, says Xue. Although  Bt   cotton, the biggest GM crop produced in China to date, has put the cotton bollworm ( Helicoverpa armigera ) under control, the population of secondary pests, such as mirids, has risen since 2001. That has led to increased pesticide use 3 ,   4 , but still at levels lower than pre-1997, when the cotton was introduced.  \n                Safety concerns \n              Debates have also flared because of rice's central role in the Chinese diet. One concern has been that antibiotic-resistance marker genes used in the derivation of the transgenic plants could invariably be taken up by naturally occurring gut bacteria and lead to resistant, pathogenic strains. Both of Zhu Zhen's  Bt / CpTI     rice lines and other    Bt   strains developed elsewhere are free of such markers. The GM plants must also be shown to be non-allergenic. Composition tests and studies assessing toxicity in non-human animals allow the developers to claim that the GM rice varieties are \"substantially equivalent\" to unmodified counterparts apart from the target-gene expression. But for food eaten three times a day by a billion people, short-term animal studies aren't enough to measure equivalence. \"If there were a health risk, we would be heading for a major disaster,\" warns Liu Bing, an expert on science and society in Tsinghua University in Beijing. Another concern is the potential environmental consequences of transgenes escaping from GM rice to its unmodified crop counterparts through cross-pollination. Several escapes have occurred around the world, including releases of unapproved GM crops such as rice and corn into human consumption streams. For example, in 2006, the European Union halted imports of US rice when an unapproved strain was found in the food supply. Trade resumed, but the problem of accidental cross-pollination, which is thought to have caused the contamination, is one that has not yet been solved. The consensus seems to be that perfect prevention of such events is impossible 5 . Lu Baorong, a biodiversity researcher at Fudan University in Shanghai, is concerned about gene flow from GM rice to its wild or weedy relatives. Wild-rice plants are undomesticated strains, whereas weedy rice, which is characterized by its seed scattering and dormancy, is thought to originate from rice crops as a result of mutations. Lu's team and another group have shown that the rate of gene flow from GM strains to wild and weedy rice is 3\u201318% and 0.01\u20130.5%, respectively 6 ,   7 . \"What is most worrying is that such gene flow is cumulative,\" says Lu. Although rice crops are harvested at the end of the season, wild and weedy rice carrying transgenes would continue to reproduce, allowing the genes to spread, subject to selection. This could threaten the biodiversity of wild rice, which provides a valuable gene pool for rice breeders (see  'The panda of the plant world' ) but is already at the brink of extinction in China. In addition, weedy rice with pest-resistant or other fitness genes could have a greater capacity to infest rice fields, causing yield loss. However, Lu says that these are not inevitable consequences of large-scale cultivation of GM rice. \"Proper regulation is the key,\" he says.  \n                A regulatory mess \n              But regulation, says Xue, is where the majority of problems lie. \"Field testing is one thing, but the reality is quite another.\" Although China has had biosafety regulations for GM crops since 1996, their implementation has proved uneven \u2014 a fact that most people approached by  Nature   acknowledged. In some provinces, such as Xinjiang, farmers began large-scale cultivation of  Bt   cotton long before approval was granted, says Xue. In several cases,  Bt   cotton strains were grown without proper labelling, some of which were experimental strains from research institutes. Cross pollination and labelling slip-ups could be disastrous for China's exports of rice and rice-related products. And proper regulation of GM crops is crucial for delaying the emergence of resistant pests. Many crops, such as cotton and rice, are grown as a monoculture in China, which would select pests that are resistant to the toxins. One way to avoid this from happening is to use seeds that produce high toxin levels; another way is to set aside some land near GM-crop fields for its unmodified counterparts, which would serve as a 'refuge' for insects. This 'high-dose and refuge' strategy, which has been widely adopted in countries that grow GM crops, is difficult to implement in China. Many Chinese farmers exchange seeds with each other or buy cheap seeds from illegal dealers, and end up growing cotton plants with low levels of  Bt   toxin. In addition, as agricultural practices in China are based on small-scale cultivation by individual families with limited resources,  Bt   cotton plants are grown without refuge areas. Fortunately, the cotton bollworm also attacks crops such as wheat, corn, soya beans, peanuts and vegetables, which are grown next to the  Bt -cotton fields and offer a safety valve against resistance 8 ,   9 . \"This is unlikely to happen with  Bt/CpTI   rice because the stem borer feeds only on rice,\" says Heong. \"Therefore, setting aside refuge areas is absolutely essential.\"  \n                Behind closed doors \n              Worryingly, many of the stakeholders, including farmers, bioethicists and environmental groups, aren't being involved in the biosafety evaluation process as spearheaded by the agricultural ministry. The country hails GM rice as a magic bullet for food-production problems and few dissenting opinions are heard. \"The whole process is rather opaque,\" says David Just, an economist at Cornell University in Ithaca, New York. \"China is trying very hard to keep the lid on.\" Experts who express their concerns are often sidelined. Xue, for example, has been repeatedly excluded from biosafety committees that are assessing GM crops. Despite these concerns, China does need to find a way to feed its swelling population. Although it has instigated plans to prevent further reduction in farmlands, boosting grain production remains the key to food security. Still, the single-minded focus on genetic modification seems misguided to many. \"Genetic-modification technologies just treat the symptoms rather than dealing with the causes,\" says Hans Herren, president of the Millennium Institute in Arlington, Virginia, and a co-chair of the International Assessment of Agricultural Knowledge, Science and Technology for Development (IAASTD). According to a report released by the IAASTD in April, the main challenges faced by agricultural development around the world are soil fertility, water management and climate change 10 . \"Life in the soil is gone after decades of heavy use of pesticides, herbicides and chemical fertilizers,\" says Manuela Giovannetti, a soil microbiologist at the University of Pisa in Italy. Herren agrees: \"Without a concerted global effort to restore soil fertility, genetic modification would be futile.\" Xue says he recognizes the potential of genetic modification, but is concerned that huge investment in the technologies \u2014 as with China's new initiative \u2014 would sap already dwindling attention from improving traditional plant-breeding technologies and conventional farming practices. However, GM strategies have a strong draw for keeping China competitive at the cutting edge of agriculture. A report by the International Service for the Acquisition of Agri-biotech Applications estimates that biotech rice could deliver benefits of $4 billion per year for China 11 . \"What is behind all this might be about who controls germ plasm and who owns intellectual-property rights,\" says Andow. The scale of the effect that commercial GM rice could have on China and the rest of the world argues for caution. Nevertheless, many interests within the country say that the time to act is now. Huang puts it bluntly: \"We cannot afford to think too far ahead but must tackle the present issues.\" Jane Qiu writes for Nature from Beijing. \n                     Web Special: China's Challenges \n                   \n                     Web Focus: GM Crops: Time to Choose \n                   \n                     International Service for the Acquisition of Agri Biotech Applications \n                   \n                     International Association of Agricultural Economists \n                   \n                     International Assessment of Agricultural Knowledge, Science and Technology for Development \n                   Reprints and Permissions"},
{"file_id": "455016a", "url": "https://www.nature.com/articles/455016a", "year": 2008, "authors": [{"name": "Cory Doctorow"}], "parsed_as_year": "2006_or_before", "body": "What does it take to store bytes by the tens of thousands of trillions? Cory Doctorow meets the people and machines for which it's all in a day's work. Ten seconds after I stepped into the roar of the data centre at the UK Wellcome Trust Sanger Institute, in rural Cambridgeshire, my video camera croaked: CARD FULL. Impossible. That morning, I'd tossed a handful of thumbnail-sized 32-GB memory cards into my pocket, each one good for a couple of hours' worth of high-definition video. Yet this one had filled up in seconds. I fumbled with my camera while Phil Butcher, the Sanger Institute's head of information technology (IT), politely waited, grinning in the shower of cold air washing down from the air conditioning. It took only a couple of embarrassing seconds to troubleshoot: I'd somehow mixed an old 32-megabyte card in with the 32-gigabyte cards. The 32-MB card is only a couple of years old; when I bought it, it probably cost more than the 32-GB cards do today. But it holds one one-thousandth of the data. That, in coincidental microcosm, is the story I'm here for: the relentless march from kilo to mega to giga to tera to peta to exa to zetta to yotta. The mad, inconceivable growth of computer performance and data storage is changing science, knowledge, surveillance, freedom, literacy, the arts \u2014 everything that can be represented as data, or built on those representations. And in doing so it is putting endless strain on the people and machines that store the exponentially growing wealth of data involved. I've set out to see how the system administrators, or sysadmins, at some of the biggest scientific data centres take that strain \u2014 and to get a sense of how it feels to work with some of the biggest, coolest IT toys on the planet. At this scale, memory has costs. It costs money \u2014 168 million Swiss francs (US$150 million) for data management at the new Large Hadron Collider (LHC) at CERN, the European particle-physics lab near Geneva. And it also has costs that are more physical. Every watt that you put into retrieving data and calculating with them comes out in heat, whether it be on a desktop or in a data centre; in the United States, the energy used by computers has more than doubled since 2000. Once you're conducting petacalculations on petabytes, you're into petaheat territory. Two floors of the Sanger data centre are devoted to cooling. The top one houses the current cooling system. The one below sits waiting for the day that the centre needs to double its cooling capacity. Both are sheathed in dramatic blue glass; the scientists call the building the Ice Cube.  \n                Blank slate \n              The fallow cooling floor is matched in the compute centre below (these people all use 'compute' as an adjective). When Butcher was tasked with building the Sanger's data farm he decided to implement a sort of crop rotation. A quarter of the data centre \u2014 250 square metres \u2014 is empty, waiting for the day when the centre needs to upgrade to an entirely new generation of machines. When that day comes, Butcher and his team will set up in that empty space the yet-to-be-specified systems for power, cooling and the rest of it. Once the new centre is up, they'll be able to shift operations from the obsolete old centre in sections, dismantling and rebuilding without a service interruption, leaving a new patch of the floor fallow \u2014 in anticipation of doing it all again in a distressingly short space of time. The first rotation may come soon. Sequencing at the Sanger, and elsewhere, is getting faster at a dizzying pace \u2014 a pace made possible by the data storage facilities that are inflating to ever greater sizes. Take the human genome: the fact that there is now a reference genome sitting in digital storage brings a new generation of sequencing hardware into its own. The crib that the reference genome provides makes the task of adding together the tens of millions of short samples those machines produce a tractable one. It is what makes the 1000 Genomes Project, which the Sanger is undertaking in concert with the Beijing Genomics Institute in China and the US National Human Genome Research Institute, possible \u2014 and with it the project's extraordinary aim of identifying every gene-variant present in at least 1% of Earth's population. As data pour off the Sanger's new Solexa sequencers, Butcher \u2014 a trim, bantam grey-haired engineer with twinkling eyes and laugh-lines \u2014 has to see to it that they have somewhere to go. A two-hour Solexa run produces a gigantic amount of information: 320 TB, according to Tony Cox, head of sequencing informatics, a figure he's mentioned to journalists in the past (a print-out on his office door reads: \"'Oh shit, that's 320 TB!' \u2014 Tony Cox,  The Guardian , 28 February 2008\"). The 1000 Genome Project needs to make use of storage and computing capacity at a (currently) impossible density. Luckily for Butcher, 'impossible' is a time-bound notion \u2014 if you don't like the compute reality, just wait around a little while and another will be along shortly. His storage density is doubling every year; the 500-GB hard-drives spinning away in his storage array are being phased out by Seagate of Scotts Valley, California, the company that makes them, in favour of a terabyte model. Finding a place for the data to go is only the beginning. Butcher also has to make sure they can get back out. The Sanger has a commitment to serving as an open-computing facility for the worldwide research community. So it faces what you could call the Google problem: an unpredictable and capricious world that might decide at any moment to swarm in with demands for shedloads of arbitrary-seeming data. Just as a news scandal can conjure a flashmob of millions of net-users to Google's homepage, all searching for 'tsunami' or 'paris hilton', an exciting discovery in genetics sends the whole bioinformatics community to the Sanger's servers, all demanding the same thing. You can't go far in this world without some sort of comparison to Google, which is the biggest of the big gorillas. How big, though, is not quite clear \u2014 and nor is it clear how it manages its flashmobs and other petaproblems. In the ten years since the company's founding, Google's data-serving systems have gone from a set of commodity PCs connected to a hard-disk array built into an enclosure made from Lego bricks to a global system of data farms unmatched by anyone else. Each of those data centres is designed from the foundation up to operate as a single big computer. Google buys components optimized for the kind of operations that relentlessly hammer its servers. It has software for the job such as Google File System (which distributes three copies of each piece of information in a way that makes it easy to recover when the inevitable failure occurs) and Google MapReduce, a system for automatically and efficiently making large data sets amenable to parallel processing. Google's distinguished engineers present papers at learned conferences explaining in detail how it all works. People such as Butcher pay close attention. But then there's the closed part: all the specific metrics and data-porn that Google considers of competitive significance. The company no longer says how big the model, or copy, of the web's material spread through its data centres is. It doesn't disclose the dimensions or capacity of those data centres.  Nature  wanted me to visit one for this piece, but a highly placed Googler told me that no one from the press had ever been admitted to a Google data centre; it would require a decision taken at the board level. Which is too bad. But it's not as if the world is bereft of other computer installations with mind-bending requirements. And at CERN, the Sanger and XS4ALL in the Netherlands, I found myself welcomed into the roaring sanctums of computing, escorted around by sysadmins eager to show off the hellaciously complex, monstrously powerful machines that they've been able to put together and put to use.  \n                Repository of all knowledge \n              The primary XS4ALL facility at the World Trade Centre near Schiphol Airport in Amsterdam was actually built to house a mighty array of huge telephone switches in 2000. KPN, the then Dutch national telecom company, fitted it out generously, with several months' worth of diesel in subterranean tanks for its uninterruptible power supply's back-up generators, and two independent cooling systems, with one raised two storeys off the ground to flood-proof it (this is the lowlands after all). But telecom deregulation was not kind to KPN, and the switches never came. So now the facility houses XS4ALL, a once-notorious Dutch Internet service provider that has somehow made it bigtime. Hacktic, a collective of hackers, established XS4ALL in 1993 to help cover the costs of the Internet link they had set up. In 1994, KPN shut down all of XS4ALL's lines after the collective published an article explaining how to cheat the phone company's punitive long-distance tolls \u2014 the ISP came back online only after posting a 60,000 guilder (US$35,000) cash bond. Just four years later, after XS4ALL had grown into one of the most successful ISPs in the lowlands, the state company bought out its former gadfly. Today XS4ALL is as independent as a subsidiary of a former government monopoly can be, but its members are not above sharing digs with their corporate parent, especially as the corporate parent is such a spendy sort of sugar-daddy. XS4ALL has taken over two storeys of the would-be switching centre with hackerish humour: the raised floors sport Perspex panels revealing neon lights and jumbles of entombed PC junk; there is a chill-out room for sysadmins who come on-site to run backups or swap drives; a poster listing the facility's regulations ends: \"Rule 12: No sex in the data centre.\" The mix of freewheeling hacker humour, deadly serious commitment to free speech and solid technological infrastructure made XS4ALL a natural choice to host a mirror copy of the Internet Archive ( www.archive.org ), a massive 'repository of all knowledge'. The archive's best-known feature is the Wayback Machine, an archive of most of the public pages on the World Wide Web that allows visitors to 'travel in time' and see what any given URL looked like on any given date since 1996. But it also serves as a repository for practically every public domain and Creative Commons-licensed digital document it can lay its hands on. It is the brainchild of philanthropist Brewster Kahle \u2014 co-creator of Wide Area Information Servers, or WAIS, one of the first Internet search engines \u2014 who wants it to provide \"universal access to all knowledge\". In a world of here today/gone tomorrow Web 2.0 companies willing to host your video, pictures or text, the archive stands out as a sober-sided grown-up with a commitment to long-term (infinite-term) hosting. Inside the XS4ALL data centre, which is about the size of a football pitch, my hosts took me past aisle after aisle of roaring machines to the Internet Archive's European mirror. \"That's it, huh?\" Two racks, each the size of a modest refrigerator, each holding north of a petabyte's worth of information. These are the PetaBoxes, the Internet Archive's web-in-a-box systems. Designed as a shippable backup for every freely shareable file anyone cares to upload and hundreds of copies of everything else, too, they betray the archive's US origins in the strip of American-style electric outlets running down one strut, a column of surprised clown-faces Fed-Exed from across the ocean. A couple of other things set them apart. Each rack draws 12 kilowatts, whereas a normal rack at the facility draws 4.5 kilowatts; the drive-housings are covered in a rather handsome fire-engine-red enamel. Apart from that, the PetaBoxes are just another pair of racks. Yet housed in these machines are hundreds of copies of the web \u2014 every splenetic message-board thrash; every dry e-government document; every scientific paper; every pornographic ramble; every libel; every copyright infringement; every chunk of source code (for sufficiently large values of 'every', of course). They have the elegant, explosive compactness of plutonium. Something dawns on me: I ask my XS4ALL tour guides, shouting over the jet-engine roar: \"If there are all those copies of the web on the PetaBoxes, what's in all the other machines?\" \"Oh, customer stuff. Intranets. Databases. E-mail. Usenet.\" In other words, all the dynamic stuff, the private stuff, the dark web that is invisible to search engines, and all the processor power needed to serve it. All the reasons that Google can't exist just in a couple of red PetaBoxes. \"How does KPN feel about housing these two extraordinary boxes?\" \"Oh,\" they say, exchanging a mischievous glance, \"I don't think they know we have them here.\" In a data centre such as this, a working approximation of 'all knowledge' can be slipped into the cracks like a 32-MB memory card jingling in my pocket.  \n                600 million collisions a second \n              The archive has three real-time mirrors: the original in San Francisco's Presidio, just south of the Golden Gate, the XS4ALL mirror, and a third under the New Library of Alexandria in Egypt. A keen observer will note that these are variously placed on the San Andreas Fault, in a flood-zone, and in a country with a 27-years-and-running official 'state of emergency' that gives the government the power to arbitrarily restrict speech and publication. Someone needs to buy Kahle a giant cave in Switzerland. Like the one I'm off to now, which will be housing the data from the biggest experiments on the most powerful machine ever conceived. Except it turns out that the data centre at CERN is less hall of the mountain king and more high-school gymnasium. The caverns measureless to man through which the LHC runs are reserved for making the data. The systems storing them have much more humdrum quarters. The slight sense of anticlimax is emphasized by the unflappable calm of Tony Cass, CERN's leader of fabric infrastructure and operations; his data centre may be about to become the white-hot focus of the entire world's high-energy physics community, but Cass is surprisingly and perhaps bit disappointingly relaxed. Indeed, when we met just a few weeks before the LHC was about to see its first circulating beam, on 10 September, he was headed off on holiday. Built in the 1970s to be the only data centre that CERN would ever need, Cass's current facility is now just a stopgap on the way to the construction of a bigger, faster centre that will absorb 15 petabytes a year of experimental data from the LHC. Although the rack after rack of systems in the current centre are nearly new, there are already plans to replace them. The basement is a graveyard of already-replaced generic PCs that are slowly being cleansed of data and shipped to bidders from the former Soviet Union. The difference between Cass's challenges and Butcher's is a difference in the way that physics and biology work. At the Sanger, the charge-coupled devices (CCDs) in the sequencers can vomit out TIFF image files by the terabyte around the clock, but they are useless until processed, analysed and shrunk down to a far more manageable summary of what those vast image files actually meant. The original data are thrown away \u2014 the Sanger is confident that there will never be anything new to be learned from looking at the raw image files later. And there would be no way of keeping it except on tape, and tape, as Butcher will tell you, is slow, impractical and failure prone. As a former sysadmin myself, I can attest to the inherent tetchiness of tape. The Sanger reduces the images to more amenable data and then sends everything off to various mirror sites using a custom-made file-transfer protocol implemented over what's known as user datagram protocol (UDP); this allows the gene genies to saturate entire transoceanic links without having to wait for any of the finicky TCP handshaking and error-correction nonsense used for Internet traffic. It's slow compared with CERN's approach \u2014 CERN leases its own fibre, at great expense \u2014 but it certainly beats the old-school open-source system used to glue together the Internet Archive's mirrors. If only high-energy physics were so amenable to throwing stuff away, Cass's life would be a lot simpler. It's not. The meaning of a sequencer run is pretty straightforward, and won't change. The meaning of a particle collision is continuously reassessed based on new information about the instrument's performance. Physicists will want to reanalyse all the collisions expected from the LHC from the first to the last. Six hundred million events a second for year after year, analysed over and over again as the physicists' models become more refined. And that means a lot of storage \u2014 the kind of storage you can't load onto a reasonable quantity of spinning drives. The kind of storage you need to put on tape. I am, admittedly, prone to swooning over a well-designed bit of IT kit, but I have never developed as deep and meaningful and instantaneous a relationship as the one I formed with the two tape-loading robots in the basement of the CERN data centres. The Vader-black machines, one built by StorageTek, a subsidiary of Sun Microsystems, the other by IBM, are housed in square, meshed-in casings the size of small shipping containers. From within them comes a continuous clacking noise like the rattling of steel polyhedral dice on a giant's Dungeons & Dragons table. I pressed my face against the mesh and peered in fascination at the robot arms zipping back and forth with tiny, precise movements, loading and unloading 500-GB tapes with the serene grace of Shaolin monks. Did I say tape is tetchy? I take it back. Tape is beautiful. Each robot-librarian tends 5 PB of data. It will jump shortly to 10 PB each when the 500-GB tapes are switched to 1-TB models \u2014 an upgrade that will take a year of continuous load/read/load/write/discard operations, running in the interstices between the data centre's higher-priority tasks. When that is done, there should be 2-TB tapes to migrate to, bringing the two robots' total up to 40 PB. At least, that's what CERN hopes. The tape libraries will allow the regular reassessment of the LHC data \u2014 unloading, reprocessing and reloading all the data on each of the tapes. A complete reprocessing will take a year, in part because, although it is higher priority than migrating the data to higher-density tapes, it still takes a backseat to the actual science \u2014 to jobs requested from anywhere in the world. CERN embodies borderlessness. The Swiss\u2013French border is a drainage ditch running to one side of the cafeteria; it was shifted a few metres to allow that excellent establishment to trade the finicky French health codes for the more laissez-fair Swiss jurisdiction. And in the data sphere it is utterly global. Cass's operation is backstopped by ten 'Tier One' facilities around the world that replicate its tape library, and some hundreds of 'Tier Two' facilities that provide compute-power to operate on those data, all linked by dedicated, high-speed fibre, part of a global network that attempts to tie the world's high-energy physics institutions into a single, borderless facility. A researcher who logs into the CERN data centre runs code without worrying which processors execute it or which copy of the data it is run on. The birthplace of the web, which demolished borders for civilians, CERN is ushering in a borderless era for data-intensive science, an era in which US researchers run code on Iranian supercomputers and vice versa, without regard for their respective governments' sabre rattling. Cass wants to weld the world's physics computers into a single machine.  \n                Sysadmin nightmares \n             At each data centre I asked the sysadmins for their worst fears. Universally, the answer was heat. Data centres are laid out in alternating cool and hot aisles, the cool looking at the front of the racks, the hot at the back. At CERN, they actually glass over the cool aisles to lower the cooling requirements, turning them into thrumming walk-in fridges lined with millions of tiny, twinkling lights. If power is cut to the cooling system in one of these places, you've got minutes for a clean shutdown of the systems before their heat goes critical. XS4ALL has a particularly impressive cooling system, a loop that runs from the 5 \u00b0C, 30-metre depths of nearby Lake Nieuwe Meer, warms to 16 \u00b0C in the centre's exchangers, and then plunges back to the lake-bottom to be cooled again. The site manager Aryan Piets estimates that if it broke down and the emergency system didn't come on, the temperature in the centre would hit 42 \u00b0C in ten minutes. No one could cleanly bring down all those machines in that time, and the dirtier the shutdown, the longer the subsequent start-up, with its rebuilding of databases and replacement of crashed components. Blow the shutdown and stuff starts to melt \u2014 or burn. Data centres do face more exotic risks. Google once lost its transoceanic connectivity because of shark bites. Butcher lives in fear of a Second World War fighter plane going astray from the airshows at nearby Duxford airfield and crashing into the Ice Box. At CERN they worry about people believing the worries that the Universe will wink out of existence when they fire up the LHC. But the real worry is power and its management. Data centres built in the giddy dotcom heyday assumed that racks would sport one processor core per unit and planned cooling and energy accordingly. But that is not the way the technology has gone. Computers have got faster not through faster cores, but through more of them. With 16 cores or more per unit, data centres around the world sit half-empty, unable to manage the power-appetites of a whole room's worth of 2008's ultra-dense computing. And everyone lives in fear of the electrical fault that sparks a blaze that wafts killing soot into the hungry ventilation intakes on the racks. A big part of the problem \u2014 and possibly of its solution \u2014 is that most of a data centre's compute capacity is idle much of the time. No sysadmin would provision a data centre to run at capacity around the clock, lest it melt down (along with the sysadmin's career) the first time something really juicy increased the load. Yet whether a network card is saturated or idle, it still burns 100% of its energy draw. The same with video cards, power supplies, RAM and every other component except for some CPUs. So these idle systems whir away, turning coal into electricity into heat that has to be cooled with coal turned into electricity turned into heat, and the planet warms and the bills soar. Every decibel of noise roaring through the centres is waste, energy pissed away for no benefit. The people with the biggest data centres have the biggest problem \u2014 and the biggest resource to throw at it. Google buys its systems in enough bulk that it can lay down the law to component suppliers, demanding parts that draw power proportional to the amount of work that they are doing. Its holistic approach to the data centres, treating each one as a single PC, means that it can plan for idleness and peak load alike, and keep the energy bills under control. Everyone agrees that something like this is the way forward, that the future of data centres must be cooler, and quieter. That said, a certain discomforting noise has its advantages. \"I don't want it to ever get too comfortable in here,\" says Cass. \"I like it that people access us remotely. It just doesn't scale, having every scientist drop in to run jobs here in the centre.\" And if Google leads the way because it has to feed people's need for Paris Hilton searches and peeks at their own roof on Google Earth, that is quite fitting. Whereas scientists unzip new genomes and summon new particles from the roiling vacuum with technologies beyond compare, the secret of data storage and processing is a lot simpler: commodity components. There is a huge ingenuity in how you use them, cool them, arrange them and keep them from melting, but the basic ingredients of a petacentre are the ingredients of life on the net. Everything I've seen on these trips was basically made out of the same stuff I've got lying around the flat. Gene-sequencers use multi-megapixel CCDs \u2014 cheap and cheerful in this era of digital photography \u2014 to generate TIFFs that I could open with the open-source image-manipulation program that came with my free Ubuntu GNU/Linux operating system. The hard-drives in the server cases are the same cheap, high-capacity Seagates and Toshibas that I have in the little box I stuck under the stairs and wired up to my telly to store away a couple of terabytes of video, audio and games. A decade ago, a firm's 'mainframe' was a powerful beast made from specialized components never seen outside the cool, dust-free environs of the data centre. Today, mainframe is more synonymous with the creaky old legacy system that no one can be bothered to shut down because it is running an obscure piece of accounting software that would be a pain to port to a modern system. The need for special hardware just isn't there any more. Even Google's 'energy-proportional' future is just an expansion of the power-management and heat-dissipation technology developed for laptops, and any gains achieved on the server side will also come to our desktops. I've got everything I need lying around the office to make my own petacentre \u2014 I just need more of it. And a much bigger fridge. Or a cool-bottomed lake. That said, I don't have a tape robot. But I really, really want one. Cory Doctorow is a digital-rights activist, author and co-editor of Boing Boing, a blog. His most recent novel is  Little Brother . See Editorial,  page 1 \n                     Big data special \n                   \n                     Future computing special \n                   \n                     Sanger Centre bioinformatics \n                   \n                     XS4ALL \n                   \n                     The Internet Archive \n                   \n                     CERN Data Analysis \n                   \n                     A history of IBM tape robots \n                   \n                     Cory's Petacentre Flickrstream \n                   \n                     Little Brother \n                   \n                     BoingBoing \n                   Reprints and Permissions"},
{"file_id": "455854a", "url": "https://www.nature.com/articles/455854a", "year": 2008, "authors": [{"name": "Eric Hand"}], "parsed_as_year": "2006_or_before", "body": "The International Space Station's one chance of scientific greatness rests on a high-profile refugee from the world of the particle accelerator - but is it too long a shot to be worth taking? Eric Hand reports. In 1994 Dan Goldin, then the administrator of NASA, was on the look out for sexy scientific ideas. In particular, he needed something that would endear the International Space Station \u2014 a merger of America's earlier space station plans with those of Russia \u2014 to scientific sceptics unimpressed by the experimental opportunities it offered. Roald Sagdeev, the former director of a major Soviet space research institute, mentioned to Goldin that Samuel Ting, a Nobel prizewinning particle physicist, was toying with ideas for a space-borne magnet that would sift antimatter from the stream of particles from beyond the stars. \"He said, 'Okay, where is this guy? I want to see him immediately',\" recalls Sagdeev. By 1995 Ting, a professor at the Massachusetts Institute of Technology in Cambridge, had won Goldin's agreement that NASA would give his Alpha Magnetic Spectrometer (AMS) space on the station, and a shuttle flight to get it there. Part of that agreement was that the agency would not have to pay for the AMS to be built. Instead Ting got the Department of Energy (DOE), which funds US particle physics, to oversee the AMS \u2014 and undertook to round up most of the funding from foreign governments. That he did. Thirteen years on, and by some accounts US$1.5 billion down the line, a vast team of physicists from 16 countries has put all but the finishing touches to the AMS. But since returning to flight after the loss of the Columbia in 2003, the shuttles \u2014 due to retire in 2010 \u2014 have been devoted almost entirely to completing the space station. A dedicated AMS flight was dropped from the manifest in 2005, and reinstating one seemed, until recently, out of the question. Yet Ting, 72, has soldiered on. A Nobel prize and decades running huge particle-physics collaborations give him considerable heft; few physicists have the clout to get an Italian foreign minister to plead their case to US secretary of state Condoleezza Rice. And the presidential campaign has provided extra leverage to the weight Ting brings to bear on lifting the AMS into the sky. An extra shuttle flight is a nice thing to promise Florida voters worried about jobs that will disappear when the shuttle is grounded for good, and both candidates for the presidency have recently made such promises. Congress has sent on to the White House a bill authorizing Ting's shuttle flight. Over tea in an empty but opulent dining room at the Mayflower Hotel \u2014 Ting's campaign headquarters in Washington DC \u2014 he folds himself into a chair and cracks open a laptop full of documents, charts, even old clips from  The New York Times  lauding his past achievements. \"I know all the technical details,\" he says. \"I'm the one responsible if something goes wrong. I don't do anything else but this.\" Every day for 13 years, it has been his focus, an all-consuming passion and worry; every day except, that is, for ten days in 1998, when the prototype AMS-01 was flown on the shuttle Discovery. With the experiment floating weightlessly, gathering data from particles passing through its doughnut-shaped magnet, Ting felt himself relax. \"For the first time, I realized that if something went wrong, there was nothing I could do.\" Ting's case, bolstered by slide after slide from the laptop, is that the AMS will open up a new spectrum to astronomy: that of charged particles. Antimatter left over from the Big Bang is an imagination-grabbing example of the sort of thing it might find. But only an example: the real need for the AMS, in Ting's mind, is to discover the utterly unforeseen. One of his slides lists the originally cited aims of a huge range of 'big' science projects over the past 50 years \u2014 and the discoveries for which they are famous. The two are always different. \"What you will see,\" he says, in his slow, soft voice, \"it's hard to predict.\" The numerous opponents of the AMS, however, think that they can predict the project's results \u2014 and that they are likely to be relatively underwhelming. No current theory leads them to expect the presence of antimatter nuclei in space of the sort Ting talked to Goldin about. There are other things, they say, on which the money that would be needed to launch the AMS could be better spent \u2014 things the astrophysical community has evaluated and prioritized. But Ting will have none of it. In 1976, his Nobel lecture offered a tale of careful experiment proving theorists wrong. Experiments are most meaningful when they disagree with theory, he says, with an emphatic tone that brooks no dissent. \"The advancement of physics depends on you destroying other people's perceptions.\"  \n                Sam I am \n              Ting's record backs up his belief in the transformative value of daring experiment. \"By being just that much more clever or careful than everybody else, Sam's able to get stuff out that other people missed,\" says John Ellis, a theorist at CERN, Europe's particle-physics facility near Geneva. It was through such painstaking measurement that Ting won his Nobel prize in the first place. At Brookhaven National Laboratory in Upton, New York, Ting managed to pick the signature of a new particle out of a very messy energy spectrum with almost-over-the-top levels of instrumentation, and a monumental insistence on thoroughness. When Ting shared the 1976 Nobel Prize in Physics for the discovery, the prize committee described his feat as being \"like hearing a cricket near a jumbo jet\". As Ting's reputation grew, so too did the size and scope of his experiments. By 1983, Ting was leading what became the largest physics collaboration in the world: the L3 experiment at CERN's Large Electron Positron (LEP) collider, the first machine to occupy the vast tunnel that now houses the Large Hadron Collider (LHC). With almost 500 physicists, L3 employed more people than any of the other three experiments spaced around the LEP ring. It was also remarkably international, cementing in place Ting's reputation as one of his discipline's great fixers. An American born in Michigan and raised in Taiwan, he got China to supply special crystals to the detector \u2014 its first foray into high-energy physics at that level \u2014 while getting the Soviet Union to contribute an Eiffel Tower's worth of iron. His leadership approach was not exactly democratic, but he accomplished things quickly and decisively, says David Stickland, a CERN physicist who worked on L3. He recalls a time when Ting was asking the DOE for an upgrade to the LEP experiment. The DOE rejected his proposal. \"Sam just stood up and said, 'I reject your rejection',\" recalls Stickland. \"He really operates outside the norms that way.\" L3 ran on the LEP until the accelerator was closed down in 2000. But Ting's attempts to build something even bigger were thwarted. The coalition he put together for a detector to grace America's planned Superconducting Super Collider involved 1,000 scientists from 90 institutions in 13 countries; but it suffered internal strains, and the collider's management rejected it in 1991. Pivoting back to CERN, Ting suggested putting a revamped L3 onto the LHC. Unusually for him, this was a comparatively cheap proposal. But CERN rejected it. Licking their wounds, Ting and his close collaborators began to think about something smaller and simpler \u2014 a break from the demands of giant collaborations, according to Ulrich Becker of the Massachusetts Institute of Technology, who has worked with Ting since 1965. The idea for the AMS was born on a coffee break from L3 work. \"I had this dream to build an experiment that would have fewer than 100 collaborators and could fit on a table,\" Becker says. The idea endured: the scale didn't. The project has involved not 100 but 500 scientists, from 56 institutions. At 7 tonnes, it would need the sort of table a minibus can be parked on. Its extraordinary 0.86-tesla magnetic field is 17,000 times bigger than Earth's and five times greater than a sunspot's. If the AMS didn't spend half its energy cancelling out the field lines that would otherwise stray beyond its confines, the space station wouldn't stay stationary very long. \"Sam Ting doesn't like to do small things,\" says Becker.  \n                Big-Bang refugees \n              The AMS team sees celestial charged particles, also called cosmic rays, as a way to look into a problem that particle physics has not yet solved on its own terms: why is the Universe mostly matter, not antimatter? Processes that favour matter over antimatter clearly played a role in the Big Bang. How those processes played out, though, is still something of a mystery. The contribution the AMS team hopes to make to this debate is to see whether the Universe's bias against antimatter is as complete as is normally assumed. If any antimatter did escape the annihilations of the early Universe, then there could be stray antiatoms still around today. And if an antihelium nucleus \u2014 the lightest antiatom that can't have been formed by any known process since the Big Bang \u2014 were to pass through the central void in the AMS's doughnut, its mass and charge would be immediately revealed by the way its trajectory bent in that awesome magnetic field. The distinctive curve of antihelium would be a revolutionary discovery. That of any heavier antimatter would be simply mind-boggling \u2014 a sign of antigalaxies and antistars somewhere far off and as yet unobserved, their nuclear fires fusing together the antimatter equivalent of the stuff of which Earth, and humans, are made. Antimatter does not have to be primordial. Its lightest particles can be made in various ways, some known and comparatively prosaic, some fanciful and as yet unseen. Some theories hold, for example, that the decay of small black holes might produce antineutrons stuck to antiprotons. Other processes can make these too, but those coming off black holes would move peculiarly slowly \u2014 something the AMS's sensitivity to mass, charge and speed would pick up. Cold dark matter (CDM) \u2014 hypothetical particles thought to account for most of the mass of the Universe \u2014 could be a contemporary source of distinctive antimatter too, in the form of high-energy positrons given off when the CDM particles decay. And then there's strange matter \u2014 matter made up from more types of quark than just the two basic ones that make up neutrons, protons and their antiparticles. The AMS could conceivably detect light, long-lived particles of this quark matter. These wonders sold the idea to Goldin in the 1990s. But they have not convinced the astrophysicists who account for most of NASA's astronomical constituency. Once every decade the US National Academies produce a major report stacking up astronomers' research projects against each other. NASA uses these reports as prioritized shopping lists. The decadal report that came out in 2001 makes only a desultory mention of the AMS, treating it as if it was something outside the report's purview, destined to happen regardless. \"The AMS was something that came out of high-energy physics as a big project at a high level,\" says Thomas Gaisser, of the University of Delaware in Newark and the panel chair responsible for reviewing projects such as the AMS for the decadal report. \"The people who had been working in the cosmic-ray fields for a long time didn't like the competition, basically.\" The decadal report did not endorse the mission.  \n                The antihero \n              In a letter from February of this year, Craig Hogan, chair of an astrophysics advisory committee to NASA, noted that the AMS also went unmentioned in a 2003 National Academies report that was specifically supposed to consider cross-community projects dealing with the nature of cosmic matter. \"The overall health of the astrophysics programme is put at risk by any mission whose science value has not been transparently compared with other missions,\" wrote Hogan, director of the Center for Particle Astrophysics, at the Fermi National Accelerator Laboratory in Batavia, Illinois. Ting responds by pointing out that the AMS is not a NASA astrophysics mission. No NASA science money has been used, and no NASA scientists have worked on it. In terms of being reviewed for scientific merit he notes DOE reviews performed in 1995, 1999 and 2006, in addition to multiple reviews by European agencies. Another line of attack is now opening up. When the AMS was first proposed, tests for antimatter in cosmic rays had hardly been tried: the new window on the Universe they offered was wide open. Now there are some data. BESS (Balloon-borne Experiment with a Superconducting Spectrometer) has looked for antimatter in cosmic rays on three high-altitude balloon flights around Antarctica; the PAMELA (Payload for Antimatter Matter Exploration and Light-nuclei Astrophysics) satellite was launched in 2006. Both, oddly enough, are cheap descendents of an earlier cosmic-ray experiment proposed for the US space station in the 1980s. Their verdict on primordial antimatter? It probably isn't there. \"Will AMS provide any fundamental answers that BESS and PAMELA haven't?\" asks John Mitchell, US principal investigator for BESS at NASA's Goddard Space Flight Center in Greenbelt, Maryland. \"The answer is, probably, no.\" This doesn't mean that the AMS has no chance of finding primordial antimatter: but the window has closed down a lot. And Ting's nippy little rivals are also making inroads into other science that the AMS could have had to itself. There are enticing, as yet unpublished, hints that PAMELA is seeing some of the high-energy positrons that might be expected from decaying cold dark matter (see  _Nature_ 454, 808\u2013809; 2008 ). If this turns out to be the case, the news for the AMS might be bittersweet. Its far greater power and spectral range would be excellent for further analysing those positrons, and so the case for launch would be strengthened. But rather than opening a new window on the Universe, its primary purpose would be reduced to that of follow-up to someone else's discovery. In the meantime, as PAMELA gathers more data, the AMS just sits at CERN, where it was assembled. Over the summer, technicians calibrated it, blasting it with high-energy particles. At the end of this year, the AMS will be taken to a European Space Agency facility in the Netherlands, where it will be tested in space-like conditions. As early as June 2009, the AMS could be ready to cross the Atlantic, tied down in a 747 Lufthansa cargo plane, to Kennedy Space Center on Merritt Island, Florida \u2014 if a shuttle were waiting. Ting is still rueful, worried about the damage to US credibility in a project where foreign partners have footed most of the bill. \"The US government made a commitment to fly it,\" he says. \"This should have been thought about a long time ago \u2014 not after more than $1.5 billion has been spent.\" To some, this seems to be protesting too much. The material costs of AMS-01 were just $33 million. AMS-01 used a lower-field permanent magnet built by the Chinese for $600,000, far cheaper than the superconducting Swiss toroid in the grown-up version, and there have been many other improvements. But can they really account for a hike in price to $1.5 billion? Simon Swordy, a cosmic-ray physicist at the University of Chicago in Illinois, takes what might be called a worldly view of the inflation: when a scientist initially sells a project, it should sound cheap; once it's built and ready to go, it's better to be expensive. \"Ting wants to say, 'You've already spent $1.5 billion on this, you've just got to fly it'.\" Ting responds by saying that the $1.5-billion figure is an extrapolation of a $1.2-billion NASA estimate made in 2005. That put the material costs of the AMS at $179 million; the other billion was in overheads, facility costs, or salaries, reflecting in part NASA's shift to 'full cost accounting'. Ting also points out that this cost estimate was made before the AMS was bumped off its shuttle flight, which is hard to square with the suggestion that he needed an artificially hefty price tag to get the AMS into space. Whatever the costs, though, they are now, as economists put it, sunk. They cannot be recouped whatever happens. In cost\u2013benefit terms, the costs that matter are those needed to get to the end of the project, not those incurred since its beginning. An extra shuttle flight squeezed into a crowded 2010 would cost between $300 million and $400 million, according to a 2008 NASA estimate. Doing it later is an order of magnitude costlier: renewing shuttle contracts for the 2011 fiscal year would cost $3 billion\u20134 billion. Engineers have explored other options for getting the AMS up \u2014 reconfiguring it as a free-flying satellite, for instance, or using the European Space Agency's Automated Transfer Vehicle to get it to the station. But at this stage, those options are not feasible, says Mark Sistilli, NASA's programme manager for the AMS.  \n                Waste of space shuttle? \n              Shuttle managers need 18 months lead time to prepare a shuttle. If the schedule to retire the shuttles by October 2010 is kept, NASA managers say they need to know in early 2009. \"Time is of the essence,\" says Sistilli. Hence Ting's regular visits to the Mayflower Hotel \u2014 part of a campaign that is meeting with new success. On 27 September, both houses of Congress passed a NASA authorization bill that specifically directs the agency to add another shuttle flight and to use it to take the AMS up to the space station. That bill is currently waiting for the president's signature or veto. Although the White House has so far been against such a flight, a veto is not seen as likely. The worry for NASA astrophysicists is that if the bill authorizing the extra flight is enacted into law, it is by no means certain that Congress's appropriations committees will come up with extra money to pay for it. If that were the case, the NASA astrophysics budget may have to pay for the AMS's ride, says Jon Morse, NASA's astrophysics division director. \"That is the risk that currently exists.\" If you want a bottom-line explanation of why the astrophysicists have not welcomed what they see as an un-peer-reviewed interloper, look no further. Congress could make the AMS flight a zero-sum game in which the $300 million\u2013400 million shuttle flight comes out of the $1.3 billion astrophysics budget, and other missions will be cancelled or delayed. The AMS team sees it from a different perspective. Ting has transcended the zero-sum game, using his political muscle to round up money, piecemeal, from the many international partners that would not otherwise have been available. The AMS scientists see the extra shuttle flight as not only the fulfilment of an original obligation, but also small compared with the $1.5-billion overall price tag. With regards to the potential robbing of the astrophysics budget, Ting says the original agreement was not with NASA's science division. \"There's no reason we should take money from them.\" That said, he is not in a position to make that call. And just as $300 million is small, say Ting and his colleagues, when compared with $1.5 billion, so $1.5 billion is small compared with $100 billion \u2014 a frequently bandied about figure for the total costs of the space-station programme. So far, they say, that astronomical sum has bought very little in terms of science: for 1% of the total, the AMS might go some way to redeeming that. \"Can you name me one single important discovery done there?\" asks Roberto Battiston, deputy principal investigator for the AMS. He recalls reading a list of 12 International Space Station highlights in 2007 that NASA published. It included the clubbing of a golf ball during a space walk, a stunt sponsored by a Canadian company; it did not include any science. \"Honestly, I think the station is there for something more than that.\" \n                     AMS at CERN \n                   \n                     AMS at MIT \n                   Reprints and Permissions"},
{"file_id": "4541042a", "url": "https://www.nature.com/articles/4541042a", "year": 2008, "authors": [{"name": "Anna Petherick"}], "parsed_as_year": "2006_or_before", "body": "If more than 90% of the genome is 'junk' then why do cells make so much RNA from it? Anna Petherick goes in search of some answers. HOTAIR is a molecule with a future. Created from a DNA sequence on human chromosome 12, it affects genes on chromosome 2, apparently working as part of the system that enables skin cells to tell where on the body's surface they are, and thus what they should be doing. Beyond these specifics, HOTAIR may also serve as a model for understanding a whole slew of similar molecules, the existence of which was not even dreamed of ten years ago and the function of which \u2014 if any \u2014 is still hotly debated. HOTAIR stands out because it is a long piece of RNA that doesn't encode a protein but still does something biologically important 1 . \"HOTAIR was a gem in a sea [of long RNAs],\" says John Rinn, a genome biologist who discovered the RNA while working at Stanford University in California. \"It told us little about what the bulk of these things are doing. For that, we can't even see a common trend.\" It is hard to comprehend the upheaval that RNA has been causing in molecular biology over the past few years. Once viewed as a passive intermediary, it was thought to faithfully carry genetic messages from the DNA sequence to the protein-making machinery, where things were made that actually got things done. Biologists were comfortable in the knowledge that only 1\u20132% of the human genome made protein-coding RNA in this way, and most of the rest was filler. So when, in 2005, geneticist Thomas Gingeras announced that some cells churn out RNA molecules from about 80% of their DNA, he astonished scientists attending the Biology of Genomes meeting at Cold Spring Harbor Laboratory in New York. Why should cells bother with so much manufacturing if, as it seemed, such a tiny fraction was involved in the important business of protein-making? Over the past three years or so the case for this 'pervasive transcription' has strengthened. The phenomenon has now been ascribed to mice, fruitflies, nematode worms and yeast. These studies, and Gingeras's original reports, came from microarrays \u2014 a technology that relies on the tendency of nucleic acids to find their complementary cousins in a solution. Gingeras works for the microarray manufacturer Affymetrix in Santa Clara, California. But not everyone has been persuaded of the extent of pervasive transcription, in part because microarrays are subject to background 'noise'. Even using no RNA, control chips will give off some signals, and results can be a matter of interpretation. For anyone who still doubts that the genomes of nucleated organisms are first and foremost RNA machines rather than protein-coding ones, sequence data are starting to provide \"ultimate information\", Gingeras says. There is something about the nitty gritty of nucleotide sequences that is enticingly reassuring to molecular biologists. New sequencing machines that can stream out data many times faster than their predecessors have made the mass sequencing of cellular transcripts possible. In 2008, this process was completed for two species of yeast 2 ,   3  using machines made by Illumina, based in San Diego, California. The results broadly agree with the microarray findings, showing transcription from 74% of the genome of brewer's yeast ( Saccharomyces cerevisiae ) and 90% from that of fission yeast ( Schizosaccharomyces pombe ). Gingeras and other researchers are now working to sequence all the RNA produced by 44 kinds of human cell as part of the Encylopedia of DNA Elements (ENCODE) project, which aims to identify all the functional parts of the human genome. At that point, any remaining sceptics will be able to overlay the many thousands of different human RNAs onto DNA regions from whence they came. At the end of this process, the covered regions will be those that give rise to RNA \u2014 and the uncovered ones, probably just a few naked holes. All this transcriptional accounting has hastened an already heady RNA rush. Even before the pervasive nature of transcription became clearer, molecular biologists had begun to trot out new classes of RNA molecules that are responsible for important happenings in cells. Thrust farthest into the limelight are the microRNAs (miRNAs), which stop the production of certain proteins, but they have been joined by a growing number of other RNA families, such as small nucleolar RNAs (snoRNAs) and Piwi-interacting RNAs (piRNAs), with vital roles in cellular and developmental processes \u2014 vital enough to earn the DNA that encodes them the label 'RNA genes'.  \n                The long and the short of it \n              On the whole, the established classes of RNAs are short molecules, around 20 or 30 nucleotides in length. The non-coding RNAs that Rinn has been championing run to 200 or even 10,000 bases apiece. The issue at the moment is whether, among this bounty of long RNAs, researchers will find anything as biologically meaningful as the shorter RNAs have proved to be. HOTAIR shows that some such molecules have function \u2014 but is it the exception or the rule? \"It's controversial whether these are mostly just noise or regulatory function,\" says J\u00fcrg B\u00e4hler of the Wellcome Trust Sanger Institute in Cambridge, UK, who led one of the yeast RNA sequencing projects. Those who doubt the importance of RNA bemoan their logical problem: it is impossible to prove lack of function. Even when an important cellular job does get pinned on a long RNA, as it did for HOTAIR, the doubters worry that it is too tempting to extrapolate across the board. Ewan Birney, a bioinformatician at the European Bioinformatics Institute and one of the leading scientists in ENCODE, says that the debate now is about what proportion of long RNAs serve a purpose. \"I used to be a much stronger sceptic three to four years ago,\" he says. \"Now I'm accepting that transcription is pretty complicated and that many transcripts are made that we don't understand. Where I still have some scepticism \u2014 what we still don't know \u2014 is what those transcripts do, if anything.\" John Mattick, the director of the Centre for Molecular Biology and Biotechnology at the University of Queensland in Brisbane, Australia, has no such qualms. He is a long-time advocate of non-coding RNA's importance. The doubters, he says, \"keep regressing to the most orthodox explanation [that the long RNAs are junk]. But they can't just sit on their intellectual backsides and tell us to prove it.\" But prove it is just what researchers are starting to do, with a growing number of examples that showcase these molecules' capabilities. The idea of long non-coding RNAs is not new. Xist, the most famous example, was discovered in 1991. Its 17,000 nucleotides can be found in almost every cell of mice and humans, where it obviates gene expression along an entire X chromosome. Because females have two Xs to their male (XY) counterparts' one, they use Xist to switch off the extra X and compensate for the disparity.  \n                Varied roles \n              Xist RNA is transcribed from the chromosome it mutes, and coats it along its length. No one really knows exactly how it attaches and what makes it so effective at gene silencing. What is clear, however, is that part of the molecule attracts chromatin remodelling complexes \u2014 enzymes that turn genes on and off by tinkering with DNA's packaging. Get enough of these complexes together, and it seems that you can turn off a whole chromosome. Over the past few years, the RNA field has compiled a brief list of other long non-coding RNAs. Many of those that have been studied control the activity of protein-coding genes. As the pace of these discoveries has picked up, they have revealed that long RNAs can control genes in a surprising variety of ways, from both near and far, and that their function is not necessarily dependent on the exact sequence of the RNA, as it is when RNA is coding for proteins. This suggests that scientists have only begun to appreciate what RNA is capable of. In one example published last year, molecular biologist Igor Martianov and his colleagues at the University of Oxford, UK, studied the human gene for dihydrofolate reductase, an enzyme involved in biochemical syntheses that has two 'on' switches for protein production. They discovered that the first of these switches actually triggers the manufacture of a 583-nucleotide-long RNA molecule, and that this RNA directly interferes with the second switch. When this happens, the enzyme is no longer made 4 . Working in a very different way, a long RNA called NRON seems to travel to the cytoplasm in order to influence the expression of protein-coding genes. Several thousand nucleotides long, NRON polices the trafficking of a transcription factor from the cytoplasm into the nucleus of the cells where it is active 5 . By doing so, it seems to control the transcription factor's activities, which include regulating T cells' immune response. When Rinn discovered HOTAIR, it reinforced the idea that RNAs could be shuttling around the genome doing important jobs. Rinn was studying skin-cell lines cultured from the finger, foot, foreskin and eight other sites on the human body, trying to find out how these cells know their position. HOTAIR, which stretches for nearly 2,200 nucleotides, is produced from within a cluster of the  HOX   genes that specify an early embryo's head end and foot end, as well as the order of the body segments in between. When Rinn found that this RNA affects the output of genes on chromosome 2, it was the first time such a cross-chromosome influence had been found. When he lowered levels of the RNA molecule, the activity of  HOX   genes on chromosome 2 jumped, and foreskin cells started behaving in an unusual way 1 . Rinn initially wanted to name the molecule STAR1. The acronym for 'Suz-Twelve Associated RNA' refers to the enzyme that ferries this molecule from one chromosome to another, and the number one reflected Rinn's optimism that there are likely to be more STARs. But Rinn's lab partner, Howard Chang, wanted a \"more humbling\" name, Rinn says, and they settled on HOTAIR instead (for  HOX   antisense intergenic RNA). \"Howard was right, but I think we are still both in search of more stars, not hot air,\" says Rinn, now at the Broad Institute in Cambridge, Massachusetts. As Rinn has said, there is a vast sea of long RNAs out there. The ones with functions already ascribed to them comprise just a minuscule fraction, and those seem to be regulating genes by very diverse means. To many, this lack of common function infers that science has only scratched the surface of the diversity of long RNAs. The massive scale on which transcription is taking place could be the least of biologists' problems compared with its mind-boggling functional complexity. What is needed, researchers say, is more data to show that RNAs do something useful on the genomic scale \u2014 but those data are proving remarkably difficult to collect. One problem, when it comes to surveying RNA's usefulness, is that sequence does not provide any simple indicator of function. The sequence of non-coding RNA is not conserved between species in the same way that it is for protein-coding genes. If a sequence is doing something important for an organism because of the protein it codes for, then evolution is likely to have kept that region more constant across related species compared with any average stretch. But the same isn't true of RNA, which does not necessarily pair up with a complementary nucleotide sequence at all. Xist is not conserved in this way, nor are any of the other non-coding RNA stars along their full lengths. Another way to seek evidence of function en masse is to get rid of long non-coding RNAs and watch how animals cope. But such an experiment may produce only subtle changes in an organism as a whole, and could still miss the importance of a transcript. \"I think the cell will use these transcripts at very different times and in very different cell types and conditions,\" Gingeras says. \"You may need to see them in a very specific context to see the function.\" That is what J\u00fcrgen Brosius of the University of M\u00fcnster, Germany, and his colleagues found when they removed a 150-nucleotide RNA from mouse neurons, where it is normally transported down the cellular fingers that communicate with other cells 6 . The engineered animals looked and acted more or less the same as the control animals \u2014 but Brosius says that on close inspection they weren't as inquisitive and had unusual exploratory behaviours. Such activity might be lethal in the wild, Mattick says, \"but it was affecting their behaviour in ways that were far too subtle to be assessed in a cage\".  \n                In search of function \n              If slicing out non-coding RNA doesn't often reveal its function, then perhaps looking at its lifespan will. This vein of thinking brings a potentially bigger blow for RNA's believers than the knockout studies: the possibility that cells are destroying long RNAs almost as fast as they are making them. Studies in yeast have shown that many long RNAs seem to be so rapidly gobbled by the nuclear exosome \u2014 a protein complex that degrades RNA \u2014 that it is hard to imagine them having any function at all. Some are labelled for destruction as soon as they peel away from their DNA blueprint. David Tollervey, who studies RNA processing at the Wellcome Trust Centre for Cell Biology in Edinburgh, UK, says that long RNAs could have almost-instant effects or cells might be making many long RNAs merely to show that they've done so. In other words, the point of the exercise might be transcription itself, rather than the transcript. There are already known examples in which RNA production seems more important than the actual product. In 2004, Fred Winston and his colleagues at Harvard Medical School in Boston, Massachusetts, studied a 551 nucleotide RNA called SRG1 that is made by brewer's yeast 7 . It switches on and off the adjacent gene  SER3 , which helps make serine (an amino acid that the yeast needs to be healthy). But in this case it is the process of making the non-coding RNA that regulates  SER3 , rather than the RNA itself. The trick here is that the DNA sequence from which SRG1 is transcribed runs through the on switch for  SER3 . So when a yeast cell is manufacturing a lot of RNA for SRG1, it blocks access to the  SER3   switch. This is what happens when the yeast sits happily in a flask of rich medium and has no need to generate its own serine. In his transcriptional surveys of humans, Gingeras has shown that about three times as many transcripts carry a molecular label for rapid destruction than do not carry one. But Gingeras thinks that these apparently doomed RNAs still do more for cells than just getting made. When a map of pervasive transcription is overlaid with a map of short non-coding RNAs, such as microRNAs, the two overlap 8 . Gingeras thinks that the short RNAs are frequently embedded within the longer transcripts, and then excised. Over the past few years, Mattick has been gathering other circumstantial evidence that long RNAs have widespread function. In a paper published in January, he and his team examined 1,328 non-coding RNAs whose expression patterns had been mapped in the Allen Brain Atlas, but the functions of which were unknown. The team found that nearly two-thirds of these molecules were produced in specific regions of the mouse brain \u2014 in certain cell types or in specific parts of neurons 9 . More recently, Mattick's team identified 174 non-coding RNAs that are expressed in mouse embryonic stem cells in a decidedly selective manner, either correlating with the cells' capacity to develop into any other cell type or with particular events along the path to specialization 10 . \"You've only got two alternatives,\" Mattick concludes. \"Either there's a hell of a lot of developmentally regulated transcriptional noise, or these RNAs are sending signals into the system.\" This approach should gain more steam as part of ENCODE. The next-generation sequencers have been chugging away since the end of last year, and in 2009 should lay out the sequences of all the RNA molecules manufactured by two types of human cell. When the project eventually delivers transcriptomes for all 44 cell types, it will allow a closer analysis of when different sorts of human cell make different long RNAs and help infer something about their function. As for Rinn, he already has evidence that non-coding RNAs are so much more than hot air. In May, at this year's Biology of Genomes meeting, he presented work suggesting that there are as many as 2,000 long non-coding RNAs in human cells that shoulder biological responsibilities on a par with those of HOTAIR and that may therefore earn the status of RNA genes. To find these, Rinn and Manolis Kellis, a computational biologist also at the Broad Institute, searched for sequences that are conserved as might befit a working stretch of RNA. They assumed that much of an RNA molecule's function depends on the three-dimensional architecture that the single-stranded molecule folds into. This, rather than the precise sequence of nucleotides, is what evolution will have worked to preserve. This means that an A can become a T, for example, as long as the T to which it anneals when the molecule folds switches in turn to an A and providing that the overall shape of the molecule is unchanged. Using these types of bioinformatic rules, the team pulled out probable RNA genes. For a sample of these, they took a stab at predicting function and then tested whether the RNA's production was induced by certain cellular pathways. Many of them were. If their results hold up, Rinn and Kellis will have discovered the first large class of long RNA genes. \"These RNAs could have functions as diverse as those of protein-coding genes,\" Rinn says. And it is not such a stretch to think that they could rival the 20,000-odd protein-coding genes in number, if there are other, as yet unidentified groups of long RNA genes out there. That still leaves a lot of the transcriptional hairball unaccounted for, and it is possible that much of it is still noise. \"With all this pervasive transcription,\" Rinn says, \"the problem to working out whether most of it is functional or not has been that people simply haven't known where to start.\" Now, perhaps, they do.\n Anna Petherick is  Nature  's Research Highlights editor. \n                     ENCODE \n                   \n                     John Rinn's homepage \n                   \n                     Ewan Birney's homepage \n                   Reprints and Permissions"},
{"file_id": "454934a", "url": "https://www.nature.com/articles/454934a", "year": 2008, "authors": [{"name": "Tony Scully"}], "parsed_as_year": "2006_or_before", "body": "When jumbo Humboldt squid disappeared from Chilean waters, it led to the demise of a world-class electrophysiology laboratory. Now the creatures are back, finds Tony Scully, and so are the scientists. It was three years ago that electrophysiologist Francisco Bezanilla heard that the squid were back. That summer he had already travelled from the University of California, Los Angeles, to the Woods Hole Marine Laboratory, Massachusetts, in pursuit of the creatures. Like other researchers, he spent two months there each year when the Atlantic or longfin inshore squid,  Loligo pealeii,  bred in local waters. He found their giant nerve cells the best biological preparation for studying the electrical signals that cross the cell membrane. But he still struggled to isolate the weak signal generated by the flow of potassium ions. So when Bezanilla heard what fellow Chilean Miguel Holmgren had to say, his ears pricked up. Holmgren, a neuroscientist at the National Institutes of Health (NIH) in Bethesda, Maryland, had word from family back home that the jumbo Humboldt squid,  Dosidicus gigas , were in plentiful supply at fish markets throughout Chile. Memories from more than 40 years ago came flooding back to Bezanilla: of his old mentor, Mario Luxoro; of the bustling Laboratory of Cell Physiology in Montemar, Chile, that lured scientists from all over the world; and of long summer days spent hunched over the squid's spaghetti-like nerve axons. Bezanilla and other researchers had been forced to leave Montemar when the Humboldt squid mysteriously disappeared from local waters in 1970. Bezanilla thought that the squid's return might allow him to resurrect the old laboratory, and that the squid's giant axons \u2014 on average twice the diameter of its Atlantic cousin and loaded with many more ion pumps \u2014 might generate the stronger potassium signal he sought. In January this year, Bezanilla and Holmgren travelled to the small seaside town of Montemar. They found the old laboratory succumbing to the ravages of time: cracks ran through the walls and a layer of grime covered every surface. \"It was so sad to see my old laboratory in this state,\" says Bezanilla, who now works at the University of Chicago, Illinois. \"The place looked awful.\" The two men scrubbed down the benches and unpacked their equipment. They paid local fisherman to take a trip out in search of the squid. In anticipation, they rigged up their electronics, ready to dissect the Humboldt's axons and, in doing so, continue a quest to understand a neuron's electrical properties that began over a century ago.  \n                Nervous steps \n              In 1902, German physiologist Julius Bernstein was the first to measure the minute electrical charges on nerve axons. He proposed that the cell membrane somehow blocks the movement of ions and maintains a difference in voltage so that the inside of axons is negative relative to the outside. That changes when a signal propagates along the nerve. The barrier opens, negatively charged potassium ions leak out and an 'action potential' \u2014 a wave of voltage he estimated to be 70 millivolts \u2014 whips along the length of the nerve. But at that time Bernstein was unable to directly measure the amplitude of the action potential because the frog nerves he was experimenting with were too small to allow him to insert an electrode inside the cell. Researchers took to experimenting on the cells of marine life such as the common shore crab and algae. Both were in plentiful supply and considerably easier to handle than frogs \u2014 plus working with crabs provided an excellent working lunch during long days in the lab. Electrophysiologists attached themselves to marine laboratories, of which, by the 1930s, the two most prominent were at Woods Hole and Plymouth, UK. Kenneth Cole from Columbia University, New York, for example, would spend the summer at Woods Hole experimenting with the large excitable cells of the sea algae  Nitella . It was British zoolologist John Zachary Young who first proposed experimenting on  L. pealeii . Squid are the one of the fastest swimmers in the ocean: giant axons that run the length of the body trigger muscular pulses that propel the creature forward. Young urged Cole to take advantage of these axons, which are up to 1 millimetre in diameter, 50 times the thickness of axons found in the common shore crab and 1,000 times that of human axons. Cole found that he could thread a wire electrode right into the cell without disrupting the electrical signal propagating down the axon. Using this 'voltage clamp' technique, with electrodes both inside and outside the cell, it was possible to measure the current as ions move across the membrane during a nerve impulse.  \n                Giant bandwagon \n              Others were quick to catch on. In 1938, Alan Hodgkin visited the lab in Woods Hole \"because some scientists have been getting the most exciting results on the giant nerve fibres of the squid\", he wrote at the time. \"Cole has been getting results which make everyone else's look silly.\" \n               boxed-text \n             The Second World War interrupted work, but soon after it ended Hodgkin and his long-time collaborator Andrew Huxley sought a local supply of  L. pealeii  in Plymouth. There were a few barren years before fisherman finally delivered a major catch in 1949. In the late summer months they made discoveries using these axons that would overturn Bernstein's cellular membrane theory. Hodgkin and Huxley showed that during an action potential the voltage across the cell membrane changed by 110 millivolts (ranging from \u221270 to +40), not the 70 millivolts Bernstein had predicted, and that ions do not simply equilibriate across the membrane. The movement of ions is controlled by selective pores, or gates, and during an action potential the positive sodium ions rush inside the cell followed almost immediately by an outpouring of potassium ions. Hodgkin and Huxley shared the 1963 Nobel prize in Physiology or Medicine with Australian John Eccles for discovering how nerve signals travel. Hodgkin later joked that their prize should have gone to the squid. L. pealeii  was big, but for some it wasn't big enough. At the Massachusetts Institute of Technology in Cambridge, Francis Otto Schmitt wanted to identify the components of axoplasm, the fluid inside nerve cells. It could be squeezed out of the giant squid axon as simply as toothpaste from a tube \u2014 but even then it was problematic because the squid rarely grows longer than a metre and its axon is highly branched. In a 1955 interview with the  New York Times , Schmitt also bemoaned the limited supply: only 300 squid per week for the annual two month breeding season. As luck would have it, multimillionaire and champion sports-fisherman Lou Marron happened to read that report. As is the wont of fishermen, he knew of a catch twice as big. He wrote to Schmitt about a species of squid he had used to bait swordfish on expeditions in the Pacific Ocean off the coast of north Chile. He offered to take him there to prove it. Two years later, the pair set off for Chile with a small team including Schmitt's Chilean PhD student, Mario Luxoro. \"I told Schmitt we had huge squid in Chile,\" Luxoro says, but Schmitt had been sceptical of their existence. \"He thought I was just an exaggerating South American.\" Still, the prospect of a bountiful supply of squid during the winter months in Massachusetts was appealing to Schmitt. And Luxoro felt vindicated when the team landed the jumbo squid now known to be the Humboldt. Named after the Humboldt ocean current that runs the length of South America, this squid grows up to two metres in length, making it one of the largest squid species in the world and twice the size of their Atlantic cousins that had been the mainstay of electrophysiologists. The University of Chile in Santiago ran a marine-biology laboratory in Montemar and Schmitt leased a small room there before returning to Massachusetts. Luxoro remained behind, hired by Schmitt to organize the dissection, preparation and packaging of the squid axons for transport to the United States. The proteins inside degrade quickly, so the preparations of squid axoplasm were packaged into boxes with dry-ice for transport back to the United States by air. Even so, few of the packages arrived in a suitable condition for analysis. Both men were unhappy with the arrangement. The lab \"was only a factory for axoplasm\", recalls Luxoro, who was frustrated at spending December to February packing squid. \"They were not interested in us doing pure research.\"  \n                Birth of a lab \n              Luxoro felt that it was time for Chileans to do their own experiments on the valuable asset swimming nearby. With other Chilean researchers, he persuaded the chancellor of the University of Chile to buy a separate house for scientists wishing to study electrophysiology. They were given a former brothel that had a water supply in every room. In 1962, the Laboratory of Cell Physiology was born. By now, scientists were trying to understand how different channels mediate ion transport across the axon membrane. The team grew to a dozen or so scientists and went on to compete with laboratories around the world. With little money and relatively crude equipment, the researchers could nevertheless ask sophisticated questions because they had an abundance of the best experimental material: the large surface area of the giant axons generated electrical signals strong enough to measure accurately. Luxoro and his student Eduardo Rojas, for example, were the first to provide experimental evidence that proteins within the lipid membrane, rather than the lipid molecules themselves, actively transport ions across the membrane 1 . It was also in Montemar that Rojas and Clay Armstrong demonstrated that movement of sodium and potassium ions during an action potential occurs via different membrane channels 2  rather than a single protein, as some had argued. The new laboratory, loosely led by Luxoro, enjoyed much independence. \"It was a great time, when we had squid we would work from eight in the morning till two in the morning,\" he says. Electrophysiologists flocked to Montemar during the Southern Hemisphere summer. \"I loved it there,\" says Armstrong, a physiologist recently retired from the University of Pennsylvania, Philadelphia. \"They were ingenious at setting up experiments using old parts. It was great fun to watch.\" During this time, a young and impressionable Bezanilla began to dissect the squid and later complete a PhD. Then, in the summer of 1970, the fishermen returned empty handed day after day. Just over a decade after Schmitt and Luxoro made their expedition to Chile, the Humboldt squid migrated out of range of local fishermen, and beyond the reach of scientists in Montemar. The squid probably followed small fish, krill and other prey, which are sensitive to environmental events such as El Ni\u00f1o.  \n                Gradual decline \n              It was the beginning of the end for Montemar as an international research station. Many left to fill posts elsewhere: Bezanilla was offered a postdoc at the NIH. The dictatorship of Augusto Pinochet from 1973 further deprived Chilean science of investment. \"It was a horrible time,\" says Luxoro, who remained in Montemar, using barnacles ( Megabalanus psittacus ) to study muscle physiology. \"A lot of people went away and science practically disappeared.\" By the end of the 1970s, the invention of the 'patch-clamp' method by Bert Sakmann and Erwin Neher had removed much of the need for large axons. The technique involves pressing a glass tube one thousandth of a millimetre in diameter against the cell membrane, isolating a small patch from the surrounding extracellular fluid. An ultrasensitive amplifier can be used to measure the flow of ions across the membrane. In 1990, Luxoro reluctantly accepted a teaching position at the main campus in Santiago and the laboratory was abandoned. But for some purposes, squid remains irreplaceable. David Gadsby, a membrane physiologist at Rockefeller University in New York, works alongside Bezanilla and Holmgren at Woods Hole every summer to study the action of ion channels in the North Atlantic squid. \"We now have three-dimensional static images of the protein pumps and ion channels, but to understand how the conformation of these proteins change in response to varying voltages we need to use the squid,\" says Gadsby. Much has been learned about how a sodium\u2013potassium pump actively transports sodium ions across the membrane to maintain the resting potential, but the movement of potassium ions is harder to measure. Bezanilla and Holmgren want to study the conformational changes in the pump as potassium ions are transported, and hoped that this would be clearer from variations in the Humboldt axon's strong potassium signal. It was for this reason, and a wish to revive his old laboratory, that Bezanilla returned early this year to Montemar with Holmgren and paid the fishermen to bring in a catch. \"We normally pay US$1,700 for a delivery in Woods Hole but here we only had to pay $100,\" says Holmgren. Their trip and the investment were rewarded: to their great relief the first consignment of squid in nearly four decades arrived at the laboratory. In their first experiments, the scientists tried to wash away the sodium ions in order to measure the weaker potassium ion signal alone. \"We found that the signal-to-noise-ratio of the ion is improved by a factor of five or six with the Humboldt squid compared with the squid we normally use,\" says Holmgren. One explanation may be that the thicker membrane contains a higher concentration of ion channels and pumps per unit area.  \n                Rebuilding the dream \n              After spending two weeks working at the laboratory, the scientists returned to the United States bolstered by their preliminary results \u2014 and determined to return. They are now applying for funding to go back to Montemar in December. Meanwhile, Ram\u00f3n Latorre, a neuroscientist who also began his career in Montemar and is now with the University of Valpara\u00edso, is in negotiations with the University of Chile to renovate the lab for visiting scientists. Lattore is applying for a half-million-dollar grant from the local government, which would fit out a conference room and three laboratories. Gadsby hopes to make his first trip to Montemar later this year. Humboldt squid are now found as far north as California, but the return of the squid to the waters near Montemar has a special significance for electrophysiologists. \"I've seen old photographs of the lab and I can't wait to see it for real,\" Gadsby says. Whether the squid will be there to greet them, though, is beyond the scientists' control. \"We're dependent on the migratory patterns of the squid,\" Gadsby says. The researchers are well aware that the creatures could vanish as mysteriously as they appeared \u2014 taking with them the hopes for their experiments and the lab's revival. As long as the creatures stick around, Bezanilla is hopeful that the lab will return to its glory days as an international research hub. \"It was a great feeling to see the place come alive again,\" he says. \"I hope that in the future people will realize the importance's of this laboratory, and that the Humboldt squid will once again attract researchers back to Chile's shore.\" Tony Scully has just finished an internship in  Nature 's Munich office. \n                     Woods Hole Marine Biological Laboratory homepage \n                   \n                     University of Chile \n                   \n                     Chris Miller, Brandeis University \n                   \n                     David Gadsby \n                   Reprints and Permissions"},
{"file_id": "454565a", "url": "https://www.nature.com/articles/454565a", "year": 2008, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "Researchers trying to develop an HIV vaccine have endured two decades of setbacks. Erika Check Hayden meets a veteran still engaged in the fight \u2014 and a rookie willing to join in anyway. As Larry Corey boarded a plane on the night of 18 September 2007, he was in shock. For four years, he had been in charge of clinical trials of a vaccine against HIV. Although previous vaccines had failed, Corey was optimistic that this one might work because it took a new approach. He thought it was the best hope against HIV. But that afternoon, Corey had met with scientists who had reviewed the early trial results. Not only did the vaccine not work, they told him, it might actually have made some people in the trial, called the STEP Study, more vulnerable to infection. On the long flight from Chicago home to Seattle that night, Corey cycled through stages of grief, regret and resolve. He had led the search for an HIV vaccine since its inception more than two decades ago. Now, as head of the flagship HIV Vaccine Trials Network run by the US National Institutes of Health (NIH), Corey had to make some difficult decisions. He had to decide what to tell the trial volunteers, and he had to think about the millions of other people who might contract HIV in the future. What could be done to make the vaccine that they so desperately needed? As Corey pondered these issues, another HIV researcher was mulling over a decision of her own. Colleen Doyle is 32 years younger than Corey and had earned her PhD only three months before. Yet she had just received a job offer from the University of Chicago. The position was a rare and precious opportunity for such a young scientist. But there was a catch: if she took the job, she would have to stop her investigations of how a vaccine could arm the body against HIV. Both Corey and Doyle had reached turning points, and their decisions could prove crucial to the search for a vaccine. The disappointing failure of the STEP trial marked one of the most difficult moments in HIV vaccine history. It told Corey and other scientists that after decades of work, they still did not understand how to prevent HIV from overwhelming the immune system. And critics were calling for an end to the vaccine hunt, so that some of the money \u2014 more than US$900 million was spent last year (see graph) \u2014 could go towards drug treatments. But where some see an end to the field, Corey and other researchers see a new beginning. They say that basic research will eventually answer the outstanding questions about HIV, if talented researchers like Doyle can be coaxed into the fight. \"We need a period of solid, quiet science, even if it takes a decade,\" says virologist and long-time HIV vaccine researcher John Moore from Weill Cornell Medical College in New York. With this resolution, veteran vaccine researchers are facing a new reality: they will not be the ones to end HIV. They must now pass the baton to a new generation of scientists. \"The real next step is going to come from outside this room,\" declared Adel Mahmoud of Princeton University in New Jersey, at an HIV vaccine summit this March in Bethesda, Maryland. But Doyle had watched the first generation of vaccine hunters endure decades of frustration. So the question she faced last September was, did she really want to heed that call? Born in 1947, Corey grew up in Detroit and decided at age 10 to become a doctor. As an undergraduate at the University of Michigan in Ann Arbor, Corey started dating Amy Glasser, whose dad had just finished supervising the field trials of the Salk polio vaccine. So when Amy and Larry were married in 1969, Jonas Salk was a guest at their wedding. No one foresaw that Corey would one day emulate Salk's quest to vaccinate humanity against a terrifying viral scourge; at the time, Larry was leaning towards cardiology. That changed in 1975, when Corey began a fellowship with King Holmes, an expert in sexually transmitted infections at the University of Washington in Seattle, and discovered that he loved research. \"That moment of discovery is a wonderful thing,\" he says, sitting in his office decades later at the Fred Hutchinson Cancer Research Center in Seattle. Now a lanky, grey-haired 61-year-old Erika Check Hayden is a senior reporter for  Nature  based in San Francisco.  See Editorial,  page 551. \n                     HIV vaccine trials network \n                   \n                     NIAID \n                   \n                     AIDS Vaccine Advocacy Coalition \n                   \n                     International AIDS Vaccine Initiative \n                   \n                     Global HIV Vaccine Enterprise \n                   Reprints and Permissions"},
{"file_id": "454930a", "url": "https://www.nature.com/articles/454930a", "year": 2008, "authors": [{"name": "Jerry Guo"}], "parsed_as_year": "2006_or_before", "body": "Recent eruptions and field expeditions may herald a return to glory for the Son of Krakatau. Jerry Guo explores what the 78-year-old island has to offer. Sprouting 300 metres above the sea, the cone of Anak Krakatau teases voyagers with anticipation. The volcano spews hot steam from an off-centre crater, as the surf crashes against the lava flows that have cooled around the base. In this contest between water and rock, the rock is winning. As the island gains a foothold in the vast Pacific, it provides scientists with a rare glimpse at how life takes hold in a newborn ecosystem. Anak Krakatau \u2014 'child of Krakatau' in Indonesian \u2014 is one of the most striking geological oddities of the modern era: an island that first rose up out of the sea in 1927 and receded and reappeared three times until it established itself permanently above sea level in 1930. It has arisen from the centre of the deep underwater caldera left by the original Krakatau volcano, which blew itself to pieces in 1883 in one of the most powerful and devastating eruptions in recorded history. The explosions and the tsunamis they created killed 36,000 people. This patch of rock three kilometres square and sandwiched in the strait between Java and Sumatra has earned itself a reputation as a laboratory for observing how life arrives, endures and perishes on an island. In the 1980s and early 1990s, research conducted there on the 170 plant, 40 bird and dozens of bat and insect species, came to define much of what is known about island colonization, ecological disturbance and community dynamics. Anak Krakatau became the testing ground \u2014 and flashpoint \u2014 for many textbook principles of island biogeography. Then interest began to wane. Scientists packed their bags and turned to other hotspots \u2014 the Galapagos, Seychelles, another newborn island in Iceland. \"The work that was easy was quickly done,\" explains Richard Field, an ecologist at the University of Nottingham, UK. Research on Anak went through a 'quiet phase'. Even volcanic activity eased up. But a slew of research projects, led by teams from Britain, Germany, Japan and Indonesia, could mark the start of a second age for the island. \"Anak still has a lot to teach us,\" says Field. \"It's one of the classic natural experiments.\" Eruptions throughout November \u2014 the most violent in more than a decade \u2014 recaptured the world's attention with news that Anak was rattling windowpanes in western Java, some 40 kilometres away, and producing 3-kilometre-high ash plumes and spewing showers of lava bombs. The volcano has remained active ever since, with eruptions this January, April, June and July, according to Australia's Darwin Volcanic Ash Advisory Centre and Indonesia's Center of Volcanology and Geological Hazard Mitigation. It is during a lull between eruptions that Tukirin Partomihardjo, a wiry botanist wearing a neatly pressed white-collared dress shirt and sporting a machete wades onto the black-sand beach at Anak. The 56-year-old 'King of Krakatau' has arrived. It's hard to take a step on Anak without crushing some sort of life: blue-ish ghost crabs, armies of ants, fast-multiplying casuarina seedlings and even a coconut that has sprouted an impressive-looking stem. These are some of Anak's colonizers, which every day continue to launch attacks on the island. \"There is a constant struggle here,\" says Partomihardjo, who works for Indonesia's Bogor Herbarium, referring to these pioneering species' battle with the elements. Much of what biologists can say about the fate of such species came from data on Anak. The textbook case-study of this 'species succession' began just 9 months after the final 1930 eruption, when a single spider was found clinging to a rock. Fungi and other microbes soon followed and, in a decade, the lower slopes began to be covered with a grassland savannah that was dominated by sugarcane. Life brought more life. At some point, seed dispersal by animals \u2014 land crabs, monitor lizards and countless species of birds \u2014 outpaced that by sea. The emerging mixed forest, with its nooks and perches, attracted more birds and ultimately bats, explains Field. Wind-dispersed species, such as orchids, then moved farther inland. The rest is history. \"Anak played a big part in helping people understand colonizing processes,\" says Field. In the early 1990s, Partomihardjo showed that banks of seeds that had been buried by ash or by crabs on the shore for as long as six decades could contribute to the first waves of life on a barren landscape 1 . Now, one of his ongoing projects is to examine the diversity of the beach seed bank. In just one day, he finds some 30 species of seed of varying descriptions: green cactus-like, flat and pea-shaped, walnut-shaped. Blown or having floated in from the nearby 'stepping stone' islands as well as from Java and Sumatra, these specimens litter the beach. Not all will survive. Partomihardjo holds up a dark, oblong mangrove seed and shakes his head. \"This can't survive here.\" The rocky, wave-dashed coast is just too inhospitable for the plant to take root and thrive.  \n                Death and rebirth \n              Heading into a modest jungle on the eastern side of the island, Partomihardjo walks through a panoply of dominant casuarinas, waist-high grasses, ferns and 30-metre-high fig trees. It's hard to imagine that this was all under water a mere 78 years ago. The forest has bald spots, though, where recent lava flows have wiped out the newly emergent vegetation. \"What's unusual is that the island keeps getting disturbed by volcanic activity,\" says Field. These eruptions can reset the clock on the succession process, yielding valuable data on ecological disturbance and recovery. But the eruptions also make continuous monitoring difficult, if not life-threatening. One of Partomihardjo's colleagues died on the island during a monitoring expedition in the 1990s. Last September, a team of 11 field biologists, led by Partomihardjo and Field, erected a network of 21 monitoring plots on Anak. They tagged and mapped every tree within the 20 \u00d7 20 metre plots to gather long-term data on the forest's health and succession dynamics. The eruptions have wreaked havoc on many parts of the island. \"Many trees have died,\" he says, blaming the ash that has covered much of the forest. \"But this place will recover.\" Partomihardjo is the world's leading expert on the Krakatau islands. He first visited them in 1981 during a training seminar, and has been back some 30 times since, leading almost all foreign research expeditions there. \"He has spent longer working on Anak than any other biologist living or dead and knows it more intimately than anyone else,\" says Robert Whittaker, an ecologist at the University of Oxford, UK, who is himself an authority on the islands. Research at Anak has been pushing ahead on numerous fronts. This March, a consortium of Japanese research groups led by Kagoshima University completed a three-year study of insect communities on Anak, including wasps, bees, ants, termites and galling arthropods. The team found 16 species of wasp and 18 species of ant that had not previously been documented on the island, and hopes to publish its findings soon. \"Anak is still very interesting for us,\" says Suzuki Eizi. \"We need to monitor it for a long period.\"  \n                Step by step \n             Field's team also started surveying trees, birds and ants last year, and have several papers in preparation. One important finding, for instance, is that colonization and extinction, at least in plants and birds, could be less random than previously thought. In 1967, Robert MacArthur and E. O. Wilson cited the randomness of bird colonization on Krakatau as a prime example for their classic theory of island biogeography \u2014 that species diversity grows along a smooth curve to ultimately reach equilibrium. Research conducted by Whittaker on Krakatau in the 1990s \u2014 and picking up again now \u2014 suggests that the equilibrium model may be too simplistic. \"Overall, a complex picture of ecosystem assembly is emerging from the Krakatau islands, one that suggests that island colonization is essentially a special case of succession,\" explains Field, who subscribes to the emerging school of thought that island biogeography is in part a deterministic process 2 . Take Anak's fig trees. Each species depends on specific birds, bats and wasps for seed dispersal and reproduction. \"You can't get figs without the wasps and you can't get the wasps without the figs. Our results don't fit with the MacArthur\u2013Wilson idea that both colonization and extinction are equally likely for any species,\" says Field. Furthermore, later research showed that colonization seemed not to follow another commonly accepted theory that terrestrial life comes from nearby 'stepping stone' islands, perhaps because the environments were so different. One study 3  revealed that Old World fruit bats can retain viable seeds in their guts long enough to transport them hundreds of kilometres, skipping over small nearby islands in the process. Geologists, too, have been showing an interest in Anak's activity. In 2005, German and Indonesian volcanologists rigged the island with three remote monitoring stations. They hope to use sophisticated neural network programs 4  to analyse the dizzying array of data \u2014 meteorological, chemical, seismic \u2014 recorded by the stations to predict future eruptions. The raw numbers are also broadcast online in real-time, including a video feed.  \n                Rewriting history \n              Meanwhile, Ken Wohletz, a volcanologist from Los Alamos National Laboratory in New Mexico, wants to restart a controversial project that claimed to have found a possible mega-eruption in Krakatau's past. He cites evidence from tree rings in Europe, ice cores in Greenland and ash deposits from the Sundra Straits that suggest a massive eruption in AD 535 \u2014 20 times bigger than the 1883 event \u2014 precipitated the Dark Ages through \"climate stabilization lasting years or perhaps decades\". He theorizes that the AD 535 eruption could have dramatically altered the climate, affecting crops and the spread of disease, and triggering the collapse of several civilizations in Indonesia, South America and Persia, but concedes that the supporting data, for the moment, are sketchy. \"It will be hard to find more evidence to date such an eruption,\" Wohletz says. For one, his radiocarbon datings yielded a wide-open time window: from 6600 BC to AD 1215; critics also point to the Byzantine Empire, which went through a Golden Age during this time, as a counterexample of a global doomsday. \"It will take years and years to put together the full story,\" he says. Such attention is a testament of the singular importance of the Krakatau islands to the scientific community, given that 'new islands' appear (and disappear) with some regularity. Since Anak Krakatau achieved permanent status in 1930, several new additions have been made to this geological family (see  'Island uprising' ). The only new island to receive anywhere close to Anak's scientific scrutiny has been Surtsey, off the coast of Iceland, a volcano strikingly similar in shape, size and geology to Anak. Despite its inaccessible location, Surtsey has a leg up on Anak in one crucial aspect: it has an 18-year-old continuous data record based on 25 permanent plots, with monitoring trips every July. \"The colonization on Anak has not been followed as frequently as here,\" says Borgthor Magnusson, a plant ecologist from the Icelandic Institute of Natural History in Reykjavik and leader of biological research on the island. Although colonization of Surtsey has followed the classic path taken by Anak in many respects, Icelandic biologists have, in the past two decades, picked up on a distinct difference: the importance of birds to the island's health and growth. \"The gull colony [some 200 breeding pairs] has a big impact,\" says Magnusson, citing its role in seed transport, soil fertilization and habitat creation. \"We're still experiencing this phase of colonization.\" Back at Anak, the Sun is about to set. Partomihardjo eyes the summit indecisively \u2014 a longstanding government directive warns people against going near the active cone. \"Why not,\" he resolves. \"Let's go.\" As he climbs nimbly towards the top, he recounts one particularly close call back in the 1990s, when he got caught during an eruption. \"Big stones were falling all around me,\" he says. Then he pauses and points to an old monitoring station nearby that was crushed by lava bombs the size of basketballs. \"I thought I was going to die.\" At dusk, he almost reaches the crater ridge (which he claims has grown 100 metres since his visit last October). But without a gas mask, he decides to turn back. Never mind the scorching ground. Still, even in this desolate landscape, with its crags and loose soil, he notices a rousing sight on the way down: a single pioneering shrub clinging bravely to life, its purple flowers in full bloom. Jerry Guo is a freelance writer in New Haven, Connecticut, reporting from Indonesia. \n                     Timeline 1880s: First reports on the 1883 eruption of Krakatau \n                   \n                     Monthly reports on Global Volcanism \n                   \n                     Surtsey Listing at UNESCO's World Heritage Center \n                   Reprints and Permissions"},
{"file_id": "454816a", "url": "https://www.nature.com/articles/454816a", "year": 2008, "authors": [{"name": "Quirin Schiermeier"}, {"name": "Jeff Tollefson"}, {"name": "Tony Scully"}, {"name": "Alexandra Witze"}, {"name": "Oliver Morton"}], "parsed_as_year": "2006_or_before", "body": "This article is best viewed as a  PDF \n               Electricity generation provides  \n               \n                 18,000 terawatt-hours \n               \n                of energy a year, around 40% of humanity's total energy use. In doing so it produces more than 10 gigatonnes of carbon dioxide every year, the largest sectoral contribution of humanity's fossil-fuel derived emissions. Yet there is a wide range of technologies \u2014 from solar and wind to nuclear and geothermal \u2014 that can generate electricity without net carbon emissions from fuel. \n             \n               The easiest way to cut the carbon released by electricity generation is to increase efficiency. But there are limits to such gains, and there is the familiar paradox that greater efficiency can lead to greater consumption. So a global response to climate change must involve a move to carbon-free sources of electricity. This requires fresh thinking about the price of carbon, and in some cases new technologies; it also means new transmission systems and smarter grids. But above all, the various sources of carbon-free generation need to be scaled up to power an increasingly demanding world. In this special feature,  \n               Nature \n               's News team looks at how much carbon-free energy might ultimately be available \u2014 and which sources make most sense. \n             \n                Hydropower \n             The world has a lot of dams \u2014 45,000 large ones, according to the World Energy Council, and many more at small scales. Its hydroelectric power plants have a generating capacity of 800 gigawatts (for a guide to power, see  \u2018By the numbers\u2019 ), and they currently supply almost one-fifth of the electricity consumed worldwide. As a source of electricity, dams are second only to fossil fuels, and generate 10 times more power than geothermal, solar and wind power combined. With a claimed full capacity of 18 gigawatts, the Three Gorges dam in China can generate more or less twice as much power as all the world's solar cells. An additional 120 gigawatts of capacity is under development. One reason for hydropower's success is that it is a widespread resource \u2014 160 countries use hydropower to some extent. In several countries hydropower is the largest contributor to grid electricity \u2014 it is not uncommon in developing countries for a large dam to be the main generating source. Nevertheless, it is in large industrialized nations that have big rivers that hydroelectricity is shown in its most dramatic aspect. Brazil, Canada, China, Russia and the United States currently produce more than half of the world's hydropower. Cost:  According to the International Hydropower Association (IHA), installation costs are usually in the range of US$1 million to more than $5 million per megawatt of capacity, depending on the site and size of the plant. Dams in lowlands and those with only a short drop between the water level and the turbine tend to be more expensive; large dams are cheaper per watt of capacity than small dams in similar settings. Annual operating costs are low \u2014 0.8\u20132% of capital costs; electricity costs $0.03\u20130.10 per kilowatt-hour, which makes dams competitive with coal and gas. Capacity:  The absolute limit on hydropower is the rate at which water flows downhill through the world's rivers, turning potential energy into kinetic energy as it goes. The amount of power that could theoretically be generated if all the world's run-off were 'turbined' down to sea level is more than 10 terawatts. However, it is rare for 50% of a river's power to be exploitable, and in many cases the figure is below 30%. Those figures still offer considerable opportunity for new capacity, according to the IHA. Europe currently sets a benchmark for hydropower use, with 75% of what is deemed feasible already exploited. For Africa to reach the same level, it would need to increase its hydropower capacity by a factor of 10 to more than 100 gigawatts. Asia, which already has the greatest installed capacity, also has the greatest growth potential. If it were to triple its generating capacity, thus harnessing a near-European fraction of its potential, it would double the world's overall hydroelectric capacity. The IHA says that capacity could triple worldwide with enough investment. Advantages:  The fact that hydroelectric systems require no fuel means that they also require no fuel-extracting infrastructure and no fuel transport. This means that a gigawatt of hydropower saves the world not just a gigawatt's worth of coal burned at a fossil-fuel plant, but also the carbon costs of mining and transporting that coal. As turning on a tap is easy, dams can respond almost instantaneously to changing electricity demand independent of the time of day or the weather. This ease of turn-on makes them a useful back-up to less reliable renewable sources. That said, variations in use according to need and season mean that dams produce about half of their rated power capacity. Hydroelectric systems are unique among generating systems in that they can, if correctly engineered, store the energy generated elsewhere, pumping water uphill when energy is abundant. The reservoirs they create can also provide water for irrigation, a way to control floods and create amenities for recreational use. Disadvantages:  Not all regions have large hydropower resources \u2014 the Middle East, for example, is relatively deficient. And reservoirs take up a lot of space; today the area under man-made lakes is as large as two Italys. The large dams and reservoirs that account for most of that area and for more than 90% of hydro-generated electricity worldwide require lengthy and costly planning and construction, as well as the relocation of people from the reservoir area. In the past few decades, millions of people have been relocated in India and China. Dams have ecological effects on the ecosystems upstream and downstream, and present a barrier to migrating fish. Sediment build-up can shorten their operating life, and sediment trapped by the dam is denied to those downstream. Biomass that decomposes in reservoirs releases methane and carbon dioxide, and in some cases these emissions can be of a similar order of magnitude to those avoided by not burning fossil fuels. Climate change could itself limit the capacity of dams in some areas by altering the amount and pattern of annual run-off from sources such as the glaciers of Tibet. Because hydro is a mature technology, there is little room for improvement in the efficiency of generation. Also, the more obvious and easy locations have been used, and so the remaining potential can be expected to be harder to exploit. Small (less than 10 megawatts) 'run-of-river' schemes that produce power from the natural flow of water \u2014 as millers have been doing for four millennia \u2014 are appealing, as they have naturally lower impacts. However, they are about five times more expensive and harder to scale than larger schemes. Verdict:  A cheap and mature technology, but with substantial environmental costs; roughly a terawatt of capacity could be added.  \n                Nuclear fission \n             When reactor 4 at the Chernobyl nuclear power plant in Ukraine melted down on 26 April 1986, the fallout contaminated large parts of Europe. That disaster, and the earlier incident at Three Mile Island in Pennsylvania, blighted the nuclear industry in the West for a generation. Worldwide, though, the picture did not change quite as dramatically. In 2007, 35 nuclear plants were under construction, almost all in Asia. The 439 reactors already in operation had an overall capacity of 370 gigawatts, and contributed around 15% of the electricity generated worldwide, according to the most recent figures from the International Atomic Energy Agency (IAEA), which serves as the world's nuclear inspectorate. Costs:  Depending on the design of the reactor, the site requirements and the rate of capital depreciation, the light-water reactors that make up most of the world's nuclear capacity produce electricity at costs of between US$0.025 and $0.07 per kilowatt-hour. The technology that makes this possible has benefited from decades of expensive research, development and purchases subsidized by governments; without that boost it is hard to imagine that nuclear power would currently be in use. Capacity:  Because nuclear power requires fuel, it is constrained by fuel stocks. There are some 5.5 million tonnes of uranium in known reserves that could profitably be extracted at a cost of US$130 per kilogram or less, according to the latest edition of the 'Red Book', in which the IAEA and the Organisation for Economic Co-operation and Development (OECD) assess uranium resources. At the current use of 66,500 tonnes per year, that is about 80 years' worth of fuel. The current price of uranium is over that $130 threshold. Geologically similar ore deposits that are as yet unproven \u2014 'undiscovered reserves' \u2014 are thought to amount to roughly double the proven reserves, and lower-grade ores offer considerably more. Uranium is not a particularly rare element \u2014 it is about as common a constituent of Earth's crust as zinc. Estimates of the ultimate recoverable resource vary greatly, but 35 million tonnes might be considered available. Nor is uranium the only naturally occurring element that can be made into nuclear fuel. Although they have not yet been developed, thorium-fuelled reactors are a possibility; bringing thorium into play would double the available fuel reserves. Furthermore, although current reactor designs use their fuel only once, this could be changed. Breeder reactors, which make plutonium from uranium isotopes that are not themselves useful for power production, can effectively create more fuel than they use. A system built on such reactors might get 60 times more energy out for every kilogram of natural uranium put in, although lower multiples might be more realistic. With breeder reactors, which have yet to be proven on a commercial basis, the world could in principle go 100% nuclear. Without them, it is still plausible for the amount of nuclear capacity to grow by a factor of two or three, and to operate at that level for a century or more. Advantages:  Nuclear power has relatively low fuel costs and can run at full blast almost constantly \u2014 US plants deliver 90% of their rated capacity. This makes them well suited to providing always-on 'baseload' power to national grids. Uranium is sufficiently widespread that the world's nuclear-fuel supply is unlikely to be threatened by political factors. Disadvantages:  There is no agreed solution to the problem of how to deal with the nuclear waste that has been generated in nuclear plants over the past 50 years. Without long-term solutions, which are more demanding politically than technically, growth in nuclear power is an understandably hard sell. A further problem is that the spread of nuclear power is difficult to disentangle from the proliferation of nuclear weapons capabilities. Fuel cycles that involve recycling, and which thus necessarily produce plutonium, are particularly worrying. Even without proliferation worries, nuclear power stations may make tempting targets for terrorists or enemy forces (although in the latter case the same is true of hydroelectric plants). A long-term commitment to greatly increased use of nuclear power would require public acceptance not just of existing technologies but of new ones, too \u2014 thorium and breeder reactors, for instance. These technologies would also have to win over investors and regulators (for nuclear fusion, see  \u2018Farther out\u2019 ). Nuclear power is also extremely capital intensive; power costs over the life of the plant are comparatively low only because the plants are long lived. Nuclear power is thus an expensive option in the short term. Another constraint may be a lack of skilled workers. Building and operating nuclear plants requires a great many highly trained professionals, and enlarging this pool of talent enough to double the rate at which new plants are brought online might prove very challenging. The engineering capacity for making key components would also need enlarging. In light of these obstacles, predictions of the future role of nuclear power vary considerably. The European Commission's  _World Energy Technology Outlook \u2014 2050_  contains a bullish scenario that assumes that, with public acceptance and the development of new reactor technologies, nuclear power could provide about 1.7 terawatts by 2050. The IAEA's analysts are more cautious. Hans-Holger Rogner, head of the agency's planning and economic study section, sees capacity rising to not more than 1,200 gigawatts by 2050. An interdisciplinary study carried out in 2003 by the Massachusetts Institute of Technology described a concrete scenario for tripling capacity to 1,000 gigawatts by 2050, a scenario predicated on US leadership, continued commitment by Japan and renewed activity by Europe. This scenario relied only on improved versions of today's reactors rather than on any radically different or improved design. Verdict:  Reaching a capacity in the terawatt range is technically possible over the next few decades, but it may be difficult politically. A climate of opinion that came to accept nuclear power might well be highly vulnerable to adverse events such as another Chernobyl-scale accident or a terrorist attack.  \n                Biomass \n             Biomass was humanity's first source of energy, and until the twentieth century it remained the largest; even today it comes second only to fossil fuels. Wood, crop residues and other biological sources are an important energy source for more than two billion people. Mostly, this fuel is burned in fires and cooking stoves, but over recent years biomass has become a source of fossil-fuel-free electricity. As of 2005, the World Energy Council estimates biomass generating capacity to be at least 40 gigawatts, larger than any renewable resource other than wind and hydropower. Biomass can supplement coal or in some cases gas in conventional power plants. Biomass is also used in many co-generation plants that can capture 85\u201390% of the available energy by making use of waste heat as well as electric power. Costs:  The price of biomass electricity varies widely depending on the availability and type of the fuel and the cost of transporting it. Capital costs are similar to those for fossil-fuel plants. Power costs can be as little as $0.02 per kilowatt-hour when biomass is burned with coal in a conventional power plant, but increase to $0.03\u20130.05 per kilowatt-hour from a dedicated biomass power plant. Costs increase to $0.04\u20130.09 per kilowatt-hour for a co-generation plant, but recovery and use of the waste heat makes the process much more efficient. The biggest problem for new biomass power plants is finding a reliable and concentrated feedstock that is available locally; keeping down transportation costs means keeping biomass power plants tied to locally available fuel and quite small, which increases the capital cost per megawatt. Capacity:  Biomass is limited by the available land surface, the efficiency of photosynthesis, and the supply of water. An OECD round table in 2007 estimated that there is perhaps half a billion hectares of land not in agricultural use that would be suitable for rain-fed biomass production, and suggested that by 2050 this land, plus crop residues, forest residues and organic waste might provide enough burnable material each year to provide 68,000 terawatt-hours. Converted to electricity at an efficiency of 40%, that could provide a maximum of 3 terawatts. The Intergovernmental Panel on Climate Change pegs the potential at roughly 120,000 terawatt-hours in 2050, which equates to slightly more than 5 terawatts on the basis of a larger estimate of available land. These projections involve some fairly extreme assumptions about converting land to the production of energy crops. And even to the extent that these assumptions prove viable, electricity is not the only potential use for such plantations. By storing solar energy in the form of chemical bonds, biomass lends itself better than other renewable energy resources to the production of fuel for transportation (see  page 841 ). Although turning biomass to biofuel is not as efficient as just burning the stuff, it can produce a higher-value product. Biofuels might easily beat electricity generation as a use for biomass in most settings. Advantages:  Plants are by nature carbon-neutral and renewable, although agriculture does use up resources, especially if it requires large amounts of fertilizer. The technologies needed to burn biomass are mature and efficient, especially in the case of co-generation. Small systems using crop residues can minimize transportation costs. If burned in power plants fitted with carbon-capture-and-storage hardware, biomass goes from being carbon neutral to carbon negative, effectively sucking carbon dioxide out of the atmosphere and storing it in the ground (see  'Carbon capture and storage' ). This makes it the only energy technology that can actually reduce carbon dioxide levels in the atmosphere. As with coal, however, there are costs involved in carbon capture, both in terms of capital set-up and in terms of efficiency. Disadvantages:  There is only so much land in the world, and much of it will be needed to provide food for the growing global population. It is not clear whether letting market mechanisms drive the allocation of land between fuel and food is desirable or politically feasible. Changing climate could itself alter the availability of suitable land. There is likely to be opposition to increased and increasingly intense cultivation of energy crops. Use of waste and residues may remove carbon from the land that would otherwise have enriched the soil; long-term sustainability may not be achievable. Bioenergy dependence could also open the doors to energy crises caused by drought or pestilence, and land-use changes can have climate effects of their own: clearing land for energy crops may produce emissions at a rate the crops themselves are hard put to offset. Verdict:  If a large increase in energy crops proves acceptable and sustainable, much of it may be used up in the fuel sector. However, small-scale systems may be desirable in an increasing number of settings, and the possibility of carbon-negative systems \u2014 which are plausible for electricity generation but not for biofuels \u2014 is a unique and attractive capability.  \n                Wind \n             Wind power is expanding faster than even its fiercest advocates could have wished a few years ago. The United States added 5.3 gigawatts of wind capacity in 2007 \u2014 35% of the country's new generating capacity \u2014 and has another 225 gigawatts in the planning stages. There is more wind-generating capacity being planned in the United States than for coal and gas plants combined. Globally, capacity has risen by nearly 25% in each of the past five years, according to the Global Wind Energy Council. Wind Power Monthly  estimates that the world's installed capacity for wind as of January 2008 was 94 gigawatts. If growth continued at 21%, that figure would triple over six years. Despite this, the numbers remain small on a global scale, especially given that wind farms have historically generated just 20% of their capacity. Costs:  Installation costs for wind power are around US$1.8 million per megawatt for onshore developments and between $2.4 million and $3 million for offshore projects. That translates to $0.05\u20130.09 per kilowatt-hour, making wind competitive with coal at the lower end of the range. With subsidies, as enjoyed in many countries, the costs come in well below those for coal \u2014 hence the boom. The main limit on wind-power installation at the moment is how fast manufacturers can make turbines. These costs represent significant improvements in the technology. In 1981, a wind farm might have consisted of an array of 50-kilowatt turbines that produced power for roughly $0.40 per kilowatt-hour. Today's turbines can produce 30 times as much power at one-fifth the price with much less down time. Capacity:  The amount of energy generated by the movement of Earth's atmosphere is vast \u2014 hundreds of terawatts. In a 2005 paper, a pair of researchers from Stanford University calculated that at least 72 terawatts could be effectively generated using 2.5 million of today's larger turbines placed at the 13% of locations around the world that have wind speeds of at least 6.9 metres per second and are thus practical sites ( C. L. Archer and M. Z. Jacobson  J. Geophys. Res.   110,  D12110; 2005 ). Advantages:  The main advantage of wind is that, like hydropower, it doesn't need fuel. The only costs therefore come from building and maintaining the turbines and power lines. Turbines are getting bigger and more reliable. The development of technologies for capturing wind at high altitudes could provide sources with small footprints capable of generating power in a much more sustained way. Disadvantages:  Wind's ultimate limitation might be its intermittency. Providing up to 20% of a grid's capacity from wind is not too difficult. Beyond that, utilities and grid operators need to take extra steps to deal with the variability. Another grid issue, and one that is definitely limiting in the near term, is that the windiest places are seldom the most populous, and so electricity from the wind needs infrastructure development \u2014 especially for offshore settings. As well as being intermittent, wind power is, like other renewable energy sources, inherently quite low density. A large wind farm typically generates a few watts per square metre \u2014 10 is very high. Wind power thus depends on cheap land, or on land being used for other things at the same time, or both. It is also hard to deploy in an area where the population sets great store by the value of a turbine-free landscape. Wind power is also unequally distributed: it favours nations with access to windy seas and their onshore breezes or great empty plains. Germany has covered much of its windiest land with turbines, but despite these pioneering efforts, its combined capacity of 22 GW supplies less than 7% of the country's electricity needs. Britain, which has been much slower to adopt wind power, has by far the largest offshore potential in Europe \u2014 enough to meet its electricity needs three times over, according to the British Wind Energy Association. Industry estimates suggest that the European Union could meet 25% of its current electricity needs by developing less than 5% of the North Sea. Such truly large-scale deployment of wind-power schemes could affect local, and potentially global, climate by altering wind patterns, according to research by David Keith, head of the Energy and Environmental Systems Group at the University of Calgary in Canada. Wind tends to cool things down, so temperatures around a very large wind farm could rise as turbines slow the wind to extract its energy. Keith and his team suggest that 2 TW of wind capacity could affect temperatures by about 0.5 \u00b0C, with warming at mid-latitudes and cooling at the poles \u2014 perhaps in that respect offsetting the effect of global warming ( D. W. Keith  et al .  Proc. Natl Acad. Sci. USA   101,  16115\u201316120; 2004 ). Verdict:  With large deployments on the plains of the United States and China, and cheaper access to offshore, a wind-power capacity of a terawatt or more is plausible.  \n                Geothermal \n             Earth's interior contains vast amounts of heat, some of it left over from the planet's original coalescence, some of it generated by the decay of radioactive elements. Because rock conducts heat poorly, the rate at which this heat flows to the surface is very slow; if it were quicker, Earth's core would have frozen and its continents ceased to drift long ago. The slow flow of Earth's heat makes it a hard resource to use for electricity generation except in a few specific places, such as those with abundant hot springs. Only a couple of dozen countries produce geothermal electricity, and only five of those \u2014 Costa Rica, El Salvador, Iceland, Kenya and the Philippines \u2014 generate more than 15% of their electricity this way. The world's installed geothermal electricity capacity is about 10 gigawatts, and is growing only slowly \u2014 about 3% per year in the first half of this decade. A decade ago, geothermal capacity was greater than wind capacity; now it is almost a factor of ten less. Earth's heat can also be used directly. Indeed, small geothermal heat pumps that warm houses and businesses directly may represent the greatest contribution that Earth's warmth can make to the world's energy budget. Costs:  The cost of a geothermal system depends on the geological setting. Jefferson Tester, a chemical engineer who was part of a team that produced an influential Massachusetts Institute of Technology (MIT) report on geothermal technology in 2006, explains the situation as being \u201csimilar to mineral resources. There is a continuum of resource grades \u2014 from shallow, high-temperature regions of high-porosity rock, to deeper low-porosity regions that are more challenging to exploit\u201d. That report put the cost of exploiting the best sites \u2014 those with a lot of hot water circulating close to the surface \u2014 at about US$0.05 per kilowatt-hour. Much more abundant low-grade resources are exploitable with current technology only at much higher prices. Absolute capacity:  Earth loses heat at between 40 TW and 50 TW a year, which works out at an average of a bit less than a tenth of a watt per square metre. For comparison, sunlight comes in at an average of 200 watts per square metre. With today's technology, 70 GW of the global heat flux is seen as exploitable. With more advanced technologies, at least twice that could be used. The MIT study suggested that using enhanced systems that inject water at depth using sophisticated drilling systems, it would be possible to set up 100 GW of geothermal electricity in the United States alone. With similar assumptions a global figure of a terawatt or so can be reached, suggesting that geothermal could, with a great deal of investment, provide as much electricity as dams do today. Advantages:  Geothermal resources require no fuel. They are ideally suited to supplying base-load electricity, because they are driven by a very regular energy supply. At 75%, geothermal sources boast a higher capacity factor than any other renewable. Low-grade heat left over after generation can be used for domestic heating or for industrial processes. Surveying and drilling previously unexploited geothermal resources has become much easier thanks to mapping technology and drilling equipment designed by the oil industry. A significant technology development programme \u2014 Tester suggests $1 billion over 10 years \u2014 could greatly expand the achievable capacity as lower-grade resources are opened up. Disadvantages:  High-grade resources are quite rare, and even low-grade resources are not evenly distributed. Carbon dioxide can leak out of some geothermal fields, and there can be contamination issues; the water that brings the heat to the surface can carry compounds that shouldn't be released into aquifers. In dry regions, water availability can be a constraint. Large-scale exploitation requires technologies that, although plausible, have not been demonstrated in the form of robust, working systems. Verdict:  Capacity might be increased by more than an order of magnitude. Without spectacular improvements, it is unlikely to outstrip hydro and wind and reach a terawatt.  \n                Solar \n             Not to take anything away from the miracle of photosynthesis, but even under the best conditions plants can only turn about 1% of the solar radiation that hits their surfaces into energy that anyone else can use. For comparison, a standard commercial solar photovoltaic panel can convert 12\u201318% of the energy of sunlight into useable electricity; high-end models come in above 20% efficiency. Increasing manufacturing capacity and decreasing costs have led to remarkable growth in the industry over the past five years: in 2002, 550 MW of cells were shipped worldwide; in 2007 the figure was six times that. Total installed solar-cell capacity is estimated at 9 GW or so. The actual amount of electricity generated, though, is considerably less, as night and clouds decrease the power available. Of all renewables, solar currently has the lowest capacity factor, at about 14%. Solar cells are not the only technology by which sunlight can be turned into electricity. Concentrated solar thermal systems use mirrors to focus the Sun's heat, typically heating up a working fluid that in turn drives a turbine. The mirrors can be set in troughs, in parabolas that track the Sun, or in arrays that focus the heat on a central tower. As yet, the installed capacity is quite small, and the technology will always remain limited to places where there are a lot of cloud-free days \u2014 it needs direct sun, whereas photovoltaics can make do with more diffuse light. Costs:  The manufacturing cost of solar cells is currently US$1.50\u20132.50 for a watt's worth of generating capacity, and prices are in the $2.50\u20133.50 per watt range. Installation costs are extra; the price of a full system is normally about twice the price of the cells. What this means in terms of cost per kilowatt-hour over the life of an installation varies according to the location, but it comes out at around $0.25\u20130.40. Manufacturing costs are dropping, and installation costs will also fall as photovoltaic cells integrated into building materials replace free-standing panels for domestic applications. Current technologies should be manufacturing at less than $1 per watt within a few years (see  _Nature_ 454, 558\u2013559; 2008 ). The cost per kilowatt-hour of concentrated solar thermal power is estimated by the US National Renewable Energy Laboratory (NREL) in Golden, Colorado, at about $0.17. Capacity:  Earth receives about 100,000 TW of solar power at its surface \u2014 enough energy every hour to supply humanity's energy needs for a year. There are parts of the Sahara Desert, the Gobi Desert in central Asia, the Atacama in Peru or the Great Basin in the United States where a gigawatt of electricity could be generated using today's photovoltaic cells in an array 7 or 8 kilometres across. Theoretically, the world's entire primary energy needs could be served by less than a tenth of the area of the Sahara. Advocates of solar cells point to a calculation by the NREL claiming that solar panels on all usable residential and commercial roof surfaces could provide the United States with as much electricity per annum as the country used in 2004. In more temperate climes things are not so promising: in Britain one might expect an annual insolation of about 1,000 kilowatt-hours per metre on a south-facing panel tilted to take account of latitude: at 10% efficiency, that means more than 60 square metres per person would be needed to meet current UK electricity consumption. Advantages:  The Sun represents an effectively unlimited supply of fuel at no cost, which is widely distributed and leaves no residue. The public accepts solar technology and in most places approves of it \u2014 it is subject to less geopolitical, environmental and aesthetic concern than nuclear, wind or hydro, although extremely large desert installations might elicit protests. Photovoltaics can often be installed piecemeal \u2014 house by house and business by business. In these settings, the cost of generation has to compete with the retail price of electricity, rather than the cost of generating it by other means, which gives solar a considerable boost. The technology is also obviously well suited to off-grid generation and thus to areas without well developed infrastructure. Both photovoltaic and concentrated solar thermal technologies have clear room for improvement. It is not unreasonable to imagine that in a decade or two new technologies could lower the cost per watt for photovoltaics by a factor of ten, something that is almost unimaginable for any other non-carbon electricity source. Disadvantages:  The ultimate limitation on solar power is darkness. Solar cells do not generate electricity at night, and in places with frequent and extensive cloud cover, generation fluctuates unpredictably during the day. Some concentrated solar thermal systems get around this by storing up heat during the day for use at night (molten salt is one possible storage medium), which is one of the reasons they might be preferred over photovoltaics for large installations. Another possibility is distributed storage, perhaps in the batteries of electric and hybrid cars (see  page 810 ). Another problem is that large installations will usually be in deserts, and so the distribution of the electricity generated will pose problems. A 2006 study by the German Aerospace Centre proposed that by 2050 Europe could be importing 100 GW from an assortment of photovoltaic and solar thermal plants across the Middle East and North Africa. But the report also noted that this would require new direct-current high-voltage electricity distribution systems. A possible drawback of some advanced photovoltaic cells is that they use rare elements that might be subject to increases in cost and restriction in supply. It is not clear, however, whether any of these elements is either truly constrained \u2014 more reserves might be made economically viable if demand were higher \u2014 or irreplaceable. Verdict:  In the middle to long run, the size of the resource and the potential for further technological development make it hard not to see solar power as the most promising carbon-free technology. But without significantly enhanced storage options it cannot solve the problem in its entirety.  \n                Ocean energy \n             The oceans offer two sorts of available kinetic energy \u2014 that of the tides and that of the waves. Neither currently makes a significant contribution to world electricity generation, but this has not stopped enthusiasts from developing schemes to make use of them. There are undoubtedly some places where, thanks to peculiarities of geography, tides offer a powerful resource. In some situations that potential would best be harnessed by a barrage that creates a reservoir not unlike that of a hydroelectric dam, except that it is refilled regularly by the pull of the Moon and the Sun, rather than being topped up slowly by the runoff of falling rain. But although there are various schemes for tidal barrages under discussion \u2014 most notably the Severn Barrage between England and Wales, which proponents claim could offer as much as 8 GW \u2014 the plant on the Rance estuary in Brittany, rated at 240 MW, remains the world's largest tidal-power plant more than 40 years after it came into use. There are also locations well suited to tidal-stream systems \u2014 submerged turbines that spin in the flowing tide like windmills in the air. The 1.2 MW turbine installed this summer in the mouth of Strangford Lough, Northern Ireland, is the largest such system so far installed. Most technologies for capturing wave power remain firmly in the testing phase. Individual companies are working through an array of potential designs, including machines that undulate on waves like a snake, bob up and down as water passes over them, or nestle on the coastline to be regularly overtopped by waves that power turbines as the water drains off. The European Marine Energy Centre's test bed off the United Kingdom's Orkney Islands, where manufacturers can hook up prototypes to a marine electricity grid and test how well they withstand the pounding waves, is a leading centre of research. Pelamis Wave Power, a company based in Edinburgh, UK, for instance, has moved from testing there to installing three machines off the coast of Portugal, which together will eventually generate 2.25 MW. Costs:  Barrage costs differ markedly from site to site, but are broadly comparable to costs for hydropower. At an estimated cost of \u00a315 billion (US$30 billion) or more, the capital costs of the Severn Barrage would be about $4 million per megawatt. A 2006 report from the British Carbon Trust, which spurs investment in non-carbon energy, puts the costs of tidal-stream electricity in the $0.20\u20130.40 per kilowatt-hour range, with wave systems running up to $0.90 per kilowatt-hour. Neither technology is anywhere close to the large-scale production needed to significantly drive such costs down. Capacity:  The interaction of Earth's mass with the gravitational fields of the Moon and the Sun is estimated to produce about 3 TW of tidal energy\u2014 rather modest for such an astronomical source (although enough to play a key role in keeping the oceans mixed \u2014 see  Nature 447, 522\u2013524; 2007 ). Of this, perhaps 1 TW is in shallow enough waters to be easily exploited, and only a small part of that is realistically available. EDF, a French power company developing tidal power off Brittany, says that the tidal-stream potential off France is 80% of that available all round Europe, and yet it is still little more than a gigawatt. The power of ocean waves is estimated at more than 100 TW. The European Ocean Energy Association estimates that the accessible global resource is between 1 and 10 terawatts, but sees much less than that as recoverable with current technologies. An analysis in the  MRS Bulletin  in April 2008 holds that about 2% of the world's coastline has waves with an energy density of 30 kW m \u22121 , which would offer a technical potential of about 500 GW for devices working at 40% efficiency. Thus even with a huge amount of development, wave power would be unlikely to get close to the current installed hydroelectric capacity. Advantages:  Tides are eminently predictable, and in some places barrages really do offer the potential for large-scale generation that would be significant on a countrywide scale. Barrages also offer some built-in storage potential. Waves are not constant \u2014 but they are more reliable than winds. Disadvantages:  The available resource varies wildly with geography; not every country has a coastline, and not every coastline has strong tides or tidal streams, or particularly impressive waves. The particularly hot wave sites include Australia's west coast, South Africa, the western coast of North America and western European coastlines. Building turbines that can survive for decades at sea in violent conditions is tough. Barrages have environmental impacts, typically flooding previously intertidal wetlands, and wave systems that flank long stretches of dramatic coastline might be hard for the public to accept. Tides and waves tend by their nature to be found at the far end of electricity grids, so bringing back the energy represents an extra difficulty. Surfers have also been known to object \u2026 Verdict:  Marginal on the global scale. See Editorial,  page 805 Reported and written by Quirin Schiermeier, Jeff Tollefson, Tony Scully, Alexandra Witze and Oliver Morton. \n                     Power for a cool planet \n                   \n                     IEA energy statistics \n                   \n                     BP Statistical Review of World Energy 2008 \n                   \n                     World Energy Council 2007 report \n                   \n                     Global Energy Technology Strategy Program \n                   \n                     IPCC scoping meeting on renewable energy resources \n                   \n                     IPCC Fourth Assessment report, Working group III \n                   \n                     Vaclav Smil\u2019s homepage \n                   \n                     David MacKay\u2019s sustainable energy site \n                   \n                     International Hydropower Association \n                   \n                     MIT report on the future of nuclear power \n                   \n                     OECD Biofuels report \n                   \n                     MIT report on the future of geothermal power \n                   \n                     Solarbuzz \n                   Reprints and Permissions"},
{"file_id": "454686a", "url": "https://www.nature.com/articles/454686a", "year": 2008, "authors": [{"name": "Corie Lok"}], "parsed_as_year": "2006_or_before", "body": "Harvard is embarking on an experiment to foster collaboration and interdisciplinary research. Corie Lok looks at whether it can change its culture and reinvent communities along the way. Harvard University is the oldest, richest, most prestigious institution of higher learning in the United States. In many fields it is also the best. But its efforts to move into new areas of interdisciplinary science are often slow or fragmented compared with more nimble powerhouses such as Stanford University or the University of California, Berkeley. A typical example is chemical biology, in which researchers use small molecules to probe and learn about biological systems. Harvard has one chemical-biology group on its main campus in Cambridge, Massachusetts; another at the Harvard Medical School, six kilometres away across the Charles River in Boston; yet another at the Broad Institute, jointly run by the Massachusetts Institute of Technology and Harvard; and still other, smaller efforts within some of Harvard's many affiliated research hospitals and institutions. Such a proliferation of similar efforts troubles Tim Mitchison, co-founder of Harvard Medical School's Institute of Chemistry and Cell Biology. \u201cIt leads to inefficient use of resources and expertise,\u201d he says. It troubles Harvard's leadership, too \u2014 which is why the university is now engaged in a controversial effort to reinvent the way that research gets done and managed there. A major part of this effort is to expand the campus in Allston, a neighbourhood of Boston just across the river from Cambridge. Construction began there earlier this year on a 46,500-square-metre, US$1-billion facility that will house the new Department of Stem Cell and Regenerative Biology. The department was formed last year as a new academic home for Harvard's stem-cell community, which, like the chemical-biology groups, is currently scattered across various schools, research hospitals and centres. Bioengineering and systems-biology researchers will also be moving to the new complex. Another key part of the effort is the Harvard University Science and Engineering Committee (HUSEC), which encompasses Harvard's provost and other top leaders of science and aims to improve the coordination of science planning across the university and to provide funding to start up new initiatives. Yet another important piece is a strategic review now under way at the medical school \u2014 the first in recent memory. Instigated by the school's new dean, Jeffrey Flier, the review is asking for recommendations on how to strengthen ties across the medical school and research hospitals in areas such as human genetics, neuroscience, technology development and therapeutics and chemical biology. A similar review is under way at the Faculty of Arts and Sciences, led by its new dean, Michael Smith. We're realizing that we can be more than the sum of our parts. Jeremy Bloxham  In effect, Harvard has launched a radical, university-wide experiment to create a more open, agile and collaborative research culture \u2014 as well as research communities that go beyond traditional departments and schools. \u201cScience is changing in dramatic ways,\u201d says Harvard's president Drew Faust. \u201cThe implications of this for the way we organize our scientific enterprise are very significant. It challenges us to break down barriers.\u201d Like the president, the provost and deans at Harvard have also bought into this vision.\u201cWe're realizing that we can be more than the sum of our parts,\u201d says Jeremy Bloxham, dean of science in the Faculty of Arts and Sciences. But what remains to be seen is whether enough faculty members and departments at a grassroots level will join them for the changes to take hold.  \n                Grand designs \n              Harvard has the resources to show on a grand scale how to foster new ways of doing science. It has a large and growing science and engineering component \u2014 1,750 investigators, including those from Harvard's 20 affiliated hospitals and research institutes \u2014 which pulls in around $2 billion a year in external funding. At $35 billion, it has the biggest endowment of any university, and it has more than 80 hectares of land on which to expand its Allston campus. But Harvard is used to being a leader and isn't in the habit of self-reflection or admitting to its weaknesses. It has a long tradition of fierce independence \u2014 every school and hospital has its own powerful leaders, administrative structures, fundraising efforts and cultures. At times, hiring committees from different parts of Harvard even compete with each other for the same candidate. And researchers doing similar work may be more likely to come across each other at international meetings than at home. Until recently, says Venkatesh Narayanamurti, dean of Harvard's School of Engineering and Applied Sciences, \u201cour organizational structure was always very decentralized. The president had very little authority, and the money was with the deans. That's not all bad, because you want decisions being made close to where the work is being done. But one has to balance the role of local autonomy with central direction.\u201d Bringing more centralized direction to Harvard was a key goal of Larry Summers, the university's president from 2001 to 2006. To help him implement that strategy, he brought in Steve Hyman, a psychiatrist at Harvard Medical School from 1992 until 1996 and then director of the National Institute of Mental Health. Summers installed Hyman as provost in 2001 and together the pair moved aggressively to change the status quo. By the end of 2003, Harvard Medical School had established a new systems-biology department; the university had announced it would collaborate with the Massachusetts Institute of Technology to launch the Broad Institute; and Summers had convened four task forces to jump-start planning of the Allston campus. One of those task forces, chaired by Hyman and focused on science and technology, envisaged that Allston would become a hub for interdisciplinary science. Accordingly, it put out a call for ideas for science projects that needed space and investment. From the 70 proposals it received, several interdisciplinary, cross-school initiatives were selected to receive start-up funding from the provost's office, including the Initiative in Innovative Computing, the Microbial Sciences Initiative and the Origins of Life Initiative. Meanwhile, the university was drawing up ambitious plans for a large fundraising campaign, with substantial sums devoted to science. University spokesman John Longbrake explains that Harvard generally needs to raise funds to support new initiatives, as 83% of the endowment is restricted by donors for specific uses, such as undergraduate financial aid or library collections, and it spends only about 5% of its entire endowment every year. Then trouble hit: in early 2005, Summers made his infamous remarks about how innate differences between men and women affect their success in science. The resulting headlines galvanized long-simmering discontent with Summers, whose aggressive, top-down leadership style had proved highly polarizing. Although some faculty members found his vision and decisiveness refreshing and exciting, others felt disenfranchised and offended.  \n                Fast and furious \n              Many critics also thought that the Allston task-force planning process was moving too quickly, and wasn't inclusive enough; and some even questioned the need to expand so aggressively into Allston at all. Others considered the creation and funding of the new interdisciplinary initiatives to be too ad hoc, and that more transparent governance structures and consultation were needed. \u201cThere was a bit of a free-for-all period,\u201d says James McCarthy, a biological oceanographer at Harvard. \u201cThere needed to be some order.\u201d In 2006, with so much unrest on campus, Summers was forced to resign after just five years in the job (see  Nature  doi:). As one of his last acts as president, he convened a university-wide faculty committee to address concerns about how initiatives that involve researchers from different departments and schools should be organized and managed. In its final report at the end of 2006, the committee recommended the formation of the HUSEC and other organizational structures for science, such as cross-school departments and interdepartmental committees. The 17-member HUSEC, chaired by Hyman, began meeting in April 2007. Among other things, its $50-million annual budget now funds the initiatives launched as a result of the Allston task forces. \n               boxed-text \n             The scientists involved with those initiatives are perhaps the ones who have been the most affected by Harvard's bumpy foray into interdisciplinary science. They began with much enthusiasm during Summers's reign, launching new research projects, holding symposia and seminars, recruiting graduate students, postdocs and other staff, and churning out papers. But with Summers's departure, a stalled fundraising campaign and shifts in their governance structure, the initiatives have lost momentum. Some have not received the funding they had initially planned for, and they have not been able to take on new research projects.  \n                Fighting the system \n              Harvard's experiment is as much about leadership as it is about science. Summers got the process moving, but in a way that also generated much opposition. Now the job falls to his successor, Faust. Faust is not a newcomer to leading institutional change. As the first dean of Harvard's Radcliffe Institute for Advanced Study, where she served for six years, Faust oversaw the transformation of the former independent women's college into a Harvard-affiliated scholarly institute. Initially, sceptics questioned whether this was the right type of experience for the Harvard presidency; the Radcliffe Institute is small and doesn't have a faculty or students, just fellows. Harvard has an opportunity to play a leadership role in the world in solving some of the great challenges like climate change. Dan Schrag  But so far, Harvard scientists have generally been pleased with Faust, saying that she's insightful, respectful, supportive of faculty and a good listener. Although a historian, Faust has been supportive of science, testifying before the US Senate in March calling for greater funding from the National Institutes of Health. She has kept Hyman on as provost, effectively allowing science planning to continue without major changes in direction. Faust has put unity high on her agenda, speaking of Harvard as \u201cone university\u201d at which collaborative research can flourish, rather than just a loose collection of independent schools. And she has hired new deans who share that vision. Smith, for instance, says that \u201cone of the things we want to do here is not just push forward with individual basic research in the sciences and engineering, but also have connections made with the professional schools.\u201d The next test of Faust's leadership will come with the revival of the fundraising campaign. As fundraising is so crucial to Harvard's ability to launch new projects, Faust's challenge will be to win over potential donors. Over the next several years, Harvard will face many questions as it expands and reconfigures science. \u201cThere's a bit of naivety in terms of how difficult it is to get these things to work, taking it beyond just talk,\u201d says Geoffrey West, president of the Santa Fe Institute in New Mexico, which specializes in interdisciplinary research. \u201cIt takes much more than just setting up [a new centre or building]. Universities need to provide mechanisms for this kind of research to be rewarded in the right ways and for people to interact both informally and formally.\u201d In particular, he says, they have to adapt their hiring, tenure and promotion criteria to make them more suitable for young interdisciplinary researchers who work with people from fields outside their own department. This will be a major challenge, given how deeply entrenched department-based structures and cultures are at many universities, says West. \u201cYou've got to get people [from different departments and fields] to break away from their departments and spend serious time together,\u201d says West. \u201cI'm not an anti-department guy. I like departments. They just have to loosen up.\u201d In fact, the departments within Harvard's Faculty of Arts and Sciences are beginning to loosen up. They are doing more joint appointments. In some tenure cases, letters of recommendation from people outside of the candidate's home department are given equal consideration. And the medical school has revised its promotion criteria for its faculty to explicitly include collaboration with groups of investigators. Another major issue is how to fund interdisciplinary projects. Alyssa Goodman, founding director of the Initiative in Innovative Computing says, \u201cI would say that half of the mission of our initiative is to do interdisciplinary science and the other half is to come up with a rational funding model for how you should do that.\u201d Dimitar Sasselov, director of the Origins of Life Initiative also thinks a lot about funding. Because his initiative doesn't fit neatly into the traditional categories of research funded by government agencies, it is especially dependent on seed money from the university and on private sources during the first few years. And that money needs to come quickly, says Sasselov, while the inspiration is still fresh. Otherwise, busy collaborators are likely to simply drift apart. What is required, he says, is a mechanism that would support interdisciplinary projects in the way that a venture-capital firm invests in a start-up company: quick but limited funding, with clear timelines and milestones.  \n                Business approach \n              That 'venture-capitalist' function is one of the multiple roles that Hyman sees for the HUSEC. So far, the committee has put out a call for proposals for seed grants of up to $75,000 per year for interdisciplinary research. It has also designated $7 million of its annual budget for the funding of first-year science graduate students. In addition, it has commissioned a review of how Harvard can beef up its bioengineering endeavours. And it has come up with a plan for what will go into the first science building on the Allston campus. Despite those steps forward, the HUSEC remains a work in progress. The committee has yet to develop a clear mission and to decide how it will prioritize its projects \u2014 not to mention put in place a system for allocating its funds. A particular challenge, many say, is to strike the right balance between support for the existing, core disciplines, and investments in the new ones. Perhaps the most prominent test case is the Department of Stem Cell and Regenerative Biology. Chaired by stem-cell researchers Doug Melton and David Scadden, it will be the first tenant in the Allston science building. It developed out of the Harvard Stem Cell Institute, also co-directed by Melton and Scadden, as a way to enhance undergraduate teaching and to hire and promote new faculty members, as institutes do not have the authority to make these decisions. Although its new building won't open until 2011, the department is already active. Several existing faculty members have moved to the new department and some will begin teaching undergraduates this year.  \n                Moving plans \n              However, says Scadden, creating the department is one thing. It will be much more complicated to figure out which members of the department will move their labs to the Allston campus, and whether they'll move over completely or maintain labs in two locations. Many stem-cell researchers who are scattered across different campuses and hospitals are excited about the prospect of being next-door neighbours in Allston. But some are concerned about losing connection with their existing communities. For instance, Gary Gilliland, head of the Cancer Stem Cell Program at the Harvard Stem Cell Institute and based in the medical campus says he is interested in Allston, but that \u201cit's possible I would have a harder time recruiting clinical fellows\u201d there. Universities need to provide mechanisms for people to interact both informally and formally. Geoffrey West  Home institutions and departments are also concerned about losing key faculty members to the Allston campus and having their communities disrupted. \u201cIt's good for them and it's maybe not good for us,\u201d says neuroscientist Jeff Lichtman, referring to the three people from the molecular and cellular biology department in which he works, including Melton, who will be moving to the new stem-cell department. \u201cWe're not losing them as colleagues; some will have joint appointments. But the truth is distance is a big problem for scientists.\u201d \u201cWe want to make sure that we do everything we can to not gut those communities,\u201d says Scadden. \u201cWe want them to be vibrant. It's essential to the success of this department and for the Stem Cell Institute that we continue to have close relationships with the hospitals and the medical school.\u201d Some of the junior faculty members are deferring their decision until they see which of the senior researchers move to Allston. \u201cIt's important for us to know who will be going there, who our neighbours would be,\u201d says Konrad Hochedlinger, a stem-cell biologist at the Harvard Stem Cell Institute, who will be part of the new department. As the university builds Allston and plans for the next science buildings there, discussions will continue about which other scientific communities will be the most appropriate ones to move there. For some emerging fields such as systems biology or regenerative biology, creating a critical mass in one location may be the key but for other, more mature fields, it may be more important to have individuals scattered throughout an established campus where they can easily interact with people from other departments and disciplines. \u201cI like the model of distributed expertise as opposed to critical masses,\u201d says Lichtman, who takes advantage of his close proximity to the physical-sciences departments to attend seminars and meet colleagues there. \u201cIt's not that I know everything a neuroscientist might say to me, but I would know a lot of what a neuroscientist might say to me because I've been talking to neuroscientists most of my career. But every time I talk to a chemist or a physicist, I learn something new.\u201d  \n                Testing times \n              In the end, perhaps the most important challenge facing Faust during this era of change is to find the right combination of top-down decision-making and bottom-up consensus-building \u2014 especially given how much her predecessor failed at this. Some faculty members who liked Summers's approach hope that Faust, who is seen as being more considered in her leadership style, won't be so cautious that Harvard settles back into its old, slow ways. \u201cHarvard has an opportunity to play a leadership role in the world in solving some of the great challenges such as climate change and I really hope that, over the coming year or two, Faust does stand up and really lead on these issues.\u201d says Dan Schrag, director of the Harvard University Center for the Environment. Faust is taking her own approach to leadership. \u201cWe need to be careful about what we do but we also need to figure out what the important initiatives are and then do them,\u201d says Faust. \u201cI don't decide to do things because they're bold. I decide to do things because they're right.\u201d Corie Lok is senior editor of Nature Network, based in Boston. \n                     Harvard to form new biology department \n                   \n                     A conversation with Harvard Medical School incoming dean Jeffrey Flier \n                   \n                     Harvard faculty proposes sweeping changes to science \n                   \n                     Science and engineering at Harvard \n                   \n                     Harvard University Science and Engineering Committee \n                   \n                     Harvard University Allston Initiative \n                   \n                     Harvard University Department of Stem Cell and Regenerative Biology \n                   \n                     Initiative in Innovative Computing \n                   \n                     Origins of Life Initiative \n                   \n                     Microbial Sciences Initiative \n                   Reprints and Permissions"},
{"file_id": "4531170a", "url": "https://www.nature.com/articles/4531170a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "From a 5-millimetre dent on a satellite to a 3-kilometre pit in the surface of Mars, the scars of impact events can be seen at every scale. We present a gallery of some particularly appealing ones from Earth and beyond. The online version of this Gallery feature is available as a slideshow - click  here  to see the pics. It is unusual for a Chevrolet Malibu to be hit by a meteorite, as Michelle Knapp's was in 1992; but the surface of a satellite such as the Long Duration Exposure Facility, which was in orbit for six years, can expect a pounding. And so can the surface of Earth. Meteor Crater in Arizona, which at 50,000 years old is relatively young and nicely preserved in the desert climate, was one of the first craters to have its impact origin recognized. Many others have since joined it, such as 10-kilometre-wide Bosumtwi, a million years old, which contains Ghana's only natural lake. Human experience of cratered landscapes has now extended to other planets, as shown by this panorama of Victoria on Mars, produced by the rover Opportunity before it ventured into the crater's depths. Large impacts can change planetary surfaces profoundly. The rings that surround the Valhalla basin on Callisto, a moon of Jupiter, take up roughly one-tenth of its surface. The basalts that fill lunar basins, such as those in Oceanus Procellarum, give Earth's satellite its 'seas' \u2014 themselves peppered with smaller craters, such as Euclides. A large enough impact can completely destroy its target; those that left the 9-kilometre-wide Stickney crater, on Mars's moon Phobos, and the 130-kilometre-wide Herschel, on Saturn's moon Mimas, came close. Sometimes, though, it is the impactor that comes to pieces: crater chains such as Enki Catena on the jovian moon Ganymede were probably formed by comet fragments crashing in one after the other. Zumba crater on Mars has a special claim to fame: it is possible that this relatively recent impact threw up into space some of the rocks that have now fallen to Earth as martian meteorites. Reprints and Permissions"},
{"file_id": "454682a", "url": "https://www.nature.com/articles/454682a", "year": 2008, "authors": [{"name": "Jim Schnabel"}], "parsed_as_year": "2006_or_before", "body": "Questions raised about the use of 'ALS mice' are prompting a broad reappraisal of the way that drugs are tested in animal models of neurodegenerative disease. Jim Schnabel reports. Several years ago, clinical neurologist Michael Benatar set out to find a drug he could test on some of his patients with amyotrophic lateral sclerosis (ALS). This condition is apt to strike otherwise healthy adults, slowly destroying the neurons that control their muscles. Its roster of famous sufferers includes the physicist Stephen Hawking and the late US baseball star Lou Gehrig. Except in rare cases, such as Hawking\u2019s, ALS progresses inexorably, causing respiratory failure within a few years of diagnosis. Decades of study have revealed a few mutant genes that could cause familial forms of the disease, but no one knows what causes the vast majority of cases \u2014 and despite trial after trial of prospective therapies, no therapy has ever been shown to have a major impact on the disease. One of the first steps Benatar, at Emory University School of Medicine in Atlanta, Georgia, took was to review published data on more than 150 drug tests that other researchers had conducted in the standard mouse model of ALS. Engineered to carry multiple copies of the mutated superoxide dismutase 1 ( SOD1 ) gene that causes some inherited ALS cases, the mouse reliably develops and succumbs to a neuron-killing disease that closely resembles the human condition. But as Benatar reviewed these mouse studies, he was dismayed to find that the data were of little use to him. Most of the published experiments, including some in top-rank journals, had been done \u201cwith small sample sizes, with no randomization of treatment and control groups, and without blinded evaluations of outcomes\u201d, he says. Benatar also found that in the spread of reported results for some drugs, there was statistical evidence that only positive results had been published. Informal conversations with other researchers convinced him that some had tried and failed to confirm reported positive results, but had never published those non-confirmations. All in all, a body of data that should have yielded useful information was, to Benatar, \u201cquestionable at best\u201d. He wrote a paper making what he could of the data, and also making clear how flawed he thought they were. When Benatar\u2019s paper 1  was published in  Neurobiology of Disease  in early 2007, it seemed to have little impact. But in the year and a half since, other investigators have come to similar, and indeed stronger, conclusions. \u201cThere is a dawning realization that we may not have designed our mouse drug trials rigorously enough,\u201d says Melanie Leitner, chief scientist at Prize4Life, a non-profit organization based in Cambridge, Massachusetts, that promotes ALS research. That realization is spreading: some researchers are coming to believe that tests in mouse models of other neurodegenerative conditions such as Alzheimer\u2019s and Huntington\u2019s may have been performed with less than optimal rigour. The problem could in principle apply \u201cto any mouse model study, for any disease\u201d, says Karen Duff of Columbia University in New York, who developed a popular Alzheimer\u2019s mouse model. In May, a dozen preclinical researchers and mouse model experts thrashed out the issues in a web discussion on the \u2018Alzforum\u2019 website, an online venue where researchers routinely gather to debate neurodegenerative disease issues. \u201cThere has to be sort of a course correction in the field,\u201d says Lorenzo Refolo, who oversees grants for preclinical work on neurodegenerative diseases at the National Institute of Neurological Disorders and Stroke in Bethesda, Maryland, \u201cotherwise these practices are just going to continue.\u201d The results of drug tests in mice have never translated perfectly to tests in humans. But in recent years, and especially for neurodegenerative diseases, mouse model results have seemed nearly useless. In the past year, for example, three major Alzheimer\u2019s drug candidates, Alzhemed (3-amino-1-propanesulphonic acid), Flurizan (tarenflurbil) and bapineuzumab, all of which had seemed powerfully effective in mouse models, have performed weakly or not at all in clinical trials involving thousands of human Alzheimer\u2019s patients. In the case of ALS, close to a dozen different drugs have been reported to prolong lifespan in the SOD1 mouse, yet have subsequently failed to show benefit in ALS patients. In the most recent and spectacular of these failures, the antibiotic minocycline, which had seemed modestly effective in four separate ALS mouse studies since 2002, was found last year to have worsened symptoms in a clinical trial of more than 400 patients 2 . The minocycline clinical trial, and the other ill-fated ALS-drug trials that preceded it, would never have happened had the prior mouse studies been done properly, says Sean Scott, president of the ALS Therapy Development Institute (ALS TDI), a non-profit biotech company based in Cambridge, Massachusetts. The ALS TDI was set up in 1999 to swiftly screen approved drug compounds for any that could slow the disease in ALS mice. In 2001, it found that ritonavir, an antiretroviral drug used against HIV, seemed to extend lifespan in the mice and the ALS TDI initiated a small safety trial in ALS patients. \u201cBut instead of just moving onto the next drug,\u201d says Scott, \u201cwe kept screening ritonavir in the mice, in the hope that we could learn how it worked and maybe improve on it.\u201d  \n                The drugs don\u2019t work \n              Scott and his colleagues were shocked to find that when they scaled up the ritonavir tests by adding more mice, the drug\u2019s effect on lifespan didn\u2019t become statistically clearer \u2014 it disappeared altogether. For the patients in the safety trial, the drug also showed no benefit, and at the highest dose, says Scott, \u201cit was even a little bit detrimental\u201d. Chastened by the experience and with the funding to find out what had gone wrong, the ALS TDI spent the next few years refining its mouse-trial methods and trying to characterize likely sources of error. Its scientists also applied their improved methodological rigour in further tests of prospective ALS drugs, including drugs such as minocycline that had seemed effective in other labs\u2019 mouse studies. \u201cBut really in the end,\u201d says Scott, \u201cwe were heartbroken, because even using dramatically more animals than any of those other labs, using very sophisticated pharmacological formulations, and paying attention to levels of the drugs achieved in the mouse nervous systems, we just could not get any of those drugs to work.\u201d At the time, in late 2006, the ALS TDI was funded in part by a US$200,000 grant from the Muscular Dystrophy Association (MDA), which has supported ALS research since the 1950s. Sharon Hesterlee, director of research development at the MDA, oversaw the grant and remembers Scott and his colleagues at meetings \u201ctaking some flak from members of the academic community, who in effect said \u2018if you can\u2019t reproduce any of our results you\u2019re just not doing it right\u2019.\u201d But Hesterlee had already concluded that the ALS TDI \u201cwere actually so much more rigorous in their approach, that if there was anything to find they would have seen it\u201d. The MDA soon organized a much larger grant to the ALS TDI, one that would provide it with about $6 million a year to continue its work. At the same time, Hesterlee also pressured the organization to publish its analysis of the problems with SOD1 mouse tests. \u201cThat way our own grant reviewers could start applying those lessons to other mouse studies,\u201d she says. The analysis that the ALS TDI eventually put together overlapped with Benatar\u2019s, but also went further: Scott and his colleagues concluded that the previous positive drug trials in ALS mice were likely to have been so plagued by non-drug-related variations in mouse lifespan that this \u2018noise\u2019 was really all they had measured. In the context of small sample sizes and a bias against negative results, they noted, a high degree of noise could easily have led to the appearance of positive results even when no drug effect had existed. In the ALS TDI\u2019s own work, the greatest source of noise had been observed when a mouse was not excluded appropriately from a study. Such an exclusion should have happened if, for example, an animal died young of non-ALS causes \u2014 Scott came to believe that mouse colonies in some academic labs were \u201cfilthy\u201d with infections \u2014 or failed to express enough copies of the mutant  SOD1  gene. Further noise could creep in if the treatment and control groups were not evenly matched by gender and litter-membership, because mouse lifespan naturally varied according to these factors. Above all, Scott and his colleagues came to recommend starting each study with at least 24 mice in each treatment or control group \u2014 roughly double the norm \u2014 to ensure that any real drug effect would statistically rise above the noise. Scott submitted the ALS TDI findings to the  Proceedings of the National Academy of Sciences  in late 2006 where, he says, \u201cone reviewer flamed us\u201d over the organization\u2019s failure to reproduce the positive mouse studies. Scott says he just \u201cwanted to get it out there\u201d and the paper was published in the journal  Amyotrophic Lateral Sclerosis  in January of this year 3 . The immediate response was \u201cmuted\u201d, Leitner observes. \u201cI do think it\u2019s a difficult thing for an academic researcher, working as hard and as fast as you can, to have some group tell you that you\u2019ve made mistakes in your experimental design \u2014 especially when that group isn\u2019t necessarily operating under the same constraints as you are.\u201d As a biotech company, the ALS TDI had never before published its research in a peer-reviewed journal. Although seven PhD scientists oversaw its laboratory work, its founder, James Heywood, was not a scientist but an engineer who had started the organization after his brother received an ALS diagnosis. Sean Scott had joined the organization as a volunteer after his mother developed the disease. He held a BA in rhetoric from the University of California, Berkeley, and had no formal training as a scientist. And he was now first author on a paper which concluded, in essence, that some high-powered academic researchers had been chasing their tails for years. Robert Friedlander, at Harvard Medical School\u2019s Brigham and Women\u2019s Hospital, and the lead author on the first positive study of minocycline in SOD1 mice 4  defends his work, saying that three other labs independently found similar results. \u201cThe fact that ALS TDI did not reproduce these results raises questions as to their methodologies,\u201d he says. As for the failed clinical trial of minocycline, Friedlander suggests that the drug may have been given to patients at too high a dose \u2014 and a lower dose might well have been effective. \u201cIn my mind, that was a flawed study,\u201d he says. Neurologist Jeff Rothstein, who runs a large ALS research lab at Johns Hopkins University School of Medicine in Baltimore, Maryland, says of ALS TDI, \u201cthey\u2019ve done some nice statistics\u201d. But the company\u2019s failure to reproduce his lab\u2019s positive study in 2002 of the anti-inflammatory drug Celebrex (celecoxib) in SOD1 mice 5  might have been due to differences in study design, he says. Rothstein says that his lab confirmed Celebrex\u2019s biological effect at reducing neuroinflammation, whereas ALS TDI didn\u2019t look for it. \u201cWere they at variance with us because they never got biological efficacy? Hard to know,\u201d says Rothstein. Celebrex later failed in a clinical trial in ALS patients. The findings of Scott and his colleagues do seem to be resonating now among preclinical researchers. Still, Leitner thinks that too many scientists \u2014 including those working on conditions other than ALS \u2014 remain unaware that their mouse-model studies may be flawed.  \n                Non-exacting standards \n              In the recent Alzforum event that Leitner organized, Scott and other researchers discussed the ALS TDI study and in general expressed concerns that these methodological issues went beyond ALS. \u201cPeople will do an experiment once with ten animals and get a result, and if it\u2019s the right result it gets published in a high-profile journal,\u201d says Duff, who has followed the ALS TDI story with keen interest. \u201cAnd there\u2019s no requirement that you show the effect again with a different set of mice, or in a larger group of mice, or in a different model.\u201d \u201cMany of the neurodegenerative models out there are on mixed and segregating genetic backgrounds,\u201d says Greg Cox, a forum participant and mouse geneticist who works for the world\u2019s largest provider of research-grade mice, the Jackson Laboratory in Bar Harbor, Maine. \u201cSo in those cases even [inbred] littermates aren\u2019t genetically identical. And if a genetic background effect [for example, on mouse lifespan] is there, you could end up measuring those background effects more than the drug effect you want to focus on.\u201d Mike Sasner, a neuroscientist also at the Jackson Laboratory, notes that spontaneous genetic changes often affect the disease-causing mutant gene directly. For SOD1 mice, and for Alzheimer\u2019s mice engineered to overexpress amyloid protein, changes are often seen in the number of copies of the disease-driving \u2018transgene\u2019. For Huntington\u2019s disease mice, spontaneous changes can alter the number of disease-causing repeat sequences within the transgene. \u201cSo if you\u2019re breeding these Huntington\u2019s mice over ten generations and you go from 100 repeats to 50 repeats, you\u2019re going to basically lose the phenotype,\u201d says Sasner. He adds that the Jackson Lab now checks the genetic make-up of its transgenic mice to reduce the problem, but not every researcher is aware of such issues. \u201cYou might create a mouse in your lab and distribute it to ten different people,\u201d he says, \u201cso there\u2019s ten different colonies all over the world, and they\u2019re all diverging from each other, genetically. So when I\u2019m publishing my paper I\u2019m talking about this mouse, and you think you have the same mouse in your hands. But do you really?\u201d Sasner says that he and other scientists now hope to draft a formal document with guidance for preclinical researchers on these issues. Leitner applauds such efforts, but thinks the National Institutes of Health (NIH) needs to get involved too. \u201cI believe that if the government doesn\u2019t encourage consideration of these issues and support them, it\u2019s going to be a very hard sell,\u201d she says. \u201cBecause basically the ALS TDI study suggests everyone needs to conduct much more thorough and expensive animal-model trials.\u201d Duff agrees. \u201cThere just aren\u2019t the resources now to do really large, well-powered mouse studies. So I think the NIH should get ahead of the curve here, for example setting up a programme and sending out Requests for Applications to study this, to look more at the mouse models and, in the same way that Sean Scott\u2019s group did, to see why they\u2019re failing to translate.\u201d Even Refolo thinks that the agency should take the lead in addressing these problems. \u201cI think there just has to be a [new] policy,\u201d he says. But he emphasizes that the NIH grant process is regulated largely by the academic community itself. \u201cThe academics have to embrace the policy, and people who are reviewing grants and papers have to embrace it and also the editors of these journals have to embrace it.\u201d And so far Refolo sees no sign of a major change coming. \u201cIf [these issues] are being addressed in study section, where these grants are reviewed, it\u2019s at least below my radar.\u201d Even if preclinical researchers ultimately do clear up the methodological flaws in mouse studies, they\u2019ll have other issues to deal with. For example, as Benatar noted in his 2007 paper, SOD1 mice are typically treated with drugs well before the onset of symptoms. Yet in nearly all human cases, Benatar says, \u201cwe have no capacity to initiate therapy presymptomatically, so it\u2019s wishful thinking to suppose that success with presymptomatic treatment in a mouse is going to translate into efficacy in a human\u201d.  \n                The wrong model? \n              Perhaps the biggest issue facing the field is whether the mouse models faithfully reproduce the biology of the human disease. Alzheimer\u2019s mouse models typically develop amyloid \u2018plaques\u2019 in their brains, but they do not develop an Alzheimer\u2019s-like dementia and anti-amyloid strategies have repeatedly failed to slow the disease in clinical trials. Parkinson\u2019s researchers have never had a good mouse model for the full disease process, and even the mouse model for Huntington\u2019s disease \u2014 a relatively simple genetic disease \u2014 does not fully reproduce the clinical signs seen in humans with the disorder. SOD1 mice have often been considered one of the most accurate animal models of any neurodegenerative disease. Although the  SOD1  gene is mutated in only around 20% of human familial cases (representing 2\u20133% of all ALS) the disease that the animals get so closely resembles the common, sporadic form of human ALS that the two maladies have been assumed to share a \u2018final common pathway\u2019 of neuronal destruction. That assumption is now being questioned. In the past two years, researchers have found evidence suggesting that a DNA-binding protein, TDP-43, could be the trigger for sporadic ALS when it is malformed or improperly processed inside cells \u2014 and that  SOD1 -driven ALS might really be a distinct disorder. \u201cThe idea that the SOD1 model could be extended to sporadic ALS patients is not holding up,\u201d says Virginia Lee, a neuropathologist at the University of Pennsylvania in Philadelphia, whose lab was the first to report the link between TDP-43 and sporadic ALS 6 . Her lab is one of several now racing to devise a mouse model of ALS carrying mutated TDP-43. The debate over SOD1\u2019s relevance to the majority of human ALS cases feeds a broader worry, namely that it may be unrealistic to think of modelling the full complexity of ageing-related human brain disorders in mice whose disease course is usually accelerated by a single, crude genetic modification. In theory, with their more human-like nervous systems, monkeys should make much better models of human neurodegenerative diseases. Aged vervets have already been used to test Alzheimer\u2019s vaccines; and Anthony Chan and his colleagues at the Yerkes National Primate Research Center in Atlanta, Georgia, have described the creation of macaques that carry the human Huntington\u2019s disease gene, and suffer from a very similar disorder 7 . \u201cI definitely think that the non-human primate models need to be brought to the forefront much more than they have been,\u201d says John Morrison, an animal-model researcher at Mount Sinai School of Medicine, New York. But aside from the moral issues that this would raise for some researchers, a switch to monkeys could be prohibitively expensive. Scott estimates that a six-month study of about 50 ALS mice would cost roughly $100,000, whereas Stuart Zola, who heads the Yerkes Center, estimates that more than $500,000 would be needed to study 50 ordinary macaques for two years \u2014 and possibly much more time and money would be required to study drugs for slow-burning neurodegenerative diseases whose effects may only become apparent with advanced age. Scott, Duff, Rothstein and others suggest that mouse models should still be used, but that drug tests in them should target specific, disease-related molecular pathways \u2014 for example, TDP-43 accumulations, if they turn out to be relevant \u2014 instead of broader endpoints such as lifespan or behaviour, where mice and men are inherently mismatched. Scott says he now sees the SOD1 mouse as \u201cperhaps a pathway model as opposed to a disease model, and if we can affect survival [with drugs], great\u201d. The ALS TDI is currently finishing a study of gene expression patterns in various tissues from SOD1 mice and humans with ALS, to find what molecular pathways of disease they share \u2014 if any. Mouse models could therefore end up being not only more difficult and expensive to use with acceptable rigour, but at the same time more narrowly predictive of the human condition. But whether preclinical researchers will accept such a radical change remains to be seen. \u201cI think there\u2019s a sense of desperation that we need a convenient model for bringing drugs to clinical trial,\u201d says Benatar. \u201cAnd I do sort of hear that concern.\u201d But desperation, he adds, is an inadequate justification for the continued use of a poor model. \u201cIt\u2019s a bit like the proverbial drunk who keeps looking for his lost keys under the lamp post, simply because the light\u2019s better there.\u201d Jim Schnabel is a freelance writer based in Maryland. \n                     Focus on Neurodegeneration \n                   \n                     User\u2019s Guide to the Mouse \n                   \n                     ALS Therapy Development Institute \n                   \n                     National Institute of Neurological Disorders and Stroke \n                   Reprints and Permissions"},
{"file_id": "454393a", "url": "https://www.nature.com/articles/454393a", "year": 2008, "authors": [{"name": "Jane Qiu"}], "parsed_as_year": "2006_or_before", "body": "Climate change is coming fast and furious to the Tibetan plateau. Jane Qiu reports on the changes atop the roof of the world. The Tibetan plateau gets a lot less attention than the Arctic or Antarctic, but after them it is Earth's largest store of ice. And the store is melting fast. In the past half-century, 82% of the plateau's glaciers have retreated. In the past decade, 10% of its permafrost has degraded. As the changes continue, or even accelerate, their effects will resonate far beyond the isolated plateau, changing the water supply for billions of people and altering the atmospheric circulation over half the planet. The plateau's pivotal role is due almost entirely to its height. Being an average of 4 kilometres above sea level makes it peculiarly cold for its latitude \u2014 colder than anywhere else outside the polar regions. Lhasa, capital of the Tibet Autonomous Region, is by Tibetan standards relatively low-lying, at 3,650 metres \u2014 yet it is higher even than La Paz, Bolivia, the highest capital city of a country. Lhasa's year-round average temperature is 8 \u00b0C; at the same latitude Houston, Texas, has an average temperature of 21\u00b0C. The altitude makes Tibet cold, especially in winter; its snow and ice cover, by reflecting sunlight, make it colder still. The very bulk of the plateau affects how winds circulate above it, and its altitude also places the surface simply closer to the stratosphere than is normal. The proximate cause of the changes now being felt on the plateau is a rise in temperature of up to 0.3 \u00b0C a decade that has been going on for fifty years \u2014 approximately three times the global warming rate. The questions are how much more change to expect in the future, and how severe the effects will be on the planet's climate as a whole. \u201cOur understanding of global climate change would be incomplete without taking into consideration what's happening to the Tibetan plateau,\u201d says Veerabhadran Ramanathan, an atmospheric scientist at the Scripps Institution of Oceanography in La Jolla, California. Perhaps surprisingly given its significance, the potential impact of the Tibetan plateau is still unfamiliar to many climatologists. One reason is that there are far fewer data available compared with the Arctic and Antarctic, which have seen a far greater number of scientific expeditions to plumb their secrets. Although fieldwork there can be tough, the plateau offers the same physical isolation coupled with political challenges, at least for Western researchers. \u201cThe plateau's remoteness, high altitude and harsh weather conditions make any research on the region very challenging,\u201d says Yao Tandong, director of the Institute of Tibetan Plateau Research, headquartered in Beijing, of the Chinese Academy of Sciences. Yao and his colleagues should know: in the 1980s, they were among the few researchers persevering in difficult field conditions to gather data on the plateau's past climate history. They drilled ice cores, up to 300 metres long, from Himalayan glaciers 7,200 metres high. \u201cIt's all done manually, and we had to carry them down the mountain. There were no helicopters, no heavy equipment,\u201d he says. \u201cIt's -30 \u00b0C, with the wind cutting through us like a knife. It's no mean feat.\u201d Such ordeals seem to have paid off: in collaboration with glaciologist Lonnie Thompson of Ohio State University in Columbus, the team's work on oxygen isotopes within the cores yielded the most comprehensive temperature reconstruction for the plateau, showing a large-scale warming trend that began in the twentieth century and is amplified at higher elevations 1 . Their findings are consistent with temperature records from meteorological stations that have made continuous measurements since the 1950s [ref.  2 ]. Some of this is what you would expect in a world undergoing greenhouse warming, but there are regional factors on the plateau that exacerbate the effect. In summer, dust from regional deserts blows towards and up against the northern and southern slopes of the plateau. One recent satellite study, for instance, tracked dust wafting in from the Taklamakan desert to the north 3 . \u201cWe were really surprised to find this much dust over the plateau,\u201d says Huang Jianping, an atmospheric scientist at Lanzhou University and lead author of the study. The dust layers can reach as high as 10 kilometres above sea level, where they both absorb and reflect sunlight, changing the amount of radiation that reaches the plateau. Combining with the dust to drive climate change are emissions of 'black carbon', the soot that results when people cook with biofuels such as wood, crop waste or dung. Southeast Asia, including the Himalayas, is one of the global hotspots for black-carbon emissions 4 . Using unmanned aircraft, Ramanathan and colleagues measured the amount of sunlight absorbed by black carbon, and found that it contributes as much as 50% of the solar heating of the air 5 . \u201cIt's the second-largest contributor to atmospheric warming in the region, after carbon dioxide,\u201d he says. He estimates that the combined effect of black carbon and greenhouse gases may be sufficient to account for a warming trend of 0.25 \u00b0C per decade in the Himalayas, roughly what has been observed so far. When black carbon settles on Himalayan glaciers, it darkens the snow and ice so that they absorb more heat and become warmer. \u201cThe melting seasons on the plateau now begin earlier and last longer,\u201d says Xu Baiqing of the Institute of Tibetan Plateau Research. Glaciers at the edge of the plateau tend to melt more than those in the middle; one study, for instance, showed that glaciers in the eastern part of the Kunlun Mountains retreated by 17% over the past 30 years, which is ten times faster than those in the central plateau. If current trends hold, two-thirds of the plateau glaciers could be gone by 2050, says Yao.  \n                Floods and droughts \n              The melting glaciers are starting to leave behind dangerous glacial lakes, in which meltwater ponds behind a dam of debris left by the retreating ice tongue. Scientists have identified 34 such glacial lakes on the northern slopes of the Himalayas, and 20 outburst floods have been recorded in the past 50 years. The risk of floods, though, is but a short-term danger far exceeded by long-term issues with water supplies atop the plateau. Runoff from the region's mountains feeds the largest rivers across Southeast Asia, including the Yangtze, Yellow, Mekong, Ganges and Indus rivers. If glaciers continue to retreat and snowpack shrinks atop the plateau, the water supplies of billions of people will be in danger 6 . A large-scale thaw of permafrost would result in the loss of its water content and trigger an ecological catastrophe. Ouyang Hua  Permafrost is also at risk, as rising temperatures cause the 'active' ground layer \u2014 which freezes and thaws every year \u2014 to thicken. That, in turn, affects how heat and moisture flow between the ground and the atmosphere, further perturbing the system 7 . Degradation of permafrost will not only put the Qinghai\u2013Tibet Railway at risk 8 , but also endangers the plateau's alpine ecosystems, which rely on permafrost to trap water in the topmost layers of soil to allow plants to thrive at an altitude that would otherwise be too hostile for them. \u201cA large-scale thaw of permafrost would result in the loss of its water content and trigger an ecological catastrophe,\u201d says Ouyang Hua, deputy director of the Institute of Geographical Sciences and Natural Resources Research in Beijing. As permafrost stores one-third of the world's soil carbon, vegetation loss would lead to a huge amount of carbon entering the atmosphere, exacerbating global warming.  \n                Competing forces \n              With all the changes the Tibetan plateau is undergoing \u2014 a warming climate, retreating glaciers, degrading permafrost and alpine ecosystems \u2014 what are the implications for the regional and global climate? The first and most important victim could be the Indian monsoon. This strong seasonal wind results from differences in the thermal properties between land and ocean. In summer, the vast land in Asia heats up more than the Indian Ocean, leading to a pressure gradient and the flow of the air and moisture from the ocean. The rise of the Tibetan plateau starting 50 million years ago (see  'Lifting the roof of the world' ) is thought to have strengthened this effect. As the land surface absorbs more sunlight than the atmosphere, the plateau creates a vast area of surface warmer than the air at that elevation, thereby increasing the land\u2013ocean pressure gradient and intensifying the monsoon. Some climate models show that global warming would lead to a greater increase in the plateau's surface temperature than over the ocean, thus augmenting the monsoon. On the other hand, some models suggest that aerosols that absorb solar radiation, and changes in land use in the region, could weaken the monsoon. \u201cThe intensity of the monsoon is likely to depend on which of these two competing forces dominates,\u201d says Ramanathan. No matter what the causes are, some studies indicate that the weakening force may be prevailing, or has prevailed for at least the past three centuries. Duan Keqin, of the Cold and Arid Regions Environmental and Engineering Research Institute in Lanzhou, and his colleagues reconstructed a 300-year history of snow accumulation by analysing ice cores from the Dasuopu glacier 9 . They believe the ice there preserves an estimate of monsoon variations in the Himalayas. \u201cWe found that the warmer it was, the weaker the monsoon,\u201d says Duan. On average, a temperature increase of 0.1 \u00b0C was associated with a decrease of 100 millimetres in snow accumulation. But similar studies on other parts of the plateau are needed to confirm the results, he notes. \u201cChanges in the Indian monsoon are not the only threat in Asia to the global climate,\u201d adds Rong Fu of the Georgia Institute of Technology in Atlanta. Her research shows that convection over the Tibetan plateau can transport water vapour and pollutants to the stratosphere[10], the atmospheric layer that is immediately above the troposphere and contains most of the Earth's ozone. \u201cThe strong, horizontal wind in the stratosphere could then spread the water vapour and pollutants globally,\u201d says Fu. Water vapour has a stronger greenhouse effect than carbon dioxide per molecule, but it normally reaches no higher than 1\u20132 kilometres below the stratosphere. The situation is different over the plateau, over which the convection layer is shifted some 6 kilometres further up so that its top boundary is around 18 kilometres up, in the lower stratosphere. In addition, the troposphere is thinner over the plateau, and the heat emitted by the surface can reach higher and make the air warmer at the base of the stratosphere. \u201cSo more water vapour is able to get to the stratosphere without being frozen or precipitated,\u201d says Fu. Warmer temperatures over the plateau can resulting increased glacial melting and water-vapour transport \u2014 which, in turn, causes strong convection and lifts even more water vapour up. \u201cIt's very worrying to think that a lot of it may reach the stratosphere,\u201d she says. \u201cWorrying\u201d, indeed, best captures the mood of researchers who work on the Tibetan plateau. They are keen to undertake large-scale, comprehensive studies and to collect as many data as possible. \u201cWe know so little about it and understand it even less,\u201d says Yao. One ongoing study is to document all the glaciers in China, recording characteristics such as their location, area, length, thickness and the position of the snow line. A similar survey was conducted between 1978 and 2002, which scientists believe could serve as a reference point to reveal any major changes. In addition, glaciologists continue to identify and closely monitor potentially dangerous glacial lakes in hopes of heading off any potential outburst floods.  \n                Quick way out \n             Reducing emissions of greenhouse gases and black carbon should be the top priority. Xu Baiqing  Meanwhile, others focus on the bigger picture of how to tackle pollution problems in Asia. \u201cReducing emissions of greenhouse gases and black carbon should be the top priority,\u201d says Xu. Ramanathan reckons that cutting down on black-carbon emissions could be a \u201cquick way out of the mess\u201d, given that its half-life in the atmosphere is about 15\u201320 days compared with the century-scale half-life of carbon dioxide. His simulations suggest that, just by removing traditional ways of cooking with wood, dung and crop residues, some 40\u201360% of the black-carbon emissions would be gone. This could be \u201ca short-term fix, a low-hanging fruit that is much cheaper and faster\u201d than reducing carbon dioxide, he says. \u201cThe key is to give villagers access to better forms of energy.\u201d In the end, the Tibetan plateau may be a crucial testing ground for how humans and the environment collide in a globally warmed world. Can the world's third pole be saved? \u201cLet's hope that the changes the plateau is going through are only transient,\u201d says Yao. \u201cWhat we do about them probably will determine what's going to happen to it in the future.\u201d Jane Qiu writes for  Nature  from Beijing. See Editorial,  page 367 , and News Feature,  page 384  For a podcast and more on China see  www.nature.com/news/specials/china/ \n                     China Special \n                   \n                     Institute of Tibetan Plateau Research \n                   Reprints and Permissions"},
{"file_id": "454024a", "url": "https://www.nature.com/articles/454024a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "Launched in 1977, NASA's Voyager missions transformed humanity's view of the Solar System. Now in their fourth decade, they are sending back information about the borderlands of interstellar space. Here, three veterans recall details and moments that meant something special along the way. \n                The little motor that could \n              We needed an instrument that scanned the sky in order to determine the direction that particles flying past the spacecraft were travelling in. But how do you get an instrument to scan 360\u00b0 of the sky when the spacecraft on which it's mounted doesn't rotate, and must always point to Earth? Our answer was to mount the instrument on a small platform and have an electrical stepping motor move it 45\u00b0 every minute or two \u2014 sometimes a lot faster. The engineers, being a conservative bunch, thought that after a few thousand steps the motor would seize up. Remember, it was 1971! A design that involved moving parts on a spacecraft was not considered a good idea \u2014 the initial response was 'Are you crazy?'. I insisted, and we tested the motor for about 500,000 steps, which was twice what we thought we'd need for the trip to Saturn. \n               boxed-text \n             Over 30 years on and after well over 5 million steps the little motor is still stepping dutifully once every 192 seconds. That's 10 million steps if we count Voyagers 1 and 2. Not only did we get wonderful directional data as we went through the magnetospheres of four planets, but the direction of particle flow signalled the approach to the termination shock a couple of years before we actually got there! It was the riskiest decision I ever made, but it paid off big time, scientifically. For a scientist, the Voyager mission is the stuff that dreams are made of. \n                The cup that cheers \n              The instrument concept for the Plasma spectrometer \u2014 the 'Faraday cup' design \u2014 dates from the beginning of the space age. The late Herb Bridge, my mentor at the Massachusetts Institute of Technology, used something very similar to make the first measurements of the energy spectrum of the shocked solar wind in the earth's magnetosheath in 1961, when he was principal investigator on the plasma instrument on Explorer 10. And there's an excellent chance that, before I retire, the Faraday cup on Voyager will directly measure the plasma in the interstellar medium. I was in high school when Explorer 10 went up, and in my lifetime we have gone from being just above the atmosphere to reaching for the interstellar medium. Pretty nifty. Everyone knows about the golden records on the Voyagers that Carl Sagan worked on. Our cup collector has a message too. The names of all the people at MIT who worked on the plasma probe are written on the surface underneath the motto \u201cLive Free or Die\u201d. Our chief engineer on the project was Bob Butler, a New Hampshire resident, and that's the New Hampshire state motto. \n                The mission of a lifetime \n              The most exciting moments for me were the six close planetary encounters, especially Voyager 2's discovery of previously unknown and unpredicted but significant intrinsic global magnetic fields at Uranus and Neptune, with their entrapped radiation belts. It was surely as exciting as discovering the magnetic field of Mercury in 1974 and 1975 with Mariner 10, or discovering the complexly magnetized crust of Mars in 1999 with the orbiting Mars Global Surveyor. For Voyager 2's Uranus fly-by in 1986 the magnetometer team invited James Van Allen, discoverer of Earth's radiation belts in 1958, to join us at the Jet Propulsion Laboratory for the encounter activities. He was as delighted to participate in the real-time learning episode as if he were a fellow principal investigator. Over the years the magnetometer team has been downsized by 50%, mainly by recent retirements. But along with long-time NASA Goddard Space Flight Center colleagues Mario Acu\u00f1a and Leonard Burlaga we soldier on as the data continue to flow in. It continues to be the mission of a lifetime. For more information on the Voyager missions, see the News and Views article on  page 38  ; the sequence of six Letters starting on pages  63 ,  67 ,  71 ,  75 ,  78  and  81  the video at  http://www.nature.com/nature/videoarchive/voyager  and the podcast at  http://www.nature.com/nature/podcast \n                     NASA's Voyager site \n                   \n                     Spacecraft leaving the solar system \n                   \n                     Voyager at the NSSDC \n                   Reprints and Permissions"},
{"file_id": "454388a", "url": "https://www.nature.com/articles/454388a", "year": 2008, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "China burns more coal than any other country; how it does so in the future will determine our planet's climate. Jeff Tollefson reports from Beijing. Huang Bin, a 30-year-old engineer, is surveying the scene at one of China's showcase energy projects: a retrofit that will make the Gaobeidian coal-fired power plant in Beijing burn just a little bit cleaner. Three engineers in red hard hats pore over a blueprint, their fingers tracing lines on paper splayed across a steel tank. Two workers adjust a valve nearby, one of hundreds on a two-storey platform erected alongside two 30-metre-high vessels that will house the chemical reactions at the heart of the project. Sparks fly as welders connect pipes; the buzz of grinders comes and goes. The ground was broken on this project just three months ago, and even an outside observer can tell that there is plenty still to do. But no one seems to doubt that the world's latest carbon-capture pilot plant will be finished in three weeks' time. \u201cChinese speed,\u201d Huang says with a smile. That was in late June. Last week, as planned, the new unit began stripping carbon dioxide out of a small stream of exhaust from the plant, a high-efficiency, 1,065-megawatt monster that churns out 10% of Beijing's power and one-third of its hot-water heat. The Huaneng Group, the government-owned company that runs the plant, plans to collect less than 1% of the CO 2  emitted here, ultimately to provide some of the fizz in locally made carbonated drinks. It is a modest goal, but for China the project is a gesture of goodwill, a tentative step into the kinds of technologies needed to decarbonize an economy that derives more than two-thirds of its energy from coal. For years, China has lagged behind the West in researching ways to burn coal more cleanly, but that is beginning to change. Huang and his colleagues are coming of age in an era in which the Chinese learn by doing, and what they are doing today is advanced coal technology. The total time for the Gaobeidian retrofit from announcement through design and commissioning was nine months. Chinese speed has raised entire cities and built modern highways, all while providing at least basic energy services to most of its 1.3 billion people. It has also frightened a world already alarmed by global warming. The planet's most populous nation has added some 170 gigawatts of coal-fired power capacity in the past two years alone \u2014 more than double Britain's entire electricity-generating capacity, installed over a century \u2014 and has overtaken the United States as the world's largest emitter of greenhouse gases. Yet China's single-minded determination to get things done, if properly harnessed, could drive down costs and commercialize advanced coal technologies that have languished in labs and boardrooms in the West. In many ways, China has already positioned itself at the forefront of coal technology, but 'advanced' does not necessarily mean clean. Climate-friendly technologies would enable companies to capture and pump CO 2  underground, eliminating most of the emissions from coal. By contrast, even new technologies for converting coal into transportation fuels without carbon capture might increase China's reliance on coal, as well as its emissions. When the Chinese government says it is going to do something, it will do it. Lu Xuedu  \u201cIt's relatively easy for me to imagine the Chinese will get way out in front of us in the United States and Europe,\u201d says Kelly Sims Gallagher, an expert in China energy at Harvard University in Cambridge, Massachusetts. \u201cThe Chinese are committed to installing advanced technology. The question right now is which technology it will be.\u201d So far, China's industrial revolution resembles a compressed version of that experienced in the West, with all the associated environmental problems and resource limitations. Evidence suggests that solutions, too, may come in rapid-fire fashion. An oft-cited statistic is that the Chinese bring a new power plant on line every week or two; less appreciated is that today's power plants generally employ state-of-the-art combustion technology, whereas older, less-efficient plants are being shut down. The main goal is to save coal. China's coal reserves rank as the world's third largest; the country mined and then devoured some 2.5 billion tonnes of coal last year, more than double the tonnage of the next-largest user, the United States. Still, the mining industry has struggled to meet demand, and imports are on the rise.  \n                Efficiency drive \n              The government has also made energy efficiency its de facto climate policy (see  'Kicking the coal habit' ), beginning with an ambitious effort to cut energy intensity (the amount of energy consumed per unit of gross domestic product) by 20% from 2006 to 2010. The emphasis is on the manufacturing industries, such as iron, steel and cement, which consume 68% of the nation's electricity and even more of its overall energy. It's not clear whether China will meet that goal on time. For many observers, though, what makes the policy real is the fact that national communist leaders now grade local officials according to their progress on energy efficiency. The government is also taking aim at conventional pollutants from coal-fired power plants, hoping to curb acid rain and the dense smog that envelops many of China's cities. Roughly half of China's power plants are now equipped with 'scrubbers' for sulphur dioxide emissions. Most of these have been installed since 2006, and there are more to come. \u201cChina now has more scrubber capacity than all of the rest of the world put together,\u201d says Robert Williams, a senior scientist at the Princeton Environmental Institute in New Jersey. Nitrogen oxides are likely to be next on the clean-up list. According to the official government line, such efforts are intended to create a wealthier and more 'harmonious' society. At the same time, leaders are under pressure from an increasingly large and vocal middle class that aspires to a cleaner, more prosperous lifestyle. Also telling is that the government has acknowledged in public documents the cold economics of pollution-related deaths and disease. Pollution is likely to slash the country's gross domestic product by anything from 2% to an eye-popping 18% by 2020, depending on how successful the clean-up initiatives are, according to Ming Lei, an environmental economist at Peking University. Those estimates do not include the potentially enormous effects of climate change. Both politicians and scientists foresee huge problems with increased floods, dwindling crop yields, and less freshwater run-off as Himalayan glaciers recede (see  page 393 ). But only a new carbon economy or a regulatory directive from the government \u2014 probably preceded by some kind of international climate agreement \u2014 is going to change the status quo, as businesses currently have no incentive to curb CO 2  emissions. \u201cUnless you tell them this will make them money, then they say 'no',\u201d says Zhang Hai, an engineering professor at Tsinghua University in Beijing. \u201cNobody is a volunteer.\u201d Fifty kilometres southeast of Beijing, in the industrial city of Langfang, China's energy economics are on display at the new headquarters and research facilities of ENN. This independent company with global ambitions is now betting big on technologies for converting coal into a substitute for diesel fuel. A visitor is driven up to the new, six-storey office building in a black company Audi, complete with tinted windows. Outside, bulldozers are clearing land for three new labs; already standing are pilot plants for coal gasification, biofuels and solar projects, as well as a solar-cell manufacturing plant. All of this has been achieved in the past year, during which ENN has hired some 4,000 employees, boosting its workforce by 20%. The facility has its own hotel, restaurant and golf course. Next up: a university. ENN began as a rental-car company in 1989 and made its money as a distributor of natural gas and other fuels. It is now pursuing coal gasification, an old technology that glimmers whenever petroleum supplies seem threatened, and is a leader in the new wave of interest in underground gasification. At its simplest, the technique involves drilling a well, igniting the coal within it and adding oxygen; another well sucks out the resulting 'syngas', a mixture of mostly hydrogen, carbon monoxide and CO 2 . The syngas can then be condensed to make liquid fuel or chemicals. Most of the early research projects in this area have run their course, although one commercial project has been operating in Uzbekistan for more than four decades. Other companies are planning projects in the United States, Canada and beyond. ENN says it has been operating two pilot projects in Inner Mongolia since last year and is now developing a commercial-scale facility. Gan Zhongxue, ENN's chief scientist, readily admits that his company is several years behind some of the most advanced Western companies pursuing gasification technologies. ENN purchased its first gasifier from US-based General Electric, one of several multinationals seeking a piece of the action in China. Gan says that the company is now talking to a different US firm about a partnership that would allow ENN to deploy new technologies in the field. If it works, both companies could profit from subsequent growth and exports back to the United States and Europe. Advocates argue that underground gasification could be one of the wisest ways to use coal, in part because it eliminates the cost \u2014 and energy \u2014 of mining and transportation. Cooking the coal in place also leaves unwanted pollutants in the ground, and any CO 2  stripped out during the chemical processing can be pumped right back where it came from. ENN isn't interested in burying CO 2  (at least not until there is money in it), although Gan is trying to diversify his energy portfolio and thus envisages using the CO 2  to stimulate the growth of algae for biofuel. And that's the problem: unless the carbon is actually captured at its source and sequestered in some form, even the newest and fanciest coal-based liquid fuels put roughly double the CO 2  into the atmosphere compared with fuels derived from oil. China is under pressure to avoid doing exactly that, and the state-owned Shenhua Group is considering carbon capture and storage for the US$1.5-billion coal-to-liquids plant it expects to start up this year in Inner Mongolia. Shenhua is using its own technology to convert some 3.5 million tonnes of coal into diesel and other transportation fuels, equivalent to more than 24,000 barrels of oil per day. The plant will also recycle water and waste products, making it cleaner than older coal-to-liquids technologies, says Julio Friedmann, a researcher at Lawrence Livermore National Laboratory in Berkeley, California. \u201cIt's an engineering marvel.\u201d China views the Mongolia plant as a technology showcase, and many think that Shenhua will eventually move forward with a plan to bury as much as 85% of the plant's CO 2  emissions. Without making promises, Ren Xiangkun, Shenhua's vice-president and head of its Coal Liquefaction Research Centre, says that the company attaches \u201cgreat importance\u201d to carbon management. Coal-to-liquids projects will move forward in \u201cclose connection\u201d with the development of carbon-capture and sequestration technologies, he says. Even if CO 2  is captured during production, however, the carbon in the fuels remains. That means the best hope is to come out neutral on greenhouse emissions.  \n                Deep burial \n              The economics seem to be enough to support sequestration. Qingyun Sun of West Virginia University in Morgantown is working with Shenhua and the US Department of Energy on the project, and says that the plant will make money as long as the price of oil is above $45 per barrel. Capturing and storing carbon emissions adds another $5\u20138 per barrel, but with the oil price hovering at around $130 per barrel, \u201cthat is still very profitable\u201d, Sun says. Nearby oil and gas fields could hold some of the extracted CO 2 , but with volumes exceeding 3 million tonnes of CO 2  annually \u2014 larger than any sequestration project in the world so far \u2014 the ultimate target will have to be saline aquifers or deep coal seams. Any lessons learned here might need to be applied throughout the industry. Shenhua's plant isn't even on line yet, and the company is already planning an expansion. Shenhua is also partnering with South African coal-to-liquids giant Sasol to build another pair of plants that could each produce 80,000 barrels of fuel, or 3.4 million tonnes, per day. In all, seven coal-to-liquids plants are under construction in China, according to Sun, and many more are in the planning stages. \n               boxed-text \n             China has also been given an opportunity \u2014 one that it didn't ask for \u2014 to lead the world in developing the first low-emission coal-fuelled power plant, by coupling coal-gasification technology with carbon capture and storage. Integrated gasification combined cycle (IGCC) is a leading technology at present because the gasification process strips out conventional pollutants and produces a clean gas to generate power (see graphic). The two-stage electrical generation converts more energy into electricity, and the plant can be configured to produce a relatively pure stream of CO 2  that can be siphoned off \u2014 for a price. Until earlier this year, the United States had been the assumed leader in the race to build the first IGCC plant with carbon capture. But in January, the US Department of Energy cancelled the signature project, called FutureGen, citing disputes with its industry partners over the $1.8-billion cost. The decision baffled and angered Chinese officials and scientists at the Huaneng Group, who were partners in the project. \u201cThis will not happen in China,\u201d says Lu Xuedu, who handles global environmental affairs as deputy chief of China's Ministry of Science and Technology. \u201cWhen the Chinese government says it is going to do something, it will do it, surely.\u201d  \n                Race for the future \n              With FutureGen off the table (at least in its original design or until the White House has a new occupant), the race is on between China and Australia to build the first plants. In China, Huaneng is leading a consortium that hopes to complete a 250-megawatt pilot IGCC plant by next year, then commission by 2015 a 400-megawatt plant complete with hydrogen production, fuel-cell electricity generation and carbon sequestration. Total cost: $1.5 billion, almost entirely funded by industry, although project officials say that figure could rise. The final permit has yet to be approved by China's National Development and Reform Commission (NDRC), but 'GreenGen', as it is known, has already \u2014 unofficially \u2014 broken ground along the coast in Tianjin, south of Beijing. The Australians are taking a different approach with the Aus$1.2-billion (US$1.17 billion) 'ZeroGen' IGCC plant in Queensland. Project managers aim to commission a 115-megawatt pilot plant with 75% CO 2  capture and storage by 2012, followed by a 400-megawatt unit with 90% CO 2  capture by 2017. ZeroGen has already brokered agreements with local landowners and begun drilling test wells into a saline aquifer. \u201cIf we can crack it, then that has the greatest commercial application all around the world,\u201d says Chai McConnell, corporate affairs manager for the ZeroGen consortium. China has an extensive base of real-world gasification experience. John Thompson  Aside from GreenGen, the Chinese Ministry of Science and Technology has supported several demonstration projects that target either IGCC or 'polygeneration', which uses gasification technology to produce both power and chemicals. In addition, companies have submitted at least a dozen other IGCC applications to the NDRC, according to multiple industry sources. All these projects are pending, generating endless speculation, but few within the coal industry expect the government to approve them all. Lu thinks that as few as one or two will make it through. That's not nearly enough to make much of a difference in terms of overall greenhouse-gas emissions, given that China could bring hundreds of coal-fired plants on line in the coming years. The problem is the cost. An analysis by Gallagher and her colleagues suggests that IGCC capital technology costs upwards of 50% more than pulverized coal in China \u2014 and that's without adding carbon capture and storage. Advanced coal power plants thus need more government subsidies or higher electricity costs, which in turn eat into government priorities such as poverty relief and economic growth. Lu says that many companies are ready to take the lead on IGCC technology, but the government has to make its own decision. If companies fail, he says, \u201cthey come to the government saying, 'give me the money'\u201d. Irrespective of how these initial IGCC plants fare, China will continue to develop its expertise in gasification technologies for producing chemicals. In some cases, these plants might even deploy small-scale power production on the side, a trend that some experts think is already under way. A new gasifier patented by East China University of Science and Technology, for instance, has been licensed at nearly 30 plants in China, according to the Clean Air Task Force, a US-based environmental group. \u201cWhat China has going forward is an extensive base of real-world gasification experience,\u201d says John Thompson, who handles coal issues for the group. \u201cThat counts for a lot.\u201d Politicians are quiet when it comes to spending massive amounts of money. Derek Taylor  Even as new coal-fired plants come on line, the old ones might eventually need to be shut down or retrofitted with carbon-capture equipment. China and Australia are collaborating on several retrofit projects, including the Gaobeidian plant and some in Australia. Plenty of other pilot-scale projects are under way around the world, but the United Kingdom might well be the first country to implement post-combustion capture on a large scale. Last November, the UK government launched a competition to demonstrate 90% capture and storage on a 300\u2013400-megawatt plant by 2014. This is also the only candidate to meet the criteria for a broader call by the European Commission for upwards of a dozen commercial-scale coal power plants with carbon capture and storage by 2015 (it's not yet clear, though, where the money will come from). \u201cThis is what I like about GreenGen. The Chinese government decided 'we will do this' and it will be done. We in Europe rather dither about it,\u201d says Derek Taylor, who works on energy issues at the European Commission. \u201cWe've had projects announced in Europe, but none of them is that far down the line. Politicians are quiet when it comes to spending massive amounts of money on it, but massive amounts of money need to be spent.\u201d  \n                Carbon credit \n              In the end, the issues faced by all these technologies are who pays, and how much. One possible source of money is the Clean Development Mechanism (CDM) of the Kyoto Protocol, which allows companies in developed nations to pay for projects that reduce emissions in developing countries. But the sums of money currently changing hands are too small. Contracts under the European emissions-trading scheme could bring in roughly $7 billion in credits to China between 2008 and 2012. That might be a hefty figure but it's not enough to affect broader energy trends in China, says Fu Ping, who works on the CDM programme under the Chinese finance ministry. Moreover, the CDM programme would need to be revised and ramped up if it is to work for even one integrated carbon-storage project. First of all, it cannot currently be used to promote underground carbon storage, simply because the rules and regulations are not in place. Fu says that the United Nations, which administers the programme, has already had talks about changing that, but it might not make much of a difference until the prices for carbon storage and credits converge. CO 2  credits currently sell for $13\u201320 per tonne on the primary market in China, he says, and estimates for the cost of storing CO 2  from coal-fired power plants generally hovers around $50 per tonne. Last year, the Bush administration proposed an international fund for direct investment into these types of technologies. The idea has garnered international support among the G8 leading industrialized nations, which have so far committed roughly $6 billion. Such a fund could be used to support clean-energy projects and, in the case of China, help pay for the additional costs of managing CO 2  emissions. The World Bank would manage the fund, but no decisions have been made on exactly how much money there would be or how it would be administered. Harvard's Gallagher has been advocating this approach for some time, arguing that a simpler and more aggressive tool than the CDM is needed to change the development pattern in China and other developing countries. One idea being aired in climate circles in China is that the country could halve its energy intensity by 2020, then commit to levelling out emissions in the subsequent decade. A similar idea arose last year in a study sponsored by WWF China and headed by Lu and other government officials. That document also pegged the necessary investment in clean-energy technologies at roughly $220 billion. Lu plays down that number, pointing out that everything that everybody, including the Chinese, thought they knew about energy development in China five years ago was wrong. He is also confident that China can and will tackle the problem one way or another. But if the goal is a rapid transition to a green economy, he says, the West would be wise to open the money spigot a little wider and send along its best technologies as well. \u201cWe need help,\u201d he says. For a podcast and more on China see  www.nature.com/news/specials/china \n                     China Special \n                   \n                     Energy Web Focus \n                   \n                     Shenhua Group \n                   \n                     Huaneng Group \n                   \n                     Energy Information Administration page on China \n                   \n                     ZeroGen \n                   Reprints and Permissions"},
{"file_id": "454270a", "url": "https://www.nature.com/articles/454270a", "year": 2008, "authors": [{"name": "Eric Hand"}], "parsed_as_year": "2006_or_before", "body": "Not all NASA launches need rockets and countdowns. Eric Hand sees the alternative in Fort Sumner, New Mexico. On the day of the launch Mark Cobble arrives well before dawn. Lightning studs the sky on the horizon; halogen spotlights bathe the three-story NASA hangar in light. The desert around Fort Sumner municipal airport in New Mexico is still, its silence relieved only by the hum of a generator, the clapping of an exhaust flap on a helium truck and the slight rustle of the wind in the scraggly trees and scrub. That windy rustle bothers Cobble, the NASA crew chief in charge of the launch that is about to get under way. Since 02:30 the airport manager has been sending up weather balloons the size of beach balls every half hour or so. Ross Hays, a former CNN weatherman now with NASA, has been on site since 01:00 to gather weather data, armed with two flasks of coffee and the peanut butter and jelly sandwiches he reserves as a treat for 'show days'. \u201cNo chance of precip?\u201d Cobble asks. \u201cNope,\u201d says Hays, an optimist. \u201cIt's all over in the panhandle. It's all clear over New Mexico.\u201d Cobble looks at the weather data gleaned from the Air Force, the National Weather Service and a network of instruments at public high schools. He strokes his chin. The wind is definitely worrisome. But the weather is only going to worsen as the week goes on. \u201cWe'll see how it goes and hope for the best,\u201d he finally says. \u201cI'll get 'em started.\u201d If your image of a NASA launch is a mission control room full of computers, pocket protectors and neck-ties, think again. This is country-and-western NASA, all boots and blue jeans; the tools of the trade are Buck knives, duct tape, parachute cord and patience, plus a whole lot of helium. And the 'spacecraft' hang beneath balloons. They have to prove the technology before people start to take them seriously. Simon Swordy  Balloons have long offered a cheap alternative to rocketry. Even if by their very nature they cannot rise above the atmosphere, they can still rise above 99% of it, leaving dust, weather and water vapour behind. In the 1980s, NASA was launching 80 balloons a year as part of its programmes in astronomy and atmospheric chemistry: balloon mounted telescopes weighing a tonne or more have made some remarkable discoveries \u2014 and remarkable recoveries. Unlike their brethren on satellites, sensors that fly beneath balloons can be used again and again, although getting them back from wind-blown landing sites can be a trial. Since the 1980s, though, the number of NASA balloons launched has steadily declined. Last year it was just 17. This is partly explained by the changing nature of the experiments and observations that astrophysicists want to make. Those that could be performed easily and quickly \u2014 the low-hanging fruit for telescopes 30 kilometres up in the sky \u2014 have been done. Now, most experiments require longer flights and more complicated instruments. And this is where a new sort of balloon comes in \u2014 the peculiarly pumpkin-shaped balloon folded up and wrapped in protective red plastic in the Fort Sumner hangar, which Cobble and his crew from NASA's Columbia Scientific Balloon Facility (CSBF) are about to launch. Its fabric, as thin as the plastic of a refuse bag, will, when inflated and aloft, measure 54 metres across and have the volume of more than 600 double-decker London buses. And if all goes well, it will provide a solution to the abiding problem of scientific ballooning \u2014 that what goes up comes down all too soon. According to its designers, this style of a balloon can stay up night and day for months at a time.  \n                Getting high \n              All scientific balloons are filled with helium. In the more-normal balloons, sometimes called zero-pressure balloons, this buoyant helium is at the same pressure as the atmosphere outside, and the balloon's volume thus changes with the temperature. This means that if it is to retain its altitude, the balloon may need to vent gas by day and drop ballast by night. This limits the mission lifetime, normally to days. In steady conditions such as the constant daylight of a polar summer balloons can stay up longer; in 2005 a NASA balloon circled Antarctica three times in a record 42-day flight. But although that is long enough for astrophysicists looking at things such as cosmic rays to get good data, longer would be even better. Immunity to the day\u2013night cycle would also allow missions to fly from more accessible places than Antarctica, and would make things easier for observers who like to do it in the dark. \n               boxed-text \n             The balloon being launched at Fort Sumner is designed to provide these benefits by maintaining a constant volume as opposed to constant pressure. The gas inside will at times be at a higher pressure than the atmosphere outside, which is why such designs are called 'super-pressure' balloons. The pressure changes are absorbed by high-strength tendons in the balloon's ribs made from Zylon, a polymer also used to protect racing-car cockpits. NASA scientists think super-pressurized balloons like this could stay aloft for 100 days or more (see graphic, right). Because they are less vulnerable to diurnal cycles they also open up the possibility of multiple trips around Earth at mid-latitudes. That opens up more of the sky to observation, and lets strictly nocturnal astronomers into the game too.  \n                Outperforming satellites \n              More time, more sky; more night-time sky, more sky all year round. In terms of science per dollar, super-pressure balloons might well outperform satellites for a far wider range of missions than the current technology allows. \u201cThere are a lot of people waiting for this technology to become mature enough,\u201d says Simon Swordy, director of the Enrico Fermi Institute at the University of Chicago, Illinois, who wants to use balloons to study the history of cosmic rays in the Milky Way. \u201cBut they have to prove the technology before people in NASA start to take them seriously,\u201d he says. Providing that proof is hard. The arguments in favour of super-pressure balloons have been around for a while;  Nature  reported on them enthusiastically five years ago (see Nature 421,  308\u2013309  ; 2003). But flight success has proved elusive. The test at Fort Sumner is part of the latest attempt to do something about that. Failure could jeopardize an increase in NASA's planned budget for scientific balloons, which calls for spending on balloons to inflate by 50% over the next four years, in part so as to be ready to take advantage of super-pressure technology as it moves from understandably temperamental prototype to work horse. Too much temperament, though, and the programme will founder. The test flights have to go well. And the New Mexico weather has to cooperate with Cobble. By 06:00 Cobble has his team hard at work. Big Bill, the 45-tonne truck that tethers the balloons until the last moment, lumbers out to a patch of cracked, weedy pavement. The sky has blossomed from black to blue to pink, and the rising sun reveals wind turbines ticking over on a distant ridge \u2014 not an encouraging sign for the balloon launchers. Danny Ball, Cobble's boss and the director of the CSBF, in Palestine, Texas, points to a small weather balloon on a long leash that the wind is pulling out at a 45\u00b0 angle. \u201cThat's about 18 knots on it right now,\u201d Ball says. \u201cThat's way, way too much.\u201d While the launch team peels the balloon from its banana-skin wrapping and checks for tears and twists, Ball paces in a red-hooded sweatshirt, shorts and sandals. He frequently cups his hands against the wind to light cigarettes that he then throws away half smoked. All of NASA's balloon launches come under Ball's jurisdiction and he embodies the programme's blue-collar culture: gruff and given to a blunt common sense. Before ballooning he worked on offshore drilling barges in the Pacific Ocean. \u201cThe satellite scientists work in clean rooms with 10-year development cycles and unlimited reviews,\u201d he says, as the crew lays out the wrapped balloon behind Big Bill. \u201cWe don't have clean rooms. We don't do any of that\u201d  \n                More bang for the buck \n              The difference is reflected in the price. Most of the satellites that hold, stabilize and power scientific instruments are made by aerospace companies such as Boeing or Lockheed Martin for tens of millions of dollars, at the cheapest. The same companies, and some others, sell rocket launches at similar prices. NASA's balloons, on the other hand, are glued together for US$250,000 a time on the work tables of Aerostar, of Sulphur Springs, Texas, a facility that has at times sold left-over polyethylene film for refuse bags. The tougher skin and tendons that go into super-pressure balloons make them cost a fair bit more \u2014 about $1.5 million \u2014 but nothing near the cost of a satellite launch. The most expensive parts of such missions are the instruments themselves, which are often exactly the same as those that will fly on the satellite missions they foreshadow. But below a balloon they can be flown in simple gondolas hanging from steel cables; no need for the thermal insulation or the hi-tech components in satellite buses. Ball even recalls flying an instrument still supported by the workbench on which it was built. Price is not the only difference. Compared with the fairly fast turnaround culture of the balloons, satellite development is sluggish and risk averse. Instruments can fly on balloons years before their equivalents or descendants fly in space (see  'Where balloons have made advances' ). In 1998, the Balloon Observations of Millimetric Extragalactic Radiation and Geophysics (BOOMERanG) experiment studied the cosmic microwave background in part of the sky, finding important evidence for the 'flatness' of the Universe and scooping some of the results from the later and much more expensive Wilkinson Microwave Anisotropy Probe. Later this year, when the European Space Agency's e600-million ($940-million) Planck spacecraft starts to map the whole-sky background it will be using instruments of the same design. According to Andrew Lange, an astrophysicist at the California Institute of Technology in Pasadena who was on the BOOMERanG team, a balloon could fly something much more impressive today. He has plans for an instrument called SPIDER that would measure the polarization of the microwave background, and thus reveal the work of gravitational waves in the early universe. If he were to get SPIDER onto one of the first operational launches of a super-pressurized balloon, as he hopes to in a few years' time, he thinks such measurements might provide conclusive proof of cosmological inflation \u2014 and thus scoop a proposed but not-yet-funded space mission, the Inflation Probe, which is unlikely to see space much before the end of the next decade. Academics welcome the rapid concept-to-completion arc of balloon experiments not just as a way of flying equipment sooner, but also as a ready-made system for minting PhDs. \u201cThe timescale of building a balloon instrument, flying it, and getting data, is a timescale that a grad student can see from beginning to end,\u201d says Martin Israel, an astrophysicist at Washington University in St Louis who is chairing a scientific ballooning planning report for NASA due later this year. The importance of that fertile training ground is not to be underestimated, says Swordy, who laments the \u201carmies\u201d of NASA scientists who sit behind comsputers but know little about building instruments. \u201cWe have to preserve a cadre of people who know how to put together instruments for the future.\u201d  \n                Lift-off \n             At 07:00, during a long lull in the breeze, everyone around the test balloon puts in earplugs. Cobble opens valves on the helium truck and with a high-pitched hiss the gas snakes its way up tubes and into a balloon. Not yet into the super-pressurized ballon, but into a tow balloon attached to the top which lifts the main balloon up. Then 900 cubic metres of helium is introduced into the main balloon, enough to give it the anti-gravitational equivalent of a small car's weight in lift. Twenty minutes later, Cobble, riding at the front of Big Bill, pulls a lever and releases the balloon from a spring-loaded pin. Two dozen weathered faces, with shaded eyes, follow it up until it disappears in the glare of the still-low sun (see  'Up, up and away' ). \n               boxed-text \n             Its charge discharged, the CSBF team relaxes. But the engineers who designed and built the balloon are still on tenterhooks. At ground level the balloon is far from fully expanded; it is supposed to take on its designed pumpkinoid shape only in the much lower pressure of the stratosphere. But its predecessors have failed to do so. Since 2001, something odd has been happening on the test flights; the balloons, as they have risen, have never fully expanded. Material has remained folded in on itself in wrinkled clefts that look like Pac-Man or the S-shaped stitches on a baseball. That leads to instability of a sort that rules the balloons out for any practical work. Some people have the perception: 'It's just a balloon. How hard can it be?' But it's a complicated structure. Henry Cathey  Structural engineers were called in to model the problem. They found that there was too much material in between the longitudinal tendons at the balloons' poles. As a result of this, the balloons naturally tended to fold in on themselves to reduce stress, says Henry Cathey, a NASA engineer at the Physical Science Laboratory of New Mexico State University in Las Cruces. \u201cSome people have the perception: 'It's just a balloon. How hard can it be?' But it's a complicated structure.\u201d In the hangar, the engineers watch live video from two cameras pointed up at the underbelly of the balloon from the gondola below. Its posterior blossoms like a flower. As the balloon rises at 18 kilometres per hour, wrinkle after wrinkle disappears. Except for one, at two o'clock on the balloon's downturned face. The engineers urge the recalcitrant cleft on: \u201cCome on. Come on.\u201d \u201cGive the dog a bone.\u201d Finally, just as the balloon reaches its target height of about 30 kilometres, the last cleft relaxes; relief spreads through the room as everyone claps and shakes hands. The balloon floats west for an hour before the team triggers a tear in one of its panels. The engineering payload and its parachute drop toward the Capitan Mountain Wilderness of the Lincoln National Forest, more than 100 kilometres away, and land safely, dangled over a barbed wire fence and a rancher's trees \u2014 the balloon drifts on 20 kilometres downwind. The crucial goal of a fully puffed-up deployment has been achieved. In August, the balloon programme will ramp up for a bigger test: a super-pressure balloon the size of the Titanic (which is still much smaller than the zero-pressure balloons now in use). In December, Ball and his team will fly another of the same size from Antarctica in hope of breaking the 42-day duration record. In 2010, if all goes well, a balloon three times bigger will be flown, one capable of lifting an instrument payload weighing a tonne, and some small payloads will be tested. Routine flights could start in 2011. The balloon programme office has requested that NASA officials start a dedicated line of funding for scientists seeking to build balloon-borne instruments, because some competitions for astrophysics funding at present only consider satellite missions. Cathey has no doubt that there will be an appetite. \u201cThis is the $2.99 16-ounce steak that nobody knows about,\u201d he says. Today's test brings that steak a good step further towards the table. That calls for celebration: but not today. At the end of July about 40 CBSF employees will head to Shreveport, Louisiana, for three days of horse racing, gambling and golf. \u201cThis is when we will celebrate,\u201d says Ball. \u201cNot on campaigns. We're steely eyed balloon guys in the field.\u201d After the adrenaline rush of the morning, Cobble heads off alone to Fred's Restaurant for a hamburger before retiring to the Billy the Kid motel. He deserves a nap. Hear Eric Hand talk about reporting this feature on the 17 July Nature podcast:  http://www.nature.com/nature/podcast/index.html \n                     NASA\u2019s Columbia Scientific Balloon Facility \n                   Reprints and Permissions"},
{"file_id": "454384a", "url": "https://www.nature.com/articles/454384a", "year": 2008, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "Can the Chinese government meet its ambitious targets on space, the environment, research, energy and health? David Cyranoski takes a look at China today and what it hopes to be tomorrow. See Editorial, \"page 367;\":http://www.nature.com/doifinder/10.1038/454367a News Feature, \"page 388;\":http://www.nature.com/doifinder/10.1038/454388a and Commentary, \"page 398\":http://www.nature.com/doifinder/10.1038/454398a. For a podcast and more on China see \"http://www.nature.com/news/specials/china/\":http://www.nature.com/news/specials/china/  See also Correspondence: \"Olympics may have a negatigve impact on China's research\":http://www.nature.com/uidfinder/10.1038/4541049b \n                     China Views from the West \n                   \n                     Nature China \n                   \n                     China National Space Administration \n                   \n                     Chinese Academy of Sciences \n                   \n                     World Bank Report: Cost of Pollution in China \n                   \n                     Zhongguancun homepage \n                   \n                     WHO China page \n                   \n                     WPRO China page \n                   \n                     Outline of 11th Five-year plan \n                   Reprints and Permissions"},
{"file_id": "454018a", "url": "https://www.nature.com/articles/454018a", "year": 2008, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "Entomologists are briefing the military on how to protect troops from the scourge of the desert: sandflies. Rex Dalton reports. The desert to the east of Palm Springs, California, is heaven for entomologists. Between date-palm fields irrigated with standing water and the inland Salton Sea buzz a bonanza of insects. Researcher Kenneth Linthicum makes his way through the low scrub, which his team has sprayed in places with the insecticide bifenthrin. He checks traps that spew carbon dioxide, a lure for mosquitoes. Linthicum is on a quest to battle these pesky creatures, particularly in vulnerable populations such as US troops stationed in Iraq and other desert areas. He is one of a handful of researchers on the front lines of an entomological battle, working to reduce the incidence of insect-borne diseases among the military. In particular, they are targeting the tiny sandfly (genus  Phlebotomus ), which transmits the protozoan that causes the sometimes deadly disease leishmaniasis. Leishmaniasis is a major public-health concern: 12 million people are infected worldwide, with an estimated 2 million new infections each year. Military researchers have taken the lead before in developing insect repellants that end up benefiting civilians; DEET, for instance, was developed as a consequence of World War II jungle battles. Now, with troops in Iraq and Afghanistan, scientists are turning their attention to the little-understood plague of sandflies. \u201cIt's amazing we didn't know more about this bug,\u201d says Linthicum, who directs a US Department of Agriculture (USDA) research centre in Gainesville, Florida.  \n                Irritating bites \n             When US troops invaded Iraq in 2003, soldiers woke regularly with a rash of highly irritating bites they called Baghdad boils. \u201cSoldiers were getting eaten alive by the sandflies,\u201d says Russell Coleman, a medical epidemiologist with the US Army who was deployed with the first troops at Tallil airbase in southern Iraq. \u201cIn certain units, 70% were taking dozens to hundreds of bites a night.\u201d 1  More than 1,450 US troops have contracted leishmaniasis in the past four years, mainly in Iraq and Afghanistan. In 2004, the military established the Deployed War-Fighter Protection Program, with a particular focus on sandflies. It receives US$5 million a year for projects overseen by the Armed Forces Pest Management Board in Washington, DC. The programme includes scientists from all the services (Army, Navy, Air Force and Marines), along with academics and USDA researchers. They collaborate with military researchers from other countries, such as an Australian team who are studying ways to improve uniforms against insect bites 2 . \u201cWe are hoping to use the combined expertise to find new methods to protect personnel,\u201d says Stanton Cope, a Navy entomologist based in Washington DC, who serves as a point of liaison between the researchers and the pest-management board.  \n                A uniform approach \n              Leishmaniasis has two main forms: cutaneous, which creates skin lesions or attacks the mucous membranes, and internal or visceral, which can lead to liver and other organ problems and, in rare cases, death. Sandflies present a whole new challenge to military entomologists who are used to fighting mosquitoes. That's because the insecticide usually used on military clothing, permethrin, often isn't effective against the sandfly. \u201cA mosquito may land on a uniform, bounce around, then fall dead from the insecticide,\u201d says Ulrich Bernier, a chemist working at the USDA's Gainesville centre. \u201cA sandfly will be knocked out \u2014 then it gets up five minutes later.\u201d Uniforms are the first line of defence against bug bites. In the past, they were made of wool or cotton, with simple but durable weaves saturated in an insecticide such as permethrin. Today, uniforms have specially designed weaves of specific thickness in a mix of threads such as nylon, rayon and cotton to make them durable and protective. They are meant to protect against mosquitoes, ticks and fleas; but sandflies can still find their way through the cloth and bite \u2014 and that's if the soldier has the uniform on in the first place, which is not common during summer nights in the desert. A further complication is that each of the four services has its own uniform, each with a different ability to guard against insects. In the early 1990s, as the first Gulf War was starting, some services began issuing new uniforms pre-treated with insecticide. This barrier would be boosted periodically in the field by adding chemicals to the uniform. Then in 2003, a deployment of about 75 marines showed how vulnerable a soldier in uniform can be. Nearly 40% of the marines stationed in Monrovia, Liberia, during a civil disturbance contracted malaria. That's when marine commanders turned to the USDA centre to identify the uniforms' weakness. Researchers there realized that the corps put their uniforms through a permanent press treatment to make them look ironed and the creases look sharp. The marines were the only service to do so. They had also switched to a twill fabric from a mixed weave. Those two factors let the mosquitoes through to bite, says Bernier.  \n                New system \n              That was enough to get the corps to change their uniforms. \u201cThe marines raised the bar,\u201d he says. \u201cThey wanted to stop 100% of the biting.\u201d Working with USDA researchers and those at the Army Soldier Systems Center at Natick in Massachusetts, the marines launched new uniforms in March 2007. The garments now come pre-treated with a stronger insecticide so as to theoretically not need field treatments during the life of the clothing. Bernier is collecting uniforms from marines in Iraq to see how the new system is working. One challenge, however, is to not saturate the uniform so heavily with insecticide that it starts to become toxic to the wearer 3 . Another complication is that the Army now treats its rayon/nylon uniforms with a fire retardant, as the earlier version would melt into wounds of those burned in bomb attacks. The question remains: how does that fire retardant interact or otherwise affect insect treatments on the same uniform? What's more, any changes to uniform designs are costly to implement. Marines go through about 385,000 uniforms a year, the Army 4 million. So beyond uniforms, researchers are looking at new and different methods for protecting soldiers from insects. One approach is to better identify the chemicals that attract insects to humans, and use those to mask skin against the insects. Bernier's team, for instance, has identified at least three chemical blends that attract insects, based on chemicals that humans emit naturally 4 . Another approach is to develop entirely new types of insecticides. Chemist Alan Katritzky of the University of Florida in Gainesville reported recently on an artificial neural network that they used to screen mosquito repellants 5 . His computer modelling system used past data on mosquito repellency to identify 34 new candidate repellants. When tested in the lab, some of the candidates repelled mosquitoes up to three times as long as DEET. At Louisiana State University in Baton Rouge, researchers are targeting not just the sandflies but also the rodents in whose burrows the flies live. Scientists are studying a feed mixed with a pesticide (novaluron) for rodents. The pesticide is passed onto immature sandflies when they eat the rodents' faeces 6 . This could help solve another problem: troops bulldozed entire areas where tents were being erected to get rid of sandflies, but more would later pop up from the rodent burrows underneath.  \n                Precision spraying \n              Meanwhile, USDA researchers in College Station, Texas, are developing a system in which a small, unmanned helicopter is programmed via a computer to spray a set area from 22:00 to 4:00, key hours for sandfly activity. \u201cWe mounted a sprayer and global positioning system (GPS) on an expensive toy helicopter,\u201d says USDA agricultural engineer Clint Hoffmann. \u201cYou set the helicopter's flight pattern with a computer mouse.\u201d Field testing is coming soon, with the first overseas study planned for Egypt. Even the size of droplets sprayed is receiving scrutiny 7 . If you're trying to hit a flying insect, you want a 25-micrometre droplet. If you want to cover plants or tents, a 100-micrometre droplet is best. \u201cWe have a fairly sophisticated drift model that can track individual droplets \u2014 even through aircraft vortices or wakes,\u201d Hoffmann says. These systems have shown that the standard methods the military used to trap and kill insects, such as spraying in the early evening when sandflies aren't around, are virtually worthless. It may take a while for the research to work its way into military consciousness, and eventually on to benefit civilians. But with the way the war is going in Iraq, there could be plenty of time left to learn the lessons. Rex Dalton writes for  Nature  from San Diego. \n                     WHO leishmaniasis page \n                   \n                     Deployed War-Fighter Protection Program \n                   \n                     Armed Forces Pest Management Board \n                   \n                     Leishmaniasis information for service members \n                   Reprints and Permissions"},
{"file_id": "454021a", "url": "https://www.nature.com/articles/454021a", "year": 2008, "authors": [{"name": "Bruce Lieberman"}], "parsed_as_year": "2006_or_before", "body": "A difference in one molecule led physician Ajit Varki to question what sets humans apart from other apes. Bruce Lieberman meets a man who sees a big picture in the finer points. The human body does not welcome an injection of horse serum. Ajit Varki discovered this when, as a young San Diego doctor in 1984, he administered some to a woman with bone-marrow failure. The serum was a standard treatment intended to stop the woman's T cells from destroying her bone marrow. But it was also known to prompt a reaction called 'serum sickness' and, sure enough, the patient broke out in hives a week after treatment \u2014 the result, Varki assumed, of her immune system's assault on proteins from another species. Soon after observing his patient's reaction, Varki learned that proteins weren't the only thing to blame. So were sialic acids, sugars that carpet the surface of mammalian cells. Some studies had suggested that the human immune system reacted against one sialic acid called  N -glycolyl neuraminic acid (Neu5Gc) in the horse serum. \u201cHow can that be?\u201d Varki remembers thinking. \u201cHow can you have a reaction against sialic acid? It's everywhere. All mammals have sialic acid.\u201d Varki wondered whether humans might in fact be the only mammal that lacked Neu5Gc. A physician and biochemist by training, Varki had already embarked on a career in the relatively new field of glycobiology, the study of the sugar chains that decorate many proteins and lipids inside and outside the cell. But it was another 14 years before he got the chance to answer his original question. In 1998, he and his colleagues used high-performance liquid chromatography to analyse blood samples from chimps, bonobos, gorillas, orangutans and humans. They found that humans are indeed the only primates missing Neu5Gc 1  and that human cells are instead rich in another sialic acid,  N -acetyl neuraminic acid (Neu5Ac).  \n                A career in evolution \n              These findings started Varki off on a road that led to his becoming not only a leading glycobiologist but a respected 'honorary' palaeo-anthropologist. He is one of the co-founders and directors of the multidisciplinary Center for Academic Research and Training in Anthropogeny (CARTA) \u2014 a research collaboration between the University of California, San Diego, and the Salk Institute in nearby La Jolla. The centre was launched in March this year with a US$3-million grant from the G. Harold & Leila Y. Mathers Foundation, based in New York state. The 'Anthropogeny' in the centre's title resurrects a term for the study of both the evolution and the individual development of human beings that would have been familiar to earlier generations of anthropologists. To Varki, the word encapsulates some of the biggest questions in the study of human origins, such as how, why and when the human brain evolved its present functions. One of his latest research projects is a collaboration with Spanish palaeontologist Juan Luis Arsuaga, of the Complutense University of Madrid, for the biochemical analysis of 900,000-year-old  Homo antecessor  fossils from Atapuerca in northern Spain, some of the oldest hominid bones yet found in Europe. What Varki is looking for is evidence that Neu5Gc was lost very early in human evolution. He believes that the fact that humans, and only humans, have lost Neu5Gc could be implicated in the emergence of hominid species. The journey from glycobiologist to director of a multidisciplinary human origins centre has been fuelled by Varki's insatiable desire for knowledge. \u201cThe guy is just an encyclopaedia,\u201d says glycobiologist Mark Lehrman at the University of Texas Southwestern Medical Center in Dallas. \u201cEven though he wasn't trained in anthropology, he's been able to educate himself in this area and become an authority. It's a remarkable gift to be able to do that and do it well.\u201d Varki initially trained as a general medical doctor at the Christian Medical College in Vellore, India. To pursue a dual medical and research career, he went to the United States, eventually taking up a fellowship under Stuart Kornfeld at Washington University in St Louis, Missouri, in the late 1970s. Kornfeld was beginning his work on sugar chains, including sialic acids, and Varki was intrigued by the opportunity to contribute to a largely unexplored area of biology. In 1982, he set up his own glycobiology lab at the University of California, San Diego, where he still works today. On a molecular level, the difference between Neu5Gc and Neu5Ac is tiny \u2014 a single added oxygen atom perched on one arm distinguishes one from the other (see graphic). But on a biological level, the difference could be enormous. \u201cWe thought if monkeys and all of our closest relatives have Neu5Gc and humans don't, then there must be a molecular basis for that,\u201d Varki says. He subsequently found it in an enzyme that converts Neu5Ac to Neu5Gc, but which is disabled by mutation in humans 2 .  \n                Selection pressure \n              Varki's discovery pointed to a definitive difference that set chimps and humans biochemically apart, says Morris Goodman, an evolutionary biologist at Wayne State University in Detroit, Michigan. It was one of the first such differences to be found, and because sialic acids serve many biological roles, primarily as cell-recognition and cell-adhesion molecules, it might explain some of the unique aspects of human biology. \u201cWhat we're dealing with here is a gene loss that has an effect throughout the whole body,\u201d says Goodman. At the time, Varki realized he knew little about human evolution except what he'd learned as an undergraduate or read in  National Geographic . So he set out to educate himself. He took a short sabbatical at the Yerkes National Primate Research Center in Atlanta, Georgia. Reviewing the animals' medical records with a veterinarian, he learned that the centre had never seen a case of rheumatoid arthritis or bronchial asthma \u2014 common conditions in humans. Chimpanzees don't get sick from the human malaria parasite,  Plasmodium falciparum . Conversely, humans can't be infected with  P. reichenowi , the malaria parasite that plagues chimpanzees. In subsequent work, Varki and his team showed that the different susceptibilities were due to the differences in sialic acids.  P. reichenowi  prefers to grab hold of Neu5Gc on chimp red blood cells, whereas  P. falciparum  favours Neu5Ac 3 . The researchers hypothesized that the selection pressure to evade  P. reichenowi  may have led humans to lose Neu5Gc and acquire resistance to this parasite \u2014 and that this loss may have helped to fuel the emergence of  P. falciparum , which could gain entry by latching onto Neu5Ac instead. Other discoveries in Varki's lab \u2014 including ten other human-specific genetic changes affecting sialic acid function \u2014 may help to explain uniquely human vulnerabilities to conditions such as Alzheimer's disease and multiple sclerosis. Varki's interest in human evolution soon extended far beyond chimps and their sugars. \u201cI found he was talking with several people on campus,\u201d says neuroscientist Fred Gage at the Salk Institute, a long-time collaborator and friend. \u201cI told him that it wasn't fair that he would have these one-on-one conversations and not share what was being talked about,\u201d he jokes.  \n                Reimagining anthropogeny \n              Gage encouraged Varki to organize a series of informal seminars on human origins at the university. Between 1998 and 2007, the Project for Explaining the Origin of Humans drew in anthropologists, primate biologists, geneticists, immunologists, neuroscientists, linguists and many others. They discussed topics ranging from the evolution of language to the differences between humans, Neanderthals and  Homo erectus , the first hominid to leave Africa. Goodman says the interdisciplinary nature of the series made it extremely important to the field. \u201cYou really had the chance to explore an issue as it relates to the evolutionary origins of our species,\u201d he says. Varki's motivations were partly selfish: \u201cOne of my goals, my secret agenda, was to educate myself,\u201d he admits. \u201cAt the last meeting I asked the people who attended if I could have a bachelor's degree in anthropogeny.\u201d Varki estimates that he has listened to more than 300 talks on various aspects of this discipline. \u201cThe idea is the linguist needs to talk to the molecular biologist who needs to talk to the neuroscientist who needs to talk to the psychologist and philosopher about these issues,\u201d he says. \u201cMost areas of human knowledge are somewhere relevant.\u201d CARTA is a successor to the human origins series. Directed by Varki, Gage, Margaret Schoeninger, a professor of anthropology at the University of California, San Diego, and Pascal Gagneux, a primate biologist and Varki's close collaborator, the centre already has some 40 San Diego-based members and more than 100 in the rest of the United States and elsewhere in the world. CARTA aims to foster connections between these researchers worldwide, facilitate access to resources for great-ape research, develop a peer-reviewed journal and offer courses on human origins. The project is in some ways comparable to the Leipzig School of Human Origins in Germany, an interdisciplinary PhD programme run jointly by the Max Planck Institute for Evolutionary Anthropology in Leipzig and Leipzig University since 2005. Varki says that CARTA will be more of a virtual organization and that \u201cthe effort should transcend disciplines\u201d, pointing as an example to his own work on sialic acids, which has required collaboration between biochemists, palaeontologists and physicians.  \n                Acid test \n              Back in the lab, Varki and Gagneux will in the next few months embark on the preliminary analysis of animal fossils from Atapuerca, to see if they can detect preserved sialic acids using high-performance liquid chromatography and mass spectrometry. If so, sialic acids are likely to be preserved in hominid fossils from the same strata and the researchers will test those next. \u201cPalaeontologists are usually seen as people interested in something that is finished and belongs to the past,\u201d Arsuaga says, \u201cand usually the idea is missed that we are looking for an explanation of living humans.\u201d He says he was persuaded to let tests be done on the precious  H. antecessor  fossils because \u201cthe damage is not big\u201d from current techniques that drill small amounts of powder from inside the bone. Varki and Gagneux hope that these fossils may help to answer some grand hypotheses about Neu5Gc and its role in human evolution. They estimate that the mutation that caused the loss of Neu5Gc first appeared among human ancestors 2 million to 3 million years ago, which coincides with the emergence of  H. erectus , and they believe that pathogens such as malaria may have initiated this change. They wonder whether the change in this ubiquitous sugar could have had other broad-ranging biological effects that helped create repro-ductive isolation between those with Neu5Gc and those without, and whether these effects could have contributed to the emergence of  H. erectus , followed by  H. antecessor . \u201cLosing Neu5Gc may have been great for survival, but it may have forced you to forgo reproduction with a whole group of your former buddies who didn't undergo this change,\u201d Gagneux says. If they can show that Arsuaga's  H. antecessor  fossils also lack Neu5Gc, this will be yet more evidence in support of their hypothesis. If ancient humans can't answer the speciation hypothesis, then perhaps mice will help. Varki and Gagneux have genetically engineered mice that lack the Neu5Gc sialic acid that humans are missing and Varki says that they display subtle human-like features 4 . Compared with wild-type mice, they have poor hearing, somewhat reminiscent of human age-related hearing loss, and slower wound healing, as do humans compared with non-human primates. Further studies should reveal whether these mice are able to reproduce with wild-type animals that still have Neu5Gc. Varki's recent work has brought him back to the immune reaction he observed nearly 25 years ago. Even though humans don't make Neu5Gc, it is eaten in animal products that contain it, such as meat and milk. Varki and Gagneux wonder whether \u2014 among meat-eaters at least \u2014 Neu5Gc elicits an immune reaction that might contribute to a whole spectrum of human-specific diseases that are associated with chronic inflammation, including heart disease and cancer. Such diseases would not have been such a problem when humans had shorter life spans.  \n                Food for thought \n              To test the idea, Gagneux took a trip to a local Whole Foods Market, loaded up a shopping cart with meat and dairy products and took them back to the lab for analysis. The researchers found the highest levels of Neu5Gc in lamb, pork and beef. \u201cWe swallowed big bowls of that and we collected every possible sample we could from ourselves in the following few weeks to see whether it shows up in our own glycoproteins,\u201d Gagneux says, \u201cand the answer is yes, it does.\u201d The team has also found that many people carry antibodies targeted against the sugar 5 . If their hypothesis holds up, it will illustrate how selection pressures change: where once selection favoured the loss of Neu5Gc to protect hominids from pathogens, now its absence could be making humans susceptible to other diseases. \u201cOnce you've lost it, you have to make do with what you have,\u201d Varki says. For Varki, who began his professional life observing patients, these studies have brought him full circle. The molecules that made humans human may be the same ones that make us uniquely vulnerable to our most threatening diseases. \u201cIn some cases, they would be what I call the scars of our evolution,\u201d Varki says. \u201cMy experience has opened my mind to the fact that understanding human evolution, where we came from, is very important to understanding who we are and where we're going.\u201d Bruce Lieberman is a freelance science writer based in San Diego. \n                     Specials: Chimp genome \n                   \n                     Varki Lab \n                   Reprints and Permissions"},
{"file_id": "4531164a", "url": "https://www.nature.com/articles/4531164a", "year": 2008, "authors": [{"name": "David Chandler"}], "parsed_as_year": "2006_or_before", "body": "Fewer people are searching for near-Earth asteroids, astronomer David Morrison said in the 1990s, than work a shift in a small McDonalds. But that group \u2014 a little larger now \u2014 has over the past two decades discovered a host of happily harmless rocks, and in doing so reduced the risk of an unknown asteroid blighting civilization (see  page 1178 ). David Chandler puts together the story in the words of those who watched, and those who watched the watchers.   Clark Chapman:   About 60 years ago, there were some prescient things written by Ernst \u00d6pik, by Ralph Baldwin, and by Fletcher Watson. Only a handful of  near-Earth asteroids  had been discovered, but they came up with order-of-magnitude-correct understandings about how often a bad thing would happen.    David Morrison:   That understanding arose almost without reference to Tunguska. \u00d6pik and  Gene Shoemaker  did some kind-of-heroic calculations based on almost zero data. In the 1950s, we only knew of a few Earth-crossing asteroids and had data on a couple of comets that had come into the inner Solar System. And they, using physical intuition and consistent with each other, made the first predictions of what the impact flux might be. Before that, it was pure arm-waving. \u00d6pik and Shoemaker quantified it, and within the right order of magnitude.   Carolyn Shoemaker:   When the first Apollo mapping studies were done, trying to get better photos of the Moon, they became much more aware of craters. People like Gene were convinced that the majority were caused by impacts. They looked so much like those on the Earth. Not only  Meteor Crater , but also nuclear-bomb craters. So he was convinced they were caused by impacts.   Rusty Schweickart:   I tramped all around Meteor Crater with him, as did all of us who were part of the Apollo programme back then. Nobody knew whether features on the Moon were impact or volcanic. So we went all over the world to volcanic sites, to impact sites, with the top people in the world, including Gene. Frankly, very few people thought the main cause was volcanic. People did think there might be some volcanic features, which of course there are.   Chapman:   Gene got very unhappy with NASA, there at the end of the Apollo programme, and kind of consciously wanted to get away from it all. He'd been interested in craters on the Earth, and clearly was aware that they were related to asteroids in the sky, and got interested in going off to sit in observatories on lonely mountain tops to discover them in the early 70s. He hooked up with  Glo Helin  to do a joint observing programme.   Steve Ostro:   Observations of near-Earth objects were growing \u2014 the stuff by Helin and by Shoemaker \u2014 and we started getting some radar opportunities on newly discovered objects. In 1979, I met all the asteroid people and got very enthused, and then I was pretty sold on asteroids and wrote my own observing proposals. Within a year I was basically doing that more than anything else.   Brian Marsden:   Helin and Shoemaker fell out and had their own separate programmes. It's always been a bit of a mystery \u2014 I've never fully understood it. Gene decided he could get Carolyn, his wife, involved. Carolyn did all the examining of the films, so she found all those Shoemaker\u2013Levy comets. Gene was more the big picture: he did do a fair bit of the actual drudge work you might say, but Carolyn did a lot more of it, just as Helin and her people had done.   Shoemaker:   I think there were only nine near-Earth asteroids known when Gene started the programme with Helin. They found a few, not many, and Gene's feeling was that we have to find them a lot faster \u2014 develop another programme to find more and faster. We went to using film that had a very fast emulsion, and we'd take two films, 45 minutes apart, so anything near would seem to move. Then Tom Gehrels started Spacewatch in the 1980s, which Jim Scotti continued.   Chapman:   Now Tom, he was sort of 'Mr Asteroid' in the 1960s. He'd been studying asteroids just for the sake of their scientific interest \u2014 measuring their light curves and other physical properties.   Tom Gehrels:   We had a  CCD  on a telescope, the dedicated telescope that was the Spacewatch telescope, by 1983. Soviet submarines had already used CCDs in the 1970s, and our colleagues there were quite free to talk about it, which was quite remarkable, so I learned a lot about CCDs. The first asteroids were discovered by Spacewatch in 1984, and the first near-Earth asteroids were found with that system in about 1986. This was done against everybody's opposition, including Shoemaker's \u2014 he fought like a tomcat.   Shoemaker:   There was some communication [with Spacewatch], and I guess competition in a sense. They were able to see much fainter magnitudes than we could. That's been their big contribution.  \n                Killing the dinosaurs \n               Gehrels:   \u00d6pik and Harold Urey had warned very clearly of the impact hazard, but I had totally ignored that, and it was the Alvarez paper that really woke that up.   Morrison:   Until the Alvarez paper on the K/T boundary there was no reason to think that any impact, short of some humongous thing, would have any global consequences. The truly remarkable thing was that such a small impact \u2014 that has no effect on the Earth's rotation, its axis, its magnetic field, completely negligible physically \u2014 could nevertheless wipe out most of the life on the Earth. To say that the biosphere was so fragile was a real revolution. In terms of the hazard, it was key.   Chapman:   So Bill Brunk and Shoemaker put together this group of I'm going to guess 40 people that included a few people from the military, asteroid scientists like myself, people who knew about orbits such as George Wetherill of the Carnegie Institution in Washington, and some NASA mission-planning-type people. It was just a wide-ranging discussion \u2014 it included the nature of the threat, the possible damage to the environment.   Morrison:   It took a while for other people to accept the idea of an impact. Especially palaeontologists, who thought this ridiculous egotistical physicist was trying to tell them what killed the dinosaurs. The astronomers embraced the Alvarez result very quickly, and geologists such as Shoemaker started drawing conclusions from it, but the palaeontologists resisted it equally dramatically, for quite a while.   Chapman:   People were aware of  Project Icarus , and there was even discussion of the politics of it. Wetherill in particular was very concerned that the project would open up the door to the use of nuclear weapons in space. All those kinds of issues were discussed during this 3- or 4-day meeting at Snowmass in Colorado. And a report was written, a very lengthy report, but it was never published.   Morrison:   That group actually made the recommendation that NASA should consider how to discover and how to defend against incoming objects. It was quite early. Now, that doesn't mean it was accepted. But the guy who really was in the intellectual leadership on this was Shoemaker.   Gehrels:   I was pretty good friends with Shoemaker. He didn't believe in it. He would say 'Tom, this is not for real'. He organized the meeting, and for a day and night we were not supposed to go out and see the snow or anything like that, we just worked, worked, worked, so we could put a book together. And by the time it was put together, Shoemaker was totally convinced that it was not for real. And so we never got the book off his desk until somebody in Flagstaff actually pinched it, and then copies started floating around, but the book was never published  \n                Congress and comets \n               Morrison:   None of the Spaceguard stuff would have happened at NASA if Congress hadn't called for it. Congress asked NASA to organize two workshops, one on detection and one on defence against asteroids. I chaired what became known as the Spaceguard workshop on detection.   Chapman:   I was asked to run the first major scientific conference on near-Earth asteroids in '91. It was a pretty large meeting, 80 people or so, and it also received some national press: it was covered by  Time   magazine, I was interviewed by National Public Radio. Morrison's Spaceguard committee held one of its meetings in conjunction with the scientific conference. And then the next year was the meeting at Los Alamos on deflection, also run by NASA but with Edward Teller and  Lowell Wood  and others playing a prominent part.   Morrison:   At Los Alamos, the astronomers, led by Shoemaker, went face to face with the weapons people led by Teller and Wood, and that was a real wake-up call for us, that this whole other world existed, which didn't speak our language, which didn't operate the way we operated.   Shoemaker:   Teller, being a man of the background he had, was interested in possibly sending something out to blow up a near-Earth object. So that became an argument.   Morrison:   The cultures of the two groups could not have been more different. Just seeing the confrontation between Teller and Shoemaker was absolutely one of the memorable things in my life. Because Teller was idealized and feared by all these people. All the weapons people seemed to be beholden to him, probably for their jobs, they almost worshipped him, they fawned over him, it was always \"Dr Teller\". If he so much as cleared his throat everybody in the room stopped to let him have his say. In contrast, here was good old Shoemaker \u2014 nobody would ever call him Dr Shoemaker \u2014 and we'd go out drinking with him and we had a much more egalitarian sense of things. And we did open peer-reviewed publications and the weapons guys didn't. It was really a clash of two cultures. The weapons people seemed to think the problem was an order of magnitude greater than the astronomers did. They were for rockets on the pad with nuclear bombs, practically what you would use for a ballistic missile, and shoot the thing down right before it came into the atmosphere. It took a long time for them to even grasp the concept that if you carried out a survey you could make the discovery long in advance, and that completely changes the calculation. At one point Teller actually stood up and said that the asteroid threat, which was real, was the appropriate justification for building much bigger nuclear bombs \u2014 a hundred or a thousand times bigger than we had.   Gehrels:   Shoemaker changed with  Shoemaker\u2013Levy 9 . After the impact with Jupiter, he was totally converted and he really threw his weight behind that.   Shoemaker:   Gene came home from a conference that was right after it had happened, I think it was in Seattle, and he said \"at last, my geologist friends believe that impacts occur\". Before, people would say yeah, maybe that's so, but they just didn't really have a deep conviction that impacts occurred. After Shoemaker\u2013Levy 9, when one could see what was happening on Jupiter, yes, we knew for sure. And geologists generally, I think, came to accept impacts. Not everyone yet, but most of them. It's really kind of amazing \u2014 it's something that was easier for astronomers to accept I think than for geologists.  \n                Surveys and scares \n               Marsden:   The discovery rate has steadily gone up as the CCD surveys came along: Spacewatch in the '80s; Helin's NEAT at end of '95; then LINEAR. In '98 they really boosted up (see  'Known near-Earth asteroids through the decades' ), and they were the leader.   Shoemaker:   The biggest and most successful sky survey is LINEAR, which started out using one of the telescopes that belongs to the Air Force, and that developed as a combination of MIT and Air Force technology. So they were way ahead to begin with on software sorts of things \u2014 they have kind of just thrust us all off our feet. They're the sort of thing that Gene knew we had to have if we were really going to find these bodies in anything like reasonable time. We just weren't doing that with our survey, with film and the old telescope.   Marsden:   When we had the 1997 XF11 situation, people had been searching for some time and finding things, but nobody had really been doing anything from the point of view of whether these objects can be a danger. For XF11 there was a possibility of its coming very close in 2028, it could have come as close as 32,000 kilometres. Because we'd had it under observation for only 3 months, it would have been impossible to say that it might not hit us a few years after that approach. There were several opportunities on subsequent approaches, particularly in 2040.   Chapman:   It's important that astronomers don't appear to be Chicken Little, and lose credibility. Public assertions by supposedly credible astronomers that there was a \"small\" chance that the Earth would be hit in the year 2028 by the 1.6-kilometre-wide asteroid, 1997 XF11, were corrected a day later.   Andrea Milani:   I went to my office and opened my e-mail, and my folder was full of mail about 1997 XF11. I was immediately quite upset. None of the scientists involved in discussing the issue actually knew how the computation of whether an asteroid could strike the Earth could be done. There was a real lack of knowledge. It would have been better if they just said \"we don't know\", rather than saying something wrong. The actual conclusion in doing the post-mortem is that the two groups of scientists who were fighting about it were both wrong. Neither had the correct algorithm.   Marsden:   I did in a way stir things up a little bit at that time. At the time, it was possible it really could have hit us in 2040, based on the information we had. As we got further information, that possibility went away. And that really did get people thinking.   Milani:   We needed a fully nonlinear theory of impact prediction. We found a method capable of doing this and in April '99 we were ready with a paper announcing a possibility of impact for the asteroid 1999 AN10 with a probability of 10 \u20139 . Our result, from our point of view, was very good precisely because we had detected a very minute possibility. From the point of view of the journals, the result was not important because the probability was minute. But of course what mattered was the method; the specific announcement was irrelevant. I think it was the most important paper I ever wrote, but for a long time I was unable to publish it. Someone found this paper on our website, and I was more or less simultaneously accused of hiding this, and of spreading alarm.   Chapman:   In 2004, an object was discovered by LINEAR and the nominal calculated orbit had the asteroid hitting the Earth the next day. A whole lot of people went into a real frenzy trying to get more observations of this object, because \u2014 well, if it's going to hit the Earth tomorrow, it's not a very big object, but it was about half the size of what caused Tunguska. Europeans tried to look for it, but the weather was bad there, it was cloudy over most of the United States, and so people could not follow up on it immediately, despite considerable attempts to do so. The people at JPL, who are as expert as any, actually concluded that there was something of the order of a 30% chance that this object would hit the Earth during the next three days. Now, it turns out that the thing was much farther away and much bigger and came nowhere near the Earth. But based on the observations that were available up to that time it was a very significant probability. So we were debating late into that night, at what point should we go public with this? That was the first incident that really raised the questions of who makes the decisions and how does the information get communicated, because before that event we've always thought of these near-Earth asteroid issues as being long-term ones, where something may be discovered now but it's going to hit in 30 years time. The idea that you'd really need to talk to people within hours just really hadn't occurred to us. Because of this event, we now realize that although the chances of something hitting tomorrow are very, very low, the way the observations are collected can certainly make it seem possible that something's going to hit tomorrow.   Schweickart:   The B612 Foundation has formed a committee, working with the United Nations, to work on things such as warnings and all-clears. We're working on a decision programme, similar to the mission rules we came up with for space flight \u2014 every possible eventuality is taken into account, so when something happens, you don't start arguing about it, you just do it. We will make recommendations to the UN in September, and it will begin debating it next year. We have to set in place a process. If somebody is going to deflect something, how do you determine who does it?   Ostro:   Sooner or later we will want to, or we will have to, send spacecraft to near-Earth asteroids, and come very close to them, possibly land on them. But it's very difficult to navigate around these objects, because of the unusual shapes, the spin states, and the low masses. The dynamics around a near-Earth asteroid are very different from around a big massive sphere such as the Earth \u2014 the orbits are gorgeous, geometrically intricate and complex, and every asteroid has a different dynamic environment.   Schweickart:   We've hired the JPL to do a full-blown analysis of the  gravity tractor  which will point the way to a demonstration mission, to learn what the control parameters are.   Ostro:   People have workshops on what do we do, should we deflect or blow it up, but they almost never use a realistic model of a near-Earth asteroid, they always assume a sphere or that we will know what the physical properties are before we have to do something. It's almost impossible to figure out what to do unless you know something about the object. That's where radar comes in. Right now, we've gotten radar echos from 340 asteroids.  \n                Looking ahead \n               Marsden:   A UK government task force produced a pretty nice report suggesting we should extend the searches down to objects 300\u2013400 metres across. Then NASA did further studies and said we've got to go down to 140 metres and find them in 20 years, 90% of them. That's tens of thousands of objects! Over 100,000, I think. That's a big survey.   Chapman:   At some level, it's going to happen anyway. At least the Pan-STARRS telescope is going to become operational any month now \u2014 it saw first light some time last year. The people running it have already spent energy on what to do if they find near-Earth asteroids on their CCDs. People are working on the  LSST , which is a project that sounds like it's ultimately going to happen. If NASA does not pay for it, it will happen more slowly, but it's going to happen.   Shoemaker:   I liked looking at the sky. I liked doing all those other things that some people thought were awfully time-consuming and might be tedious, but which to me were fun. I retain my interest in all the results, but I don't do the work anymore. For me, the romance of observing is gone. One of the real pleasures of our programme was the fact that if we found something exciting, and needed confirmation or more positions, we could ask people here and there throughout the world, and they would do follow-up work for us. And by the same token we would do the same thing for them. It was both competitive and cooperative. It doesn't always happen in science, but it worked pretty well for us in those days, and that was a real joy. \n               David Chandler is a freelance science writer in Massachusetts.  \n             See Editorial,  page 1143 , and Commentary,  page 1178  . \n                     Cosmic impacts special \n                   \n                     NASA on near-Earth objects \n                   \n                     David Morrison\u00e2\u20ac\u2122s impact page \n                   \n                     Catalina Sky survey \n                   \n                     LINEAR survey \n                   \n                     Spacewatch survey \n                   Reprints and Permissions"},
{"file_id": "4531157a", "url": "https://www.nature.com/articles/4531157a", "year": 2008, "authors": [{"name": "Duncan Steel"}], "parsed_as_year": "2006_or_before", "body": "The most dramatic cosmic impact in recent history has gathered up almost as many weird explanations as it knocked down trees, writes Duncan Steel. \n               Sooner or later, it was bound to happen. On June 30, 1908, Moscow escaped destruction by three hours and four thousand kilometers \u2014 a margin invisibly small by the standards of the universe. \n             So begins  Rendezvous with Rama , a 1972 novel by Arthur C. Clarke in which mankind learns the hard way about the dangers posed by incoming asteroids. The 2077 impact in northern Italy that Clarke goes on to describe is fictional: the 1908 blast was real. The early morning of 30 June 1908 saw, in an area around the Stony Tunguska river, the most explosive cosmic impact in recent history, hundreds of times more powerful than the atomic weapons set off over Hiroshima and Nagasaki. And yet, in part because it happened so far from civilization, and in part because it left no crater, it has not always been recognized as such. For decades it existed in a strange realm between science and pseudoscience, blamed on antimatter, black holes and alien spacecraft as easily as on a very fast bit of interplanetary refuse, and developing a mystique that has seen it associated with everything from energy drinks and rock bands to military missiles and  The X-Files . The approximate site of the blast's epicentre is now marked by a totem pole that researchers have dedicated to Agdy, the god of thunder in local mythology. Getting there is quite a trek, but the fascination of the site still draws an intermittent stream of scientists to the remote wilderness about 1,000 kilometres north of Lake Baikal; they leave offerings at the totem pole to commemorate the trek. In the years directly after the blast, though, no one came at all. The first researchers did not arrive until the 1920s. That does not mean there was no significant contemporary evidence to bring to bear. Siberia was and is an empty place \u2014 but a blast which, had it happened over Chicago, would have been heard from Georgia to the Dakotas, still drew a lot of attention. In the days following the blast, A. V. Voznesenskij, the director of the Irkutsk magnetic and meteorological observatory near Lake Baikal, began collecting accounts that are vivid with detail 1 . There are people being knocked off their feet, a man needing to hold onto his plough to avoid being swept away by a powerful wind, the feeling of great heat \u201cas if my shirt had caught fire\u201d, herds of hundreds of reindeer being killed, trees set alight by the radiance of the fireball only for the flames to be snuffed out by the subsequent blast wave. And the reports are unequivocal on the source of the blast. G. K. Kulesh, head of a meteorological station at Kurensk, 200 kilometres from the epicentre, told Voznesenskij that: There appeared in the northwest a fiery column \u2026 in the form of a spear. When the column disappeared, there were heard five strong, abrupt bangs, like from a cannon, following quickly and distinctly one after another \u2026 there had been a strong shaking of the ground, such that the window glass was broken in the houses \u2026 It is probably established that a meteorite of very enormous dimensions had fallen. In the days after the blast, much of Europe experienced eerie 'bright nights': readers wrote to  The Times  in London, remarking that its columns could be read outdoors at midnight. Polarization measurements are consistent with this being due to sunlight scattered by dust in the very high atmosphere; observatories recorded increased atmospheric opacity and scattering across the Northern Hemisphere. This spreading dust may have been due to a plume ejected backwards along the incoming object's path by its explosion. Such plumes were seen on Jupiter when the fragments of Comet Shoemaker-Levy 9 slammed into it in 1994; hydrodynamic modelling by Mark Boslough and his colleagues at Sandia National Laboratories in Albuquerque, New Mexico, indicates that a similar terrestrial plume could be expected for an impact such as that at Tunguska 2 . There was, however, one good reason to doubt that a small asteroid was involved: the belief of the time that this would deliver a valuable hunk of iron to the surface. The Russian meteorite hunter Leonid Kulik, who led the first expedition to the epicentre in the 1920s, obtained funding from the Soviet government on the basis that he would find a valuable ore body there. But when he reached his goal in 1927 he found no metal. Nor did he find the crater that an impact was expected to leave. (There are now claims that nearby Lake Cheko might be such a crater, but these are widely disputed.) There were clear signs of violence \u2014 trees knocked flat over a vast swath of land \u2014 but no big hole in the ground. What could have happened? In 1930, US astrophysicist Harlow Shapley suggested that the lack of a crater was due to the nature of the impactor. If it had been a comet, and comets were light and fluffy, then it would have exploded at altitude. This idea persisted for decades: in 1982 some planetary scientists were willing to postulate the extraordinarily low density of 3 kilograms per cubic metre in order to explain Tunguska in terms of the blast from a disintegrating comet. Other explanations were even more far fetched than candyfloss comets. Soviet science-fiction author Alexander Kazantsev realized, as Shapley had, that the best explanation involved an explosion at altitude, and suggested in 1946 that a nuclear-powered alien spaceship exploding just before landing might have been the culprit, an idea taken up eagerly and earnestly in the following decades. A more scientifically promising possibility was naturally occurring antimatter, a suggestion made independently by various people at various times. In 1940, Vladimir Rojansky of Union College, Schenectady, New York, suggested that some meteors and comets might be made of antimatter \u2014 'contraterrene' matter in the terms of the time \u2014 and that their odd behaviour might be detectable 3 . (More than 30 years later Rojansky suggested that it would be worth checking if Comet Kohoutek was one of the antimatter ones.) In 1941, Lincoln LaPaz of Ohio State University in Columbus published two articles in the magazine  Popular Astronomy  that argued that large terrestrial craters and the craterless Tunguska explosion were bothdue to antimatter meteors; he later wrote to the Soviet Academy of Sciences suggesting a search for anomalous isotopes at the site.  \n                Radioactive remains \n              More than a decade later, Philip Wyatt, a graduate student at Florida State University in Tallahassee, and Boris Podolsky, author of a famous paper with Einstein exploring apparent paradoxes of quantum mechanics, went to a movie in which antimatter featured. Podolsky pointed Wyatt towards Rojansky's 1940 paper and suggested he look into the impacts idea. Wyatt \u2014 now the chief executive of the Wyatt Technology Corporation in Santa Barbara, California \u2014 says that he was \u201cmostly interested in looking for residual radioactivity\u201d and published some ideas on the subject in  Nature 4 . This notion was expanded on by three eminent American scientists (including 1960 Nobel Prize winner Willard Libby and Clyde Cowan, co-discoverer of the neutrino) in 1965. Libby, the original developer of the carbon-14 dating technique, found support for the idea of an antimatter impact from what seemed to be an elevated carbon-14 level in tree rings around the world in 1909, suggesting that significant quantities of the isotope had been created by radiation given off when the antimatter annihilated itself on contact with the thicker layers of the atmosphere 5 . Even at the time, though, there were good arguments against the idea: among other things, the first gamma-ray-detecting satellites were not seeing the tell-tale radiation from antimatter annihilation elsewhere in the nearby cosmos. Even more extreme, in 1973 two University of Texas physicists suggested that the cause was a black hole passing through Earth 6 . This was nothing if not fashionable: miniature black holes had just been postulated by Stephen Hawking as after-effects of the Big Bang. Again the explanation was incomplete and its implications \u2014 an exit on the other side of the planet, and a seismic signal lasting well after the initial impact \u2014 unobserved. Similar caveats apply to the intriguing hybrid idea, aired as recently as 1989, that the culprit was a deuterium-rich comet turned into a hydrogen bomb by the heat and pressure of its arrival in the atmosphere. Another approach has been to suggest that, despite the straightforward implications of eyewitness accounts of a bright object zipping across the sky, the source of the blast was in fact beneath the surface. A recent example is a claim that it was due to a 10-million-tonne belch of methane that subsequently exploded high in the sky. Others see a geophysical source involving peculiar tectonic behaviour.  \n                Hammer time \n              The fact that such ideas were entertained (and still are, in some circles) speaks both of a certain fascination with the fanciful and the abiding need to explain that confusing lack of a crater. The fact that, by the 1960s, various craters around the world had been accepted as meteorite strikes meant that the anomalous lack seemed all the more confusing. In 1993 that confusion was allayed, at least for most people, by Chris Chyba, Kevin Zahnle and Paul Thomas 7 . With the help of computer simulations derived from nuclear weapons' tests they showed that a solid, stony object about 50 metres across \u2014 the most likely sort of thing in that size range to hit the Earth \u2014 would not be expected to reach the ground. There was no need to invoke weirdly low cometary densities \u2014 at the relevant speeds the shock wave generated within a solid body as it slams into the atmosphere would rip up an everyday rock just fine. Formations such as Meteor Crater in Arizona (see  page 1172 ) are left by tougher impactors made of metal; the shock waves don't get the better of them until they've reached the ground. A similar explanation was arrived at by Jack Hills, working at Los Alamos National Laboratory in New Mexico with Patrick Goda 8 , and both teams had been to some extent pre-empted by a Soviet team led by V. P. Korobeinikov, the work of which had not been widely appreciated in the West 9 . These various models led to an estimate that the blast was equivalent to about 15 megatonnes of high explosive \u2014 bigger than all but the very largest thermonuclear weapons. However, work by Boslough indicates that the energy required to fit the observed phenomena could be rather less, around 3 to 5 megatonnes. That analysis assumes that the impactor was a stony asteroid \u2014 but a comet is still a possibility. In 1978, L'ubor Kres\u00e1k suggested the Tunguska impactor was a fragment of Comet Encke[10]. The peak of an annual intense meteor shower associated with dust from Encke occurred around 30 June 1908, but because the meteors arrived from the direction of the Sun, the shower would not have been visible to the naked eye. What the eyewitnesses said about the direction of the Tunguska projectile is consistent with that idea. An analysis of many hundreds of possible pre-impact orbits for the object published in 2001, by a team that had been led by the late Paolo Farinella, indicated that an asteroidal orbit was more likely than a cometary orbit[11] \u2014 but using that paper's definitions, Comet Encke, which takes just 40 months to orbit the Sun, has an asteroidal orbit. Another line of evidence, suggested in 1977, was that a comet might explain the carbon-14 signature reported by Cowan in the 1960s; a comet in space might naturally be thoroughly irradiated[12]. The question of what the object was is not purely academic. If Tunguska was indeed a 15-megatonne event, it was rather unlikely \u2014 such things are expected only every 1,500 years or so. That calculation, though, assumes that the flux of near-Earth objects is constant over time. If the population of near-Earth objects is replenished from time to time by the break-up of a comet, then shortly after that break-up, impacts from Tunguska-sized fragments will be more likely. Earth may suffer near misses from Tunguska's dark and stealthy cousins every time it passes through Encke's dust stream \u2014 fragments too small to be easily observed, but big enough to cause quite a mess if they hit. In  Rendezvous with Rama , Clarke's solution to the threat of impacts was an asteroid search programme aimed at ensuring that such a catastrophe could never occur again: he called it Project Spaceguard (see  page 1178 ). This became the name of a real-life programme, and that search continues. But 50-metre objects are too small to spot far in advance of their impact. So although another Tunguska coming out of the blue is not a likely event in any given June, it is not out of the question. \n               Duncan Steel is an astronomer and writer after whom Arthur C. Clarke once named a robot. \n             See Editorial,  \n                     page 1143 \n                   , News Feature,  \n                     page 1170 \n                   , and Commentary,  \n                     page 1178 \n                    . \n                     William Hartmann's paintings of eyewitness testimony \n                   \n                     The X-Files \n                   \n                     Wyatt Technology \n                   \n                     The Tunguska air-defence system \n                   \n                     \u201cEverything you know is wrong\u201d by Antimatter \n                   Reprints and Permissions"},
{"file_id": "454154a", "url": "https://www.nature.com/articles/454154a", "year": 2008, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Does the difficulty in finding the genes responsible for mental illness reflect the complexity of the genetics or the poor definitions of psychiatric disorders? Alison Abbott reports. Every family has its foibles, but this one has more than most. The first member came to researchers' attention in 1968 as part of a genetic survey of juvenile delinquents who had been admitted to Scottish detention centres. One boy carried a major 'translocation', in which a chunk of chromosome 11 had been switched with part of chromosome 1. The translocation and the boy's bad behaviour were more than just coincidence. Years later, when Edinburgh researchers traced the family, they found that the same chromosomal abnormality spanned four generations, with remarkably varied effects. Of those who carried it, five had depression, six had schizophrenia or related disorders, three had adolescent conduct disorder and two had anxiety disorder. One had attempted suicide and died in a mental hospital 1 . Several of those without the translocation had their own problems, including anxiety, minor depressive disorder and alcoholism. The translocation chops up a gene called, with some justice, disrupted-in-schizophrenia 1 ( DISC1 ) 2 . This gene is arguably the strongest contender for involvement in any psychiatric disease, and a flurry of activity has shown that it codes for a hub protein involved in multiple biochemical pathways in the nervous system. But the association and its Scottish heritage are also a textbook example of the complexity of psychiatric genetic studies.  DISC1  has not shown up in many other screens for schizophrenia genes, even though it has occasionally been identified in genetic studies of other illnesses, including bipolar disorder and autism. Researchers are convinced that variations in  DISC1  and many other genes can scramble the intricate and as yet inscrutable processes by which brain circuits develop and function, and that this scrambling leaves some people at greater risk of psychiatric problems than others. Even if one or more risk genes are present, external stress or some other event may decide whether and what symptoms are triggered. Finding genes involved in psychiatric conditions is proving to be particularly intractable because it is still unclear whether the various diagnoses are actually separate diseases with distinct underlying genetics or whether, as the  DISC1  story suggests, they will dissolve under the genetic spotlight into one biological continuum. Indeed, some researchers suggest that it would be better to abandon conventional clinical definitions and focus instead on 'intermediate phenotypes', quantifiable characteristics such as brain structure, wiring and function that are midway between the risk genes involved and the psychopathology displayed. In the past two years, researchers have pulled out a host of genes involved in other multifactorial diseases, such as diabetes and obesity, by use of genome-wide association studies. These use powerful new genomic tools to scan for variations in the DNA sequence called single nucleotide polymorphisms (SNPs) that tend to occur in individuals with a particular condition. They allow scientists to see which gene variants pop up more frequently in people who have a disorder. Finding small genetic signals is a question of statistics: a weak association between a gene and a disease may stray into significance only when a study has hundreds or thousands of participants. But instead of helping to firm up which genes might be candidates, the largest population studies completed so far in psychiatric genetics seem to be eliminating them. A study this year led by Patrick Sullivan, a geneticist at the University of North Carolina at Chapel Hill, involved nearly 750 patients with schizophrenia and a similar number of controls, and analysed almost half a million SNPs. But not one gene met the rigorous statistical requirements needed to show it was a risk factor 3  \u2014 not even  DISC1 . Advocates of such studies don't take this as a sign of defeat. \u201cIt shows that the studies so far are still statistically underpowered,\u201d says Jonathan Flint, a molecular geneticist at the University of Oxford, UK. \u201cWe need even bigger studies.\u201d On the basis of successful genome-wide association studies in diseases such as diabetes, many geneticists think that several thousand cases and controls may be needed if a firm association is to be found. Some large prospective studies launched by funders such as the US National Institutes of Health (NIH) and the UK Wellcome Trust are pumping up the numbers (see  table ). And last year, a US$100-million donation from the Stanley Medical Research Institute in Chevy Chase, Maryland, to the Broad Institute in Cambridge, Massachusetts, became the largest philanthropic gift ever given to an institution for psychiatric research and much of it will support these types of study. \u201cThese studies have not thrown up any replicable findings so far, and we might have to wait years,\u201d says Daniel Weinberger, an experimental psychiatrist at the National Institute of Mental Health in Bethesda, Maryland. \u201cDo we really want to spend millions of dollars to extend the size of studies that may not throw up the right signals?\u201d \n               boxed-text \n             Weinberger is the most outspoken advocate of 'intermediate phenotypes'. He focuses on candidate genes \u2014 those for which there are already good biological reasons to think that they might confer risk. Weinberger argues that hallucination, panic attacks or other disordered behaviours are consequences far downstream of faulty genes. What genes code for more directly, he rationalizes, are the biological traits in the brain that he measures, and that combine to alter behaviour. Following this logic, the coupling between the gene and the intermediate phenotype should be closer, cleaner and easier to find than the coupling between the gene and the actual diagnosis. Weinberger has produced a string of high-profile papers, and some scientists have been persuaded to adopt his approach. This month, his methodology will get its own massive cash endorsement when the Lieber family and the Essel Foundation \u2014 which Constance and Stephen Lieber created in the 1960s to support neuroscience research and education \u2014 announces an endowment that will more than rival that of the Stanley Medical Research Institute. But many geneticists \u2014 those pursuing the genome-wide association studies \u2014 are not shy to say that they find Weinberger's approach inadequate. \u201cLooking for candidate genes is a waste of time and money,\u201d says Flint.  \n                Breaking down boundaries \n              How best to pull out psychiatric risk genes has become an acrimonious discussion, one hotly debated in meetings at which Weinberger's forceful defence of his theory has on occasion fuelled flames. Weinberger insists that much of the dispute is ideological, because the problem is forcing geneticists and neuroscientists to cross into each other's domains, and that this makes his papers a tough sell with referees. \u201cGeneticists know nothing about psychiatric disease and have no frame of reference for thinking about the brain,\u201d he says. \u201cBut then brain neuroscientists aren't thinking about genetics either.\u201d The field may be divided about how to identify risk genes, but it is united in its desire to do so. Such a catalogue might make it possible to identify those at risk of mental illness before it arises and, when symptoms appear, to place them accurately on the spectrum of mental disorders. It might also point to new biochemical pathways at which to direct treatments. A consensus is emerging that from a catalogue of 100\u2013300 risk genes, only a few \u2014 say between 5 and 10 \u2014 will be needed to precipitate psychosis in an individual. Each gene is likely to confer a very low risk on its own, so will be hard to identify unequivocally with any method. The challenge of identifying these genes is magnified by the difficulty in identifying the right population to study. This is well illustrated by the widely used handbook of the American Psychiatric Association called  DSM IV  ( Diagnostic and Statistical Manual of Mental Disorders , fourth edition), which lists nearly 300 subcategories of psychiatric disorder. The full version of the  DSM  takes up more than 700 pages and took 1,000 individuals six years to produce. But few psychiatrists would deny that it is a crude approximation and that the symptoms used to assign categories substantially overlap. Delusions are frequently a symptom of schizophrenia, but a typical person with schizophrenia might also exhibit apathy. Patients with major depressive disorder usually exhibit apathy, but some will also be delusional. A clinical diagnosis \u201cmay be the mental end-stage of all sorts of different neurobiological disorders\u201d, says Robin Murray, a psychiatrist from Kings College London. \u201cIt may be a little like a diagnosis of 'renal failure' which is the end-stage of all sorts of medical disorders from heart disease to toxic shock.\u201d This presents a profound problem for genetic studies because it means that some researchers may be looking for shared risk genes in a group of people who may not actually share any or, indeed, have the same disease. It is an absolutely crucial point; many researchers think that poor and inconsistent diagnoses may explain much of the past failure of genetic-association studies and will be the factor on which future success hinges. The difficulties became apparent even before the genome-association studies. The  DISC1  story \u2014 in which a strong candidate identified in one study is rarely found in another \u2014 has been repeated over and over in family studies and in case\u2013control studies, in which groups of unrelated individuals with a disorder are compared with healthy controls. Weinberger is convinced that the right signals will be found by his candidate-gene approach, and says that millions of the Lieber family's money will be spent to that end. Weinberger likens the huge endowment to the Manhattan Project in its ambition and scope. \u201cWe'll try to work out the risk genes, work out the faulty circuitry and work out how to develop new ways of treatment,\u201d he says. Weinberger's first major breakthrough involved a gene called  COMT , which encodes a key enzyme in the metabolism of dopamine, a neurotransmitter that is important in schizophrenia and the target of many antipsychotic drugs.  COMT  had already been linked to schizophrenia in family and case\u2013control studies. But Weinberger instead examined whether variation in the gene was linked to one of his intermediate phenotypes. In this case he used functional magnetic resonance imaging (fMRI) to examine the activity of the brain's prefrontal cortex, which is known to function abnormally in schizophrenia. In a 2001 paper, Weinberger and his colleagues showed that people with a variation in  COMT  that causes a methionine amino acid to be substituted for a valine in the protein tended to have worse working memory and lower activity in the prefrontal cortex than those without the mutation. They also found that the mutated version of the gene and the poorer working memory were found in more parents whose offspring had schizophrenia than in those who didn't 4 . That makes biological sense, says Weinberger, because the two forms of the enzyme differ in how efficiently they break down dopamine, so children who inherit the methionine version would have upset levels of a key transmitter that might make them more susceptible to schizophrenia. The study was a turning point in the field, says Jeremy Hall, a neuroscientist at the University of Edinburgh, UK, who was one of those convinced to adopt the approach. But Flint argues that no one has yet shown that intermediate phenotypes have a tighter link to the genes responsible than the disease itself. \u201cI just don't buy that brain size or whatever will work better,\u201d he says. Weinberger quickly followed his 2001 paper with another in which he examined variants of the serotonin transporter gene that are known to be risk factors in psychiatric disorders such as depression. Rather than looking for an association between genetic variants and a specific disorder, Weinberger looked for links with another intermediate phenotype. He used fMRI to show that those with the 'short' version of the gene \u2014 who have less efficient serotonin signalling \u2014 also had more activity in the amygdala when they reacted to fearful stimuli such as angry or afraid faces 5 . It was the first time that anyone had shown that a gene could change how a region of the brain responds to an emotional stimulus \u2014 the idea being that it could predispose someone to a range of psychiatric conditions if he or she encounters the right environmental trigger. In 2003, a study done in New Zealand independent of Weinberger identified one of those potential genes. The team used data from a long-term study that tracked 1,000 children through young adulthood, and showed that those with the short variant of the serotonin transporter gene were more likely to be thrown into serious depression when faced with a stressful life crisis, such as relationship angst, than were those with the long variant 6 . It's a classic case of gene\u2013environment interaction, and Weinberger has gone on to identify other gene variants, such as one that could make children more likely to develop schizophrenia if they are exposed to a period without oxygen during birth.  \n                Emotional response \n              The main criticism of the candidate gene approach is that it is restricted to the tiny list of genes that have established links to the disease, precisely what researchers are short of. \u201cWe are just too ignorant of the underlying neurobiology to make guesses about candidate genes,\u201d says neuroscientist Steven Hyman, provost of Harvard University in Cambridge, Massachusetts. \u201cCandidate genes are like packing your own lunch box and then looking in the box to see what's in it.\u201d That's why Hyman and many other researchers say that, despite their past failures, bigger and better genome-wide association studies are a more promising way forward for psychiatric genetics. Several meta-analyses are now under way in attempts to increase the statistical power and to pick up risk genes that earlier studies might have missed. One such initiative, the Psychiatric Genome-Wide Association Study Consortium, comprises groups from all over North America and Europe doing such studies. It focuses on five disorders \u2014 autism, attention-deficit hyperactivity disorder, bipolar disorder, major depressive disorder and schizophrenia, and includes some 59,000 individuals. Where possible, the scientists are pooling their data so that they can detect genetic associations that their individual efforts could not pick up alone. For example, the consortium is currently comparing 13,000 cases of major depression with 10,000 controls. Brute numerical force might still not be enough if researchers are not exquisitely careful about the way they diagnose patients, and even Hyman expresses reservations. \u201cThere are potential differences in different patient samples \u2014 the name autism or schizophrenia does not necessarily identify similar populations because of a lack of objective medical tests.\u201d If patients grouped together as having schizophrenia actually include some with other disorders, then statistical power becomes moot. Sullivan, a member of the consortium, agrees that it will take great care to ensure that cases are comparable in terms of diagnosis and ancestry, and across different genetic technologies. He would like critics to wait for ongoing studies to be completed before pronouncing on their likely fate. \u201cIt is not the time to play Chicken Licken \u2014 'the sky is falling, the sky is falling',\u201d he says. One way to ensure that all recruits are handled identically is to diagnose them carefully when they are enrolled rather than relying on existing diagnoses. Flint is working with other investigators around the world to register 6,000 women with recurrent depression and 6,000 controls in such a study. The first results won't be out until 2011. \u201cIt is delayed gratification,\u201d says Ken Kendler, one of the study's principal investigators and director of the Virginia Institute of Psychiatric and Behavioural Genetics at Virginia Commonwealth University in Richmond. \u201cIf you are in a rush, this isn't the field for you.\u201d More immediate gratification is coming from association studies of a different type, those that look at stretches of the genome that may be duplicated or deleted from one person to the next. One study published this year found such copy number variants (CNVs) linked to schizophrenia in up to 10% of non-familial cases \u2014 those that have not been inherited 7 . The CNVs arose spontaneously in the affected individuals, rather than being inherited from the parents. Some of the genetic regions that are being identified have been implicated in autism, underlining how some risk genes are shared across diagnoses. These mutations tend to confer a higher risk of the disease, perhaps explaining why they are being picked up in association studies where SNPs were not, and they may turn out to be particularly useful in diagnosis. But everything must always come back to the biology, says an unimpressed Weinberger. He likens these genome searches to a study trying to understand why road accidents happen by throwing everything that's known about drivers into the pot. \u201cYou'll find that the only thing they all have in common is ownership of a driving licence,\u201d he says. \u201cBut that's not going to give you any insight into the diverse things that really matter,\u201d such as local speed limits, foggy weather and when the local bars close. Similarly, genome-wide association studies for schizophrenia might highlight a gene linked to oxygen deprivation at birth \u2014 one of the strongest risk factors for the disorder \u2014 but that has nothing to do with the brain. \u201cThe endgame can't be statistical,\u201d Weinberger says, \u201cit has to get down to gene variants that impact the neurobiology involved.\u201d  \n                Back to biology \n              Population geneticists insist that without rigorous statistics, candidate genes, logical though they may sound, could turn out to be false leads. And once statistically validated genes are in hand, the plan is to investigate their biological role in disease, along with their interactions with other genes and the environment. No one doubts that the animosities will defuse as more data are generated and the power of the approaches becomes clear. Whether those data will help to refine the 300 categories of disease, remains uncertain. If the same pool of risk genes underlies all the disorders, the existing groupings could vanish, only to be replaced with an even longer list in which diagnosis is based on countless genetic combinations. Mental-health professionals are already working on the fifth edition of the diagnostic handbook. The final version is due to be released in 2012, but even that may be too soon for genetics to help substantially in reclassification. Psychiatrists are already coming to the conclusion that the  DSM  should be used flexibly. They realize that treatments should control the specific symptoms a patient displays \u2014 depression by antidepressants, for example, even if their formal diagnosis is schizophrenia. With questionable diagnoses and impenetrable biology, symptoms are the best indicators that the psychiatric profession has to go on. But when the dust settles, and the data do start to flow, genetics may finally offer some clarity.\n See Editorial,  page 137.  See also Correspondence \u2014  Mental health: maybe human troubles don\u2019t fit into categories ,  Mental health: don\u2019t overlook the environment and its risk factors ,  Mental health: drop ideological baggage in favour of best tools \n                     Neuroscience \n                   \n                     Genetics Genetics \n                   \n                     Psychiatric disorders focus \n                   \n                     Catalogue of genome-wide association studies \n                   \n                     Candidate gene list for schizophrenia \n                   \n                     Gene Association Information Network (GAIN) \n                   Reprints and Permissions"},
{"file_id": "454151a", "url": "https://www.nature.com/articles/454151a", "year": 2008, "authors": [{"name": "Laura Spinney"}], "parsed_as_year": "2006_or_before", "body": "Armed with a map depicting a 10,000-year-old landscape submerged beneath the North Sea and fresh evidence from nearby sites, archaeologists are realizing that early humans were more territorial than was previously thought. Laura Spinney reports. Pilgrim Lockwood, the skipper of a British fishing trawler named  Colinda,  wasn\u2019t quite sure what to make of the thing his nets had scraped up from the bottom of the North Sea. Just over 21 centimetres long, it was made of antler with a set of barbs running along one side. Back on land, Lockwood gave the artefact to the ship\u2019s owner, and it eventually made its way to a museum in Norwich, UK. It turned out to be a prehistoric harpoon point dating to the Mesolithic period, between about 4,000 and 10,000 years ago. That was 1931, and archaeologists studying the artefact, which became known as the Colinda point, began to realize that hunter-gatherers would once have roamed across a vast plain that connected Britain to the rest of Europe. But they had no idea what the plain looked like or what life would have been like for the harpoon\u2019s makers. Now researchers have drawn the first map of that lost world, sketching out a 10,000-year-old landscape filled with marshes, rivers and lakes. It turns out that the region they call Doggerland may have been a sort of paradise for Mesolithic people. Because the archaeological evidence from the period is thin, Mesolithic people have in the past been depicted by researchers as restless nomads and Doggerland as a land bridge through which they passed without leaving a trace. The new map suggests that, on the contrary, Doggerland would have been an ideal environment for them to linger in \u2014 until sea levels, rising since the end of the last ice age, finally inundated it, turning Britain into an island about 8,000 years ago. Along with other new discoveries in Britain and continental Europe, the research is helping to fill in crucial gaps in the current knowledge about Mesolithic life. \u201cDoggerland is key to understanding the Mesolithic in northern Europe,\u201d says Vince Gaffney, a landscape archaeologist at the University of Birmingham, UK. \n               boxed-text \n             Along with his colleagues Simon Fitch and the late Ken Thomson, Gaffney established the mapping project to outline the terrain of Doggerland, named after the sandbank and shipping hazard of the Dogger Bank (see \u2018Mesolithic sites around the North Sea\u2019). They managed to borrow seismic survey data, which outline sediment layers below the seabed, from the Norwegian oil company Petroleum Geo-Services. The researchers then put their powerful computers to work to reconstruct Doggerland in three dimensions. In a pilot project beginning in 2002, the researchers reconstructed 6,000 square metres of the ancient landscape \u2014 slightly larger than a football field. There, about 10 metres beneath the modern seabed, they discovered the course of a major ancient river, almost as big as today\u2019s Rhine. They named it the Shotton River, after Birmingham geologist Fred Shotton who, among other things, was dropped behind enemy lines to map the geology of the Normandy beaches before the D-Day landings. Now confident that the reconstruction would work, the researchers expanded the project. The result is a 23,000-square-kilometre map of a part of Doggerland \u2014 an area the size of Wales \u2014 that they hope eventually to extend northward as well as eastward, towards the Netherlands 1 . Archaeologists are excited about the map for several reasons. First, with an idea of how the terrain undulated, they can work out how, and how quickly, it was submerged. It is thought that the sea level rose no faster than about one or two metres per century, and that the land would have disappeared in a series of punctuated inundations. \u201cIt was perfectly noticeable in a generation,\u201d says marine archaeologist Nic Flemming, a research fellow at the National Oceanography Centre of University of Southampton, UK. \u201cBut nobody had to run for the hills.\u201d Second, researchers can now start to predict how Mesolithic people might have used the terrain. But it won\u2019t be easy. Working with divers and remotely operated vehicles is complex and expensive, and the new map isn\u2019t detailed enough for underwater excavation purposes: the smallest detectable feature on it is about 10 metres high and 25 metres wide. To get a more detailed picture of what Doggerland might have looked like, computer specialist Eugene Ch\u2019ng of the University of Wolverhampton, UK, is building a virtual-reality simulation. Starting with a 27-kilometre stretch of the Shotton River, he has recreated a Stone Age settlement by the water\u2019s edge, at the confluence of two rivers, complete with thatched huts and racks for drying fish and tanning hides 2 . The virtual vegetation is faithful to the palaeobotanical record, right down to the stinging nettles. There are fish in the rivers, birds in the air and boar in the woods \u2014 although for now these are just avatars rather than accurate biological models. The only thing that is missing is the people, but Ch\u2019ng will add them soon. He is constantly feeding new data into the simulation, and his ultimate goal is to turn the virtual reality into augmented reality, in which archaeologists need only don a headset to enter the Mesolithic landscape.  \n                Found artefacts find meaning \n              The most immediate way in which the map will be useful, however, is in giving context to marine archaeological finds. For more than a century, fishing boats \u2014 particularly Dutch beam trawlers, whose nets scrape the seabed \u2014 have been scooping prehistoric material out of the North Sea. Most of it dates from the Palaeolithic, the vast era that ended around 10,000 years ago, and includes the bones of woolly mammoths and reindeer from the last ice age. But there is also some more recent, Mesolithic material. Until now, archaeologists haven\u2019t been sure how to interpret these scattered remains. But with the Doggerland map, \u201cwe\u2019ll be able to position the archaeological finds within that landscape to understand their meaning,\u201d says Hans Peeters of the National Service for Archaeology, Cultural Landscape and Built Heritage in Amersfoort, the Netherlands. Jan Glimmerveen, a Dutch amateur archaeologist, has over the past decade collected around 100 Mesolithic artefacts, which he gets from fishermen trawling the southern North Sea. The material includes an adze- or axe-like tool made from an antler, with part of the wooden shaft preserved, and a tool made from the bone of an aurochs, a large type of extinct cattle. Some of the artefacts have been radiocarbon dated to between 10,000 and 8,100 years ago, and all come from a small area just off the southern edge of the Birmingham map of Doggerland. The Dutch call it \u2018De Stekels\u2019 (\u2018The Spines\u2019) because there are steep dunes that were probably once close to a river. Although the artefacts were lying loose on the seabed, Glimmerveen is convinced there was a Mesolithic settlement on or close to those dunes, and Peeters agrees. \u201cYou can look at it in a similar way to ploughed fields,\u201d he says. \u201cObjects may have been displaced, but not over very large areas.\u201d Dutch researchers have a unique opportunity to retrieve Mesolithic material that was once underwater but now is part of reclaimed land. Peeters, for instance, works in a region of the central Netherlands, the Flevoland polders, that was reclaimed from an inland sea in the mid-twentieth century. In the early Mesolithic period, this area would have been tidal wetlands. Later it became a peat marsh, and the ancient bog has yielded a wealth of objects, including pottery and flint tools, that he thinks were deposited as a ritualized reference to lost ancestral lands 3 . Back on the British side, archaeologists are extracting Mesolithic information from a submerged site known as Bouldnor Cliff in the Solent, the stretch of water separating the Isle of Wight from mainland Britain. As sea levels rose, swelling rivers deposited sediments over a Mesolithic valley. When the Solent began to form around 5,000 years ago, it eroded first the sediments and then the original valley floor. That erosion is continuing today, and near the Isle of Wight\u2019s shore, it is uncovering signs of Mesolithic human activity. A few scattered flints emerged first, followed by the remains of what could be a wooden dwelling and then, last summer, tools, wood chippings and part of a log boat. \u201cIn general the preservation is immaculate,\u201d says Garry Momber, director of the Hampshire and Wight Maritime Trust for Maritime Archaeology in Southampton, who is overseeing the project. He believes Bouldnor Cliff may have been a boat-building site, which is significant because it was far from the coast and so the boats would have been used only on a local lake. \u201cWe\u2019re finding evidence of sedentism,\u201d he says. \u201cThese people would have been living and working the land, maybe to a greater extent than we understand now.\u201d  \n                Nomads settle down \n              Attachment to the land, ritual practices and sedentism are usually associated with later, Neolithic people. The boundary between the Mesolithic and Neolithic periods is defined as when farming begins to be practised in an area, and it generally dates to between 4,000 and 6,000 years ago in northern Europe. The stereotype of Mesolithic people is as \u201csurviving in this harsh wilderness\u201d, says Peeters. \u201cIt was only with the introduction of farming that this poor and risky way of life was gradually brought to an end.\u201d This view, he thinks, short-changes Mesolithic people and the imaginative ways they may have used the landscape, both in life and in burial practices (see  \u2018Death in the Mesolithic\u2019 ). Evidence supporting this more complex view of Mesolithic life comes from T\u00e9viec and Ho\u00ebdic, two Mesolithic burial sites on the coast of Brittany, France. Here, archaeologist Rick Schulting of the University of Oxford, UK, has analysed stable isotopes \u2014 mainly carbon and nitrogen \u2014 in human bones to get an idea of what the locals ate. T\u00e9viec and Ho\u00ebdic are only 30 kilometres apart, a trivial distance for hunter-gatherers, and yet Schulting has found consistent differences in the bone isotopes between the two sites. He thinks these reflect differences in their diet: residents of Ho\u00ebdic, for instance, seem to have got a lot more of their protein from marine resources than those in T\u00e9viec. \u201cThat suggests to me that these people were quite embedded into the landscape over the long term,\u201d says Schulting. \u201cThey weren\u2019t moving around on a great scale.\u201d A similar picture is emerging from Britain. In 2000, a team led by Clive Waddington of Archaeological Research Services, in Derbyshire, UK, began excavating a Mesolithic hut at Howick in northeast England. By combining radiocarbon dating with analysis of the soil strata, they were able to determine that three huts had been built at the site, each on the ruins of the previous one. Together, the huts were inhabited over about 150 years. That occupation wasn\u2019t necessarily continuous, Waddington says; nevertheless, over three or four generations people kept returning to that place. To him, this suggests that Mesolithic people may have been staking out their group\u2019s territory. \u201cNot that hunter-gatherers usually have any sense of ownership,\u201d he says. \u201cBut what they do have is a very strong sense of rights of access to land.\u201d Waddington argues, in fact, that the drowning of Doggerland led directly to the development of sedentism and territoriality 4 . Although the idea is speculative, it fits with the growing body of evidence for Mesolithic life in and around Doggerland. Land would have become an increasingly precious resource as the sea rose. All these sites, taken together, may illuminate how Doggerland\u2019s residents adapted to the changing landscape. But when and how did those changes begin? The Birmingham map offers a possible clue in the shape of a giant basin called the Outer Silver Pit, which stretches for up to 100 kilometres through Doggerland. Fed by an inlet to the east, the pit would at one time have been a lake. But two sandbanks running almost its full length could only have been formed by fierce currents. Gaffney speculates that, as the sea rose, the peaceful lake became a fast-flowing estuary into which only the most foolhardy fisherman dared launch his canoe. So what started as an attraction for water-loving people might eventually have driven them away, triggering the migration whose long-range effects Waddington is seeing at Howick. Exact details of this upheaval will be hard to prove, not least because most of Doggerland remains uncharted. Still, archaeologists are in little doubt that such turbulent environmental change required an equally dramatic human response. In just a few thousand years, Doggerland was transformed from a harsh tundra into a fertile paradise, and eventually into the northern European landscape that we know today. \u201cIt put human adaptability to the test,\u201d says Gaffney.\n Laura Spinney is a freelance writer in London and Paris. \n                     North Sea Palaeolandscapes \n                   \n                     The Howick Project \n                   \n                     Hampshire and Wight Trust for Maritime Archaeology \n                   Reprints and Permissions"},
{"file_id": "454570a", "url": "https://www.nature.com/articles/454570a", "year": 2008, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "Electricity grids must cope with rising demand and complexity in a changing world. Emma Marris explores the intricacies involved in controlling the power supply. On a hilltop in the small city of Vancouver, Washington, just across the Columbia river from Portland, Oregon, sits a concrete building owned by the the Bonneville Power Administration (BPA), the federal agency that runs the electricity grid in the Pacific Northwest. In that building's basement one can find Albert Orona enthroned behind a bank of monitors, facing a wall-sized map of the region's power grid. Orona is a dispatcher, one of the unseen men and women who keep power moving smoothly through the grid. It is not a job that would even occur to most users. Electricity is taken for granted: just flip a switch and the light goes on. But to Orona, contending with a network of almost unfathomable complexity, electricity is anything but a given. From his basement command post he oversees more than 24,000 kilometres of high-voltage wires channelling power from one nuclear power plant, several coal plants, 31 dams scattered throughout Washington, Oregon, Idaho and Montana, and an ever-increasing number of wind farms. And that is just the 'bulk transmission grid'; the local distribution from substation to customer is handled separately by the utility companies, the people you send your money to each month. In Orona's domain, something always needs adjusting. When a generator goes down for scheduled maintenance, or fails, he and his team will call for other generators to increase their output. If a disturbance in the grid is imminent, Orona will 'island' parts of the system to isolate them from power fluctuations that could otherwise cause a blackout. And if a blackout threatens to spread \u2014 well, it is hard to imagine the calmly competent Orona discombobulated. \u201cIf I was a kid,\u201d he says, \u201cI would love to do this for free. It is a fun job.\u201d But in a crisis, he admits, \u201cthere is a certain adrenaline flow\u201d. That adrenaline is flowing through the whole electric-power industry these days. Demand for electricity is soaring worldwide. And yet, at least in developed countries such as the United States, a combination of high costs, environmental concerns, and uncertainty over post-Kyoto carbon regulation is making it harder and harder to build new power plants or run new transmission lines. So the grid is increasingly run on the ragged edge of failure, flirting dangerously with 'unstudied states' \u2014 situations in which, as Orona puts it, \u201cif I lose another element, that might lead to cascades that would take out that part of the world\u201d. The challenges that Orona faces at Bonneville are all too typical of these global issues. Fortunately for him and for others in his position, research is now being conducted on ways to hold those unstudied states at bay. New tools include sensors and other 'smart-grid' technologies that will make the grid better able to manage itself. They include energy-smart appliances, local wind- and solar-power generators, as well as 'demand-side' technologies that will help consumers control how much power they draw from the grid. And, of course, they include measures to get both approaches \u2014 smart grids and demand management \u2014 working together. After all, the more that sources of power generation diversify, and the more information that clever appliances send back up the wires, the smarter the grid will have to be to cope with the complexity. The result could be something completely new. If today's grid is Hollywood in the 1930s \u2014 with a few big studios piping content for viewers to watch passively in a theatre \u2014 tomorrow's grid will be YouTube, with thousands of smaller players and the line between consumer and supplier considerably blurred.  \n                Mapping the system \n              One of Orona's colleagues is on a ladder updating the aqua-tiled wall display with paper status tags. The board shows the grid like a subway map. A lit bulb indicates that a plant or substation is offline. And black dots indicate power plants that can 'black start', or begin generating power from scratch without any power flowing in. Meanwhile, Orona is sipping his soft drink and flipping through different screens on his seven monitors. Alarms are going off at the rate of about two a minute. One sounds like a van backing up, another like the wrong answer on a game show. Some Orona seems to ignore, others cause his head to flick momentarily to one monitor or another. There are three fundamental facts that ensure Orona's job is never dull. First, electricity moves at nearly light speed, so that transmission is essentially instantaneous; the electric field illuminating your light bulb this second was born in a dam or some other generator this same second. Second, electricity cannot be stored, except on a very limited scale. Taken together, these two facts mean that Orona and his counterparts elsewhere have to keep generation almost exactly balanced with demand on a second-to-second basis. When a baseball stadium flips on the lights in Seattle, the Grand Coulee Dam in eastern Washington has to route a bit more water to its turbines. Except that it isn't that simple \u2014 which is the third fundamental fact: the grid is extraordinarily complex. The power flowing through the stadium lights cannot actually be traced back to a single generator; it comes from the common flow of energy fed by all the generators. (Think of dipping a bucket into a reservoir fed by many rivers: it pulls up water from all of them.) So the grid has to be managed as an integrated whole. Yet the grid is also a hotchpotch: in much of the world it has been 'integrated' over the decades by patching together small, local grids as the opportunities arose. And the energy pulsing through that hotchpotch can be downright willful. If a transmission line goes dead, the electricity will spontaneously reroute itself along any other path it can find. So if there aren't a lot of redundancies in the system \u2014 as often happens these days \u2014 and if the extra power moves to other lines that are already near capacity, those lines might also overload and shut down. This can lead to still more shutdowns, in an ever-increasing chain reaction that becomes a region-wide blackout. Or maybe not. Anticipating every conceivable sequence of failures is all but impossible in a system this complex. That's why Orona talks about unstudied states. Even the grid's normal operation is difficult to predict. All the current computer simulations are lousy, says Bonneville modeller Scott Simons, despite ongoing efforts to improve them. \u201cWe are getting pretty good for one hour from now, not so good for two hours,\u201d he says. \u201cThree hours is pretty bad.\u201d In short, humanity has come close to building a machine that is so intricate that it can't be comprehended. But Orona and his fellow power dispatchers have to manage that machine anyway; modern society depends on it. Given the increasing complexity of the grid, the most immediate priority is to give the dispatchers an upgrade in the 'situational awareness' department. Situational awareness, a favourite phrase of grid managers, means 'knowing what is happening'. Bonneville is already working to modernize all those displays and alarms. For example, since May, when the visit described in this article took place, the aqua-tiled wall display has been replaced with an electronically updatable digital version. Efforts are also under way to improve the user interface so that Orona can get the same data about the grid on fewer monitors.  \n                Web of information \n              Out in the field, next-generation sensors will soon be feeding Orona better data. New 'syncrophasor' units, for example, use the precision of the clocks on Global Positioning System satellites to compare two frequency measurements taken in different parts of the grid at the exact same time. This allows them to hunt for sudden changes that hint that the grid is under a lot of stress, or that something major has gone out and the rest of the grid is shifting accordingly. Compared with the grid's older mix of analogue and digital sensors, says Carl Imhoff, an engineer at the Pacific Northwest National Laboratory in Richland, Washington, \u201cphasor networks sample the grid 30 times a second instead of once every 6 seconds\u201d. Meanwhile, improvements in real-time simulations of the grid are helping dispatchers to make better use of that information. Simons is working on one such computer model that will provide operators with the most economical mix of generation to meet demand at any given moment. The cheapest power on the grid isn't always the best deal, because power leaks out of the lines at a rate that depends on distance and wire type. According to Simons, the model even takes into account whether certain efficiency gains are too puny to be worth a dispatcher's time to make the requisite phone call. \u201cIf we called Grand Coulee Dam and said 'tell you what, move your generation up two megawatts' [about 0.03% of the dam's total capacity of 6.8 gigawatts] the laughter would deafen you,\u201d says Simons. Still, despite the computer programs, and the efforts to reduce the cognitive load, Orona's chief tool in managing the grid continues to be a red telephone of the direct-line-to-the-Kremlin variety. When there is a problem, or even the hint of a potential problem, Orona gets on the red phone and, for example, asks another utility to make an adjustment. In the not-too-distant future, however, his part of the grid may very well be able to talk to that part of the grid without human aid. Indeed, there has been a global resurgence of efforts to move most or all of the management of the grid into the grid itself. Under its E2.3-billion (US$3.6-billion) Seventh Framework Programme, for example, the European Commission is planning to fund a number of projects in smart energy networks. And the recent US Energy bill, signed into law on 19 December 2007, calls for a 'Smart Grid Task Force' to run up to five demonstration projects focused on smart grids and energy reliability, and calls on the National Institute of Standards and Technology to develop a set of standards that will allow all such smart grids to be interoperable.  \n                The self-managing grid \n              In some ways the grid is already intelligent. No human needs to trigger a circuit breaker on an overheated transmission line; that happens automatically. \u201cThe grid is always changing, and the way it is designed it is almost self-correcting,\u201d says Jon Ludwigson, a grid expert at the federal government's General Accountability Office in Washington DC. \u201cIf a power line goes out, the grid still flows.\u201d \n               boxed-text \n             To add to this, the controllers are aided by programmed remedial action schemes, also called special protection schemes. These are rules that will automatically trigger an action when a particular threshold is reached. Imagine, for example, that a significant transmission line is lost on a congested route over a mountain pass. To avoid a cascading outage, the reprogrammed routine might automatically cut back on power generation and, to keep things in balance, reduce the load on the grid by shutting off a few power lines downstream of a substation. A few blocks of one town might go black, but the rest of the towns won't even know what a close call they had. \u201cWe stretch the power lines to the limits, and the remedial action scheme allows us to run it at the limits,\u201d says Orona. But in a truly smart grid, the analogous programs would run in processors attached to the various key components of the grid, and would be fed by a rich stream of real-time data flowing in from sensors all over the grid. In principle, says Massoud Amin, an electrical engineer at the University of Minnesota in Minneapolis, such a smart grid could be not only self-managing, but \u201cself-healing\u201d (see  graphic ). In his vision, he says, the processors in the future grid will be able to \u201clocalize and anticipate the consequences of disturbances, whether they are natural disturbances, such as lightning or hurricanes, or intentional disturbances\u201d. By the time the wind-snapped or sabotaged line hits the ground, the whole grid will have shimmered into a new configuration to stymie disastrous cascades. \u201cElectricity travels almost at the speed of light,\u201d says Amin, \u201cso we have a few milliseconds to take this action before it becomes widespread.\u201d He estimates that such a grid in the United States would take ten years to roll out and cost between $10 billion and $30 billion a year to install, shouldered by a public\u2013private partnership. That's no budget operation but, according to Amin, it would cost just a seventh or an eighth of the current annual cost to society of power interruptions. Many grids outside the United States are further ahead in the intelligence-raising process, with particularly advanced projects in Italy, Sweden, and in the state of Victoria in Australia. Enel, Italy's largest utility company, replaced 30 million old power meters with smart, microprocessor-equipped meters that have made possible new pricing structures that encourage customers to shift their power usage to off-peak hours. However, no matter how smart the grid becomes, it will eventually be overwhelmed if demand keeps rising. So industry planners are also moving to embrace micro-generation and to reduce demand. \u201cIn the old days we kept the whole thing balanced with one wrench: the supply side,\u201d says Imhoff. Being able to tweak the system from the demand side will be a huge improvement; it will also make the grid that much more complex. Currently, most consumers suck up electricity in a predictable and mostly unconscious pattern. In the morning, people turn on lights and radios, use kettles and toasters, adjust thermostats and check e-mails. Spike number one. Then people head to work or school, and the electricity dispatchers can have a cup of coffee. After the midday lull, the pattern reverses. People drive home, turn on the heat or air conditioning, cook dinner, do the laundry, surf the Internet and watch television. Spike number two. But if electricity were to cost a lot more during peak hours, and if people's appliances could tell them so, perhaps that unconscious pattern would change. Then again, maybe it wouldn't be enough. Until it is tried, all anyone can say for sure is that the usage peaks would be less predictable. Price fluctuations would ripple because of people's behaviour and then price signals would race backwards towards buyers and sellers in a highly nonlinear set of feedback loops. Small home generators might simply cut off the top of peaks of demand \u2014 or they might turn the peaks into troughs, by sending extra electricity into the grid. Suddenly, the game would have a lot more players.  \n                Supply and demand \n              The first steps towards this future may look familiar and reassuring to consumers. At the Pacific Northwest National Laboratory (PNNL) in Richland, Washington, a decidely normal-looking demonstration kitchen sits in the corner of a lab. But if the stolid white refrigerator detects a disturbance in the grid \u2014 as evidenced by frequency fluctuation \u2014 it will shut off for a few seconds to shed load. Crucially, however, it will not shut off power to the circuit for the little light bulb inside the fridge; the customer will never notice that the appliance is off. Likewise, the clothes in the dryer will keep tumbling even though the heat has been momentarily shut off. Recently, the PNNL tested 150 such dryers in homes in the area. Consumers didn't notice a thing and said they'd be happy to buy them. The researchers think that if the whole country has such appliances, 20% of national demand could be put on hold at any moment. But that is just the beginning. The PNNL is also working on dynamic pricing and distributed generation. Many commercial customers in California already have the option of saving money by running their machinery at night, but PNNL is examining how real-time price fluctuations could be extended to every end user. In a pilot project on the Olympic Peninsula of Washington state that ran from March 2006 to March 2007, residential, commercial and municipal customers in this mountainous region could track prices on their computers, with updates every five minutes. Residential users could set their thermostats, dryers and heaters to respond to certain price points. Small generators \u2014 for example, backup diesel generators for a municipal water-pumping plant \u2014 were programmed to kick in when grid power got too expensive. In the end, the experiment smoothed out grid congestion and customers saved, on average, 10% on their electricity bills. One enthusiastic participant, Jerry Brous, wrote a memo for the project's final report about paying attention for the first time to how he and his wife Pat use electricity, and discovering just how cold or hot they could tolerate their house and water. \u201cIt is also great fun,\u201d Brous wrote, \u201cto sit at a picnic table at an RV park and jump online through a Wi-Fi connection and tell the water heater and heat pump in our house to wake up and get to work, we are coming home early.\u201d Similar projects around the world have generally been received favourably by participants. If such technology is deployed widely, then in 20 years the average residential consumer might well be much more strategic and conscious about their use of energy. That prospect becomes even more likely if small household generation with solar panels or other renewables becomes commonplace; coupled with smart-grid technology in the distribution network as a whole, this would enable neighbourhoods inside a city to 'island' themselves from the wider grid when there is a disturbance. Another piece of the puzzle is provided by energy storage. In Japan, to take just one example, the Tokyo Electrical Power Company has co-sponsored the development of large commercial sodium\u2013sulphur batteries that can hold electricity made in off-peak hours for deployment during peak demand times. Denmark, which produces more wind power per inhabitant than any other county, is likewise looking into storing off-peak energy in vanadium-based fuel cells. Of course, none of these developments is going to put Orona and his team out of work anytime soon; some cognitive leaps are still beyond computers. \u201cSometimes you can have a problem that occurs because you have bad data,\u201d says Orona. \u201cOnly the human can say, 'Hey \u2014 those are bad data'.\u201d When Bonneville engineer Lawrence Carter was asked how long it would take for a major blackout to develop if all the dispatchers at Ditmer and other operations centres suddenly disappeared, he took a while to answer. \u201cEventually, the system would fault,\u201d he said at last. \u201cIt can't run itself.\u201d A high-speed computer, Carter says, would take three years to compute every single fault path in the system. But a human is smarter. \u201cPeople learn about their paths,\u201d he says. \u201cYou may not have to do those 30 million simulations; you might be able to just say, 'This is going to go next'.\u201d Still, there is no doubt that the future of the grid will involve an ever-expanding cornucopia of new technologies: smart appliances, dynamic pricing, micro-generation, energy storage, built-in protective responses \u2014 on and on. \u201cNo one by themselves is going to help us address all the challenges that we have,\u201d says Gil Bindewald, acting deputy assistant secretary for research and development at the Department of Energy's Office of Electricity Delivery and Energy Reliability. \u201cYou might have underground cables in one place, overheads in another, a plug in hybrid there, a microgrid there. I think the entire grid and how we interact with it will change. It is just not the socket in the wall.\u201d See Editorial,  page 551. \n                     Booneville Power Administration \n                   \n                     Tour of GridWise kitchen at Pacific Northwest National Laboratory \n                   \n                     Lighting Africa \n                   Reprints and Permissions"},
{"file_id": "453581a", "url": "https://www.nature.com/articles/453581a", "year": 2008, "authors": [{"name": "Apoorva Mandavilli"}], "parsed_as_year": "2006_or_before", "body": "The human body teems with microbes. In the  first of two features , Asher Mullard looks at the global efforts to catalogue this vast 'microbiome'. In this, the second, Apoorva Mandavilli meets the surgeons who have a rare opportunity to watch an ecosystem being established as they transplant guts from one person to another. Stephanie is the first to admit that she never had the guts for life. She was born with familial adenomatous polyposis, a genetic disorder in which thousands of polyps form in the colon. By the age of 22, much of the organ had to be removed. Four years later, a massive benign tumour choked off the blood supply to her small intestine, so doctors cut out all but a metre of it. For the next six years, she was fed by a tube every night until the feeding left her liver badly scarred and fighting recurring infections. \"I was given a month to live,\" she says. That's when doctors referred Stephanie to Georgetown University Hospital in Washington DC. There, on 17 April 2006 , surgeons cut out her stomach and what was left of her small and large intestine and replaced it with new organs from a donor who had died days earlier in Tennessee. \"Oesophagus to anus, her entire gastrointestinal tract was in the garbage can,\" says Tom Fishbein, who directed the surgery. \"She got a brand new one.\" All organ transplants are complicated, but there are only a handful of centres in the United States that have the expertise to transplant a small intestine, the seven metres of coiled tissue connected up to the stomach at one end and the large intestine at the other. The technique is complicated because the gut is teeming with trillions of bacteria and other microbes, plus the bulk of the body's lymphocytes. Before such transplants, the donor's intestine has normally been flushed with antibiotics. But rates of infection and rejection from such transplants are very high because, it is thought, some foreign bacteria and immune cells survive the cleaning process and are thrust into an immunosuppressed recipient. The idea that these intestinal bacteria are a menace is now under review. By teaming up with microbiologists, the surgeons are taking advantage of a rare chance to study microbes as they colonize the walls of the gut after transplanting an intestine: which ones arrive first, and how they restore the ravaged microbial communities. \"An all new ecosystem of organisms had to populate that bowel from scratch,\" says Fishbein. Their new appreciation of that ecosystem, along with their growing surgical experience, suggests that the populations might be better left intact before a transplant. The same studies may also offer insight into how the gut is first populated by microbes after birth, how it recovers from the damage done by a heavy course of antibiotics and, perhaps, how to minimize that damage. \"Most people study this in animal models, but this is a real-person model,\" says Brett Finlay, a microbiologist at the University of British Columbia in Vancouver, Canada. \"It's an artificial system in some sense, but it's a neat model.\" How the moist, pink intestinal tubing lives in such harmonious contact with bacteria has puzzled scientists for decades. But \"it's hard to get in there, especially in a healthy person\", says David Relman, who studies microbiology and immunology at Stanford University, California. \"And to do it in a way that doesn't perturb the system, and to do it every week or every day, well, forget it.\" For this reason, most researchers interested in the contents of human innards have had to collect and filter faeces.  \n                Beautiful opportunity \n              The transplant scenario is a unique and attractive alternative. For the first few months after an operation, the end of the gut that would normally go into the rectum is left poking out of the abdomen so that doctors can check the transplant is stable. As often as necessary, doctors can probe this stoma, or opening, with an endoscope to pinch off biopsies of the intestinal wall with its microbial community intact. Yellow faeces and glistening, pink skin are signs that the new intestine has successfully taken. \"That is beautiful stuff,\" says Stuart Kaufman, medical director of the intestinal-transplantation programme at Georgetown. \"We live for poop like this.\" The adult samples are also beautiful stuff for researchers who want to chart the arrivals of the different bacterial species. The material is put on ice and shipped to the lab of Jonathan Eisen, a microbial geneticist at the University of California, Davis. There, graduate student Amber Hartman identifies the inhabitants by analysing distinguishing gene sequences that vary slightly between different species of bacteria. It's far too early to draw firm conclusions from their data; Fishbein and his collaborators have only studied 15 patients over the past 2 years. But what they have found suggests that the gut is populated first by enterobacteriaceae, a large family of facultative anaerobes, which can grow with or without oxygen. This suggests that the transplanted tissue has higher oxygen levels than the normal gut, where anaerobic bacteria dominate. Inflammation may boost oxygen levels, giving enterobacteria the advantage. Hartman and Eisen have also found that each person studied so far has had different proportions of the various microbial species, and that these oscillate rapidly in the first few months after a transplant. Their preliminary observations suggest that the more chaotic the variations over time, the worse the outcome of the transplant. \"One thing that becomes very obvious is that the amount or degree of fluctuation is much greater in the sicker patient,\" Hartman says. They now want to know whether the microbiota becomes more stable and reaches equilibrium as a patient recovers, but that requires following more patients over longer times, and intestinal transplants are rare. A different bloody, messy procedure arrives all too often though: a birth. Here, too, researchers see a fascinating opportunity to explore how microbes colonize a gut, one thought to be sterile inside the womb. During birth and in the hours after, babies can swallow bacteria from the mother's birth canal, faeces and from whatever environment they arrive in. No one knows yet whether bacteria move into a baby's spanking new innards in the same way they grab a foothold in a used, adult transplant. Much of what scientists know about the former process has been learned from a study of Relman's, in which he and his colleagues collected used nappies from 14 babies beginning with the first stool after birth and at regular intervals throughout the first year of life 1 . What they found mirrored some of the discoveries in the transplanted intestines: every baby's microbiota is unique but dynamic, with different populations of bacterial species shifting in abundance. And as in the transplant study, the babies showed a succession of colonization, with facultative bacteria settling in first, followed by a more complex and diverse population.  \n                Nice and dirty \n              Whatever parallels may emerge from these studies, there is one obvious difference: the transplanted gut has already been soiled by the faeces, microbes and immune cells it hosted before. And, until recently, doctors did all they could to scour away the muck. Paradoxically, the surgeons at Georgetown began to notice that the more antibiotics they used to keep the microbes to a minimum, the more intestinal infections they saw after the transplant. At first, the alternative seemed too fantastic to contemplate: taking an essentially infected organ and placing it in a body crippled by immunosuppressant drugs. But about a year ago, Georgetown and other centres began shifting their practice to do exactly that. Early evidence indicates that those who receive a gut replete with its native microbiota have fewer chaotic fluctuations. That makes sense in retrospect, notes Eisen, because people who need transplants may do so precisely because they had trouble colonizing their bowels properly to begin with. In some patients \u2014 those with Crohn's disease or ulcerative colitis, for example \u2014 the native bacteria may turn on the body, inflaming the gut and scarring the intestinal walls. So the new intestine might be more likely to help if it comes with its own set of inhabitants. \"To take out all the microbes seems completely inane,\" Eisen says. The Georgetown researchers have now started to investigate how those bacterial fluctuations during colonization are controlled. They suspect that a gene called  NOD2,  which is expressed in some immune cells, is essential for keeping the chaos to a minimum. The NOD2 protein recognizes components of bacterial cell walls and controls the production of defensins, small proteins that kill particular bacterial species. The team has found that about 35% of their transplant patients carry mutations in  NOD2,  regardless of the intestinal disorder, and that those with mutations are 100-fold more likely to have a failed transplant compared with controls 2 . The hypothesis is that this mutation somehow lowers production of defensins, so the immune system is unable to maintain the appropriate proportions of bacterial species. Perhaps patients with a mutant  NOD2  gene might benefit from doses of the bacteria that they are missing. In one patient with a  NOD2  mutation who later died, \"the proportions of normalness were very, very off\", Hartman says. Quite what 'off' is, is hard to define. The team still has only a cursory understanding of what the microbiota looks like in healthy people, compared with their subjects. \"It's like watching colonization of a disturbed ecosystem without knowing what was originally in the forest,\" says Eisen. That may be helped by the National Institutes of Health's Human Microbiome Project and other new research efforts that aim to catalogue the microbes in the human body (see  page 578 ). \"We need that field guide to microbes to understand when something is not normal,\" Eisen says. In the meantime, clinical signs are still the best predictors of a transplant's success. Two years after a new gut was slotted into Stephanie's body, she still has scars criss-crossing her abdomen but she is a healthy weight and eats whatever she wants. It is still not clear which microbes to thank: the donor's bugs that survived from before the transplantation, or new microbes that settled there afterwards. \"Whoever's microbes have prevailed, they're probably good ones,\" says Fishbein, \"because she's done exceptionally well.\" Apoorva Mandavilli is a freelance writer based in New York. See Editorial,  page 563,  and News Feature,  page 578. \n                     Microbiology subject page Microbiology \n                   \n                     Georgetown University Transplant Institute \n                   \n                     Jonathan Eisen \n                   \n                     Relman Lab \n                   Reprints and Permissions"},
{"file_id": "453714a", "url": "https://www.nature.com/articles/453714a", "year": 2008, "authors": [{"name": "Mark Buchanan"}], "parsed_as_year": "2006_or_before", "body": "Animal behaviour is an endless challenge to mathematical modellers. In this, the first of two features, Mark Buchanan looks at how a mathematical principle from physics might be able to explain patterns of movement. In  the second,  Arran Frood asks what current models can teach us about ecological networks half a billion years old. Food is not, in general, spread equally around the world; it comes in lumps. Foragers thus need a strategy for finding those lumps. One appealing option is a L\u00e9vy flight \u2014 a mathematical concept used in physics. L\u00e9vy flights are many-legged journeys in which most of the legs are short, but a few are much longer. They are found in some sorts of diffusion, in fluid turbulence, even in astrophysics. In animal behaviour, the longer the flight, the farther afield a creature will get, offering a way to efficiently exploit food nearby but also to discover sources farther away. \"The pattern captures what biologists often notice,\" says behavioural ecologist David Sims of the Marine Biological Association Laboratory in Plymouth, UK. \"Animals often take lots of short steps in a localized area before making long jumps to new areas.\" But just because it makes qualitative sense doesn't mean it is a mathematical key to the real world. Hard evidence is needed to show that the pattern is a real L\u00e9vy flight, in which the frequency of steps of given distances is firmly constrained. And this evidence is what physicist Gandhimohan Viswanathan, then a graduate student at Boston University in Massachusetts, and his colleagues seemed to find in 1996. Albatrosses soar over tremendous distances as they circle the oceans, alighting here or there to feed on squid, fish or krill before heading off again. Observers had thought the foraging was random; but any hidden pattern would be evident only on the scale of seas and oceans. It was this large pattern that Viswanathan, now at the Federal University of Alagoas in Brazil, decided to look for, using electronic logging data gathered by field ecologists at the British Antarctic Survey (BAS) in Cambridge. Viswanathan and his colleagues found a scale-free fractal-like pattern in the data 1 , just what a L\u00e9vy flight ought to produce. Three years later, they seemed to be on the track of a new principle of ecology when they showed that this way of moving is, under some conditions, theoretically the best way for animals to find scarce prey 2 . They and other researchers soon reported the same pattern in the movements of everything from reindeer and bumblebees to soil amoebas and the habits of fishermen 3 . The phenomenon is attracting more and more interest, and it seems to apply to more than just foraging. Research in this week's  Nature   shows that it applies to the movements of mobile-phone users 4  too (see  page 779 ).  \n                Flights of fancy? \n              There's just one problem. Although other examples stand up to scrutiny, the one that started the field off does not, at least for now. There's a lesson in that. When modellers use data from the field, they have to be sure that the data really represent what they think they represent, and that they fit tightly to their model. The devil is in the detail, when sparse data can put almost all conclusions on shaky ground. The case for L\u00e9vy flights by albatrosses ran into problems in 2004, when physicist Sergey Buldyrev, also of Boston University and one of Viswanathan's co-authors on the original albatross paper, analysed new data on albatross movements. The L\u00e9vy pattern didn't turn up. Revisiting the original data collected by the BAS researchers, Buldyrev discovered that the longest flights recorded, which were crucial to the distinctive fractal fingerprint, might have been artefacts of the recording technique. The original albatross data came from devices called immersion loggers attached to the birds' legs. The devices recorded the proportion of time in each quarter-minute that the birds sat on the sea surface. From these data, the researchers could then infer flights as periods during which the birds remained dry. From five birds, the researchers had obtained a total of 363 flight times, which seemed to show the L\u00e9vy pattern. But Buldyrev wondered whether the longest periods of dry-leggedness \u2014 which always seemed to be the first and last in a bout of movement \u2014 might in fact record the birds sitting on their nests. The data had not been saying what the team thought they were saying. Finding that the L\u00e9vy pattern vanished when these data points were omitted, Buldyrev and his colleagues wrote up a manuscript and sent a draft to ecologist Richard Phillips at the BAS. Phillips, working with ecological modeller Andrew Edwards, also at the BAS, confirmed that there was no support for L\u00e9vy flights. Later, when they discovered that some of the albatrosses also had location trackers fitted to them, the BAS team proved that the birds weren't moving during the alleged long flights. \"I was disappointed,\" says Viswanathan, \"but also curious, surprised and perplexed.\" The L\u00e9vy flight notion took another blow last October, when the Boston and Cambridge groups collaborated to publish a comprehensive reanalysis of the original albatross data, including an analysis of a new data set and a reconsideration of earlier studies of deer and bumblebees 5 . They found that the deer and bumblebee data were also ambiguous \u2014 the deer data, for example, actually reflected time spent cropping and processing food at a particular feeding site, rather than time spent moving between sites. Using improved statistical techniques, the teams found that none of the data offered strong support for the L\u00e9vy flight pattern. The results, they say, \"question the strength of the empirical evidence for biological L\u00e9vy flights.\" It looked like a simple tale of problematic data corrected. But later last year, Sims and his colleagues presented strong evidence for L\u00e9vy-like patterns in the foraging of numerous marine predators, including sharks, turtles and penguins 6 . They used what all researchers agree are more sophisticated statistical methods, and much larger data sets. Sims and others now suggest that the data really do point to L\u00e9vy flights for a variety of animals, including humans. Not everyone yet agrees with this position. But they do agree that the episode illustrates the difficulties inherent in identifying statistical patterns with limited data. The difference between a L\u00e9vy flight and a more familiar form of random walk, brownian motion, is the distribution of steps of different lengths. In brownian motion, as seen in the jittering of a pollen grain buffeted on all sides by invisible molecules, the distribution of distances follows a bell-shaped curve, so the size of the next step is at least crudely predictable \u2014 it is never 10 or 100 times bigger than the average, for example.  \n                Doing the L\u00e9vy walk \n              A L\u00e9vy flight is a similar sort of random walk \u2014 but the distribution of distances is different. For example, the probability of large steps of size D might fall off in proportion to d \u03b3 , with \u03b3 being a number somewhere between 1 and 3. This distribution, in what is known as a power law, gives more frequent long steps than a bell curve, and produces a pattern characterized by lots of smaller movements broken episodically by long excursions. Diagnosing a true L\u00e9vy flight means showing that the power-law distribution holds. There is a simple statistical approach to this. First 'bin the data': that is, count up the events that fall within each small range of distances to get a measure of the way the probability of differing distances is distributed. If a power law holds, the relationship between the logarithm of this probability distribution and the logarithm of the distance will be linear. Hence, if the log of the first is charted against the log of the second, you'll get a straight line. As Edwards points out, however, this technique can lead to trouble. \"It's well known that log\u2013log axes tend to make relationships look straight.\" The problem is at its worst when data are in short supply. A more rigorous approach, he says, is to decide mathematically which of two possible distributions, say a power law or an exponential, the data fit better. But such determinations need a lot of data. Sims agrees. Inspired by the original albatross paper, he and his colleagues used satellite-linked tags to gather data on plankton-feeding basking sharks. They found horizontal tracks reminiscent of L\u00e9vy-like movements, but never obtained enough data to permit a sound statistical analysis. \"A lack of data,\" he says, \"means you can fail to detect the pattern even if it's there, or detect an apparently similar pattern even if it is not.\" Two years ago, Sims hit on the idea to look at sharks' vertical movements instead. These were recorded at 1-minute intervals for months on end, providing more than 400,000 data points for analysis. Using statistical methods developed in part by Mark Newman of the University of Michigan, Ann Arbor, and similar to those used by Edwards and his team, they found a strong signal of L\u00e9vy behaviour. Sims then organized a collaboration of 18 researchers from four countries to gather and test similar data for other marine predators, finding the L\u00e9vy pattern for tuna, cod, leatherback turtles and penguins 6 . Sims says that his paper \"represents some of the strongest evidence for L\u00e9vy-like behaviour in wild predators\". \"The debate has shifted,\" says Frederic Bartumeus of Princeton University in New Jersey, who in 2003 found L\u00e9vy patterns in the movements of plankton. \"The question now isn't whether animals perform L\u00e9vy walks, but when they do \u2014 and why.\" Although welcoming the use of larger data sets, Edwards, now at the Pacific Biological Station in Nanaimo, British Columbia, Canada, doesn't think that these studies end the debate. He says that some scientists have started to use the somewhat softer phrase 'L\u00e9vy-like' to describe their results, which may make their claims more defensible, but also introduces some vagueness into the discussion. \"How 'non-L\u00e9vy-like' do the data have to be for them not to be considered 'L\u00e9vy-like' any more?\" he says. The matter is not mere pedantry: getting the pattern right should help researchers to answer meaningful biological questions \u2014 which organisms, if any, forage optimally, and why. Yet for Sims, the qualifier 'like' is not without its uses. It could be useful in probing the complex, interacting factors that affect movement patterns. \"Animals often undertake other behaviours interspersed with searching, such as social interactions or predator avoidance,\" he says, which may weaken the L\u00e9vy signal.  \n                Man in the mirror \n              However the debate plays out, analyses of data from one particular animal, humans, are likely to be increasingly important. Over the past decade, technology has transformed researchers' ability to gather quantitative data on human activities, ranging from patterns of e-mail use to consumers' buying habits. People happily carry radio trackers and tags around in the form of mobile phones. \"We finally have objective measurements of what people do,\" says Albert-L\u00e1szl\u00f3 Barab\u00e1si, a researcher studying human dynamics in this way at the Center for Complex Network Research, based at Northeastern University in Boston. \"Our observations don't influence them.\" This work can be viewed, perhaps, as the beginnings of a natural ecology of human behaviour, for which understanding patterns of physical movement \u2014 the crude equivalent of animal foraging \u2014 would offer an obvious first goal. Two years ago, physicist Dirk Brockmann of the Max Planck Institute in G\u00f6ttingen, Germany, took an indirect stab at the issue using the website  http://www.wheresgeorge.com , which facilitates the tracking of dollar notes moving through the United States. People can go to the site and enter the date, their location and the serial numbers of dollar bills in their possession. As the bills move, the site shows their changing locations. Almost 60% of bills starting in New York City were reported 2 weeks later still within 10 kilometres of their starting point. But another 7% had jumped to distances beyond 800 kilometres. If this seems similar to the L\u00e9vy pattern, it is. The researchers found that the distribution of distances travelled over a short time follows a power law with a \u03b3 equal to about 1.6 (ref.  7 ). These data don't directly say anything about the human movements that transport dollar bills. But a team led by Barab\u00e1si has now gone one step further, using anonymized mobile-phone data to track the movements of more than 100,000 people over a 6-month period. The statistics, they found, again show the L\u00e9vy pattern, although with some additional complexity 4 . The team found, overall, that the distribution of the distance moved between two subsequent phone calls follows a power law with an exponential cut-off. The best way to explain this pattern, the researchers argue, is through a combination of two effects \u2014 first, a real tendency for individuals to move in a L\u00e9vy-like pattern, with many short movements and less frequent long excursions, but also a difference between people in the overall scale on which they move, with some people being inherently longer travellers than others. When the researchers normalized the measurements so that the person-to-person scale factor no longer played a part, the data for all the participants fell onto a single curve. \"There are a lot of details that make us different,\" says Barab\u00e1si, \"but behind it all there's a universal pattern.\" And what of the albatrosses? Are they an oddity \u2014 an error that nevertheless served as the basis for insights into truth? Perhaps. \"I think of it like the Bohr model of the atom,\" says Eugene Stanley, a physicist from Boston University who was one of the original authors. \"It was wrong, yet it turned out to be fruitful. The remarkable fact is that flawed data led to a fascinating idea: a general law governing animal movement.\" Or perhaps albatrosses do roam the high seas in the way that L\u00e9vy might have anticipated, and we will know that in time with better data and analyses. As Viswanathan points out, he and his colleagues' 1999 paper showing the theoretical optimality of L\u00e9vy-style foraging provides a good  a priori   reason to expect that some animals, and quite possibly albatrosses, might exploit this trick. \"Given the power of natural selection,\" says Viswanathan, \"it seems unlikely to me that L\u00e9vy walks wouldn't exist somewhere in animal biology. It would be as strange as if vision had never evolved.\" \n               Mark Buchanan is author of  \n               The Social Atom.  \n             See Editorial, page  698  , and News Feature,  page 717  . \n                     British Antarctic Survey \n                   \n                     Dave Sims \n                   \n                     Center for Complex Network Research \n                   \n                     Mark Buchanan \n                   Reprints and Permissions"},
{"file_id": "4531160a", "url": "https://www.nature.com/articles/4531160a", "year": 2008, "authors": [{"name": "Eric Hand"}], "parsed_as_year": "2006_or_before", "body": "A giant crater on the lunar farside holds the key to a catastrophic bombardment that reshaped the Moon, Earth and other planets. Eric Hand reports. Dhofar 961 wasn't like the other Moon rocks. Looking at its freshly cut face, geochemist Randy Korotev noticed immediately how dark it was \u2014 almost purple \u2014 and that it contained big metallic grains. It was so different from anything he'd seen before that he began to wonder. Was it from the 'big one'? Korotev, of Washington University in St Louis, Missouri, already knew that Dhofar 961 was a piece of the Moon, chipped off by some anonymous impact so that it escaped the Moon's feeble gravitational grasp and succumbed to Earth's. Tens of thousands of years ago, Dhofar 961 fell into the Oman desert. A few years ago, it fell into the hands of collectors eager to make a buck. After fruitless searches on eBay, Korotev found a reputable dealer online selling a 6-gram piece of Dhofar 961 for US$1,000 per gram \u2014 30 times more expensive than gold. Korotev bought a sliver, and sacrificed a third of it for a chemical analysis that confirmed his suspicions. Unlike the other 59 known lunar meteorites, which have chemical compositions that trace back to three specific regions on the Moon, Dhofar 961 probably hails from a fourth: a deep, dark hole at the bottom of the lunar backside, known as the South Pole\u2013Aitken basin. It marks the site of the biggest-known blast the Moon has seen. And trapped within Dhofar 961 might be a record of that event, which would make it a clue to whether, and when, the inner Solar System endured a catastrophic pummelling in its youth. Really large impacts such as this one leave not just craters but basins, deep and complex, their shock waves frozen into concentric rings like a bullseye. Even by basin standards, though, South Pole\u2013Aitken is a doozy. Within the Solar System it is second in size only to Mars's 10,000-kilometre-long Borealis basin, which as scientists report in this issue (see  page 1212 ) was made by an impact so large that it seems to have sliced the top off Mars's northern hemisphere. South Pole\u2013Aitken itself is more than 2,600 kilometres across and 12 kilometres deep, big enough to blot out half of China and hide the highest mountains of Tibet. South Pole\u2013Aitken is not only the biggest basin on the Moon, but also the oldest, based on the relative chronology that geologists piece together by mapping the way craters overlap each other. An absolute date for it is, however, unknown. Just after the Solar System formed 4.6 billion years ago, leftover planetesimals regularly blasted the newborn planets. The barrage even knocked off enough of Earth to create the Moon in the first place. By 3.8 billion years ago, impact rates had tailed off to a level not too different from those of today (see graphic, right). The question is what happened in between: did the impacts decrease smoothly, or was there, as many scientists suspect, a big spike 3.9 billion years ago? Given South Pole\u2013Aitken's prominence at the bottom of the cratering heap, its age provides a crucial constraint on this 'late heavy bombardment' or 'lunar cataclysm'. An early date for South Pole\u2013Aitken means a broader peak in the bombardment rate, or possibly a steady rate throughout the period. A later date speaks to cataclysm. This is why the US National Academies last year called dating South Pole\u2013Aitken the most important goal in lunar science. Date the basin, and you test the idea of a cataclysm, with all its ensuing implications. It tells you what was happening on the Moon early on and, by inference, what was going on in the rest of the Solar System. It tells you whether the inner planets got smacked suddenly in an atmosphere-annihilating blast of impacts. And that has implications for the origins of life. Was the great bombardment so severe that it sterilized any life that had got started before then? Did it create the hellish conditions that many of the earliest life-forms seem to have endured? Could it even have moved life from one planet to the next, throwing travellers such as Dhofar 961 from surface to surface, complete with bacterial hitchhikers? \u201cIt's a major unsolved problem,\u201d says Jeffrey Taylor, a geologist at the University of Hawaii. \u201cAnd the Moon is the only place we can address it.\u201d  \n                Dating the impacts \n              Look at the Moon for even a moment, and it's clear that the place has been brutalized. Yet for many years, scientists thought that its cratered surface resulted from inner turmoil rather than outer. Impact craters were mistakenly identified as volcanic calderas, the remnants of explosive eruptions. \u201cYou have no idea how pervasive this idea was \u2014 that volcanics were responsible for everything on the Moon,\u201d says Don Wilhelms, a retired geologist who was the first to map the South Pole\u2013Aitken basin in the early 1970s, when working at the US Geological Survey in Menlo Park, California. With Steve Squyres, Wilhelms was also the first to propose the existence of the Borealis basin on Mars 1 . As the space race took off, rich new Moon maps were produced, and pioneering astrogeologists buried the volcanic theories (see  page 1164 ). The big basins, they deduced, were all caused by impacts. Then, by the early 70s, the Apollo astronauts had brought back another surprise: Moon rocks that showed that the impacts were all roughly the same age. The huge Imbrium basin came in at an age of 3.85 billion years; nearby Nectaris, separated in the relative chronology by hundreds of substantial craters, was just 50 million years younger. Nothing was older than 4 billion years. In 1973, Fouad Tera and his colleagues at the California Institute of Technology in Pasadena first used the term 'cataclysm' to explain the extreme pace of the impacts. \u201cIt must in any event have been quite a show from the Earth, assuming you had a really good bunker to watch from,\u201d they wrote in an abstract to that year's Lunar and Planetary Science Conference. It wasn't just a 'show from the Earth', though; it was the greatest show the Earth itself has ever experienced \u2014 the sort of show you're lucky to come through intact. The Moon and Earth are so close to each other that whatever happened on the Moon also happened on Earth \u2014 and then some. The record has been lost on Earth because most impact craters are erased through weathering, erosion and the continuous churn of plate tectonics. That makes the Moon \u201ca witness plate for what happened on the Earth,\u201d says David Kring, a geologist at the Lunar and Planetary Institute in Houston, Texas. And what a bad time it was. To model what Earth went through, Kring scaled up what happened to the Moon by a factor of 13 to account for the fact that Earth is a much larger target 2 . Given Wilhelms' estimate of 15 major lunar-impact basins in the 50 million years between Nectaris and Imbrium, this meant to Kring's team that a projectile big enough to form a 20-kilometre crater hit Earth every few thousand years. Every million years, something would come along big enough to make a 1,000-kilometre basin. Such impacts would have vaporized Earth's oceans and steam-sterilized the surface; Kring says an atmosphere of rock vapour could linger for thousands of years after the impact. Here's the crazy part: Kring's estimate is, in fact, very conservative. Earth's strong gravity could attract impactors at a frequency as much as 500 times higher than the Moon would. Moreover, Kring does not include in his calculations the 30 other huge basins that, according to Wilhelms, were formed after South Pole\u2013Aitken and before Nectaris. If South Pole\u2013Aitken turns out not to be significantly older than Nectaris, then the frequency of doomsday rocks hitting Earth rises yet higher. What's more, Wilhelms' basin count \u2014 a baseline for many studies \u2014 is now old, and is conservative itself. Herbert Frey, at the Goddard Space Flight Center in Greenbelt, Maryland, recently finished a new hunt for basins, based on topographical data collected by the Clementine Moon orbiter. At the 2008 Lunar and Planetary Science Conference, Frey's team reported 92 basins bigger than 300 kilometres across \u2014 twice as many as Wilhelms. \u201cWe've grossly underestimated the actual flux of objects that hit the Moon,\u201d Frey says. \u201cIt means that the Earth was probably not a very good place to be 4 billion years ago.\u201d Yet some astrobiologists say that a cataclysm may have catalysed the origin of life rather than snuffed it out. A stream of comets or asteroids hitting the planet would have brought foreign organic material to Earth. The bombardment might have pierced the crust, stirring up deep convective currents in the mantle in such a way as to establish early continental crust. And, although surface oceans might have been stripped away, subsurface water and heat could have nourished heat-loving organisms. It's probably no coincidence that in phylogenetic trees of life, the roots of the three major branches \u2014 bacteria, archaea and eukaryotes \u2014 tend to be heat-loving 3 .  \n                Ultimate causes \n              One thing the lunar rocks make clear is that the bombardment dropped off pretty quickly about 3.85 billion years ago. But what was doing the pummelling in the first place? Some have claimed it was simply the expected collisions of things left over from the formation of the Solar System, but there's no obvious way there would have been enough projectiles at the beginning to last as long as would be needed for that. Others say the cataclysm was a fresh spike of new bombardments, but what would have caused such an influx of new impactors? \u201cSome people didn't like it because they couldn't think of a mechanism,\u201d says Wilhelms. A few years ago, one possible explanation surfaced from researchers based in Brazil, the United States and in Nice, France. The team developed a dynamical model for the Solar System that explained why Uranus and Neptune circle the Sun farther out and more eccentrically than expected 4 . In their model (sometimes called the Nice model), they started the infant Solar System with Neptune's orbit inside that of Uranus, and let the clock run. Some 700 million years later, Jupiter and Saturn fell into an orbital pattern, and the resulting gravitational pull caused Uranus and Neptune to be kicked farther away from the Sun. That in turn disrupted a massive disk of icy comets in the Kuiper belt beyond Pluto, and sent them hurtling into the inner Solar System. There's one problem with all this. The chemical composition of material within lunar craters, as well as their size distribution, matches nicely with asteroids, not comets \u2014 suggesting that asteroids were the main, or most recent, impactors during the bombardment. The Nice modellers have an answer for that: the changes in Jupiter's and Saturn's orbits may have also disrupted the asteroid belt between Mars and Jupiter. That could have been enough to send asteroids smashing into Earth, the Moon and more. Korotev says he is now a believer in the lunar cataclysm, thanks in part to the Nice model. Other work is resolving his other long-standing problem with the cataclysm hypothesis: his belief that the dating of most Moon rocks to around 3.9 billion years ago is the result of an artificial selection bias. He and some other researchers have argued that the astronauts might have simply kept picking up rocks from the Imbrium impact over and over again, and that scientists interpreted them as being from different impacts. Marc Norman, a cosmochemist at the Australian National University in Canberra, continues to wring new dates from the Apollo collection that may counter this challenge. Norman has been looking at impact melts, the recrystallized remains that contain an isotopic record of a rock being melted by an impact. In one Apollo rock, Norman dated 21 impact melts to within a 200-million-year window 5 . And he found that the melts fell into a number of age clusters, which he interprets as representing four different impact events. If his interpretation is correct, that would be more evidence that multiple big basins were formed within the narrow time period of the bombardment. Other evidence is coming from meteorites, which unlike the geographically constrained Apollo rock collection are thought to have been hacked from all over the Moon. Working with Kring, Barbara Cohen, now at the Marshall Space Flight Center in Huntsville, Alabama, analysed four meteorites containing impact melts representing seven to nine impact events. None of the melts, she found, was older than 3.92 billion years 6 . Even the notorious martian meteorite Allan Hills 84001 \u2014 famous a decade ago for claims that it contained evidence of life \u2014 offers support for the lunar cataclysm theory: parts of the 4.5-billion-year-old rock were altered in some sort of major event 3.9 billion years ago.  \n                Reaching for the Moon \n              Still, some lunar scientists are not satisfied with Moon rocks fetched by astronauts or fallen from the sky. The best way to date South Pole\u2013Aitken, they say, is to go there, get a rock, and date it. Sending a simple robotic lander would be relatively cheap, but the robot's dating capabilities would not be good enough. Radioisotope dating requires a mass spectrometer, and one small enough to fly on a lander would have uncertainties of 10%. On a 4-billion-year-old rock, that's 400 million years \u2014 exactly the sort of error that a mission travelling to date South Pole\u2013Aitken is supposed to dispose of, not create. \u201cYou want to be able to say that your 4.2-billion-year-old age is different from a 4.0,\u201d explains Taylor. \u201cIt requires more accuracy than we have at present.\u201d And so a group of lunar scientists is pushing for a South Pole\u2013Aitken sample return mission, which would be the first lunar sample return since the last Soviet Luna spacecraft returned 170 grams of soil in 1976. The group, led by Brad Jolliff of Washington University in St Louis, plan to propose a mission for the next NASA New Frontiers competition, a mission class capped at $650 million. NASA intends to start accepting proposals in December, with eventual selection in 2010 and a launch date no earlier than 2015. In the most recent New Frontiers contest in 2005, a South Pole\u2013Aitken mission called Moonrise made it to the final round but was ultimately bested by Juno, which is set to launch towards Jupiter in 2011. At the time, the risk of doing a sample return mission, with its many stages and components, was considered riskier than a simple orbiter such as Juno, says Jolliff, who was the deputy principal investigator on that proposal. The old Moonrise project proposed two separate landers: one to go near the basin rim and the other, the centre, where impact melts are apt to be concentrated. Jolliff is leaving open the option to send just one lander to the basin centre. By 2020, if NASA's plans to return people to the Moon are realized, astronauts could already be encamped at the nearby Shackleton crater, placing them near the rim of the South Pole\u2013Aitken basin. The Moonrise lander would collect rock and soil, and return to Earth with about a kilogram of material, Jolliff says. Samples from a revamped Moonrise mission would allow lunar scientists to date many impact-melt crystals. The oldest, and most frequent, dates should correspond to the South Pole\u2013Aitken impact. But other impact-melt dates would undoubtedly pollute the picture, as there are half a dozen other large basins within South Pole\u2013Aitken. And debate would continue. \u201cLanding in the middle of a field and getting a scoop of dirt is not going to give you the answer you need,\u201d says Norman. In fact, Norman advocates both robotic and manned missions to the Moon, saying both are needed for a balanced exploration programme. \u201cMy feeling about sample return may be a little more nuanced than simply humans versus robots,\u201d he says. \u201cBoth can do the job provided we do the geologic homework, and neither will do an adequate job if we don't.\u201d Others, however, argue that the mystery of the great backside basin won't be solved until a human goes to the source and plucks a rock from within its blast shadow. But returning people to the Moon will cost at least $230 billion over two decades (according to the US Government Accountability Office), compared with the New Frontiers $650-million cut-off. And Korotev thinks he can solve some of the mystery simply by dating the half-gram piece of the Moon he bought for $542 (plus $12 shipping and insurance). Later this year, he will share his precious sliver of South Pole\u2013Aitken with Cohen. In Huntsville, she plans to sink a diamond-tipped drill bit into Dhofar 961 and extract several cores, each as fine as a human hair. After vaporizing the samples with a laser, she will measure argon gas that has been trapped inside the rock crystal lattice for billions of years. Counting those atoms might allow her to count back in time to the blistering crucible of the bombardment. If she's successful, she will extract a date desired by lunar scientists for many moons \u2014 a big message from a little bottle. See Editorial,  page 1143 , and News Feature,  page 1164  . \n                     Planetary Science Research Discoveries \n                   Reprints and Permissions"},
{"file_id": "453578a", "url": "https://www.nature.com/articles/453578a", "year": 2008, "authors": [{"name": "Asher Mullard"}], "parsed_as_year": "2006_or_before", "body": "The human body teems with microbes. In this, the first of two features, Asher Mullard looks at the global efforts to catalogue this vast 'microbiome'.  In the second,  Apoorva Mandavilli meets the surgeons who have a rare opportunity to watch an ecosystem being established as they transplant guts from one person to another. Any story about a human's microbes tends to invoke impressive numbers. Take the 10 trillion or so microbial cells living in the gut, which exceed the number of human cells by 10 to 1. Between them, they harbour millions of genes, compared with the paltry 20,000 estimated in the human genome. To say that you are outnumbered is a massive understatement. But that might not be a bad thing. There is strength in numbers; so much so, in fact, that some biologists regard a human as a 'superorganism' \u2014 a community that adds up to more than the sum of its parts. The body itself is merely one, albeit encompassing, component. \n               boxed-text \n             Some smaller but nonetheless striking numbers about human microbes come in cash amounts. Late last year, the US National Institutes of Health (NIH) pledged US$115 million to identify and characterize the human microbiome, the name given to the collection of microorganisms living in and on the human body. Also last year, the European Commission and various research institutes committed \u20ac20 million (US$31 million) to similar ends. And smaller sums are being thrown in by funding agencies in countries that include China, Canada, Japan, Singapore and Australia (see map). Given the multifaceted nature of the microbiome, perhaps it is only right that it is studied by a large and varied community. The NIH's five-year Human Microbiome Project will spend much of its money identifying which bacteria are lodged where in the body and compiling a reference set of their genetic sequences. By contrast, the European Commission's four-year initiative, called Metagenomics of the Human Intestinal Tract (MetaHIT), will focus on microbial inhabitants of the gut, the main repository of the microbiota, and how they contribute to obesity and inflammatory bowel disease. Researchers involved in these and other initiatives say they will team up within a larger international consortium, but hints of competition simmer beneath the surface. \u201cThe intention is to work together,\u201d says George Weinstock, a geneticist at Washington University in St Louis, who is helping to organize the Human Microbiome Project, \u201cbut for the moment it is more about working in parallel until we can understand how to work together\u201d. The microbes that swarm in and on the human body have always held a certain fascination for researchers. Studies over the past century have shown that mice raised in a germ-free bubble have weak immune systems, inefficient digestive systems and abnormally small internal organs. They have shown that microbes are also an essential part of human biology. For the germ-ridden human masses who don't live in plastic bubbles, however, it has been difficult to work out exactly who these microbial passengers are and how they interact with one another. This is partly because there are so many of them, and partly because so few are easily grown in the lab.  \n                A numbers game \n              Things have changed in the past few years. A few million foreign genes no longer sound so daunting in the face of advanced genetic-sequencing methods that have staggering numerical endowments of their own. Using the newest technologies, researchers can process hundreds of millions of base pairs in just a few hours. So they can bypass the need to grow bacteria, exploring the human microbiome by studying genes en masse, rather than studying the organisms themselves. In 2006, Steven Gill at the Institute for Genomic Research in Rockville, Maryland, and his colleagues, threw around some then-hefty numbers when they carried out such a metagenomic analysis of the microbes in two people's intestines. After 2,062 polymerase chain reactions and 78 million base pairs, the researchers still had only a sketch of the gut's genetics, but they had revealed an abundance of genes that are involved in producing amino acids and in other aspects of human metabolism 1 . Although interest in large-scale studies of the human microbiome has been mounting over the past few years, it was the realization that sequencing technology really was up to the task that finally prompted the money to roll in, Weinstock says. Preliminary surveys such as Gill's have indicated that genes, age, diet, lifestyle and geography all affect which bacteria live in a person's body. But these first surveys involved too few individuals and sampled too few microbes, usually from only the gut or the mouth, to provide an adequate description of the microbiome. How many bacterial species colonize the entire body remains anyone's guess. So does the question of which ones everyone shares. \u201cOne of the things that is obsessing microbiologists is: 'What is the size of the core microbiome',\u201d says Jeremy Nicholson, a biological chemist who studies microbes and metabolism at Imperial College London.  \n                Bringing order to chaos \n              The Human Microbiome Project is just the project for such obsessed microbiologists. In this, its first year, researchers will collect samples of faeces plus swabs from the vagina, mouth, nose and skin from 250 volunteers. They will sequence short, variable stretches of DNA that code for components of ribosomes in order to roughly identify which bacteria are present in each person and how many the volunteers have in common. With an estimate of diversity in hand, the researchers then plan to mine deeper. For this, they will also use shotgun sequencing to analyse many short pieces of DNA from all over the microbes' genomes and reveal which genes are present. Like any biology project that involves large sums and grand aims, the Human Microbiome Project has brought out some critics. One challenge, they say, is that the core microbiome might be incredibly small. Even though two people may each have 1,000 types of bacteria living in their guts, they might have only 10 species in common, for instance. And the commonalities might not lie in the genes, but rather in bacteria that have the same metabolic and physiological role. The Human Microbiome Project will do little to assess the function of microbes during its first year, although it may focus on this later. Sarkis Mazmanian, a microbiologist at the California Institute of Technology in Pasadena, voices another reservation. \u201cThere's very little in terms of actual application to disease as part of the initiative. The lion's share of the efforts is in sequencing.\u201d This criticism tends to be aired for all big sequencing efforts and in this case, one-quarter of the money has been earmarked to examine the role of the microbiota in health and disease. Weinstock says that the project's main goal is to build up a research community and to generate a sequence resource, akin to that developed during the Human Genome Project, so that other basic and applied questions can be tackled later on. MetaHIT is a different sort of beast. It will concentrate almost exclusively on the role of the gut microbiota in obesity and inflammatory bowel disease. And whereas the Human Microbiome Project is initially comparing people's microbiota on a species level, MetaHIT aims to find differences in microbial genes and the proteins they express without necessarily worrying about which species they came from. \u201cWe don't care if the name of the bacteria is  Enterobacter  or  Salmonella . We want to know if there is an enzyme producing carbohydrates, an enzyme producing gas or an enzyme degrading proteins,\u201d explains Francisco Guarner, a gastroenterologist at Vall D'Hebron University Hospital in Barcelona, Spain. We want to \u201cexamine associations between bacterial genes and human phenotypes\u201d, says Dusko Ehrlich, coordinator of MetaHIT and head of microbial genetics at INRA, the French agricultural research agency in Jouy-en-Josas.  \n                Gut reactions \n              Jeffrey Gordon, a microbiologist at Washington University School of Medicine in St Louis, has already shown how illuminating these associations can be. Gordon, one of the pioneers in the human microbiome field, showed two years ago that obese and lean individuals have radically different profiles of bacteria in their guts 2 . When the obese volunteers went on a one-year diet and lost up to one-quarter of their bodyweight, their bacterial profiles changed to look more like those of the lean people. The theory, based on studies in mice 3 , is that part of the propensity to gain weight might lie in 'obesity-causing' bacteria in the gut that release more calories from food than those found in lean people. Researchers hope to gain further insight into how this happens by comparing the microbial genes in thin and fat people, and the findings could help to determine whether probiotics or other interventions could be used to shape the microbiome. Some of this work is already under way as part of MetaHIT. In Denmark, a team led by Oluf Pedersen at the Steno Diabetes Centre in Copenhagen is collecting faecal samples from 120 obese volunteers and 60 controls to tease out specific microbial genes that might contribute to obesity. A similar-sized study in Spain, led by Guarner, will compare the microbiotas of patients with inflammatory bowel disease with those of genetically matched controls and examine the effect of drugs. Whatever their differences, those involved in the Human Microbiome Project and MetaHIT sometimes find themselves on common ground \u2014 and that comes down again to those mind-boggling numbers. How do you effectively study such a vast and unknown community? Both groups will be using shotgun sequencing, which generates scraps of sequence from the many different genes and species. To make the most of this approach, a reference sequence is needed against which researchers can compare and identify these scraps, but at present there aren't enough known bacterial gene sequences to match the fragments against. Earlier studies, such as Gill's, produced many sequences that were impossible to assign functions to.  \n                Into the unknown \n              Both the Human Microbiome Project and MetaHIT plan to sequence the complete genomes of hundreds of bacterial species and deposit them in a shared database. They will initially use shotgun sequencing of a few select species that can already be grown, and piece together their whole genomes from overlapping fragments. The Human Microbiome Project plans to provide 600 'reference genomes', MetaHIT will do another 100, and other sequencing efforts by the NIH and elsewhere will make additional contributions. With a broad enough reference database, researchers hope to be able to predict the genetic capabilities of some of those recalcitrant, unculturable species solely on the basis of similarities with known genes. Even with such a reference, \u201cit is pretty hairy from a computational biology analysis point of view\u201d, says Peer Bork, the biochemist who heads MetaHIT's computational centre at the European Molecular Biology Laboratory in Heidelberg, Germany. Even with the immense power of supercomputers to process the sequencing data, it will take some clever analysis to compare the millions of sequence reads that span thousands of species between hundreds of healthy and unhealthy people. It may be even hairier if, as many suspect, subtle genetic patterns are what is important in disease rather than the presence of a single gene or species. In many ways, such bioinformatics brings the need for collaboration into stark relief. When all the projects are running at speed, reams of data will be generated worldwide. But because different groups are using different techniques to collect samples, extract DNA and annotate data, the data sets are difficult to compare, Bork says. Enter the as-yet-unlaunched International Human Microbiome Consortium. Scientists from several international projects, including the Human Microbiome Project and MetaHIT, have been meeting since late 2005 to figure out how to collaborate on a range of issues such as the compatibility of data and which bacteria to sequence for the reference database. The group is already setting up infrastructure and \u201cbeginning to address the tough questions\u201d, says Weinstock. But it is too early to say how well it will grease the wheels of collaboration. Its official launch, scheduled for April, was postponed for six months to allow the NIH and the European Commission to overcome bureaucratic hurdles. Even so, optimism for the collaboration runs high, partly because its members can still pursue their own pet projects. \u201cTalented people are doing what they think is the most important research to do, rather than being forced to do what somebody else has decided would be the best,\u201d says Ehrlich. One of questions being addressed by the consortium is over intellectual property. As with other genomic projects, members of the consortium will be expected to release sequence data into the public domain as soon as they are generated. But this doesn't necessarily preclude disputes over intellectual property if, for instance, a particular bacterial gene proves to be a useful diagnostic marker for a disease. Another unresolved question is whether a laboratory can have one project that abides by the consortium's regulations, and another that doesn't. \u201cThere are grey areas, and I feel that until we have a test case, they will have to be watched very carefully,\u201d says Bhagirath Singh of the Canadian Institute of Health Research, who is helping to develop the Canadian Microbiome Initiative. Participants from microbiome projects around the world say that they plan to sign up to the consortium. But the number of independent projects being launched speaks to at least some underlying competition between those involved, and some of the difficulties the group may face. In addition to differences in focus and scope, MetaHIT's operating budget is only a quarter the size of the Human Microbiome Project's. \u201cThis is giving a huge advantage to the Americans,\u201d Guarner says. \u201cThey are going to be quicker and they have more equipment.\u201d But some members of MetaHIT feel that they actually have an edge because money for their project has already been distributed and data collection is under way, whereas the Human Microbiome Project will not announce many of its funding decisions until later this year. \u201cWe have an advantage already, we have a show on the road,\u201d says Willem de Vos, a microbiologist at Wageningen University in the Netherlands and a member of MetaHIT. Given the number of separate projects, all at such an early stage, it's almost impossible to make out where the starting line lies or who, exactly, is edging ahead. And the intention to pool data means that there may be no clear line separating them. \u201cIf it is an international consortium, it doesn't matter where the data are generated,\u201d Bork adds. \u201cFor example, we can be the pirates here, sitting at the end in Europe, and use American data to make the discoveries.\u201d Plundering may be unwarranted. With trillions of microbes to sift through, most researchers feel that there is more than enough of the microbiome to go around. \u201cThere's so much to learn, so much we don't know and so many adventures,\u201d Gordon says. \u201cThere's enough room for everyone.\u201d \n               Asher Mullard is an assistant editor for  \n               Nature Reviews Microbiology \n                and  \n               Nature Reviews Molecular Cell Biology. \n             See Editorial,  page 563 , and News Feature,  page 581  . See also Correspondence:  Human microbiome: hype or false modesty?  and  Human microbiome: take home lessons on growth promoters? \n                     Microbiology subject page  \n                   \n                     Ocean Genomics \n                   \n                     Nature Reviews Microbiology Metagenomics poster \n                   \n                     Human Microbiome Project website \n                   \n                     MetaHIT website \n                   \n                     Jeffery Gordon Lab website \n                   \n                     Human Oral Microbiome Database \n                   \n                     Human Metagenome Consortium, Japan \n                   Reprints and Permissions"},
{"file_id": "453839a", "url": "https://www.nature.com/articles/453839a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "There is a growing disparity at the heart of biomedicine. In some ways, the field is experiencing a golden age: the quantity of basic research is shooting off the charts and budgets are far higher than they were two decades ago. Yet the impact of this research is growing at a much more modest rate: new cures and therapies are ever more expensive to develop and worryingly thin on the ground. The term 'translational research', hardly heard ten years ago, is now on everyone's lips because it is seen as the solution to this disparity. It is expected to ensure that the bounty of the golden age is 'translated' into benefits in the everyday world of cancer, dementia or heart disease, for example. It thus has a key part to play in improving our lives and also in rationalizing the social contract between researchers and the taxpayers who help fund them in expectation of future cures (see  page 823 ). In this issue we examine a range of topics in translational research. On  page 840 , Declan Butler finds out what proponents of the idea at the US National Institutes of Health expect to achieve, and how they plan to deliver on their promises. And on  page 846 , Helen Pearson visits the Ludwig Institute for Cancer Research, which claims to have been attempting translation for 20 years and so has a trick or two to teach newcomers. The relationship between academia and industry is key to successful translation. University technology-transfer offices can loom large in this relationship. On  page 830 , Meredith Wadman discusses concerns that some of these offices are more of a hindrance than a help. Closer collaboration, in which industry pays universities to solve problems that it defines, might be one solution (see  page 853 ). More radically, a book reviewed on  page 855  argues that the entire drug industry should be restructured, with research and development pried free from marketing. The concept that translational research is a one-way flow from bench to bedside is seen by many as outdated. Clinical data and human trials can inspire insights that flow from bedside to bench, too, as Heidi Ledford reports on  page 843  . The observation that some anti-inflammatory drugs may delay the onset of Alzheimer's disease, for example, has led one group to find out how they block formation of dangerous protein products (see  pages 861  and  925 ), which may help to refine the drugs. And  pages 863  and  921  detail progress towards a transgenic monkey model for Huntington's disease. Riches abound. If these discoveries can be carried to the clinic, biomedicine will be all the richer. \n                 Click here for more on translational research \n               Reprints and Permissions"},
{"file_id": "453970a", "url": "https://www.nature.com/articles/453970a", "year": 2008, "authors": [{"name": "Jane Qiu"}, {"name": "Daniel Cressey"}], "parsed_as_year": "2006_or_before", "body": "Is it really possible to stop rain, invoke lightning from the heavens or otherwise manipulate the weather? Jane Qiu and Daniel Cressey report on the once-scorned notion of weather modification. China wants everything to be under control at the opening of the Beijing Olympic Games on 8 August \u2014 even the weather. The chance of rain that day is 47%, according to the Beijing Meteorological Bureau. The iconic 91,000-seat main stadium, nicknamed the 'bird's nest' because of its interlacing steel beams, has no roof. So Chinese meteorologists will use weather-modification technologies to try to stop rain from spoiling the party. Beijing's plan for the games is the most conspicuous example of the country's massive weather-modification efforts. Most of the time, the focus is not on keeping things dry, but on making it rain in places that desperately need the water. In ancient times, the Chinese believed that dragons controlled the weather, and elaborate rituals were performed to bring about sufficient rainfall and good harvests. Today, they are turning to technology to change the moods of the sky, part of a national obsession to 'tame nature', as championed by Mao Zedong. China has one of the largest programmes for weather modification in the world. It spends between 400 million yuan (US$60 million) and 700 million yuan a year on it, and employs 32,000 people to operate 35 specially equipped planes, 7,000 anti-aircraft cannons and 5,000 rocket launchers. Official figures from the China Meteorological Administration say that the country created 250 billion tonnes of rain between 1999 and 2006, an annual production of more than 30 billion tonnes. This is enough to meet the needs of more than 500 million of its 1.3 billion people, but the country aims to generate 50 billion tonnes a year by 2010. Many researchers, both in and outside China, doubt that sufficient evidence has been accumulated to support this claimed success. \u201cIn fact, China is very much behind in this area,\u201d says Zhang Hong-fa, an atmospheric scientist at the Cold and Arid Regions Environmental and Engineering Research Institute in Lanzhou. \u201cA false sense of achievement would impede genuine progress.\u201d China also faces long-standing scepticism about weather modification in general. Proponents argue that it's possible not only to produce more rain, but also to get rid of fog, prevent hail and even divert hurricanes from making landfall (see  'Forget the weather forecast' ). Critics say that many of these claims are laughable, and that most of the projects under way are based on little more than faith. Even supporters are dubious about what China may be able to pull off. \u201cThe concern for me is that the Chinese are promising they're going to do something that they really don't have optimum assurances they can deliver,\u201d says Bruce Boe, director of meteorology at Weather Modification in Fargo, North Dakota, one of the world's largest weather-modification companies. \u201cThis has been something that has plagued cloud-seeding since its infancy.\u201d Although people have been trying to influence the weather since early tribes danced at harvest time, scientific weather modification began after work done in 1946, at the General Electric Research Laboratory in upstate New York. There, Vincent Schaefer and Irving Langmuir discovered that seeding clouds with carbon dioxide created nuclei around which water could freeze. Bernard Vonnegut (brother of the novelist Kurt) discovered that silver iodide could also be used, a method that has now been adopted across the globe.  \n                Splashing out \n              Today, countries from Australia to Iran practise some form of cloud-seeding \u2014 as do nearly a dozen US states, mainly in the drought-prone western parts of the country. Estimates vary as to how effective the practice is, although it is generally accepted that increases in rainfall of up to 10% can be accomplished in certain types of clouds. California, for instance, claims that its annual spend of around $3 million has generated an additional 370 million to 490 million cubic metres of water a year \u2014 a 4% increase over what would happen without cloud-seeding. Seeding is generally done in one of two ways, depending on the cloud type. In supercooled clouds, which usually reside at high altitudes, water freezes around particles that serve as nuclei. When the ice crystals get too heavy, they fall from the sky and melt as they go, turning into rain or snow. There can be few particles at high altitudes to serve as nuclei, so seeding supercooled clouds \u2014 a process called glaciogenic seeding \u2014 aims to add more nuclei. Silver iodide is the most widely used glaciogenic chemical, although other materials, such as solid carbon dioxide, can also be used. Warmer clouds, which are usually at lower altitude, are targeted through hygroscopic seeding. This approach uses compounds such as sodium, lithium and potassium salts. The idea is to generate larger droplets, either by providing larger nuclei to condense around or encouraging small droplets to come together to form larger drops that can fall. The amount of water vapour and size of the water droplets are crucial for the seeding effects. Many other factors affect how well cloud-seeding works, such as when and how to apply the chemicals in question. Another factor is which clouds to target: taking into account their temperature, thickness and convective patterns, and the way that the winds flow into and out of them. The criteria, says Boe, are very stringent, and \u201cthe majority of clouds fail\u201d. The best clouds to shoot for, many agree, are orographic clouds, which are produced when air is forced upwards over mountain ranges. Such clouds \u201care short-lived, relatively shallow and contain much water\u201d, says Daniel Rosenfeld, an atmospheric scientist at the Hebrew University of Jerusalem. That's why Weather Modification is targeting orographic clouds in Wyoming for one of the largest US seeding projects, the $9-million, five-year Wyoming Weather Modification Pilot Project. The Wyoming plan calls for one aircraft and remote-controlled units on the ground, which seed clouds in the winter in an effort to create more snow and build up the snowpack for use in the summer when it melts. The first, ground-based-only seeding took place in the winter of 2006\u201307; last year, the plane and 25 ground stations were involved. Daniel Breed, a meteorologist at the US National Center for Atmospheric Research in Boulder, Colorado, who is helping evaluate the programme, says \u201cpromising results\u201d suggest that seeding materials are getting into the right clouds and showing up as greater numbers of ice nuclei there. Such work can only help the reputation of weather modification, he says. \u201cThere have been a lot of extravagant claims, and it has been a real disservice to science in general, let alone atmospheric science and weather modification.\u201d  \n                Each to their own \n              Most of the work in China focuses on promoting rain and deterring hail, in an effort to reduce damage to agricultural crops, but some of it deals with breaking up fog or diverting lightning 1 . Most of China's 34 districts have their own weather-modification office, and close to two-thirds of its 2,900 counties have their own cloud-seeding stations. Some Chinese rain-makers have tried to conduct controlled seeding experiments to evaluate how effective the technique is. For example, Shi Li-xin, deputy director of the weather-modification office at the Hebei Meteorological Bureau, and his colleagues have been trying to identify suitable seeding conditions by studying the properties of clouds using a combination of ground and satellite technologies, as well as  in situ  measurements by planes. After assessing factors such as cloud thickness, the content of supercooled water and the suitable seeding layer of the cloud, the team selected three regions \u2014 one operational region of 36,500 square kilometres, and two controls that cover 19,800 and 20,000 square kilometres each. In the early 1990s, the researchers found that rainfall rose by 18% as a result of 21 seeding operations \u2014 but the sample was too small for the results to be statistically significant. In an earlier study, conducted between 1975 and 1986, meteorologists in Fujian province, in southeast China, conducted a randomized seeding experiment with two 14,000-square-kilometre regions. Over the course of 244 experimental days, they found that areas that had been seeded had 20% more rainfall than did those that had been left to their own devices. Still, the results of these cloud-seeding experiments have not been published in peer-reviewed journals, and much scepticism lingers. According to Shi, the national statistics on rain creation come from weather-modification offices at all levels, from federal down to provincial and village offices. And as they are tied to performance appraisals and funding for the offices, there is plenty of incentive to exaggerate. A source close to the China Meteorological Administration, who asked not to be named for fear of political repercussion, says that such scepticism is secretly shared by many Chinese atmospheric scientists and meteorological officials. \u201cYou may think that the solution is straightforward: stop applying cloud-seeding technologies until their effectiveness is proved,\u201d he says. \u201cBut it's not so simple.\u201d \u201cThere is a huge demand from the farmers for those technologies,\u201d says Lu Da-ren, a researcher at the Beijing-based Institute of Atmospheric Physics, part of the Chinese Academy of Sciences. \u201cSo it's not just a scientific issue.\u201d Lu admits that the effectiveness of weather modification needs to be better validated, but maintains that this should not exclude a trial-and-error approach at the same time. \u201cMaybe it's not as effective as one thinks,\u201d he says. But \u201cfrom the farmers' point of view, it's better than nothing\u201d. For the Olympics, the Beijing Meteorological Bureau aims to change how and when the rain falls. Starting in 2002, the year after Beijing was chosen to host the games, the bureau has used radar, satellites and weather balloons to scrutinize properties such as structure, temperature and size of droplets of clouds passing over Beijing, as well as over the adjacent Tianjin municipality and Hebei province. Chinese rain-makers plan to get the clouds to rain out before reaching Beijing, or to prevent small clouds from getting bigger by dissipating them with salt. Failing that, they will over-seed the clouds hoping to reduce the size of each water droplet or ice crystal, which is then more likely to be dissipated before reaching the ground. The meteorological bureaus have amassed cannons, rocket launchers and planes in more than 100 locations in Beijing, Tianjin and Hebei. In recent months, the bureaus have intensified their field testing. The Beijing Meteorological Bureau declined to provide details of the results, but maintains that keeping the opening ceremonies dry will depend largely on the cloud properties and the accuracy of the weather forecast. \u201cWe can achieve reasonably good results with local, weak weather patterns, but are unable to bring about complete elimination of rain in face of large, thick clouds covering a large region,\u201d the bureau said in a statement.  \n                Sketchy evidence \n              Part of the problem is that so little is known about the mechanisms through which cloud-seeding might work. An influential 2003 report 2  from the US National Research Council (NRC) declared that \u201cthere still is no convincing scientific proof of the efficacy of intentional weather-modification efforts\u201d. A riposte from the Weather Modification Association accused the members of the panel of lacking experience or knowledge of weather modification \u2014 and said they had held cloud seeding to a standard of scientific proof \u201cthat few atmospheric problems could satisfy\u201d. For one thing, clouds can be very different at distinct locations; they also vary over time at a particular place. And lack of funding in some countries hasn't helped; in the United States, for example, federal funding for weather modification peaked at around $20 million a year in the late 1970s, but is now negligible. States have been left on their own to fund individual projects. \u201cWe have been in the dark ages in terms of scientific research in weather modification for going on 25 years,\u201d says William Cotton, an atmospheric physicist at Colorado State University in Fort Collins. \u201cAnd so when somebody wants to have something really quantitative, we can't deliver.\u201d \u201cIt is the same problem as weather forecasting,\u201d adds Deon Terblanche, divisional manager of research for the South African Weather Service in Pretoria and chairman of the World Meteorological Organization's Expert Team on Weather Modification. \u201cIt's very difficult to forecast even 30 minutes ahead what a specific cloud will do. If you do something to that cloud and you can't even forecast what it will do exactly in nature, it becomes difficult to prove what your effect was.\u201d He does, however, think that some of the weather-modification technologies currently in use show a big enough effect to be statistically significant. Michael Manton, of Monash University in Victoria, Australia, cites three reasons for the difficulties in proving the impact of cloud seeding: a mismatch between the scale of the impact and the scale at which seeding acts; the natural variability of rainfall versus the incremental impact of seeding; and the expense of evaluation. \u201cI believe we have not moved significantly forward since [the NRC report in] 2003,\u201d he says. \u201cIn many parts of the world there has been an implicit or explicit faith in cloud seeding, so seeding has not been carried out under carefully controlled conditions.\u201d Moreover, there is growing realization of the importance of air-pollution aerosols on cloud formation and precipitation. Some researchers think that aerosols can reduce rainfall by decreasing the size of water droplets in warm clouds; there are also indications that aerosols could affect precipitation in cold clouds and the dynamics between the two cloud systems, says Zhanqing Li, of the University of Maryland in College Park. \u201cThis may be a missing piece of the jigsaw in the puzzle of cloud-seeding research. Under inappropriate seeding conditions, we may get the opposite effect to what is intended.\u201d The problem with aerosols underlies a new debate on whether Israel's randomized seeding experiments \u2014 once seen as some of the most convincing evidence for the effects of cloud-seeding \u2014 were as effective as once thought. Two long-running experiments, each of which was carried out over 6 years in the 1960s and 1970s, suggested that seeding increased rainfall by 12\u201315%. But some have questioned those conclusions. Most recently, an ongoing study led by Zev Levin of Tel Aviv University took aerosols into consideration and showed that seeding might have zero or even negative effects in regions with high levels of pollution. \u201cThis is just the beginning of the debate, but strikes a note of caution on large-scale cloud-seeding operations in the presence of heavy pollution,\u201d says Li. This is, of course, a particular concern for China, with its heavy aerosol burden. Israel's water commission has just launched another major cloud-seeding experiment, with a budget of US$1 million a year for several years. Researchers from the Hebrew University of Jerusalem and Tel Aviv University will build on what has been learned from previous experiments and incorporate cutting-edge technologies to measure cloud properties and to trace seeding materials and assess their role in rain formation. The ultimate goal is to see whether technologies can replenish water reserves in Lake Tiberius (also known as the Sea of Galilee) in northern Israel. A report, commissioned by the World Meteorological Organization and the International Union of Geodesy and Geophysics, calls for a better understanding of the role of aerosols in precipitation and climate systems. \u201cAerosols are involved in a long chain of reactions and complicated feedback mechanisms leading to precipitation,\u201d says Levin, chief editor of the report, which will be released within the next month. \u201cCloud seeding adds to that complexity we know so little of.\u201d In addition, the report stresses the serious limitation of using statistical tools in cloud seeding without a proper understanding of the underlying physical processes. Rosenfeld maintains that the Chinese will be able to make genuine progress only by combining approaches such as measurements of cloud properties, numerical modelling, as well as randomized and targeted seeding experiments. \u201cWithout these cloud-seeding technologies, the Chinese are just shooting in the dark,\u201d he says. But long-term, randomized or targeted seeding experiments over a large region would mean forgoing opportunities to seed suitable clouds, and to some Chinese meteorologists, this is unthinkable given the country's water crisis. \u201cIf suitable clouds are there, we have to conduct cloud-seeding operations,\u201d says Shi. \u201cThe farmers would be furious if we didn't.\u201d The stakes are so high that in some instances, farmers have accused those in neighbouring villages of \u201cstealing\u201d their rain by seeding passing clouds. Many researchers are concerned that China's weather-modification scheme has been heavily tilted towards operations, and that researchers and operations managers need to collaborate much more than they have until now. The newly established Centre for Weather Modification at the Chinese Academy of Meteorological Sciences may help start to address some of these issues, says its director, Guo Xueliang. The centre, set up last December, will eventually house some 60 researchers. They will use numerical models, laboratory simulation and field experiments to analyse cloud properties and precipitation principles. \u201cWe recognize the importance of basic research in guiding cloud-seeding operations, and will apply scientific rigour in testing their effectiveness in controlled, cross-province field experiments,\u201d Guo says. And China's lavish programme is likely to receive even more money; the country has listed weather modification as one of the key projects in its eleventh five-year plan, and says it will introduce the latest equipment for both research and operations. Several large-scale national projects are also in place to aid collaboration between academic institutes and meteorological offices, such as a five-year project funded by the science ministry. China has found plenty of reasons to move forwards with its work. As the momentum of the Olympics gathers pace, the Beijing Meteorological Bureau is under increasing pressure to give its best performance yet. The promised extravaganza of the opening ceremony will not be the only focal point as the opening ceremonies begin; meteorologists around the world are also eager to hear what Beijing will say about its rain-suppression operations \u2014 especially if the weather turns out to be dry. See Editorial, page  957  . \n                     National Research Council on weather modification \n                   \n                     China Meteorological Administration \n                   \n                     Weather Modification Inc \n                   Reprints and Permissions"},
{"file_id": "453840a", "url": "https://www.nature.com/articles/453840a", "year": 2008, "authors": [{"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "A chasm has opened up between biomedical researchers and the patients who need their discoveries. Declan Butler asks how the ground shifted and whether the US National Institutes of Health can bridge the gap. \u201cNIH stands for the National Institutes of Health, not the National Institutes of Biomedical Research, or the National Institutes of Basic Biomedical Research.\u201d This jab, by molecular biologist Alan Schechter at the NIH, is a pointed one. The organization was formally established in the United States more than half a century ago to serve the nation's public health, and its mission now is to pursue fundamental knowledge and apply it \u201cto reduce the burdens of illness and disability\u201d. So when employees at the agency have to check their name tag, some soul searching must be taking place. There is no question that the NIH excels in basic research. What researchers such as Schechter are asking is whether it has neglected the mandate to apply that knowledge. Outside the agency too there is a growing perception that the enormous resources being put into biomedical research, and the huge strides made in understanding disease mechanisms, are not resulting in commensurate gains in new treatments, diagnostics and prevention. \u201cWe are not seeing the breakthrough therapies that people can rightly expect,\u201d says Schechter, head of molecular biology and genetics at the National Institute of Diabetes and Digestive and Kidney Diseases in Bethesda, Maryland. Medical-research agencies worldwide are experiencing a similar awakening. Over the past 30 or so years, the ecosystems of basic and clinical research have diverged. The pharmaceutical industry, which for many years was expected to carry discoveries across the divide, is now hard pushed to do so. The abyss left behind is sometimes labelled the 'valley of death' \u2014 and neither basic researchers, busy with discoveries, nor physicians, busy with patients, are keen to venture there. \u201cThe clinical and basic scientists don't really communicate,\u201d says Barbara Alving, director of the NIH's National Center for Research Resources in Bethesda. Alving is a key part in the NIH's attempt to bridge the gap with 'translational research'. Director Elias Zerhouni made this bridge-building a focus in his signature 'roadmap' for the agency, announced in 2003 (see   Nature 425, 438; 2003 ). Spearheading the NIH effort will be a consortium of 60 Clinical and Translational Science Centers (CTSCs) at universities and medical centres across the country, which will share some US$500 million annually when they are all in operation by 2012. Late last month, the NIH doled out the most recent grants in this programme to 14 institutions, including Indiana University School of Medicine in Indianapolis and Harvard University, bringing the consortium up to 38 member centres since its launch in 2006. Yet the money for the CTSCs will total only 1\u20132% of the NIH's annual budget of $29.5 billion, and at this early stage it is not clear how much these catalysts will be able to change the terrain. Even so, some people credit the organization and its leader for trying. \u201cLots of people say they hate Zerhouni. I love him. He had the courage to come forward and say that the NIH was not delivering on its promise,\u201d says Lee Nadler, head of the new CTSC at Harvard. Ask ten people what translational research means and you're likely to get ten different answers. For basic researchers clutching a new prospective drug, it might involve medicinal chemistry along with the animal tests and reams of paperwork required to enter a first clinical trial. For groups wanting to developing diagnostics, imaging tools, or screening and prevention methods the route would be different.  \n                New image \n              In some sense much translational research is just rebranding \u2014 clinical R&D by a different name. But it also involves investing in training, research and infrastructure to help researchers engage in clinical research \u2014 and cross the valley of death. Funding agencies hope that this will break down barriers in the transformation of basic-science breakthroughs into clinical applications ('bench to bedside') and enable more research on human subjects and samples to generate hypotheses that are more relevant to people than to animal models (see  page 843 ). \n               boxed-text \n             The barriers to translational research are relatively recent. Back in the 1950s and 60s, basic and clinical research were fairly tightly linked in agencies such as the NIH. Medical research was largely done by physician\u2013scientists who also treated patients. That changed with the explosion of molecular biology in the 1970s. Clinical and basic research started to separate, and biomedical research emerged as a discipline in its own right, with its own training. The bulk of biomedical research is now done by highly specialized PhD scientists (see  graph ), and physician\u2013scientists are a minority. The basic biomedical research enterprise has now evolved its own dynamic, with promotions and grants based largely on the papers scientists have published in top journals, not on how much they have advanced medicine. And many clinicians who treat patients \u2014 and earn fees for doing so \u2014 have little time or inclination to keep up with an increasingly complex basic literature, let alone do research. This has diminished the movement of knowledge and hypotheses back and forth between bedside and bench. At the same time, genomics, proteomics and all its cousins are generating such a volume of potential drug targets and other discoveries that the pharmaceutical industry is having trouble digesting them. With pharma spending more on research but delivering fewer products (see  graph ), it is no longer in a position to take forward most academic discoveries. \u201cThere is a real crisis in the industry,\u201d says Garrett Fitzgerald, head of the CTSC based at the University of Pennsylvania in Philadelphia. One crude way of tracking the rupture is to see when people developed a new rhetoric to deal with it. The term 'translational research' first appeared in PubMed in 1993, sparked by the characterization of  BRCA1  and other cancer genes, which suggested immediate applications in early detection and treatment of cancers. Use of the term remained low throughout the 1990s, in just a handful of papers annually, until around 2000, after which it has cropped up in several hundred articles each year. In 2000, the US Institute of Medicine convened the Clinical Research Roundtable, which held a series of meetings that are credited with putting translational research high on the agenda. The process pinpointed two blockages in the transfer of research knowledge into practice ( S. H. Woolf  J. Am. Med. Assoc.   299,  211\u2013213; 2008 ). The first was preventing laboratory advances being converted into new medical products and tests in humans; the second was stopping proven improvements in treatment \u2014 a new drug combination, for instance \u2014 becoming adopted in medical practice.  \n                Out of the comfort zone \n              Biomedical research agencies are responsible for the first block. As anyone attempting translational research will testify, basic scientists have few incentives to move outside their comfort zone. It means getting involved with complex regulatory and patent issues. There is the risk of career damage to boot, because it is not the sort of research that gets published by the top journals and spurs promotion. Publicly funded biomedical science has become disconnected from the processes that lead to cures and treatments, says Rudy Balling, a proponent of translational research at the Helmholtz Centre for Infection Research in Braunschweig, Germany. \u201cMost biologists haven't a clue about real medical needs,\u201d he says, or about the difficulties of applying their research. When Zerhouni became director of the NIH in 2002, \u201cthat's exactly the situation as I found it\u201d, he says. \u201cThere was a widening gap between basic and clinical research, which if left alone would have been a major barrier to progress.\u201d As head of the world's top-spending biomedical research agency, Zerhouni was under pressure to make progress. The NIH's budget had doubled since 1998 to $27 billion in 2003, and tax-payers were demanding a return on their investment. \u201cThat is the accountability factor that Congress is asking us to address,\u201d he says. At the time, Zerhouni convened a series of whirlwind meetings with top clinicians and scientists who also realized they needed to change the way they worked so that existing knowledge didn't end up sitting on shelves. These meetings convinced Zerhouni \u2014 a radiologist himself \u2014 that it was a priority \u201cto get over this gap\u201d by redesigning the agency's translational research programmes.  \n                New lamps for old \n             The NIH already had projects under the old 'clinical research' label, including 78 General Clinical Research Centres (GCRCs) created in 1959 at universities and medical centres nationwide. But the centres were generally limited to providing services for conducting clinical trials. They did not tackle Zerhouni's new priority, spelt out in the roadmap, to boost the agency's ability to train physician\u2013scientists and translational researchers capable of bridging the valley of death. The CTSCs will replace the GCRCs. Science and innovation have become too complex for any nostalgic return to the physician\u2013scientist on their own as the motor of health research. Reinventing that culture is therefore the focus of the CTSCs, in the form of larger, multidisciplinary groups, including both basic scientists and clinicians, but also bioinformaticians, statisticians, engineers and industry experts. Zerhouni says he expects them to be breeding grounds for a new corps of researchers who will effectively stand on the bridge and help others across. Scientists at the centres will be evaluated with business techniques, such as milestones and the ability to work in multidisciplinary groups, rather than by their publications alone. Since 2006, Fitzgerald's centre in Philadelphia is using its CTSC money to pull together 400 or so staff who were previously scattered across research centres and hospitals and install them in a new bricks-and-mortar institute. For researchers with work to translate, the new centre offers support with regulatory issues, patents and clinical trial design. Fitzgerald would ultimately like 20% of new medical-school graduates to follow translational research courses, and the centre also offers master's and other degrees in the new discipline for MDs and PhDs. One of Fitzgerald's programmes is exploring the aftermath of painkillers called COX2 inhibitors, which were more or less abandoned by the pharmaceutical industry after they were found to increase the risk of heart attack and stroke. Researchers at the centre are looking for biomarkers that might identify those who escape these side effects and salvage a future for the drugs. Scientists in other countries are watching the NIH flagship effort with interest. In Britain, which is second only to the United States in biomedical research output, the government last year announced a doubling of the Medical Research Council's budget to almost \u00a3700 million (US$1.3 billion) by 2010, largely to finance a new focus on translational research. In Europe, around 20 national research and government agencies are exploring a European version of the CTSCs. Coordinated by Balling, the European Advanced Translational Infrastructure in Medicine wants to create a multimillion-euro network of biomedical translation hubs across Europe, based on existing research centres. Time will tell whether the NIH's translational centres can come up with the goods. Gary Pisano, an expert in innovation at Harvard Business School, calls them \u201can experiment worth doing\u201d. Government support has been used with some success to further application of other research fields, he points out, such as defence funding that supported applied research in electronics, communications and the Internet. Measuring the outcomes of translational research is notoriously difficult, as they do not lend themselves to the simplistic bean counting of publications. Because drug development can take up to 20 years, the eventual impact of such efforts on the drug pipeline will only emerge with time. At the NIH, Alving has set up a commission to advise how the CTSCs should be evaluated. This might be done by tracking researchers' career paths and surveying productivity by, for example, counting patents, clinical trials and collaborations with industry. But until patients see a benefit, the aims of the programme risk appearing laudable but vague. Some basic scientists baulk at the $500 million annual costs of the centres when the NIH budget is under extreme pressure. But Zerhouni says there will be no significant diversion of resources to translational research and that the CTSCs will be funded largely by absorbing the $290 million budget of the old GCRCs. Some $95 million will come from the NIH's Common Fund, and the rest will be redirected from other clinical projects. Zerhouni says the NIH has a current balance of 60% basic and 30% clinical and argues that it needs more, not less, basic research to feed the translational pipeline. Others assert that the 30% clinical figure is artificially inflated because it classifies a proportion of work \u2014 such as that on animal models \u2014 as clinical that others would call basic, something Zerhouni denies. With a tiny fraction of the NIH budget, and much of that shuffled from existing clinical programmes, critics might charge that the CTSCs are little more than business as usual. Schechter thinks that the NIH needs to go further down the translation road by reforming the monopoly of investigator-driven research grants as the agency's main funding mechanism. This system rewards individual success and does little to encourage the type of collaboration that translational research demands. He points to alternative models for doing translational research, such as the Multiple Myeloma Research Foundation, based in Norwalk, Connecticut, and other charitable groups that operate more like businesses in their drive to get research into clinical trials. \u201cThere are other structures for doing biomedical research than that which the NIH has hewed to for 40 years.\u201d Zerhouni is sensitive to the need for reform, and points to new awards for multiple investigators. He acknowledges there is no 'right' model for translational research, but he is confident that the NIH will learn about the best ones by giving the CTSCs the freedom to explore a diversity of approaches. As to what the NIH stands for \u2014 National Institutes of Health, National Institutes of Biomedical Research or National Institutes of Basic Biomedical Research \u2014 \u201cwe are all of the above\u201d, says Zerhouni. And perhaps it will take many aliases and many attempts to cross this particular chasm. See also  pages 823  and  843 , and online at  http://tinyurl.com/3tt3y3  Declan Butler is a senior reporter at  Nature , based in France. \n                     Drug Discovery@nature.com \n                   \n                     Medical Research@nature.com \n                   \n                     NCRR Clinical and Translational Science Awards \n                   \n                     CTSA Clinical and Translational Science Awards \n                   \n                     NIH Roadmap \n                   \n                     IOM Clinical Research Roundtable \n                   \n                     European Advanced Translational Infrastructure in Medicine \n                   \n                     Pennyslvania's Institute for Translational Medicine and Therapeutics (ITMAT) \n                   \n                     RAND Europe \n                   \n                     Multiple Myeloma Research Foundation \n                   \n                     UK Medical Research Council reforms \n                   \n                     \u201cTranslational research\u201d in PubMed \n                   \n                     A history of the NIH \n                   Reprints and Permissions"},
{"file_id": "453975a", "url": "https://www.nature.com/articles/453975a", "year": 2008, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "As the first grants from the European Research Council begin to come through, Geoff Brumfiel investigates whether the new system is meeting its goals. Markus Reichstein is obsessed with dirt. If he could just do a better job of simulating the stuff, says the 35-year-old climate modeller, he could minimize a major source of uncertainty in climate predictions. His fellow climate modellers \u201cdon't like to dig into the soil\u201d, Reichstein explains from his office at the Max Planck Institute for Biogeochemistry in Jena, Germany, located in the hills above that city's medieval streets and Communist-era smokestacks. Instead, they represent Earth's incredibly complex and dynamic top layer \u2014 which is a huge carbon reservoir \u2014 with ridiculously simple approximations. Until recently, however, Reichstein's obsession with improving this situation was frustrated by a shortage of money. Research funding in Europe varies considerably from country to country, but scientists are typically funded through their home institutions. And the level of funding rises with seniority \u2014 which is why senior researchers receive the bulk of the funding and run the majority of groups. Reichstein wasn't old enough to get all the money he needed from the Max Planck Institute. Nor was he in any position to go after one of the big research grants given out by the European Union (EU). Those awards are targeted at multinational collaborations, with a stated goal of strengthening ties between member nations. Besides, says Reichstein, who was as frustrated by the system as most other young scientists in Europe, the resulting collaborations \u201cwent too much in the direction of applied science\u201d. There was no obvious place for him to marshal the resources needed to tackle his big questions about dirt. That is why Reichstein paid close attention last year when the European Commission launched the European Research Council (ERC): a semi-autonomous agency that would award its grants in a decidedly different way. Instead of focusing on political goals, its only criterion would be the quality of scientific proposals as judged by an international group of peer-reviewers. By European standards \u201cwe are absolute radicals\u201d, says Fotis Kafatos, the ERC's president and an immunogeneticist at Imperial College in London. \u201cThere is no consideration of nationality in the evaluation \u2014 absolutely none.\u201d The impetus for this radical idea came from the scientific community itself, starting around the turn of the millennium. Researchers were increasingly concerned that the politically oriented selection process of the EU was overlooking some of the best science. Their model was the US National Science Foundation (NSF), a US$6-billion agency that has been making peer-reviewed grants to individual researchers (as well as to its larger centres) for nearly 60 years, and that is regularly praised for its ability to fund the best research in a broad range of fields. \u201cThe dream of everyone is the National Science Foundation,\u201d says Ernst-Ludwig Winnacker, the ERC's secretary general, based in Brussels, Belgium. The ERC was formed in February 2007 as part of the EU's Seventh Framework Programme, which sets the research funding trajectory from 2007 through 2013. The ERC's total budget for that period is e7.5 billion ($11.6 billion), or about e1 billion a year. And it mirrors the NSF in several ways: its operations are overseen in part by a scientific council independent of the EU itself; it is divided into directorates that cover everything from the social sciences to physics; and most importantly, it makes its grant selections using a continent-wide network of peer reviewers. Reichstein was particularly taken with the new council's emphasis on fundamental research throughout Europe. National funding bodies can be parochial in their choice of which projects to back, he says. The ERC brings a broader perspective. Better still, the ERC's first round of granting would target researchers like him: early-career scientists who had completed their PhDs between two and nine years ago, and were in the process of establishing an independent group.  \n                Flood of applications \n              Reichstein saw the ERC as a perfect opportunity to dig into dirt, and lost no time in applying. He was hardly alone: the council received almost 9,200 applications for just a few hundred grants, according to Helga Nowotny, a social scientist from ETH Zurich (the Swiss Federal Institute of Technology) and member of the ERC board. \u201cSome panels were just overwhelmed by sheer numbers,\u201d she says. To deal with the huge volume of applicants, extra evaluators had to be brought in, and a pre-screening process was set up. The process was further strained by the bureaucratic rules of the European Commission, according to Robert May, a zoologist at the University of Oxford, UK, and, until he stepped down on 31 May, a member of the ERC scientific council. Peer-reviewers were required to fill out lengthy conflict-of-interest forms and board members were saddled with travel regulations. Although no fault of the ERC directly, he says, the granting system \u201cis hedged about with multiple bits of paper that makes everything extravagantly cumbersome\u201d.  \n                The selection process \n              Still, the grant-allocation process moved forward. The pre-screening reduced the pool of applicants to about 550 investigators, who were then brought to Brussels and interviewed. Reichstein was one of them. After a five-minute presentation about his project, reviewers grilled him with questions that were \u201ccritical but very constructive\u201d, he says. \u201cThey wanted to find out if I was really dedicated.\u201d The ERC finally announced its first round of grants in December 2007, relatively on schedule. Reichstein heard that he'd been selected while he was attending the American Geophysical Union's annual meeting in San Francisco. He instantly brought a round of beers for his colleagues. \u201cIt was surreal at that point,\u201d he says. Over the next five years he will receive e1 million to integrate soil data from European observation stations into global climate models. The ambitious project will seek to characterize microbes in the soil, understand carbon transport through its layers, and ultimately develop computer code that can replace current 'black box' models of dirt. \u201cThe final goal is to move towards a more realistic description of the soil,\u201d he says. \u201cAnd I think we are at the point where we can.\u201d Other first-round winners are similarly enthused about the opportunities the grants provide. \u201cIn Italy it's very difficult for young researchers,\u201d says Livia Conti, a physicist at the National Institute for Nuclear Physics in Padova. Conti will use the money to look at how temperature fluctuations can contribute to noise in gravitational-wave detectors. She says that the funds will allow her to hire a theorist. \u201cThis is way bigger money than anything you could get in Sweden,\u201d agrees Johan Elf, a molecular biologist at Uppsala University. Elf is studying single-molecule dynamics inside cells, and the money will go towards buying specialized equipment and hiring more staff. In September, Elf returned to Sweden after a two-year postdoc at Harvard University. \u201cThe ERC money is a great motivation for staying in Europe,\u201d he says. \n               boxed-text \n             Of course, the old bureaucratic barriers haven't vanished overnight. Red tape is still holding up funding for some, although Reichstein's contracts have finally been signed. The general feeling is that the process was successful, says Nowotny: \u201cOverall it went surprisingly well.\u201d Members of the scientific council are especially relieved that there has been so little political resistance to the ERC, even though the vast majority of winners work in the traditional strongholds of European research (see map). The United Kingdom, France and Germany together are home to nearly half of the selected proposals. Newcomers to the EU such as the Czech Republic and Hungary fare much worse, hosting only a handful of winners between them. Italian scientists had the lowest success rate \u2014 submitting more than 1,500 applications but winning just 26 grants. Nonetheless \u201cneither the [European] Commission nor the Parliament has interfered with the process,\u201d says Winnacker. \u201cI actually expected more problems from the politicians,\u201d adds Micha\u0142 Kleiber, a computer scientist at the Polish Academy of Sciences in Warsaw and a member of the ERC science council. \u201cTo my pleasant disappointment, there wasn't much heard from Poland.\u201d Kafatos thinks that countries on the losing end of the first round didn't object because they saw the ERC's process as fundamentally transparent and fair. \u201cThey are disappointed,\u201d he says. \u201cBut they also realize that they have to do some homework.\u201d With the first round of grants now complete, the ERC is looking to the future. Later this year, they will award a separate round of 300 'advanced grants' for senior researchers. Meanwhile, they are in the process of nearly doubling their staff of 110 and moving towards becoming a European Commission 'executive agency', which should allow them to issue grants more quickly with less paperwork. The council is also working to reduce the number of substandard applications by requiring applicants to demonstrate a track record and supply a five-page technical summary of their work. Additionally, the eligibility period for young investigators will be narrowed to just three to eight years after their PhD. Even then, not all the best applications will receive funding. This year, for example, some 130 applicants passed the agency's threshold, but did not receive money. Kafatos would like national governments to help: \u201cWe would like to see the national system use the results in ways that might be helpful to them,\u201d he says. And that is beginning to happen: France, Italy, Spain and Switzerland have begun national initiatives to fund young investigator grantees that the ERC ranked highly but was not able to fund. Overall, the ERC is off to a running start, says May. \u201cThe critical hurdles have been cleared,\u201d he says. Kafatos agrees: \u201cIt's not everyday that you get more than 9,000 applications for a first call. It has been an amazing experience.\u201d Indeed, says Kafatos, the Council has seen a more manageable volume of applicants for the grants going to senior researchers, thanks in part to refined requirements. Back in his office in Jena, Reichstein is gearing up for his grant, which will begin in September. He says he will use the money to fund two PhD students, a postdoc and an assistant to begin working on the improved soil model. \u201cIt really allows me to attack a big question,\u201d he says. \u201cThat would not have been possible before.\u201d See Editorial,  page 958.  See also Correspondence:  Small countries are unexpected winners in ERC grant tables \n                     European Research Council \n                   \n                     Seventh Framework Programme \n                   Reprints and Permissions"},
{"file_id": "453717a", "url": "https://www.nature.com/articles/453717a", "year": 2008, "authors": [{"name": "Arran Frood"}], "parsed_as_year": "2006_or_before", "body": "Animal behaviour is an endless challenge to mathematical modellers. In  the first  of two features, Mark Buchanan looks at how a mathematical principle from physics might be able to explain patterns of movement. In this, the second, Arran Frood asks what current models can teach us about ecological networks half a billion years old. It began, appropriately enough for research into food webs, over lunch. And appropriately for ambitious and interdisciplinary research, that lunch in 2001 was at the Santa Fe Institute in New Mexico, an organization famous for bringing together creative assemblages of scientists from different fields. Jennifer Dunne, an ecologist at the institute, had been giving a talk about untangling food webs \u2014 the networks of who eats who within an ecosystem \u2014 by using sophisticated new models. In the audience, and now at her lunch table, was Douglas Erwin, a palaeobiologist based at the Smithsonian Institution in Washington DC. Erwin is, among other things, the curator of the institution's Burgess Shale collection, which despite being more than 500 million years old is one of the best-preserved fossil assemblages in the world. \u201cWe immediately began to brainstorm about whether network analyses could be done with ancient communities,\u201d says Dunne. In the past, researchers have assumed that fossil data were not good enough to construct food webs that would be accurate and useful down to the species level; but Dunne's approach, the two thought, might change that. \u201cWe weren't sure how the palaeontology community would perceive it,\u201d Erwin says. \u201cBut we were all pretty excited by the idea.\u201d They hoped that it would not merely provide a new look at an ancient community, but that ancient ecologies might cast new light on modern ones, too. Ecologists have shown that modern food webs follow certain rules, such as the distribution of links increasing proportionally with the number of species, or with diversity. But are there universal laws, or are these rules merely a reflection of the world as it happens to be at the moment?  \n                Seeking universals \n              Ecosystems on other planets would be a great help in sorting out the necessary from the contingent, but remain stubbornly undiscovered. Mimicry in the lab can't capture the necessary subtlety. Food webs from the fossil record might thus be the closest to something completely different that contemporary researchers can ever hope to get their hands on. And the Burgess Shale fauna from British Columbia, Canada, were the obvious choice. Another  lagerst\u00e4tte  ('resting place' \u2014 the term for a site of exceptional preservation), the early Cambrian Chengjiang in southern China, was chosen to serve as a comparison. \u201cOne major plus of the Cambrian data is that there is excellent soft-body preservation across taxa,\u201d says Dunne. \u201cMost fossil assemblages don't have that.\u201d Another plus is that the Cambrian is the period in which predators and prey are first unequivocably present in the fossil record; it's hard to see how there could be interesting food webs any earlier. In the years after that original lunch, Dunne and Erwin mined existing databases and compiled, reviewed and revised the two Cambrian food webs. \u201cOur first surprise was that we could put these data together,\u201d says Dunne. With the food webs ready, the team then analysed them according to the 'niche model' first promulgated by Richard Williams and Neo Martinez 1 . In the niche model, each predator is constrained to eating from one 'dimension', and is expected to eat everything in that dimension. So if a dolphin eats tuna and sardines, it is presumed to eat everything between the two on that dimension. In practice, such a dimension maps closely to body size, but that's not how it is defined; it is a statistical creation that represents many traits, of which body size simply happens to be a significant one in most systems.  \n                Same old same old \n              Just three of the 17 properties of the Burgess Shale web that they measured fell outside the predictions of the niche model. \u201cMost palaeobiologists would assume that these half-billion-year-old food webs should look really different from modern food webs,\u201d says Dunne. \u201cBut it looks like in most ways, at least that we characterize, the organization looks really similar.\u201d 2 Dunne feels that the overall success suggests that the niche model is capturing something fundamental: the energetics of the system. Food webs are resource distribution networks, and body size or foraging strategies are limited by the energy expended hunting food and the value derived from eating it. \u201cThe patterns that we see may reflect not evolutionary history, but the fundamental physics of the system,\u201d says Dunne. \u201cThe fact that the patterns seem to be robust and they stand up quite well is very impressive,\u201d says Richard Bambach, a palaeoecologist at the Smithsonian. The clearest difference between then and now is that both Cambrian webs have a higher variability in the number of links per species, something that seems to reflect the higher proportion of predators found in Cambrian fossil assemblages across the world. What this might mean is unclear. Dunne says she thinks there may have been more predators than would be expected today because natural selection hadn't yet weeded out a category of 'hyper-vulnerable' species that modern webs do not have. Conversely, Bambach speculates that the oddities of structure might reflect predators that simply weren't yet very good, with the best of them not good enough to outcompete the rest. More webs might clarify things. \u201cIt would be interesting to do this throughout the fossil record,\u201d says Bambach. He has mapped changes in animal life strategies, such as the proportion of motile and non-motile organisms 3 , over geological time and his research suggests that only great extinction events \u2014 of which there have been a handful since the Cambrian \u2014 are strong enough to break down ecosystems to the point where they reassemble along different lines. That idea may now be testable. \u201cThe general conclusion that basic food-web topology of benthic-dominated marine communities might have been established by the early Cambrian seems to be well supported,\u201d agrees Peter Roopnarine, a palaeobiologist at the California Academy of Sciences in San Francisco who has made less well-resolved ecological models of fossil assemblages. But he is not convinced that the apparent similarities between the ancient and modern ecosystems reflect absolute constraints such as those of energy flow. \u201cThere could alternatively be fundamental pathways of web assembly that are favoured over others, either by selection at the species level, or selection at the food-web level itself.\u201d Roopnarine says that his reconstructions hint at long-term community-level selection based on species' dietary ranges 4 . But not all palaeobiologists think the new research has much to offer. Nick Butterfield of the University of Cambridge, UK, is unconvinced by the Cambrian webs. The Lobopodian sponge-feeder  Aysheaia pedunculata,  for example, was a centipede-shaped organism a few centimetres long that resembled today's velvet worms. \u201cNine specimens of  Aysheaia  are known to be associated with sponges in the Burgess Shale so the authors say all Lobopodia eat all possible sponges,\u201d says Butterfield. \u201cThat assumption alone gives them more than 30 connections.\u201d The evidence simply doesn't go that far, he says. To get a grip on such problems, Dunne and her colleagues give each link a certainty value. If fossilized gut contents revealed what the organism had been eating, the link was described as 'certain' \u2014 but only 5% of links, at best, meet that standard. When physical association and morphology indicated a likely interaction, the link was 'probable'; these links comprised roughly a quarter or a half of the total for the Chengjiang and Burgess webs, respectively. When the evidence was mainly morphological and from what their presumed modern-day relatives do, a link was 'possible', as in the case of  Aysheaia. The team estimated their model's sensitivity by removing links, sometimes doing so at random and sometimes according to their likelihood. \u201cIn most cases, the results were robust even to the point of pulling out most of the uncertain links,\u201d says Dunne. Roopnarine agrees that the technique looks good: \u201cI think it will be adopted broadly as a measure of variance in web reconstruction.\u201d Another method of taking uncertain links into account was working with 'trophic species' rather than true species. A trophic species is defined as a set of organisms that all eat the same thing, and will typically consist of a number of true species.  Aysheaia  and its apparently close relative  Hallucigenia  thus collapse into a single trophic species because they both are thought to have fed on sponges, and were in turn fed on by the same predators. Grouping species sacrifices resolution, but can clarify the core functional feeding relationships in a web. To Butterfield, though, this is folly compounded on folly. \u201cThere is no direct evidence that  Aysheaia  ate sponges; and to assert this and then expand it to include all Lobopodia and all sponges over a period of 10 or 20 million years is laughably absurd,\u201d he says. Butterfield suspects that the webs' aptness to niche analysis comes from a circularity in the argument. \u201cThis claim that all big things eat all little things instantaneously builds the ecosystem,\u201d he says. \u201cThey have built the conclusion into their analysis by their assumptions of who's eating who. And that's what we want to know.\u201d Dunne counters by saying that the niche model does not simply assume that big things eat little things; the statistical 'dimension' in the niche is not reducible to a physical dimension. \u201cInsects feed on trees; carnivorous pack animals feed on herbivores that are bigger than them; and parasites feed on much bigger hosts.\u201d Would the researchers have identified different trophic relationships if they had been scuba diving in the seas over what would one day be Canada 505 million years ago? \u201cAbsolutely,\u201d says Erwin. \u201cBut would there be enough difference between that and the web we have now to make a difference? Probably not.\u201d Arran Frood is a UK-based science writer. See News Feature,  page 714. \n                     Burgess Shale \n                   \n                     Jennifer Dunne's web page \n                   \n                     Pacific Ecoinformatics and Computational Ecology Lab \n                   \n                     Richard Bambach homepage \n                   \n                     Peter Roopnarine homepage \n                   Reprints and Permissions"},
{"file_id": "453846a", "url": "https://www.nature.com/articles/453846a", "year": 2008, "authors": [{"name": "Helen Pearson"}], "parsed_as_year": "2006_or_before", "body": "The Ludwig Institute for Cancer Research is focused on translating research into cures. Helen Pearson investigates whether its sometimes unusual methods are producing results. There were two phones on the Baltic island: one in the barn near where Webster Cavenee was seeking seclusion to write up a paper, the other on a village telephone pole. Both were ringing. When Cavenee ignored the first, the villagers brought him messages from the second: a mysterious American was trying to contact him from Brazil, then England, then Australia. He paid them no heed: \u201cCalls from a guy I didn't know, from an organization I didn't know anything about \u2014 I thought he was a crank.\u201d His paper finished, Cavenee flew home to Ohio, where he was working as a geneticist at the University of Cincinnati College of Medicine. As he opened his mail, the mystery caller rang again. \u201cHe said, 'I'm calling about the job. I understand you're interested',\u201d Cavenee recalls. \u201cI said 'I don't know anything about your job'. He said 'No one's talked to you? We need to meet'.\u201d The man told Cavenee to go to a Hilton near Chicago O'Hare airport, and ask the manager to take him to the boardroom. Intrigued, Cavenee did as he was told. Good call. That meeting in 1984 changed Cavenee's life. A doctor called Hugh Butt from the Mayo Clinic in Rochester, Minnesota, one of whose patients was the billionaire Daniel Ludwig, offered the researcher US$3 million a year for a new cancer research institute. \u201cHe was telling me they'd give me enough money that I could never blame failure on anyone but myself,\u201d Cavenee says. Cavenee accepted, and has worked for the Ludwig Institute for Cancer Research (LICR) ever since. \u201cYou have to admit it's not the normal kind of job-recruitment process,\u201d says Cavenee. But then the LICR is not a very normal kind of research institute. Ludwig's legacy now supports Cavenee's research centre in San Diego, California, and eight more around the world (see 'Nine branches, seven countries'). It also \u2014 crucially, in the opinion of its leaders \u2014 spends between 15% and 20% of its annual $100-million outlay on an infrastructure that deals comprehensively with intellectual-property issues and clinical-trials management, and more on facilities that make some of the biological reagents that those trials require.  \n                Clinical discovery \n              Those additional features are part of the LICR's formula for 'clinical discovery', a term that the chairman of the LICR's board of directors, Lloyd Old, prefers to the more modish 'translational research' and a goal he has pursued since the 1980s. In 20 years as the LICR's scientific director, and thus its effective chief executive, Old focused on strategies for getting cutting-edge science as close to the clinic as he could before turning it over to drug companies. \u201cThere is no question that we are the model,\u201d he says. \u201cI don't know another organization that saw it so clearly and put it in place.\u201d The LICR has indeed won itself some influential admirers, most recently collaborating with leaders at the National Medical Research Council in Singapore who are keen to tap into its infrastructure. It also has its critics. The centralized power structure that allowed Old to put his vision of clinical discovery into place also allowed him to pursue his own particular scientific enthusiasms, a pursuit on which the jury is still out. And although clandestine-feeling meetings in hotel rooms are no longer part of the institute's recruitment procedure, its decision-making is still opaque enough for others to find it hard to see what they might most usefully copy. The same whiff of the hermetic may also harm the LICR itself in its ability to recruit or retain the best in the field. \u201cWhile the LICR has many strong attributes, there is a certain level of the old-boys' network,\u201d says Jim Woodgett, who worked for the LICR centre in London and now directs the Samuel Lunenfeld Research Institute in Toronto, Canada. \u201cThey should be willing to open themselves up to scrutiny.\u201d A tendency to secrecy may be another part of the LICR's inheritance from Ludwig, a determined but publicity-shy entrepreneur who built up a tremendous fortune in shipping and other businesses. Why he decided to give so much money to cancer research is not known; one anecdote puts it down to one-upmanship. When Richard Nixon declared a 'war on cancer' in 1971 and requested an extra $100 million for research, Ludwig, a friend of the president's, is said to have boasted \u201cHah, well I can beat that!\u201d He founded the LICR that year with an endowment of slightly more than $500 million. Ludwig reputedly liked to do business with his friends, and relied on business and scientific directors from his inner circle, such as Butt, when it came to setting up the institute. In the first phase of its life that inner circle did the hiring and firing, selecting and approaching people such as Cavenee to establish institutes and then leaving them to pursue their own direction more or less autonomously. At this point some of the decisions reflected Ludwig's other interests: a branch in Brazil was connected to his massive land purchases in the Amazon. By the late 1980s and early 1990s, the institute had established an international reputation for sterling basic cancer research in areas such as signal transduction. It was in 1988 that Old, initially recruited onto the LICR board while he was working at the Memorial Sloan-Kettering Cancer Center in New York, became the LICR's scientific director. His interest in cancer was partly spurred by his family: his grandfather, mother, father and aunt all developed cancer. His lab at Sloan-Kettering played a part in some seminal discoveries, including the discovery of tumour necrosis factor (TNF), a molecule central to inflammation and cell death. Now 75, Old is recovering from heart-bypass surgery in his Upper West Side Manhattan apartment, with parquet floors, a grand piano and French windows that open onto West End Avenue. Although no longer the LICR's scientific director, he is still the chairman of its board and wields considerable influence. His voice is soft and hoarse from having had a tube down his throat, his hands shake and he has a tendency to ramble. But he leans forward excitedly when he talks about his passions, such as translational research and searching for the secrets of the immune system's successes \u2014 and failures \u2014 in fighting off cancer. Old says he faced obstacles when he tried to carry out clinical work related to his discoveries at Sloan-Kettering. When he came in as the LICR's scientific director, \u201cI wanted to create structures that I would have liked to be there for me when I came along.\u201d He set about making the rest of the organization take that desire on board. \u201cOld said to the institute's branches, 'You do it or else',\u201d says Eric Hoffman, who directs the LICR's Office of Clinical Trials Management.  \n                Control is key \n              Old's mantra, taken up by his colleagues, has been that the key to translational research is control. 'Control' means retaining the intellectual-property rights and having a support staff that allows LICR researchers to do at least some early-phase clinical trials, rather than leaving a treatment's post-lab life entirely in the hands of pharmaceutical companies. That sort of control has made the institute one of the largest not-for-profit DNA patent holders in American biomedicine. It also means that LICR researchers can design trials to investigate a therapy's mechanisms, rather than just its performance or lack of it. That lets them explore speculative applications that would be ignored by a drug company with more obviously promising leads to hand. \u201cThat gives us immense power to get the job done,\u201d says Jonathan Skipper, director of intellectual property at the LICR. \u201cYou're not beholden to anyone\u201d. \u201cThey can do these wacky things and demonstrate proof of concept for something new,\u201d says Paul Bevan at biotech company Wilex in Munich, Germany, who is collaborating with the LICR. The idea is not to go it alone without pharmaceutical companies. It is to get therapies far enough down the road under the institute's control that they will be in pole position for transition to the clinic. \u201cIn many cases when you license it you lose control,\u201d says Joseph Tomaszewski, who is involved in translation of cancer discoveries at the National Cancer Institute (NCI) in Bethesda, Maryland. \u201cSo you want to take it as far forward as you can.\u201d The other aspect of the LICR's 'control' is ensuring that researchers have access to their own supply of the physical reagents needed for their trials. The LICR owns facilities in Melbourne, Australia, and Ithaca, New York, dedicated to the production and storage of antibodies and other biological reagents. \u201cSynthesizing a monoclonal antibody that can be put in humans can cost a million dollars or more, a very expensive and tedious procedure that academic institutions usually can't afford,\u201d says Otmar Wiestler, chairman of the German Cancer Research Center, Heidelberg, who works with the the LICR. \u201cThe LICR is really unique in providing such an incredible infrastructure,\u201d he goes on. \u201cI don't know any other academic institution that does it on this scale.\u201d Take the institute's work on cancer vaccines that might provoke the immune system into killing tumours. Since the early 1990s, Thierry Boon, director of the LICR's Brussels branch, Old and others have identified a family of 'cancer/testis' antigens that are expressed almost exclusively in tumour cells and not healthy tissues. The LICR has agreements with GlaxoSmithKline to work on the commercial development of these and other antigens. But the institute has also retained the right to perform clinical trials itself, and the ability to make its own antigens to that end. Vincent Brichard, who deals with the vaccine effort for GlaxoSmithKline in Rixensart, Belgium, agrees that trials performed by the LICR tackle scientific questions that have not figured in GlaxoSmithKline's development programme, but may yet prove crucial in the company's decision to develop an antigen. A monoclonal antibody called G250 provides another example of the Ludwig approach. G250, which was discovered not by LICR researchers but at the University of Leiden in the Netherlands, binds to clear-cell renal carcinoma, a common and aggressive form of kidney cancer. Old's lab did much of the work that characterized its behaviour, hoping it might have a therapeutic role. Chaitanya Divgi, who once worked in Old's lab at the LICR, and is now at the University of Pennsylvania in Philadelphia, wanted to test the idea that the antibody could be used to identify which kidney masses are malignant clear-cell carcinoma as opposed to other, less dangerous tumours. This is something that currently requires surgery and a biopsy. It was not one of the applications being pursued by Wilex, the company to which the antibodies are licensed by the LICR, so the LICR funded an independent trial. Divgi showed that the antibody attached to kidney masses in 15 out of 16 patients who were subsequently found to have clear-cell renal carcinoma \u2014 and in none of those whose kidney masses were more benign 1 . A commercial assessment spurred by these results has convinced Wilex to launch a phase III trial of G250 as a diagnostic agent alongside its trial on the antibody as a possible therapy for reducing recurrence of the disease. \u201cWe would have struggled to find the resource and commitment to do that [initial study] alone,\u201d says Bevan.  \n                Privileged position \n             A visit to the LICR's plush New York office (right) reveals that ten metres of shelving is devoted to regulatory paperwork required to test a new agent in humans: just short of a metre for every one of the 11 investigational new drug applications the institute holds in the United States. That shelf explains exactly why translational research looks uninteresting, frustrating or overwhelming to many \u2014 getting a satisfied buzz out of rules and regulations takes a certain type. And the LICR has employed 15 such types to deal with clinical-trial paperwork alone. The LICR owes this infrastructure not just to Old's vision; the \u201chistorically privileged position\u201d, as Old puts it, of the institute's bank balance is crucial. Its endowment of $1.4 billion is not in the same league as those of the Wellcome Trust, the Howard Hughes Medical Institute, or the Bill & Melinda Gates Foundation. But it is substantial, focused on cancer and entirely under the control of a board of like-minded trustees: there is no peer-review process. Old and Andrew Simpson, who has succeeded him as scientific director, suggest that other institutions in privileged positions \u2014 such as Harvard, with its $34 billion endowment \u2014 could learn something from the LICR's model. (In May, Harvard announced that it would be receiving $117.5 million over 5 years from the National Institutes of Health in Bethesda, Maryland, for a new centralized translational research effort in which the university is concomitantly investing $15 million annually.) But the LICR is hardly proselytizing. The only time that the central office has published anything about its vision and infrastructure was in a 2004 letter to  Nature Medicine 2 . Old says he \u201cwas taught not to toot his own horn\u201d, although perhaps he is now overcoming that childhood stricture a little. Becoming modesty can edge into opacity. It is unclear, even to some of those who work for the organization, how top-level decisions are made and priorities set. \u201cIt's fairly autocratic in the way it's run,\u201d says Gerard Evan, who worked at the now defunct Cambridge, UK, branch in the 1980s and is now at the University of California, San Francisco. Old and Simpson acknowledge that big mistakes were made in the 1980s, when five branches were shut on the basis of business decisions by the board. \u201cI'd go in every day and check the fax machine to see if I was still in business,\u201d Cavenee recalls. More recently, some researchers who worked in the London branch when it was directed by Michael Waterfield were dismayed by the decision to move it from University College London to Oxford after new director Xin Lu took over in 2005. \u201cIt's a little sad after all the good work,\u201d says Bart Vanhaesebroeck who worked at the London branch and is now at Queen Mary, University of London, with continued funding from the LICR. Simpson says that the branches have always been anchored around the research interests of the relevant director. Old says an organization ought to reflect its leadership.  \n                Results matter \n              Some observers say the organization could benefit scientifically by being more open to review. \u201cIf they're really doing the right thing they shouldn't have to defend themselves so much against external review,\u201d Woodgett says. \u201cIt's hard to argue with yourself and that's the problem with the Ludwig.\u201d But Simpson defends the LICR approach. \u201cThere is a philosophy we'd lose if we didn't have that autocratic element,\u201d he says. \u201cIt's important for smaller more nimble organizations like ourselves to plough a different furrow. Judge us by our results, not by the way we're doing it.\u201d What, then, are those results? The LICR has only one licensed therapeutic success to point to \u2014 granulocyte-macrophage colony-stimulating factor (GMCSF), which is used to stimulate production of white blood cells after chemotherapy \u2014 and that grew out of work done before Old's focus on translation got underway. That record may sound poor, but the development of cancer therapies takes time whoever is doing it. For example, the NCI set up its Rapid Access to Intervention Development (RAID) programme in 1998. Designed to take a basic discovery and put it through all the phases of toxicology, pharmacology, large animal studies, production and paperwork that is required for an investigational new drug, with about $100 million spent so far, RAID has got 32 of 121 candidates to the clinical-trial stage, according to Tomaszewski, who has been involved in the programme since its outset. This is not a field with quick wins, and by RAID standards the LICR's 120-odd clinical trials since 1996 look at least par for the course. That said, Old's stated belief that various aspects of the LICR's work are now coming to fruition suggests that if its success rate doesn't climb in the coming decade there will be something amiss. But what? The translational approach that Old developed \u2014 or the particular enthusiasms at which he has directed it? Tony Burgess, who heads the LICR's Melbourne branch, says that Old's personal research interest has produced a clinical programme biased towards cancer vaccines \u2014 which have a history of failure \u2014 and immunology. \u201cI think the investment is very risky,\u201d he says. \u201cI happen to think it might help some people but not many.\u201d If risky ideas don't pan out the efficiency of their translation is moot. Old says that the institute will be judged on the \u201cunyielding yardstick of human benefit\u201d. But interest and emulation from admirers may offer an interim tape measure. \u201cThey are small and elite in a positive way,\u201d says Edward Holmes, chairman of the National Medical Research Council in Singapore, where a new LICR institute is now being planned. Singapore has made an aggressive investment in biomedical science over the last five years and, for the last two, has been pushing particularly hard to develop a more translational and clinical side. The LICR and Singapore government has agreed to jointly invest $7 million per year to establish an LICR branch there focused on clinical research. Cavenee is on the recruitment committee for the director of the Singapore branch \u2014 which is proceeding in a comparatively standard way with advertisements and word-of-mouth recommendations. Having built up two LICR branches, first in Montreal, Canada, then in San Diego, and trained more than 300 young scientists since his own recruitment, he's grateful for the opportunities the LICR has offered that other settings might not have. The organization's support, worldwide expertise and provision of enough money did indeed remove any external risk of failure. \u201cThere was never a dull moment,\u201d he says happily. That unyielding yardstick, though, does not care about dullness \u2014 and it has yet to measure the interesting institute's success. See Editorial,  page 823,  and online at  http://tinyurl.com/3tt3y3. \n                     Translational research special \n                   \n                     Ludwig Institute for Cancer Research \n                   Reprints and Permissions"},
{"file_id": "453843a", "url": "https://www.nature.com/articles/453843a", "year": 2008, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "Results can be thrust from bench to bedside, but there is also much to be learned by pushing the other way. Heidi Ledford tells tales of clinical trials that have prompted a change in tack. In April this year, Nobel laureate Sydney Brenner brought the crowd to its feet at the American Association for Cancer Research meeting in San Diego, California. Brenner pioneered the use of the nematode  Caenorhabditis elegans   as a simple model for studying growth and development. But in his talk, he championed experiments on a more complicated creature:  Homo sapiens.   \"We don't have to look for model organisms anymore because we are the model organism,\" he said. Brenner is one of many scientists challenging the idea that translational research is just about carrying results from bench to bedside, arguing that the importance of reversing that polarity has been overlooked. \"I'm advocating it go the other way,\" Brenner said. Bedside to bench means that clinical trials and patients' unexpected responses are valuable human experiments, and failed trials can stimulate new hypotheses that may help refine the experiment in its next iteration. Reverse translation of this type comes with its own challenges, such as gaining access to clinical samples. And only certain trials, for example those the research community has already invested in, tend to get this type of scientific scrutiny. Below,  Nature   recounts three stories in which results from human experiments inspired new avenues of research.  \n                A moving target \n             It was a front-page article in  The Boston Globe   that first caught cancer researcher Daniel Haber's attention. The 2003 article 1  described the strange results from an experimental cancer drug called gefitinib, one of the first generation of 'smart drugs' designed to target a specific protein, in this case one called epidermal growth factor receptor (EGFR). Several types of tumour churn out higher than normal levels of EGFR, and gefitinib was designed to block the receptor and prevent further tumour growth. In 2003, the US Food and Drug Administration conditionally approved the drug, marketed by AstraZeneca as Iressa, to treat a severe lung cancer called non-small-cell carcinoma. And in some cases \u2014 typically non-smokers, women and Asians \u2014 the drug yielded dramatic results, seemingly eradicating signs of the disease from patients whose illness was thought to be terminal. \"The response was magical,\" says Haber, director of the Massachusetts General Hospital Cancer Center in Boston. But in other patients, as the  Globe   article explained, gefitinib was ineffective. Trials completed after the drug's approval showed that it failed to improve patients' survival when averaged across all individuals 2 . Regulators took the drug off the US market in 2005, although it is still approved in some other countries, and for certain exceptional cases in the United States. To many, the drug's failure was a bitter disappointment. But Haber was intrigued and wondered whether there was a molecular explanation for the discrepancy in responses between one patient and the next. Down the road at the Dana Farber Cancer Research Institute, Matthew Meyerson and his colleagues were wondering the same thing. Neither group initially thought to look for answers in the sequence of the  EGFR   gene: the experiment was so obvious that both assumed it had already been done. \"It was naivety,\" says Meyerson. In 2004, Haber, Meyerson, and William Pao at the Memorial Sloan-Kettering Cancer Center in New York, independently published results showing that most tumours that responded to gefitinib harboured mutations in  EGFR   that rendered the protein more sensitive than usual to the drug\n 2 ,  3 ,  5 . As a result of these findings, researchers have developed genetic tests for  EGFR   mutations, and several clinical trials are now under way to determine whether the drug is effective when it is given only to the patients with a mutated receptor. But there was another puzzle to solve. Early trials had shown that even those who did initially respond to the EGFR inhibitor soon become resistant, many within a year of starting treatment. By genetic analysis of tumour samples and cancer cell lines, researchers have found that these resistant tumours acquire secondary mutations that render the drug impotent. Sometimes, the culprit is a second mutation in EGFR that stops the drug from binding to the protein; at other times, a gene called  MET   is also amplified, which allows the tumour cells to multiply even when EGFR is not working 6 . Occasionally both EGFR mutations and  MET   amplification arise in different cells within the same patient, illustrating how cancer cells will use every genetic trick in the book to continue growing. Findings such as these partly inspired the Cancer Genome Project, which aims to sequence genes from multiple cancers to reveal their genetic idiosyncrasies. And early clinical trials are under way to test whether gefitinib-resistant tumours that have accumulated a secondary mutation can be tackled with alternative drugs, such as EGFR inhibitors that bind more tightly to the protein and are unaffected by the mutations. Few failed drugs receive so much scrutiny. Gefitinib was a special case because it had been designed specifically for a target, and because it produced such a 'magical' response in some patients. One of the major obstacles in bedside-to-bench research is obtaining high-quality tissue samples from trials. Few physicians are willing to collect the multiple invasive biopsies that are needed to determine the molecular changes in a tumour as it evolves, but that offer no direct benefit to the patient. Meyerson had to track down the investigators who had tested the drug and apply for separate approval to use the samples from the many ethics committees who approved the gefitinib study. \"I didn't understand how hard it was to get samples from a clinical trial at that time,\" he says.  \n                Tale of the unexpected \n             When leukaemia first developed in a child given gene therapy, there was still hope that it was just a coincidence. \"We didn't know what to make of it,\" says Brian Sorrentino, who directs the gene-therapy programme at St Jude Children's Research Hospital in Memphis, Tennessee. \"Then the second case came, and it was clear this was going to be a recurring problem.\" Since 2002, 5 of the 21 children who received a high-profile, experimental gene-therapy treatment for a disease called X-linked severe combined immunodeficiency (X-SCID) have developed leukaemia. X-SCID is caused by a mutation in a gene, called  IL2RG,   that is required for the immune system to generate working T cells and other cells in the immune system. The condition is commonly called 'bubble boy disease' because babies with X-SCID must live in sterile environments to avoid lethal infections. At first, the two X-SCID trials, in France and Britain, were heralded as the clearest success in the controversial gene-therapy field. Most patients developed a functional immune system, and the first recipients now live a normal life, says Marina Cavazzana-Calvo, a researcher at the Neckers Children's Hospital in Paris, France, who helped conduct the French trial. The technique relied on a retrovirus to shuttle a functional copy of  IL2RG   into the patient's bone marrow stem cells, from which immune cells are generated. Researchers expected that the retrovirus would integrate into the patient's genome at random. But shortly after the two trials started, Christopher Baum of Hannover Medical School in Germany, and his colleagues published a short report indicating that the virus preferentially inserted itself next to a cancer-causing gene, causing leukaemia in mouse models 7 . \"The initial reaction was that our mouse model would not be relevant to the clinics,\" Baum says. Once the third case of leukaemia came to light in 2005, the French trial was put on hold, sending a chill through the entire gene-therapy community. \"It was a difficult time,\" says immunologist Frank Staal, who studies gene therapy at the Erasmus University Medical Center in Rotterdam, the Netherlands. For many clinical trials, such a disaster would mark an immediate end to the research. But because this therapy had looked so promising and because the disease is so devastating, researchers were anxious to find out what had gone wrong. Using a method to isolate and amplify only the regions of DNA that surrounded the virus, scientists have found that the insertions were far from random in the bone-marrow stem cells. The virus had multiplied and slotted into hundreds of different sites, preferring to settle near highly expressed genes 8 ,   9 . In the French trial, some of the patients with leukaemia had viral inserts near possible cancer-causing genes such as  LMO2 , which is involved in blood-cell formation 10 . The researchers suspected that genetic elements in the virus that were used to activate  IL2RG   also stimulated expression of  LMO2   and other genes nearby the virus' insertion site. This probably caused the proliferation of T cells that caused the leukaemia. Since then, researchers have been trying to get around this by modifying the viruses used to transfer the gene. Adrian Thrasher at University College London has designed a vector with genetic control regions that are less likely to activate nearby genes and that contains a 'kill switch' to prevent it from replicating once it has inserted into the genome 11 . Pending final approval, this vector will be used in the next round of gene-therapy trials for X-SCID, says Thrasher, who is one of those leading the trial. Of the five children who developed leukaemia, four were successfully treated and their gene-therapy-repaired immune systems remained intact. Sorrentino notes that if the gene therapy itself had not been so successful, the community might not have rallied so readily to fix the leukaemia problem. \"At the time it felt terribly depressing,\" he says. \"But we've worked through that and now I feel very enthusiastic.\"  \n                Test shot \n             By the time Merck's HIV vaccine candidate made it into a phase II trial, there was no shortage of strong opinions about its probable fate. Over the past few decades, the HIV vaccine community had seen one failed attempt after another. Merck's approach \u2014 to stimulate a T-cell response, rather than one from antibodies \u2014 was new. So some voiced hope for the vaccine's success. Others pointed to the raging HIV epidemic as a reason to move any promising candidate into clinical testing. And yet more were downright pessimistic. But no one expected the vaccine to make some study participants more susceptible to infection. \"That really shocked the field,\" says Barton Haynes, from Duke University in Durham, North Carolina, and director of the Center for HIV-AIDS Vaccine Immunology (CHAVI). Researchers remain at a loss to explain what went wrong with the trial, called STEP, which was halted last September. But they have been galvanized to find that explanation, and are lining up to access samples and trial data. Part of the willingness to explore the failure lies in the nature of the HIV vaccine field, which was born of 'bedside-to-bench' research, says Bruce Walker of Massachusetts General Hospital in Boston. \"In the beginning, we didn't have any idea of what this disease was,\" he says. \"It required going from the bedside back to the bench to figure it out.\" Plus the research community was already heavily involved in developing the vaccine and the trial was co-sponsored by the National Institutes of Health. The fact that this vaccine increased the rate of infection injected a sense of urgency into the trial's post-mortem because of concern that other vaccines might do the same. \"There is no more important question than determining what happened,\" says Mark Feinberg, Merck's vice-president of medical affairs and policy. \"The implications for the field are enormous.\" Some have cautioned that another failure like the STEP trial could spell the end of the HIV vaccine quest altogether. Preliminary analysis of the trial data has revealed that those who became more susceptible were typically uncircumcised males and carried pre-existing immunity to the virus used in the vaccine. That virus, a disabled form of a common cold virus called adenovirus serotype 5, was used to ferry HIV genes into the patient to elicit an immune response. But researchers do not yet know whether the pre-existing immunity was important in reducing resistance to HIV, or whether it is merely a surrogate for some other factor. One hypothesis is that the increased susceptibility could have a genetic cause, and researchers in the CHAVI consortium have put forward a proposal to scan the genome of trial participants to look for such a genetic signature. Others want to determine whether immune responses to the vaccine's vector may have rendered participants more vulnerable to infection by fuelling an increase in the subset of T cells that HIV infects. The HIV vaccine community has formed a scientific review panel to evaluate proposals for research that will use STEP samples, and Merck is planning to create a central repository of samples from the trial to facilitate their distribution. So far, more than 25 investigators have proposed projects, says Julie McElrath, an HIV researcher at the Fred Hutchinson Cancer Research Center in Seattle, Washington, who is helping to coordinate the effort. The challenge, McElrath says, lies in recruiting new basic researchers with fresh ideas to work with the samples because many do not know that they are available or how to go about getting their hands on them. \"It seems like this huge barrier,\" she says. Prompted by the failed trial, the US National Institute of Allergy and Infectious Diseases made it clear at an HIV vaccine summit in March that the institute will shift the balance of its vaccine funding away from clinical trials and towards basic research, in an effort to stimulate new approaches to vaccination. Those involved say that the shocking scale of the HIV epidemic demands a dogged approach that is sometimes absent from other areas of research. \"In academia, we'll come up against a brick wall and we'll tend to move to a different area,\" says Haynes. \"With the HIV vaccine, we have to keep looking at that problem.\"  \n                Learning lessons \n             Francesco Marincola, a cancer immunologist and advocate of translational medicine at the National Institutes of Health Clinical Center in Bethesda, Maryland, says that the level of organization and persistence found in the HIV vaccine community is rare. \"There is a lack of that in other, more complex diseases,\" he says. One reason is that HIV researchers have a clear target \u2014 the disease-causing virus itself. Another is that industry and government-funded researchers collaborated on the vaccine's development, so the project wasn't just abandoned when the results turned out negative. Other fields may begin to catch up. The current emphasis on developing biomarkers to monitor disease progression is encouraging more investigators to collect patient samples. Without them, it is difficult to look back and evaluate what went wrong if the trial fails. \"That's the reason why we haven't learned from these failures,\" says Marincola. \"Instead, we have gone from one failure to another.\" See Editorial,  page 823 , and online at  http://tinyurl.com/3tt3y3  . \n                     Cancer Diagnostics Insight \n                   \n                     Signalling in Cancer Insight \n                   \n                     Nature Medicine blog \n                   \n                     Clinical trials database \n                   \n                     HIV Vaccine Trials Network \n                   Reprints and Permissions"},
{"file_id": "453150a", "url": "https://www.nature.com/articles/453150a", "year": 2008, "authors": [{"name": "Helen Pearson"}], "parsed_as_year": "2006_or_before", "body": "The inner life of a cell is noisy. Helen Pearson discovers how the resulting randomness makes life more challenging - and richer. Sunney Xie's eight-year-old twins have a lot in common \u2014 starting with their genome. But they have different fingerprints, footprints and very different personalities. Any father might wonder how two apparently identical cells could turn into daughters fairly easily distinguished, at least by a parental eye. For a father who is also a biochemist at Harvard University in Boston, Massachusetts, that question is not just one for idle musing. \"It's my biggest mystery,\" Xie says. Two years ago, Xie's interest in 'identical' cells that act differently led him to produce the first comparisons of individual cells producing proteins one by one 1 ,   2 . Although each of the cells studied under his cutting-edge fluorescent microscope produced the proteins he was looking for in bursts of up to 20 at a time, the sizes of the bursts and rhythm of their timing changed from cell to genetically identical cell. Xie wonders whether some such stochastic difference might have propelled the two identical cells that were to become his children down divergent biological paths. In text books, molecular biology seems to leave little room for chance and random fluctuations. Cells are seen as exquisite machines, their component molecules fitting together to form mechanisms as neatly as any blind watchmaker could require. Genetic information is treated as software for this hardware to run, and the ways in which chance might make the software run differently in different cellular machines are seen as sources of error. But in the past few years, some researchers have been exploring a far more haphazard picture of what goes on in the cell. Using techniques that allow one cell to be compared with another, they have found identical genes in identical cells doing wildly different things. Now researchers are trying to understand whether this variation or 'noise' actually matters. How hard do cells work to suppress it, what mechanisms do they have for tolerating it? A study from 2006 suggests that the workings of cells get noisier and noisier as they get older, and that this might contribute to the ageing body's decline 3 . \"People are fascinated by how we do what we do despite this noise,\" says James Collins who studies stochastic processes at Boston University. \"That's what everyone is ultimately trying to get at.\" Noise in cells is inevitable. It traces back to the fact that each cell carries only one or two working copies of each gene. The DNA in which the genetic information sits is constantly shifting in both shape and structure, while jiggling proteins attach themselves to it and fall off again. At a given moment, the same piece of DNA may be bound by an activating protein in one cell, and not in another. In the first, a gene will be switched on, triggering the transcription of messenger RNA (mRNA) molecules and their subsequent translation into proteins. In the second, it will not, for the time being. There was no plan \u2014 it was all dependent on tiny changes in the configuration of the DNA, or the proximity of specific molecules. \"You can never get away from the fact that it's all probability. Every event is up to chance,\" says Johan Paulsson, a systems biologist at Harvard Medical School. It is this that makes the noise in the cell so pervasive, and so interesting to those who wonder how cells control themselves. It is not just that the turning on and off of genes is noisy; the systems that regulate that turning on and off are themselves ineradicably noisy too. \"Every chemical process is a constant battle between randomness and correction,\" says Paulsson. An awareness of the part that chance might play has ebbed and flowed over the history of molecular biology, although many current researchers are \"stupefyingly unaware\" of the issue, according to Roger Brent, a molecular biologist at the Molecular Sciences Institute in Berkeley, California. The main difference between that earlier thought and today's interest, he says, is new tools that provide \"all kinds of abilities to ask questions about single cells.\"  \n                A million states \n              An early example of these tools is a 2002 study by Robert Singer at the Albert Einstein College of Medicine in New York. Singer's team used the relatively recently developed technology of fluorescent tagging to make four different coloured probes that bound themselves in different combinations to the mRNA transcripts of eleven different genes 4 . Each mRNA was thus given a different colour 'barcode'. The team showed that cells in a genetically homogeneous sample tended to each switch on a different combination of genes at different times. \"No two cells seemed to be doing the same thing at the same time,\" Singer says. \"Each cell could be in one of a million different expression states.\" The amount of mRNA and protein produced by a population of cells typically forms a classic bell-shaped curve about the mean. The noisiness of a gene's expression is reflected in the width of the relevant bell curve. Some genes are quiet, with neatly peaked curves. Loud ones have their curves spread out. In the noisiest cases, the lowest performers may be making almost an order of magnitude less protein than the most productive cells. Simply doing away with all this noise is not an issue. In collaboration with Glenn Vinnicombe, an engineer at the University of Cambridge, UK, Paulsson has carried out theoretical work showing that noise abatement can take a huge amount of effort. The pair modelled bacteria with varying numbers of DNA rings called plasmids. The plasmids control their own copy number by making repressor proteins that block plasmid replication. The more fluctuation there is in the number of repressors, the more fluctuation there will be in the number of plasmids: thus noise will tend to amplify itself. At the same time there is a very strong incentive for the plasmids to clamp down on the noise by producing more repressor proteins \u2014 because the more wildly the plasmid number swings, the greater its chance of that number dropping to zero. Selection will favour plasmids that avoid going extinct in this way.  \n                Turning down the volume \n              According to Paulsson and Vinnicombe's unpublished model, a cell must produce enormous numbers of repressor proteins in order to control plasmid number noise \u2014 producing, for example, 160,000 repressors per cell division to make it possible to keep four plasmids to within a 5% standard deviation. Only by manufacturing so many repressors can the cell ensure that the inhibitor levels provide a faithful read-out of the underlying plasmid levels, so that there are enough around to control plasmid number. (Real versions of this type of plasmid use some of the strongest promoters found in nature to turn out many thousands of inhibitors per cell cycle.) Paulsson's theoretical work has convinced him that only the most critical cellular processes are worth the large investment it takes to keep noise under control. \"If you want to do a good job you have to spend a huge amount of energy,\" he says. \"If you want to do a half-decent job you pay much less, but it's still surprisingly expensive.\" If removing the noise is not feasible, cells need to make a trade-off between the effort that is required to lower the noise levels and the effort that's required to live with them. The default noise level would thus be, almost by definition, the loudest that's bearable \u2014 any higher and the system breaks down. \"I think in lots of cases the noise we see is simply the best the cell can do,\" says Paulsson. \"People ask how come an organism works so well. Perhaps it doesn't work so well. Perhaps organisms without these fluctuations would outcompete us.\" On that analysis, noise is simply something to be put up with. But another school of thought suggests that it might be more than that. It may be that for some purposes cells have learned to like it noisy. Rather than using energy to minimize all sources of noise or insulate all systems from its effects, the cells may be using the noise to their own ends. Collins is one of those who sees advantages to noise. His studies over the past 14 years have shown that mechanical and electrical noise applied to neurons can help them respond to weak incoming signals, because they are already partially excited. He also showed, on the same basis, that vibrating insoles can help elderly people balance, and has applied to patent the technology. In 2006, Collins's team described engineering mutations into the control region of a gene that confers antibiotic resistance to create two strains of the yeast  Saccharomyces cerevisiae , one with noisier expression of the gene, one with something more steady. Faced with a lethal antibiotic, the noisier strain survived better 5 . This result supports the idea that noise is a form of 'bet hedging' for cells: a population is more likely to survive in a changing environment if its members are noisy because some are likely to be making the quantity of a protein best suited to that situation. \"A system that is covering more possibilities has a greater chance of survival in unpredictable settings,\" says Collins.  \n                Two-colour eyes \n              Another situation in which cells may have used the stochasticity of gene expression to their advantage is to create a scattered pattern of photoreceptors in the developing retina of the fruitfly  Drosophila . Here, some 30% of cell clusters called ommatidia include cells that are sensitive to blue light and 70% include those sensitive to green light. Claude Desplan at New York University and his colleagues showed that cells in the developing retina randomly switch on the  spineless   gene, making a transcription factor that controls other genes' activity 6 . Once on,  spineless   triggers a feedback loop that ensures the cell making it, and the one next door, together form a green-light sensitive ommatidium; those that do not turn on  spineless   make a blue-sensitive ommatidium (see picture, below). Desplan says that evolution has taken advantage of small and random asymmetries between cells to ensure that the photoreceptors are spread across the retina. \"Stochasticity makes life very difficult but it must have an evolutionary advantage,\" he says. Other researchers are not yet convinced that noise has such biological value. \"You can always make up a story of how cell-to-cell variation could be an advantage,\" says Brent. Noise could present a major obstacle when information is at stake \u2014 for example when a cell is measuring conditions outside its walls and needs to transmit that information faithfully to the nucleus. When yeast cells receive an external signal in the form of a pheromone, Brent has shown that their responses (such as recruiting signalling proteins to the cell membrane) are precisely calibrated to the strength of the signal, thus ensuring that they switch on pheromone-response genes in proportion to the pheromone's strength. But in a mutant strain, the signal and response are no longer perfectly aligned, and the cell's pheromone response becomes more unpredictable. \"Noise is actively worked around,\" says Brent, which suggests that evolutionary pressure to ensure signals are transmitted in noise-resistant ways must be strong. Cellular noise could also pose a problem in the development of an embryo, when cells have to act together to produce precisely timed and coordinated events. \"How does a cell control that and make sure it doesn't take a wrong exit at the wrong time? That's a very interesting question,\" says Alexander van Oudenaarden, a physicist at the Massachusetts Institute for Technology in Cambridge. Working with  Caenorhabditis elegans   embryos, van Oudenaarden is examining the type of developmental mutations that are puzzling because they cause an obvious defect in some animals, but leave others intact. His idea is that fluctuations in noisy genes could explain this behaviour, nudging only some embryos down the mutant developmental pathway. A similar phenomenon might explain why some people with high-risk mutations that predispose them to breast cancer or another disease actually escape unscathed.  \n                The chance of ageing \n              If noise-abatement is important in suppressing disease, its breakdown may have particular ramifications at the end of our lifespan. Jan Vijg of the Buck Institute for Age Research in Novato, California, measured the expression level of 15 genes in individual muscle cells plucked from the hearts of young, six-month-old mice and elderly 27-month olds 3 . The variation in gene expression was around 2.5 fold larger in older cells when compared with younger ones, showing that biological noise increased with ageing. Vijg suggests that heart cells accumulate wear and tear to their DNA, and that this upsets the systems the cells normally use to keep noise reined in, a phenomenon that may contribute to the gradual decrease in heart function with age. \"There is a lot of evidence that ageing has this stochastic component,\" he says. If it is true that ageing interferes with the control of almost all genes, then drugs or treatments that can reverse the ageing process may be unrealistic, Vijg says. \"It will be very difficult to intervene in a basic process of life that you can't get rid of.\" Vijg and other researchers now wonder whether there are genes and proteins the main function of which is in controlling or suppressing biological noise. Chaperones, proteins that help other proteins fold into the correct shape, are one candidate. They might help compensate for volatile protein concentrations by stalling the folding process until all of a system's components are present in the numbers needed to form a working piece of machinery. Whereas some are working on the controls over noise, others are looking at its causes. In one of the first such experiments, van Oudenaarden studied genetically identical bacteria making a green fluorescent protein 7 . His group engineered mutated strains in which either transcription of DNA to RNA or translation of RNA to protein was altered \u2014 and showed that the noisiness in protein output could be changed systematically. This and other studies have led to the idea that most noise stems from the rarer events during transcription \u2014 typically the infrequent activation of a gene or the slow, fitful production of mRNA molecules. If only five mRNAs for a given protein are present in each cell, then the creation or destruction of one or two extra can produce large variations relative to the mean. If, by contrast, a cell contains 500 copies of an mRNA molecule then the addition of a few extra makes little difference. Michael Elowitz, one of the leaders in the field at the California Institute of Technology in Pasadena, makes the comparison to raindrops falling on a roof. When rain is light, the numbers hitting each roof tile can vary dramatically; but when rain is heaver, those fluctuations are washed away. This means that you need a lot of Paulsson's repressors around to stop these variations in number from having a drastic effect. Experiments such as van Oudenaarden's have left some researchers wanting more. The total concentration of protein in one cell is itself the result of many separate stochastic moments, separate events in which mRNA and then protein were made one by one. Some scientists wanted to eavesdrop on this process. \"We realized we needed to go one step higher in resolution,\" says Ido Golding, a physicist at the University of Illinois in Urbana-Champaign. This has been a technological challenge, because fluorescent cameras are usually only sensitive enough to detect the signal coming from 20 or 30 fluorescent proteins in one spot \u2014 and the signals can be blurred by diffusion of the protein.  \n                Random bursts \n              In 2005, Golding described a way around this resolution problem, working at the time in the lab of Edward Cox at Princeton University, New Jersey. The researchers introduced a repetitive motif into one  Escherichia coli   mRNA molecule so that each molecule was bound by 50\u2013100 copies of green fluorescent protein 8 . As each mRNA molecule was produced, enough glowing proteins latched onto it to see it in real time. As they watched, the gene suddenly switched on and produced an intense burst of mRNAs rather than, as most had thought, constant, steady production. The group calculated that this gene would be off for an average of 37 minutes and then on for 6 minutes, during which time it would produce anywhere from one to eight RNA molecules. The bursts may be caused by random changes in the conformation of DNA that permit transcription, or the random binding and detaching of proteins that activate or repress transcription. And it is these bursts, which vary both in length and in the numbers of molecules produced, that eventually produce much of the wide variation in protein levels between cells. According to Singer, only certain genes may be prone to bursts of activity. Other genes may be permanently accessible for transcription, which proceeds at a more leisurely and continuous pace. For these 'housekeeping' genes, involved in running mitochondria, say, or cellular transport, it may be too risky for cells to leave production to random bursts. For others, such as those involved in responding to a rapidly and unexpectedly changing environment, it may be too risky not to. Xie found echoes of burst activity in his studies of proteins 1 ,   2 , where each burst corresponds to the brief lifespan of a single mRNA before it decays. He was able to visualize single proteins because he studied some of those which are anchored in the cell membrane. Being secured in place, their fluorescent signals are not blurred by diffusion. And in the last two years, Xie has edged a little closer to understanding how his two genetically identical daughters became who they are. He has shown how the stochastic behaviour of a single molecule can drive two genetically identical cells to adopt different fates. Xie's team studies bursts in the production of LacY, a membrane channel through which the sugar lactose enters into  E. coli . When lactose is present at low levels, a repressor protein binds DNA and prevents production of the channel and other proteins involved in lactose metabolism. When lactose levels are high and enough has entered the cell, the repressor detaches and the cell switches into a state where it can metabolize and use the sugar. This classic system of gene regulation was first studied by Francois Jacob and Jacques Monod in the 1960s. Xie's graduate student Paul Choi tracked the channel proteins and the repressor. He found that, now and then, the repressor protein drops off the DNA entirely and takes a minute or so to latch back on. This allows production of mRNA followed by a massive burst of 200\u2013300 channel proteins. Lactose floods into the cell, preventing the repressor from rebinding and triggering lactose metabolism. So a chance event \u2014 a single repressor protein dropping off DNA \u2014 thrusts the cell into an entirely different state, or phenotype. And by chance, only a proportion of cells in a population will trigger this switch. This may work to the bacteria's advantage when lactose concentrations are too low to support the entire population of cells, because only some will flip into a state where they can take up and make use of the sugar. \"Stochasticity of a single molecule solely determines a life-changing decision of the cell,\" Xie says. \"I don't think it's a stretch to think that these single molecule events will determine more complex phenotypes as well.\" After Xie published his protein-bursting papers in  Science   and  Nature   in 2006, he says he received a surprise phone call from a programme manager at the Bill & Melinda Gates Foundation, in Seattle, Washington, asking him to think about whether his work could somehow help in their fight against tuberculosis. So Xie thought \u2014 and he is now embarking on a project to investigate whether stochastic events such as those he has observed in  E. coli   might explain why, from a population of genetically identical bacteria, only some will become extremely drug resistant. Stochastic events can turn two genetically identical bacteria into ones that behave differently. Can the same really be said of Xie's genetically identical twins? They are, after all, far more complicated organisms than the unicellular types under his microscope. \"In very simple cells we have shown for sure that a single stochastic event can lead to different phenotypes,\" Xie says. For his daughters, \"I can't say for sure it's due to that\". See Editorial,  page 134. \n                     Molecular Cell Biology \n                   \n                     Imaging in Cell Biology \n                   \n                     The Xie group website \n                   Reprints and Permissions"},
{"file_id": "453146a", "url": "https://www.nature.com/articles/453146a", "year": 2008, "authors": [{"name": "Rachel Courtland"}], "parsed_as_year": "2006_or_before", "body": "A new way to analyse seismic vibrations is bringing order out of noise to help predict volcanic eruptions or create detailed images of Earth's interior. Rachel Courtland reports. Seismologists like noise \u2014 the louder, the better. Thumpers, airguns, explosives and earthquakes are their favourite tools. The vibrations made by these tools \u2014 called seismic waves \u2014 can be recorded on a detector, sometimes located thousands of kilometres away. With enough events, and enough detectors, seismologists can follow the waves' paths to illuminate the structure of the Earth below, just as X-rays illuminate the internal structure of a human body. For decades, waves from these large, single sources have told us what the planet looks like under the surface. But Earth is also awash with much quieter vibrations that may turn out to be just as illuminating. This persistent hum doesn't have a single source, but instead comes from a combination of minor tremors, the long drawn-out echoes of major earthquakes and the crashing of ocean waves. These faint vibrations come from all directions and bounce around multiple times in Earth's interior. On a seismograph, they show up as a seemingly meaningless series of spikes and troughs. Yet over the past few years, seismologists have transformed the hum from a nuisance to a powerful tool to image Earth's crust and upper mantle. The new field, known as ambient-noise tomography, has several advantages over traditional seismic imaging. Its improved resolution may help to answer some persistent geological questions. It also means that researchers do not have to wait for one-off events such as earthquakes, opening up the possibility of imaging Earth's crust over time. In addition, the technique works for seismically quiet areas that do not routinely see the earthquakes needed for traditional imaging studies. Seismologist Andrew Curtis of the University of Edinburgh, UK, says that using the technique is like discovering an unknown continent. \"An incredible new area of exploration has come upon us,\" he says.  \n                Noisy beginnings \n              Random noise has long been used to tease out the intrinsic properties of materials. The theoretical underpinnings trace back to the 'fluctuation-dissipation theorem', formulated in 1928 to describe how noise in an electrical signal can reveal a material's response to current. The theorem suggests that, with proper analysis, random vibrations inside Earth could act like seismic waves, revealing structure and hinting at properties such as temperature, composition, orientation and stress. All noise passing through Earth must pass through the same internal structures, and hence each signal \u2014 although inherently random \u2014 retains a sort of 'memory' of the material it has passed through. So by looking at correlations in noise between two seismometers, researchers can extract information on the material lying between the pair. \"The idea is that in waves that are fully garbled, there is still some sort of residual coherence,\" says Richard Weaver, a physicist at the University of Illinois at Urbana-Champaign. The research started to take off in 1999, when Weaver attended a workshop on random waves in Corsica and met Michel Campillo, a geophysicist at Joseph Fourier University in Grenoble, France. At the time, Campillo was looking at earthquake codas, the multiply reflected echoes of quakes, recorded by a seismometer network in southern Mexico. Campillo thought that codas could reveal how the crust and upper mantle scatter seismic waves. But he wasn't quite sure how to proceed \u2014 how, for instance, should he incorporate waves that bounce off the Earth\u2013air boundary in his analysis? Weaver suggested a fix, and an additional direction: compare the noise in pairs of detectors a specified distance apart, look for evidence of correlations, then invert the timing of any correlated spikes to get the speed of the seismic waves. It was such a radical idea that Weaver says: \"in hindsight, I'm not so sure why I was so confident.\" After the workshop, the two went off to investigate how the technique could be used to probe the properties of materials. As a proof of principle, Weaver showed that ambient ultrasound vibrations in aluminium could be used to determine the material's elastic properties 1 . Then Campillo applied the same technique to the earthquake coda; the noise seemed to be correlated just as he hoped it would be 2 . So Campillo began to eye the ambient noise that dominates seismographs when the codas fade, in the hope of quickly and finely imaging the crust. It worked. In 2005, two teams 3 ,   4 , one of which included Campillo and his colleagues, used ambient noise to construct three-dimensional maps of seismic-wave speeds in southern California. The images had a horizontal resolution as small as 60 kilometres, more than four times the detail in traditional earthquake tomography. The success of the technique was a surprise to Campillo. Seismic noise on the California coast is lopsided, as the eastward-moving waves from the Pacific Ocean tend to swamp noise from other directions. \"If you want to use the noise, you have to assume the noise has the right properties, that there is an even distribution of sources,\" says Campillo. \"And of course that's not true \u2014 it was not at all obvious that noise would work as well.\" Most of Earth's ambient noise reverberates through the shallower parts of the planet, making the technique most useful in the upper 60 kilometres of Earth. But since 2005, several teams have used the technique to look deeper into Earth's crust \u2014 and even into the upper mantle.  \n                Collisions to climates \n              Paradoxically, some of the clearest pictures of those depths come from the highest plateau on Earth, the Qingzang Gaoyuan plateau (often referred to as the Tibetan plateau). At an average elevation of 4.5 kilometres above sea level, the plateau is being pushed ever-higher by the ongoing collision between the Indian and Eurasian continental plates. The thickest part is on the eastern side of the plateau, where the crust is roughly 80 kilometres thick \u2014 twice the average for the continental crust. Working out exactly how and when the plateau got so high could help to improve climate models, as some researchers posit that the arrival of Indian monsoons coincided with the rise of the eastern side of the plateau. Geologists are also still debating how the crust on the eastern side grew so thick. One theory holds that it thickened as it was squeezed against the stronger crust of the neighbouring Sichuan basin. Others think that the crust from the Indian subcontinent is largely responsible, pushing and flowing through the plateau, inflating it like a tire. \"It's sort of a conservation of mass problem. You're shoving one continent into the belly of another continent, and you want to know where all the mass goes,\" says Thorne Lay, an Earth scientist at the University of California, Santa Cruz. To investigate, Rob van der Hilst of the Massachusetts Institute of Technology in Cambridge and his colleagues set up 25 seismometer stations on the eastern edge of the plateau, then combined data from ambient noise and traditional tomography to produce a picture of the plateau down to a depth of 280 kilometres 5 . The clearest images, which came from ambient tomography done roughly 50 kilometres down, showed chopped-up areas where seismic waves travelled slower than usual. These low-velocity regions may be weak parts of the crust that have slowly flowed underneath and between other crustal layers. Although the images cover too small an area to be definitive, Lay says, \"it's a step in the right direction\". Proponents of the flow model are encouraged. \"I would say it's very new, exciting evidence. We didn't know there were low-velocity zones down there at all,\" says van der Hilst's colleague Leigh Royden, whose model of the plateau's uplift predicts crustal flow. The results also suggest that the model of crustal flow is more complicated than originally expected; the weak parts do not form a uniform, flowing layer, suggesting a complex network of conduits where the crust flows through the middle and lower mantle. Later this year, van der Hilst plans to return to the area to collect data from a recently installed Chinese array of 300 seismometers. The additional data will help to improve resolution and might also help to differentiate between seismic speeds in different directions. This directional dependence, or anisotropy, may hint at material under strain, an indicator of flow.  \n                Early warning \n              The wealth of data offered by ambient noise tomography may also assist in more dynamic measurements, such as detecting precursors of volcanic eruptions. For the past several years, Campillo and his colleagues have set their sights on Piton de la Fournaise, an active volcano poised on the edge of the island of R\u00e9union, near Madagascar. Piton de la Fournaise runs almost like clockwork: Averaged over the last 200 years, it has erupted every 10 months. Although most of the island's roughly 800,000 inhabitants live beyond the volcano's reach, past eruptions have overflowed the caldera, reaching the ocean as well as villages. Over the years, seismologists have used Piton de la Fournaise as a test bed for several techniques that might help to predict future eruptions. Most rely on geodetics \u2014 sensors that measure strain, tilt or displacement on the surface of the volcano to detect slight changes, such as bulging from pent-up pressure. Seismometers are also set to look for any increase in shaking that might signal an impending explosion. But in the absence of shaking, which sometimes starts just days or hours before an eruption, the volcano is a black box. \"Seismic activity is less reliable for volcano eruption forecasting, because it can be linked to a lot of phenomena,\" says Florent Brenguier at the Institute of Earth Physics in Paris. \"It is not a clear sign of an oncoming collapse.\" Last year, Brenguier and his colleagues showed that 18 months of data from 21 seismometers scattered around the volcano could be used to reconstruct its three-dimensional structure 6 . The images showed a unique structure that matched up with previous experiments: an anomalous area of high velocity, one kilometre east of the volcano's main vent, which seems to be a chimney full of solidified magma. Unexpectedly, comparing images separated by several months showed that seismic waves slowed down by a tiny amount \u2014 0.1% \u2014 when travelling through this structure. What's more, the waves started to slow down as early as 20 days before four eruptions that occurred between July 1999 and December 2000. Clear indicators of the change in speed showed up 5 days before the eruption. Why this happened isn't clear, but Brenguier says the speed changes probably match up with the opening of fissures within the edifice, through which the magma eventually flows to the surface. But there are challenges in trying to use ambient noise to predict volcanic eruptions. For one thing, seismic waves in Piton de la Fournaise also show long-term changes in their speed, which may stem from mechanical changes due to tides or fluctuations in water level. \"We don't know at the moment how to correct for these long-term variations or how they could be used for real-time monitoring,\" says Brenguier. The team's latest data, though, suggest that large changes in wave velocity correspond to larger eruptions, and down the line it might at least be possible to predict the size of an eruption 7 . Bernard Chouet, a volcanologist at the US Geological Survey in Menlo Park, California, questions whether warnings from ambient noise will ever be better than the few days' notice researchers typically get from the long-period seismic events generated when magma starts to gurgle in a volcano. But, he says, \"all these things put together are going to put us closer to forecasting things more precisely\". Extending the technique to volcanoes with more irregular eruptions will be a crucial test; such studies are already under way, including one at Mount Merapi in Indonesia.  \n                Graphic detail \n              In the years to come, the true bounty of ambient noise tomography may lie in high-resolution, three-dimensional maps of large swaths of Earth's crust, rather than having to wait for earthquakes. Some of the most detailed maps produced so far with this technique have come from the western United States. In the past few years, researchers have studied ambient noise using USArray, which includes a rolling grid of 400 transportable seismometers that began mapping the west coast in 2004, but are moved eastwards each year. USArray was conceived before the advent of ambient noise tomography, but the dense array of detectors was perfectly suited for the technique. With proper analysis, the seismometers can combine to create 80,000 source-detector pairs that can be used to image the intervening Earth. \"One of the frustrations of seismology historically is that we have to wait for earthquakes as the source of energy, and they're most certainly not on a grid \u2014 they're in the Pacific Ring of Fire, in subduction zones or at plate boundaries,\" says seismologist Michael Ritzwoller of the University of Colorado in Boulder. The hum of ambient noise has revealed a number of features in fine detail, including subducting slabs, structures called mantle drips under the southern Sierra Nevada and evidence of the movement of Yellowstone's hotspot track across the Snake River Plain. Like others, Ritzwoller is anticipating new results from anisotropy measurements, which may help illuminate stresses within the mantle and fractures and fault zones within the crust. \"I think what this is ultimately going to do is put the emphasis on processes and not structures,\" he says. \"It is actually a huge leap.\" As rapidly as ambient noise tomography has been adopted, many researchers are still trying to perfect the analytical underpinnings of the technique. Challenges include figuring out how to account for the crashing of ocean waves that come from different directions, and for changes in topography at Earth's surface. Still, researchers are enlisting a growing number of seismometer arrays, including some in China, New Zealand, Korea, Japan, the United Kingdom, on glaciers and on the floor of the South Pacific Ocean. \"I don't think the story's finished,\" says Campillo, \"and it is not even really on the way.\" \n               Rachel Courtland is an intern reporter in Nature's Washington DC office.  \n               See Editorial,  \n                     page 134 \n                    . \n                     US Array \n                   Reprints and Permissions"},
{"file_id": "453275a", "url": "https://www.nature.com/articles/453275a", "year": 2008, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "A high-profile scientist, a graduate student and two major retractions. Erika Check Hayden reports on a case that has rocked the chemistry community. When Mary Dwyer was looking for a doctoral adviser, Homme Hellinga was her first choice. A biochemist at Duke University Medical Center in Durham, North Carolina, Hellinga had ground-breaking ideas and an exciting research programme. He also shared Dwyer\u2019s interest in the relationship between protein structure and function. But there was a problem: students in Hellinga\u2019s lab were warning Dwyer away. \u201cIt\u2019s pretty tough,\u201d they told her; \u201cthere are other good labs.\u201d One student even pulled her aside and told her flat out that working with Hellinga was so difficult that she should not join the lab. By that time, that student remembers, many more students had left Hellinga\u2019s lab than had earned doctoral degrees under his tutelage. Yet Dwyer had done a short rotation with Hellinga\u2019s group, and had seen nothing alarming. \u201cI felt like I would probably be able to handle it,\u201d she recalls \u2014 and so, about nine years ago, she decided to join the lab. Dwyer\u2019s work under Hellinga led to major publications in journals including Nature and Science, adding sparkle to Hellinga\u2019s already shining career. But last year, another scientist found problems that forced the eventual retraction of two papers \u2014 and Hellinga turned on Dwyer, accusing her of fabricating data. The episode has sparked controversy and condemnation, while highlighting the pressures on scientists working in cutting-edge research. Hellinga is a bold scientist with a sterling pedigree. From his first Nature paper 1  onwards, Hellinga has been fascinated by one question: how does a series of amino acids encode a protein\u2019s function? Cracking that code is one of the major goals of science, because it would enable researchers to design custom proteins. In 1991, Hellinga, together with his postdoctoral mentor Frederic Richards of Yale University in New Haven, Connecticut, published a computer program 2  intended to do just that. Called DEZYMER, the program predicts protein sequences that might adopt target structures and functions \u2014 some of which are new to nature. It was fitting that Hellinga should take on such a problem. Those who know him describe Hellinga as highly confident in his intellect and interested only in grand challenges. One scientist recalls, for example, that Hellinga once asked a companion, \u201cDo you think I\u2019ll be more famous than Darwin one day?\u201d Asked whether he agrees with claims that he is arrogant, Hellinga replies, \u201cI would say no. Can I appear to be personally arrogant? I would imagine yes. When you are trying to do a difficult experiment, you have to have a certain amount of self-confidence to say, \u2018All right, this is the moment and we think we have the techniques and ideas together to try and give this a go\u2019.\u201d  \n                Shapely targets \n             \n               boxed-text \n             Around 2002, Hellinga decided to embark on his most difficult challenge yet: radically reshaping a humble protein into a highly active enzyme \u2014 a biological catalyst \u2014 called triose phosphate isomerase (TIM). The enzyme is part of a biological chain of reactions called the glycolysis pathway that is found in most organisms. Hellinga\u2019s goal was audacious; other scientists had designed weak enzymes 3 , but nothing as active as TIM \u2014 considered a \u2018perfect enzyme\u2019 because of its extremely high efficiency (see graphic).  Hellinga chose Dwyer and another student, Loren Looger, to work on the project in Escherichia coli bacteria. The pair were to transform E. coli\u2019s ribose-binding protein, which has no enzymatic activity, into a TIM. Looger and Hellinga wrote computer programs to model how the structure of the ribose-binding protein could be changed to make it work like a TIM. Dwyer used the program to design mutated ribose-binding proteins, dubbed \u201cNovoTIMs\u201d, and tested whether they worked in the lab.  Dwyer, who describes herself as a \u201cpretty conservative person\u201d, was sceptical that the project would pan out. \u201cI had my doubts all the time,\u201d she says. After about 6 months testing 25 designs, Dwyer found that a couple of the designed proteins were active, but she also noticed some problems. The E. coli bacteria made much smaller amounts of the NovoTIM proteins than of their own natural, or native, proteins. And the NovoTIMs were very unstable.  Perhaps because of these issues, Dwyer\u2019s experiments yielded confusing data about NovoTIM activity. When she measured the enzymes\u2019 kinetic parameters \u2014 characteristics that describe how enzymes work \u2014 the tests didn\u2019t always give the same results. \u201cI felt like we couldn\u2019t nail down the kinetic parameters because of the variability that we were seeing,\u201d Dwyer recalls. Even after she started working with another member of the lab, \u201cwe were also getting a lot of variability. We just didn\u2019t understand it,\u201d Dwyer says. Hellinga says that the variability was \u201cno more than you would expect in [such] an experiment\u201d.  By early 2004, Hellinga was ready to publish. On 29 March, he submitted a paper describing the NovoTIMs to Science, which accepted it on 6 May. The paper did not mention the variability Dwyer had noticed. It included only her best data and claimed victory 4 . \u201cWe have successfully converted a protein devoid of catalytic activity into a triose phosphate isomerase, using computational design techniques,\u201d it stated. Dwyer was the first author on the Science paper, which was co-authored by Hellinga and Looger, who left Duke that year and now works at the Howard Hughes Medical Institute\u2019s Janelia Farm in Virginia. But Dwyer did not celebrate the accomplishment. \u201cIt was kind of strange,\u201d she recalls. \u201cI wanted to work more on the variability issue,\u201d along with other odd results she had seen. \u201cI felt like we weren\u2019t quite there yet.\u201d Dwyer says that she raised her concerns with Hellinga at the time. But Hellinga says he does not feel he pushed Dwyer or anyone else to publish prematurely. \u201cThese things were talked through very carefully with all the people involved,\u201d he says. That September, the National Institutes of Health gave Hellinga one of its nine inaugural Director\u2019s Pioneer Awards, worth US$2.5 million over five years. In October, he received the $10,000 Feynman Prize for experimental work from the Foresight Nanotech Institute in Palo Alto, California. Around the same time, he says, he and his wife, Duke structural biochemist Lorena Beese, were considering multiple job offers, including one from Yale. But in April 2005, Duke named Hellinga a James B. Duke Professor of Biochemistry, and Beese received the same honour the following year. Duke also created a new institute co-headed by the couple, the Institute for Biological Structure and Design.  \n                To the letter \n              As Hellinga\u2019s career was skyrocketing, it was perhaps easy for him to overlook a letter that crossed his desk in December 2004 amidst the flurry of accolades. \u201cDear Professor Hellinga,\u201d it began. \u201cI was wondering if you would be interested in collaborating.\u201d  The letter was written by John Richard, a chemical biologist at the State University of New York in Buffalo. Richard had studied with giants of the enzymology field: Perry Frey at the University of Wisconsin-Madison, Bill Jencks at Brandeis University in Waltham, Massachusetts, and Irwin Rose, now at the University of California, Irvine, who shared the 2004 Nobel Prize in Chemistry for discovering how a protein called ubiquitin marks other proteins for destruction in cells.  Richard had developed a method to analyse reactions catalysed by TIM 5 , 6 . He had seen Hellinga\u2019s Science paper and wanted to compare the characteristics of the NovoTIMs with those of normal TIMs. Richard proposed such experiments to Hellinga, but received no response. \u201cIt wasn\u2019t a high priority,\u201d Hellinga says.  The two men come from very different scientific cultures. Richard was trained in mechanistic enzymology and is known for his work in physical organic chemistry \u2014 fields that are no longer in vogue, perhaps because \u201call the easy experiments have been done\u201d, Richard says. Richard has gained respect in these fields, which require carefulness and meticulousness. \u201cJohn is clearly one of the best physical organic chemists in the world today working on enzymes,\u201d says Joseph Kappock, a biological chemist at Purdue University in West Lafayette, Indiana. By contrast, protein design \u2014 a hot field \u2014 requires daring, as it seeks not just to understand nature, but also to improve on it. In July 2006, Richard was discussing the Science paper with another chemist, Jack Kirsch, an emeritus professor at the University of California, Berkeley, where Hellinga had given a seminar on his work. On 9 August, Kirsch sent Hellinga an e-mail. \u201c[Richard] informed me recently that he had sent you an e-mail requesting materials,\u201d Kirsch wrote. \u201cIs there any reason why you cannot comply with his request?\u201d  That e-mail seemed to grease the wheels. On 20 October, Hellinga wrote to Richard, agreeing to send DNA templates for the NovoTIMs he had made for the Science paper. He also sent templates for a second batch of NovoTIMs made by Dwyer and another researcher the year before. A paper describing these new proteins was about to be published in the Journal of Molecular Biology 7 . Hellinga sent Richard instructions for expressing and purifying all the NovoTIMs, as well as a note: \u201cI hope that your experiments will be successful, and look forward to seeing the profiles for these designs.\u201d In Buffalo, Richard hired a technician, Astrid Koudelka, to work on the NovoTIM project. Koudelka followed Hellinga\u2019s notes, which instructed her to purify the NovoTIMs using a method called step gradient elution. But there was a problem: the step gradient could not separate the NovoTIMs from other contaminating proteins. Then Richard\u2019s wife, chemist Tina Amyes, measured a kinetic parameter of the NovoTIMs \u2014 a value called the Michaelis constant. She found that it was different from the one reported in Hellinga\u2019s Science paper, but similar to that of natural, or wild-type, E. coli TIM. As Amyes studied the NovoTIMs throughout the first half of 2007, nothing about them was as Hellinga had reported, and everything suggested that they were wild-type TIMs.  Koudelka then modified Hellinga\u2019s procedures by using a continuous gradient elution, a more powerful purification method than the step elution. The new method cleanly separated the NovoTIMs from the contaminants. But when Amyes analysed the pure NovoTIMs, they had no enzymatic activity. Instead, the contaminating proteins were active \u2014 and looked just like wild-type E. coli TIMs.  By last July, the Buffalo group was convinced that something had gone wrong with Hellinga\u2019s experiments. By using step purification, they felt, Hellinga\u2019s lab had failed to separate the NovoTIMs from the TIMs found naturally in E. coli. The NovoTIMs were inactive; instead, all the activity that Hellinga had reported in his papers was probably due to contaminating wild-type TIM. \u201cI was sort of distressed,\u201d says Richard. \u201cWe spent quite a bit of time, money and resources to basically do nothing, to show something was wrong.\u201d Yet the team felt an obligation to try to correct the scientific record. \u201cJust saying, \u2018This is not right, let\u2019s discard it and move on\u2019 \u2014 that\u2019s not fair to the scientific community,\u201d Koudelka says.  \n                Quick response \n              On 26 July, Richard sent a long e-mail to Hellinga that laid out his team\u2019s evidence, and pointed out what he saw as additional problems in some of Hellinga\u2019s other papers. Richard copied in the editors of the Science and Journal of Molecular Biology papers and two other chemists. \u201cI think that these issues need to be dealt with in an expedient manner,\u201d Richard wrote, adding, \u201cPlease understand how difficult it has been for me to write this letter.\u201d  This time, Hellinga responded quickly. In a 30 July e-mail, Hellinga wrote that the key experiments \u201chave been repeated several times by different individuals in my research group\u201d. The experiments included the tests that detected NovoTIM activity, and a set of negative control experiments. These negative controls \u2014 not shown in either paper \u2014 found no activity in purified ribose-binding proteins, Hellinga said. But he agreed to look again at the NovoTIMs: \u201cWe will carry out a purification similar to the one that you describe,\u201d he wrote.  All this time, Dwyer had heard nothing about Richard\u2019s communication with Hellinga. After earning her doctorate in 2004, she had left Hellinga\u2019s lab in 2005 to pursue postdoctoral research in a different department. So she was not seriously concerned when Hellinga e-mailed her on the Labor Day holiday on 3 September last year, asking her to meet with him later in the week to discuss issues about NovoTIM. But Dwyer\u2019s new adviser, Donald McDonnell, a professor of pharmacology and cancer biology, advised her not to meet Hellinga alone; he felt she should go with someone who could advocate on her behalf. McDonnell arranged a meeting later that week at which he, Dwyer and Hellinga were joined by two other faculty members from the biochemistry department. And that\u2019s when Hellinga dropped the bombshell. \u201cHe said, \u2018I find it really hard to believe that you didn\u2019t make this up\u2019, and he kept saying that kind of statement over and over again,\u201d Dwyer says. \u201cIt was horrible.\u201d  Dwyer\u2019s adviser defended her, and she proclaimed her innocence. \u201cI said, \u2018That\u2019s ridiculous, no, I didn\u2019t do that\u2019,\u201d she says. \u201cWhat he was saying wasn\u2019t true.\u201d  A few weeks later, McDonnell, Hellinga, Dwyer and the head of the biochemistry department met again. Dwyer\u2019s husband, who is also a scientist, was there. Dwyer showed Hellinga the data from her lab notebooks that, she thought, exonerated her. But, she recalls, \u201che didn\u2019t want to look at any of that. It was just flat out my fault, and that was it.\u201d Hellinga remembers it differently. \u201cThat\u2019s not true,\u201d he says. \u201cOf course I looked at the data. I also had people in my lab repeat the experiments,\u201d he says. On 8 October, Hellinga wrote to Richard. \u201cWe have completed our repeat experiments on NovoTIM,\u201d he wrote. \u201cI concur with your finding that the NovoTIM designs do not exhibit enzymatic activity, and that the reported activity is due to a contaminating activity which is very likely to be the endogenous, wild-type triose phosphate isomerase.\u201d The repeat negative control experiments, Hellinga wrote, had found \u201cTIM activity in the wild-type [ribose-binding protein] preparations prepared by the step gradient elution method.\u201d He added that the repeat experiments were done by three people, \u201cbut NOT Mary Dwyer, the author responsible for executing the experiments described in the Science paper, and responsible in large part for the negative control experiment in the Journal of Molecular Biology paper.\u201d By naming Dwyer as the scientist primarily responsible for the experiments, Hellinga seemed to contradict his 30 July e-mail to Richard, in which he said \u201cdifferent individuals\u201d had been involved. However, Hellinga clarified to Nature that his July e-mail was \u201cslightly inaccurate\u201d; at that time, Dwyer was the only person who had performed the negative controls, he says.  To Richard, Hellinga continued: \u201cDwyer has been contacted in an attempt to seek an explanation \u2026 The matter has been referred to the Office of the Dean of the Medical School for further enquiries, which are now in progress.\u201d A committee on research misconduct convened a formal inquiry hearing in December, at which Dwyer was asked to address the claims against her. On 4 February, she received a letter from Wesley Byerly, an associate dean in the medical school, clearing her of the allegation of falsifying and fabricating results.  \n                Culture of blame \n              But word about the inquiry had already spread, outraging chemists who felt it was wrong for a mentor to accuse a student of fraud. \u201cIt is repre\u00adhensible,\u201d says Frey. \u201cIt is up to the adviser to instruct the student, to guide the student to find out what problems exist with the data and their interpretation of it, and to show the student what the pitfalls are.\u201d  This February, both the Science and Journal of Molecular Biology papers were formally retracted. \u201cThe triose phosphate isomerase activity observed in our reported preparations can be attributed to a wild-type TIM impurity,\u201d stated the Science retraction; the other retraction was similar. Other chemists were surprised that Hellinga\u2019s lab had been fooled by a simple contamination problem. \u201cIt is a bush-league error not to purify your proteins well, especially in a paper like this,\u201d says Wallace Cleland of the University of Wisconsin-Madison.  Still, exactly what happened remains murky. On 10 March, Science published letters from Richard and Kirsch listing issues they said were not resolved by the retractions. For instance, they wrote, the kinetic values Hellinga reported for NovoTIM are not the same as those of wild-type TIM, which is difficult to understand, given that all the activity in Hellinga\u2019s papers was supposed to have come from the wild-type enzyme. Kirsch also raised questions about other experiments in the Science paper that \u201cwould make sense only if the design were successful\u201d. For instance, the paper reported that NovoTIMs could substitute for wild-type TIMs in E. coli that lack TIM enzymes. And a different test supposedly showed that mutated NovoTIMs became less active, just as DEZYMER had predicted. Neither of these results makes sense if the designed enzymes never worked.  Hellinga does not have explanations for the issues Kirsch and Richard have raised. Dwyer thinks that the issues with protein expression and assay variability are partly to blame, and says that in retrospect, the apparent decreased activity of NovoTIM mutants was actually insignificant, once experimental error is taken into account. But no one has offered a clear answer for what went wrong. That is frustrating to Richard, who has spent considerable time and resources trying to get to the truth.  But thanks to Richard\u2019s work, another research team has been able to earn credit for the breakthrough Hellinga once claimed. In March, a team led by biochemist David Baker from the University of Washington in Seattle published two papers showing that computer programs could indeed be used to design working enzymes 8 , 9 . Meanwhile, other scientists have questioned whether Hellinga himself should be investigated. Some point to a Duke policy that states that if an allegation of misconduct is found to be \u201cbaseless and malicious or reckless, the matter will be dealt with in accordance with existing university policies and mechanisms\u201d.  Hellinga says he has received no formal notification that he is under investigation. Duke would not comment specifically, saying only: \u201cWe are aware the retraction by Dr Hellinga has generated considerable debate in the scientific community. Duke continues to follow this debate and is evaluating various points that are being raised.\u201d Asked whether he would have done anything differently in the NovoTIM experiments, Hellinga says, \u201cI would like to not have the problem that we encountered.\u201d When asked whether the lab moved too quickly, he says: \u201cGiven how we understood things to be at the time, no. Obviously if we had known things had gone wrong, we wouldn\u2019t have moved forward with the speed we did.\u201d  As for Dwyer, she still feels rattled by the experience. \u201cI feel incredibly guilty that I didn\u2019t catch it, but I didn\u2019t, and I just have to live with that. It\u2019s been really hard,\u201d she says. She is trying to move forwards with her life and career, she says, and is working in a new lab in a new field \u2014 endocrinology \u2014 with McDonnell. But sometimes, Dwyer says, she thinks back to the people who tried to steer her away from Hellinga\u2019s lab so many years ago. And she wonders how different things might have been if she had heeded their advice. \u201cEverybody gets warned, but nobody listens,\u201d she says. \u201cMaybe now they will.\u201d See Editorial, page  258  . See also Correspondence:  In the wake of two retractions, a request for investigation \n                     The Sceptical Chymist (Nature\u2019s chemistry blog) \n                   \n                     Homme Hellinga lab \n                   \n                     John Richard lab \n                   Reprints and Permissions"},
{"file_id": "453446a", "url": "https://www.nature.com/articles/453446a", "year": 2008, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "Some researchers think that the evolution of languages can be understood by treating them like genomes - but many linguists don't want to hear about it. Emma Marris reports. Consider a word as it tumbles through history:  khun,   a Nepali word for blood. In the early twentieth century, the word fell all too often from the lips of the Gurkhas, a Nepalese brigade in the British Army, in songs describing the horror of the First World War. To linguist Ralph Lilley Turner, who fought beside the Gurkhas by the Suez Canal,  khun   was one of many words transcribed phonetically for the English\u2013Nepali dictionary he compiled in the midst of the fighting. And then, in the 1960s, the word  khun   stopped being a word so much as a node of data in the work of linguist Isidore Dyen. Working at Yale University in New Haven, Connecticut, Dyen used Turner's dictionary to assemble a list of 200 basic Nepali word meanings, including blood, and coded them onto IBM punch cards that were fed into early computers. He used this, and similar lists from another 83 languages, to try to measure the rate at which languages change over time. His discipline of lexicostatistics is now discredited for the crude assumptions it made. But as Dyen worked on his monograph, he transferred his data on to computer disk and, in the late 1990s, published his data online 1 . Nearly a century after it was sung by the Gurkhas,  khun   became a few figures of code in the computer models of evolutionary biologist Mark Pagel at the University of Reading, UK. Last year, Pagel used Dyen's online data to generate trees that estimate when languages such as Nepali diverged from one another 2 . In these models, the word is stripped of much of its rich human history. But it is exactly this type of pared-down language that speaks to researchers such as Pagel the most. \"Some linguists have spent a career studying a language that becomes a single data point in one of these analyses,\" he says. \"We do things because they are mathematically elegant, and are delighted when they can be simplified.\"  \n                A new approach \n              In the past five to ten years, more and more non-linguists such as Pagel have used the computational tools with which they model evolution to take a crack at languages. And one can see why. Like biological species, languages slowly change and sometimes split over time. Darwin's Galapagos finches evolved either large beaks or small; Latin  amor   became French  amour   and Italian  amore.   Darwin himself noted the 'curious parallel' between the evolution of languages and species in  The Descent of Man, and Selection in Relation to Sex.  The advent of molecular genetics provided a new depth to the analogy. Just as the four nucleotides of DNA can produce a staggering variety of creatures, the alphabets of the world's languages can generate an infinite number of sentences. These alphabets, the words they make, and the sounds and grammar rules that frame them are passed down from parent to child in a process that, at least superficially, resembles the inheritance of DNA. Even some complications are the same. Just as species can shade off into a maddening continuum of subspecies, populations and hybrids, languages dissolve into an untidy collection of dialects and intermediate forms. And the rampant borrowing of words between languages resembles, graphically at least, the promiscuous horizontal gene transfer that microbes engage in. There are limits to the analogy. It is unclear, for example, what the 'selection pressures' are for language, if any. A language with a greater number of speakers is not obviously 'more fit' than a dying language. Although a speaker of the prevalent tongue could communicate with more people, it is not the intrinsic properties of a language that make it more widely spoken. Instead, languages seem to rise to prominence on the coat-tails of the culture that speaks them, just as the prevalence of English traces back to the broad reach of British colonialism. It's no wonder, then, that mathematical biologists such as Pagel have become interested in a system that is intriguingly like, and intriguingly unlike, genes. \"I think sophisticated mathematics will increasingly become part and parcel of what we mean when we say that we have 'explained' the phenomena of language change over time,\" says Erez Lieberman, who studies mathematics and biology at Harvard University in Cambridge, Massachusetts.  \n                The old school \n              However, there is already an old and venerable field of language-tree makers. Historical linguists have been reconstructing languages since the 1780s. Their tool is called the comparative method and it relies on extensive knowledge of the language group at hand, along with a broad grasp of, and intuitive feel for, the ways in which languages change. A linguist might notice that the way a vowel is spoken has shifted in two languages when compared with an ancient one, and infer that the shift happened before the two languages split. This will help to place the split relative to other splits but gives no information about when it happened. Hence the comparative method produces trees, but no dates. It is putting it mildly to say that many historical linguists find the evolutionary biologists working on language histories to be bungling interlopers who have no idea how to handle linguistic data. It is also an understatement to say that some of these interlopers feel that their critics are hidebound traditionalists working on a hopelessly unverifiable system of hunches, received wisdom and personal taste. And that's just the mood between the historical linguists and the newcomers. Lots of the newcomers don't like each other either. \"Why get excited about it when it is still so preliminary?\" says Johanna Nichols, a historical linguist at the University of California, Berkeley. \"We are not impressed by a computational or mathematical paper per se. We have to see that it blends well with what is known by historical linguistics and really adds to our knowledge. Then we will be excited.\" Perhaps the most famous and controversial study 3  produced by the new school is a 2003 paper by Russell Gray and Quentin Atkinson at the University of Auckland, New Zealand. The pair started with Dyen's lists of word meanings for 84 languages from the Indian and European subcontinents, plus a few extras from extinct tongues. The data already included Dyen's opinion on which of these words were 'cognates', descended from a common word in a mother language, but the researchers converted this information into numerical code and generated trees showing how and when the languages were most likely to have branched off from one another. This same type of likelihood algorithm is used to compare species' DNA sequences and produce evolutionary trees. Specifically, Gray and Atkinson dated the origin of a language family called Indo-European to around 7,800\u20139,800 years ago. This ineffable date has been one of the most intensely studied and disputed points in all of historical linguistics and, based on archaeological and linguistic data, had previously been put at anywhere between about 6,000 and 10,000 years ago. When Gray and Atkinson's paper made the rounds of linguistics departments, howls of protest ensued. Some critics took the paper as a return to glottochronology, a discredited method from the middle of the twentieth century \u2014 and cousin of Dyen's lexicostatistics \u2014 which in most cases disastrously assumed that all languages change at a constant rate and which helped turn linguists against any quantitative analysis of their treasured subject. But Gray and Atkinson's statistical method does not assume uniform rates of change. Many historical linguists also felt that similarities between words are a terrible proxy for similarities between languages. They tend to argue that common sounds and grammatical rules are stronger evidence for common descent than individual words, which may be similar due to chance, borrowing, or even 'nursery formations' such as mama and dada \u2014 words that mirror each other simply because all infants babble similar things. \"I think that some of these researchers think that these analyses are going to supplement or even supplant historical linguists,\" says Lyle Campbell, a linguist at the University of Utah in Salt Lake City who was one of those unimpressed. \"So far, the ones that try to go beyond what we've done don't seem to work.\" Gray says that the tree does work even though it doesn't take into account the subtleties of sounds and grammar, and he puts much of the criticism down to territoriality: \"When people come from outside, you see a bit of hostility and suspicion.\" Although it might have been \"politically more palatable\" to have a linguist as one of the authors on the 2003 paper, \"it wouldn't have changed the answers\", he says. Ultimately, many linguists felt that this type of analysis oversimplified their cherished subject more than they could bear. Linguists love the little details that give a language personality: to them, the identifying sounds or peculiar borrowed words are nuances that tell the tale of a tongue. The new breed brushes over these details in pursuit of generalities, trends and statistical rules. \"We try to find mathematical patterns in nature,\" says Martin Nowak, an evolutionary modeller at Harvard. \"If someone works on the detailed classification, they might be dissatisfied with something that is cruder.\"  \n                Grand ambitions \n             That dissatisfaction looks set to grow as many in the new school pursue grander ambitions: to find quantitative laws that describe language evolution. In a recent example, published in  Science   earlier this year, Pagel, Atkinson and their colleagues used word lists to build trees in three of the world's major language families; Indo-European, Bantu (an African language family that includes Swahili) and Austronesian \u2014 spoken on Pacific Islands 4 . They found that between 10% and 33% of divergence among these languages happened in what they called 'punctuational bursts', phases of accelerated language evolution just after a language splitting event. The finding echoed the controversial 'punctuated equilibrium' theory in which Niles Eldredge and Stephen Jay Gould proposed that biological evolution often occurred in rapid bursts amid longer periods of relatively slow change. Pagel and his team speculate that the bursts could arise from the spoken idiosyncrasies of a small number of population founders, or a desire within a new population to sound different from the other group. So here is one general law, perhaps: up to one-third of language evolution occurs in punctuational bursts after splitting events. A second possible law arose from studies of word frequency. In their 2007 study, which was published in  Nature,   Pagel and his team found that 50% of the difference in language evolution rates could be explained by the frequency with which words within the language were being used 2 . Often-used words were 'stickier' and resisted change. \"What really excites me about the frequency effect is that we are identifying a general evolutionary law,\" Pagel says. \"We think it will hold and will have held since we began talking.\" In the same issue of  Nature,   Lieberman, Nowak and their co-workers showed that irregular English verbs become regularized more quickly if they are rarely used 5 . So the past tense of a rare verb such as 'gnaw' would have a 50% chance of regularizing to 'gnawed' from the Old English form 'gnagan' in 700 years. By contrast, a very common verb such as 'be' would have a 50% chance of regularizing to 'beed' in 38,800 years, perhaps explaining why 'was' remains the preferred form today. The researchers even had a precise mathematical description of the trend: a verb that is used 100 times less frequently regularizes 10 times as fast.  \n                Different language \n             These findings completely underwhelmed most historical linguists. They already knew that commonly used words change more slowly, and the fact that some aspects of this trend could be quantified did not really interest them. \"I don't think the numbers are very exciting,\" says Campbell. \"I would much rather it be relativized to 'in general, more frequent words change more slowly'.\" One reason that many linguists have been unreceptive to such work is that they are not trained in statistics, and are unsure of how to compare and evaluate this type of numerical model themselves. \"They feel they've been asked to just accept things,\" says Tandy Warnow, a computer scientist at the University of Texas at Austin who works with linguists. And even those people, such as Warnow, who can evaluate the models say that they are too unsophisticated at this stage to pronounce firm dates or quantitative rules. She says that the biological models need to be tailored to language and that they should incorporate the sound and grammar changes that are so important to linguists.  \n                Better representations \n              Paul Heggarty, a linguist at the McDonald Institute for Archaeological Research at the University of Cambridge, UK, is already trying to refine his models in this way. Heggarty is building network diagrams rather than trees to show how similar languages interrelate. He thinks that these can provide a better representation of the relationship between two languages when two cultures rub shoulders very closely and borrow words freely. Then the links between branches of the tree \u2014 equivalent to horizontal gene transfer \u2014 become more important than the vertical branching. \"It is entirely natural for languages to stand in complex cross-cutting relationships to each other that may not be compatible with any branching genealogy at all,\" he says. As part of the network building, Heggarty is also trying to assign more subtle values to word changes than zeroes and ones. A superficial analysis of the word 'dog', for example, might show that the English word is not cognate to the German word ( hund)   and score 1, or 'changed' for the pair. But if the English word 'hound' is chosen instead, it creates a match and would score 0. Because 'hound' isn't the main word for 'dog' in English, Heggarty would score it somewhere in between 0 and 1, perhaps 0.4. He hopes that this type of refined method can create networks that reproduce the real relationships between very closely related languages and, by extension, reveal something about the histories of the peoples who spoke them in the past.  \n                Getting quantitative \n              Such model tweaking is unlikely to win over the historical linguists, but at least some are beginning to warm to the methods. Campbell acknowledges that the sheer number-crunching power of computer models can speed up the good old comparative method. And he sees the appeal in getting a bit more quantitative. If the field does not become more statistical and accountable, he points out, it may lose respect by those in other disciplines. \"I think we'd like the legitimacy,\" he says. Another fan is Harvard University's Steven Pinker, who famously appreciates language in all its fullness. \"There has got to be information in the statistics of language overlap that you simply can't exploit by looking at it intuitively, by eyeballing,\" he says. \"Linguists have been slow in accepting that extra dollop of information that statistics provides, even if there are errors, even if there is noise.\" Noise \u2014 of the statistical kind \u2014 is not comfortable territory for many historical linguists when precious words such as  khun   are at stake. So perhaps the onus now lies on the newcomers to show that their methods will not drown out languages, or their rich and idiosyncratic narrative. \"Hope,\" Pinker says, \"is not that the older generation of linguists will lay down their arms; hope is that the younger generation will follow their noses to what is fruitful.\" \n                     Paul Heggarty's project \n                   \n                     Tandy Warnow's project \n                   \n                     Martin Nowak's programme \n                   \n                     Mark Pagel's research \n                   \n                     Russel Gray's research and open databases \n                   \n                     Lyle Campbell \n                   \n                     Johanna Nichols \n                   Reprints and Permissions"},
{"file_id": "452930a", "url": "https://www.nature.com/articles/452930a", "year": 2008, "authors": [{"name": "Sharon Weinberger"}], "parsed_as_year": "2006_or_before", "body": "Battling rumours of death beams and mind control, an ionosphere research facility in Alaska finally brings science to the fore. Sharon Weinberger reports. It's a Strangelovian scenario that only the Pentagon could dream up: North Korea, in the throes of a military coup, launches a nuclear weapon that explodes 120 kilometres above the Earth. The blast fills the atmosphere with 'killer' electrons that would within days knock out the electronics of all satellites in low-Earth orbit. It would cause hundreds of billions of dollars of damage, and affect military, civilian and commercial space assets. If this doomsday scenario sounds outlandish, then the possible response may sound even more improbable: injecting radio waves into the atmosphere to force these energetic electrons out of orbit. Yet this is exactly what the US Department of Defense is looking at in a major ionospheric research facility in Alaska. The High Frequency Active Auroral Research Program (HAARP) has been entwined with controversy since its birth. Originally envisioned as a way to facilitate communications with nuclear-armed submarines, HAARP took almost two decades to build and has incurred around US$250 million in construction and operating costs. It consists of 360 radio transmitters and 180 antennas, and covers some 14 hectares near the town of Gakona about 250 kilometres northeast of Anchorage. \n               boxed-text \n             With 3.6 megawatts of power at its command, HAARP is the most powerful ionospheric heater in the world. At its heart is a phased-array radar that emits radio waves that are partially absorbed between 100 kilometres and 350 kilometres in altitude, accelerating electrons there and 'heating' the ionosphere (see graphic, right). In effect, HAARP allows scientists to turn the ionosphere, the uppermost and one of the least understood regions of the atmosphere, into a natural laboratory. It is one of several ionospheric heaters scattered around the world. The facilities create unique opportunities to study the fundamental physics behind how plasma and electromagnetic waves interact. Researchers have already used HAARP to create an artificial aurora and otherwise study the basic physics of how charged particles behave in the ionosphere. Experiments have been ongoing for several years, but the facility didn't reach full power until last June. As yet it may be too early to assess whether its research potential has been worth the time and money invested in it, particularly given the ever-changing justifications for building it. The facility, which has been passed around varying military agencies, including the Office of Naval Research, the Air Force Research Laboratory and the Defense Advanced Research Projects Agency (DARPA), is perhaps the only research facility that has had to justify itself as being neither a death beam aimed at Russia nor a mind-control device. So prevalent are the conspiracy theories that HAARP has even been referred to in a Tom Clancy novel, in which a fictional facility is used to induce mass psychosis in a Chinese village. In fact, HAARP is a unique case of cold war-era military goals meshing with scientific research, and then maintaining that linkage even after the end of the war. If the conspiracy theories surrounding HAARP draw on fantastical ideas of death beams, then the real history of the facility is almost as colourful.  \n                Death beams and submarines \n              HAARP traces its origins back to cold war-era concerns over nuclear annihilation, when US and Soviet submarines prowled the deep seas, engaged in an elaborate game of hide and seek. By staying underwater, the submarines avoided detection, but they also couldn't communicate well \u2014 the deeper they went, the weaker the contact signal became. Then, in 1958, Nicholas Christofilos, a physicist at the Lawrence Livermore National Laboratory in California, proposed using extremely low frequency (ELF) waves to communicate with submarines underwater. His idea, adopted as Project Sanguine, eventually led to the development of operational facilities in Michigan and Wisconsin. But these were mired in controversy. They were huge \u2014 needing 135 kilometres of antenna wire to transmit the signal \u2014 and many took exception to their goals and to the possible detrimental effects on the health of people living nearby. The Navy eventually closed them down in 2004, saying that they were no longer needed. Another approach to ELF submarine communication was to take advantage of electrojets \u2014 currents of charged particles that flow through the ionosphere and could act as a virtual antennas, transmitting messages to submarines. Once this idea was proven experimentally 1  in the mid-1980s, physicist Dennis Papadopoulos, then of the Naval Research Laboratory in Washington, DC, began trying to drum up support for a new facility. At the time the Pentagon was shutting down over-the-horizon radar sites that had been designed to detect Soviet bombers attacking the United States \u2014 including one in Gakona, an ideal location because it is underneath an electrojet. So Papadopoulos, who is now at the University of Maryland in College Park and has served as a scientific adviser for HAARP since the project's inception, argued for building an ionospheric heater there. The facility would help the Navy to study ELF waves, it would provide scientists with an ionospheric heater and it would guarantee continued life for the military site in Alaska, something that Alaskan Senator Ted Stevens, famous for steering congressional dollars to his home state, also liked. \u201cThat,\u201d says Papadopoulos, \u201cwas the genesis.\u201d But even before construction began, people started to speculate about what the facility could be used for and why it was being built. In a news conference in 1990, Stevens talked about bringing energy from the aurora borealis \u201cdown to Earth so it could be used\u201d to solve the world's energy crises, earning him the mockery of physicists. Others such as Nick Begich, the son of another Alaskan lawmaker, began claiming that HAARP was really intended as a missile defence weapon. According to Papadopoulos, these claims, although far-fetched, were based on a sliver of truth: Bernard Eastlund, a consultant to one of the firms building HAARP, had filed a series of patents making extraordinary claims that HAARP-like technology could be used as a defence shield by transforming natural gas into microwaves, which would knock out incoming Soviet missiles. The idea, jokingly dubbed the \u201ckiller shield\u201d, was even reviewed by the JASON defence advisory group, but was dismissed as \u201cnonsense\u201d, according to Papadopoulos.  \n                From annihilation to defence \n             With the breakup of the Soviet Union, submarine communications no longer seemed as crucial, and HAARP needed a new  raison d'\u00eatre.  Supporters proposed new tactics, such as studying ELF waves' ability to map out underground bunkers like those found in North Korea, a goal that quickly drew scepticism. After the terrorist attacks of 9/11, however, the military found a new use for HAARP. In 2002, a panel headed by Anthony Tether, the director of DARPA, recommended that the facility be used to study ways to counter the effects of a high-altitude nuclear detonation, which would release energetic electrons that could cripple low-Earth satellites. Electrons are produced naturally in this region when the solar wind, a stream of energetic particles flowing from the Sun, slams into the magnetic envelope that protects Earth. The planet has its own self-cleaning mechanism to rid itself of the particles: it eventually dumps them lower into the atmosphere through natural auroras and lightning. Scientists are now looking at whether they can accelerate this process by creating 'whistler' waves, which would kick the electrons into low enough altitudes \u2014 around 100 kilometres \u2014 where they would rain out naturally. No one knows for sure whether it will work. \u201cIt is what we call a data-starved area \u2014 theory is ahead of actual observations,\u201d says Paul Kossey, HAARP's programme manager at the Air Force Research Laboratory at Hanscom Air Force Base, Massachusetts. Several experiments are being done to look at this possibility. Stanford University in Palo Alto, California, for example, is involved in the One Hop Experiment, which uses HAARP to inject very-low-frequency waves into the magnetosphere to create whistlers. The investigators use a buoy and ships in the South Pacific, where the waves fall back to Earth, to measure the presence of whistler waves 2 . Mitigating the radiation from an atmospheric nuclear detonation would require an entirely new facility, and the technology would be daunting. In 2006, a New Zealand-led group of scientists published a paper 3  arguing that any attempt to remediate radiation could lead to worldwide blackouts of high-frequency radio waves, disrupting communications and navigation. And some say that countering such high-altitude nuclear detonations is simply unrealistic. \u201cI think scientific research to better understand Earth's ionosphere is a worthwhile endeavour,\u201d says Philip Coyle, a former associate director of the Livermore laboratory who served as the Pentagon's chief weapons tester during the administration of President Bill Clinton. But, he adds, they don't know how much energy they would need to flush the electrons, or how, ultimately, injecting this much energy would change the ionosphere. In the meantime, there are plenty of straightforward science questions for HAARP to look into. The ionized part of the atmosphere has long captivated researchers, going back to the days of Nikola Tesla, who dreamed of using it to send electricity around the world. In 1933, scientists found that changing the electron density in the ionosphere could alter the propagation of radio signals 4 . That discovery eventually led to the development of ionospheric heaters to study these and other effects.  \n                Bells and whistles \n              Radiation from solar flares is one area of interest. \u201cThese things are really important because it is the radiation coming off the Sun that is the main cause of satellite failure or potential death in human space exploration,\u201d says Michael Kosch, the deputy head of the communication systems department at Lancaster University, UK. Other areas include looking at the processes that cause an aurora \u2014 when electrons in the magnetosphere collide with the uncharged particles of the atmosphere, creating the optical emissions often seen as brilliantly coloured lights in the night sky. One of HAARP's most cited accomplishments is the creation of the first artificial aurora visible to the naked eye 5 . On zapping the ionosphere, HAARP created a green aurora between 100 and 150 kilometres high \u2014 in the middle of a natural aurora. \u201cThat was something you couldn't predict,\u201d says Michael Kelley, a physicist at Cornell University in Ithaca, New York, who has been involved with HAARP. Other ionospheric heaters around the world include a lower-power US facility in Arecibo, Puerto Rico, which has been offline since a flood several years ago (although plans are under way to refurbish it), and one in the Russian city of Vasilsursk, which has struggled with funding issues. HAARP's closest peer is a powerful ionospheric heater at the European Incoherent Scatter (EISCAT) Scientific Association in northern Scandinavia. EISCAT's heater has cost roughly $24 million to build and operate to date, and was the first to create an artificial aurora, even before HAARP. HAARP, though, has the highest power as well as the most advanced optics and diagnostic equipment. But most of all, its phased-array radar means that the signals can be steered and controlled digitally. It can also create multiple beams, which can be shaped, or changed instantaneously to sweep north, south, east and west. \u201cI think the main thing that makes it unique is that it has a much wider frequency operating range,\u201d adds Kosch, who has also worked extensively at EISCAT. HAARP operates between 2.8 and 10 megahertz, whereas EISCAT operates between 3.9 and 8 megahertz. \u201cIt can operate in a much lower frequency range than the one we can use here in Europe,\u201d Kosch says. As HAARP was only finished in 2007, scientists and Pentagon officials involved in the project concede that management issues, such as allocating time at the facility, are still in the formative stages. In fact, one of the most recent HAARP experiments is something that's not likely to show up in the scientific literature at all: an experiment done in January that involved sending radio waves to the Moon and then having amateur radio enthusiasts and a receiving antenna in New Mexico measure the reflected signals. But Papadopoulos says that the experiment was more for the amateur radio community than for scientists. At the moment, time at the facility is divided between researcher-directed work, which takes place during 'campaigns' of two to three weeks, and military needs. \u201cIt's a fairly complicated situation in which we support new researchers, and new people, by getting them involved in the campaigns, which is relatively cheap,\u201d says Kossey. \u201cThen of course we also fund [military] proposals and contracts that come in under broad agency announcements, in which researchers propose research that is of interest to the various organizations.\u201d And even though HAARP is a military-owned facility, academics say that access has not been a problem. Umran Inan, the lead scientist for the Stanford work, says that Stanford has been one of the most frequent users, with numerous graduate students and foreign scientists working at the site. \u201cObviously, there are security arrangements, because it's a US Department of Defense facility,\u201d says Kosch. \u201cI'm a foreigner \u2014 escort required \u2014 but I am already so familiar to the people there, and so familiar with the facility, that it's not really a major problem.\u201d HAARP's evolution may not have been straightforward, but it is, in the minds of many scientists who work there, a success. \u201cHAARP has been a boon to science in this area, and I think the managers that run HAARP, from the very beginning, have involved the community,\u201d says Inan. So unlike many other Department of Defense facilities that are built before there is a clear rationale, \u201cin this case the community was involved from the very beginning, so the properties of the facilities were all defined with the involvement of the community. Now, I think it's a thriving success,\u201d he says. As for HAARP's original legacy, as an antenna to send signals to submarines, that era has come and gone with the end of the cold war. \u201cThe communications for submarines is not as important any more,\u201d says Papadopoulos. \u201cThere are,\u201d he acknowledges, \u201cno submarines from the other side.\u201d Sharon Weinberger is a freelance writer in Washington DC. \n                     HAARP \n                   \n                     EISCAT \n                   Reprints and Permissions"},
{"file_id": "453022a", "url": "https://www.nature.com/articles/453022a", "year": 2008, "authors": [{"name": "Philip Ball"}], "parsed_as_year": "2006_or_before", "body": "How does our classical world emerge from the counterintuitive principles of quantum theory? Can we even be sure that the world doesn't 'go quantum' when no one is watching? Philip Ball talks to the theorists and experimentalists trying to find out. Keith Schwab builds bridges. By most people's standards, they are very small bridges indeed: around 8 thousandths of a millimetre long and 200 millionths of a millimetre wide, visible only under a microscope. But to Schwab's eye, these objects are huge. That's because he is hoping to see them behave according to the rules of quantum mechanics \u2014 rules that allow for bizarre, counterintuitive behaviour such as being in two places at once. Quantum mechanics is generally thought to govern objects such as individual atoms, not lumps of matter like these bridges, which contain tens of billions of atoms. It is an ambitious goal. But Schwab, based at Cornell University in Ithaca, New York, is one of several experimentalists seeking to probe one of the great conundrums of modern physics: the quantum\u2013classical transition. Here, the fuzzy quantum world somehow gives way to the solid, definite certainties of the everyday 'classical' world as we go up the scale from atoms to apples. If these experiments manage to confirm current theories of this transition, they could turn long-standing preconceptions about quantum theory on their head. Early quantum theorists treated the quantum\u2013classical transition almost as a kind of sleight of hand, something that had to be imposed on quantum mechanics to recover the familiar world. Now, however, there are strong signs that the transition can be understood as something that emerges quite naturally and inevitably from quantum theory. If that's so, it implies that 'classicality' is at root simply another quantum phenomenon. \"There's good reason to believe that we are just as much part of the quantum world as are the tiny atoms and electrons that sparked quantum theory in the first place,\" says quantum theorist Maximilian Schlosshauer of the University of Melbourne in Australia. Testing the new description of the quantum\u2013classical transition involves experiments on systems ranging from photons to superconductors to microscopic vibrating beams. These efforts pose an extreme challenge to experimentalists, as they involve looking for very small effects on comparatively big things \u2014 rather like trying to detect the sag when a fly lands on San Francisco's Golden Gate Bridge. The effects very quickly get so small that many physicists believe it is absurd to try to see them. \"One crowd says: 'Of course it will work \u2014 quantum mechanics says so',\" says Schwab. \"The other says: 'There's no way it will work \u2014 these guys are nuts'.\" Yet there's now some urgency to the search: understanding the transition is becoming crucial for the emerging field of quantum information technology, which includes quantum computing and ultra-secure data encryption and transmission. Such practical applications may depend on an ability to hold classical behaviour at bay, sustaining the quantumness at big scales. In part this is a technical challenge, which the experiments on the quantum\u2013classical transition might help to address. But it is also a challenge to our understanding of the basic theory. Some physicists think that how quantum behaviour should be interpreted remains as unclear today as it was when Niels Bohr, Werner Heisenberg, Albert Einstein and others were developing (and arguing over) the theory. \"This is an emotional issue,\" says quantum technologist Chris Monroe from the University of Maryland in College Park. \"Some insist that there is no problem; others insist that there must be an infinity of universes, each with their own classical description of a definite quantum state.\" (see refs  1  and  2 .) As explorations of the quantum\u2013classical transition become more sophisticated, it may become increasingly feasible to put such ideas to the test, answering questions that until now have remained almost metaphysical.  \n                Where does the weirdness go? \n              To understand what the quantum\u2013classical transition really means, consider that our familiar, classical world is an 'either/or' kind of place. A compass needle, say, can't point both north and south at the same time. The quantum world, by contrast, is 'both/and': a magnetic atom, say, has no trouble at all pointing both directions at once. The same is true for other properties such as energy, location or speed; generally speaking, they can take on a range of values simultaneously, so that all you can say is that this value has that probability. When that is the case, physicists say that a quantum object is in a 'superposition' of states. Thus, one of the key questions in understanding the quantum\u2013classical transition is what happens to the superpositions as you go up that atoms-to-apples scale? Exactly when and how does 'both/and' become 'either/or'? Physicists have proposed many answers to that question over the decades. But the most favoured one involves a phenomenon known as decoherence 3 , which was identified and elucidated in the 1970s and 1980s. Crudely speaking, decoherence is a sort of leaking away of quantum behaviour when a particle interacts with its surroundings \u2014 for example, when an atom or molecule collides with those around it, or when light bounces off it. All we are left with is a partial picture of the system \u2014 a picture in which only a well-defined subset of macroscopic properties, such as position, are apparent. Why those particular properties? In the 1980s, Wojciech Zurek, now at the Los Alamos National Laboratory in New Mexico, offered an answer. In effect, different quantum states have very different resistances to decoherence. So only the resistant states will survive when a system interacts with its environment. These robust states are those that feature in classical physics, such as position and its rate of change, which is associated with momentum. In a sense, these are the 'fittest' states \u2014 which is why Zurek and his colleagues call their idea quantum darwinism 4 ,   5 . Decoherence also predicts that the quantum\u2013classical transition isn't really a matter of size, but of time. The stronger a quantum object's interactions are with its surroundings, the faster decoherence kicks in. So larger objects, which generally have more ways of interacting, decohere almost instantaneously, transforming their quantum character into classical behaviour just as quickly. For example, if a large molecule could be prepared in a superposition of two positions just 10 \u00e5ngstroms apart, it would decohere because of collisions with the surrounding air molecules in about 10 \u221217  seconds. Decoherence is unavoidable to some degree. Even in a perfect vacuum, particles will decohere through interactions with photons in the omnipresent cosmic microwave background. In summary, decoherence offers a way \"to understand classicality as emergent from within the quantum formalism\", says Schlosshauer. Indeed, this picture means that the classical world no longer sits in opposition to quantum mechanics, but is demanded by it. It all sounds good in theory \u2014 but is decoherence real? Can we actually see quantum effects leaking away into the environment? Serge Haroche and his colleagues at the Ecole Normale Sup\u00e9rieure in Paris began to put the idea to the test in 1996 by studying bunches of photons held in a type of light trap called an optical cavity 6 . They passed a rubidium atom through the cavity in a superposition of two states. In one state, the atom interacted with the photons to cause a shift in their electromagnetic oscillations, whereas the other state caused no change. Then, when the experimenters sent a second atom through the trap, it was affected by the state of the photons induced by the first atom. But that effect became weaker as the photon field decohered, so the measurements made on the second atom depended on the time delay since the passage of the first. In this way, Haroche and his colleagues could watch decoherence set in by altering the timings of the two atoms 7 .  \n                Sliding doors \n              The decoherence description shows that there is no abrupt boundary, no critical size, at which quantum behaviour switches to classical. And that blurry boundary itself shifts depending on how it is measured. \"It is the choice of the measuring apparatus that defines whether a specific object is quantum or classical,\" says Anton Zeilinger of the University of Vienna. His team provided an example of this nine years ago, when it demonstrated quantum interference between beams of C 60  fullerene molecules 8  \u2014 hardly as classical as the footballs they resemble, but nonetheless big molecules that can be seen with an electron microscope. Interference \u2014 the addition or cancellation of overlapping waves \u2014 is in this case a purely quantum effect, and can't be understood if the molecules are viewed as discrete particles. It is possible only if the molecules are in a superposition of states \u2014 in several places at once. \"If you scan with a scanning tunnelling microscope a surface to which fullerene molecules stick, you see the little soccer balls sitting there as classical objects,\" says Zeilinger. \"But if you choose our interference experiment set-up, they are quantum mechanically delocalized.\" In other words, he says: \"The same object can behave as a quantum system in one situation, and as a classical system in another.\" This makes the fullerene experiment an ideal quantitative test of the theory of decoherence. In a subsequent experiment with even bigger C 70  molecules, Zeilinger's team saw that the molecules' interference patterns gradually disappeared as the gas through which the molecules passed became more dense. In a denser gas there are more collisions, and thus more coupling to the environment, so the molecules decohere more quickly. The researchers found that the decay of quantum coherence and interference happened at precisely the rate predicted by the theory. They have now seen similar behaviour with even more massive molecules, including fluorinated C 60  (C 60 F 48 ) and modified porphyrin molecules with a plate-like shape (C 44 H 30 N 4 ) (ref.  9 ). \"The important point is that the conditions for an observation of interference effects can be precisely specified and quantified using theoretical decoherence models,\" says Schlosshauer. \"We no longer have to limit ourselves to the assumption of a poorly defined yet fundamental divide between the quantum and classical realms.\" Individual molecules still seem a long way from our classical world. But decoherence has also been watched in genuinely macroscopic objects: hoops of superconducting material. Superconductivity is an inherently quantum-mechanical behaviour, and electrical currents in a ring of superconducting material called a superconducting quantum interference device (SQUID) can be placed in a superposition of states that circulate in opposite directions 10 ,   11 . These states can then be monitored by looking at the interference between them. In 2003, researchers at Delft University of Technology in the Netherlands showed that a related approach, using pulses of microwaves to excite oscillations in a superposition of superconducting current states, could also be used to investigate decoherence 12 . As in Haroche's experiments, decoherence shows up as a weakening of the correlations between oscillations of the superposition states as the time between pulses lengthens. Superpositions in superconducting currents may still seem a little remote from the apparent absurdity of finding tangible objects, much bigger than atoms or molecules, in two places at once. But Schwab and his collaborators think that it should be possible to detect such quantum superpositions in 'rough' lumps of matter: their tiny resonating beams, which are examples of nanoelectromechanical systems (NEMSs). These objects are small enough that their vibrations should be governed by quantum mechanics: they should be restricted to specific energy levels, and thus specific frequencies 13 . But the separation between energy levels is very small, so quantum behaviour will be blurred unless the resonators are kept very cold to eliminate thermal noise. Schwab says that he thinks he is now very close to seeing a NEMS resonator in its lowest-energy (ground) quantum state. At temperatures of around 25 millikelvin, he and his colleagues have managed to restrict the vibrations to only the 25 lowest energy levels of the system, and he says he hopes to 'suck out' all the remaining excited states by active cooling, akin to the laser cooling used to draw heat from small clusters of ultracold atoms. The Cornell researchers have proposed an experiment to search for superposition states in NEMS resonators 14 . As in other quantum systems, Schwab says, \"as soon as you look for the superposition, you destroy it\". So the superposition can only be inferred from its effect on another system to which the resonator is coupled. Schwab's group will look at how interactions between the resonators and a nearby quantum bit or 'qubit' \u2014 a quantum device that can exist in two states, like the binary memory elements of a computer \u2014 induce decoherence in the qubit. This type of coupling will have a particular signature, says Schwab: coherence will reappear periodically at a rate equal to the vibration period of the beam. \"That's the smoking gun,\" he says. This would reveal quantum effects in a system containing around ten billion atoms \u2014 much more than is feasible with molecular-interference experiments. Even more ambitious, however, is a proposal by Dirk Bouwmeester of the University of California, Santa Barbara, and his colleagues to create superpositions in the position of macroscopic mirrors moved by the radiation pressure of a single photon in a superposition state 15 . Inevitably, this would involve extremely high-precision measurements: the researchers calculated that for a cube-shaped mirror measuring 10 micrometres in each dimension, with a mass of 5 trillionths of a kilogram and containing about 10 14  atoms, they would have to measure positions to within 10 \u221213  metres \u2014 close to the width of a proton, but nevertheless potentially feasible with interferometric methods.  \n                Smoke and mirrors \n              Bouwmeester hopes that it will soon be possible to test an alternative theory of the quantum\u2013classical transition devised by Roger Penrose of the University of Oxford, UK. Penrose suggests that the 'collapse' of a superposition, rather than being a gradual affair resulting from environment-induced decoherence, is a rather abrupt event that is mediated by gravity. That is, it involves the emission of a graviton, the hypothetical fundamental quantum of the gravitational force, in much the same way that the decay of an excited molecule may happen via emission of a photon. He thinks that the cost in gravitational potential energy of keeping objects in a superposition becomes too great as objects get bigger, so that the objects 'go classical' on a definite timescale, which he estimates to be about a second or so for dust particles. Bouwmeester's mirror experiment should, if he can scale it up without losing sensitivity, be able to spot such a switch. \"I am sceptical about this idea, but think it is worth testing,\" he says. Understanding quantum states has become much more than an intellectual curiosity, because handling quantum data may hold the key to the future of information technology. By exploiting the extra degrees of freedom that superpositions offer, quantum computing could greatly increase computer power, at least for some computational problems. And encoding data in entangled quantum bits (the polarization states of photons, say) can provide a secure way to send information, as intercepting or reading the data would trigger irreversible quantum collapses that should be detectable. So understanding decoherence, the coupling of quantum systems to their environment and the fundamental nature of quantum measurement becomes crucial to manipulating information along quantum paths. \"To build a quantum computer,\" says Schlosshauer, \"the main challenge remains to shield the computer sufficiently from the environment to minimize decoherence while keeping it sufficiently open to allow its control from the outside. Research on quantum computing has inspired many ways of actively controlling decoherence and even of effectively 'undoing' its effects.\"  \n                Too much information \n              At first glance, quantum computing seemed to be sunk by the fact that superpositions can't be measured without destroying the 'information' they encode. But about 10 years ago researchers found that, as long as the background decoherence level in a quantum computer is small enough, quantum information stored in qubits can be retained by redundant encoding. \"Several qubits are used to store just a single effective qubit of information,\" says Monroe, \"and particular measurement schemes allow full recovery of quantum information in the face of decoherence.\" One of the challenges in engineering these devices is identifying what is causing the decoherence. \"The particular way to encode qubits will depend on the details of the decoherence source,\" Monroe explains. \"Some decoherence sources are much easier to recover from than others.\" If these efforts succeed in scaling up quantum computing from the mere handfuls of qubits demonstrated so far to truly macroscopic systems, the result will be not only a boon for computing itself but a forceful illustration of how quantum effects can survive at scales normally deemed 'classical'. \"Some day we will have a roomful of equipment that will have to be considered a single quantum entity in the same way as a single atom is,\" says David DiVincenzo, who works on quantum computing at IBM's research labs in Yorktown Heights, New York. On the other hand, maybe in pushing the envelope of the quantum\u2013classical transition, quantum information technology will uncover some unanticipated, fundamental aspect of it. \"If in the process of building a large-scale quantum computer we find that decoherence always sets in at some level of complexity \u2014 a universal law that says quantum mechanics only goes so far \u2014 then that would be very interesting,\" says Monroe. Zurek, meanwhile, sees a deeper significance in quantum information technology: he says it shows that information lies at the core of quantum theory. \"Taking information seriously has profound consequences,\" he says. \"Perhaps the most important is that there is no information without representation \u2014 a physical state of some object is needed to store information.\" Concepts such as decoherence and quantum darwinism make sense only because information matters, he says. \"They all have to do with how information flows.\" The decoherence description of the quantum\u2013classical transition is not necessarily the end of the matter \u2014 it leaves unresolved some more fundamental questions about the interpretation of quantum theory (see  page 39 ). But at present, it seems a fair bet that what we think of as the classical world is really only the quantum world viewed through the lens of decohered states. \"The conceptual leap would then be to conclude from this that quantum mechanics is truly universal,\" says Schlosshauer, \"in the sense that everything, including us, is described by entangled quantum states.\"   See Essay,  \n                     page 39 \n                    , and News and Views,  \n                     page 50 \n                    . Reprints and Permissions"},
{"file_id": "453018a", "url": "https://www.nature.com/articles/453018a", "year": 2008, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "Can a state do what a country cannot, and transform the way stem-cell research is funded? Erika Check Hayden reports on the California Institute for Regenerative Medicine. Evan Snyder could hardly believe what he was hearing. About four years ago, Snyder, a neuroscientist and paediatrician, was sitting in the Los Angeles living room of Hollywood mogul Jerry Zucker, the producer and director behind movies such as  Airplane!  and  The Naked Gun.  Zucker's wife, Janet, and a mix of fellow scientists, Hollywood heavyweights, political staff and patient advocates were also there. And they were throwing around numbers that astounded Snyder, director of a stem-cell research programme at the Burnham Institute for Medical Research in La Jolla, California. Snyder had been invited to the meeting to discuss what he thought was an idea to raise US$3 million for stem-cell research. That seemed like an enormous sum. Yet as the meeting progressed, it became clear that he had been mistaken. \u201cThey said, 'You're missing the point \u2014 the $3 million is to be able to get this on the ballot and get it passed for a bill that's gonna be $3 billion',\u201d he says. \u201cAnd I thought, I really am in la-la land now. Words like 3 billion dollars don't come out of the mouth of a scientist.\u201d If $3 billion seemed like a dream four years ago, it is now a reality that is changing not only the way science is done in California, but is resonating across the US biomedical landscape. Since August 2001, when President George W. Bush banned federal funding for research on human embryonic stem-cell lines created after that date, six other states have passed initiatives to fund such work using state money. But none of them is even remotely close to the scale of the California Institute for Regenerative Medicine (CIRM), the research agency brought into being after California voters passed a ballot initiative in November 2004. All the other states together have pledged just over $1 billion for stem-cell work (see  Nature   451,   622\u2013626;  2008). Now, after giving about $260 million through 123 research or training grants to 22 institutions (see 'Top 10 CIRM-funded institutions'), the CIRM is about to distribute an additional $262 million to finance a network of major buildings. Next week, its oversight committee will make final decisions on recommendations made by a working group about where the money should go. The state's universities stand to receive major boosts to their already powerful biomedical infrastructures, and have been anticipating this 'major facilities' initiative more eagerly than any other since the birth of the CIRM. But even as the agency is changing California's scientific outlook, it is also facing pressure to prove its worth to voters \u2014 and to show that it can deliver the medical and economic benefits it promised in order to convince taxpayers to fund it in the first place.  \n                Grand plans \n              On an ocean bluff next door to the Salk Institute for Biological Studies in La Jolla, an eroded strip of unused road winds through a 3-hectare grove of eucalyptus trees. Overhead, seagulls fly through a chilly afternoon fog towards the shore, where paragliders are launching themselves off the edge of a cliff. If everything goes to plan, in two years the eucalyptus trees will be gone. In their place will be a 12,700-square-metre building housing 280 scientists, who will be able to look out from their lab benches and watch the gliders drift by. This site is the future home of the San Diego Consortium for Regenerative Medicine, one of seven 'CIRM institutes' likely to be awarded funding next week \u2014 and one that would not have happened without prominent local philanthropists such as John Moores, Malin Burnham and Irwin Jacobs. Scientific institutions have long relied on philanthropy to some extent, but the CIRM was born of their generosity. Individual donors contributed the bulk of the $26 million in donations that backed the $35-million campaign for Proposition 71, the ballot initiative that created the CIRM. Private donors again came to the agency's aid after the initial state bond offering to fund the agency was held up by lawsuits filed by groups opposed to stem-cell research. And in 2006, philanthropists lent the CIRM $45 million, $14 million of which bankrolled its first round of grants before governor Arnold Schwarzenegger (pictured, left, in research mode) came to the rescue with a $150-million state loan. The litigation finally ended last year, freeing California to begin selling bonds. For the imminent major facilities awards, the CIRM required institutions to come up with matching funds \u2014 which led San Diego scientists, such as Snyder and Larry Goldstein, stem-cell research director at the University of California, San Diego (UCSD), to Moores' door a few years ago. Moores, a software magnate, gave the scientists an imperative: he would give them the money to transform that eucalyptus grove next to the Salk only if they would build it together. Moores saw a joint CIRM building, housing scientists from San Diego's four premier research institutions, as something more than a monument to science; it would be a point of civic pride for San Diego. In 2006, Moores put up $6 million, and the San Diego Consortium for Regenerative Medicine was born, comprising UCSD, the Salk and Burnham institutes and the Scripps Research Institute. On 5 April, the CIRM's working group recommended that the consortium be awarded the agency's second-largest facility grant \u2014 $43 million \u2014 to help build a $115-million shared building. The consortium is an example of how California's philanthropists have become heavily involved in shaping the agency \u2014 and how scientists, in turn, have tapped into the state's private wealth. In exchange, Californian universities have benefited enormously. The CIRM working group recommended that Stanford University in Palo Alto could receive the agency's largest facilities grant, of $47.5 million, to build a $200-million building it has had in the works since 2001. About 65 kilometres north, architects for the University of California, San Francisco (UCSF), designed an elegant building cascading down the face of Mount Sutro \u2014 the hill that towers over the university's main campus \u2014 with a 'green' roof and an innovative system of split-level floors. For its bid, Stanford garnered at least $40 million from alumnus and Business Wire founder Lorry Lokey; UCSF hauled in a $16-million donation from Ray Dolby, founder of Dolby Laboratories, and his wife. Altogether, California institutions have committed and raised $495 million in matching and leverage funds towards the facilities awards.  \n                Luring scientists \n              When the new buildings are complete, perhaps as early as 2010, they will be stocked with some of the dozens of stem-cell scientists who have been lured to California by the CIRM. Some are high-profile hires. In 2006, Peter Donovan left Johns Hopkins University in Baltimore, Maryland, for the University of California, Irvine, and Martin Pera moved to the University of Southern California, Los Angeles, from the Monash Institute of Medical Research in Victoria, Australia. Pera's former colleague, Alan Trounson, become president of the CIRM last year. But the CIRM has also been a boon for scientists whose names are not so well-known \u2014 people such as Su Guo, a 37-year-old UCSF biologist, and a postdoctoral fellow working with her, 35-year-old Julio Ramirez. \n               boxed-text \n             At 9 a.m. on a Tuesday morning, Guo and Ramirez study a petri dish of mouse embryonic stem cells in a lab at San Francisco's J. David Gladstone Institutes. The space is so jammed with fume hoods, refrigerators and microscopes that there is almost no room to move around, and five other scientists, some of whom work for CIRM-funded investigators, are jostling for space around Ramirez. But he doesn't mind the crowd \u2014 he has been studying animal stem cells for only four months, and the others are helping him learn the ropes. \u201cI can ask them anything,\u201d Ramirez says. Before this year, Ramirez studied plant stem cells, and Guo studied brain development in zebrafish. Yet they are now working with mouse and human embryonic stem cells, thanks to a $564,000 grant from the CIRM. Guo's grant was one of 72 that the agency gave out last February as part of the $45-million Scientific Excellence through Exploration and Development (SEED) awards, designed to support scientists who have never worked with stem cells. With her grant, Guo hired Ramirez. First, Ramirez will look for chemicals that have positive effects on fish brain cells that produce a neurotransmitter called dopamine. Then, he will test these drugs on human neurons derived from embryonic stem cells to see whether they might treat symptoms of Parkinson's disease, which are caused by a dopamine deficiency. Guo has good funding support from the National Institutes of Health (NIH) \u2014 about $800,000 a year for her zebrafish studies. But her CIRM money is letting her do something she has always wanted to do \u2014 test her work in human cells. \u201cI always wanted my work to be applicable to patients,\u201d she says. As Guo and Ramirez examine the cells under a microscope, there is disappointment. The day before, Ramirez had injected the cells with a plasmid \u2014 a piece of DNA \u2014 that codes for a green fluorescent protein, so today the cells should be glowing green. But they are stubbornly pale. Guo quizzes him: is he growing the cells using the right ingredients? Did he use the correct plasmid? Culturing stem cells is sometimes more art than science, as Ramirez is discovering, and even when you follow all the rules, sometimes things still don't work. Even once Ramirez becomes expert at manipulating the cells, further obstacles could torpedo the project. \u201cWhat we've proposed to do is quite daunting,\u201d Guo admits. Which raises the biggest question about the CIRM: will scientists be able to deliver the results it promised?  \n                Delivering results \n             This is an urgent concern for the leaders of the CIRM, because it won the hearts of California voters by saying it would produce cures for a number of debilitating diseases. Robert Klein, the chair of the CIRM board who masterminded the campaign behind Proposition 71, has a son with diabetes. The group formed to support the proposition, Yes on 71, paid for advertisements featuring patients such as the late Christopher Reeve, who implored voters to \u201cstand up for those who can't\u201d from his wheelchair. That ad also referred to research done by Hans Keirstead of the University of California, Irvine, who, in the run-up to the vote, gave talks around the state featuring a video of a formerly paralysed rat walking, thanks to transplants derived from stem cells. Keirstead's advocacy drew criticism from scientists who pointed out that he hadn't published his work in a peer-reviewed journal, and accused him of hyping the potential of stem-cell research. (He eventually published a paper: H. S. Keirstead  et al. J. Neurosci.   25,  4694\u20134705; 2005.) Klein's coalition has now morphed into a group called the Americans for Cures Foundation, whose leadership includes former CIRM staff members and a patient advocate, Don Reed, who has a paralysed son and has lobbied heavily on behalf of the CIRM. (Reed had previously lobbied the California legislature, successfully, for money that helped fund Keirstead's experiments.) The group's stated mission is \u201cto support fellow advocates in the fight for stem-cell research and cures\u201d. In 2006, the CIRM adopted a strategic plan pledging to meet high expectations with numerous and specific breakthroughs within ten years \u2014 including \u201cclinical proof-of-principle that transplanted cells derived from pluripotent cells can be used to restore function for at least one disease\u201d, as well as additional clinical trials of \u201ctherapies based on stem-cell research\u201d. Yet such timelines will be difficult, if not impossible, to meet. The Tufts Center for the Study of Drug Development in Boston, Massachusetts, for example, has estimated that the average drug takes 7 years to develop, at a cost of $800 million, whereas biotechnology products require an average of 8 years and $1.2 billion. The CIRM has only 10 years and $3 billion. And, unlike drugs, stem-cell therapies have not yet been shown to work in humans, so they will almost certainly require a larger investment than a typical drug. No clinical trials of treatments derived from embryonic stem cells are yet under way, but Geron, a biopharmaceutical company in Menlo Park, California, has said it will ask the US Food and Drug Administration for permission to begin the first one this year. \n               boxed-text \n             Further lengthening the odds, the CIRM has already been criticized for exaggerating its role in research. Last month, for instance, the agency claimed credit for enabling a clinical trial to treat a group of blood cancers called myeloproliferative disorders. The trial is based on research described in an article published on 8 April (I. Geron  et al. Cancer Cell   13,  321\u2013330; 2008). On that date, the CIRM issued a statement claiming that an agency SEED grant to one of the authors \u2014 Catriona Jamieson at UCSD \u2014 had contributed to the article, and to the clinical trial. But the article was submitted last September \u2014 the same month Jamieson received her first SEED funding cheque. Still, the CIRM stood behind its statement, explaining that one of Jamieson's postdoctoral fellows had been drawing funding from an agency training grant since July 2006, and that the SEED grant was used to help the lab perform follow-up experiments requested by reviewers. And Jamieson calls the CIRM's funding \u201cvital and instrumental\u201d.  \n                Instrument of change \n             Agency leaders are also trying to shorten and cheapen the clinical-development process. For instance, they require CIRM institute applicants to describe how they plan to move discoveries into clinical trials. And next year, the CIRM plans to award up to $20 million per team in 'disease team research' awards, which will fund groups of researchers and doctors to try to move into clinical trials. \u201cWe have to be an instrument of change,\u201d says Trounson. \u201cBecause if every single stem-cell treatment costs $800 million, we're just never going to get there.\u201d The CIRM's structure is supposed to help keep it focused on cures. For instance, it gives patient advocates the power to make decisions about research right alongside scientists and businesses: on its 28-member board, 10 seats are set aside for patient advocates, 12 for Californian research institutions and universities, and 4 for businesses. But the CIRM's structure has, at times, seemed to hamper its own mission. That was painfully evident at a meeting in January, when one doctor found himself begging for funding from 13 board members who were competing directly against him for money. The doctor, Bertram Lubin, is head of the Children's Hospital Oakland Research Institute (CHORI), a hospital-affiliated entity that draws about $48 million in NIH funding a year. Lubin says that he applied last October for a $5-million facilities grant to build labs where CHORI scientists could perfect and expand their method for using human-cord blood cells to treat sickle-cell anaemia and thalassaemia. The labs would also allow them to study whether placental cells could cure both diseases, which are blood disorders that disproportionately affect African Americans and other ethnic minorities. Because CHORI already had good clinical and preclinical data, and was asking for far less money than other institutions, Lubin's friends at other universities thought he was a shoo-in. So he was stunned when the CIRM's peer reviewers gave CHORI one of the lowest scores in the entire competition.  \n                Conflicts of interest? \n              The public review of Lubin's proposal stated that reviewers found several faults with it, including a lack of work on embryonic stem cells, no clear vision for integrating the preclinical and clinical work, and the fact that the \u201cclinical program was reviewed as only incrementally advancing the field \u2026 In summary, reviewers felt these proposals would be more appropriate for individual translational or clinical research programs,\u201d the review stated. To Lubin, it seemed like a classic example of how clinical-research proposals often fare poorly with peer-review panels composed largely of basic scientists. Yet the CIRM was supposed to be different. \u201cThis was supposed to be taxpayer dollars for cures,\u201d he says. So he decided to make his case directly to the CIRM board. But at its 16 January meeting, board members first argued over whether Lubin should be allowed to speak, and then debated whether he should be given three or ten minutes to make his comments. Finally, Lubin was allowed to speak in a three-minute comment. \u201c[This] would bring cures to patients in this state within the first year or two, and when you report on what CIRM has done, this will be a major accomplishment,\u201d he argued. Lubin seemed to win over the patient advocates on the board, such as Jeff Sheehy, a communications officer at UCSF, who represents people living with HIV/AIDS. \u201cWe're going to make a lot of rich people richer. Why don't we cure somebody?\u201d Sheehy said at the time, according to a meeting transcript. But when the board voted on CHORI's application, it was defeated, ten to five; four of those who voted against it represented universities that had applied for facilities funding. Klein argues that Lubin's application was trying to fit a square peg into a round hole. \u201cHe was trying to use a facilities grant process to fund a clinical grant,\u201d Klein says. But Lubin sees it differently. \u201cWe're not in the 'in' crowd,\u201d Lubin says. \u201cSo a project that was really going to go into patients was essentially triaged.\u201d The episode is only one in a series of incidents that have raised questions about the wisdom of putting the institutions that benefit from the CIRM in charge of governing it. Last December, for instance, CIRM board member John Reed, president of the Burnham Institute, said he would recuse himself from the board pending an investigation by California's Fair Political Practices Commission. The commission is investigating a letter that Reed sent last August, excoriating CIRM staff for excluding a Burnham adjunct professor from a grant competition. Ten grant applications had to be removed from the same competition because they included letters of support from members of the CIRM board. Both situations were inappropriate, CIRM staff said, because board members are not supposed to influence decisions that affect their own institutions. That is not the way that Klein hoped the CIRM would be a model when he drafted Proposition 71. And he has far bigger dreams for the agency, ones that hinge on working the kinks out in the California system first. Klein is looking to use the CIRM as a model to rethink the way Congress funds the nation's biomedical research through the NIH \u2014 he would like to see stable funding for many years, rather than the current system of year-by-year appropriations. Improbable as that hope may be, Klein cites baby steps that he thinks make it less unlikely. For instance, before the CIRM, no US government had sold bonds to finance research; now, not only has California done so, but Klein is lobbying the federal government to switch from taxable to tax-exempt bonds for this purpose. Klein says he has powerful friends on Capitol Hill, including Democrats Nancy Pelosi of California, speaker of the House of Representatives, and Harry Reid of Nevada, the Senate majority leader. In 2002, Pelosi and Reid supported Klein's quest to convince both houses of Congress to pass a unanimous consent measure, requiring the support of every member of the House and Senate, to fund $1.5 billion in diabetes research in the waning days of that year's congressional session. Never before had a unanimous consent measure been used to fund medical research; Klein says the victory \u201cdemonstrates that patient advocates have the potential to do some remarkable things\u201d. At 62, Klein seems unwilling to let go of the agency he brought into existence; there are some who speculate that he might be laying the groundwork for a political campaign in the future. And although it seems improbable that Klein could change the way Congress does business, four years ago, there were many \u2014 including people such as Evan Snyder, sitting in that Los Angeles living room \u2014 who felt the same way about Proposition 71. And next week, $262 million will show just how wrong they were. See Editorial,  page 1. \n                     Nature Reports Stem Cells \n                   \n                     CIRM \n                   \n                     Guo lab \n                   \n                     Bertram Lubin at the Children's Hospital Oakland Research Institute (CHORI) \n                   Reprints and Permissions"},
{"file_id": "453442a", "url": "https://www.nature.com/articles/453442a", "year": 2008, "authors": [{"name": "Laura Spinney"}], "parsed_as_year": "2006_or_before", "body": "A major, but flawed, study of identity parades, or line-ups, has set science and the police at odds. Laura Spinney investigates. In 1981, 22-year-old Jerry Miller was arrested and charged with the kidnap, rape and robbery of a woman in downtown Chicago. After two eyewitnesses to the crime picked him out of a line-up, and the victim identified him at his trial, he was convicted and sentenced to 45 years in prison. In March 2007, however, semen on the victim's clothes was subjected to DNA testing and found not to belong to him. His conviction was quashed a month later. He had spent more than 24 years in jail. Miller's story is not unique: his was the 200th conviction to be overturned in the United States on the basis of DNA evidence. According to the Innocence Project, an advocacy group based in New York that campaigns to overturn wrongful convictions, and that took on Miller's case in 2005, the number today stands at 216, of whom 16 spent time on death row. Mistaken eyewitness testimony was a factor in more than three-quarters of those cases. Those exonerations have highlighted the fact that police procedures for eyewitness identification in the United States are out of step with almost three decades of psychological research, and have triggered a row over whether the procedures should be reformed. As the situation is similar, or worse, in most other developed countries, the rest of the world is watching closely. The traditional US procedure is familiar to any fan of television cop shows. Witnesses are presented with a line-up that includes both the suspect and a number of innocent people, or 'foils', and are asked to identify the perpetrator. In the early 1990s, however, when the confidence of the justice system had been badly shaken by the first wave of DNA exonerations, the then attorney-general, Janet Reno, invited experts to form a working group to address how this method could be improved. The group immediately homed in on the fact that most line-ups are overseen by the case's investigating officer, who knows the suspect's identity. For scientists, this is a major error: even something as seemingly objective as a clinical trial can be affected if the nurse who administers the injection knows whether the syringe contains a drug or a placebo. It is all but impossible for an experimenter \u2014 or an investigating officer \u2014 to avoid giving away the 'right' answer through body language, tone of voice or other such unconscious hints. \u201cI have argued for years that the more important reform is for line-ups to be conducted double blind,\u201d says Gary Wells, a psychologist at Iowa State University in Ames and a member of Reno's original working party. Witnesses should also be told that the perpetrator may not be in the line-up so that they do not feel obliged to identify someone. In every one of the DNA exonerations that involved mistaken identity, says Wells, the witness had picked the suspect: \u201cIt's just that the suspect was innocent.\u201d Although the real perpetrator was not in the line-up, the witness somehow ended up picking the person the detective had in mind. The working group's most important recommendation was that line-ups should be conducted in a double-blind fashion, so that neither the witness nor the official overseeing the procedure would know who the suspect was. The group also recommended that the suspect and foils be presented sequentially rather than simultaneously, and that the witness be asked to make a decision after each one rather than waiting until the end. This would encourage witnesses to compare each individual to their memory of the offender, rather than to one another \u2014 a method the scientific data suggested was more likely to produce a correct identification. In light of these recommendations, several US states reformed their eyewitness identification procedures. However, according to James Doyle of the John Jay College of Criminal Justice in New York, the recommendations encountered pockets of resistance, one of which was in Illinois. Illinois also happens to have one of the highest DNA exoneration counts in the United States, 28, which in 2000 led the state to impose a moratorium on the death penalty until the causes of its wrongful convictions had been identified and eliminated. That moratorium is still in place. Nonetheless, Illinois police tended to view the working group's guidelines not as safeguards against wrongful convictions, but as hopelessly academic and impractical. So in 2003, the Illinois State Police commissioned its own study 1  to test line-ups under real-world, field conditions, and put it under the direction of lawyer Sheri Mecklenburg, general counsel to the Superintendent of the Chicago Police Department.  \n                Crime watch \n              The inherent problem with field studies in this area \u2014 and the reason that so few have been done \u2014 is that there is no way of knowing for sure whether the suspect is guilty. In a mock crime staged before witnesses in a lab, experimenters know that the suspect is the perpetrator. But in the real world, all that can be said for certain is that the foils did not do it. Hence, the measure that experimenters pay most attention to is how often witnesses identified a foil as the perpetrator. With this in mind, and with the cooperation of two psychologists and three of the state's police departments \u2014 Chicago, Evanston and Joliet \u2014 Mecklenburg's Illinois Pilot Program spent a year conducting some 700 eyewitness identifications. Some of the procedures were non-blind and simultaneous; the rest were double-blind and sequential. Both conditions were a mix of live line-ups and photo arrays. The team found that the double-blind, sequential technique produced higher rates of foil picks \u2014 that is, clear errors \u2014 and lower rates of suspect picks than the traditional, non-blind line-up. Mecklenburg concluded that the existing system should not be changed. That conclusion, announced in 2006, immediately sowed confusion in the dozen US states that had been considering updating their procedures. One state, New Jersey, and several smaller jurisdictions had already passed reforms, which remain in place. But some states put their reforms on hold, or even threw them out. Horrified, psychologists pointed to deep methodological flaws in the study, which was never submitted to peer review. According to the psychologists, the decision to change two variables at the same time \u2014 blind/non-blind and simultaneous/sequential \u2014 made it impossible for Mecklenburg and her colleagues to draw any conclusions from their data. \u201cIf one of my undergraduates came to me with that experimental design, I would say 'Go away and do it correctly',\u201d says Daniel Wright, psychologist at the University of Sussex in Brighton, UK. Writing in the journal  Law and Human Behaviour 2 , psychologists Daniel Schachter of Harvard University in Cambridge, Massachusetts, and Nobel laureate Daniel Kahneman of Princeton University in New Jersey and their colleagues were unanimous that the design flaw \u201chas devastating consequences for assessing the real-world implications of this particular study\u201d. After that critique was published, a handful of US jurisdictions did reform their procedures, and others set up task forces to consider the options. But for Stephen Saloom, policy director at the Innocence Project, that wasn't enough. \u201cGiven that the vast majority of jurisdictions nationwide have yet to adopt the reforms, and that eyewitness identification has a role in a set of crimes far greater than those in which DNA can prove innocence or guilt, it is of great concern that the majority of jurisdictions are simply sticking with what they have always done,\u201d he says. Last year, the National Association of Criminal Defense Lawyers in Washington DC and the MacArthur Justice Center at Northwestern University in Chicago, Illinois, both of which back reform, sued the police departments that took part in the pilot programme to release their data so that they could have them reanalysed. So far the departments have refused, and Mecklenburg calls the lawsuit \u201ca waste of energy\u201d. She says it has only alienated the police, making it less likely that they will cooperate with field studies in future. Regarding the study itself, Mecklenburg has stood her ground. She argues that pristine lab studies capture none of the tension and gravity of an actual line-up. Moreover, although admitting that the study's design was not ideal, she argues that she and her collaborators were doing their best given the operational realities of police work. One problem is manpower. If another person replaces the investigating officer to coordinate a line-up, then that extra person has to be taken off their other duties. Time and complexity are also issues. For the 40% of crimes in the Illinois Pilot Program that involved more than one perpetrator, the police found the sequential technique too cumbersome and abandoned it halfway through the study. Furthermore, real-world witnesses are often reluctant to cooperate. Their willingness to participate can hinge on the investigating officer's ability to build trust \u2014 a relationship that could be broken if the witness suddenly had to deal with a stranger conducting the line-up 1 . That's one reason that modern 'line-ups' are often taken to the witnesses and conducted in the field, by showing them arrays of photos.  \n                Ongoing dispute \n              Despite these difficulties, however, Mecklenburg claims that her results are still valid \u2014 even though another field study 3 , conducted in Minnesota at around the same time, supported the lab data. And so the dispute rumbles on. Everyone agrees that more field studies are needed, and several are under way in the United States, including one commissioned by the Department of Justice. In the meantime, some support for the beleaguered Illinois study has come from an unexpected quarter \u2014 Britain, which prides itself on its eyewitness identification procedures. These are now routinely done in specialized suites, in the form of video parades. Video images of the suspect and foils are presented to the witness sequentially, but witnesses are asked not to give a decision until they have seen the whole sequence twice. In a study published last year 3 , psychologist Tim Valentine of Goldsmith's College at the University of London, UK, and his colleagues applied strict double-blind sequential rules to British-style video parades. This was a lab study, but with what psychologists call high ecological validity \u2014 a measure of how well the experimental conditions match those in the real world. Witnesses saw a live, staged theft and attempted to identify the perpetrators a week later from a video parade constructed by police from their database. Valentine's most striking finding was that the number of correct identifications dropped from 65% under British rules, to 36% under the strict rules. \u201cSo there is a big cost in terms of sensitivity,\u201d he says. Under strict rules, \u201cpeople are inhibited when it comes to choosing\u201d. The Illinois study also found lower sensitivity, but it was overlooked in all the furore about the experimental design. If the effect is real, the implication is that the proposed reforms might reduce the wrongful conviction rate \u2014 the number of false positives, or type I errors \u2014 but at the cost of increasing false negatives, or type II errors. In other words, more criminals might be let off the hook.  \n                Uncertain benefits \n              Valentine wholly supports the introduction of double-blind line-ups, but he has serious reservations about the sequential technique. Although his group found a reduction in false positives with sequential line-ups, it was not statistically significant, at least in Britain. \u201cIn other words, we are taking a cost for a benefit we can't really be sure about,\u201d he says. Saloom disagrees, arguing that a slightly different picture has emerged from US research: for every three wrongful identifications of innocent people avoided with the sequential technique, you might let one guilty party go free. The Innocence Project understands the reluctance of law enforcers to lose that one correct identification. However, Saloom says, \u201cUltimately we believe that it's smarter, and a better way to protect the public, to make that larger trade-off.\u201d Identity parades of one form or another are used in many countries. But Britain has benefited from centralized coordination of police practices through the government's Home Office, and is regarded by many countries as advanced in the way it extracts identifications from eyewitnesses. This is in large part due to its history. In 1976, a government inquiry chaired by high-court judge Lord Devlin combed British case law, searching for wrongful convictions based on mistaken identity 4 . DNA evidence wasn't available then, but the committee highlighted cases in which other evidence undermined witnesses' identification. The recommendations made by the committee were strongly influenced by the scientific data. For example, they picked up on the need to tell witnesses that the perpetrator might not be in the line-up. Perhaps the most important outcome of the Devlin report, however, was to encourage a climate of cooperation between police and scientists. \u201cI go and talk to cops and they are very interested in what I have to say,\u201d says Wright. This spirit, in turn, has led to an ongoing process of consultation and reform \u2014 the most significant in recent years being the shift from live line-ups to video parades in 2004. Among the benefits this reform has brought is that the parade can take place sooner after the crime, because there is no need to marshal a suspect or foils. The use of DNA in a forensic context was a British invention, and it is used in a wider range of crimes in Britain than in the United States \u2014 not just serious crimes such as rape and murder. Perhaps surprisingly, therefore, no one has systematically collected data on DNA exonerations in Britain. But if they had, Wright believes, those data would show that the British system, too, is far from perfect. Now that parades take place in specialized facilities with trained officers, they are sometimes done in a blinded fashion. But the practice is still not compulsory in the United Kingdom. A spokesman from the Home Office says that before blind parades would be incorporated into the Home Office's codes of practice, they \u201cwould require evidence to support the need for change at operational level\u201d. Indeed, perhaps the one clear lesson from this controversy is that no system is perfect. As Mecklenburg has pointed out, any reforms that psychologists propose must be workable in the real world \u2014 and they won't be unless police are given the resources and incentives required to make them work. But as Doyle points out, operational difficulties are no excuse for rejecting a fairer system. Nor are they an excuse for letting policy be swayed by a single, flawed study such as the Illinois Pilot Program. Moreover, he says, this tension urgently needs to be resolved, because the real problem isn't the study. It is the deeper clash of scientific and law-enforcement cultures. \u201cThis is really the first of the social science by-products of the DNA exoneration cases,\u201d he says. \u201cSo there is a battle being fought now about whether science is going to be allowed to affect practices.\u201d Although mistaken eyewitness testimony is a major cause of wrongful convictions, it isn't the only one. Others include false confessions, the use of convicted informers and flawed or fraudulent scientific evidence. If the scientists back down now, Doyle warns, they will be setting a dangerous precedent. Laura Spinney is a freelance writer based in London and Paris. \n                     Gary Wells' web site \n                   \n                     The Innocence Project \n                   \n                     John Jay College of Criminal Justice \n                   Reprints and Permissions"},
{"file_id": "453583a", "url": "https://www.nature.com/articles/453583a", "year": 2008, "authors": [{"name": "Nick Lane"}], "parsed_as_year": "2006_or_before", "body": "Programmed cell death is usually seen as the unique prerogative of plants and animals. So how is it that photosynthetic plankton have been killing themselves by uncannily similar methods for billions of years? Nick Lane investigates. One evening 20 years ago, Paul Falkowski left the lab so tired that he omitted to refresh the solution in his culture flasks of  Emiliania huxleyi,  one of the world's most widespread coccolithophores. The following morning he was shocked to find the flasks full of clear solution, the merest sediment lining the bottom, all that remained of the plankton. \u201cI had never seen anything like it,\u201d he recalls. \u201cThey just dissolved overnight.\u201d The speed and totality of their demise couldn't be put down to run-of-the-mill mortality, which leaves a gory mess of living cells, dying cells and clumps of dead matter. The tidy dissolution of the plankton was more reminiscent of apoptosis in animals \u2014 a synchronized wave of death orchestrated by some invisible hand to some unknowable end. Falkowski became determined to reveal that hand and understand its purpose, and now, in his lab and others, the revelations are beginning to add up. Now a professor of marine biogeochemistry at Rutgers University in New Jersey, Falkowski is better known for his research on marine productivity and nutrient cycles. From that mainstream perspective, his interest in programmed cell death among plankton might seem a quirky morbid streak. Dying, though, is something phytoplankton do a great deal of. Phytoplankton \u2014 bacteria and eukaryotes that photosynthesize, such as coccolithophores and diatoms \u2014 fix as much carbon every year as all the plants on all the continents. Yet at any one time they account for just 1% of Earth's biomass. This means their rate of turnover is huge; on average, the world's phytoplankton population is replaced once a week. Most models of marine systems simply put that mortality down to bad luck. They tacitly assume that phytoplankton are in principle immortal, but in practice always seem to be eaten by zooplankton, wiped out by viral infections or starved by nutrient deprivation. But Falkowski thinks that there is more to it than that. Phytoplankton don't just dissolve when neglected in the lab. Vast marine blooms, too, can disappear overnight. If that process is indeed a manifestation of programmed cell death, then it has implications that not only touch on the global nutrient cycling, but also foreshadow the ultimate causes of our own mortality. The phytoplankton of the ancient oceans might have been some of the first creatures to learn how to die. Falkowski's hunch that the dissolution of his phytoplankton might have parallels with apoptosis in animals has since turned out to be remarkably close to the mark. Apoptosis is masterminded with extraordinary finesse by a group of protein-splitting enzymes known as caspases. The scissor-like ability to cut proteins that sets these enzymes apart is constrained to very particular sequences of amino acids. They operate in cascades in which one caspase activates the next by slicing through a protective protein sheath. Each step amplifies the death signal until an army of executioners has been let loose on the cell, dismantling membrane structures, slicing up DNA, dicing proteins. This carefully orchestrated cascade leaves tell-tale signs at the microscopic level. The nuclear material condenses; chromosomes fragment; the cell membrane blebs off into tiny bubbles; internal organelles disintegrate; then the cell dissolves around them. Working with Kay Bidle, a marine microbiologist also at Rutgers, Falkowski found that cultured phytoplankton precisely reproduced every microscopic step. The pair also found that antibodies raised against human caspases cross-reacted with extracts taken from phytoplankton, suggesting that the extracts must contain proteins similar to the caspases 1 . Furthermore, molecules known to be specific inhibitors of caspases interfered with the cells' death programme. Remarkably, the evidence of programmed death was not constrained to eukaryotes; it was found in photosynthetic cyanobacteria too. By the late 1990s, similar findings had been reported in plants. But no one had managed to identify the caspases presumed responsible, either in the plants or in the plankton. Then in 2000, researchers at the National Center for Biotechnology Information in Bethesda, Maryland, and the biotechnology firm Genentech in San Francisco, California, discovered a related family of proteins by analysing genes from a wide range of species for sequences that might produce caspase-like activity when expressed 2 . The  in silico  discovery of these 'metacaspases' paved the way to understanding programmed cell death in plants, algae and fungi, and offered the first real insight into the origin of the caspases, now known to be found only in animals.  \n                Cosmopolitan lifestyle \n              Within a couple of years, Eugene Koonin and Lakshminarayanan Aravind, two of the researchers on the original discovery team, had sketched out the protein family's distribution across the tree of life 3 . In doing so, they confirmed the molecules' presence within a few groups of bacteria that show a touch more complexity than most of their peers, notably the photosynthetic cyanobacteria, which sometimes form multicellular chains. It was already clear, Koonin recalls, that the metacaspases and caspases derived from the same ancestor. Identifying that ancestor, though, is hard. Bacteria have very fluid chromosomes, and frequently incorporate genes derived from totally unrelated bacteria. So although cyanobacteria have the richest metacaspase reservoir, and thus might seem a likely point of origin, two other groups of bacteria have metacaspase genes and so need to be considered: the actinomycetes (a group of soil bacteria) and a few of the \u03b1-proteobacteria. The most compelling account of the origin of mitochondria, the 'power plants' of almost all modern eukaryotic cells, is that they are descended from the \u03b1-proteobacteria. Metacaspases are widespread among eukaryotes, so they were probably acquired at an early point in eukaryotic history. \u201cThe fact that some \u03b1-proteobacteria contain metacaspases makes mitochondria the most likely source of ancestral metacaspases in eukaryotic cells,\u201d Koonin says. How the \u03b1-proteobacteria got their metacaspases, though, is an open question, and transfer from cyanobacteria is utterly possible. It is also possible that some eukaryotes received their metacaspases from two different lineages \u2014 once via the \u03b1-proteobacteria that became the mitochondria, and once via the engulfed cyanobacteria that evolved into chloroplasts. Bidle points out that some of the metacaspases found only in algae and plants are aimed at targets in chloroplasts, which hints they may have come in that way. But why do cyanobacteria have all those metacaspases? In their 2002 paper 3 , Koonin and Aravind argued that, in general, bacterial metacaspases have some sort of signalling role; but they also wondered whether, in a few more-complex 'cosmopolitan' bacteria, they might mediate cell death as the animals' caspases do. And that was exactly what Bidle and Falkowski were wondering, too. Metacaspases are in at least one sense not caspases: for one thing few, perhaps even none, cleave the classic caspase target sequence. For this and other reasons some researchers, notably Frank Van Breusegem and his colleagues at Ghent University in Belgium, doubt that metacaspases execute cell death in plants and phytoplankton \u2014 or at least whether the case is yet properly proved. Bidle and and Ilana Berman-Frank, at Bar Ilan University in Israel, have put some effort into that proof. They have shown that the number of metacaspases rises and that caspase activity sky-rockets during cell death in cyanobacteria such as  Trichodesmium  \u2014 a subtropical bloom-forming bacterium that might have evolved as long as three billion years ago. The microscopic demise of these cells looks like apoptosis \u2014 and the cyanobacteria in question turn out also to make several other key enzymes found in the cell-death cascade of animals. (Apoptosis-activating factor, apoptotic ATPases and NACHT-family NTPases, since you ask.) But this evidence is circumstantial, and the same team has also found a lot of metacaspases that are not involved in cell death. This February, for example, Bidle and his colleagues reported that  Thalassosiria pseudonana , a single-celled diatom, has six putative metacaspases 4 , not far short of the nine found in plants such as the thale cress  Arabidopsis,  or indeed the twelve caspases found in humans. Only two of these are activated during cell death, however. The rest are apparently expressed in healthy cells, or sometimes in stressed ones, as found by Koonin. \u201cAlthough it looks as though some metacaspases really are involved in cell death, they certainly seem to have biological functions that go beyond apoptosis,\u201d says Bidle. The clearest-cut evidence for metacaspases in cell death was published this January by Patrick Gallois from the University of Manchester, UK, and his co-workers, who showed through careful genetic engineering of  Arabidopsis  that at least one metacaspase does directly mediate cell death 5 . In light of these findings, as well as some older reports from yeast and phytoplankton, Gallois thinks that metacaspases are probably \u201cpart of an ancestral pathway that activates programmed cell death in response to a damaging level of oxidative stress\u201d.  \n                Stress relief \n              The tie-in with oxidative stress \u2014 a state in which there is more reactive oxygen around than a cell can tolerate \u2014 emphasizes another strong link with phytoplankton, says Assaf Vardi, a molecular biologist who joined Falkowski and Bidle at Rutgers in 2007. Ultraviolet radiation, carbon dioxide limitation, iron deficiency and viral infection can all trigger cell death in eukaryotic algae, he says. And what these things have in common are reactive oxygen species \u2014 superoxides, hydroxyls and the like. Long perceived merely as pathological by-products, these reactive oxygen species are now known to be used by cells as signals. Vardi has found that in phytoplankton, signals from reactive oxygen species are amplified by nitric oxide. This gas does a great deal of signalling work in plants and animals, and has a key role in inflammation, immunity and cell death. Vardi is the first to show that the gas is actively generated by phytoplankton, and he has even filmed it using fluorescence microscopy. Vardi's latest work shows that  Phaeodactylum tricornutum  \u2014 a diatom, like  T. pseudonana  \u2014 calls on an enzyme found only in chloroplasts to generate nitric oxide. Cells that overproduce that enzyme grow slowly and are highly sensitive to stress, activating as many as six metacaspases during the cell-death programme. The steps involved are eerily familiar to those who study apoptosis. Nitric oxide blocks the electron-transport chains needed for photosynthesis and respiration; that generates reactive oxygen species, and they in turn trigger the death cascade. This seems to show that Falkowski's hunch was correct. Stressed phytoplankton blooms generate nitric oxide, and reactive oxygen species then activate the machinery of death. This machinery includes (but is probably not limited to) metacaspases that are functionally related to the caspases found in apoptosis in animals. But why does so much of the cell-death apparatus associated with complex animals \u2014 in which cellular self-sacrifice can make obvious sense \u2014 operate in single-celled phytoplankton? What can a plankton possibly have to gain by killing itself? Falkowski suspects that the ultimate brokers of death are viruses. Sea water contains viruses in shocking abundance \u2014 hundreds of millions of viruses per millilitre of sea water \u2014 and phytoplankton are the targets of many if not most of them. Falkowski thinks that the death apparatus is part of an ancient tug of war between viruses and their prey. Falkowski draws an analogy to the 'addiction modules' found in simple bacteria such as  Escherichia coli.  In that case, viral genes in the cell encode a long-lived toxin as well as its short-lived antidote; cells that stop expressing the viral genes run out of antidote and die.  \n                Power struggles \n              Cell-death systems that use metacaspases might have evolved in plankton through similar processes, Falkowski says, with viruses turning metacaspases into dangerous weapons of subjugation. Host cells then evolve new systems to wrest the viruses' hands away from the trigger. The system might be seen as a coevolved product of the virus and the host, with each trying to control it. Last year, Bidle and Falkowski showed that viral replication can be blocked by inhibiting the death apparatus of the coccolithophore  E. huxleyi,  implying that the virus usually uses the system to kill the cell when death is to its advantage. The odd thing is that sometimes the cells may want to turn the death programme on when the viruses want to keep it turned off. If cells can kill themselves more quickly than suits their viral invaders, they can thwart the viral spread. As most plankton in a bloom are near identical genetically, from the perspective of their genes, a die-off that creates enough scorched earth to stop the viral advance can make sense. To understand this as self-sacrifice, though, might be to oversimplify. \u201cIt is at least as much murder as suicide,\u201d says Vardi. \u201cIt definitely blurs the boundary between altruism and selfishness.\u201d Vardi has found that injured phytoplankton release mediators \u2014 he calls them 'infochemicals' \u2014 into their surroundings. In response, damaged cells overproduce reactive oxygen species, activating the death apparatus, whereas in healthier cells the signals lead to several processes designed to deal with stress: the formation of tough, long-lasting cysts, the creation of biofilms on nearby surfaces, even differentiation into separate sexes. The system is assigning genetically identical cells different fates \u2014 rather as developmental programmes do in multicellular organisms. Death is, in a way, the simplest form of development \u2014 a binary choice. It may be that caspases and their cousins have been called on as managers of death time and again, in shaping tissues by killing cells or in weeding out sicker plankton to benefit the rest, because these are the settings in which cells with the same genes need to thin their numbers. The programmed-death function may have been inherited from the cyanobacteria along with the metacaspases used for it. But it is also possible that the cascade-friendly features of such molecules lend themselves so well to a self-destruct sequence that they get roped in whenever such a capability is called for. Either way, the links from today's complex world back to the origins of death in a simpler one seem oddly fortunate. The bacterial partners in the symbioses that led to eukaryotes might not have had anything as useful as metacaspases in their make-up. They might have been getting by with some simpler death system, such as an addiction module, or no death system at all. If so, eukaryotes could have started out without the cell-death mechanisms that are so crucial to their development. As Koonin puts it: \u201cMuch of the glorious ascension to the ultimate complexity of higher plants and animals is owed to a lucky choice of bacteria with complicated differentiation processes as partners in the origin of the eukaryotic cell.\u201d The complex seeds of death made life what it is today. Nick Lane is author of  Power, Sex, Suicide: Mitochondria and the Meaning of Life . \n                     Apoptosis Insight \n                   \n                     Paul Falkowski\u2019s team at Rutgers \n                   \n                     Eugene Koonin's group at NCBI  \n                   \n                     Nick Lane\u2019s website \n                   Reprints and Permissions"},
{"file_id": "452926a", "url": "https://www.nature.com/articles/452926a", "year": 2008, "authors": [{"name": "Bijal Trivedi"}], "parsed_as_year": "2006_or_before", "body": "Lyle Palmer has plans for a 'ludicrously ambitious' gene - disease research project. Bijal Trivedi reports on the trials at Joondalup. Every hour of Connie Colgan's day is carefully choreographed. Rising at 5:30 a.m. she prepares for a marathon of waking, dressing and feeding her six children before chauffeuring the eldest four to school. Sean and Conor, 3 years old and 19-months respectively, wreak havoc at home as their mother tidies, does laundry, cooks and then ventures out again to ferry the kids to swimming, Irish dance, piano and gymnastics classes. Gruelling as her schedule is, Colgan is adding another routine \u2014 she has enrolled her family in an ambitious new health study that will rank her brood among the most biologically characterized humans on Earth. After donating blood samples, from which DNA will be extracted and probed at one million locations with gene chips, Colgan's family, and potentially 80,000 other Western Australians, will enter a newly built facility where they will wind their way through a maze of tests: whole body scans, retinal exams, hearing tests, muscle strength, respiratory tests and more. In all, about 3,500 measurements will be made of each participant. Then, every three years, they'll do it again. The effort, planned to start next year, is one of the newest in a series of population databases, or biobanks. By storing DNA and blood samples as well as the medical and family history of each volunteer, biobanks provide a tremendous resource from which researchers have been able to extract risk factors for common diseases \u2014 such as obesity, asthma, depression and heart disease \u2014 that are deluging the healthcare systems of developed and developing nations. Genomewide association studies and more complex genome interaction analyses have begun to reveal ways in which these diseases might be treated. Beyond probing genes, however, the more ambitious of these biobank projects hope to account for the effects of diet and the environment and even such factors as state of mind. With these interactions becoming cheaper and more realistic to peruse, biobanking efforts have sprung up all over the world. They can be pricey, difficult to develop, ethically complex to navigate and their returns are only speculative. But none of this has deterred Lyle Palmer, the brains behind the efforts in Western Australia, from devising one of the most ambitious and cutting-edge biobank projects to date. For Palmer, chair of genetic epidemiology at the University of Western Australia in Perth, the perfect site for what he calls a \u201cludicrously ambitious\u201d project is Joondalup \u2014 a regional city north of Perth \u2014 where efforts to recruit participants and build excitement about the Joondalup Family Health Study have been ongoing since 2005. Palmer speaks quickly, especially when excited. And the prospects for epidemiological research in Western Australia excite him. The public-health system is one factor. It tracks the prescription-drug use of all residents. Plus, roughly 40 years ago the Western Australia state government passed a law that all public and private hospitals would share patient records with the health department. The data bank contains births, deaths, marriage registrations, emergency-department diagnoses, surgical history, and midwife, mental-health and cancer records. \u201cI don't know why they made that decision but it is a godsend that they did. And we are now the beneficiaries,\u201d says Palmer. Drug data and medical history of all the state's residents will provide a welcome context for the imminent flood of genetic data. Although the state lacks the centuries of genealogy that help power biobanks in Iceland or Utah, Palmer and his colleague John Bass, who studies health informatics at Curtin University of Technology in Perth, are cobbling together a genealogy of the state's residents that dates back to 1840.  \n                Line of volunteers \n             Joondalup's signature will be the scope of its medical testing. In planning the project, Palmer is pushing to not just build on other prospective epidemiology studies, but to blow them out of the water. \u201cHow about we just look at every single body system we can think of and take every single measurement?\u201d he says. Once the project is in full swing, Palmer's colleague Anne Pratt, who is leading the data collection, envisions that a new volunteer will enter the testing facility every 15 minutes. After a quick interview, questionnaire, and collection of blood, urine and saliva, each person will embark on a colour-coded 'throughput cycle' that will guide them through various tests. Among the tests they might take are dual-energy X-ray absorptiometry scans showing fat and body mass, brain positron-emission tomography scans, multi-slice computed-tomography scans, cardiorespiratory stress tests, hearing tests, and measurements of back muscles endurance, lung volume and bronchial responsiveness, more than 20 lengths, widths and circumferences, more than 50 blood chemicals and thousands of other variables. The University of Western Australia recruited Palmer from Harvard in 2003 to build and lead a world-class genetic epidemiology facility that would capitalize on the state's untouched genetic resources. He admits that \u201cthis wasn't the most attractive academic job or the most attractive from a financial point of view\u201d. But the tug of family, he says, and the desire to give back to the system that had given him a free education, led him back to Australia. And, if Palmer were going to leave Harvard, he was determined to create something that would rival all other epidemiology resources. Palmer describes his plan with the uncompromising confidence of a luxury-car salesman with no need to oversell. He should be practised. He has spent the past four years explaining to politicians, scientists and companies how troves of untapped medical data could be harnessed to make Western Australia the world's go-to region for gene hunting, drug testing and health research. The project has secured Aus$150 million (US$140 million) from local institutions and other collaborators that should carry the project for three years.  \n                Battle of the banks \n             \n               boxed-text \n             In the past 18 months, there has been something of a revolution in gene hunting business. \u201cWe found more genes for complex diseases in 2007 than in the entire history of the field,\u201d says Lon Cardon, a statistical geneticist and the newly appointed leader of the genetics division at GlaxoSmithKline (GSK). The discovery boom began as the quantity of genomic data hit a tipping point. After the human genome project was completed in 2001, another large project began to generate a map of human genetic variation by cataloguing single nucleotide polymorphisms (SNPs).This, in turn, laid the groundwork for the rise of genomewide associate studies that were geared to look at the frequencies of these SNPs in disease populations and make it easier to link genes to diseases. Biobanking is the next logical step to translate genetic data into clinical applications, says Cardon. At least 18 countries have launched or are planning population biobanks including Iceland, the United Kingdom, Estonia, India, Sweden, China, Mexico, Japan, Gambia, Canada and the United States. Pretty much any tissue collection is a biobank, and they vary wildly. Some are cohorts in which the volunteers provide a DNA sample and are followed up over time. Others are snapshots, surveys with a single sample and questionnaire but no follow-up. Sizes of biobanks range from just a few thousand to half a million recruits. DeCODE Genetics of Reykjavik, Iceland, has become synonymous with biobanking and gene hunting. It recruits participants through Icelandic physicians. Volunteers give blood, from which DNA is extracted, and the doctors share the diagnoses with deCODE. With this approach deCODE doesn't characterize the physical, health or behavioral qualities \u2014 the patients' 'phenotype' as opposed to genotype \u2014 but rather relies on the physicians' measurements. To date, the company has collected DNA and blood samples from 120,000 Icelandic citizens; 95% agreed to allow deCODE to use their DNA for any study approved by the national bioethics committee. The UK Biobank in Stockport, one of the largest planned biobanks, intends to recruit 500,000 adults aged 40\u201369 years. When volunteers visit an assessment centre each one gives written consent, completes a lifestyle questionnaire, enters a one-on-one interview to provide medical history, and then undergoes a medical examination and sample collection. Roughly 20 months ago the Children's Hospital of Philadelphia in Pennsylvania launched the Children's DNA Database. The goal: to collect the DNA of 100,000 of its child patients and scan for genetic markers associated with cancer, irritable bowel disorder, epilepsy or diabetes, for example. After just a year and a half, the hospital and its 29 satellite medical centres have amassed 48,000 blood samples with approximately 35,000 from children and 13,000 from parents. \u201cIt has been very successful,\u201d says Hakon Hakonarson, the director of the Center for Applied Genomics at the hospital and ex-deCODE executive. The response rate is high \u2014 only 10% of parents decline, and the DNA bank only requires a one-time contact with the family to collect blood. But if the parents agree, researchers can access the child's electronic medical records, which are updated every time the child visits the hospital or its satellites. Officials at the US National Institutes of Health (NIH) in Bethesda, Maryland, have also argued for the establishment of a 500,000 person biobank (see Nature 429,  475\u2013477;  2004). Some disagreed, and a number of factors have stood in the way, including the lack of a national healthcare system and linked data. Estimates have pegged the price tag of such a venture at around US$3 billion dollars, a figure that doesn't look likely to come from NIH's tight budget.  \n                Regulation and collaboration \n             The international consortium Public Population Project in Genomics (P3G) keeps tabs on 123 biobanks, mostly cohort studies, and tries to ensure that members follow basic legal and ethical guidelines. One of their major objectives is to encourage collaboration between biobanks and to foster data harmonization by standardizing questionnaires and research protocols. Such collaboration, however, has its downside. \u201cI find many of the cohorts designed today are very similar \u2014 same choices of phenotype,\u201d says Tom Hudson, scientific director of both P3G and the Ontario Institute for Cancer Research in Toronto. \u201cWorking too closely seems to have suppressed innovation.\u201d In this respect, isolation may have served Palmer well. Hudson, who was a visiting professor at the University of Western Australia two years ago, says he was struck by the diversity and novelty of phenotypes that Palmer's team will be measuring. \u201cThat's what impressed me.\u201d In addition to all the physical measurements, DNA and blood analysis, Palmer is also collecting data about lifestyle \u2014 health, behaviour, work, family, school and community \u2014 using Internet questionnaires that probe subjects as wide ranging as computer use, bullying, noise exposure and Australian values. Kari Stefansson, the chief executive of deCODE Genetics, calls Palmer's \u201cdeliberate brute-force phenotyping\u201d excessive. But no one has optimized the biobank formula, yet. \u201cIt is so early in the knowledge of the genome that we have to cast a wide net because we don't know what we are going to catch,\u201d says Teri Manolio, director of population genomics at the National Human Genome Research Institute at the NIH. \u201cOne of the challenges with a study like this is not picking what to include but what to leave out.\u201d DeCODE certainly leads the pack when it comes to linking genes, gene variations and loci to specific complex diseases. But its ethical blunders, say many in the field, have more demonstrated how not to launch a biobank. In 1998, the Icelandic government granted deCODE a 12-year exclusive licence to a central database containing the health records of all Icelanders. This, in combination with 1,000 years of genealogy and DNA from the Icelandic populations, would have proved a formidable tool for hunting disease-causing genes. The Icelandic Supreme Court overturned this decision in 2003 because such \u201cpresumed consent\u201d rather than informed consent was unconstitutional. In 1998, the drug firm Hoffmann LaRoche, based in Basel, Switzerland, struck a US$200 million deal with deCODE for the right to develop drugs based on deCODE's data. But the perception of a foreign company profiting from Iceland's medical database turned the stomach of many citizens. The company tried to calm the waters by promising free drugs and diagnostics to Icelanders. \u201cWe feel very strongly still today that the people who contribute data to our discoveries have some tangible benefits coming from it,\u201d says Stefansson. Other biobank efforts have steered clear of these informed-consent troubles. Few, if any, promise free drugs or diagnostics, fearing, as many bioethicists have pointed out, that such lures coerce sick individuals and their families. Palmer who says that deCODE began with a \u201ccrazy model\u201d insists that the Joondalup Family Health Study has taken adequate measures to protect its participants. Despite these reassurances, Michelle Kosky, executive director of the Health Consumer's Council of Western Australia, says that the participants are still vulnerable to human-rights violations. She is concerned that the database containing genetic data could be hacked, jeopardizing individual rights to income insurance protection and life insurance. And consent continues to plague many biobanks that include children, including the Joondalup study. \u201cWhen children turn into legal adults, they don't have a say how their DNA and medical records are used \u2014 they're gone, they're already out there,\u201d says Patricia Roche, a bioethicist at Boston University School of Public Health in Massachusetts. \u201cDo parents really have the right to do that?\u201d  \n                If you build it, will they come? \n              \u201cChange the world\u201d is the promotional motto emblazoned on the orange rubber bracelets that Joondalup-study participants wear, reflecting Palmer's belief that the discoveries will improve health around the globe. And, just as deCODE put Iceland on the map for biomedical research, creating local jobs and opportunities, he expects that the Joondalup study will do the same for Western Australia. He plans to do this by holding the data in what he calls a \u201ccharitable trust\u201d, with about ten managing academic and government institutions controlling who can use the biobank's services and funnelling the profits back into the national health system. Western Australians will handle the studies and run trials for collaborators. No data, or access to raw data, will be sold. Unappealing as that may sound to the pharmaceutical industry, the arrangement seems to work, says Eric Schadt, director of genetics at Rosetta Inpharmatics \u2014 a subsidiary of Merck \u2014 in Seattle, Washington. Schadt collaborated with deCODE to probe the genes that underlie obesity and says that the company was \u201cextraordinarily cautious\u201d about the protecting the privacy of Icelanders and only gave analysed results back to Rosetta. \u201cI don't think you need your hands all over the data to get value out of them,\u201d says Schadt. If, like deCODE, Joondalup provides \u201chigh-value, scientifically solid results\u201d that withstand scientific scrutiny, Schadt thinks that \u201cmost companies would be happy with that\u201d. They seem to be. The US Healthcare IT provider Cerner and IBM are already partners, and Merck, GSK and AstraZeneca have all been eyeing Joondalup. Dan Burns, senior vice-president of pharmacogenetics at GSK speculates that the Joondalup study will place gene-disease associations in context: \u201cThere are plenty of examples of genes or genetic risks associated with diseases. But we don't know what that means, so the game is rapidly moving to function.\u201d DeCODE led the way, says Burns, but it \u201cgot started around this belief that the goal was to identify the gene and I think the whole community has matured in its thinking. Finding a gene is a very important step along the way, but it is only a step.\u201d  \n                Power in the numbers \n              Joondalup, says Schadt, has the potential to be more powerful than deCODE. The key is in the in-depth phenotyping. \u201cWhy haven't we seen 15\u201320 highly replicated [genomewide association study] results for obesity like we have seen for diabetes? The reason, I think, is that it is an incredibly complex disease involving so many different parts of the system that the only way to really get a handle on that is to partition populations based on phenotype.\u201d The more phenotypes, the easier it is to stratify populations into subtypes and to get a handle on more complex diseases such as asthma or obesity. Phase 4 clinical trials \u2014 post-marketing safety surveillance \u2014 will also be possible, says Palmer. \u201cWe know what drugs everyone has been on for the past 20 years, and we will have characterized them more carefully than any population has been characterized. So we can look for the subtle effects of two drugs interacting.\u201d That's going to appeal to pharmaceutical companies hoping to avoid debacles such as the one over Vioxx. The greatest challenge that Palmer faces, says Troy Pickard, mayor of the City of Joondalup, is achieving the high participation rate. Low recruitment can stunt a biobank at its root. Lower-than-expected participation during the pilot phase caused concerns for the UK Biobank, although they say they are now on track to meet their goal by 2010. In Joondalup, Pickard says, the community is keen to participate and with good marketing Palmer should meet his goals. \u201cLyle is a very motivating guy,\u201d says Hudson, \u201chis drive is going to make this happen.\u201d With his three years worth of funding in hand, Palmer is anxious to begin the study. But, the project is in limbo until the participating institutes can agree on intellectual property and governance. \u201cWhenever you get lawyers involved there are delays,\u201d says Palmer, with a note of weariness that seems at odds with his upbeat demeanour. Palmer admits that every day, he wakes up and expects the whole project to fall apart, but somehow it continues. Colgan is also anxious for the study to begin. For a woman most definitely in control of her family's day-to-day schedule, health represents the only unknown. Colgan says that she knows which dietary and lifestyle choices are right for her family, but she's concerned about all the genetic factors that might be beyond her control. She says she'll do anything to ensure the best health for her children. As for all the testing? She says \u201cthe kids will probably like it\u201d. Bijal Trivedi is a freelance writer based in Washington DC. \n                     HapMap Web Focus \n                   \n                     The Argument for a US Cohort \n                   \n                     Joondalup Family Health Study (JFHS) \n                   \n                     Public Population Project in Genomics (P3G) \n                   Reprints and Permissions"},
{"file_id": "452520a", "url": "https://www.nature.com/articles/452520a", "year": 2008, "authors": [{"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "Low- and zero-energy buildings could have a huge impact on energy use and carbon emissions. We have the technologies, but if they are to mitigate climate change, green-building design must hit the mass market, says Declan Butler. \u201cIt felt surreal,\u201d says Karsten Voss, thinking back to January 2008 and the winter meeting of the 50,000-member international heating, cooling and ventilation research association, ASHRAE. \u201cHere we were sitting talking about zero-energy buildings, one of the biggest topics on the programme, inside a hotel that had no proper glazing or insulation \u2014 while it was \u221210 \u00b0C outside.\u201d It was a classic illustration of both the challenge and the promise of green architecture, says Voss, who should know. An architectural engineer at the University of Wuppertal in Germany, he also heads assessment of ultra-low-energy building demonstration projects built under the country's national programme on research for energy-optimized construction (EnOB). Germany has pioneered such research, thanks to its long-standing interest in the environment, and has some of the toughest energy-efficiency regulations around. Buildings worldwide account for as much as 45% of energy consumption, and a similar share of greenhouse-gas emissions. That makes buildings the biggest single contributor to anthropogenic climate change \u2014 a worse offender than all the world's cars and trucks put together. But it also makes the design of energy-efficient buildings a \u201cmonumental but essential task\u201d in the effort to mitigate climate change, according to a research roadmap published last November by the research committee of the US Green Building Council. Indeed, the latest report from the Intergovernmental Panel on Climate Change (IPCC) estimates that improvements in the energy efficiency of buildings could potentially reduce projected global carbon emissions up to 29% by 2020, and up to 40% by 2030. Moreover, the IPCC's estimates are deliberately conservative, based on a pessimistic view of how rapidly the building industry can reform its practices. If the reforms could somehow be accelerated, the reductions in fossil-fuel energy needs could be that much more dramatic. Such a speed-up is certainly possible \u2014 in principle. Voss and his fellow 'green-building researchers' have already developed and tested many of the necessary tools and strategies, to the point where they can now build homes, offices and other buildings that use 80\u201390% less energy than existing buildings. The most efficient of these structures are almost completely 'passive', meaning they require very little, if any, traditional heating or air-conditioning. Yet the overall comfort they provide is, if anything, superior to existing buildings. Nor is there necessarily a cost penalty: these ultra-energy-efficient buildings are often no more expensive to build than conventional structures, and work out far cheaper if energy bills during their occupation are taken into account. In practice, however, getting the construction industry to change its ways is a daunting prospect. Especially in the developed world, more than a century of cheap energy has divorced the architecture of buildings from energy considerations, and from their environment. Architects typically design whatever their client wants, mostly on the basis of cost or aesthetics, and then pass the design to engineers who have to make the building work and be comfortable \u2014 typically by bolting on energy-intensive air-conditioning, heating, artificial lighting and whatever else it takes. As a result, most builders and architects today haven't got a clue how to design buildings with ultra-low carbon emissions. And even if they did, there is the sheer scale and diversity of the trillion-dollar worldwide building industry \u2014 a vast labyrinth that involves as many different professions and sectors as there are sorts of wallpaper, plus an almost unlimited number of types of buildings, regulations, norms, materials and environments. In the face of all that, \u201cthe major impediments to increase energy efficiency in the building sector are institutional barriers and market failures rather than technical problems,\u201d notes a report on green buildings published last year by the United Nations Environmental Programme. \u201cEven if high-tech competence exists in most countries, the institutional and economic conditions have hindered the technical competence to be effectively applied in day-to-day design, construction and operation of buildings.\u201d Change is in the air, however. Green buildings, once the preserve of ardent environmentalists, are going mainstream. Architect Edward Mazria of Architecture 2030, a non-profit organization in Santa Fe, New Mexico, set a challenge in 2006 \u2014 a voluntary commitment to reduce the carbon consumption of all new and retrofitted buildings by 50% immediately, and to meet further reduction milestones to become carbon neutral by 2030. These targets can already largely be met using existing technology, argues Mazria, who has been practising sustainable architecture for 30 years. And indeed, the challenge has been endorsed by the US Conference of Mayors, several US cities, the American Institute of Architects and more than 400 other professional organizations. Meanwhile, the US Energy Independence and Security Act, which came into force in December last year, requires all federal buildings to meet stepwise goals almost identical to those of Architecture 2030's challenge. The act also introduces tougher energy targets for both commercial and residential buildings. In Europe and elsewhere, similar, or even tougher, targets are often being made legally binding on many buildings. There is a \u201csea change\u201d underway, says Gregor Henze, an architectural engineer at the University of Nebraska-Lincoln in Omaha. Carbon emissions weren't even on the radar of most architects five years ago, he says, adding that the number of architect firms, construction companies and other members of the US Green Building Council is \u201cexploding\u201d \u2014 it now has 13,500 member organizations, and 91,000 members, up some ten-fold since 2000. One driver is growing public concern over spiralling fuel prices and climate change, says Ken Baker, chief economist of the American Institute of Architects. The institute surveyed house purchasers' expectations at the end of 2007 and found that, despite the subprime mortgage crisis, most were willing to pay more for a house that would use less energy and be kinder on the planet. \u201cConsumers are becoming increasingly aware of energy-efficient options,\u201d says Baker, \u201cand they are requesting that architects incorporate them into the design and remodelling of their homes.\u201d  \n                Innovate and renovate \n              Meanwhile, green-building researchers are working hard to expand those options. The most obvious and urgent priority is to take energy into account during retrofitting. Although new construction in Europe and the United States annually amounts to around 1% of the existing building stock, about twice that many structures are renovated every year. Given the typical life cycle of buildings, each renovation that uses inadequate insulation or poorly glazed windows is an opportunity for lowering energy demand that is lost for decades. One demonstration project carried out at the J\u00fclich Research Centre in Germany shows what can be done with energy-efficient design strategies. Although the gains are not as impressive as those with new buildings, the renovations nonetheless cut the test building's total primary energy needs in half, from 1,235 kilowatt hours per square metre per year (kWh/m 2 /year) to 600 kWh/m 2 /year. Ultimately, however, the biggest pay-offs will come from new buildings, where ultra-low-energy use can be designed in from the beginning. This is particularly important for the developing world, and urban areas, where so much new construction is expected in coming decades. The goal in such ultra-low-energy design is to make buildings as 'passive' as possible, meaning they can satisfy most of their occupants' heating, cooling, and lighting needs from the outside environment \u2014 and then maintain the occupants' comfort as the outside cycles from day to night, summer to winter. More research is still needed, for example, to develop cheap glazing and insulation materials that can adapt themselves to changes in lighting and heating loads over the course of a day, or a season. Research is also vital to adapt existing solutions to different sorts of buildings or environments. Nonetheless, many of the technologies are already mature. Almost two decades of research, mainly in Europe, has resulted in designs for fully passive houses that require primary energy needs of 15\u201350 kWh/m 2 /year \u2014 compared with current-generation European houses that typically require 160\u2013300 kWh/m 2 /year \u2014 and need only supplementary heating or cooling, in tiny amounts, or just on the coldest and hottest days. Such efficiency levels make the buildings amenable to being made zero-carbon by meeting their low energy needs, either from small, local renewable sources, or using grid electricity generated from renewable sources. \u201cYou can do a lot with existing technology\u201d, says George Jeronimidis, an engineer who heads the Centre for Biomimetics at the University of Reading, UK, and who is developing next-generation building materials that adapt their shape, such as vents that open and close in response to humidity levels. \u201cWe are missing a lot of opportunities because we don't think about energy in the design phase of a building.\u201d  \n                Holistic house-building \n              This does require a different mindset, says Maria Wall, a green-building researcher at the University of Lund in Sweden who was involved in building passive terrace houses in Lind\u00e5s (see photo, right). \u201cGetting architects and engineers sitting down together from the outset is critical,\u201d she says. \u201cOne has to consider the building as a whole, and in the context of its environment.\u201d But new software is helping architects and engineers do just that, says Andreas Wagner, a building scientist who heads the architecture faculty at the University of Karlsruhe in Germany, and who is also a member of the design and monitoring team within the EnOB programme. The software provides a \u201ccommon language\u201d that allows architects and engineers to jointly quantify the energy implications of design decisions. Using databases and models of average temperatures, sunlight and solar radiation from different north\u2013south orientations, it allows them to pick from a supermarket of passive design strategies, techniques and materials to adapt designs to the local conditions. \u201cWe are now moving on from research and prototypes into the building market,\u201d says Wagner. That market is most mature for residential homes, which account for three-quarters of the energy consumption of the building sector. Passive houses currently make up only 2% of new buildings in Germany. But market research suggests that fraction will climb quickly as the necessary competences spread, and as consumers begin to take advantage of low-interest loans and other recent government incentives. Passive houses are also spreading fast in Austria, Switzerland and other European countries. Passive-house design uses extreme insulation of the building envelope \u2014 including triple-glazed windows, often filled with an inert gas \u2014 in particular, in an effort to eliminate what engineers call 'thermal bridges'. These are fault lines, typically at the window frames and at the intersection of floors and walls, where large, unwanted energy transfers occur with the outside. The result is an airtight 'skin' that prevents energy leakage. Thermograms of buildings then show impressive gains in 'nega-watts' (see photos, right). With insulation like that, the building can get its heating from the solar gains through glazing as well as through waste heat from appliances and even our bodies. Another key technique for temperature control in passive houses is at first counterintuitive: simply let fresh air in from the outside. A pump draws fresh air through a grid of pipes buried several metres underground, where the temperature is relatively constant throughout the year, 10\u201314 \u00b0C in the United Kingdom, for example. When this fresh air arrives at the house, its temperature has already been modulated \u2014 warmed up or cooled down by the ground, depending on the season. Then, in a second trick for heating, the incoming fresh air is put through a heat exchanger to recover 80% of the heat from the warmer stale air being expelled from the building. This system of air-based cooling and ventilation not only saves energy by recycling heat, but vastly improves air quality. Many of these same tricks can be used for office buildings and other large structures. However, these buildings tend to be more complex than residential houses, with many more interactions between different interior zones and higher heat loads from staff and machinery. So it is considerably more difficult to make them completely passive. Then, too, research in this area is relatively recent, even in Germany. Still, EnOB recently published results from the construction of 25 prototype passive office and other large buildings, and found that most kept energy for heating, lighting and cooling below 100 kWh/m 2 /year. \u201cThat's a factor of four smaller than existing equivalent buildings in Germany, and of six than in the United States,\u201d says Wagner. Three of the buildings reached energy consumptions of less than 50 kWh/m 2 /year. The EnOB study tested multiple concepts, including passive cooling. One such concept is night ventilation, which is increasingly being deployed in large buildings worldwide. The idea is to open automatically the skylights during the night, when the air in the building is warmer than the outside air. A natural breeze will then release the hot air out the top while drawing in cooler outside air from ventilation grills in the facade.  \n                Cool control \n              Another concept tested in the study was to circulate water in boreholes drilled up to 100 metres down, and then pass the water in closed circuits through concrete slabs in the ceilings of the building. The water was a relatively warm 17 \u00b0C. But there was so much of it that the cooling effect was comparable to that achieved by air-conditioning compressors. \u201cWe use the environment, the ground, as a heat sink,\u201d says Wagner. Many new German houses now use such slab cooling, or 'thermally activated building systems', which were pioneered in Switzerland. The buildings require only very small amounts of energy; all they need is a small pump. A multi-million-euro renovation of the United Nations offices in Geneva, Switzerland, will use a similar system, drawing water from Lake Geneva. These systems will work in even the hottest areas of California, Wagner maintains, although a small amount of conventional air conditioning might have to kick in during the late afternoon to keep temperatures below what people have become accustomed to. \u201cBut it would reduce most of the current needs for air conditioning.\u201d One problem is that the range of daytime temperatures achieved by passive cooling and ventilation are still less predictable than what conventional systems can deliver, in particular in large buildings; careful modelling and scaling are required to cover the possible extremes. As a result, risk-averse architects and engineers worried about potentially unhappy clients tend to stick with the safest option, and sell buildings that use central heating and air conditioning. \u201cThe need is to build more demonstration passive buildings, to show everyone from carpenters to house purchasers that it is not such a strange thing to build an energy-efficient house,\u201d says Wall, \u201cand that not only do these buildings work, but they are actually superior to conventional buildings.\u201d Fully passive designs can be adapted to hotter climates, although much less research has been done for those areas. In some cases, it works out cheaper to use a strategy based on renewable energy. That's the tack being taken by Jeff Christian, head of the Buildings Technology Center at the Oak Ridge National Laboratory in Tennessee, to design cheap, energy-efficient homes for low-income families. In Tennessee, he has to deal with not only heat, but also with high humidity. Underground passive ventilation systems bring in too much moisture for such environments. And although there are possible solutions, such as solar-driven dessication, this is currently too expensive. Nonetheless, says Christian, low-energy buildings for the mass market are already doable there. His focus is what he describes as \u201ca little bit of solar, and a lot of energy efficiency\u201d \u2014 that is, using better insulation and using solar panels. His building in Lenoir City, Tennessee (see photo, right), also maximizes cooling through natural cross-ventilation. The solar panels cost $20,000 upfront, he says, but he cut costs by mass-producing wall panels and other parts of the building. For all the progress, though, Christian, for one, is convinced that cheap, low-energy houses will take off in the United States only if the government wields both carrot and stick. \u201cThe financial incentives we need to drive this are not in place,\u201d he says, adding that tougher and compulsory energy requirements on buildings \u201cis the only way we will get industry to do the research and development to get there\u201d. Yes, he says, green buildings can potentially be built as cheaply as their conventional counterparts, if not more so. But those psychological and institutional barriers are still very real. If engineers, architects and builders have to struggle with, for example, unfamiliar green-architecture techniques, they will incur a heavy 'transaction cost' in the form of confusion, mistakes and delay. And if they have to add renewable energy, they will saddle the project with higher capital construction costs \u2014 which clients in the large rental sector will have little interest in paying, as it is the tenants who will reap the rewards in the form of lower energy bills. The mass-production of wall panels and other parts of the building advocated by Christian is also seen as a promising avenue in Europe. Cost can be reduced further by avoiding one-off designs and constructing an identical series of buildings. \u201cThe passive terrace houses we have here in Wuppertal are identical to those in [the German cities of] Wiesbaden or Hannover,\u201d says Voss. \u201cThe only difference is the colour.\u201d Nonetheless, it is still an open question how quickly passive architecture strategies, combined with renewable energy sources, will become the mainstream of construction. The good news is that the interest is there. Witness the turnout of 30,000 delegates in Chicago last November at the \u201cGreenbuild\u201d conference \u2014 which was opened by former US president Bill Clinton. On the last day of the conference, Maria Atkinson, head of sustainability at Lend Lease, a multinational real-estate company, threw out a challenge that should please Voss. \u201cThe hotels of Boston have 12 months to step up to the challenge of ensuring they are green for Greenbuild 2008, because this year's delegates will be demanding green hotels as part of their commitment to green buildings.\u201d \n                     Nature Reports Climate Change \n                   \n                     UN climate conference: Bali \n                   \n                     Climate politics \n                   \n                     Energy for a cool planet \n                   \n                     German national programme on \u201cResearch for energy-optimised construction\u201d (EnOB) \n                   \n                     Division of Energy and Building Design, Lund University, Sweden \n                   \n                     Architecture 2030 \n                   \n                     US Department of Energy's (DOE's) Building America Program \n                   \n                     12th International Conference on Passive Houses 2008 \n                   \n                     EU Intelligent Energy programme \n                   \n                     US Green Building Council (USGBC) \n                   Reprints and Permissions"},
{"file_id": "452406a", "url": "https://www.nature.com/articles/452406a", "year": 2008, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "Induced pluripotent stem cells look just like embryonic stem cells, but are easier to create and free of the heavy ethics baggage. David Cyranoski separates fact from fiction in a burgeoning field. Excited by their potential for biomedical research and therapy and lured by the ease with which they can be created, many researchers are looking into induced pluripotent stem (iPS) cells. Created from adult cells by a simple genetic trick, iPS cells seem to have regained an embryonic 'stemness' that might allow them to become any type of cell in the body. The concept is so appealing that some scientists and policy-makers even argue that related approaches such as therapeutic cloning and embryonic stem-cell research, which require the destruction of embryos, should be halted. But for biologists, iPS cells still present a black box. As resources pour in and patients' expectations rise, some scientists wonder whether the cells are being overhyped. Here,  Nature  looks at the status of the five most pertinent issues on people's minds.  \n                1. Anyone can do it \n              When Shinya Yamanaka and his postdoctoral student Kazutoshi Takahashi from Kyoto University in Japan discovered that four genes could reprogram adult mouse cells, they kept it secret for nearly six months. They stopped having weekly laboratory meetings, and Takahashi fibbed to colleagues about the status of his work. All because the process is so simple. \u201cIf someone found out, they could have caught up in a flash,\u201d he says. Familiar genes \u2014  Oct3/4,   Sox2,   c-Myc  and  Klf4,  in Yamanaka's original recipe \u2014 do the trick 1 . The genes are cloned into viral vectors, and simply adding the vectors to a culture of skin cells under the right conditions results in reprogrammed cells. But as simple as this procedure might seem, iPS cells are not easy to make. Kathrin Plath at the University of California, Los Angeles, estimates that each of the reprogramming genes (she used six) has only a 15% chance of making it into a given cell. Even if they all make it, the cell has only a 5% chance of being fully reprogrammed. The low efficiency presents a riddle for scientists, but with millions of cells available in a biopsy sample, it is not a roadblock. The trickiest part, says Konrad Hochedlinger from the Harvard Stem Cell Institute in Cambridge, Massachusetts, is finding the few cells that have been reprogrammed and culturing them. But the new iPS cells are picky: they require just the right culture conditions \u2014 much the same as those needed for embryonic stem cells \u2014 to stop them differentiating into more specialized cell types. \u201cExpertise in human embryonic stem-cell culture is absolutely critical,\u201d says Hochedlinger. Nevertheless, this type of expertise is becoming more common. Within a year, Yamanaka's results had been repeated and improved in mice by his own and two other groups. And since Yamanaka 2  and James Thomson 3  from the University of Wisconsin in Madison first produced human iPS cells independently of each other in November 2007, several other groups have now achieved that feat. The stakes are high; the technique is easy to learn; and researchers are flocking to the field, says Thomson. \u201cThe whole world is doing it now.\u201d  Status: Fact (mostly)   \n                2. Everyone can have their own custom-tailored cells \n              'Therapeutic cloning' \u2014 cloning human embryos to generate stem cells that can be used to replace tissue that has been lost or damaged without the fear of rejection by the host's body \u2014 has floundered since it was first proposed in the late 1990s, mainly because of the unexpectedly difficult challenge of acquiring the human eggs necessary for the procedure. iPS cells are moving full-steam ahead towards the same goal, patient-specific stem cells. But don't expect to have custom-made cells any time soon. Some of the viral vectors used to transfer the genes into cells, as well as some of the genes themselves, may cause cancer. Scientists expect that the heated race to find alternative systems, such as proteins or drugs that simulate the crucial genes, or safer ways to deliver the genes, will produce results soon \u2014 perhaps in the next two years. Nevertheless, says Hans Keirstead of the University of California, Irvine, the \u201cgreatest challenge still exists: the generation of high-purity, clinically relevant cell populations\u201d. It takes a couple of months to establish a cell line, several more to expand it, more still to differentiate the iPS colonies into the cell types required, a few more to expand those, and then a good half-year of testing to ensure that the cells do not form tumours. The cells also have to be processed in facilities that adhere to 'good manufacturing practice', which adds greatly to the cost. To use custom-made cells \u201cwould take a ridiculous amount of money\u201d, says Yonehiro Kanemura, a neurosurgeon at Osaka National Hospital in Japan. It would be several times more, he estimates, than the current, rare, patient-specific skin grafts, which cost as much as US$100,000. The roughly two-year process is also too slow to treat disorders such as spinal-cord injury, which require prompt treatment if the damage is to be minimized. Kanemura's solution is 'ready-made' iPS cells. This April, working with Yamanaka and Hideyuki Okano of Keio University in Tokyo, he will start establishing a national library of therapy-ready cell lines from donated placental and cord-blood tissue. At first he plans to use viral vectors, switching to virus-free lines as these become available. Over the next five years, he aims to generate 200 iPS cell lines and 200 neuronal cell lines derived from those cells. These cell lines will not be patient-specific, but Kyoto University's Norio Nakatsuji estimates that 50 well-chosen lines could provide close immunological matches for 90% of the Japanese population. People who need the treatment urgently could use the best immunological match, whereas those with chronic disorders might decide to fork out the money for a line specific to them, says Kanemura. Rich people might want to bank their own iPS cells for a rainy day \u2014 a desire that some companies will no doubt try to capitalize on.  Status: Fiction (unless you're rich)   \n                3. The cures are on their way \n              iPS cells will probably provide models for disease first, cures later. Researchers could soon culture a 'disease in a dish' of, say, motor neurons from a patient with amyotrophic lateral sclerosis, heart muscle cells from a person with heart disease, or retinal cells from a patient with macular degeneration. Those lines could be screened and observed as they develop, and biotech companies could test preventative or therapeutic drugs on them. The University of California, Los Angeles, and the Harvard Stem Cell Institute, among others, are discussing plans to start iPS cell banks for this purpose. Hochedlinger says that the stem-cell institute is considering \u201cthe major diseases \u2014 neurodegenerative, metabolic, cardiovascular, diabetes\u201d. Reaching the clinic will depend, like modelling, on how faithfully iPS cells differentiate into the affected cell type as well as on the development of safe and effective ways to deliver them into the body. The foundations are already being laid. Rudolf Jaenisch, from the Massachusetts Institute of Technology in Cambridge, for example, used blood progenitor cells created from mouse iPS cells to treat a mouse with a humanized version of sickle-cell anaemia 4 . He says that blood disorders, in which clinicians have considerable experience in transplanting cells, might see early application of iPS cells. Kyoto University's Jun Takahashi, who studied neuronal precursor cells derived from embryonic stem cells in monkey models of Parkinson's disease 5 , is now pursuing clinical treatment with neuronal precursor cells derived from both embryonic stem cells and iPS cells. He hopes that the cells will be in clinical trials within five years. Whether cultures of differentiated cells for therapeutic use still retain undifferentiated embryonic stem cells or iPS cells is a point of grave concern. \u201cGrafting even a very small number of undifferentiated stem cells, perhaps as few as one pluripotent cell, carries a risk of tumorigenesis,\u201d says Arnold Kriegstein of the University of California, San Francisco. Everyone will be watching closely a clinical trial planned for the middle of this year \u2014 the first trial of embryonic-stem-cell-based treatment \u2014 in which the pharmaceutical firm Geron of Menlo Park, California, will be implanting oligodendrocytes derived from embryonic stem cells in patients with spinal-cord injuries. \u201cApplication of iPS cells largely depends on how that trial goes,\u201d says Okano. Unpublished work by Okano, based on iPS-cell treatment of spinal-cord injury in mice, could accelerate application. He claims that he has a method to weed out the potentially dangerous cells before they are transplanted into the mice.  Status: Too soon to tell   \n                4. Embryonic stem cells are the same as iPS cells \n              \u201cThere are no major differences, yet,\u201d says Plath, based on a rigorous characterization of morphology, chromosome profile and gene expression of human iPS cells 6 . But everyone is hedging their bets as dozens of scientists start to examine the key question: whether iPS cells will differentiate as stably and diversely as embryonic stem cells. For the time being, iPS pioneers are looking at subtler hints, such as protein markers that characterize the two types of cells. But Hochedlinger says that \u201cmarkers don't mean anything\u201d. Some tumour cell lines express protein markers of pluripotency but don't make anything other than tumour cells, for example. And reports of the iPS cells' properties have been conflicting. Thomson, for instance, found that iPS cells not only expressed similar genes to embryonic stem cells, they also expressed them more consistently 7 . This means that their differentiation might be more predictable than that of embryonic stem cells. However, Robert Lanza of Advanced Cell Technology, a biotech firm in Los Angeles, California, says that iPS cells are much more variable. \u201cEmbryonic stem cells all do more or less the same tricks. But some iPS cells express just a few markers of pluripotency, some express all,\u201d he says. \u201cThe resulting cell types will presumably differ as well.\u201d Even if iPS cell lines seem to differentiate into the cell of choice, some variation between lines is unavoidable. Each line will require rigorous testing, says Keirstead, who is involved in the Geron trial. \u201cA different line may have a different tumorigenic potential, differentiation potential, migratory potential, and react with the host in a unique way. It represents a different product, so must be fully tested as such.\u201d For the same reason, Plath recommends doing any tests or drug screens with multiple lines. Despite some scepticism about iPS cells, many key researchers embrace them as a preferred alternative to embryonic stem cells. \u201cOnly time will tell, but I know where I'm going,\u201d says Thomson, who was the first to establish human embryonic stem-cell lines in 1998. If things go as he predicts, it could be the end of an era. \u201cIf you can't tell the difference between iPS cells and embryonic stem cells, the embryonic stem cells will turn out to be a historical anomaly,\u201d he says.  Status: Fact (so far, anyway)   \n                5. iPS cells have no ethical issues \n              Days after Yamanaka and Thomson announced the creation of human iPS cells, President George Bush hailed the research as a sign of \u201cscientific advancement within ethical boundaries\u201d \u2014 a feat for which he gives himself partial credit. A week later, though, Yamanaka told  Nature : \u201cWe are presenting new ethical issues, maybe worse ones, because many people can do this \u2014 and without telling anybody.\u201d Yamanaka was concerned that someone might use iPS cells to derive gametes \u2014 human reproductive cells. Eggs and sperm could both be derived from iPS cells from a man, for example, and then be used in an  in vitro  fertilization procedure. The result would not be an identical clone because genes reassort during formation of the gamete. But it would be \u201cstrange and potentially dangerous\u201d, says Yamanaka. Gametes from iPS cells could meet demands for infertility treatments. And producing eggs from male iPS cells would allow a gay couple to produce offspring between them. (Lesbian couples would be out of luck, as Y-chromosome genes are needed to produce sperm.) Such fertility treatments would be plagued by safety issues, but judging from experiments with embryonic stem cells, they won't happen soon. Morphologically similar versions of both eggs and sperm have been derived from embryonic stem cells, but only one group has reported that embryonic-stem-cell-derived gametes \u2014 mouse sperm in this case \u2014 led to live births when combined with normal eggs, and the results have yet to be repeated 8 . iPS cell adventurers might also try to create a live, cloned human. Jaenisch managed to clone mice, by transferring iPS cells into a specially developed embryo made by fusing the cells of a two-cell embryo. By putting these embryos into surrogate mothers, Jaenisch produced several fetuses that were genetically identical to the iPS cell source. (There were no live births, but Jaenisch says that is only a matter of trying.) Repeating the experiment in humans would, according to Jaenisch, \u201cbe possible in principle\u201d. He adds, however, that because \u201cmore than 100\u201d embryos are probably needed to make it work, it would be unrealistic and a ridiculous thing to do. But as fertilized embryos are easier to get than the fresh eggs used in cloning, some maverick might give it a try. iPS cells generated from a person could also be inserted into a fertilized embryo to make a chimaeric baby. These reproductive strategies would probably fail, at least with the current state of the technology. But given the rapid rate of innovation and the wide range of iPS cell capabilities, dangerous experiments will be more difficult to monitor. \u201cBefore, you had a specific community to focus in on \u2014 the practitioners of assisted reproduction. [With iPS cells] it will be difficult, especially in a place such as the United States, where there is so much dependence on self-regulation,\u201d says Paul De Sousa of the Scottish Centre for Regenerative Medicine in Edinburgh. Yamanaka's concern about the ethics drove him to lobby the government for regulation. On 21 February, Japan's science ministry sent all universities and research agencies a notification specifically forbidding \u201cthe implantation of embryos made with iPS cells into human or animal wombs, the production of an individual in any other way from iPS cells, the introduction of iPS cells into an embryo or fetus, and the production of germ cells from iPS cells\u201d.  Status: Fiction (depends on what you want to do) Yamanaka says that society, not scientists, must quickly deal with the challenges that iPS cells present. \u201cI am proud of this technology, but I feel a great responsibility,\u201d he says. David Cyranoski is  Nature 's Asia\u2013Pacific correspondent. See Editorial,  page 388. \n                     Nature Reports Stem Cells \n                   \n                     20 Years of Embryonic Stem Cells \n                   \n                     Making Stem Cells \n                   \n                     Kyoto University Institute of Frontier Medical Scienes \n                   \n                     White House statement \n                   Reprints and Permissions"},
{"file_id": "452269a", "url": "https://www.nature.com/articles/452269a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "More than a billion people do not have access to safe drinking water and two billion have inadequate sanitation. This is despite two international decades, a millennium declaration goal, two international years and a string of global celebratory days \u2014 all dedicated to drinking-water or sanitation. Why has progress been so slow? One reason could be that the current global targets \u2014 the Millennium Development Goals \u2014 do not provide sufficient incentives for all nations to ensure that everyone has access to water and sanitation (see  page 283 ). Another reason is that the pressures on water resources are continuing to rise: whether through population growth, economic development or climate change (see  page 270 ). The effects of water shortages are already spilling over from health and sanitation into key economic activities such as agriculture and energy production. Here the challenges are clear, if not fully appreciated: agriculture could easily require twice as much water in the next few decades (see  page 273 ). And the global demand for energy is projected to rise by 57% by 2030 (see  page 285 ). It we want to improve global access, it is time to rethink our strategies to water, and to respond to global trends in food and energy production. New concepts are emerging, with experts and policy-makers stressing simple solutions to improve crop yields in the rain-fed areas where many of the world's rural poor live. This is preferable to expanding the area of irrigated farmland, which is already the biggest consumer of freshwater worldwide. Growing energy demands will require a more integrated strategy for managing freshwater \u2014 to prevent cities and farms, or upstream and downstream users, from coming into conflict. This sort of joined-up policy-making has been sorely lacking, but will be crucial to water management at the river basin, and at both regional and transnational scales (see  page 253 ). Elsewhere in the issue, a Review Article (see  page 301 ) highlights the purification technologies that scientists hope will improve access to safe drinking water. An Essay (see  page 291 ) explains how physicists still argue over theories about the structure of water. And Books & Arts reviews a television documentary on the privatization of water supplies (see  page 288 ). For more see the  Nature News Special on Water \n                     Nature News Special on Water \n                   Reprints and Permissions"},
{"file_id": "452404a", "url": "https://www.nature.com/articles/452404a", "year": 2008, "authors": [{"name": "Michael Hopkin"}], "parsed_as_year": "2006_or_before", "body": "The 'Doomsday vault' buried in the Arctic ice will provide a backup for the world's seeds. But more needs to be done to safeguard food diversity, says Michael Hopkin. Fittingly, the end-of-the-world vault lies at the end of the world. The plane to Svalbard gets you to within about 1,000 kilometres of the North Pole \u2014 further north than any other commercial flight. Today, the beautiful if desolate island of Spitsbergen in the Svalbard archipelago is home to a tight-knit population of just 2,000 miners, hoteliers and Arctic researchers. In a few years time, it will also be home to seeds from some 1.5 million varieties of crop. On 26 February, a small group of officials, politicians, scientists and journalists carried the first deliveries of seeds down an icy tunnel dug deep into the frozen flank of an Arctic mountain and into the Svalbard Global Seed Vault. When the collection is completed over the next four or five years, the vault will have amassed seeds from virtually all the recognized varieties of 150 crop species routinely grown and eaten by humans \u2014 including the 100,000 varieties of rice, the world's top staple and the crop that accounts for more than 20% of all calories eaten worldwide. Conserving crop biodiversity is an urgent undertaking. The Intergovernmental Panel on Climate Change (IPCC) predicts that 25\u201330% of plant species will be extinct or endangered in the next century. \u201cWe're losing crop diversity every day, going out with a whimper, not a bang,\u201d says Cary Fowler, executive director of the Global Crop Diversity Trust, which will curate the vault's collection. \u201cIn a real sense, Doomsday is every day. Built by the Norwegian government at a cost of 45 million kroner (US$8.8 million), the vault was built in Svalbard partly because its storage rooms, deep under the permafrost and chilled to the optimum seed-storing temperature of \u221218\u00b0C, will remain insulated even in the unpredictable centuries to come. The seeds it will store are copies of those already held by the roughly 1,400 existing national and regional seed banks worldwide. It's nicknamed the Doomsday vault because of its intended role in bailing out these banks in the event of mishaps \u2014 anything from power cuts, to flooding, to war. \u201cIf we had built this vault ten years ago, we would have used it at least ten times already, for the loss of the gene [seed] banks in Iraq and Afghanistan, for example,\u201d Fowler says. It's an extra insurance policy. But no matter how well intentioned and executed, Svalbard cannot single-handedly save the world's threatened biological resources, edible or otherwise. A seed vault won't help to conserve the threatened livestock breeds that billions of people depend on for both dietary and financial sustenance, or indeed the complex ecosystems that exist alongside both crops and livestock and in many cases allow them to thrive. Conserving these, and hence securing humanity's food supply, will require schemes even more impressive than an Arctic dugout. Nevertheless, the global approach behind the Doomsday vault may offer inspiration to those aiming to conserve these other resources. The Svalbard effort already shares its ethos with another scheme, called the Millennium Seed Bank Project, run by Britain's Kew Gar- dens. Based in the temperate garden of an English stately home, this bank is somewhat less theatrical than a frozen mountain bunker. But Britain's freezer is the only other to be storing seeds from all over the world, and aims to safeguard more than 24,000 wild plant species, including some crops. Together with Svalbard, the banks will still preserve only a fraction of the world's plant species.  \n                Cold comfort \n              Keeping spares of the animal kingdom poses even more of a challenge. \u201cIt's a lot easier to put 1,000 radish seeds in cold storage than frozen animal material,\u201d says Don Bixby, a cryopreservation technician and former executive director of the American Livestock Breeds Conservancy (ALBC), a North Carolina-based non-profit organization. Yet the problem of dwindling diversity is just as great, if not greater, in the world's livestock as it is in its crops. Last year, a survey by the United Nations Food and Agriculture Organization revealed that 16% of the world's 7,600 recorded indigenous breeds of cattle, pigs, sheep and poultry are at risk of disappearing \u2014 11% have already gone extinct. This has mostly resulted from the rise of high-yield breeds such as the familiar black-and-white Holstein\u2013Friesian dairy cow, now found in 120 countries. International breeding companies have promoted these breeds, and farmers, keen to increase their output, have willingly adopted them. All of the estimated one billion Holstein\u2013Friesians in the world were ultimately sired by the same few dozen bulls, who have been bred for daughters with high milk yields and whose sperm is collected, frozen and sold worldwide. \u201cWe're dealing with an effective population of 30\u201335 animals globally,\u201d says Bixby. This is a \u201cvery worrying number\u201d, because the homogeneous animals are not adapted to local drought conditions or to resist disease. Can endangered livestock breeds be given their own version of the Svalbard seed vault? It's a tricky proposition. Unlike seeds, which are generally capable of germinating after being frozen, delicate sperm, eggs and embryos are far more susceptible to damage by freezing \u2014 and methods for doing so have to be refined for every species' unique physiology. Cattle sperm is one success story. \u201cFifty-year-old semen still produces calves,\u201d Bixby says, but, \u201cfor many species we're not able to put away material that can be reconstructed into live animals. If there is hope of developing an animal counterpart to Svalbard, it may lie in the network of regional tissue banks being developed by the US Department of Agriculture. It has given itself a mandate to collect reproductive material from every livestock breed found in the country, with an emphasis on conserving genes that will be useful for maintaining future food production. This could go a long way towards saving the world's resources, because the United States' history of importing and breeding animals with valuable traits \u2014 which these days would be decried as biopiracy \u2014 means that many foreign breeds are represented there. But the US tissue banks are not designed to be a global repository for animal diversity, and there is no talk of starting such a store elsewhere. Cryobanks cannot, however, be the whole story, and parallels can be drawn with wildlife conservation, where breeding from cryopreserved material is still seen as a last resort. The argument here, in essence, is that there is little point in resurrecting frozen seeds or embryos if their natural environment has vanished too. \u201cWildlife conservation realized 15 or 20 years ago that it's not good enough to just save a seal, or a quail \u2014 we have to save the ecosystems where they live, and in many cases that's also true for livestock and food plants,\u201d Bixby says. But agricultural systems are often excluded from conservation efforts because they are, by their nature, artificial. Some credit the organic farming movement with helping to preserve indigenous crops and livestock and their surrounding ecosystem. The Svalbard seed vault is generating almost universal good will, but even its supporters acknowledge that it's not enough to bank existing biodiversity for use after Doomsday. They also advocate efforts to withstand impending disaster \u2014 by creating breeds that can resist whatever endangers them. Gordon Conway, chief scientific adviser to the UK government's Department for International Development (DFID), argues that biotechnology techniques will prove instrumental in preserving biological diversity. They allow, he says, genes that confer drought resistance or other valued traits to be moved from one variety to another more quickly than conventional selective breeding. Conway cites the example of a new, devastating variant of wheat rust fungus, called Ug99, which is spreading through East Africa towards the Indian subcontinent. Earlier this month, the UN reported that it has already been detected in Iran. Efforts are underway to cross disease-susceptible strains with natural varieties carrying a gene resistant to Ug99. \u201cWe have to get that gene into Asian variants, and we have to do it quickly,\u201d Conway says. In this sense, he argues, environmentalists need to embrace techniques such as genetic modification that are sometimes viewed with suspicion. The Svalbard vault's creators have not so far suggested using its seeds as a gene bank that can be mined in the future. But the bunker is \u201cnot the only element in the master plan\u201d, says Fowler. Besides overseeing the vault, the Global Crop Diversity Trust has amassed a war chest of more than $260 million, much of which will be spent on projects to breed hardier crops, and strengthen national and regional seed banks. Donors include the Bill & Melinda Gates Foundation, DFID, agribiotech giants such as DuPont and, most recently, the Norwegian government. The trust's work will add to the existing efforts of international philanthropists such as the Rockefeller Foundation to boost the effectiveness of plant breeding, not to mention the profit-driven activities of multinational crop firms. Farms thus represent the real battleground where the fight to save food diversity will be won: it is there that the many varieties of food species grow today \u2014 and it is there, ideally, that they will be preserved intact. For those at the Svalbard vault's bitterly cold opening ceremony, it's hard to imagine its icy caches ever being thawed. And that, really, would be the best case scenario \u2014 after all, no one wants to have to cash in their insurance policy. \n                     Evolution & Ecology \n                   \n                     Biotechnology \n                   \n                     Food and the Future \n                   \n                     GM Crops: Time to choose \n                   \n                     Organic farming \n                   \n                     Svalbard Global Seed Vault \n                   \n                     American Livestock Breeds Conservancy \n                   \n                     International Livestock Research Institute \n                   \n                     Millennium Seed Bank Project \n                   \n                     USDA Germplasm Resources Information Network \n                   \n                     UN Food and Agriculture Organization \n                   Reprints and Permissions"},
{"file_id": "452400a", "url": "https://www.nature.com/articles/452400a", "year": 2008, "authors": [{"name": "Katharine Sanderson"}], "parsed_as_year": "2006_or_before", "body": "Chemists have long wanted to recreate photosynthesis in the lab \u2014 and to improve on its efficiency at converting sunlight into fuel. Katharine Sanderson reports on their latest efforts. \n                 See also Correspondence: \"Mimicking photosynthesis, but just the best bits\":http://www.nature.com/uidfinder/10.1038/453449b  \n               \n                     Powering the Planet \n                   \n                     Photosynthetic efficiency \n                   \n                     Artificial photosynthesis \n                   \n                     Nate Lewis, Powering the Planet \n                   \n                     Solarbuzz \n                   \n                     Electrons in photosynthesis \n                   Reprints and Permissions"},
{"file_id": "452270a", "url": "https://www.nature.com/articles/452270a", "year": 2008, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "In parts of the world already facing unreliable food supplies, an uncertain climate adds to the future stress for soils, plants and people. Quirin Schiermeier reports on water strategies for a drier world. The record-breaking European heatwave of 2003 did not come out of the blue. It was preceded by an unusually dry spring during which soils dried up across the continent. The lack of moisture resulted in strongly reduced soil evaporation and cooling, which in turn intensified the temperature extremes during the summer. Climate scientists believe that in the second half of this century, extreme summer heat and drought could become the rule rather than the exception as global temperatures rise. In any case, rapid loss of soil moisture early in the year now seems to be a signal for subsequent summer heatwaves in Europe 1 . A feedback loop appears to be at work: as heat dries up the soil, the dry soil amplifies the heat. Changes in soil moisture content may have other feedbacks, affecting soil erosion, surface runoff, soil nutrients and even cloud formation. But predictions of soil drying in response to rising temperatures are still very uncertain. For Africa and South America, climate modellers are not even confident about the sign of the simulated changes. \u201cWe are told climate variability will increase and that it may get drier in some regions, but we really know too little about the details,\u201d says Malin Falkenmark, a hydrologist and water-management expert at the Stockholm International Water Institute in Sweden. This uncertainty hasn't stopped Falkenmark, along with other hydrologists, from recommending changes to water-management practices in response to climate change, and to declare an end to the wait-and-see approach of the past 2 . \u201cWe don't know for sure how climate change will unfold, but there's no doubt any more that it is happening and that there needs to be some preparedness,\u201d Falkenmark says. \u201cRiver flow in some dry regions may decrease by up to 40%, for example. That must alter water-resource planning methods. We cannot just wait until it happens.\u201d Current models suggest that more rain will fall, but less often, leading to longer periods during which soil moisture is critically depleted. Observations from several regions, including North America, Europe, southern Africa and Australia, confirm a trend towards heavier rainfall events, with longer dry periods in between, particularly during the summer 3 .  \n                Down to earth \n              Observable trends for soil moisture are more elusive. As yet, soils seem to be more resilient to global warming than, say, mountain glaciers or polar ice sheets. In the few regions where good records are available \u2014 such as the Ukraine, where scientists have measured soil moisture for 45 years \u2014 researchers have found no evidence for much of a downward trend, if any. \u201cSoil moisture is not an easily measured quantity,\u201d says Jerry Meehl, a climate researcher at the US National Center for Atmospheric Research in Boulder, Colorado, and a lead author for the Intergovernmental Panel on Climate Change (IPCC). \u201cThe IPCC first predicted increased mid-continental summer drying of soils almost 20 years ago,\u201d he notes. In the absence of observations to support or refute this prediction, the science has not advanced much since then. Climate models are consistent in predicting greater summer soil dryness after 2050 in parts of every continent except Antarctica. But where that will change, and how much, depends heavily on the model (see  maps ), none of which are yet good enough to allow detailed soil moisture predictions at the river-basin scale or below \u2014 the scale that matters to water-management experts such as Falkenmark. \n               boxed-text \n             The main reason for the fuzziness is that it is much more difficult to model rainfall than temperature. The processes that control rainfall, such as cloud and droplet formation, occur on much smaller scales than are used by existing climate models. Soils are also too patchy to be reliably represented in current models. Finally, the complex interactions between rainfall, evaporation, carbon dioxide concentration, plant growth and soil moisture are not easily computerized. Because soil moisture and rainfall influence each other, the models desperately need better soil data to improve. Yet the world's soils are not nearly as well monitored as temperature or precipitation;  in situ  observations are few and scattered. To disentangle the complex interplay, scientists would need to find some way to measure soil moisture content directly and continuously. There is hope that satellite measurements will help. Both the European Space Agency (ESA) and NASA are planning missions to observe soil moisture, expanding the work of ESA satellites ERS-1 and ERS-2. The microwave sensors on board the planned missions will give almost global coverage of soil moisture changes in real time. Where dense vegetation hides the soil, greenness can be used as a substitute. At the same time, increasing computer power is allowing researchers to improve their models. There's still some way to go, admits Peter Cox, a climate modeller at the University of Exeter, UK. Current models are not yet fine-grained enough to model individual tropical storms, for example. But Cox says that some regional models are getting close. \u201cThe trick is to use various sources of information and fuse them together so as to construct a global data set,\u201d says Cox. \u201cAs climate models and satellite observations are converging in scale and resolution, we can start ingesting satellite data into our models and make them more powerful.\u201d The public usually associates water shortages with a lack of drinking water. But global water scarcity is primarily an issue of hunger, not thirst. Declining soil moisture generally means an increasing risk of drought. Monitoring and understanding possible soil moisture changes is therefore vital for crop management in all regions at risk of water scarcity. Researchers expect the most severe impacts to occur in the transition zones between wet and dry climates. In very wet regions, where soil water is always plentiful, evaporation and precipitation are hardly sensitive to soil moisture. And in very dry regions the rate of evaporation is too small to generate much precipitation anyway. In one of the best available estimates, a multi-model study conducted by the Global Land\u2013Atmosphere Coupling Experiment, run by the World Climate Research Programme, the hot spots of coupling between soil moisture and precipitation appear in the plains of North America, sub-Saharan Africa and northern India 4 . These regions, and in particular the 'hunger belt' from the Sahel to the Horn of Africa, are thought to be most at risk from the effects of climate change, such as more frequent droughts and floods, and accelerated soil erosion. Soils store rainfall in the root zone of plants. This is called 'green water', as opposed to the blue water in rivers, lakes and groundwater stores. In dry regions, blue water is usually very scarce, often accounting for less than 10% of the overall water balance. All rain-fed agriculture in tropical and savannah regions, where irrigation is minor, depends on soils' capacity to capture what little rain falls. \u201cGreen water is the key to water and food security in drought-prone regions,\u201d says Falkenmark, who coined the term in the early 1990s. But experts believe that only 10\u201330% of rainfall in the world's savannah belt \u2014 the dry to moderately wet zones on all continents \u2014 is being used in a productive way. The effect of climate change on water scarcity in regions that lack food security is becoming evident. Given the degree of human interference with climate and water, Falkenmark and other international experts recently declared dead the idea that water planners need consider only natural variability (and not human influence) when managing water supplies 2 . What the developing world needs now is a second 'green revolution', aimed at increasing yields by improving green-water management, soil conservation efforts, and more efficient protection of crops from prolonged dry spells, she says. \n               boxed-text \n             Green and blue water are not separate resources, of course. Irrigation turns blue water into green (see  graphic ). But in dry regions it is difficult to improve water availability through engineering works such as dams. \u201cIt is very unsatisfactory, therefore, that most water engineers are still mainly thinking in blue-water terms,\u201d says Falkenmark. To capture green water in dry African regions, farmers need to make sure that enough rain can infiltrate the soil after dry spells, for example by adopting more soil-friendly ploughing techniques, which have already increased yields in Latin America. And experts recommend that farmers harvest water from local runoff to use during dry spells in the growing season (see  'Keep it simple' ).  \n                Going green \n              Even without climate change, rain in the savannah belt is erratic. In sub-Saharan Africa, for example, dry spells typically occur even in 'wet' years. \u201cIn Africa,\u201d says Falkenmark, \u201cthe term 'rainy season' means that rain can fall, not that it will fall.\u201d For soil moisture and green water, the local frequency and intensity of rainfall are at least as important as the total amount of precipitation. Heavy rain cannot penetrate parched and crusted soils, and without efficient water and land-use management, researchers warn that more variable rainfall in vulnerable regions threatens to increase runoff, erosion, water stress on plants and flooding. Models agree that global warming will amplify the entire hydrological cycle, from evaporation to precipitation to runoff 5 . Global precipitation over land may slightly increase, especially in some northern latitudes or tropical regions, with a greater fraction occurring during the heaviest events. Markus Reichstein, a carbon-cycle expert at the Max Planck Institute for Biogeochemistry in Jena, Germany, has studied the consequences of more extreme rainfall on ecosystems. He says all levels and processes of the ecosystem are likely to be affected, from runoff to soil evaporation and nutrient availability. Changes will affect all climate zones, but some ecosystems may respond very differently to others, a 15-strong interdisciplinary team concludes in its as yet unpublished review. Plants' ability to adapt to changing water and nutrient availability might be crucial for their survival in a warming world. Ecologists think there are thresholds beyond which plants become stressed. But these vary between ecosystems, and so may plants' responses to climate change. Soil water availability generally limits plant growth and photosynthesis. But nutrient availability in soils increases during dry spells, which suppress nutrient uptake by plants more severely than nutrient mineralization. Still, in all semi-arid regions more extreme rainfall will increase stress on crops and vegetation, scientists believe 6 . Unfortunately, these are also densely populated regions with unreliable food production. In sub-Saharan Africa, longer dry spells will harm vegetation and, without supplementary irrigation, decrease yields.  \n                A question of breeding \n              How best to adapt? The 2003 heatwave, which reduced yields in some European countries by more than 50%, shows that the rich world is not immune from the consequences of a warming climate, and from the need to adapt. But climate change is without doubt a much bigger threat to food security in poorer regions. Experts warn that poverty tends to entrench the deficiencies of rain-fed agricultures in developing countries. As poor farmers cannot afford to invest in their crops, foreign investment aid or cheap loans are vital. A recent analysis of climate risks for crops in 12 regions with food insecurity shows that crops such as oilseed rape, corn (maize) and wheat in south Asia and southern Africa are most vulnerable. Agricultural investment and adaptation efforts should focus on these crops and regions, the authors suggest 7 . \u201cWe're seeing a massive challenge,\u201d says David Lobell, an agricultural ecologist at Stanford University in California and one of the authors of the study, who warns that plant breeding is under-resourced. \u201cWe must urgently develop new crop varieties tolerant to heat and drought, and not just maize,\u201d he says. \u201cAnd we need to work hard and very quickly on it. Don't forget it can take 15 years of development effort until a new variety is adopted by farmers.\u201d But without prior investment in water and land management, crop-adaptation efforts will be less effective, says Deborah Bossio, director of research at the International Water Management Institute in Colombo, Sri Lanka, and a lead author for the International Assessment of Agricultural Science and Technology Development, an international effort akin to the IPCC for agriculture. Investment in water is particularly essential in south Asia and sub-Saharan Africa, says Bossio. And it should consider the full range of water storage and delivery options, she says, from the most local \u2014 soil water storage and farm ponds \u2014 to community projects such as small reservoirs. But she warns that too much focus on crop production may put crops and livestock into conflict over water, with the risk that vulnerability is increased. \u201cLivestock are always a very important component of the livelihood systems in areas at risk from water scarcity,\u201d says Bossio. Adaptation to water scarcity has to consider all the components that affect people's lives. Quirin Schiermeier is a reporter for  Nature  based in Munich.  See Editorial,  page 253  . For more see the  Nature News Special on Water \n                     Nature News Special on Water \n                   \n                     Earth Observation Special \n                   \n                     Web Focus Global Water Crisis \n                   \n                     Intergovernmental Panel on Climate Change \n                   \n                     UNESCO on Water \n                   \n                     Stockholm Environment Institute \n                   \n                     Stockholm International Water Institute \n                   \n                     Vienna Technical University Soil Moisture Databank \n                   \n                     Rutgers University Global Soil Moisture Databank \n                   Reprints and Permissions"},
{"file_id": "452146a", "url": "https://www.nature.com/articles/452146a", "year": 2008, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "A surprisingly large number of university-inspired patents may be going to industry instead. Rex Dalton reports. patent battles go, it seemed one never to be repeated. In 1999, the Californian biotechnology company Genentech resolved a long-running law suit with the University of California. For US$200 million, the firm agreed to settle over claims that a researcher had slipped human growth hormone DNA out of the university's lab on a midnight run down to the firm \u2014 which promptly turned the material into its blockbuster drug Protropin (somatrem). But such patent diversions may be more common than was thought, a study now suggests. A management researcher at the University of Georgia in Athens and his colleagues say that US universities are losing many of their researchers' inventions to industry. One-third of the patented discoveries made at universities were redirected to belong to private firms instead \u2014 and those taken away were the ones most highly cited in future patents, suggesting that they were the most valuable. \u201cThis is largely a system failure \u2014 professors [are] systematically engaging in invention diversion into a black market, where industries patent them,\u201d says Gideon Markman, who led the team that published the findings (G. D. Markman, P. T. Gianiodis and P. H. Phan  J. IEEE Trans. Eng. Manage.   55,  29\u201336; 2008).  \n                Royal top-up \n              For universities, the financial stakes can be substantial. For instance, the 10-campus University of California system received $193.5 million in fiscal year 2006 from patent royalties and fees. The absolute numbers may be small compared with overall institutional budgets \u2014 the state budget for the University of California that year was around $3.5 billion \u2014 but the flow from patents is, in essence, free money for the universities at a time when funds are getting tight. Markman is one of just a few scholars looking at how universities are faring in the wake of revolutionary changes in patenting academic research. In 1980, a law called the Bayh\u2013Dole Act allowed US universities to start patenting inventions stemming from federally funded research. For a particular invention, a university could either forgo a patent right or claim it and then encourage development under a licensing agreement with a firm. The firm would then pay royalties to the university and inventing professor. Technology-transfer offices sprang up to facilitate this new law. But as researchers, their business partners and even universities can make fortunes off publicly funded discoveries, temptations to use the system for personal benefit have grown immensely. University executives, government officials and leaders of watchdog organizations often chatter about subversions: a lab animal making valuable protein might quietly disappear, or an engineering tool might shift from a professor's academic computer to his or her consulting business. Markman and his team have been tracking what happens to this lost information. Their study focuses on 7,650 patented discoveries in biomedicine, information science and engineering made between 1989 and 2003 at 54 US universities. It shows that the more valuable the patent is, the more likely it is to be diverted to industry. \u201cI'm not surprised \u2014 that's consistent with my own thinking,\u201d says Michael Crow, president of Arizona State University in Tempe, which is itself trying to spin off an industrial cluster. \u201cThe technology-transfer concept is still in its infancy, and universities are still figuring out how to do things.\u201d Administrators at technology-transfer offices are more sceptical. \u201cI really question if one-third of the best inventions are leaking out the back door,\u201d says Jon Soderstrom, director of Yale University's technology-transfer office. \u201cIt seems awfully high to me \u2014 but it may be.\u201d Data confirming Markman's results are expected in the near future from David Audretsch, a researcher from Indiana University in Bloomington who is on leave at the Max Planck Institute of Economics in Jena, Germany. Audretsch's earlier work found that 30% of researchers funded by the US National Cancer Institute do not assign their patents to their universities.  \n                Deal makers \n              Others aren't so sure. A team from the Georgia Institute of Technology in Atlanta has published a study online in the National Bureau of Economic Research (J. Thursby, A. Fuller and M. Thursby  http://www.nber.org/papers/w13256  ; 2007). The results showed that many discoveries that could be interpreted as being 'lost' in fact occurred when professors were doing outside consulting with industry that had been previously approved. The team found that 26% of 5,800 patents went solely to outside firms \u2014 a similar proportion to what Markman found \u2014 but say that follow-up interviews suggest these numbers are due to acceptable outside consulting. For his part, Markman says that professors who do the most consulting also divert the most technology. Once an academic has started working with an outside firm, he or she is in a better position to divert technology, Markman argues. Markman plans to examine the diverted patents in case studies to determine exactly what happened to each. \u201cIt should be fascinating to see what we find,\u201d he says. In the meantime, experts suggest simple ways to keep patents at the universities at which they were made. Step one is to break down all barriers between technology-transfer offices and faculty members, to ensure proper working relations. Step two is to maintain constant relationships with industry, intellectual-property attorneys and venture-capital firms. Given the history of patent diversion, though, both steps may be harder than they seem. Rex Dalton is a US west coast correspondent for Nature. \n                     Association of University Technology Managers \n                   Reprints and Permissions"},
{"file_id": "452682a", "url": "https://www.nature.com/articles/452682a", "year": 2008, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "Collaborations spawn fresh ideas and boost productivity - most of the time. Heidi Ledford examines what happens when a working relationship breaks down, and asks how to avoid it. Break-ups can be painful, as biologist Paul Weldon and chemist Andrew Evans can attest. Weldon spent nine months and filed a lawsuit trying to retrieve a sample he had shared with Evans as part of a collaboration. And Evans estimates that the failed collaboration cost him dearly in wasted time, energy and materials. Of course, that was by the bitter end of the relationship. At the start, two years ago, things looked a lot more promising. Weldon, a research associate at the Smithsonian Institution's National Zoological Park in Washington DC, had just isolated a compound called bovidic acid from the oily skin secretions of the gaur ( Bos frontalis),  an endangered ox. He suspected that it was the animal's natural mosquito repellent, but he needed a chemist to create a synthetic version of the molecule and confirm that he had found the right compound. Weldon asked Evans, then at Indiana University in Bloomington to team up with him, and he sent Evans a sample of bovidic acid. The two corresponded by e-mail, and Weldon travelled to Evans's lab. Neither saw the need to formalize their relationship in writing \u2014 they had collaborated with researchers before and such measures had never felt necessary. It was a decision they would both come to regret. The partnership slowly fizzled as the two collaborators' interests diverged. Then conflict arose over who owned the samples. \u201cCollaborations always start off in a woolly way,\u201d says Evans, who is now at the University of Liverpool, UK, \u201cand it either takes off and goes great or one of you goes off in another direction \u2014 and that's what happened here.\u201d \u201cI feel like I've been hit by a truck,\u201d Weldon says. \u201cI'm going to be very circumspect from now on.\u201d But Evans has sent material to  Nature  suggesting that another, earlier collaboration of Weldon's, relating to bovidic acid, had also ended with disagreement between the collaborators. Weldon says that of about 30 collaborations he has entered, only those two have ended under difficult circumstances.  \n                Teaming up \n             More and more researchers are entering into collaborations, often with multiple members and in distant locations. Among the top 200 research universities in the United States, science and engineering publications involving multiple institutions increased 48% between 1988 and 2001 (ref.  1 ). And between 1990 and 2000, the proportion of publications from international collaborations nearly doubled, accounting for close to 16% of all scientific articles published in Thomson Scientific's ISI Web of Knowledge  2 . The rise of interdisciplinary research and the ease of long-distance communication have encouraged this trend, along with funding agencies that earmark grants for collaborative projects. Many of these collaborations are productive, generating new friendships, data and ideas. But researchers often underestimate the effort it takes to nurture a successful collaboration, or fail to anticipate the many ways one can go sour. \u201cIt is astonishing how little communication there is in the scientific community concerning planning of the project and talking about who is doing what,\u201d says Ulrike Beisiegel, an ombudsman for the German Research Foundation in Hamburg. Without such planning, and a written agreement, scientists can find themselves in testy exchanges over paper authorship and data ownership. At best, these are rocky patches that can be smoothed over; at worst, such conflicts can lead to abandoned projects, wasted research funds and lawsuits. Weldon spent the next nine months fighting to get the bovidic-acid sample back, not because it was valuable but \u201cout of principle\u201d, he says. Richard DiMarchi, then the chair of Indiana's chemistry department, urged Weldon and his collaborator to work it out. Weldon hired a lawyer and, last September, sued Indiana University for possession of all bovidic-acid samples related to the collaboration. Because the bovidic acid had been purified at Indiana University, the university felt that it could have a claim to ownership, says Beth Cate, associate general counsel of the university. But \u201cin the end we decided it would be best to be done with this matter\u201d, she says. \u201cI think there were some misunderstandings here that led to some heightened distrust or emotion.\u201d Two months after Weldon filed the lawsuit, the university told him that they would return the sample, and he dropped his claim. This whole story runs counter to the idea that scientific collaborations are advantageous. In a 2006 analysis of publications, pharmacologist William Figg at the National Cancer Institute in Bethesda, Maryland, showed that the number of citations earned by biomedical articles in elite journals rises with the number of authors on the publication  3 . Another study found that biotechnology papers with at least one international co-author tend to have a higher citation rate than those with a national co-author at a different institution  4 . University administrators and funding bodies sometimes use such studies to promote the virtues of collaboration in the hope of increasing the impact of the research they pay for. Some funding agencies, including the US National Science Foundation (NSF) and the European Commission, have set aside money specifically for interdisciplinary or international collaborations. The European Commission's Seventh Framework Programme, for example, has set aside \u20ac32.4 billion ($51 billion) for international collaborative research projects. But Jonathon Cummings, professor of management at Duke University in Durham, North Carolina, questions whether this type of forced collaboration is as productive as the figures initially suggest. \u201cThere are unintended consequences when large funding agencies force people to collaborate,\u201d he says. \u201cIf you say we're not going to give you money unless you bring in two to three other universities, it's forcing their hand.\u201d Some sociologists have pointed out that it is inaccurate to measure the success of collaborations by counting up publications and then seeing how many stemmed from teams. It overlooks those collaborations that collapsed before papers were written and therefore skews the data towards successful efforts. In an attempt to reduce this bias, last year Cummings and his colleagues surveyed participants in 491 NSF-funded research collaborations, more than half of which spanned multiple universities. They collected data on all the projects that were funded \u2014 including less successful collaborations \u2014 and measured what they produced in terms of 'knowledge outcomes', such as patent applications, conference presentations or published articles. The study suggested that projects involving multiple universities produced fewer knowledge outcomes than those involving a single institution  5 . Cummings says that this is because some of the collaborators failed to plan sufficiently for the challenges of coordinating research across the disciplinary and geographical boundaries, so they either wasted time and effort or their collaborations broke down. \u201cWhen you come together in an attempt to get money but don't really think through how you're going to work together, it ends up backfiring in terms of lower outputs and lower productivity,\u201d he says. Scientists have a rosier view. They often compare their collaborations to romantic relationships and, like true romantics, they feel that these relationships are built on trust. They prize the ability to form spontaneous, unstructured liaisons \u2014 ones that spring from chance encounters at a meeting or coffee room \u2014 and enjoy the flush of generating new ideas with like-minded colleagues. \u201cEntering collaborations is much like dating,\u201d says Neil Smalheiser, a neuroscientist at the University of Illinois, Chicago, who co-founded the  Journal of Biomedical Discovery and Collaboration.  \u201cSome people obsess over it, whereas some proceed blithely and blindly without considering long-term ramifications at all.\u201d The consequences of a failed collaboration cut just as deeply as a failed affair. The most common source of conflict is over publications, which are often the first time that anything is written down. This can raise a host of thorny questions about when data are ready for publication and \u2014 the point on which careers are made or broken \u2014 who should be on the author list and in what order. The US Office of Research Integrity in Rockville, Maryland, has reported that it receives many misconduct allegations about researchers who have published some aspect of a joint project without crediting a former collaborator, but that these cases are not within its remit. Academic institutions are left to decide whether to launch investigations. Data ownership is another common source of disagreement, particularly when valuable intellectual property is discovered as part of the work. \u201cPeople and institutions can get greedy,\u201d says Diane Sonnenwald, a sociologist at the University of Gothenburg in Sweden who has studied scientific collaborations. \u201cResearch has shown that it's easiest to address these issues before there's real intellectual property to argue over\u201d. Beisiegel says that she advises scientists to resort to a lawsuit if a collaborator has threatened their intellectual property. But some researchers shy away from such battles. \u201cIt's always a question of how much time and energy you want to spend on these things,\u201d says Wesley Shrum, a sociologist at Louisiana State University in Baton Rouge. \u201cSome battles are worth fighting, and some are not.\u201d  \n                Write it down \n             Among those who study collaborations, the solution is clear: potential partners should get everything in writing at the start. Written 'pre-collaboration agreements' can spell out division of labour, data ownership and who will be on the author list. They are sometimes dubbed 'collaboration prenuptials' or 'prenups' in reference to the legal documents that some couples sign before getting married, delineating how property and wealth will be split in the event of a divorce. But many scientists baulk at the idea of a prenup, saying that it can be awkward to press a new collaborator into a written contract before materials and ideas are exchanged, and before knowing where the research will lead. \u201cWe recognize that using scientific prenuptials goes against the informal norms of science,\u201d wrote Howard Gadlin, ombudsman for the US National Institutes of Health in a 2002 article on responsible research conduct  6 . \u201cBut we have seen the damage that can be caused, both scientifically and personally, when scientists at the NIH overlook questions such as these in their enthusiasm to launch an intellectually exciting collaboration.\u201d Participants in large, formal collaborations often use their grant applications as a form of prenup, because it forces them to decide precisely who will do what. Many universities have template agreements for researchers to use before establishing a collaboration with industry, but such agreements are not required and often not even encouraged for collaborations within academia. \u201cWe don't see a lot of those at all,\u201d says Peggy Fischer, associate inspector general for misconduct investigations at the NSF. \u201cMaybe we don't see them because they work, and maybe we don't see them because they don't exist.\u201d  \n                A rough outline \n             Smalheiser argues that written prenups are not always necessary, particularly when collaborations are with people one already knows and trusts. \u201cI don't see formal agreements as something practical to pursue,\u201d he says. \u201cEspecially because the nature of the research and the nature of the collaboration may be continually evolving.\u201d He has developed a checklist of issues to hash out early in the collaboration, such as authorship and data sharing  7 . The checklist is intended to raise important issues for discussion up front and, although it can be used as the basis for a prenup, it does not have to yield a formal agreement, Smalheiser notes. Even sketching out a rough outline of the collaboration can be useful, and updating the agreement as conditions of the collaboration change (see  'The collaborators' prenup' ). Long-distance relationships can be particularly testing, in science as in romance. As a general rule, the further away a collaborator is, both in discipline and in geography, the more prone the collaboration is to conflict, says John Walsh, a sociologist at the Georgia Institute of Technology in Atlanta. Researchers at different institutions are more likely to face conflicts over schedules, for example, and disciplines may have differing customs for authorship. \u201cThere is pressure now both to have larger collaborations and to have remote collaborations that incorporate resources from different places,\u201d says Walsh. \u201cYou're more likely to be faced with difficulties that you wouldn't face with a local team of modest size.\u201d Sorting through the wreckage of a crumbled international collaboration can be complicated by conflicting international policies and laws (see Commentary,  page 686 ). \u201cWhen you were collaborating with somebody down the road or in the state next door, everybody was operating by the same rules,\u201d says Nicholas Steneck, director of the Research Ethics and Integrity Program at the University of Michigan in Ann Arbor. \u201cNow all of a sudden you've got collaborations with labs around the world, and there aren't the same rules and understanding.\u201d The key, says Cummings, is not to break up collaborations but to allow more time and money for management. This might involve hiring staff to help coordinate the programme. In response to his study, the NSF began requesting management plans (which are effectively compulsory prenups) in some of their programmes, particularly interdisciplinary ones. \u201cThey've realized that it's one thing to write a proposal for some grand idea,\u201d says Cummings. \u201cIt's another to spell out who's going to do what and how you're going to do it.\u201d The European Commission also requires management plans to accompany its Seventh Framework Programme funding for collaborative projects. But \u201cit's still not a common occurrence across all funding agencies,\u201d Cummings says. Paul Jeffrey of Cranfield University, UK, suggests that funding agencies could also boost collaborative productivity by promoting long-term collaborations, rather than favouring new partnerships. \u201cYou can't lock two strangers in the room and expect them to get along, and you can't do that with scientists either,\u201d he says. \u201cThe overhead of setting up a productive relationship is very high.\u201d Weldon and Evans's overheads were certainly steep. In the battle over bovidic acid they lost time, money and, perhaps most importantly, trust. Weldon now has the bovidic-acid sample back. And although Evans says the sample is intact, Weldon says he is still a little nervous about opening it up and wants to wait until a colleague is present. \u201cI know it may sound a little paranoid,\u201d says Weldon. \u201cAfter all of this, I just want to be careful.\u201d So does Evans. Next time, he says, \u201cI'd do everything by the book. You must get the relevant documentation, it's absolutely imperative\u201d. See Editorial ,  page 665. \n                     Collaborations: Investigating international misconduct \n                   \n                     Nature Nautilus blog \n                   \n                     US Office of Research Integrity \n                   \n                     NSF Office of Inspector General \n                   Reprints and Permissions"},
{"file_id": "452678a", "url": "https://www.nature.com/articles/452678a", "year": 2008, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "Some creatures have what it takes to survive long dry spells. How they do this may be revealed in their genes, reports Erika Check Hayden. See also Correspondence: \"If you don\u2019t need change, maybe you don\u2019t need sex\":http://www.nature.com/uidfinder/10.1038/453587a and \"Ancient asexuals: darwinulids not exposed\":http://www.nature.com/uidfinder/10.1038/453587b \n                     Nature Web Focus: John Maynard Smith \n                   \n                     Wheelbase: A database of rotifer biology \n                   \n                     The Mark Welch Laboratory \n                   \n                     Meselson Lab Rotifer Research \n                   Reprints and Permissions"},
{"file_id": "452525a", "url": "https://www.nature.com/articles/452525a", "year": 2008, "authors": [{"name": "Erik Vance"}], "parsed_as_year": "2006_or_before", "body": "Sometimes it is necessary to immerse yourself in a subject. Erik Vance meets a woman whose research takes her deep \u2014 waist-deep \u2014 into cetacean anatomy. If you work as a toll collector at the Lincoln Tunnel in New York City, you might think you've seen it all. Until the day a woman drives past your booth with a dead dolphin stuffed in her hatchback. \"I had to fold the front seat over on the passenger's side and shove the bottlenose dolphin's face out of the front passenger window to fit the thing in the car,\" says Joy Reidenberg, almost losing her breath in laughter at the memory. \"And so coming in through the Lincoln Tunnel, they're saying 'What do you have as a passenger?'\" Almost all conversations with Reidenberg, a specialist in cetacean anatomy at the Mount Sinai School of Medicine in New York, end with her deep, hearty laughter. In an age where molecular-level work is king and medical students rarely touch dead bodies, she is one of a handful of researchers left who still embrace the title 'anatomist'. Although colleagues describe her as one of a dying breed, she regularly publishes research that breaks new ground \u2014 and maintains that anatomy is as important today as it was in the days of Aristotle. \"But this is not for the weak stomach,\" she says in her distinctive New York accent. \"Breathe through your mouth and hope the wind is strong.\" To obtain specimens for her research, Reidenberg has to bring back parts of cetaceans stranded on beaches from Florida to Maine. This means all-day marathons that are a cross between a Jacques Cousteau film and a night at the graveyard. \"To bring back a larynx the size of this table,\" she says, thumping on a conference room table, \"takes six people, a tug-of-war, and maybe a backhoe and a crane.\" Reidenberg has extracted body parts such as fins, brains and tracheas from all manner of cetaceans, including once from a massive blue whale. This may sound simple, but body parts can weigh tonnes and carcasses are often hard to access. At times she has been chased by sharks, lowered by crane into intestines as deep as a swimming pool, and even pinned against a boat by the errant rolling head of a pilot whale. Yet she finds plenty of eager volunteers to help. \"She was, and is, incredibly passionate about this type of anatomy. And that's pretty infectious,\" says Armand Balboni, a former student who is now a postdoctoral researcher at Mount Sinai. Her gross anatomy classes are hugely popular \u2014 although it's not clear whether this is because of her engaging personality or her habit of bringing whale flippers and sea turtles to class. But Reidenberg is more than just a popular teacher with an overflowing freezer. She holds academic appointments at the Smithsonian Institution, the Woods Hole Oceanographic Institution and the New York Consortium of Evolutionary Primatology. She has dissected more than 140 cetaceans of various sizes, representing 19 different genera. And she publishes regularly across a wide spectrum of topics. Much of her work examines the cetacean throat \u2014 including how breathing, eating and sound generation work together in an underwater environment. For instance, the mechanisms by which some baleen whales release clouds of tiny air bubbles to confuse prey have long been a mystery. By comparing seven humpback whale carcasses Reidenberg discovered some odd morphology in the epiglottis, a structure that helps separate eating and breathing. She deduced that it is specially formed to flip back and forth, channelling air either to the blowhole or to the mouth, where it would be forced through the baleen as a bubble cloud. This not only answers a question of mechanics, but it means that bubble clouds \u2014 which whales may use daily in colder waters \u2014 are valuable enough to risk water from the mouth entering the trachea and asphyxiating the animal (J. S. Reidenberg & J. T. Laitman  Anat. Rec.    290,   569\u2013580; 2007). Reidenberg has also looked into the mystery of sound generation in cetaceans. Vocal folds don't seem to exist in whales, and many researchers have assumed that most noises come from the nasal area. But that is because vocal folds in land animals run perpendicular to air flow. Reidenberg discovered a structure in cetacean larynges that runs parallel to air flow, thereby not blocking the airway when it vibrates (J. S. Reidenberg & J. T. Laitman  Anat. Rec.    290,   745\u2013759; 2007). She discovered a similar structure in hippos, the closest living relative to cetaceans. Evolution of the cetacean voice is still mysterious because the soft tissues of the throat don't fossilize well. But to Reidenberg, her discovery reinforces the idea that whenever possible, nature finds solutions using structures at hand, rather than inventing something new. In the case of whales, low-frequency sound travels well underwater. To make such sounds, you need longer folds. Long folds can't fit perpendicularly across the airway, so nature turned them 90\u00b0 for more elbow room.  \n                The fear factor \n             Reidenberg's closest associate and mentor, Jeff Laitman of Mount Sinai, co-authored both studies. But as a department head he doesn't always share her enthusiasm. \"The two words that have usually sent me into near heart attack have been the words 'mass stranding'. To her, this is one of the greatest excitements in the world,\" he says. \"I, on the other hand, have the wonders of explaining to the institutions, the boards, the loading dock, the security people the wonderful material that comes in \u2014 trying to stand there with a straight face and say, 'Odour? What odour?'\" With a lab overlooking Central Park, taking up some of the world's most expensive real estate, Laitman says that he cringes every time Reidenberg excitedly calls to say that she has contracted a flatbed truck to bring back some giant specimen. \"She's out there and my thoughts are, 'Oh Lord, please let it be something in which the larynx is less than five feet',\" he says. There are upsides to the job. For instance, Reidenberg rarely has to open her carry-on luggage once inspectors learn what's inside. And then there was the time she was asked by an airline whether her travel was because of a sudden death. Not thinking, she said it was. She flew to North Carolina under a special rate, presuming she was in mourning for a dead sperm whale. As a child in suburban Connecticut, Reidenberg says that she was always more interested in trucks and road kill than dolls. Once, at about age eight, she decided one of her few dolls needed a fur coat. Rather than asking her mother for one, she went out and found a dead chipmunk. She flayed it and was drying the hide when, to her horror, a raccoon took her prize. Today her road kill requires a little more work. In addition to trucks and cranes, a large whale necropsy requires perhaps a dozen people with specialized hooks and knives, which have often been often confiscated from whaling ships that wander into US waters. Dead whales are categorized into one of five codes, based on the level of decomposition. Code 1 is freshly dead, or as Reidenberg says, \"like walking into a butcher shop\". Code 5 is essentially a dried skeleton. The most common is Code 3, a terrible middle ground where the carcass still contains useful scientific material, but can be best described as putrid. At this stage, the dark red meat has turned tan and is peeling from the body, and organs have liquefied into an indecipherable mush. Many of the best dissections happen in the winter when the animals are preserved by the cold. But the thin latex gloves Reidenberg wears mean that she must intermittently bury her hands in the carcass to stay warm enough to feel her tools. Yet she will happily work for 12 hours extracting a brain that isn't even for her own research. Lori Marino, a neuroanatomist at Emory University in Atlanta, Georgia, who works with Reidenberg, calls her a \"scalpel and table\" anatomist. In truth, she is a hammer, chisel, axe, flensing knife and hand-saw anatomist. (She spurns chainsaws for their \"fling\" factor.) Marino says this gives her a crucial perspective over the entire body that few scientists have in this era of specialization. Together, Reidenberg and Laitman argue that researchers cannot understand molecular biology or gene expression without better understanding the form and function of anatomy. Further, they say that putting the human body in context requires understanding how it relates to other mammals. Yet anatomy remains a hard field to lure students to, and even harder to find funding in. Although Reidenberg's work has informed researchers who study human throat maladies such as acid reflux, she admits that it is not the kind of thing that interests important backers such as the National Institutes of Health. In fact it's so hard to get gross-anatomy funding that many medical schools don't expect their professors to publish at all. According to the American Association of Anatomists and various anatomy department heads, this leads to a vicious circle with fewer academics seeing a future in anatomy and then even less funding. \"Where is the next generation of anatomists coming from?\" asks Reidenberg. \"This is a big problem.\" She is trying to do her part in her classroom. But if you have a question about a lecture, you may not find her during office hours. That is, unless you are handy with an axe and don't mind the smell. Erik Vance is a freelance science writer in Berkeley, California. Reprints and Permissions"},
{"file_id": "452528a", "url": "https://www.nature.com/articles/452528a", "year": 2008, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Portugal's spending on research is near the lowest in western Europe. Can a single-minded lady with half a billion euros change things, asks Alison Abbott. Three years ago, everyone seemed to want neuroscientist Zachery Mainen. Janelia Farm Research Center, the Howard Hughes Medical Institute's prestigious new hub in Ashburn, Virginia, had just made a generous offer to recruit him and his frontier studies on decision-making. His current employer, Cold Spring Harbor Laboratory in New York, made a more tempting counter offer. But it was a third, unexpected proposal that he couldn't refuse. Even though the research centre in question did not yet exist. And even though it was in Portugal. Never in his most fanciful moments had Mainen imagined shifting his career to this small, relatively poor corner of Europe. Yet last July he became one of the first scientists to be signed up by the Champalimaud Foundation Research Centre in Lisbon. When completed in 2010, the centre will be the most grandiose and expensive life-sciences research project the country has even seen. The scheme has been master-minded almost single-handedly by a forceful lady named Leonor Beleza. Beleza knew little about science until 2004, when she found herself named president of a half-billion-euro endowment for biomedicine left by Portugal's richest man, Ant\u00f3nio de Sommer Champalimaud, on his death. Beleza spent a year or so consulting with the world's scientific \u00e9lite before deciding how to spend it \u2014 lavishly, as it turned out. The Champalimaud Foundation Research Centre will have everything money can buy: a striking US$120-million building designed by renowned architects, top-of-the-range equipment and at least 300 scientists. It will also have a waterfront location on the very spot from which Vasco da Gama and other fifteenth-century navigators departed to discover the unknown world. What convinced Mainen to move there was the opportunity to pursue his research \u2014 on how animals use experience and sensory input to make decisions \u2014 in any direction he chose, and without undue pressure to secure additional funding. \u201cIt's got the backing to be a world-class research institute,\u201d he says from his temporary laboratory at the Gulbenkian Institute of Science in Oeiras, where he is working until the new building is finished. It is as bold a project as can be imagined in a country that for some time has had one of the lowest investments in science and research in the European Union outside the former communist member states. In the past few years, the Portuguese science base has grown and modernized rapidly, and spending has risen, says research minister Jos\u00e9 Mariano Gago. Even so, in 2005, Portugal spent 0.8% of its gross domestic product on research and development, compared with an average in the European Union of 1.8%. Some hope that the unprecedented injection of money from Champalimaud will quicken efforts to transform the country's research. \u201cThis new institute will make a huge difference to life sciences in Portugal,\u201d says Ant\u00f3nio Coutinho, director of the Gulbenkian Institute.  \n                Generous tycoon \n             Champalimaud began building up his fabulous fortune in the 1930s with a cement factory. Portugal, a dictatorship since 1926, was a friendly country for the determined capitalist at the time. But after the 1974 'Carnation revolution', the new socialist regime forced Champalimaud nearly penniless into exile. Undaunted, he rebuilt his wealth in Brazil and, with the maturing of Portugal's democracy, Champalimaud was able to return home in 1992 and buy back many of his former companies. Champalimaud was a very private man and made his own decisions. It was only after his death that his relatives found out he had earmarked one-quarter of his wealth for biomedical research. But on the day Champalimaud signed his will, he called Beleza out of the blue to ask in confidence whether she would agree to be president of the planned foundation. \u201cI was flattered \u2014 but really surprised,\u201d she says. \u201cI had only met Champalimaud fleetingly on a few occasions over the years and was not aware he had particularly noticed me.\u201d Attracted by the philanthropic opportunity, Beleza said yes. She heard nothing more until Champalimaud died four years later. Beleza, a carefully elegant and engaging lady nearing 60, trained as a lawyer but spent most of her professional life in politics. Known for her dogged determination, she was once considered Portugal's Margaret Thatcher. But she dropped from the limelight after she was implicated in a scandal involving the distribution of blood contaminated with HIV during her stint as health minister in the late 1980s. More than 120 people with haemophilia had become infected by blood products imported from Austria.  \n                Uncharted territory \n              Beleza pulled out of politics altogether when she took up the reins of the Champalimaud Foundation and had to decide what to do with the sudden fortune. Champalimaud had left no guidance on how the money should be spent other than that it should be for biomedical research. Seeking direction, Beleza took herself on a brisk tour of the scientific world, visiting places such as the Massachusetts Institute of Technology (MIT) in Cambridge, Cold Spring Harbor Laboratory and the local Gulbenkian Institute, which was funded by another Portuguese foundation established by a wealthy benefactor, Calouste Gulbenkian. \u201cI didn't know Champalimaud as a person, so I had to try to reconstruct in my mind what he might have wanted,\u201d Beleza says. Two things occurred to her immediately. Being an entrepreneur, Champalimaud would probably like to support translation of results from the bench to the clinic, she thought. And since he was nearly blind when he died at 86, it would be appropriate to support research into vision. Beleza's tour refined her initial ideas. She recalls having an epiphany when talking with MIT's Nobel laureate Susumu Tonegawa, who told her that vision was too narrow an area for a research institute. \u201cHe explained how much our vision is controlled by our brains and that we need to understand much more basic neuroscience before we can start systematically translating results to the clinic.\u201d This and other conversations convinced Beleza that a large part of the money should support basic research without any requirement for translation or, indeed, any strings attached at all. Coutinho offered to help develop a competitive neuroscience programme for the centre, along with temporary lab space for new recruits, allowing them to start without delay. And he encouraged her to also consider cancer research, an area that is more readily translatable. It is also particularly appropriate in Portugal, where cancer care there is among the poorest in Europe, according to a 2005 consultancy report on health care commissioned by the Champalimaud Foundation. In the final plan, Beleza settled on an annual prize of \u20ac1 million (US$1.6 million) for vision \u2014 to be awarded for research or for preventing blindness \u2014 together with the new research centre, which will combine clinical cancer research and basic neuroscience with a focus on identifying the neural circuits involved in driving behaviours. The decision came as a disappointment to many scientists around Portugal who had lobbied for a more general distribution of the wealth. But Beleza was adamant about pursuing the big project. Her conversations with the scientific \u00e9lite had convinced her of the value of focusing resources. They had also shown her that good scientists will move to Lisbon only if the conditions on offer are better than those elsewhere. One advantage that the new principal investigators will have, for example, is a 'parachute fund'. This will allow them to take some of their allocated research money with them if they decide to bail out of the Champalimaud institute \u2014 an important provision for scientists who may be wary about joining a new and unproven institute.  \n                Changing tack \n              This, and the opportunity to do blue-sky basic research, are tempting Portuguese expatriate Rui Costa, currently at the National Institutes of Health in Bethesda, Maryland, to move permanently to the Champalimaud Institute. Costa already has an adjunct position there, examining the dopaminergic circuits involved in movement. \u201cIt's nice that the Champalimaud people will allow you not only to do good science, but to do it under new conditions,\u201d he says. So far Beleza has recruited just three principal investigators and not, as yet, a director. She says she wants the best scientists from all over the world, but \u201cit would be nice if some were Portuguese\u201d. Reaction to the centre has generally been positive, although it is still early to say how well it will compete with established research institutes or whether the mix of cancer and neuroscience will prove fruitful. With no thought to modesty, top award-winning architect firm Charles Correa Associates \u2014 who built the Brain and Cognitive Neuroscience Complex at MIT \u2014 has been contracted to build a research institute with a cancer clinic worthy of the project's grandiose aims. The target completion date of 2010 will coincide with the centenary of Portugal's republic. Until then, on the foundation's webpage, the virtual fly-through of the architectural plans provides an exhilarating ride. Outside, the complex boasts a broad public plaza that ascends gradually alongside the building to yield a sudden, stunning view of the Atlantic Ocean. The design is a fitting symbol of voyages into the unknown, of both fifteenth-century ocean explorers and twenty-first-century researchers. And Mainen, for one, is excited about the discoveries ahead. \u201cI never expected to end up here,\u201d he says, \u201cbut I hope this project will plant Portugal's flag on the research map.\u201d \n                     Neuroscience \n                   \n                     Champalimaud Foundation \n                   \n                     Champalimaud Foundation Research Center \n                   \n                     Gulbenkian Institue of Science \n                   Reprints and Permissions"},
{"file_id": "452806a", "url": "https://www.nature.com/articles/452806a", "year": 2008, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "Old human remains found on the Pacific islands of Palau are caught in the crossfire between entertainment and science. Rex Dalton reports. Circled by a protective coral reef, the 300-island archipelago of Palau is one of the Pacific Ocean's most biodiverse ecosystems. The first intrepid voyagers who arrived here, more than 3,000 years ago, would have found lush plants and waters teeming with fish and crustaceans. By 2,500 years ago the Palauans were even practising sophisticated agriculture, creating terraces on the archipelago's largest island on which to grow crops. Given this evidence, many archaeologists were stunned by a report last month that a tribe of small-bodied humans had purportedly lived on some of the islands 1,000\u20133,000 years ago and possibly suffered dwarfing because of limited resources. The article 1 , by palaeoanthropologist Lee Berger of the University of Witwatersrand in South Africa and his colleagues, suggested that the tiny Palauans might help inform the debate over the controversial 'hobbits'. These remains, found on the Indonesian island of Flores, 2,000 kilometres to the south of Palau, are believed by many researchers to represent a distinct dwarf species called  Homo floresiensis,   which died out 12,000 years ago 2 ,   3 ,   4 . Berger and his team argue that their Palau find suggests that the hobbits may not be a separate species, as dwarfing can occur in island environments. But veteran Pacific anthropologists quickly began dissecting the new article. Many of them now hold that the report offers little support for the assertions of a dwarfed tribe. In fact, they say the bones are likely to be from normal-sized island dwellers, possibly juveniles. \"The more I read their paper, the more I am convinced it is complete nonsense and cannot be accepted as serious science,\" says Michael Pietrusewsky, an anthropologist at the University of Hawaii at Manoa, who is considered to be the primary authority on Pacific island skeletons. The use of these bones to make assertions about  H. floresiensis   or other  Homo   species, he says, \"is totally inappropriate\". Berger responds that such criticism is ill-founded. \"Might it be that such critics have not read our manuscript as carefully as is required of a sophisticated debate on human variation before commenting?\", he told  Nature   in an e-mailed response to questions.  \n                The drama of discovery \n              To some, the Palau story illustrates how science can get caught up in the entertainment process. Like many palaeoanthropologists, Berger has long worked with film crews to document discoveries. But sometimes the demands to catch a significant finding on tape can clash with the slow, rigorous nature of the scientific process. The question anthropologists are asking now is: did entertainment needs in Palau overwhelm the evidence from field research? In other areas of his research, Berger has worked on a planned television series featuring him called  Fossil Hunter,   which uses the slogan \"entertainment first, science second\". For the Palau work, he teamed up with a London-based production company, Parthenon Entertainment, to create a film later distributed by the National Geographic Society of Washington DC. The National Geographic Society awards some $5 million in grants annually to researchers from a number of disciplines \u2014 including the popular ones involving human ancestors, dinosaurs or animal stories. For scientists in these fields, small seed grants to pay for field trips are hard to secure, so the society's grants are valued highly and often produce quality research. But National Geographic is also a non-profit media empire, netting $100 million on revenues of $500 million in 2006. Its editors work to get featured discoveries by its funded researchers into both its flagship magazine and peer-reviewed journals at the same time. This arrangement can sometimes backfire, such as in 2000 when the magazine featured a report on a flying dinosaur fossil that later turned out to be a cleverly faked composite 5 , 6 . Berger's project in Palau provides a behind-the-scenes view of when entertainment and science meet. It is a process that can sometimes leave scientists disgruntled (see  'Caught out on camera' ), and can lead to accusations from some communities in developing nations that they have been exploited. For some, what happened in Palau offers examples of some of the pitfalls. \"There is something unseemly about how this evolved and was orchestrated,\" says William Jungers, a palaeoanthropologist at Stony Brook University in New York.  \n                Paradise place \n             The tale of the Palau project began in April 2006, when Berger, an American by birth, was on holiday with his family from Johannesburg. With countless channels of placid shallows, coral reefs and more fish than Hawaii, Palau is known worldwide to divers and kayakers. Its jungle has also been used to film the reality television programme  Survivor . On the last day of his trip, Berger says, a guide took him on a kayak trip to a cave called Ucheliungs, which a brochure noted held bones. Venturing inside, Berger nosed around and spotted some remains, including a skull with a prominent brow over the eye sockets \u2014 a sure sign of primitive traits. The island containing the high-domed cave is off-limits to visitors, officials say. Berger later acknowledged that he didn't know the rules at the time. But other researchers, such as Timothy Rieth, an archaeologist at the International Archaeological Research Institute in Honolulu, Hawaii, questioned those steps ashore. A decade ago, Rieth worked on an archaeological assessment of an unrelated Palau project. On a boat trip one day, Rieth viewed the mouth of the other cave studied by Berger, called Omedokel, but didn't venture inside even though bone piles could be seen. \"I don't just go inside burial caves on vacation because it's fun,\" says Rieth, who was with a Palau cultural official at the time. Berger has made waves with his research before. In 1995, early in his career in South Africa, he and his Witwatersrand colleague Ron Clarke gained attention for their proposal that the 'Taung child', a 2.5-million-year-old fossil from South Africa, had been damaged by an eagle or other large bird of prey 7 . Berger still strongly supports this hypothesis: in 2006, he published a paper arguing that the bird had actually preyed on the Taung child 8 . After returning to South Africa from Palau, Berger flew to Washington DC, seeking research funds from National Geographic to explore the caves. By June 2006, he was back in Palau's largest city, Koror, with an emergency grant to seek permission from authorities to commence a full research project. Formerly a US protectorate, Palau became independent in 1994. Today, the nation is still learning how to balance traditional customs with US-like laws. Quickly, Berger's proposal became caught in a political power struggle between the Koror government and the state Council of Chiefs, whose members represent various traditional groups. Culturally, Palauans like to be accommodating. In the end, they supported Berger, who received the necessary permits from the Palau Bureau of Arts and Culture. In summer 2006, accompanied by a film crew, he began exploring both Ucheliungs and Omedokel. Team member Steven Churchill, a palaeoanthropologist at Duke University in Durham, North Carolina, recalls how excited they all were at finding so many bones of what they thought were small-bodied and ancient people; they suspected that they might have found a cave of dwarfed humans that predated any other Palauans. If so, the people might have been related to the Indonesian 'hobbits', or might offer insight into how  H. floresiensis   shrank in size over time. This was a selling point of the National Geographic grant application, says a member of the society's research committee, palaeontologist Philip Gingerich of the University of Michigan in Ann Arbor.  \n                Short shrift \n              To archaeologists who have long studied the archipelago, bones in Palauan caves are nothing new. An archaeological survey that started in the 1950s reported Omedokel as a burial cave for the young, including the child of an ancient chief 9 . In the late 1970s, Pietrusewsky began studying another collection of western Pacific skulls, including 13 gathered on Palau by Germans in the late nineteenth century. He also studied other Palau skeletal remains in different museums. \"I found no evidence of people of short stature,\" says Pietrusewsky. And starting in 2000, archaeologist Scott Fitzpatrick of North Carolina State University in Raleigh began excavations in a burial cemetery on the island of Orrak, about 4 kilometres north of Ucheliungs. In 2003, he reported the oldest human burials in the western Pacific, dating to 3,000 years ago 10 . These included the remains of many youths and children. Fitzpatrick says he has also found explicit evidence undermining Berger's theory that the island residents were small-bodied individuals. The Berger paper describes heads of femur bones that suggest the people there were small. But the shafts of these leg bones are missing from the specimens, limiting the ability to compute body height. Fitzpatrick, however, found femoral heads of similar size at Orrak, but with the shaft. Analysing the shaft indicates that the people at Orrak were of normal height, he says. Berger, charges Fitzpatrick, \"hasn't made adequate comparisons to other skeletal material from Palau. And I don't think he understands variance in human populations.\" Berger notes that the techniques for estimating body size can have large error bars, but says that more than 61 skeletal elements from the caves suggest the people were at the extreme low end of the range. He also says that the bones described in the team's paper all show indications of coming from adults. \"We published the entire inventory,\" adds Churchill. \"A number are adults. I'm willing to accept there are some juveniles, but they don't change our conclusions.\" John Hawks, an anthropologist at the University of Wisconsin in Madison who served as editor for the article's peer review, says \"it is quite evident\" that the skeletons represent humans \"in the normal range\" of being small. Another area of dispute is how exactly the people, if adults, got so small. The original founding population of the caves, says Berger, may have been naturally small. Or the tribe may have dwarfed over time, perhaps because of a lack of large mammals or reptiles to hunt or because of environmental stresses. But other experts disagree \u2014 including Stephen Athens of the Honolulu research institute, who has drilled repeated cores in Palau wetlands capturing pollen records dating back 5,000 years 11 . There is nothing in the environmental record to indicate resource depletion, he says. Pollen shows taro root and coconut. Along with the marine resources, \"that is a pretty darn good diet\", Athens says. In fact, the pollen record shows a sharp rise in grasses and ferns about 4,000 years ago \u2014 which could mean humans were clearing forest for agriculture then. And work by Jolie Liston, an American archaeologist studying Palau, describes elaborate agricultural terraces by 2,500 years ago on Palau's largest island, Babeldaob 12 . \"These terraces run five to six kilometres,\" says Liston. \"Land is sculpted, dirt removed, gullies formed \u2014 all to create about a dozen distinct districts.\" In late 2006, when the first skull bones from Berger's excavations were prepared in the Palau National Museum in Koror, Berger's team learned it didn't have, as hoped, a group that dated to 10,000 years or more. The heavy brow over the eyes that suggested primitiveness turned out to be calcite, which came off as the bones were being prepared. \"The idea they were pre-Palauans went out the window then,\" says Churchill. Instead, the team decided to focus on describing the bones; the small nature of the remains could provide context for other finds such as  H. floresiensis.  Meanwhile, resource managers in Koror hope the attention will bring funds from National Geographic to help protect the caves. Increased attention, they say, will bring more people wanting access to the caves, which need to be preserved. Koror governor Yositaka Adachi says the state hopes to get about $500,000, including a new patrol boat. Barbara Moffet, a spokeswoman for the society, says that the society hasn't had any such requests yet, but that \"we are willing to consider helping them in some way if such assistance is appropriate\". But for other researchers, the affair has tainted the entire field. \"Ultimately,\" says Fitzpatrick, \"I worry this will put a strain on all archaeologists doing research in Palau.\" \n                     Flores Man web focus \n                   \n                     Lee Berger \n                   \n                     Fossil Hunter television series \n                   \n                     National Geographic \n                   \n                     International Archaeological Research Institute \n                   Reprints and Permissions"},
{"file_id": "452798a", "url": "https://www.nature.com/articles/452798a", "year": 2008, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "Is the Arctic's biggest ice sheet in irreversible meltdown? And would we know if it were? Alexandra Witze reports. When people talk about catastrophic climate change, there's a fair chance that Greenland is on their mind. If they use the term 'tipping point', then it is pretty much a sure thing. One-twentieth of the world's ice is locked up atop that island, and if it were to melt completely, global sea levels would rise by seven metres. The collapse of the Greenland ice sheet is in the front rank of potential climate catastrophes. Melting is already undoubtedly and dramatically underway. Glaciers are spitting icebergs into the ocean and scurrying back up their narrow fjords like rats up drainpipes. Giant lakes are forming on the frozen surface, sending torrents of water plunging through fissures in the ice sheet and thus, perhaps, accelerating its slipping and sliding seawards. Over the past four summers, Greenland has shed an average of between 380 billion tonnes and 490 billion tonnes of ice each year \u2014 on average 150 billion tonnes more than it gains in snow in winter. That's a lot of water. It is not, as yet, a lot of Greenland's ice, which totals 2.9 million cubic kilometres. Such size brings with it an inherent sense of stability. We do not expect things bigger than mountain ranges just to go away. But there's a disturbing sense in which Greenland shouldn't be here in the first place. It is a holdover of the most recent ice age, a creature of conditions that no longer apply. No ice sheet would grow in Greenland if the current one were to vanish \u2014 even without human-induced warming, the climate would not allow it. The ice is a relic, stranded out of time. And relics are fragile. The question is, how fragile? Has the warming the sheet has experienced so far and the further warming already in the pipeline enough to push the ice sheet past a point of no return 1 ? If that is not yet the case, how far from that threshold are we? And if the sheet does start to go, how fast will it do so? The sheet will not vanish tomorrow, nor in a century \u2014 but assumptions that such processes take millennia are being reexamined on the basis of the changes already seen. The most recent synthesis report from the Intergovernmental Panel on Climate Change notes that the changes seen in Greenland today are not fully factored into the estimates of sea-level rise given in earlier science reports from the panel \u2014 a note that those who see Greenland as a potential poster child for catastrophe have made much of. As yet, these pressing questions simply cannot be answered. They require models and theories not yet fully developed. And that lack of development is in part a lack of data \u2014 good data that show clear trends. Even though researchers scatter themselves around the island every summer to try to capture the meltdown's extent and processes, there is no systematic, long-term, broadly based monitoring of the sort needed to produce a truly comprehensive account of what is happening with the ice sheet. \u201cDo we have the data we need to understand what's driving these changes?\u201d asks Ian Howat, a glaciologist at Ohio State University in Columbus. \u201cThe answer is definitely no.\u201d  \n                The gravity of the situation \n              To get the best overview of the great Greenland meltdown, you need to go to space and look for gravity. The Gravity Recovery and Climate Experiment (GRACE) is a pair of US\u2013German satellites that orbit Earth 500 kilometres up, one close behind the other. Through constant interchange of microwaves the satellites measure the distance between them very precisely, and that distance changes as massive objects below tug at the leader and the follower in slightly different ways at any given instant. The small discrepancies so produced can be used to calculate a gravity map for the planet. As masses move around, that map will change. Data from GRACE have revealed how the water flow in the Amazon basin changes with the seasons, and which Asian aquifers are replenished by the monsoons. The mission has also provided new information about the flow of water off the massive ice sheets in Greenland and Antarctica. \u201cWhen you look at a lot of the insight we have\u201d about Greenland, says Konrad Steffen, a glaciologist at the University of Colorado at Boulder, \u201cit's GRACE\u201d. The estimates vary as to how much mass is lost through melting each summer. Isabella Velicogna of the University of California, Irvine, leads a group that takes a large-scale approach 2 , averaging the global gravity numbers provided by GRACE for each 30-day period. Her latest estimate suggests that 211 billion tonnes of ice are being lost each year, mainly from southern Greenland. \u201cThere is no doubt that things are changing faster than we expected,\u201d she says. Meanwhile, Scott Luthcke of NASA's Goddard Space Flight Center in Greenbelt, Maryland, takes a different tack, using the changing distance between the satellites to calculate the pull of smaller mass concentrations on the ground over time 3 . Including the 2007 melt season, he gets preliminary estimates of 154 billion tonnes of ice lost per year. The numbers sound different, but both groups emphasize how close they are, and over time there seems to be some convergence. \u201cThese are two vastly different ways of processing data, and they're almost within the error bars,\u201d says Luthcke. \u201cGreenland is losing a lot of mass.\u201d GRACE is also providing clues as to how the situation varies from year to year \u2014 particularly for the last melt season, when surface temperatures were 4\u20136 \u00b0C higher than average and during which 500 billion tonnes of ice vanished. That's 30% more than the previous year, and 4% more than the previous record, set in 2005. \u201c2007 was a shocking year,\u201d says Luthcke. And GRACE's findings are bolstered by observations of dramatic ice losses by other satellites. Radar measurements, for instance, have shown 4  that glaciers in southern Greenland are dumping ice into the ocean ever more quickly. At the American Geophysical Union meeting in San Francisco in December, Velicogna presented results showing that the GRACE estimates are supported by data taken from the Ice, Cloud and Land Elevation Satellite (ICESat), which uses a laser altimeter to measure elevation changes on the ice sheet.  \n                An island rising \n              One reason why extra information is needed to supplement the data from GRACE is the problem of 'post-glacial rebound'. As big as Greenland's ice sheet is today, in the ice age it was just a part of something far bigger, ice that reached as far south as the Ohio Valley and as far east as the Urals. That vast mass pressed the crust beneath it down into the denser mantle below. Although most of the ice has long since disappeared, large parts of the high-latitude crust have yet to recover from this repressed position. Scandinavia, for instance, rises 9 millimetres higher every year as the denser mantle pushes the lighter crust back up. This ongoing bounceback makes analysing the GRACE data harder. Help may soon come from a system of global-positioning receivers that have just been installed around Greenland to measure how the bedrock is rising over time. Last summer, a team of researchers from the United States, Denmark and Luxembourg put 24 stations around the rocky, ice-free edges of the island \u2014 tripling Greenland's global positioning system (GPS) infrastructure in one field season, according to Michael Bevis, the project leader at Ohio State University. The Greenland GPS Network (GNET) is one of the northern components of a two-pole effort called POLENET to measure post-glacial rebound and other phenomena; there will eventually be around 50 GNET stations in Greenland. \u201cWe need much improved models of post-glacial rebound, otherwise GRACE measurements will have very limited value in Greenland and Antarctica,\u201d says Bevis. \u201cIf we can pull this off, GRACE will become the most powerful system ever devised for measuring ice mass change.\u201d The GNET stations are strung along the rocky margin of Greenland, mainly in remote areas (see map, right). They require a lot of battery capacity to continue operating throughout the winter months, and links to five of the stations installed last summer have already gone down. The team is planning to retrieve the data manually and fix the stations this summer. All this makes GNET a fairly expensive proposition. The last field season consumed about US$1 million, and flat budgets, rising fuel costs and the weak dollar are making things even tighter this year. The rest of the GNET stations will have to go in over the next two summers instead of all in 2008, as originally planned. The GNET receivers are expensive, highly precise, heavy and, in principle, durable. Another monitoring strategy takes the opposite tack; it uses GPS equipment cheap enough to lose, embedded at the calving fronts of some of Greenland's most active 'outlet glaciers'. These are the thick streams of ice that flow through narrow fjords into the oceans surrounding Greenland. A decade ago, researchers thought that these outlet glaciers moved slowly, creeping downward from the high centre of the ice sheet. In recent years, though, the glaciers have been doing a veritable hokey-cokey on their approach to the ocean, first advancing rapidly, then pulling back. It started more than a decade ago with the biggest outlet glacier of all, Jakobshavn Isbr\u00e6 on the west coast, which among its claims to fame is the most likely source of the iceberg that sank the  Titanic . Between 1992 and 2003, Jakobshavn Isbr\u00e6 accelerated from 5.7 kilometres per year to 12.6 kilometres per year 5 . \u201cThat was incredibly dramatic,\u201d says Ian Joughin, a glaciologist at the University of Washington's Applied Physics Laboratory in Seattle. \u201cA decade ago, nobody would have anticipated one of Greenland's biggest outlet glaciers doubling its speed.\u201d Faster glacier movement means more ice dumped into the ocean, and a thinning of the central ice sheet from which the glaciers feed. Over on the east coast, the island's other two big outlet glaciers also started speeding up 6 : Helheim in 2002, and Kangerdlugssuaq in 2005. The process didn't go smoothly. Helheim, for instance, retreated more than 3 kilometres between 2001 and 2003 as its front melted away faster than new ice flowed down to make up the difference. Then in 2005 it began advancing again as flow took over. This back-and-forth is captured most dramatically in remote-sensing images from satellites such as NASA's Terra and Aqua. \u201cThere is a lot of variability, and the important thing to remember is we only have a few good years of observations,\u201d says Joughin. \u201cWe don't know if we are looking at the beginning of a longer-term trend.\u201d Joughin and others suspect that the back-and-forth of the outlet glaciers has a lot to do with the geometry of the fjords the ice squeezes through. The glaciers inch forward until their ends finally break off, calving icebergs into the ocean. This relieves stress on the glacier, which begins to surge, much as removing a buttress holding up a rickety old house will cause the house to collapse. But the scenario is not as clear-cut as it might seem. In the past, glaciers advanced and then calved off icebergs when they got too long. Now, the calving happens while the glacier is advancing. What this means is unclear, but it does suggest that the glaciers are behaving in a fundamentally different manner than just a few years earlier. \u201cThis is what we cannot predict,\u201d says Steffen. But the unpredictable is not necessarily unprecedented. During the 1920s, Greenland experienced a rapid warm-up; average annual temperatures rose more than 2 \u00b0C over the decade. At Ohio State, meteorologist Jason Box and student Adam Herrington have been looking for records of what happened to the Big Three outlet glaciers back then, to see whether there are lessons about what to expect in the future. Among their finds was a series of maps showing the snout of the Kangerdlugssuaq glacier. Over just a few years in the early 1930s, the glacier retreated some 10 kilometres upstream \u2014 having lost an area up to 70 square kilometres in what may have been a single large calving event. The break-up, says Box, was \u201cexceptional\u201d in that the ice would have taken years to grow back to its previous state. And it suggests that the sort of rapid response to warming seen in recent years is the glaciers' expected response to warming. One emerging area of research is the effect that ocean temperatures \u2014 as opposed to air temperatures \u2014 have on the outlet glaciers. Howat says, for instance, that warm ocean temperatures in the summer of 2003 coincided with a time when several of the outlet glaciers feeding into that warmer sea began speeding up dramatically. But little work has been done to correlate ocean temperatures with the glacier retreats. The ocean has been \u201ca total blank spot on the map,\u201d Howat says. \u201cYou have a big ice sheet with a lot of it sitting in the water \u2014 you'd think you'd want to know what's happening in the water.\u201d Some researchers are starting to target this as their next area of interest.  \n                Lakes on ice \n              The water that surrounds Greenland has been there forever. More novel is the increasing amount of water which, in summer, sits on top of it. What starts out in the winter as cold white snow ends up in the summer as a landscape of blue water, as more than 1,000 shallow melt lakes up to 5 kilometres across form on the ice. It is like Minnesota \u2014 but white. 2007 was a particularly good year to study this surface melting, because there was a great deal of it. High-pressure weather systems throughout much of the summer kept storms away, allowed the Sun to beat down on the ice almost without cease. The melt season lasted 25\u201330 days longer than average, and 19,000 square kilometres turned from ice to water, says Marco Tedesco of Goddard \u2014 that is roughly the area of Wales. The effect was particularly noticeable at higher elevations; as warm air swept ever higher, the area that melted at 2,000 metres or greater was 150% larger than normal. Even in a normal May to August field season, researchers have to make sure that their instruments stay anchored on the ice sheet, planting their poles 2\u20133 metres deep to make sure they can withstand the melt. It's not just the water that makes things difficult \u2014 it's the unpredictability. Melt lakes have been known to drain away tens of millions of cubic metres of water in the space of a day, swirling down some unknown drain channel in the ice. Huge waterfalls appear and then disappear overnight. How exactly the water gets from the top of the ice to its bowels isn't known, but understanding the plumbing could help illuminate a crucial question \u2014 does the water that reaches the bottom of the ice sheet lubricate it in a way that encourages movement and collapse 7 ? This has become a commonplace speculation among Greenland catastrophists, but the degree to which it is actually happening, how well it explains the ice loss measured by GRACE and to what extent it may change the shorelines of the world is not yet clear.  \n                Back to the Eemian \n              The suddenly apparent pace of change has led some to question previous, rather staid models of ice-sheet dynamics, which suggest that even fast changes take several centuries. \u201cIt has only been in the past five years that we have realized that hey, the ice sheet is falling apart, these changes are happening, our models are way off,\u201d says Howat. But predicting how far off they actually are \u2014 and some believe they may not be as soon as catastrophists predict \u2014 is not easy. Anecdotes of this or that particular, however momentous, are no match for thorough, consistent monitoring. If you want models of the future you can rely on, you have to monitor the process you model. \u201cThe modelling has not happened because there are just not enough data,\u201d says Philippe Huybrechts, an ice-sheet modeller at the Free University in Brussels. Researchers on the ice might see moulins forming, or outlet glaciers calving, but \u201cit is just in one place and for one season, or for a few weeks\u201d, he says. \u201cTo generalize from that over a whole ice sheet in a way that you can predict things, it's just not possible.\u201d At the moment, the best Greenland modellers are stretched by trying to explain what has already happened, without even thinking about what is to come. They are responsive, not predictive. But some are trying to change that. At the University of Kansas, Cornelis Van der Veen is helping to lead an effort to improve ice-sheet models; he and others are planning a major conference to be held in July in St Petersburg, Russia. The idea is to identify the big unknowns and figure out how to tackle them, one by one. \u201cOne outlet glacier speeding up isn't really the end of the world,\u201d he notes. \u201cBut if they are doing that all over the place then that is an indication that something is going on that we really do not understand. It is not something that can be solved within a couple of months.\u201d \u201cIt's very difficult to model a new process, such as why glaciers accelerate, before you have an understanding of why it is happening,\u201d says Dorthe Dahl-Jensen of the University of Copenhagen. \u201cOne thing is to observe it. The next step is to understand it. The third is to put it into the model and predict the future. We are at step two, struggling to understand the process.\u201d One route to understanding may be through palaeoclimate studies. Dahl-Jensen is leading a team that aims to drill a core 2,500 metres into the ice of northwestern Greenland over the next couple of summers. This core \u2014 the North Greenland Eemian Ice Drilling \u2014 would complement the pioneering climate records cored out of Greenland's ice over the past couple of decades. None of the earlier cores was able to extract an unbroken record of the Eemian stage, some 120,000 years ago and the last time that Earth was in a warm 'interglacial' period. Understanding what Greenland was like then could help scientists understand how the ice sheet might respond in a warmed future, says Dahl-Jensen. Temperatures in Greenland were roughly 5 \u00b0C higher during the Eemian than they are today. Yet sea level was only one or two metres higher, and every ice core that has ever been drilled deep enough on the island has included some ice from the Eemian. \u201cA major part of the Greenland ice sheet survived,\u201d says Dahl-Jensen \u2014 and argues that more sampling of this period might help to pinpoint the factors that could allow ice to stick around when temperatures are higher than today. But this is not necessarily the encouraging news it might seem. Because global warming is amplified near the poles, 5 \u00b0C of warming in Greenland might be achieved with just 2.5 \u00b0C of average global warming \u2014 which is quite plausible. And an important regional factor here might be the dramatic recent reductions seen in the sea ice to the island's north. The extent to which the cold, reflective ice on the sea keeps Greenland cool is simply not known.  \n                The long view needed \n              More sampling of the past, together with that of the present, may help us to unravel the future of the Greenland ice sheet. For now, though, things seem to be getting more ravelled, not less. Every year brings a new set of data, a new insight into the behaviour of Greenland's interlocking ice-sheet dynamics, stream flows and glacial surges. \u201cWe are just learning so much,\u201d says Leigh Stearns, a glaciologist at the University of Maine at Orono. \u201cEvery summer brings something totally different.\u201d But if the learning is copious, it is not systematic. Despite the real risk of a meltdown \u2014 and the real benefits to be gained from being able to say something reliable about how long there is to go, and how high the seas might rise \u2014 the investigation of the ice's every nook and cranny is far from over. One idea is to use unmanned aerial vehicles (UAVs) to fly across the ice sheet gathering data such as the depths of melt lakes. But this is easier said than done; John Adler, a PhD student at the University of Colorado at Boulder, ran some tests last August in which he took three types of commercially available UAVs to Kangerlussuaq airport and ran flights over the melt lakes for a week. He is still getting the kinks out of the system, but says that UAVs could provide a cheaper and more repeatable way to get local measurements than relying on expensive helicopter flights, as is done today. Even so, the UAVs are labour-intensive and cannot be operated all year round. Over the long term, satellites should provide the most coherent record of change. The Terra and Aqua satellites, along with Europe's Envisat and other surface-monitoring satellites, are workhorses that regularly photograph the advance and retreat of outlet glaciers. Despite its glitches, the ICESat altimeter sends back elevation changes that track the thinning of the ice sheet; a successor, ICESat-II, is already in the works. And the European Space Agency is working to launch a successor to the ice-thickness-measuring CryoSat, the first incarnation of which failed after launch in 2005. Yet major problems remain in acquiring and using Earth-observation data (see  Nature 450, 782\u2013785; 2007 ). Access to data from Canada's Radarsat-1 and Radarsat-2 for instance, is ensnarled in a potential takeover by a US company, a sale that was blocked last week by the Canadian government. Even with the right satellites, not everything can be done from space. Yet very few researchers have Greenland as the main focus of their scientific work. Decades from now, this could turn out to be one of the most short-sighted allocations of resources that began the twenty-first century. Climate change elsewhere in the Arctic has been swifter than anticipated. The remarkable shrinkage of the sea ice is \u201cthe largest change in Earth's surface that humans have probably ever observed,\u201d Howat points out. Trying to get any and every handle on how that affects the poised mass of ice next door must surely be a priority, he says. \u201cThis should be a critical thing to study.\u201d See Editorial,  \n                     page 781 \n                    . For a video of a camera being dropped down a moulin on the Greenland ice sheet, see  \n                     http://tinyurl.com/5dua42 \n                    . \n                     GRACE mission \n                   \n                     POLENET \n                   \n                     Video of a camera down a Moulin \n                   Reprints and Permissions"},
{"file_id": "452803a", "url": "https://www.nature.com/articles/452803a", "year": 2008, "authors": [{"name": "Liesbeth Venema"}], "parsed_as_year": "2006_or_before", "body": "Some experts think that a quantum computation could be plaited like a skein of string. And now they may have found the sorts of string they need, finds Liesbeth Venema. How much absurdity can you take? If you are a mathematician or a quantum physicist, the answer is probably quite a lot. Topologists, for example, regard a coffee mug and a doughnut as essentially one and the same: two different variations on a basic shape-with-a-hole-in-it theme. Quantum-computer enthusiasts are happy to deal with the idea of 'qubits' that can be ones and zeroes at the same time. Condensed-matter physicists have no problem with the idea that electrons can break up into 'quasiparticles' with fractional charges \u2014 a third that of one of the original electrons, say. And theorists are happy to accept that there are particles called anyons that live only in two dimensions and are part electric charge, part magnetic flux. But there are limits. And when in 1997 Alexei Kitaev, now at the California Institute of Technology in Pasadena, published a preprint suggesting that the topological properties of quasiparticles, moving around each other and behaving as anyons, could be used as the basis for a new form of error-proof quantum computing 1 , it seemed that he had gone beyond them. \u201cI laughed when I first read it,\u201d recalls Nick Bonesteel, a theoretical physicist at Florida State University in Tallahassee. And there may still be some people laughing today \u2014 but at least a few of them are doing so with excited anticipation. \u201cIntellectually, the idea might seem far-out,\u201d admits mathematician Michael Freedman, currently based at the Microsoft research group called 'Station Q'. Station Q was founded in 2005 at the University of California, Santa Barbara, specifically to tackle this sort of topological quantum computing. \u201cTo understand it requires knowledge of the maths as well as condensed-matter physics, which may be off-putting at first.\u201d But gradually, the far-out came in. The attraction of the proposal is that the quasiparticles singled out by Kitaev happen to turn up in one of the most intensively studied condensed-matter systems of the past few decades \u2014 a quantum Hall device. These are systems in which, under the right conditions, charge carriers are forced into 'current channels' governed by the rules of quantum mechanics. In the early 2000s several theorists, including Bonesteel, began thinking seriously about ways to create qubits, the building blocks of quantum computing, in a quantum Hall device. Sankar Das Sarma, a theoretical physicist at the University of Maryland in College Park, teamed up with Freedman. \u201cFirst we developed this crazy idea about manipulating quasiparticles with a microscope tip. Then we realized that the current paths in a quantum Hall device already move quasiparticles around,\u201d says Das Sarma. Their blueprint for a topological qubit, together with related proposals from other groups, was presented at a meeting on 'Emergent phenomena in quantum Hall systems' in Taos, New Mexico, in 2005. Moty Heiblum, an experimental physicist at the Weizmann Institute of Science in Rehovot, Israel, says that the meeting made him and his colleagues return to the lab. \u201cMost of us had been feeling that the topic was already exhausted, but the meeting revived the field. For the first time, it was driven by theorists,\u201d he says.  \n                Neither boson nor fermion \n              A very clear task was set out for the experimentalists; construct a quantum Hall device in which the right type of quasiparticle shows up. Quantum Hall devices are made from stacks of semiconductor put together using well-known microelectronics fabrication techniques so as to create a two-dimensional electron gas, or 2DEG, at the interface between two of the layers in the stack. When a magnetic field is applied perpendicularly to one of these electronic flatlands, current channels form along the edges of the sample in which a specific number of electrons can travel \u2014 this is the quantum Hall effect. Chill the 2DEG down further, and crank up the magnetic field, and the electrons' behaviour becomes stranger; they start to interact strongly with each other and, as a result, appear to break up into smaller pieces: these are the quasiparticles. They have no independent existence \u2014 you couldn't lift one out of a 2DEG and set it on its way \u2014 but within the 2DEG they really do act in a particle-like manner. Still, they have peculiarities. Because they move in just two dimensions they do not follow the same quantum mechanical rules as real particles out in the three-dimensional world. Those particles come in two types: bosons, like the photon, and fermions, like the electron. These particles are something fundamentally different: anyons (see  'Braiding anyons' ).  \n                Non-abelian abilities \n              Not any old anyon can work as a qubit, though. A qubit has to be able to exist in several states at once. That is what, in computational terms, lets them represent '0' and '1' at the same time, which is what gives a quantum computer its power; where classical computer bits are either '0' or '1', qubits can be various degrees of both \u2014 and thus, in effect, can carry out different versions of a computation in parallel. This powerful capability, also called 'superposition' can be exploited to design quantum computers that can tackle challenging tasks such as factoring large prime numbers to break codes, searching databases much larger than any yet built, and simulating complex processes in interesting materials such as high-temperature superconductors. Superposition has already been demonstrated for some physical systems. Qubits with 'superposition' states can be based on trapped atoms and electrons; the sorts of quantum-logic operation that could lead to ultrafast factoring have been demonstrated on them. But this has happened only in very small circuits of four or fewer qubits. And it has become frustratingly clear how difficult it is to protect the fragile states of a qubit based on a trapped particle from the outside world. Even minute interactions with nearby surroundings can destroy them. And for each additional qubit in a circuit, many more calculation errors are introduced. \u201cIn theory it is possible, but the engineering problem to build a scalable quantum computer may be too big\u201d says Das Sarma. The topological nature of anyon-computing would get round the sensitivity problems, as long as the anyons were of the right sort. The key attribute is that the order in which a given set of interactions between anyons happens has to matter. That allows a given state to be a superposition of different histories \u2014 different ways of braiding the particles together. This 'order matters' criterion is expressed, mathematically, in terms of the structure of the group, which describes the interactions as being \u201cnon-abelian\u201d. If order didn't matter, they'd be abelian. Whether a quasiparticle will behave as a non-abelian anyon or not depends on the details of the way that the fractional quantum Hall effect gets electrons in the 2DEG to interact. By slowly turning up the magnetic field, experimentalists can 'tune' a quantum Hall device into different regimes. The regime seen as the most likely breeding ground for the quasiparticles that can act as non-abelian anyons has been described before 2 , but it is a delicate one, and has been hard to characterize or investigate. One of the most salient features, and one of those that was hardest to discern, is that the quasiparticles appearing in this regime should have a quarter of the charge of an electron. The evenness of the fraction is in itself is a curiosity: so far, mainly odd fractions, such as a third, or a fifth of an electron, have been seen. But not any round number would do. Half-charge quasiparticles would not be non-abelian, and this has been a real worry to the field, as there seemed to be a possibility that quarter-charge particles would clump into half-charge ones, thus losing their non-abelian mojo. Now such worries can be put to rest. In this issue of  Nature , Moty Heiblum and his co-workers report 3  measuring fractional charges in quantum Hall devices by forming a narrow constriction of controllable width, called a point contact, in the middle of the 2DEG, making it look something like a flattened hour-glass (see inset picture right). Current injected at one end is carried in the form of quasiparticles along one edge of the 2DEG and through the central constriction. But if the constriction is thin enough, not all the quasiparticles can make it from one lobe to the other: some fail to get through the point contact and jump to the other edge of the lobe in which they started. The discrete movements of quasiparticles that are diverted in this way at the point contact cause fluctuations in the currents on both sides of the contact. Analysis of this reverberating 'shot' noise reveals the precise value of the quasiparticles' charge. Because there are many other sources of noise that can mask the fluctuations, this kind of experiment demands an extremely clean and carefully constructed device. Merav Dolev, the graduate student on the project at the Weizmann Institute, had to balance many parameters during fabrication of the device which is about a millimetre in width and is made from a number of layers of gallium arsenide and aluminium gallium arsenide. The density of electrons in the 2DEG has to be very high \u2014 about three billion in a square millimetre \u2014 while the structure has to be so clean that electrons can move through the 2DEG for up to half a millimetre without hitting any sort of defect. And the 2DEG will inevitably be damaged in the process of putting down metallic wires to form the point contact, which means it has to start off even better than is needed.  \n                2DEG or not 2DEG \n              But the effort has paid off. An enormous amount of shot noise recorded while the device was cooled down to 10 milliKelvin has provided solid evidence in favour of quarter-charge quasiparticles. Independent confirmation of the existence of quarter-charge particles in such a set-up comes from a team at Harvard, the Massachusetts Institute of Technology in Cambridge and Bell Laboratories in Murray Hill, New Jersey. These researchers used a similar device, but measured the conductance between the two current channels at the point contact, rather than the shot noise 4 . This is the news the field has been waiting for. \u201cIn all my talks on topological quantum computation I've said this is the one experiment that needed to be done first. And frankly I didn't expect it this soon,\u201d says Das Sarma. However, although experts regard the experiments as important and necessary, many also see them as insufficient in themselves. To really validate the idea of making a computer this way you need to show that the quarter-charge quasiparticles really do behave in a non-abelian way. This can be done by making the quasiparticles encircle an island of anyons, as a rudimentary anyon braid. To do this, at least two point contacts need to be constructed that can be shut tightly so that all quasiparticles can be made to jump from one side of the 2DEG to the other. The quantum properties of the braid that is formed by this weaving around of quasiparticles will show up in the measured current-voltage characteristics. Building such a 'quasiparticle interferometer' is a challenge, given that making just one point contact in the required quantum Hall regime has proved difficult enough, but it is not a far-fetched ambition. Can we expect it this year? Dolev is cautious. \u201cThis field is just starting. In the next few months we will find out what the experimental difficulties are,\u201d she says. For example, the interferometer needs much more stable point contacts than were used in the system that first demonstrated the existence of quarter-charges. However, having identified a particular side-effect of the 2DEG fabrication process that produces instabilities in the point contacts, the group is already getting stabler devices. And if they don't succeed, competitors inspired by their work may. \u201cProving the non-abelian properties of [quarter charge] quasiparticles would be a hard experiment, but the work by Dolev  et al . will certainly motivate people to try it,\u201d says Bonesteel. \u201cOne of the most exciting aspects of this demonstration is that it shows the quality of devices is steadily improving,\u201d says Kirill Shtengel, a theoretical physicist at the University at California, Riverside. \u201cWith luck, we might see a non-abelian interferometer within a year.\u201d There are many experimental challenges to be met before a topological qubit sees the light. But everyone agrees that the concept of topological quantum computation is so attractive that it is worth a try. Ady Stern, a theorist with the Weizmann group, explains; \u201cOther quantum computing approaches have a head start, but the concept is amazingly beautiful and the one advantage \u2014 topological protection of quantum states \u2014 is huge.\u201d Shtengel agrees: \u201cOther approaches have serious problems with scalability. Right now we are behind, but once we have a non-abelian quasiparticle interferometer, a topological qubit should soon follow.\u201d Getting those qubits to knit together as computations will require geometries a lot more complex than a flattened hour-glass. In the long run, it is possible that some more suitable system will be found for topological quantum computing \u2014 something perhaps a little more user, or developer, friendly than an ultra-clean and precisely designed quantum Hall device that needs to be cooled down to near-zero temperatures. Whether it will sound any less absurd to begin with, though, we will have to wait and see. Liesbeth Venema is a senior physical sciences editor at  Nature . See News & Views,  page 823.  and Correspondence:  A prime problem that even quantum computing can\u2019t solve \n                     Moty Meiblum's homepage \n                   \n                     Nick Bonesteel's homepage \n                   \n                     Microsoft Station Q \n                   \n                     Sankar Das Sarma's homepage \n                   \n                     Kirill Shtengel's homepage \n                   Reprints and Permissions"},
{"file_id": "452148a", "url": "https://www.nature.com/articles/452148a", "year": 2008, "authors": [{"name": "Ewen Callaway"}], "parsed_as_year": "2006_or_before", "body": "Huanglongbing, a disease that could devastate the US citrus industry, pits national security against plant pathologists looking to battle natural outbreaks, Ewen Callaway reports. In California's San Joaquin valley, it is citrus season. An open-air warehouse on the outskirts of Dennis Johnston's 600-hectare citrus grove brims with huge crates of oranges, mandarins and grapefruits. This year, his farm will ship around US$8-million to $10-million worth of citrus around the United States and the Pacific Rim. Cruising through the labyrinthine dirt roads that criss-cross his farm, Johnston surveys the winter crop, passing tree after tree, each a mop of dark green dotted with baseball-size fruits. The trees are productive and the fruit healthy, but in the back of Johnston's mind is the possibility that trees could fall sick from insect-spread infections. Tristeza virus, for instance, fells a tree every few years, he says. Johnston is vigilant. He serves on his county's pest-control board and educates other growers about plant diseases that range from minor nuisances to epidemics that could ruin the San Joaquin valley's billion-dollar citrus industry. \"We have a hard time getting growers to understand the severity of these bacterial and viral infections,\" says the easy-going third-generation grower.  \n                Green alert \n              Mention huanglongbing, and Johnston cringes. \"We were being warned by the scientists and the people in the know that this is a serious, serious thing and we've got to, at all costs, keep it out of the valley and keep it out of California,\" he says. Named for the colour of the infected leaves, huanglongbing is Chinese for yellow dragon disease. It is a bacterial infection that spreads from tree to tree via citrus psyllids, draining nutrients from the plant and resulting in diminutive green fruits, thinning branches and eventually death of the tree. It has devastated the citrus industry in many Asian countries, killed hundreds of thousands of trees in Brazil, and swept through 26 Florida counties since it first arrived on US soil in 2005. It is considered the most dangerous citrus pathogen in the world. The US government agrees. The bacterium that causes the disease,  'Candidatus   Liberibacter', comes under the same regulations that restrict research on Ebola, anthrax and other microbes that have been deemed potential agents of terror. An attack on the food supply with a plant pathogen such as  'Ca . Liberibacter', some say, could have disastrous effects on the nation's economy. Researchers who study these 'select agents' must submit to background checks by the Federal Bureau of Investigation (FBI), install extra security outside their labs, and file mounds of paperwork. These measures are necessary, the government says, to keep the pathogens from falling into the wrong hands. A terrorist-spread epidemic of huanglongbing, for instance, could kill millions of trees and rattle consumer confidence, says Jacqueline Fletcher, director of the National Institute for Microbial Forensics and Food and Agricultural Biosecurity in Stillwater, Oklahoma. Other scientists, though, say that treating  'Ca . Liberibacter' and other emerging plant pathogens as potential agents of terror leaves growers such as Johnston worse off. Slapping the select-agent status on plant pathogens inevitably means that fewer researchers can study the organisms, slowing the development of countermeasures. Compliance with the strict regulations is costly and time-consuming, researchers say. And officials in Florida contend that the bacterium's select-agent status sapped their response to the 2005 outbreak of huanglongbing, which continues to rage. Diagnostic tests have been slow to develop and researchers still haven't figured out how to grow the bacterium in the lab. \"There's no question that select-agent type regulations are necessary for Ebola virus and 1918 influenza,\" says Caitilyn Allen, a microbiologist at the University of Wisconsin\u2013Madison. \"The question that needs to be considered seriously is whether these plant pathogens pose a similar risk.\" Indeed, the US government is now deliberating changes to the select-agent list to reflect the growing prominence of huanglongbing in Florida.  \n                The war on agroterror \n              In 1996, the US government created its first list of select agents, after Larry Wayne Harris, a microbiologist and white supremacist, tried to purchase vials of the bacterium that causes bubonic plague. The initial list included pathogens in humans and animals, but not plants. But biosecurity soon became an important issue for plant pathologists too. In 1999, the American Phytopathological Society in St Paul, Minnesota, hosted a well-attended symposium on the potential for plant pathogens to be used as weapons. That same year, the organization issued a statement recommending better surveillance of plant-disease outbreaks, eyeing the possibility of agroterror. Those flirtations with biosecurity turned serious after letters laced with anthrax started appearing in the US mail system shortly after the terrosist attacks of 11 September 2001. Fletcher was then president-elect of the American Phytopathological Society. In February 2002, she and several other scientists briefed Congress on the potential for plant pathogens to be used by terrorists. \"While there is no evidence that agriculture might be a current target of terrorism, September 11 has made us all more aware of the need to be prepared for any possibility,\" Fletcher said at the time. \n               boxed-text \n             Congress and President Bush listened. On 12 June that year, Bush signed the Agricultural Bioterrorism Protection Act, ordering the US Department of Agriculture (USDA) to include plant pathogens on the select-agent list. Two months later, the agency began enforcing the law, listing nine plant pathogens, including two species of  'Ca . Liberibacter' (see ' The select few '). To ensure flexibility, the USDA included only exotic pathogens \u2014 those usually found outside the United States \u2014 as select agents and mandated that they be removed from the list once they took hold in the country. The law would be updated every two years to accommodate such changes, although not until extensive consultation has been done. Some researchers view this elevation of status as a boon. \"Having the select-agent list helps us to prioritize,\" says Fletcher. \"It also targets special funds for working on those things if they should come in.\" Indeed, the USDA's biodefence budget bloomed from $200 million in 2003, the first year that numbers were made available, to $340 million in the 2008 budget, according to the Center for Biosecurity at the University of Pittsburgh Medical Center in Baltimore, Maryland. Heightened awareness of plant pathogens also spurred the USDA to establish a national network of laboratories to diagnose plant diseases. Before 9/11, every state ran its own plant-pathology lab, each of which had different procedures for testing diseased plants. But funding was scarce and \"all these labs were languishing\", Fletcher says. Although agroterrorism was the motivation behind the long-overdue network, improved diagnostics and communication between state labs will speed the response to any outbreak, she says. On 23 August 2005, plant pathologists had their worst fears realized: huanglongbing was found in the pomelo trees of two homeowners near Miami. Ten days later, the USDA confirmed the diagnosis. In the months that followed, officials found the disease in thousands of trees in most of Florida's citrus-growing counties. Where it came from no one knows, but historical records suggest that the disease originated in the Guangdong province in southern China or in central India in the late 1800s. It quickly sped through Asia, where one species,  'Ca . L. asiaticus', is now endemic from Japan to Pakistan. A second species,  'Ca . L. africanus' is found throughout eastern, central and southern Africa, and a third species,  'Ca . L. americanus' has been discovered in Brazil.  \n                Yellow dragon \n              Citrus trees infected with huanglongbing show few symptoms for several years. Eventually, their leaves develop characteristic yellow splotches, and trees produce puny, discoloured fruit. Production drops rapidly, until the trees eventually die. Worse, infected trees churn out bacteria for the psyllid to cart elsewhere. Given the havoc the disease has wreaked around the world, there is good reason to worry that huanglongbing will spread throughout the United States, says Phil Berger, acting director of the USDA's Center for Plant Health Science and Technology in Raleigh, North Carolina. A survey in the late 1990s estimated that 53 million trees in Asia were infected with huanglongbing, and 10 million in Africa. A systematic survey of the Reunion Islands in the 1980s found that huanglongbing had killed 65% of their citrus trees within seven years of planting. And in parts of northern Thailand, the bacterium kills at least one-tenth of tangerine trees every year. In the state of S\u00e3o Paulo in Brazil, an estimated 800,000 trees have been lost since an outbreak hit in 2004. As the citrus psyllid moves west from Florida \u2014 it has turned up in Texas and Mexico \u2014 officials and growers are preparing for the worst. \"The threat is enormous,\" says Larry Bezark, of the California Department of Food and Agriculture in Sacramento. Bezark spearheads the state's response to huanglongbing \u2014 scouring farms, nurseries and even ports for psyllids and infected plants. \"This is a disease that can wipe out citrus as we know it,\" he says. But while California prepares for an outbreak, researchers and officials in Florida feel handcuffed by the pathogen's continued status as a select agent, even though it is now widespread in Florida. \"We had our hands tied,\" says Wayne Dixon, chief of plant pathology at the Florida Department of Agriculture and Commerce in Gainesville. Whenever a sample from a tree turns up positive, the select-agent law mandates that the lab notify the USDA and destroy the sample within a week. This complicates efforts to perform additional diagnostics, says Dixon, who supports listing some exotic plant pathogens as select agents but not  'Ca . Liberibacter asiaticus'. To comply with the select-agent listing, researchers must register with the federal government, install added security to their labs such as video monitors and fingerprint-accessed doors, and submit to background checks. Foreign researchers have an especially hard time getting approved to work on select agents. The penalties for violating the law are harsh \u2014 up to $250,000 and five years in jail.  \n                Species swap \n              In August 2007, the USDA proposed updating the select-agent list to remove the asiaticus species found in Florida, while adding the Brazilian americanus species to the list. It is now reviewing the comments it received on its proposals, which are overwhelmingly in favour of lifting the select-agent status of the asiaticus species \u2014 and hopes to issue a final rule later this year, says Michael Firko, a USDA official who is leading the agency's efforts. While the proposal to remove the asiaticus strain from the select-agent list seems to be headed for approval, several other plant pathogens may be added, including diseases of soya, rye, woody trees and shrubs, and rice. The proposed addition of a strain of the rice pathogen  Xanthomonas oryzae   has left some microbiologists scratching their heads. The cause of leaf streak in rice,  X. oryzae   has done significant damage to crops in Asia and Africa. Yet the pathogen is unlikely to take hold in the United States because of different farming practices and a hostile climate, says Leach, who studies the bacterium. The added cost of complying with the select agent rule may force some researchers to abandon their work on  X. oryzyae . \"I stay up late at night worrying about this stuff,\" says Pam Ronald, a geneticist at the University of California in Davis who studies the rice pathogen Firko contends that the select-agent status of  'Ca . Liberibacter asiaticus' hasn't hampered research. \"We have not refused registration to anybody for this agent, and I don't think we have refused any experiment that people want to do.\" Classifying  'Ca . L. asiaticus' as a potential agent of bioterror is paradoxical, says Eric Triplett, a microbiologist at the University of Florida in Gainesville. \"It doesn't make sense to have such enormous restrictions on laboratories and confine it to strict BSL-3 conditions when it's already outside our window.\" To study the disease, Triplett and his lab members must travel to a facility 225 kilometres away that has been approved specifically for work on select agents. The microbiologist would like a lab of his own to work on huanglongbing, but the government has yet to approve it. \"Few, if any, plant pathogens should be considered bioterror agents,\" says Tim Gottwald, a plant pathologist at the research wing of the USDA in Fort Pierce, Florida, who studies huanglongbing. Gottwald estimates that he spent $50,000 upgrading his lab to comply with the select-agent rules. More troubling, he says, are delays in getting scientists approved to work on the disease. \"I have 12 to 14 people somewhere in the FBI black hole,\" he says. The pathogen's select-agent status may also scare away promising young scientists because the restrictions slow down the progress of research, says Triplett. \"It's very hard to persuade a post-doc or grad student to work on a project when it's going to hamper [his] career,\" he says. More than 3,000 kilometres away, Johnston has his own worries as he scans his grove at the end of a long day. He stops at a navel tree and plucks a shoulder-high fruit, then slices the orange open with a couple of quick motions of a pocket knife. Its juice squirts out and the tangy perfume fills the air for a second. Huanglongbing would reduce the sweet fruit to a withered, bitter gall. Meanwhile, Johnston says that he hasn't even heard of the select-agent rule. \"I am much more concerned that the US government, through inaction, will allow this stuff to move from Florida to Texas to California.\" When asked about agroterror, he laughs and points out that those wishing to do harm could pick a better weapon than huanglongbing. \"Are they going to try to kill orange trees rather than people? I doubt it.\" Ewen Callaway is a science journalist in Washington DC. See Editorial,  page 127  . \n                     Agricultural Select Agent Information \n                   \n                     American Pytopathological Society biosecurity initiatives \n                   Reprints and Permissions"},
{"file_id": "452022a", "url": "https://www.nature.com/articles/452022a", "year": 2008, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "New York University is trying to establish a world-class archaeological institute \u2014 with funds from a philanthropist who has been linked to looted artefacts. Rex Dalton reports. Unearthed last summer at an ancient crossroads in central Asia, two bronze lamps hark back to the Greek myth of the Golden Fleece. They come from the region once known as Colchis, in present-day Georgia, where Jason and the Argonauts are said to have searched for the precious wool 1 . The lamps also reflect the multitude of cultures that passed through the region \u2014 with an unusual intermingling of elephants, a woman's snake-wrapped torso and gods decorating their sides. The lamps, which date from between the third and first century BC, are now bringing their story to the United States, where they and other stunning Georgian antiquities will form the inaugural show at the newly opened $220-million Institute for the Study of the Ancient World at New York University (NYU). The 130 artefacts include gold jewellery, bronze masks, sculptures and wine goblets, all dating from the last few centuries  BC . But while the public admires the artefacts in the institute's gallery, many in the archaeology community are still sputtering over its financial creator \u2014 Shelby White, a wealthy New York connoisseur whose name has been linked with looted antiquities. Last month, she returned nine major artefacts to Italy, from where they were looted decades ago; a Greek vessel will also be returned in 2010. White has insisted that she had no knowledge of their illicit route to her world-class collection. The worldwide market for antiquities, which collectors such as White underpin, thrives on unprovenanced historical artefacts, sold to collectors as drawing-room 'art' or donated to museums as tax write-offs. Objects that move from the archaeological dig to the auction house are often lost to scientific study. Some observers even argue that, by raising demand for such antiquities, collectors are the real looters 2 .  \n                Objects and objections \n              White is one of the most high-profile private collectors, as she regularly lends her artefacts to museums for display. That visibility has drawn extra scrutiny of her collecting habits. \u201cShe is an unrepentant collector, whose approach to the past I believe is misguided,\u201d says Colin Renfrew, a British archaeologist and former director of the McDonald Institute for Archaeological Research at the University of Cambridge, UK. \u201cI look at anything she funds with suspicion.\u201d By chance, Renfrew will lecture next week at NYU's main campus in Greenwich Village, where some senior members of the university's anthropology department are similarly sceptical. Randall White, an NYU anthropologist who studies Palaeolithic cave art and is no relation to Shelby White, notes that the new institute was created without the involvement of the anthropology department, one of the top dozen in the nation. He resigned in protest from NYU's Center for Ancient Studies after university authorities accepted the $200-million endowment from Shelby White's foundation two years ago. In his resignation letter, he wrote that \u201cNYU's acceptance of such support tells the world we condone the collection of unprovenanced antiquities that may have been clandestinely exported from their country of origin\u201d. Others at the university are also distancing themselves. \u201cThe institute is built more around the interests of Shelby White,\u201d says Fred Myers, chair of the anthropology department. \u201cWe weren't involved in its development.\u201d Privately, a prominent department member voiced concerns that an affiliation with the institute might taint a career: \u201cIf we wanted to work anywhere in the world, we couldn't be associated with the institute.\u201d NYU spokesman John Beckman says that such an assessment \u201coverstates\u201d the campus rift. \u201cThere was wide consultation among the faculty about the establishment of the institute,\u201d he says. \u201cThe response was generally, but not universally, favourable.\u201d Despite \u2014 or perhaps because of \u2014 the doubts in the community, the institute has set about establishing itself as an academic powerhouse in New York's posh Upper East Side. Housed in a six-storey limestone town house built a century ago, the institute has offices for five initial faculty members, space for future analytical facilities, a conference centre and a library headed by a specialist from the American School of Classical Studies at Athens in Greece. A new doctoral programme is planned, hopefully to start in 2009. For its first faculty hire, the institute's search committee, which included Shelby White, recruited Roger Bagnall from Columbia University in New York as director. An authority on ancient texts, Bagnall is widely respected for his scholarship and professionalism. Last year, he created an advisory panel of half-a-dozen international authorities to begin hiring; it recruited Alexander Jones, a historian of mathematics and astronomy who came from the University of Toronto in Canada. Jones, who will start as a fully tenured NYU professor in July, says he was attracted by the centre's cross-cultural nature. \u201cThe institute is very exciting because it isn't centred around an individual idea or culture, like many university departments,\u201d he says. \u201cWe wanted to create a place where different cultures could be studied to appreciate their interdependence,\u201d affirms Shelby White, whose former personal curator, archaeologist Jennifer Chi, is an associate director at the centre. \u201cThat's the underlying uniqueness of the institute.\u201d  \n                Looking back \n              For now, the institute's research mission has yet to be defined, beyond the general outline that it will focus on the time period from about 3000  BC  to  AD  1000. \u201cArchaeology and history will be our core,\u201d says Bagnall. As the institute takes shape, White's shadow is ever-present. She is well-known in collection circles for her work with her financier husband, Leon Levy, who died in 2003. They regularly lent parts of their private collection of antiquities to major museums, including the Metropolitan Museum of Art (the Met), just blocks away from the new NYU institute. Yet they have also received some of the most public criticism questioning the provenance of their artefacts. White herself has drawn fire for more than a decade. In 2000, President Bill Clinton nominated her to sit on a panel meant to fight the trade in illegal artefacts, a decision that triggeredan outcry from many archaeologists. The then-president of the Archaeological Institute of America, Nancy Wilkie, wrote that White's \u201ccollecting practices certainly would not meet with the public's approval\u201d. White's nomination eventually died when President George W. Bush took office and dropped all of Clinton's nominees for such appointments. In recent decades the archaeological community has fought back against looting, helping to craft new legislation to block trade in antiquities, launching international investigations, and even helping to close down some artefact auctions in London. Oscar Muscarella, an archaeologist at the Met who speaks out regularly on artefact provenance, says White and her husband seemed to live in the past, when collecting was seen more of an art initiative and less of a force that drives illicit looting. Muscarella recalls his last talk with Levy. \u201cI liked him as a person,\u201d he says. \u201cHe wanted me to be his curator. But I read him the riot act on all the plundered stuff. He smiled, we shook hands and never spoke again.\u201d Today, Shelby White remains active at the Met, where she is a benefactor and sits on the search committee to replace recently resigned director Philippe de Montebello. Renfrew, for one, says it is \u201camazing\u201d that any museum would continue contact with anyone linked to questionable artefacts. \u201cInstead of asking themselves how their policies could be so inadequate,\u201d he says, \u201cthe Met and some other museums are in denial.\u201d Coincidentally, similar issues have arisen recently in California, where last month 200 federal agents raided four major museums, including the Los Angeles County Museum of Art, as part of a criminal investigation into smuggling and alleged tax-donation violations involving artefacts from southeast Asia and Native American lands. A financier's private museum outside Chicago was also raided. During the five-year probe, undercover agents tape-recorded a network of buyers, gallery owners and museum curators engaged in alleged scams \u2014 including purchasing smuggled artefacts and donating others to museums for inflated tax write-offs. Federal search warrants indicate that some museum officials knew the objects were looted and values falsely inflated. At the NYU institute's inaugural exhibition, the Georgian artefacts have a much clearer history. They have mostly come on loan as part of an arrangement worked out by David Lordkipanidze, director of the Georgian National Museum in Tbilisi. Lordkipanidze, renowned for his discovery 4  of early hominin fossils dating back 1.7 million years in Dmanisi, Georgia, says that he sees the exchanges as a way to advance studies in his homeland. Georgian conservators will receive training at the new institute. And in the catalogue for the upcoming show, he pays special tribute to White, writing that her \u201cunstinting care for the archaeology of antiquity provides an amazing example of leadership and demonstrates what philanthropy and public\u2013private partnership can do\u201d. Most of the Georgian artefacts were on display at the Arthur M. Sackler Gallery of the Smithsonian Institution in Washington DC until last month; they have been supplemented for the NYU opening with some new finds, including candelabra, incense burners and the bronze lamps adorned with elephants. In the future, academic archaeologists will be watching closely to see how the institute evolves \u2014 particularly in terms of what antiquities can be exhibited or accessioned. If artefacts with suspect credentials are showcased, the outcry is likely to be swift and savage.\n \n                 Rex Dalton is a US West Coast correspondent for  \n                 Nature. \n               \n                     Institute for the Study of the Ancient World \n                   \n                     The Shelby White'Leon Levy Program for Archaeological Publications at Harvard \n                   \n                     Looting Matters \n                   \n                     Saving Antiquities \n                   \n                     Lawyers Committee for Cultural Heritage Preservation \n                   \n                     New York Observer piece \n                   Reprints and Permissions"},
{"file_id": "452018a", "url": "https://www.nature.com/articles/452018a", "year": 2008, "authors": [{"name": "Trudy E. Bell"}], "parsed_as_year": "2006_or_before", "body": "The cosmos is thought to be awash with gravitational waves to which humanity is, as yet, deaf. Trudy E. Bell reports on LISA, an experiment on an unprecedented scale designed to put that right. Imagine a new constellation \u2014 a narrow triangle about as deep as the scoop of the Big Dipper. But this constellation, unlike the familiar natural ones, moves through the sky, always appearing in the evening sky after sunset. The new constellation slowly rotates, each component circling around the centre once every year. And as it does so, it also expands and contracts. Unaided earthly eyes will never actually see the Laser Interferometer Space Antenna (LISA), as this artificial constellation is to be named. Its three component spacecraft will be too small, and the light with which they shine will be invisible infrared. Unseen as it may be, though, it will still be humanity's largest ever creation \u2014 5 million kilometres on a side. And the aspirations it embodies are similarly grandiose. In 2018, or thereabouts, it will try to detect extraordinary cosmic events, such as the births of galaxies and the deaths of black holes, in a fundamentally new way. LISA is perhaps the most ambitious space mission envisaged for the coming decades. A recent assessment of astrophysics research proposed for NASA's Beyond Einstein programme by America's National Research Council (NRC) 1  gave the \u201cextraordinarily original and technically bold\u201d project its highest scientific ranking. Under joint development by NASA and the European Space Agency (ESA), LISA's goal is to detect gravitational waves \u2014 fluctuations in the fabric of space and time \u2014 by measuring the relative motions of three spacecraft with great precision. Although that assessment carried out by the NRC saw LISA as the \u201cleast scientifically risky\u201d of the proposals for future flagship missions, there is no getting away from the fact that no one has ever tried anything remotely similar before. LISA is designed to measure the gravitational waves predicted by Einstein's general theory of relativity. Relativity defines gravity in terms of the geometry of space and time; gravitational waves are transient fluctuations that stretch and compress that geometry. Although they have never yet been observed directly, indirect evidence for them was enough to earn a Nobel Prize. Over three decades Russell Hulse and Joseph Taylor, now of Princeton University, New Jersey, observed a system in which two neutron stars whirl round each other once every 8 hours. Over the years, their spinning speeded up \u2014 evidence that the system was losing energy. The rate at which energy was lost matched exactly the power with which relativity predicted that the pair of stars would be emitting gravitational waves.  \n                Geodesics and gigantism \n              Even when gravitational waves are generated by massive objects and ungodly accelerations, such as those involved in that neutron-star doublet, detecting them requires almost unthinkable attention to detail. In relativity, the shape of spacetime is defined as the path taken by a body falling free under no influence but that of gravity \u2014 a 'geodesic'. Gravitational waves affect these geodesics, and can thus be detected; but only through careful comparisons. \u201cYou cannot demonstrate that one particle is freely falling and following a geodesic,\u201d explains Stefano Vitale, a physics professor at the University of Trento in Italy. \u201cYou must have two particles, and compare the shapes of their paths by exchanging a ray of light between them.\u201d The small amplitude of gravitational waves means that detecting them by comparing geodesics in this way requires immense precision. The long wavelengths of the gravitational waves that most interest astrophysicists means that the geodesics being compared must be separated from each other by astronomical distances. Using freely falling objects to spot gravitational waves in this way, as LISA is intended to do, is a three-step process. First you set up a situation in which masses can fall freely along their geodesics without being disturbed by magnetic fields or other spurious forces. Then you must measure with extraordinary precision how the distance between their geodesics changes when passing gravitational waves distort the local curvature of space. The last step is analysing these changes to determine the exact shape, frequency and intensity of the distortions to the curvature of space, so as to learn about the nature of distant events producing them. To provide Vitale's geodesic-joining rays of light, LISA uses neodymium-YAG lasers, which shine at a wavelength of a little more than a micrometre, a wavelength they stick to with extreme precision. These will illuminate cubes four kilograms in mass but just four centimetres on a side \u2014 polished 'test masses' of gold-coated gold\u2013platinum alloy as beautiful as fine jewellery and much more costly. The test masses respond to gravity and not much else. \u201cThe gold\u2013platinum alloy has a magnetic susceptibility almost as low as glass,\u201d explains Paul McNamara, an ESA project scientist in Noordwijk, the Netherlands. Once in space, the test masses' sole purpose is to follow their own paths, each falling freely along its geodesic within one of the LISA spacecraft while reflecting the laser light with which the other spacecraft illuminate it. LISA will be arranged so that these geodesics are 5 million kilometres apart, with the spacecraft falling around the Sun 20\u00b0 behind the Earth in the same orbit. That will put them about 50 million kilometres from their planet of origin. Once during every annual orbit the triangle will 'breathe', its sides taking turns to grow and shrink by about 50,000 kilometres.  \n                Hearing double \n              LISA's spacecraft do not need to measure the exact distance between the test masses each contains any more than a police officer with a radar gun needs to know the exact distance to a car before he can tell whether it is speeding. The system's 'breathing' causes a predictable Doppler shift \u2014 similar to the lowered tone of a receding train's whistle, or the redshift of a distant galaxy's light \u2014 in the light reflected from the test mass in one spacecraft to the instruments in another when the distance between them grows, and an opposite shift on contraction. A passing gravitational wave will impose a variation on this predictable Doppler signal, because it will curve space between spacecraft and lengthen the route that the light connecting them has to take. Compared with the changes in the Doppler shift caused by the breathing, however, frequency changes due to gravitational waves happen much faster \u2014 over periods of between 10 seconds and 3 hours. It is here that the unprecedented precision of LISA's metrology will come into play. LISA's detectors will treat the infrared laser beam in the same way that an FM radio in a car treats a radio signal. An FM radio station transmits a steady high-frequency signal (typically around 100 megahertz, or 10 8  cycles per second) modulated by lower-frequency audio programming (in the kilohertz range, or about 10 3  cycles per second). A home or car radio receiver contains a local oscillator that generates a signal at roughly the same higher frequency as the transmitter. Subtracting the signal received over the air from the local oscillator signal, a process called heterodyning, unmasks the much lower-frequency signal of the audio programming. That audio-frequency signal then drives the radio's speakers. Optical heterodyning within LISA, using the million-times higher frequencies of infrared laser light (about 300 terahertz \u2014 3 \u00d7 10 14  cycles per second), will do much the same but with unprecedented precision. The incoming laser beam (as reflected off a distant test mass) will be combined with some light from the outgoing laser beam (treated as the reference signal) on a photodetector. The comparison will reveal the expected lower-frequency (1 megahertz) 'hum' of LISA's regular breathing. Gravitational waves would show up as a modulation of that hum at really low frequencies (a millihertz, or 0.001 cycles per second). Using signal processing akin to that in satellite-navigation receivers, LISA will measure that low-frequency signal to a precision of a few millionths of a cycle, corresponding to a few picometres in distance.  \n                Finding the path \n              Unsurprisingly, some wonder whether this wizardry can be pulled off in practice. Specifically, can external disturbances \u2014 for example, the solar wind and the radiation pressure of the Sun's light \u2014 be understood and avoided well enough for LISA to achieve the sensitivity it needs? \u201cThe devil is truly in the details,\u201d cautions Robin Stebbins, LISA project scientist at NASA's Goddard Space Flight Center in Greenbelt, Maryland. \u201cWhen doing precision measurements, an instrumentalist must estimate all possible errors, verify in the lab that those estimates are correct, and constantly search for errors that may have been overlooked. We have a list of about 50 physical phenomena that could disturb LISA's test masses, of which perhaps 35 are significant.\u201d There is, however, nothing as convincing as actually trying the technology out. That's where LISA Pathfinder comes in. LISA Pathfinder, currently being assembled by contractor EADS Astrium in Stevenage, UK, is a spacecraft designed to demonstrate LISA's underlying technologies: reflecting infrared laser beams off freely falling masses following their own geodesics, picometre metrology, and reducing unwanted disturbances to the test masses. \u201cFlight hardware is being delivered almost every day for launch in 2010,\u201d says McNamara, who is LISA Pathfinder's project scientist.  \n                Lissajous orbit \n              LISA Pathfinder will be launched into an elaborate Lissajous (pronounced, by coincidence, \u201cLisa-ju\u201d) orbit around the 'L-1' point 1.5 million kilometres sunwards of Earth, where the pull of the Earth and Sun mostly cancel each other out. \u201cWe want to get out of the gravitational pull of the Moon or away from the temperature drop of being eclipsed by Earth's shadow,\u201d McNamara says. \u201cL-1 has constant solar power and constant temperature, and is relatively cheap to get to.\u201d Once the spacecraft is in this orbit, two test masses just like those to be used in LISA will be gently released from the cages that have held them in place. This is no easy task: \u201cA caging mechanism that would release the cubes with zero velocity was one of the toughest engineering problems \u2014 we've been working on it since '94,\u201d says McNamara. Once released the test masses, just like the ones in LISA, will fall freely along their own geodesics; only they'll do so separated by a few tens of centimetres, rather than a few million kilometres. The rest of the LISA Pathfinder spacecraft will serve to shield the masses within from several dozen forces that would otherwise disturb any measurements. Like a parent holding an umbrella over a child in a rainstorm, the spacecraft will follow the test masses without touching them, continuously correcting its position by firing micronewton thrusters so gentle that their force is \u201cthe same as your breath from 200 metres away\u201d, McNamara says. If Pathfinder can protect its test masses, leaving them to fall as freely as if they were the only things in the Universe, then LISA's three spacecraft should be able to do the same. LISA Pathfinder \u2014 a single spacecraft with a 90-day mission quite close to Earth \u2014 has a budget of \u20ac300 million ($440 million). LISA itself \u2014 three spacecraft with a 5- to 10-year lifetime, optical heterodyning equipment, deep-space communications and additional intricacies \u2014 is put at around US$2 billion. The cost is one reason why LISA is being pursued jointly by NASA and ESA, and why it has taken so long to get into space. \u201cTo get any mission into space, one must first convince the space community of its scientific value, because space is a zero-sum game,\u201d explains Karsten Danzmann, director of the Albert Einstein Institute in Hannover, Germany, and co-chair of the LISA science team. \u201cIn the early 1980s [when ideas for a mission such as LISA were first mooted] the astronomers and planetary scientists had neatly divided the cake. No one wanted to share the cake with the gravitational-wave physicists because it would mean getting a narrower slice. Also, no one at the time could see how to measure such small changes over such vast distances.\u201d LISA made its first formal appearance in 1992, when ESA issued a request for proposals for medium-sized scientific missions and Danzmann and his colleagues \u2014 about half of them in the United States \u2014 proposed a version of the idea with six small spacecraft. The European agency had taken the lead because NASA was more focused on repercussions from the space shuttle Challenger accident and the faulty mirror aboard the Hubble Space Telescope. But by 1997, NASA and ESA were anticipating a joint mission through coordinated studies of the concept that are still under development today.  \n                Screaming stars \n              So if LISA finally starts to resonate with the Universe's gravitational radiation at the end of the next decade, as is currently planned, what new science will be revealed? Making a presentation to the NRC panel last year, Scott Hughes, a physicist at the Massachusetts Institute of Technology in Cambridge, provided a striking answer. Without warning, he played an audio recording of a tiger attacking and devouring a live monkey. As the monkey's screams echoed throughout the auditorium, \u201cevery primate in the room came to attention\u201d, recalls Stebbins. Hughes was demonstrating just how much one can learn from sound as a way of emphasizing the extent to which LISA's unique output can be thought of in terms of noise, rather than vision. The analogy between LISA and human hearing \u201cworks surprisingly well\u201d, he says. A pair of star-sized black holes spiralling into each other would be expected to radiate gravitational waves of frequencies in human audio range. If human ears were sensitive to gravitational waves, the final merging of the black holes would sound like a rising warbling that ends in a mouse squeak, if not quite a monkey scream. That final squeak is what gravitational-wave detectors on Earth are trying to hear (see  'Meanwhile back on Earth' ). At the much longer wavelengths to which LISA is sensitive, merging supermassive black holes within star clusters would emit a rising roar \u201cmore energetic than all the stars in all the galaxies\u201d, Stebbins adds. And the source of the waves should be apparent through LISA rather as the direction of a tiger's kill is obvious to a listener in the jungle. Just as with two human ears, \u201cyou can tell roughly where a sound originates, because the sound wave hits one ear before the other\u201d, says Hughes, so the distance between the LISA spacecraft allows any waves that it detects to be traced back towards their source. The three LISA spacecraft, 13 times as far from each other as the Moon is from Earth, will be 17 seconds from each other at the speed of light \u2014 which is also the speed of gravitational waves. Time lags in the signals measured across the different edges of the triangle should allow LISA to discern the approximate direction of a source of gravitational waves, in part so radio and optical telescopes could take a look. It is a triangle that triangulates.  \n                The infancy of the Universe \n             LISA is expected to hear back to when the Universe was younger than a billion years old. \u201cHalf a billion years after the Big Bang was the cosmic dark ages, when the Universe was filled with neutral hydrogen gas that absorbed photons,\u201d Hughes explains. \u201cWe hope to use gravitational waves to peer through this dark fog to listen to the birth of the first galaxies.\u201d (In speech, LISA scientists go back and forth between auditory and visual metaphor.) In addition, there will be a constant background \u201chissing and crackling\u201d of higher-frequency gravitational waves from \u201c20 million more ordinary white-dwarf star binaries that fill our galaxy,\u201d Danzmann says. But there will be deeper tones, too \u2014 notably, the booming collisions of entire galaxies. LISA's sensitivity to such events is one reason for being sure there is something for LISA to hear. \u201cThere are supermassive black holes of a million or 10 million solar masses in the centre of virtually every galaxy. And we see from photographs that galaxies are colliding everywhere,\u201d says Rainer Weiss, an emeritus physics professor at the Massachusetts Institute of Technology. \u201cSo at the wavelength range of LISA, we know there are sources.\u201d The project's promise goes beyond astronomy into fundamental physics. \u201cSince the beginning of the Copernican revolution, astronomy has posed challenges to physics. And we've used physics to understand astronomical objects,\u201d said Charles Kennel, a former NASA associate administrator, now director of the Scripps Institution of Oceanography in San Diego, California, who served as co-chair of the NRC report. \u201cLISA will be able to find merging black holes and test general relativity in the strong field limit.\u201d That's a relativist's term for situations where gravity is so powerful \u2014 that is, the geometry of space is so sharply curved \u2014 that phenomena that cannot be observed in any laboratory, or by any observations of nearby astrophysical objects, come into play. Relativity's predictions about this limit could be tested, though, by observing how matter and the fabric of space and time are wrenched apart near supermassive black holes in the centres of galaxies. General relativity predicts that when smaller black holes, with masses of ten times the Sun's, fall into these much vaster monsters, the geometry of space and time is also being wrenched and kinked in ways never seen anywhere more mundane \u2014 and the specifics of the gravitational waves given off should reveal the theoretical limits of general relativity. Only at the edges of masses capable of such acts of cosmic cannibalism \u2014 second in violence only to the Big Bang itself \u2014 are gravitational waves anticipated to reveal the theoretical limits of general relativity. \u201cPeople often say that gravitational waves will open a new window for astronomy,\u201d says Peter Bender, a physicist at the University of Colorado's institute JILA in Boulder. \u201cMore accurately, gravitational waves will throw open whole doors onto an entirely new and vast vista of the Universe.\u201d\n Trudy E. Bell is the author of a dozen books on astronomy, space science, history and cycling. \n                     NASA's LISA page \n                   \n                     ESA's LISA page \n                   \n                     The sociology of gravitational waves \n                   \n                     NRC report on Beyond Einstein \n                   Reprints and Permissions"},
{"file_id": "452142a", "url": "https://www.nature.com/articles/452142a", "year": 2008, "authors": [{"name": "Eric Hand"}], "parsed_as_year": "2006_or_before", "body": "Three teams are racing each other to build the next generation of telescopes that would dramatically dwarf the largest on Earth today. Eric Hand checks out the competition. Buddy Martin gets nervous before setting foot on the glass. He reaches into his pockets and sets aside a mobile phone, coins and fingernail clippers. He checks the soles of his shoes for grit. Then he gingerly steps out onto the 8.4-metre-wide disc, parked underneath the University of Arizona football stadium in the hangar-like cavern of the Steward Observatory mirror laboratory in Tucson. Still unpolished and milky white, the disc sits like a record on a turntable \u2014 except that this record is borosilicate glass instead of vinyl, spins at 1 revolution per minute instead of 45, and weighs as much as 3 buses. Martin, a polishing scientist, is the disc jockey. He needs to shape the surface like a saddle and get it right to within 20 nanometres. Proportionately, it is like bulldozing all of Brazil to a smoothness of less than a centimetre. Thus Martin's anxiety over errant scrapes: \"We have rules such as 'no tools above the glass',\" he says. \"A wrench falling would wreck it.\" When Martin can confirm that the glass is polished correctly, it will be ready to become a mirror. Technicians will seal a giant bell jar over the glass and vaporize a soda can's worth of aluminium over the surface. The result will be as large or larger than all but two of the telescope mirrors currently in use around the world. But that is only the beginning. The Giant Magellan Telescope (GMT), pictured above, which is under development by a consortium led by the Carnegie Observatories in Pasadena, California, and the University of Arizona, will require six more mirrors just as big. If the first mirror is Brazil, there is still the rest of the Americas, all of Europe and a good chunk of Africa to go. \n               boxed-text \n             With its seven mirrors the GMT will be 25 metres across, dwarfing the biggest telescope mirrors in the world today \u2014 those in the twin 10-metre Keck scopes in Mauna Kea, Hawaii. Yet even the GMT will be the smallest of the bigs in a game of one-upmanship that has emerged among astronomers. A second group, led by the University of California and the California Institute of Technology (Caltech) in Pasadena, is planning a giant named the Thirty-Meter Telescope (TMT). And the European Southern Observatory (ESO), headquartered in Garching, Germany, aims to build the biggest 'light bucket' of all: a 42-metre design called the European Extremely Large Telescope (E-ELT). All told, these three competing behemoths could cost more than US$3 billion (see Table). The giant size of these scopes leads exponentially to big science. A telescope's ability to gather light increases with the area of the primary mirror, and its ability to resolve details increases with the diameter. All told, tripling in diameter from the Kecks, this new generation of telescopes will be roughly nine times better at grasping the light from distant stars, and nine times better at distinguishing them from darkness. Telescopes this big could tackle fundamental and currently unanswered problems, such as the direct imaging of light from an extrasolar planet or measurement of the mysterious repulsive force that's accelerating the expansion of the Universe. \"It's not just pushing for bigger toys,\" says Matt Mountain, director of the Space Telescope Science Institute in Baltimore, Maryland. \"They open up a new parameter space that we don't have access to.\" The giant telescopes would rival, even in some ways surpass, the James Webb Space Telescope (JWST), the 6.5-metre replacement to the Hubble Space Telescope that is supposed to launch into orbit in 2013. By getting above Earth's atmosphere, the space telescope will be able to see deeper into the infrared portion of the spectrum. But the ground-based telescopes, with their larger mirrors, would be able to spot smaller and fainter objects. Both the JWST and the ground-based telescopes are aiming to spot 'first light', the time 400 million years after the Big Bang when stars first began their fusion fire. \"We want to observe photons from the first stars in the Universe,\" says TMT observatory scientist David Silva. \"Are we crazy or what?\" The three groups are lining up partner nations and some are competing for private donors. The Canadians have committed to the TMT, and the Japanese are likely partners too. The Australian National University in Canberra, joined eight American institutions in the GMT project's growing list of partners, and now South Korea is working to find the money to become the tenth partner. Private funding is also playing a large role; in December, Intel founder Gordon Moore, a Caltech graduate, gave the TMT project $200 million, making it the team to beat. It's far from clear if all three scopes will be built, but each is pressing ahead, hoping to commence construction within the next two years. Part of the competitiveness stems from the importance of finishing first. The Keck telescopes beat other telescopes in their class to operation by a few years and skimmed the creamiest science off the top. And so the TMT's competitors want to keep it from drinking everyone's milkshake before they have their own straws ready. But the tension also derives from a historical discord between the three rival groups, groups that have generally got what they wanted even as they pursued telescopes of different sizes and shapes. \"The worst job in the world would be to get these three teams in a room and say, 'How do we collaborate?'\" says Mountain.  \n                The acrimonious divorce \n              To understand two-thirds of the competition, one needs to go to Pasadena, home to the small, privileged institutions of Carnegie and Caltech. For astronomers through much of the twentieth century, this Los Angeles suburb was the sociological centre of the Universe. Initially united, Carnegie and Caltech worked together to finish in 1948 the 200-inch (5-metre) Hale Telescope at Palomar Observatory in the southern California mountains. For decades it was the biggest eye on the Universe, one that discovered the incredible redshift of quasars in distant galaxies. But in 1979 the two institutions divorced. The director at the time, Maarten Schmidt, orchestrated the dissolution of the telescope's joint operation, frustrated by managing a place that had two bodies to approve every decision. Carnegie, the scorned spouse, left to establish the two 6.5-metre Magellan telescopes in Chile, while Caltech pursued the twin Kecks with the University of California system (and, later on, NASA). The two institutions are separated by only a few miles, but today they are divided by more than just fault-lines and freeways. Bitterness lingers. \n               boxed-text \n             On one side is the TMT headquarters, a renovated Catholic hospital on Caltech's satellite campus, where staff scientists work in the former dormitories of nuns. These blue-ribbon scientists work as if for a blue-chip business, jetting around the world to nail down partners, vendors and site licences. Recently they gathered in a glass-walled conference room to debate the finer points of five possible sites for the telescope \u2014 even though many acknowledge that a site on Hawaii's Mauna Kea would do the most to seduce the Japanese, who are also flirting with the ESO. While one astronomer lectures on the emerging problem of light pollution at a potential Baja California site, the remaining scientists peck away on laptops, multitasking. Silva, who has also worked for the ESO programme and the US National Optical Astronomy Observatory in Tucson, finds the TMT culture different from that of his former workplaces. \"It's a confidence, it's an arrogance, it's a sense of not being afraid to be leaders,\" he says. \"It's a sense of 'We are the best and we're not going to be humble about that'.\" It's Caltech to a T. Caltech's rivals work just a few miles away, at the shady Santa Barbara Street headquarters for Carnegie. The GMT programme is the smaller operation, with 20 full-time employees to the TMT's 100. For the moment they also have much less money \u2014 although that could change at any moment with partners such as Harvard University, with its $34-billion endowment, and two Texas universities that have recently benefited from Houston billionaire George Mitchell and his new-found interest in cosmology. Carnegie director Wendy Freedman says she's expecting several major donations in the coming months that could collectively rival Moore's to the TMT.  \n                Grand design \n              But the biggest difference between the two projects is in the technological ambition of the GMT design. The TMT mirror is designed on the same basis as the Kecks, just on a grander scale: 492 hexagonal mirrors 1.44 metres across instead of 36 mirrors 1.8 metres across. The seven mirrors of the GMT, however, represent a huge step up \u2014 some say a leap of faith \u2014 from the Carnegie's single-mirrored Magellan telescopes. The organization has never before dealt with the difficulties of focusing light from multiple primary mirrors. To remind his colleagues of the challenge, staff astronomer Steve Shectman got the building crew to paint the outline of the GMT's 8.4-metre mirrors in the car park at Carnegie. The circles dwarf the parked sport utility vehicles. The intent, he says, was to scare Freedman into a more cautious approach. \"But it didn't scare her. It just got her all excited.\" The two groups have been jockeying for years, ever since a Keck follow-on called the California Extremely Large Telescope was reborn as the TMT in 2002. Caltech's leadership had realized just how expensive the TMT would be, and that they would need as many partners as possible, not just Californians. But by that time Carnegie had gone off to start its own designs, says Richard Ellis, a lead TMT astronomer and former Palomar director. \"Great opportunities were lost to get everyone on board.\" Led by Freedman, the GMT board worked quickly to level the playing field, which they felt favoured its competition, particularly given the involvement of the Association of Universities for Research in Astronomy (AURA). AURA operates US public telescopes with taxpayers' money, and was a partner on the TMT but not the GMT project. In late 2005, the GMT board sent a letter to AURA president William Smith, complaining that the association had already \"picked the winner\" and noting that no federal support had gone to early GMT work. The National Science Foundation (NSF) then told AURA to back out of the TMT project and shepherd both designs along. The GMT group has since lobbied the NSF to split whatever support it offers the two projects. Many scientists at the TMT, predictably, given its lead in money and design progress, would prefer a winner-take-all competition. Some astronomers suggest that it would have been difficult for the groups to work together anyway. Too much time, money and prestige, they say, has been invested in two rival technologies: the segmented-mirror approach pioneered by Jerry Nelson at the University of California, Santa Cruz, now being applied by Caltech, and the honeycombed, spun-cast mirrors developed by Roger Angel at the University of Arizona, used by Carnegie. \"There are almost religious issues tied up in these designs,\" says Mountain.  \n                Vanity mirrors \n              Nelson has been pushing fly-eyed telescopes for nearly three decades. The inspiration for using segments as opposed to a complete mirror, he says, wasn't hard to find: the Romans used small tiles to create larger mosaic wholes. The real challenge was in finding a way to shape the segments. Most telescope mirrors are parabolic \u2014 a curved surface that reflects incoming parallel light waves towards a central focus. These mirrors are relatively easy to polish to high precision. But in a segmented telescope, each off-axis mirror occupies just a portion of the parabola and needs a custom shape \u2014 a much more difficult polishing task. In the late 1970s at Lawrence Berkeley National Laboratory in California, Nelson discovered that he could warp the mirror precisely by hanging lead weights in buckets from the edges of a mirror segment. He polished the mirror as if it were a symmetric parabola, then released the weights. The mirror popped back into the precise asymmetric shape he needed. \"Working with the edges was all we needed to do,\" he says. At one level, the GMT is also a segmented mirror, but one with only seven giant segments \u2014 each one of which relies on the method long advocated by Angel. Nearly 30 years ago, using a backyard kiln constructed from bricks and heating coils bought from Kmart, Angel and his former graduate student John Hill began experimenting with Pyrex glass, fusing together glass bowls from Angel's kitchen cupboard. Eventually, Angel worked towards two innovations: honeycomb moulds, to create a stiff, lightweight mirror that was mostly hollow, and spin-casting. Spin-casting involves spinning the glass as it cools so that centripetal forces guide the glass naturally into a parabolic shape, granting the grinders and polishers a head start. And the honeycomb technique gives the glass a thinness and lightness that keeps it from deforming because of its own weight and thermal expansion. But like Nelson, Angel faces the problem of asymmetry. The earlier 8.4-metre mirrors made in Tucson were polished as parabolas; the six off-axis mirrors for the GMT have to be saddles. And that means Martin, the polishing scientist, thinks very hard about what they call \"The Test\". The test is a problem of metrology \u2014 how to measure mirror curvature to a strict standard. Measuring incorrectly means failure, such as when the main mirror for the Hubble Space Telescope was ground ever-so-slightly wrong, resulting in blurry pictures. \"We all stay awake at night trying to figure out how not to make that mistake,\" says Martin. So he and other opticians have constructed an elaborate Rube Goldberg-like system in a 28-metre tower above the GMT mirror, which will bounce light through two more mirrors and a hologram before an interferometer measures the patterns that signify perfection or not. One of the test mirrors alone is 3.75 metres across, nearly as big as the Hale Telescope atop Palomar. All sides acknowledge that the test of the first saddle-shaped mirror, scheduled for July, will be the GMT's crucial juncture. The GMT has received good publicity by being the first group to cast a mirror. But what the GMT team is really trying to do, in casting a mirror early and moving quickly to the test, says TMT scientist Chuck Steidel, is \"retire risk\".  \n                The other side \n              Meanwhile, the Europeans have remained out of the fray, quietly corralling a consensus among the ESO's 13 member states and working towards a design that borrows from both the TMT and the GMT. The E-ELT will be segmented like the TMT, but bigger, with nearly 1,000 mirror segments. These are similar in size to the TMT segments, which will allow the two projects to pit vendors against each other. The European design borrows from the GMT in that the adaptive optics will be integrated into the telescope's mirrors (whereas the TMT corrects for atmospheric effects with extra mirrors farther down the optical path). Proponents of the former approach say that it yields a cleaner image and a wider corrected field of view. The Europeans have had to set their sights a bit lower than originally planned, abandoning the preciously named OWL, or OverWhelmingly Large Telescope, after its 100-metre primary mirror proved preposterous. But by developing designs for the 42-metre E-ELT, a mirror that can be made now rather than in 20 years, the Europeans are playing to their strong suit: the dependable ESO budget. \"On a year by year basis, I don't have to go and ask for money,\" says Jason Spyromilio, director of the E-ELT project office. With its \u20ac900 million (US$1.37 billion) price tag, the European project is by far the most expensive of the three, roughly twice as expensive as the Very Large Telescope \u2014 a four-telescope array in Chile that the ESO finished in 2000 after more than a decade. But the Europeans still think they can build the E-ELT without private money. \"Yes, it's a big step, a larger project, a much more complex project,\" says E-ELT principal investigator Roberto Gilmozzi. \"It breaks every assumption we've made about the art of building telescopes so far. But we're restricting our worries to the technical side. Finding the funds is something we can do.\" Others say that Spyromilio and Gilmozzi are optimists \u2014 that they will either have to ask the ESO's member states for more money or find more partners (the two American projects are united in their fear that Japan or the NSF will partner with the ESO instead of with them). A third option, Spyromilio says, is that the ESO programme takes out a loan against its future funding streams \u2014 mortgaging the ESO's future to pay for the E-ELT now. All eyes are now on the NSF to see what it will do next. All three projects need more money just to begin building their telescopes. Yet the NSF will have none to disburse until it finishes the Atacama Large Millimeter Array (ALMA) radio telescope in Chile, in the next five years. NSF astronomy director Wayne van Citters told astronomers in November that after ALMA's completion, the agency could provide a similar contribution \u2014 about $500 million \u2014 towards the construction of a giant telescope. Also on the minds of the Americans is the question of who will pay for operational costs, which typically amount to 10% of construction costs annually. At that rate it would cost more than $100 million each just to operate the GMT and the TMT each year, and the NSF would be likely to have to abandon its long-term support of telescopes in the 2-,4- and 8-metre classes. The astronomy community seems unlikely to want to make that sacrifice; a 2006 'senior review' from the US astronomical community found that these smaller telescopes provided valuable training and employment for the majority of astronomers. Small telescopes, the report noted, also provide excellent science; the first extrasolar planet around a Sun-like star, for instance, was discovered by a modest 2-metre telescope in France. And the discovery of dark energy \u2014 a huge discovery that surely will be afforded a Nobel prize some day \u2014 was achieved with a hodgepodge of telescopes, most of them 4 metres or smaller. For now, the TMT seems to be holding on to its lead, although members of all projects know how tenuous such an advantage can be. The TMT programme is managed by Gary Sanders, who has shepherded through other massive science projects such as the Laser Interferometer Gravitational-Wave Observatory sites in Livingston, Louisiana and Hanford, Washington state. But he also worked on the Superconducting Super Collider in Waxahachie, Texas, a project that Congress killed in 1993 after construction had already started. \"If the astronomy community spoke with unanimity and rallied around one telescope,\" Sanders says with the voice of experience, \"the case would be clearer.\" But the deep historical competition between American astronomers may end up working to everyone's benefit. The number of Gordon Moores in the world \u2014 billionaires with an eye to the heavens \u2014 is clearly finite. But Sanders remembers a time in the 1980s when astronomers thought that there was only enough money for a single 8-metre-class telescope in the world. In the end, astronomers found enough money to build a dozen. And so if history is any guide, the future of astronomy may end up looking like its past. In 1996, Schmidt, the former Palomar director, gave an interview to a historian, and marvelled at how the Carnegie\u2013Caltech divorce seemed to lead directly to the Keck and the Magellan telescopes, record setters in their time. \"And I cannot believe that if we'd stayed together we would have had four of the largest telescopes in the world together.\" Eric Hand covers physics and astronomy from  Nature 's Washington DC office. \n                     Giant Magellan Telescope \n                   \n                     Thirty Meter Telescope \n                   \n                     European Extremely Large Telescope \n                   Reprints and Permissions"},
{"file_id": "452024a", "url": "https://www.nature.com/articles/452024a", "year": 2008, "authors": [{"name": "Mark Schrope"}], "parsed_as_year": "2006_or_before", "body": "Algal blooms can make life miserable for coastal dwellers and wreak havoc on marine ecosystems. Mark Schrope reports on Florida's efforts to predict these red tides. On 3 January 2005, a fisherman working 25 kilometres off the southwest coast of Florida noticed that the baitfish in his seawater tank were spinning and dying. He had seen this strange behaviour before, and knew that it meant more than just a ruined fishing trip. He called the authorities to warn them, and within days a 'red tide' swamped the coast. In the weeks that followed, potent levels of an algal species called  Karenia brevis  flooded the water with toxins. Soon, tourists and residents alike were dodging beaches covered in dead fish and a salt spray that made eyes water and throats instantly raspy. Visits to hospital emergency departments spiked. Manatees, sea turtles, dolphins and other animals began to die; the scourge lasted for more than a year. Red tides occur around the world, from Europe to New Zealand, and are caused by blooms of similar algal species.  Karenia brevis  is, however, a particularly nasty one; its outbreaks have afflicted the coast of Florida since as far back as the 1500s, and now do so almost every year to varying degrees. The state and federal governments spend millions of dollars each year trying to research and monitor the outbreaks. Yet despite all the money and effort put into developing sophisticated monitoring systems, for now the solitary fisherman offshore has as good a chance of catching the birth of a red tide as a satellite does. \u201cWe have had a name for this [organism] now for decades, and we have evidence of humans coughing ever since [Hernando] de Soto took his trek around the Gulf of Mexico,\u201d says Bob Weisberg, an oceanographer at the University of South Florida (USF) in St Petersburg. \u201cBut we still don't have a good handle on what leads to a bloom, what sustains a bloom, and ultimately what leads to the demise of a bloom.\u201d Weisberg and his colleagues are hoping to change that, with a new US$1.25-million Center for Prediction of Red Tides. The centre is a collaborative effort based at the USF's College of Marine Science, a series of low, non-descript, 1950s-era white buildings overlooking Tampa Bay. Its goal is to improve existing models to explain more accurately and then predict the complex progression of a red-tide bloom. Successful forecasts could, for instance, allow fishermen to scoop up shellfish before a bloom takes hold, warn businesses to brace for a drop in beach tourism or alert managers to which environmentally sensitive areas they should be monitoring most closely. \u201cFrom an environmental-management standpoint, forecasting gives us huge benefits,\u201d says Cindy Heil, who leads work on red tides at the Florida Fish and Wildlife Research Institute in St Petersburg, which provided the funds for the new centre and is a partner in the work. But monitoring and modelling  Karenia  are not simple tasks. The dinoflagellate, which has plate-shaped cells measuring 18\u201345 micrometres across, is found throughout the submerged shelf that extends from west Florida out to the deeper basin of the Gulf of Mexico. Physical models have to account for factors such as its ability to grow at various depths and in a range of salinities, and how ocean currents affect its spread and growth. \u201cWe have got to get the ocean currents right so that we can get the rest of it right, and the rest of it is a lot more complicated,\u201d says Weisberg.  \n                Practical applications \n              For years, Weisberg and his colleagues have maintained a growing collection of open-ocean buoys from the southern tip of Florida up to the Florida panhandle. As well as these buoys \u2014 which may actually go offline soon, as their funding dries up \u2014 the team uses tools such as an ocean-bottom profiler that rises into the water column at programmed intervals, then sinks back to the bottom, measuring temperature, salinity and currents as it goes. Weisberg's group also uses two-metre-long autonomous underwater vehicles (AUVs) that can be programmed to roam large swaths of the ocean gathering data. The data gathered with these tools are then combined with an evolving model developed by the US Navy to create a working physical model focused on red tides, but with numerous other applications. These include identifying favourable conditions for fishermen or for search-and-rescue efforts. In nearby Sarasota, collaborators at the Mote Marine Laboratory have outfitted their own AUVs, and one USF buoy, with instruments dubbed 'breve busters'. The breve busters are designed to detect the presence of  Karenia  and collect data that allow rough calculation of its concentrations. Still, says Mote's Gary Kirkpatrick, \u201cwe are certainly nowhere near what we envision as an operational system that can provide a quick look at where  Karenia  is or isn't on a minute-by-minute basis\u201d. Such capabilities, he thinks, could arrive within a few years. The work to understand the physical aspects of red tides, though, still has gaping holes: the physical models used do not incorporate any biology. They can track where a bloom is likely to spread on the basis of winds, currents, and the like, but they can't account for the algae growing, for instance. The group is now working to feed such biological factors into physical models retroactively, to see how well an output matches what actually happened. Such 'hindcasting' allows the researchers to test theories about how blooms might be controlled. Like all dinoflagellates,  Karenia  has characteristics that blur taxonomic lines, which makes modelling challenging. It is an alga, so it photosynthesizes, but like an animal it can also move under its own momentum, using structures called flagella. Much has been learnt about  Karenia  biology, such as how fast the cells can multiply and swim and what nutrients they need. The current challenge is to understand what fuels the transition to the bloom stage, the explosive growth that follows and the bloom's ultimate demise. Normally,  Karenia  are found in concentrations of about 1,000 cells per litre of water. Once the concentration hits 5,000 cells per litre, shellfish can become so contaminated with the toxins produced by the algae that they become off-limits for collecting and eating. A bad red tide can mean millions of cells per litre. Where the nutrients come from to sustain such a massive bloom, especially the nitrogen that usually limits  Karenia 's growth, is one major unknown. The alga is unusual in that it can use organic and inorganic forms of nitrogen, expanding the number of potential sources. Several possible explanations are being investigated, and more than one of the possible nutrient sources could have significant roles. One idea is that dust carried from Africa by winds contributes iron, stimulating the growth of the bacterium  Trichodesmium,  which in turn converts atmospheric nitrogen into more bioavailable forms that support  Karenia 1 . A more recent hypothesis is that nutrients from the Mississippi River drive cycles of plankton growth and nutrient recycling that, under the right wind and current conditions, eventually carry nutrients to the West Florida Shelf 2 . Still another idea, supported by some initial hindcasting work, is that the early stages of a bloom might be fuelled by bacteria such as  Trichodesmium;  then, as  Karenia  proliferates, the fish it kills start to decompose, producing new nutrients to fuel the bloom 3 . \u201cWe do have testable hypotheses,\u201d says Weisberg, \u201cbut like any other complex problem in nature, every time we think we understand red tide it surprises us.\u201d  \n                Nutrient pollutants \n              Perhaps the most contentious explanation has been the idea that run-off from land is a major contributor to algal blooms. In Florida, one important source could be the nutrient-rich water that at times pours out of the Caloosahatchee River into the Gulf of Mexico during heavy rains and when managers release water from the massive Lake Okeechobee inland reservoir as a flood-control measure. Much of this water would once have been filtered by the Florida Everglades, but for decades now, much of it has been diverted via a canal to the river and on to the sea, laden with nutrient pollution from the cities and farms that portions of the Everglades were drained to create. Fully addressing this problem would be a complex and expensive prospect, affecting powerful lobby groups such as developers and sugar farmers. Still, some work is already under way, including the multibillion-dollar Everglades restoration programme, which could eventually prevent much of the water from being released into the Caloosahatchee, although the work is many years from being completed. Environmentalists and researchers have charged for years that the state has not adequately examined the potential connections between run-off and algal blooms. As the state's red-tide leader, Heil has been a lightning rod for the dissent, sometimes accused of intentionally playing down a human contribution. The fact that red tides have occurred in the region for centuries makes it clear that humans don't have to be involved. But some researchers, such as Larry Brand from the University of Miami, believe strongly that at least some of the time, the nutrients from the Caloosahatchee dramatically exacerbate the problem. Brand and his colleague Angela Compton recently reviewed a database of red-tide research since the 1950s and concluded that  Karenia  concentrations have risen about 15-fold 4 . Heil doesn't dismiss the possibility, but says that limitations in the data set, such as sampling bias, hamper interpretation of the data. She becomes visibly shaken when discussing the review, which analysed a data set compiled by Karen Steidinger, her long-time colleague and  Karenia's  namesake. \u201cIt's hard to see her database misinterpreted,\u201d says Heil. For his part, Brand says he has yet to see anything to convince him that his analysis wasn't sound. In work not yet published, Brand has also worked with a physical modeller to conclude that water from the Caloosahatchee could have driven some recent blooms. In other unpublished work, although the calculations are only rough, Brand also says that data he has collected through several years of sampling suggest that at times the amount of nitrogen making it from the river to the sea would be enough to support a bloom. Lee County, through which the Caloosahatchee flows, has helped fund some of Brand's work, as officials there have been unhappy with the state's level of attention to the topic. The county also supported work by Brand's collaborator Brian Lapointe, from the Harbor Branch Oceanographic Institution in Fort Pierce, to study related ties between Caloosahatchee nutrients and recent massive blooms of red-drift algae that periodically clog beaches in the area 5 .  \n                Back in time \n              Lapointe says that research as far back as the 1950s showed a strong tie between the Caloosahatchee and red-tide blooms 6 , but was later ignored. Indeed, a 1962 report for the Army Corps of Engineers 7  refers to the connection as a given. Heil and others say that the older work was set aside in large part because it was conducted before the view emerged that the red tides initiate 20\u201360 kilometres offshore \u2014 rather than around the river mouths, as was thought at the time. Brand argues that that is essentially a moot point in terms of managing the problem. \u201cThe Caloosahatchee is not causing the blooms, so to speak,\u201d he says, \u201cbut if we dump more nutrients in, then whenever conditions are right for a red tide [the nutrients] make it worse.\u201d Heil allows that coastal inputs may indeed promote blooms. \u201cSome are natural probably, and some are not,\u201d she says. Her main argument, for which she has been criticized, is that there simply aren't enough data to settle the question. At one point, the local branch of the environmental group the Sierra Club considered filing a lawsuit against the state to try to force more research into the purported link between run-off and red tides, but has since decided not to. \u201cWe have moved past those points of real serious contention on those issues \u2014 we have a much more collaborative relationship now,\u201d says Stuart Decew, the club's red-tide campaign leader. \u201cAlthough we could still do a great deal more to fully answer questions the public is asking.\u201d Decew says encouraging recent developments include nearly $5 million in new funding on the nutrient issue from the National Oceanic and Atmospheric Administration, and a comprehensive Florida red-tide review released last August by a new Marine Policy Institute at the Mote Marine Laboratory 8 . Among many conclusions and recommendations, the report acknowledged the extreme difficulty of reliably pinpointing the nutrient sources for blooms, but suggests that available data warrant action to reduce nutrient inputs. Weisberg laments that red-tide research isn't further along, which he believes is at least partly due to state- and federal-level funding and research decisions being driven more by politics than by a comprehensive, science-based plan. \u201cI think we can proceed with a lot more speed and efficiency if the agencies kind of changed their ways,\u201d he says. \u201cI don't know how to say that nicely. The problem is not just here, but everywhere around the United States.\u201d Heil has high hopes for what will be accomplished in the coming years. \u201cWe're at that stage where we're progressing fairly rapidly,\u201d she says, \u201cand [the Center for Prediction of Red Tides] is the first step in trying to transition all these research products to management.\u201d Although unravelling the factors that drive the blooms, and putting technologies in place to monitor them better, pose significant challenges, she and Weisberg are optimistic that the work will lead to the ability to forecast the rise and fall of blooms, long before the fishermen see their baitfish doing death spins. \u201cThat's where we're heading,\u201d says Weisberg. \u201cIt may take a while to get there, but this is the starting point.\u201d\n Mark Schrope is a freelance writer on Florida's east coast who wheezed this winter through a red tide. \n                     Fish and Wildlife Research Institute \n                   \n                     NOAA site on harmful algal blooms \n                   \n                     University of South Florida ocean circulation group \n                   \n                     Mote Marine Laboratory \n                   Reprints and Permissions"},
{"file_id": "451512a", "url": "https://www.nature.com/articles/451512a", "year": 2008, "authors": [{"name": "Dan Jones"}], "parsed_as_year": "2006_or_before", "body": "What can evolution say about why humans kill - and about why we do so less than we used to? Dan Jones reports. \"It is scientifically incorrect to say that we have inherited a tendency to make war from our animal ancestors \u2026 that war or any other violent behaviour is genetically programmed into our human nature \u2026 [and] that humans have a 'violent brain'.\" These are the ringing words of the 'Seville Statement on Violence', fashioned by 20 leading natural and social scientists in 1986 as part of the United Nations International Year of Peace, and later adopted by the United Nations Educational, Scientific and Cultural Organization (UNESCO). It was written to counter the pessimistic view that violence and war are inevitable features of human life. The decades since have not been kind to these cherished beliefs. A growing number of psychologists, neuroscientists and anthropologists have accumulated evidence that understanding many aspects of antisocial behaviour, including violence and murder, requires the study of brains, genes and evolution, as well as the societies those factors have wrought. At the same time, though, historians, archaeologists and criminologists have started to argue that in most places life was more violent \u2014 and more likely to end in murder \u2014 in the past than it is today. The time span of this apparent decline in violence has been too short for appeals to natural selection to be convincing. If humans have evolved to kill, then it seems that they have also evolved to live without killing, given the right circumstances.  \n                Going too far \n              Just two years after the Seville Statement was issued, Martin Daly and Margo Wilson of McMaster University in Ontario, Canada, published  Homicide 1 . The book was to become one of the founding texts of a new \u2014 or at least thoroughly rebranded \u2014 discipline called evolutionary psychology. Drawing on animal behaviour, anthropology and patterns of violence and murder in modern societies, Daly and Wilson provided an evolutionary account of the various forms of homicide, from one man killing another to spousal murder and the rarer killing of step-children. But although they argued \u2014 in direct contradiction of the Seville Statement \u2014 that humans have brains and minds with violent proclivities, they also argued that killing was, by and large, not something that evolution had selected for. Instead, Daly and Wilson argued that murderous actions are usually the by-product of urges towards some other goal. The purpose of the sometimes violent competition that goes with human urges for higher status and greater reproductive success is not to kill, any more than the purpose of its stylized quintessence boxing is. But sometimes people die. Most evolutionary psychologists agree, in general terms, with this 'by-product' view, although there are exceptions. David Buss, of the University of Texas at Austin, and Joshua Duntley, of the Richard Stockton College of New Jersey in Pomona, have developed a controversial 'homicide adaptation theory'. The theory proposes that, over evolutionary history, humans have repeatedly encountered a wide range of situations in which the benefits of killing another person outweighed the costs \u2014 particularly when the assessed costs of murder are low, success is likely and other non-lethal options have been closed off 2 . The killing of an unwanted child or the stealthy murder of a sexual rival might be examples. \"Homicide can be such a beneficial solution to adaptive problems in certain, specific contexts that it would be surprising if selection had not fashioned mechanisms to produce lethal aggression,\" says Duntley. Other evolutionary psychologists are yet to be convinced. \"I wouldn't want to hitch my wagon to the by-product argument,\" says Daly, \"but I don't think anyone, including Duntley and Buss, has figured out a good way to identify the hallmarks of homicidal adaptations.\" A key condition for an evolutionary account of homicide is an explanation of the fact that most deadly violence is committed by men. Evolutionary psychologists say that this is because men have evolved to compete more intensively than women in the race for status, material wealth and sexual partners. In terms of the by-product theory, men are more likely to suffer the consequences when competition gets out of hand. This competitive kindling, Daly and Wilson argue, is at its most combustible in men of low socioeconomic status in regions of high social inequality, suffused with a sense of everything to gain and little to lose. Although women also compete, they may be less likely to do so in ways that risk escalating to the use of deadly force because, for women, the costs of such escalation have historically been higher. Rebecca Sear at the London School of Economics and Political Science and Ruth Mace of University College London recently studied the effects of losing kin on child survival in 28 populations from around the world over the past three centuries 3 . The death of a mother has an impact on child survival \u2014 but often the death of a father does not. From a gene's eye view, a woman who might die is thus a bigger problem than a man facing the same level of risk. A meta-analysis 4  of studies looking at sex differences in aggression by John Archer of the University of Central Lancaster, UK, reveals that men and women don't differ much in their experience of anger, the primary accelerator of aggression. Anne Campbell, an evolutionary psychologist at Durham University, UK, suggests that the differences in aggressive behaviour thus reflect differences in the strength of the factors controlling the behavioural expression of that anger. \"Developmental studies show that girls generally score higher on empathy measures, are more fearful and are better at controlling their behaviour,\" says Campbell. In crude terms, women may in general have better brakes with which to stop a violent impulse and people who are violent may, in general, lack such brakes. Psychologist and neuroscientist Richard Davidson, of the University of Wisconsin-Madison, suggests that dysfunction in the brain circuits that normally inhibit emotional impulses \u2014 those associated with the prefrontal cortex \u2014 is a crucial prelude to violent outbursts 5 . In 1997, Adrian Raine and Lori LaCasse, then at the University of Southern California (USC) in Los Angeles, and their colleague Monte Buchsbaum from Mount Sinai School of Medicine in New York published one of the first explanations of the neurobiology of homicide. Among the brains of 41 murderers pleading not guilty by reason of insanity, they found lower activity (as measured by glucose metabolism) in the prefrontal cortex, and greater activity in structures in the limbic system, thought to drive aggression, than they found in non-murderous brains 6 . \"Put crudely, murderers don't have the prefrontal resources to regulate that unbridled emotional output,\" says Raine. Raine has since found a link between a lower volume of grey matter in an area of the prefrontal cortex known as the orbitofrontal cortex, which has been associated with decision-making and regulation of emotion, and more aggressive and antisocial behaviour. He says that the difference in the average volume of the orbitofrontal cortex between men and women accounts for about half of the variation in antisocial behaviour between the sexes. Just as evolution has shaped men's bodies to be, on average, larger than women's, it has also distributed the resources needed to regulate emotion and aggression unevenly between the sexes. In an intriguing turn, Raine and his USC colleague Yaling Yang have recently pointed to a link between homicidal behaviour and the capacity to follow moral guidelines. Over the past six years, brain-imaging studies aimed at understanding moral judgements have illustrated the crucial role of the emotional feeling that comes with violating moral codes. Parts of the prefrontal cortex and amygdala that are abnormal in violent individuals and murderers are activated when making moral judgements. Raine and Yang have proposed that these systems serve as the engine that translates moral feelings into behavioural inhibition \u2014 an engine that has blown a gasket in the antisocial, violent and murderous 7 .  \n                A lethal legacy \n              Men are not just more likely to kill other people than women are, they are also more likely to do so in groups \u2014 and for some researchers it is in these realms that killing offers real evolutionary value. The murder of one person by another may be almost accidental, an unlooked for by-product of aggression. The murder of members of one group by those of another could be an adaptive behaviour that evolution has encouraged. Humans are not the only primates to form coalitions that kill members of neighbouring communities. Since the behaviour was first reported at the Jane Goodall research centre in Gombe, Tanzania, in the 1970s, five long-term study sites dotted around Africa have seen murderous 'gang violence' in chimpanzees. In one case that is hard not to see as a war, the adult males of one community systematically attacked and killed the males of another group over a period of years, with the victorious group eventually absorbing the remaining victims. Harvard anthropologist Richard Wrangham has been observing primates in the wild for more than 30 years. He thinks that the roots of chimpanzee warfare lie in the social organization and behavioural ecology of their societies. Although chimps live in communities of around 150, they are rarely all found together. Instead they typically travel around their territory in parties of up to 20 animals. From time to time, a roaming party from one group will cross paths with a roaming party of another. If they are of equal size, there will be a lot of screaming and charging. When there is an imbalance of power, the larger party will often try to isolate and attack an enemy chimpanzee, sometimes holding their victim down while the frantically excited attackers hail down lethal bows. Although these attacks can be risky \u2014 small parties have been seen running to attack a lone neighbour only to find themselves surrounded by a much larger party, at which point they hurriedly try to flee \u2014 they can also have big pay-offs, especially over the long-term. By dominating or eliminating neighbouring communities, aggressors can expand their range, which means a better food supply, healthier adults and faster reproduction 8 . Raids on neighbouring communities are also common in anthropologists' accounts of small-scale human societies. These often follow the chimpanzee template: a small band of men leaves its home ground, sneaks up on the neighbours and tries to kill one or more of them. Wrangham, working with Michael Wilson of the University of Minnesota in St Paul and Martin Muller of the University of New Mexico in Albuquerque, has moved beyond remarking on the general similarity to looking at some real numbers. They compared death rates from conflict between groups of chimps in the five long-term study sites 9  with data for inter-group human conflicts in numerous subsistence-farmer and hunter\u2013gatherer societies assembled by anthropologist Lawrence Keeley of the University of Illinois at Chicago. Overall, humans and chimpanzees showed comparable levels of violent death from aggression between groups.  \n                A history of violence \n              Moving from studies of chimpanzee coalitional violence and comparisons with small-scale tribal conflicts to understanding modern warfare is, however, far from straightforward. 'War' is a broad term, points out Robert Hinde, a zoologist at the University of Cambridge, UK, and one of the signatories to the Seville Statement. Although Hinde mostly agrees with Wrangham's take on the parallels between inter-community conflicts in chimpanzees and humans, he has reservations about extrapolating too much from these studies. \"In major international wars people do what they do mainly because it is their duty in the role they occupy; combatants in institutionalized wars do not fight primarily because they are aggressive,\" says Hinde, who served as a fighter pilot in the Second World War. But some of the normal machinery that inhibits violence \u2014 the moral engine described by Raine and Yang \u2014 might become selectively disengaged in warring armies. Ideology, propaganda and denigration of the out-group can harden the barrier between 'us' and 'them', says Hinde \u2014 a barrier to which the mind's moral faculty is very sensitive. As a result, killing comes to feel permissible. Even, sometimes, right. What about comparisons of aggression and killing within groups? Chimps often turn on their own, particularly infants and young adults. According to Wrangham and his colleagues, in-group killing exceeds death from between-group conflict in at least some chimp communities. Humans in small societies, by contrast, die much less frequently from fights within their group than from group battles. One possible explanation is that they simply fight less. Anthropologist Victoria Burbank of the University of Western Australia in Crawley has recorded[10] rates of non-lethal acts of physical aggression in an aboriginal Australian population; by Wrangham's reckoning, chimps display such behaviour 200 times more frequently, if not more. For an increasing number of behavioural scientists, including Hinde, this prosocial lack of violence looks like a fundamental aspect of human nature \u2014 the human ability to generate in-group amity often goes hand in hand with out-group enmity. Using computer simulations, economists Jung-Kyoo Choi from Kyungpook National University in South Korea and Samuel Bowles from the University of Siena in Italy have produced models in which altruism and war co-evolve, promoting conflict between groups and greater harmony within them[11]. \"It all falls into place when you see the evidence that early humans lived in small, competing groups,\" says Hinde. \"Your group was more successful if you cooperated with its members but not with outsiders.\" None of this means that a tendency to kill is set in stone; if anything, it shows that humans have evolved to be much less of a risk to each other within groups than they would be if they were as bellicose as chimps. And there is evidence that this risk is reducing further in studies of death rates from both inter-group homicide and intra-group warfare, both of which seem to have plummeted over the millennia. Incessant tit-for-tat tribal raids in which a high percentage of people took part led to shocking rates of death at human hands, spears, axes and clubs. Harvard psychologist Steven Pinker was relying on estimates of this violence derived by anthropologists when he suggested that \"if the wars of the twentieth century had killed the same proportion of the population that die in the wars of a typical tribal society, there would have been two billion deaths, not 100 million.\"[12] A decline in inter-personal violence (as opposed to inter-group war) can be seen over the shorter timescale and narrower field of modern European history. Criminologist Manuel Eisner at the University of Cambridge has documented a trend of declining homicide rates estimated from historical records left by coroners, royal courts and other official sources spanning Europe from the twelfth century to the modern day[13]. After rising from an average of 32 homicides per 100,000 people per year in the thirteenth and fourteenth centuries to 41 in the fifteenth, the murder rate has steadily dropped in every subsequent century, to 19, 11, 3.2, 2.6 and finally 1.4 in the twentieth century. England is typical of the trend, going from 23 homicides per 100,000 people per year to 1.2 over the same period. Eisner rules out better policing and improved medical treatment as causes of the decline for the simple reason that it started before professional police forces appeared and techniques for dealing with wounds became more effective. And a few centuries is too short a time for evolution to have shaped human nature much. A part of the answer that is consistent with an evolutionary approach is a long-term reduction in inequalities of life circumstances and prospects \u2014 the inequalities that Daly and Watson see as driving the conflict that leads to killing as a by-product. \"In places such as Sweden where every cabbie drives a Mercedes,\" says Daly, \"people don't bother to kill so often.\" Better provisioning of life's necessities has also powered the decline, agrees Duntley. When contested resources are made more plentiful, he says, conflict over resources decreases and homicide rates drop.  \n                Moral rearmament \n              The picture, though, is hardly simple. Societal specifics play a part as great as or greater than that of any evolutionary generalities. Eisner points out that across Europe, both geographically and through time, countries with the highest homicide rates are typically plagued by familial feuding and blood revenge, such as in the Scottish highlands in the eighteenth century and Sardinia in the nineteenth. The death toll was frequently exacerbated by cultures laying weight on a male strength in arms and a willingness to demonstrate it. Perhaps against the spirit of Daly's argument, violence was particularly prevalent in \u00e9lites, who would often use it with impunity against their social inferiors. \"Violence is a very functional thing, and the \u00e9lites used it to their advantage,\" claims Eisner, pointing out that violence as a phenomenon of lower-class youths \u2014 the sort of violence Daly and Wilson have studied in Chicago crime statistics \u2014 is a recent trend. \"In the early modern period, local \u00e9lites and nobility become integrated into the state and they increasingly find violent and aggressive behaviour to be useless or dysfunctional,\" says Eisner. \"It becomes much better to be economically successful, and so the \u00e9lites abandon their violent behaviour.\" Systems of justice in which the right people are pardoned and the right people punished push up the costs of violence and homicide, and can put an end to the otherwise incessant family feuds. They can also provide clean alternatives to the shedding of civil blood. According to Eisner, European records reveal that 10\u201320% of medieval homicides were related to conflicts over land ownership. \"Administrations that determine who owns what, and access to civil law courts that help you resolve disputed claims, make resorts to violence much less likely \u2014 in a modern society, it's actually counterproductive,\" he says. A drop-off in war could also lead to reductions in other forms of violence. In cultures and societies with a recent history of warfare, children tend to be socialized to tolerate pain and to react aggressively, which prepares them for the possibility of becoming a soldier (arguably something that evolution would favour) or a potentially deadly brawler (probably something it wouldn't). But in much of the world, histories of warfare are becoming more distant. \"If we grow up without these experiences, which is the case for most people in modern democracies that could affect how aggressive we are and our moral views of our options,\" says Wilson. The evidence suggests that humans may indeed have what the Seville Statement called a 'violent brain', in as much as evolution may favour those who go to war. But evolution has also furnished us with a moral sense. The complexities of the relationship between morals and violence may prove a fruitful field for future research, in as much as they can be disentangled from the social and historical factors that clearly hold great sway over the ultimate levels of violence. Evolution is not destiny; but understanding it could help maintain the hard-to-discern progress of peace.\n Dan Jones is a freelance writer in Brighton, UK. \n                     Web focus on the chimp genome \n                   \n                     Web focus on hominid evolution \n                   \n                     Discover chimpanzees \n                   \n                     Human security centre \n                   \n                     The films of Stanley Kubrick \n                   \n                     Dan Jones's blog \n                   Reprints and Permissions"},
{"file_id": "4511047a", "url": "https://www.nature.com/articles/4511047a", "year": 2008, "authors": [{"name": "Michael Hopkin"}], "parsed_as_year": "2006_or_before", "body": "Zambia, with help from partners around the world, is stepping up its battle against malaria. Michael Hopkin reports from the rural front line. It's not long after dark on a starry night, and three SUVs are bumping along a dirt track in a remote corner of southern Zambia. Inside the vehicles is a troop of more than a dozen men, most dressed in green overalls that bear more than a passing resemblance to army fatigues. Their mission is mundane, even boring: they will spend the entire night sitting still \u2014 up to 12 straight hours punctuated only by the odd insect bite. They say that the boredom really begins to kick in at around 02:00. But for all the tedium, the mosquito-hunters' mission is vitally important. The insects they catch will provide information about exactly when and where people receive bites that transmit the malaria parasite and help researchers to compile a genetic catalogue of the local mosquito population. This is the front line of the battle with malaria. Zambia sees nearly four million cases of malaria diagnosed each year, and some 50,000 deaths, mostly among children. Two years ago, its Ministry of Health embarked on an ambitious plan to cut the incidence of malaria by 75%. As a result, the country is now a destination for a significant fraction of the estimated US$3.6 billion pledged to fight malaria worldwide by a host of sources, including the World Bank, the Global Fund to Fight AIDS, Tuberculosis and Malaria and the Bill & Melinda Gates Foundation. Bill Gates has described the Zambian initiative as an \u201cinspiring example of a nationally coordinated effort\u201d. Last October, Gates and his wife called for the malaria parasite to be sent the same way as the smallpox virus, which remains the only pathogen that humans have ever successfully removed from the natural world. Few have dared to suggest that malaria, which still kills a million people every year, could be totally wiped out \u2014 most efforts focus on control rather than eradication. But although the Gates admit that the goal will take \u201cmultiple decades\u201d to reach, they believe it can be done. And Zambia is seen by many as one of the places where the winning formula might be devised. It is a long way from the world of megabucks philanthropy to the grassroots work that will make inroads against the disease (see  page 1051 ). And people do not always agree on how best to fight the battle. Some researchers in Zambia share the Gates's absolutist attitude; others argue that it needs to be tempered with a little more pragmatism and a lot more data. As the mosquito-catchers at their nightly vigil will testify, collecting malaria data is tedious work. The researchers, from the Malaria Institute at Macha (MIAM), some four hours' drive from the Zambian capital Lusaka, spend hours catching mosquitoes because it's the best way to learn about how malaria spreads.  \n                Captive audience \n              The technique is called 'human landing capture', and involves sitting with sleeves and trouser-legs rolled up, waiting for a mosquito to bite. The researchers work in pairs \u2014 as one is bitten, his partner uses a plastic tube to suck up the mosquito and collect it for study. They do their fieldwork in real dwellings, one pair sitting inside and another outside, to sample both indoor and outdoor biters. The team is currently calibrating a set of mosquito-attracting light traps that will eventually do the same job with less manpower. Researchers at MIAM are also preparing a bid to wipe out malaria from a 1,000-square-kilometre swath of countryside around Macha, as a proof of principle. The plan is in its infancy, but will probably be a 'shock-and-awe' campaign consisting of several interventions. \u201cWe'll be using techniques we know work \u2014 we'll throw everything at it,\u201d says Sungano Mharakurwa, MIAM's scientific director. \u201cIf it works in that area, we would want to go for the district and then the national level. On ethical grounds, people won't want to wait.\u201d The methodology they envision is not that different from the nationwide plan, although the government is more pragmatically focusing on a set of malaria-control targets, rather than total eradication. The national strategy, which has emerged as the favoured combination for confronting malaria in Africa, focuses on four key interventions, each of which has a coverage target: getting insecticide-treated bed nets to 80% of the population; treating 80% of acute malaria cases with a new generation of drugs called artemisinin-based combination therapies (ACTs) within 24 hours; the routine provision of an alternative malaria drug combination, sulphadoxine and pyrimethamine, to pregnant women to protect unborn children; and indoor spraying of pesticide in 85% of households in 15 urban districts. The global community has set itself a goal of halving the number of malaria deaths by 2010 using similar interventions.  \n                Slow but sure \n             So far, Zambia's progress has been slower than it had hoped. The initial time-frame for cutting malaria by 75% set a 2008 deadline, but this has been put back to 2011. To date, the Zambian government claims that the number of people dying from malaria has dropped by 10% since its project began in earnest in 2006. Yet a preliminary report 1  for four African countries compiled by the World Health Organization (WHO) offers hope of greater progress to come. Clinical data for Zambia collected between November and December 2007 show that, since 2000, malaria incidence in children under five \u2014 the agegroup at greatest risk \u2014 has declined by 29%; mortality has fallen by 33%. Two other countries covered by the WHO report, Ethiopia and Rwanda, achieved even greater reductions in childhood deaths from malaria, at 51% and 66%, respectively. Unlike Zambia, these two countries have high-altitude terrain that limits mosquito breeding, but they managed to achieve these cuts even though they had not met their coverage targets for any of the interventions. The WHO data, based on hospital admissions, are the first nationwide update on Zambia's malaria situation since a 2006 government survey 2  of some 3,000 of the nation's more than 2 million households. Mark Grabowsky, who helped to compile the WHO figures, says that Zambia and its partner organizations need to spend more resources on monitoring the progress of its programme \u2014 getting up-to-date information on incidence and mortality to decide which interventions should receive the most effort. \u201cThere is no surveillance system in place that allows these decisions to be made.\u201d He says that most big health projects, such as the international drive to stamp out polio, have conventionally put aside at least 10% of their funds for surveillance. John Miller, a monitoring and evaluation specialist with the Malaria Control and Evaluation Partnership in Africa (MACEPA), admits that although there is fairly strong evidence of a downward trend in malaria, the exact figures are hard to compile, not least because some of its symptoms can also be caused by other diseases. \u201cMalaria is a difficult disease to measure well \u2014 it is a disease of poverty, it is a rural disease. Understanding how much is there is difficult,\u201d Miller says. The Gates foundation is planning another survey for this year, and aims to do one every two years to monitor progress, says David Brandling-Bennett, a US-based senior programme officer with the foundation. The 2006 survey revealed some useful information on interventions that has guided MACEPA's activities. For example, the report showed that only 50% of the population owned any sort of bed net, and barely 20% of children slept under an insecticide-treated net. So in 2007, the project handed out 3.4 million bed nets across the country. Of even greater concern is the shortfall in speedy access to ACTs. The survey showed that only 13% of children with acute malaria received the drugs within 24 hours. But Miller says that \u201cnot all interventions can be scaled up at the same pace. Delivery of antimalarial treatments through the public-health sector by its very nature requires many broader health systems to function well.\u201d Zambia is still ahead of many other countries in providing such drugs and infrastructure. \u201cA lot has happened since 2006,\u201d says Miller.  \n                Death traps \n              Back in Macha, MIAM's mosquito-trappers are collecting the sort of valuable information that could help to delay the emergence of resistance to malaria drugs \u2014 something that is essential if malaria control or eradication efforts are to succeed. Earlier attempts at malaria eradication in the 1960s failed because the effectiveness of the best available drugs, such as chloroquine, was severely hampered by the emergence of resistant strains of malaria. Today, chloroquine is almost useless in sub-Saharan Africa. Mharakurwa believes that detailed monitoring of the malaria parasite in the field is key to preserving the effectiveness of newer drugs. \u201cDrugs are gettting more and more expensive, so we want to get it right,\u201d he says. And his field data, although only preliminary, illustrate how the complex biology of the malaria parasite can affect the emergence of resistance. The researchers who study the emergence of resistance in Zambia's main malaria parasite,  Plasmodium falciparum,  have only a limited understanding of how resistance spreads. Based on his research at Macha, Mharakurwa thinks that spraying buildings with long-lasting pesticides to help keep mosquitoes at bay helps reduce drug resistance. Samples of the parasite taken from local infected people showed that almost all were resistant to sulphadoxine\u2013pyrimethamine, which is a drug combination still widely used. But the  P. falciparum  population living in mosquitoes was different: in areas where houses were sprayed, only 10% were resistant to the drug. Furthermore, Mharakurwa thinks that spraying reduces resistance through subtle effects linked to the complex life cycle of  P. falciparum.  His data suggest that although drug-resistant parasites have a competitive edge when living in humans, this advantage may come at a cost of poorer performance inside the mosquito's body. And this population effect could be narrowed still further by spraying, which reduces mosquito numbers and so increases the competition among the parasites. Spraying is a key component of the government's malaria plan, but it has not been rolled out nationwide. Unlike drug distribution, which can piggyback on existing health infrastructure, spraying requires additional government coordination and training of personnel to scale up effectively. As a rural area, Macha is not currently included in the spraying programme, which focuses on the one-third of the population in the most urbanized districts. But Mharakurwa is keen to introduce it there. After years of controversy over the side effects of the pesticides used, the WHO finally endorsed spraying as an effective way to fight malaria in September 2006. But some still see drug use or bed nets as the dominant strategies to be pursued. And the latest WHO data from Ethiopia and Rwanda could support this view. Both countries achieved dramatic drops in malaria deaths by relying on mass distribution of bed nets and ACTs with little if any documented spraying. Nevertheless, Mharakurwa insists that spraying is a proven weapon and that \u201cwe should attack malaria with all the weapons that we have to hand\u201d. \u201cIt depends where you are and what goals are feasible,\u201d he says, \u201cone size does not fit all. However, if we want to eventually eliminate the problem of malaria, which many believe is realistic in southern Africa, then spraying is one tool that we know works.\u201d If his bid to eradicate malaria in the 1,000-square kilometre patch works, it will earn Macha a reputation as a test bed for dealing with malaria in remote rural areas, where the poorest, and hardest hit by malaria, live. As with the national plans, MIAM will need to collect careful data on malaria prevention, treatment, cases and deaths to bolster their case. \u201cIt's not easy,\u201d says James Phiri, Macha's environmental-health technician, a vocal advocate of spraying and the man in charge of handing out bed nets to the local community. \u201cRight now I estimate that we have 4,000 nets, against a population of 20,000.\u201d Popular stories about bed nets being used for fishing or even as wedding dresses suggest that some people at a local level are yet to be fully convinced of the benefits of malaria programmes.  \n                Double jeopardy \n              Next door to MIAM is the local hospital, which despite being two hours' drive from the nearest paved road not only deals with malaria but also offers a twice-weekly HIV clinic \u2014 a reminder that no health problem, no matter how pressing, can ever be viewed in isolation in Africa. The facility also illustrates the often-fragmented nature of healthcare in the developing world. The hospital pharmacy has a healthy stock of ACTs, including Coartem (artemether and lumefantrine), the Novartis-owned drug currently viewed as the best available treatment. But logistics are a constant headache \u2014 the only transport link is a bumpy, waterlogged dirt road to Choma, the district's main town. \u201cLast year, we ran out of Coartem and saw a real, real increase [in illness],\u201d says the hospital's clinical officer Onaty Hanyuma. Another problem is that, in the absence of nationally coordinated health records (Macha's hospital still uses cards to record patients' details), it can be difficult to keep track of case numbers at all. Hanyuma is reserving judgement on figures for 2008 until after the peak malaria months of April and May, by which time several months of rains will have bolstered the mosquito population. During those times, Hanyuma says, he typically sees around 30 new cases of acute malaria a day. He thinks that cases \u201chave gone down a bit\u201d since the government programme got under way, but he cannot provide more details than that. The experts at MACEPA are fervently hoping that the lessons learned in Zambia can be transferred to other parts of sub-Saharan Africa. But critics point out that Zambia is at the southern end of malaria's distribution, and the problem is more entrenched elsewhere. They also suggest that strategies against  Anopheles arabiensis,  the main mosquito carrier in Zambia, may not work in more northerly regions, where the disease is transmitted by other mosquitoes. In the meantime, donors are determined to press ahead with a continent-wide boost in the scale of malaria programmes. A branch of MACEPA called the Learning Community, based in Lusaka, is aiming to spread Zambia's expertise to several countries, including Ethiopia, Tanzania, Malawi and Zimbabwe. And household assessments are planned by the governments of more than 40 countries in Africa. \u201cWe are confident that these are the same strategies that will work in a Tanzania or a Zimbabwe, for example,\u201d says Judith Robb-McCord, who runs the Learning Community. \u201cThey may have a different spin in terms of their adaptation in particular countries, but we're relying on the theory that practices that work here will work in other countries.\u201d See Editorial,  page 1030 , and online at  http://www.nature.com/news/specials/malaria/index.html  . \n                     Malaria Institute at Macha \n                   \n                     Zambia National Malaria Control Centre \n                   \n                     Malaria Control and Evaluation Partnership in Africa \n                   \n                     Bill and Melinda Gates Foundation \n                   \n                     World Health Organization malaria factsheet \n                   \n                     Nature focus on malaria \n                   \n                     Nature web focus, malaria \n                   \n                     More health and medicine stories \n                   Reprints and Permissions"},
{"file_id": "451388a", "url": "https://www.nature.com/articles/451388a", "year": 2008, "authors": [{"name": "Melinda Wenner"}], "parsed_as_year": "2006_or_before", "body": "Viral and microbial interactions within living tissues are more complex than previously thought. Melinda Wenner explores whether a periodic table of the infectious could help sort out the mess. In 2001, Paolo Lusso asked his colleague Leonid Margolis for a favour. Lusso, a virologist at the San Raffaele Scientific Institute in Milan, Italy, had recently discovered that HIV patients are often co-infected with human herpesvirus 6 (HHV-6). Although typically benign, HHV-6 seemed to hasten HIV progression, and no one knew why. Lusso was studying HHV-6's effects on lymphoid cells but wanted to see what the virus did to whole pieces of lymphoid tissue. So he asked Margolis, a virologist at the US National Institute of Child Health and Human Development in Bethesda, Maryland, and an expert on three-dimensional-tissue, to perform some experiments for him. Margolis agreed. Human lymph-node tissue was hard to come by, but tonsils, which doctors remove from patients all the time, are also lymphoid tissue ? and Margolis had developed an experimental tonsil-tissue system to study HIV pathogenesis. Because HHV-6 infection was often found alongside HIV, Margolis and his colleague Jean-Charles Grivel co-infected tonsil tissue with both viruses. He predicted that the herpesvirus, normally suppressed by the immune system, would be free to replicate in immune-compromised HIV-infected tissue. But, weeks later, when Grivel analysed the infected tissue, something was wrong. HIV wasn't replicating. Although excited, the scientists didn't want to jump to conclusions, so Grivel performed the experiments again. ?There was another three weeks of waiting, which was really very emotional,? Margolis says. Nevertheless, they got the same result. ?We couldn't believe our eyes,? Margolis says. HHV-6, at least in this situation, seemed to protect against HIV 1 . So infectious agents interact with each other and with hosts in unpredictable ways. An average human body is rife with viruses, and benign and not-so-benign bacteria as well as 'endogenous retroviruses', which buried themselves in the human genome eons ago. This crowded house is a different beast from the sterile cell lines used as models. Scientists such as Margolis therefore argue for a broader approach to virology ? one that involves studying infections in a more true-to-life context, predicting their interactions and sometimes taking the unexpected good with the usual bad that infections bring. That microbes can benefit their hosts is by no means new. For example, bacteria living in the human gut are known to influence immune function, and help our body absorb nutrients. But only recently have scientists suggested that infectious viruses could provide their hosts with benefits as well. Viruses influence the host immune system in significant ? and occasionally beneficial ? ways, a concept that isn't surprising when one considers that they have been interacting with immune systems for millions of years. Recently, Herbert 'Skip' Virgin, an immunologist at Washington University School of Medicine in St Louis, Missouri, infected mice with dormant viruses genetically similar to human Epstein?Barr virus and human cytomegalovirus. These viruses, he found, protected the mice from the bacterial pathogens  Listeria monocytogenes  and  Yersinia pestis . Virgin and his colleagues suggest that the viruses upregulate the production of immune factors that prevent further infection rather than interacting directly with the microbes 2 .  \n                Helping or hindering? \n              The endogenous retroviruses that cemented their place in human history by infecting the eggs and sperm of our ancestors account for more than 8% of our genome, and some report that as much as half of our genome is composed of fragmented viruses. These viruses seem to influence immune function; for example, the susceptibility of mice to Friend virus, a strain of murine leukaemia virus, is controlled by two genes derived from endogenous retroviruses 3 . Some have proposed that endogenous retroviruses, long fixed in mammalian genomes, provide the immunosuppression that allows a fetus to develop in its mother's body, despite the differences between the immune systems. Viruses also interact with each other directly, as Margolis discovered for himself in 2001. Similar viruses sometimes compete with each other, causing one to eventually 'win over' a cell and literally block infection by others. Viruses that are different enough from each other can co-infect cells cooperatively; in a process known as complementation, one virus provides another with a useful protein that it co-opts for its own use. Occasionally, viruses become dependent on other viruses. One such example is hepatitis D, which requires the presence of hepatitis B in order to replicate. Margolis uncovered why HHV-6 prevents HIV replication under certain conditions. A subtype of HIV, most often found in early infection, generally gains entry into the cell by binding the receptor CCR5. When HHV-6 infects first, however, it instigates production of an immune chemical that binds to CCR5 receptors, blocking HIV's access. HIV can develop resistance to this chemical over time and HHV-6 co-infection may exert selective pressure on HIV to become immune-resistant, or switch to a different co-receptor ? a change accompanied by increased HIV virulence. This explains the often poor prognosis of patients infected by both viruses. Other human viruses influence HIV replication. Margolis found that human herpesvirus 7 (HHV-7) inhibits HIV replication, albeit via a different mechanism from HHV-6 (ref.  4 ). In 1998, two groups independently reported that HIV patients infected with a seemingly innocuous hepatitis-like virus called GBV-C live longer, although neither group knew why. The studies piqued the interest of Jack Stapleton, director of the University of Iowa's Helen C. Levitt Center for Viral Pathogenesis and Disease in Iowa City, who was, at the time, running an AIDS clinic while studying hepatitis C?HIV interaction. ?We thought it was a statistical fluke and that it wouldn't hold up in a larger study,? Stapleton recalls. With access to hundreds of samples, Stapleton decided to replicate the GBV-C study on a group of 362 HIV patients from his clinic. What he found surprised him and confirmed the findings: ?If patients had [GBV-C], they were three times more likely to be alive at follow-up,? he says 5 . Stapleton's subsequent research, along with work completed by a group in Germany, has shown that a GBV-C peptide interferes with early replication of HIV, and that GBV-C, like HHV-6, increases production of an immune chemical that blocks HIV's entry into the cell.  \n                Always dangerous \n              No one would suggest purposefully infecting individuals with a persistent infection to ward off HIV. After all, one of the hallmarks of viruses is that they evolve quickly. ?Any virus that is not causing disease has the potential to cause disease,? says Robert Gallo, director of the Institute of Human Virology at the University of Maryland School of Medicine in Baltimore, and the co-discoverer of HIV. That said, Gallo, who worked with Lusso on the HHV-6 discovery, and his team recently tested the immune chemical produced during HHV-6 infection as a vaginal microbicide in macaque monkeys and found that it significantly lowered risk of contracting a monkey-infecting version of HIV 6 . To make further progress, scientists need to expand which viruses they study, and how they study them, Margolis says. They need to use tissue systems that preserve immune function and cellular communication, because cells within tissues communicate with one another differently and have a different architecture than do cells that are cultured  in vitro. Moreover, Margolis suggests, scientists should study infections in tissues harbouring the same persistent viruses present in humans. ?If you study a pathogen such as HIV in a cell-culture model where there is no immune system, there is no effect from other microbes that are in the normal state in an infected human,? says Stapleton. ?You're really missing important factors that will influence the pathogenesis.? What would be ideal, says Margolis, is a table, not unlike Mendeleev's table of the elements, for infectious agents. ?I fantasize about creating a periodic table of microbes,? he says. Each cell in the table might feature the name of the microbe, the immune factors it affects, the receptors it uses and the signalling systems it incorporates, he says. Just as the elemental version predicts how two substances react with one another, the periodic table of microbes would predict how two microbes interact in the human body. ?It's off the wall, but it could generate enormous insights,? says Michael Lederman, the director of the Center for AIDS Research at Case Western Reserve University in Cleveland, Ohio. Other scientists say that although such a table may have some practical value, the concept is potentially more interesting as a catalyst for scientific ideas and approaches. Philip Murphy, chief of the Laboratory of Molecular Immunology at the US National Institute of Allergy and Infectious Diseases in Bethesda, Maryland, says that scientists can only get so far with existing tissue and animal models. ?Pathogens are very host-limited, so there is a whole range of human pathogens that you could never do an experiment with in mice models,? he says. For example, although Margolis's lymphoid-tissue model is incredibly useful, he says, there are a number of pathogens that will never infect lymphoid tissue. Scientists will, in other words, need to get creative. Expanding the study of virology in these ways is challenging for other reasons, too. It is difficult, for example, for a scientist with expertise in DNA viruses such as herpesviruses to study their interactions with RNA viruses such as HIV. But collaboration can help, and although broadening the context of virology might make experiments more complicated, it will also make them more realistic, Margolis says. ?In a more complicated system, you probably can understand less,? he admits. ?But what you understand is really relevant.?\n Melinda Wenner is a science writer based in New York City. \n                     Nature Insight: Host-Pathogen interactions \n                   \n                     Web Focus on Plant-Microbe interactions \n                   Reprints and Permissions"},
{"file_id": "451516a", "url": "https://www.nature.com/articles/451516a", "year": 2008, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "Genomewide association studies are starting to turn up increasingly reliable disease markers. Monya Baker investigates where we are now and what comes next. Who would have thought that the future of human health would read like a list of car number plates? Last year, a suite of studies 1 ,   2 ,   3  pinned an increased likelihood of developing heart disease on some mysterious culprits: seemingly incomprehensible strings of numbers such as rs10757274 and rs1333040. The number sequences, technically known as single nucleotide polymorphisms or SNPs, are located close to one another on chromosome 9. No one knows what they do, if anything. But carrying two copies of any of them boosts a person's chance of developing heart disease by the same amount as smoking ten cigarettes a day. The effect is less than that brought on by diabetes or heavier smoking, but it is still one of the strongest risks so far identified by genomewide association studies, which attempt to find genetic variants that occur more frequently in one group of people than another. The association is so robust that it is already being used as a positive control for further genomewide studies of cardiovascular disease. \"If you don't find it, you know something is wrong\" with the analysis, says Ruth McPherson of the University of Ottawa Heart Institute in Canada and lead author on one of the two articles that first announced the association 1  in May 2007. Researchers from deCODE Genetics in Reykjavik, Iceland, published the other 2 . Having one copy of any of the wrong SNPs boosts risk by about 40%. Having two copies of the SNP doubles one's chances of having a heart attack early in life. But despite the strength of the association, pegging meaning to it is difficult. Genomewide association studies find SNPs, single letter changes in the genome, that appear commonly and may correlate to more variation in nearby DNA. SNPs are not necessarily the causative mutations. They might not even be in gene-coding or gene-controlling regions, and thus may not contribute, even indirectly, to risk. The heart-risk SNPs indicate only that the culprit is located within a long genomic region on chromosome 9 known as 9p21.3. No protein-coding genes are apparent within the region, and investigations of two nearby genes \u2014 both tumour-suppressors \u2014 haven't yielded an explanation.  \n                Making sense of the numbers \n              Still, enthusiasts of genomewide association can't wait to find more mysteries like rs10757274 and rs1333040. Scanning hundreds of thousands of SNPs for a disease or condition can turn up dozens of associations. Early studies gained notoriety for their unreproducible results, turning up many false positives. But that is changing with increasing statistical savvy and larger populations for whom more SNPs have been measured. Last year, replicated studies identified loci associated with common diseases such as type 2 diabetes, Crohn's disease and cardiovascular disease 4  (see  'SNP spotting' ) leading some to call 2007 the year of genomewide association studies, as research groups pumped out dense lists of associated SNPs and gene regions like so many car number plates \u2014 meaningless without further information. Ask researchers what is next for 2008 and 2009, and they will gush about longer lists. As researchers scan larger populations for more SNPs, more SNPs will be associated more reproducibly with more diseases. But ask when these obfuscated strings of digits and letters will be used to help find a drug, explain a disease, or recommend tailored treatments for patients, and you'll get a sober response that years of work lie ahead. Researchers readily admit that SNP-scanning studies cannot find important contributors to disease, such as environmental factors or extra copies of genes. Moreover, identified variants, or alleles, contribute to only a small part of the overall risk. Still, by scanning the entire genome, association studies can uncover unsuspected connections between genes and disease. \"Finding the initial SNP is not the same as finding the underlying biology,\" says human geneticist David Altshuler of the Broad Institute in Cambridge, Massachusetts, \"but it's a big step forward.\" Companies are already selling or planning to sell tests that scan for genetic variants associated with disease. Last November, deCODE began selling a test to doctors that will reveal how many copies of the risk allele at 9p21.3 an individual carries. Those kinds of tests worry Muin Khoury, director of the National Office of Public Health Genomics at the Centers for Disease Control and Prevention in Atlanta, Georgia, who has written cautionary commentaries on the issue. \"Even if the association is replicated in many, many studies,\" he says, \"it is still weak information.\" Right now, family history is more predictive. There is no evidence that results of genetic tests encourage people to adopt healthier lifestyles, Khoury says, and such tests could even yield false hopes, as someone who lacks identified risk alleles could be carrying risk variants that are as yet unidentified. Evidence that they make a difference may be a long time coming. \"The private sector doesn't want to do the research and right now no one is telling them they have to,\" Khoury says.  \n                Cooperation is key \n              Nevertheless research continues to seek out new associations in the hope of developing a richer understanding of disease. Ultimately, common variants of even very important genes will have small effects; finding them depends on having enough data to sort through. Genomewide association studies are expensive. Although prices have plummeted, genotyping a few hundred thousand SNPs still costs a few hundred dollars per individual, not counting the complex and expensive tasks of collecting and managing patient data. \"Sample size has been our big enemy here, and sharing data is the best way possible to address this issue,\" says Lon Cardon, a statistical geneticist at the University of Washington in Seattle. To boost sample sizes, the Wellcome Trust has created its Case Control Consortium (WTCCC) and assembled 2,000 patient samples for each of several diseases along with a generic set of 3,000 controls, some 19,000 subjects in all. The rationale is that by comparing SNPs between diagnosed and undiagnosed individuals, genetic risk associations will be apparent.  \n                Missing the point \n              Nilesh Samani, chair of cardiology at the University of Leicester, UK, and one of two lead investigators responsible for coronary heart disease with the WTCCC, explains that even studies with many samples will miss variants with modest effects. Suppose, he says, that there are 10 loci in a genome that each increase the likelihood of a condition by 20%. Statistically, an examination of 2,000 cases and 2,000 controls would pick up at most three of these loci. An independent group with similar sample sizes might also find two or three loci, but they might be different loci, and the plague of false positives would make results inconclusive. \"It's only when we pool all of these studies together that we have a realistic chance of picking up all of those loci,\" Samani says. The data provided by the WTCCC are largely limited to genotype, age, gender and presence of disease. They are proving invaluable for certain studies, but finding how variants affect specific traits such as cholesterol levels or blood pressure requires richer data. A particularly rich vein of it came from the Framingham SNP Health Association Resource (SHARe), which genotyped 550,000 SNPs in more than 9,000 individuals participating in the Framingham Heart Study. Sponsored by the National Heart, Lung and Blood Institute in Bethesda, Maryland, the 60-year-old Framingham study has followed three generations of individuals in Massachusetts. Data from it established smoking and high cholesterol as risks for heart disease. Although not all phenotypic data exist for every individual, the data set contains thousands of clinical variables from blood analyses to vascular imagery to lifestyle surveys, sometimes from the same individual over a span of years. Assembling the database required Framingham to undertake a sort of archaeology expedition of old medical records and publications, but, as the acronym implies, the data are available to other researchers. As of 25 January 2008, all genomewide association studies funded through the US National Institutes of Health (NIH) are required to deposit their data in the Database of Genotype and Phenotype (dbGaP). Although researchers who collect the data have exclusive rights to publish their analyses for at least nine months, scientists who promise to uphold privacy safeguards will be able to access others' data sets immediately. SHARe has a similar moratorium. Such practices have become standard in genomics studies. Nevertheless, some worry that if scientists can publish analyses of downloaded data, they might have fewer incentives to collect the data necessary for answering new, interesting questions. They may also not properly handle the data they do get. Bruce Psaty, a cardiologist and epidemiologist at the University of Washington in Seattle, examined how researchers used data available from two large studies. He found that not only did some researchers go beyond the scope of original agreements, they failed to account for the study's design in their analyses. For example, some analyses searching for predictors of a patient's first heart attack did not exclude patients who had already had heart attacks 5 . Poor analyses of large epidemiological studies could mean real associations are missed or false ones found. Resources are wasted either way.  \n                Share and share alike \n             If scientists are careful and peer reviewers rigorous, giving more researchers access to more data should mean better science at lower prices, says Altshuler. \"No single group can make full use of such large data sets. The creativity of the whole world has to be greater than the creativity of the group that collected the data.\" K\u00e1ri Stef\u00e1nsson, chief executive of deCODE Genetics, says that researchers are already doing a good job of finding collaborators but he resents what he calls the \"Soviet flavour\" of the NIH mandate. \"I don't want to share my data with anyone because the NIH decides I should,\" he says. \"I want to do it because I decide to do it.\" Teri Manolio, director of the Office of Population Genomics of the National Human Genome Research Institute in Bethesda, Maryland, acknowledges such concerns but says that science will adjust. The WTCCC and the Genetic Association Information Network (a public\u2013private US collaboration with goals similar to those of the WTCCC) both grew out of the Human Genome Project. As that 'big-science' project got under way, she recalls, there were \"big concerns\" that people's careers would end when they put their data on the web. That didn't happen. \"It quickly became apparent that just putting a sequence up wasn't a publication, and that that kind of data-sharing didn't actually hurt people,\" says Manolio. In fact, she adds, some initial contributors to the dbGaP have gained additional collaborators and recognition by sharing data, and several academic and corporate investigators plan to contribute data even though they are not required to. With the need for large data sets and the danger of false-positives now both widely recognized, Cardon hopes that scientists can be relied on to do what is necessary to conduct interesting, convincing analyses. \"I think it is really useful to put data up on dbGaP, but it is no substitution for collaborating with the people who collected them,\" he says. \"It's certainly more powerful than just downloading and trying to go it alone.\"  \n                Strength in numbers \n              When multiple data streams come together, results are tangible. After surveying hundreds of thousands of SNPs from 2,758 individuals for associations with blood lipid levels, researchers working with data from the Diabetes Genetics Initiative had settled on 196 SNPs for further analysis. Then they were contacted by another group with data from two more studies that had measured SNPs and blood lipid levels in 6,058 people. Sharing data helped both groups identify more SNPs for further analysis, resulting in publications from both groups 6 , 7 . One SNP identified in that collaboration 7 , rs599839, on the short arm of chromosome 1, was found to be associated with lower levels of low-density lipoprotein cholesterol, and further study showed increased expression of three genes in the liver, including one involved in glucose uptake. Interestingly, that SNP had also been associated with heart disease just months earlier, in a study 3  that confirmed the risk on 9p21.3. Work to understand SNPs' influence will mean chasing down more informative phenotypes and, eventually, lab work to confirm function. Recently, deCODE found that the SNPs on the 9p21.3 region associated with heart disease are also associated with brain and abdominal aneurysms 8 . McPherson's group found the locus associated with arterial disease, suggesting that the region has something to do with the integrity of blood vessel walls. Nevertheless, figuring out the underlying mechanism is proving elusive for both groups. \"We haven't found anything new through very thorough sequencing of the region,\" says Stef\u00e1nsson. McPherson is looking for RNA sequences that might affect gene expression, but the region is long, she says, and even when the responsible sequence is ferreted out, the mechanism might not be obvious. Even if the effect of an identified variant is tiny, the importance of finding a new mechanism could be huge, and genomewide association studies can uncover these in places in the genome that no one would think to look. What many researchers are looking forward to most is surprises. \"For the past 20 years, traditional risk factors were the A-to-Z of cardiovascular biology,\" says cardiologist Heribert Schunkert at the University of L\u00fcbeck in Germany, who, along with Samani, confirmed the association of 9p21.3 with heart disease 3 . Many loci being identified today have no link to these risk factors. \"That's exciting,\" he says. \"It tells me we know very little about the true biology.\"\n Monya Baker is the editor of  Nature Reports Stem Cells   and writes for  Nature   from San Francisco. \n                     HapMap Web Focus \n                   \n                     Wellcome Trust Case Control Consortium \n                   \n                     Database of Genotype and Phenotype (dbGaP) \n                   \n                     deCODE Genetics \n                   Reprints and Permissions"},
{"file_id": "451622a", "url": "https://www.nature.com/articles/451622a", "year": 2008, "authors": [{"name": "Meredith Wadman"}], "parsed_as_year": "2006_or_before", "body": "Scientists and politicians in New Jersey thought that they had a chance to make their state a stem-cell player. Voters thought otherwise. As proponents prepare for a second attempt, Meredith Wadman investigates what went wrong in the Garden State. In the middle of downtown New Brunswick, New Jersey, a small dirt parking lot lies nestled between the state's medical school, the university hospital and the Cancer Institute of New Jersey. It's not much to look at. But Wise Young sees something else: The Stem Cell Institute of New Jersey \u2014 a 14-storey tower dedicated to cutting-edge stem-cell research of the sort that Young, a highly regarded neuroscientist at nearby Rutgers University, says is vital to the future of medicine. The tower is more than just a figment of Young's imagination. Plans have been drawn up; the money to build it has been authorized; and the governor, Jon Corzine (Democrat), posed for cameras at the formal ground-breaking ceremony last October. But ceremony has given way to quiet reality; the tower has little purpose if there is no money to fund research within it. Young has dedicated years to trying to get that money (see ' The long trail ' timeline;  printer friendly version ). In 2007, he was one of the leaders of a group trying to convince New Jersey's citizens to borrow almost half a billion dollars to finance research in the tower and at other facilities. The funding required a public vote, and the polls indicated that New Jersey was game. Nevertheless, Young and his colleagues were trounced: outmanoeuvred and out-marketed by two nimble political action groups. In November, Young wants to go into battle again. Otherwise, he says, New Jersey will fall behind other states in stem-cell research. But first he and his colleagues need to learn from the failures of their last campaign, which have left the future that he dreams of stuck in a down-at-heel parking lot. \n               boxed-text \n             Several US states have launched campaigns against the national policy that restricts federal funding to work on the 20 or so stem-cell lines derived before 9 August, 2001. New Jersey was the first to appropriate state funding for stem-cell research, approving US$10 million in January 2004. In November of that year, the move was eclipsed when voters in California approved Proposition 71, authorizing $3 billion in state borrowing to fund stem-cell research for 10 years. Encouraged by California's success, other states followed, among them New York, where the state government last year established a $600-million stem-cell research fund, and Maryland, which has established a commission to dole out $38 million. Wisconsin is set to spend $750 million on research facilities. And last year, Massachusetts governor Deval Patrick proposed $1 billion in state funding for biomedical research \u2014 half of which would be used to establish a research centre that would house the nation's largest embryonic stem-cell bank. But not surprisingly in a country with stark political divides from state to state, many have opposed the research (see  'The state funding scrum' ). Six states have criminalized it. Colorado voters may weigh in this November on a referendum declaring that legal 'personhood' begins at the moment of conception. A year ago, observers could have been forgiven for thinking that New Jersey had a good chance of boosting its own stem-cell coffers. In addition to being the first to fund the research, it's a liberal state with a governor who is highly supportive of the measure and major population centres that are effectively suburbs of Manhattan and Philadelphia. The 'Garden State' is home to 17 of the world's biggest pharmaceutical companies, and research and development and biotech firms are blossoming up and down the New Jersey Turnpike, the toll road that runs the length of the state. But as in California, advocates such as Young also faced challenges to their grand vision. Some were predictable, such as the state's $32-billion debt and its looming $3-billion budget deficit. Others they should have anticipated \u2014 the short campaign season, the off-year election and fiercely committed opponents who were able to rally major grass-roots support to influence the media and win the election.  \n                Medical innovator \n              Young, 58, was born in Hong Kong and grew up in Japan. After getting an MD at Stanford University in California and a doctorate in physiology and biophysics from the University of Iowa, he started a neurosurgery residency at New York University Medical Center. But after Young had to tell the parents of a 17-year-old wrestler that their son would never walk again, he quit the residency for full-time spinal-cord research. By the time he was 40, he and his collaborators had upended conventional wisdom on the irreversibility of spinal-cord injury by showing that high doses of the steroid methylprednisolone could save about 20% of a victim's function if given within 8 hours of injury (M. Bracken  et al .  N. Engl. J. Med.   322,   1405\u20131411  ; 1990). Young became a hero to legions of people in wheelchairs, including Christopher Reeve, the former actor and stem-cell research advocate, who sought Young out as an adviser and confidant after he was paralysed in a riding accident in 1995. Two years later, Rutgers University wooed Young and made him the founding director of the W. M. Keck Center for Collaborative Neuroscience, and its patient-outreach arm, The Spinal Cord Injury Project. Then, in 1998, scientists managed to isolate human embryonic stem cells. Young saw the potential of the cells for spinal-cord-injury research. \"It opened new possibilities we hadn't realized before,\" he says. He was therefore thrilled when, early in 2004, New Jersey's then-governor James McGreevey (Democrat) signed a law permitting human embryonic stem-cell research and somatic-cell nuclear transfer in the state of New Jersey. But that wasn't enough for Young. Days after the law was signed, he wrote to McGreevey proposing that the state fund a $50-million bond initiative to establish a state-financed stem-cell institute. \"The eyes of the world are watching New Jersey,\" he wrote. \"It would be a shame if stem-cell research in the state does not advance.\" Nine months later, Reeve, who grew up in Princeton, died at the age of 52. Young was devastated. \"It was the fact that he died without seeing what he fought for come to fruition \u2014 that was the saddest part,\" he says. \"It was Christopher's death that really galvanized me.\" By December 2006, when the new governor Jon Corzine signed a bill into law to establish several stem-cell research facilities in New Jersey, the $50 million in bonds that Young had proposed to McGreevey had grown to $270 million. Of this, $150 million would build the Stem Cell Research Institute of New Jersey, run jointly by Rutgers and the University of Medicine and Dentistry of New Jersey's Robert Wood Johnson Medical School. As 2007 started, all that remained for Young and his allies was to find funding for the research that would fill that brand-new building. After a protracted battle in the state legislature, Jersey lawmakers passed a third bill in June, the New Jersey Stem Cell Research Bond Act. It proposed a nearly half-billion-dollar loan that the voters would have to approve. The ballot question, as laid out in the bill, asked for permission for the state to borrow $450 million over 10 years to fund stem-cell research. In an accompanying \"interpretive\" statement that voters would read on election day, ballot question two noted that the loan could benefit New Jersey residents \"with diseases and severe injuries such as Alzheimer's disease, cancer, diabetes, Lou Gehrig's disease, Parkinson's disease, sickle-cell anaemia and spinal-cord injuries.\" Ballot question two didn't mention higher taxes explicitly; it asked voters to approve both the loan and unspecified \"ways and means\" for the state to pay back the capital and interest. But buried in the 17-page bill that created the bond question was a sentence that caught the notice of Steve Lonegan, founder of the New Jersey branch of Americans for Prosperity, an anti-tax group with national headquarters in Washington DC. The sentence that leaped out at him said that if the state lacked the funds to pay back the interest and capital on the bonds \u2014 payments that government estimates put as high as $37 million per year at their peak \u2014 it must tax \"the real and personal property\" of New Jerseyans to make up the deficit. In a state with some of the highest property taxes in the nation, Lonegan knew a target when he saw one. So, from his office in Bogota, a gritty town at the north end of the Jersey Turnpike, Lonegan spent his summer raising cash. Lonegan collected some $450,000, all of it, he says, from unnamed New Jersey donors, who were also motivated to fight two other ballot questions that Lonegan's group was opposing \u2014 on sales taxes and borrowing to preserve open space. The bulk of the money poured into his office between 2 October and 6 November. In a media market in which television time can cost as much as $30,000 per 30 seconds, and a day of radio airtime tens of thousands of dollars, money was crucial.  \n                Fighting back \n              An hour south on the turnpike, Marie Tasy was also preparing for battle. As executive director of New Jersey Right to Life, Tasy is a political force to be reckoned with. She had been fighting stem-cell research since it first found its way onto the political agenda. Tasy was furious, in particular about the wording of the ballot question. It did not spell out that stem-cell research included research on human embryonic stem cells. She was equally incensed that it didn't point out that, under New Jersey's 2004 law, the funds could be used to clone human embryos from which to harvest stem cells, or even \u2014 in her interpretation of the law's wording, which has not been tested in court \u2014 to implant cloned embryos in a womb for gestation. \"It was a very deceptive measure,\" says Tasy, a well-kept woman with huge brown eyes and a direct gaze, who keeps a portrait of a beatific Mother Teresa praying the rosary and a picture of herself with President George W. Bush in her Piscataway office. \"It did not specify what type of stem-cell research would be performed. It did not mention that cloning would be involved up to the point of the fetal stage. It did not mention how the funding for the bonding was going to be paid back.\" On 18 September, New Jersey Right to Life sued the Corzine administration in the state's superior court, arguing that the referendum should be stopped because of this \"deceptive\" wording. The final decision allowing the vote to commence wouldn't arrive until 26 October, 12 days before the election. Meanwhile, Tasy came up with a killer nickname for ballot question two: \"Loan to clone\"\u2014 a media-friendly sound bite that she repeated at every opportunity. She also hired Rick Shaftan, a conservative media consultant. On 19 October, he helped produce a television advertisement featuring Steven McDonald, a Long Island police detective who was shot and paralysed in the line of duty in 1986. In it, the wheelchair-bound detective, in front of a US flag, declares earnestly that \"question two is about taking your tax dollars for something that Wall Street and the drug companies won't invest in. Think about it.\" New Jersey Right to Life reports $7,500 in media expenditures for the period, but Shaftan says that $50,000\u201375,000 would be a reasonable estimate of the group's total spending on the campaign. During an in-person interview, Tasy declined to specify what her group spent, and she did not respond to later requests for further information.  \n                Campaign trail \n              In late July, thirteen miles east of Tasy's headquarters, Young and his pro-stem-cell allies first met around a conference table at the law firm of Wilentz, Goldman & Spitzer, which occupies five floors of a ten-storey office building in Woodbridge. There, Ed Albowicz, a 32-year-old associate reputed to have the tenacity of a bulldog, had been signed off by his bosses to work on behalf of the stem-cell cause. Albowicz's mother had recently died of ovarian cancer, and he was convinced that the research, given a chance, could help save others like her. The firm's octogenarian senior partner Warren Wilentz still visits the office in the wheelchair he's been confined to since a car accident several years ago. The meetings became a weekly event, attended by ten or so people including Young, Albowicz and Russ Oster, a Pennsylvania-based Democratic political operative and direct-mail specialist who had been involved in dozens of campaigns. Young and his allies had reason to be confident: earlier that month, a poll done by Quinnipiac University in Hamden, Connecticut, had reported that 71% of New Jersey voters supported embryonic stem-cell research; 19% opposed it. But there was also cause for concern: that support softened to 49%, and opposition rose to 39%, when people were asked whether New Jersey should finance the research. Oster, for his part, remembers the summer as a time when stem-cell proponents \"were talking among themselves, saying: 'We gotta do something. What do we do?'\" He was especially worried by that 49% support figure: \"Any time you start a campaign where you're trying to get people to vote 'yes' and you're under 50%, it's a warning sign,\" he says. \"It was pretty evident to me that this was definitely going to be a fight.\" They began to map out a strategy: they would aim to run television ads for at least two weeks across the state; be in people's mailboxes twice before the election; do phone outreach by harnessing groups like the Juvenile Diabetes Research Foundation and the Parkinson Alliance. They would need, they calculated, $2 million. They had nothing. It was 24 September before the group formally registered New Jersey for Hope as a political action committee, launched by money from Young and his colleagues. A month later, the campaign had collected $19,546, much of it in small donations. It wasn't for lack of trying. \"We made multiple, multiple calls,\" says Albowicz.  \n                On the fence \n              One obvious ally wasn't biting. The ballot-question backers met twice, hat in hand, with the HealthCare Institute of New Jersey, the trade association for the state's pharmaceutical and device industries. \"We really didn't receive anything,\" Albowicz recalls. Hollie Gilroy, the group's director of communications, says that because of the diverse positions of its 28 member companies, the group couldn't take a stand on the ballot question. \"It's up to the individual companies to choose whether or not to support the issue,\" she says. As the election drew nearer, the anxiety of the proponents grew. \"We had all the plans in the world,\" says Oster. \"But I kept revising the budget down and the overall plan down on a daily basis.\" Opponents of the measure had both money and messengers. On 7 October, New Jersey's priests read from their pulpits a written plea from all five of the state's Roman Catholic bishops, asking parishioners to pray that New Jerseyans vote against ballot question two. In a state where 40% of residents are Catholic, that had a large impact. By blanketing every parish, the church, for a \"_de minimus_\" cost, grabbed the secular media, says Pat Brannigan, the executive director of the New Jersey Catholic Conference. Newspaper editors weighed in with the fiscal argument. On 11 October, the  Star-Ledger  \u2014 the only state-wide newspaper \u2014 published a sceptical editorial, suggesting that New Jersey was too debt-ridden to afford another, $450-million loan. At Right to Life, Tasy was being quoted regularly in the press, watching the hits on her group's website grow daily and fielding \"a lot of phone calls\". \"I didn't need to hold a press conference,\" she notes. \"The media came to us \u2026 They sensed that this was something that could be defeated.\" In principle, the advocates for ballot question two had the power of the Democratic legislature behind them \u2014 and of Corzine, who had campaigned on a pro-stem-cell platform for his win two years prior. (Corzine and his Commissioner of Health, who was at the time his top stem-cell staffer, declined to be interviewed for this article.) But in practice, most lawmakers were preoccupied with their own re-election races and saw little to gain by campaigning on what was clearly a controversial measure. \"We had none of these politicians out there, standing up on this. They all ran for the hills, Democrats and Republicans,\" says Lonegan. Lonegan, however, wasn't worried about his lack of high-profile allies. He and his group were assiduously funnelling their arguments to newspaper editorial boards, printing pamphlets referring to the measure as \"a half-billion-dollar corporate welfare handout for embryonic stem-cell experiments involving human cloning\" and preparing anti-ballot-measure lawn signs. But Lonegan's greatest impact may have been a television advertisement that he hired Shaftan, the media consultant, to produce. In the ad, which blanketed the state's cable networks for 10 days in October, a charlatan figure hawks \"Governor Feelgood's Embryonic Stem-Cell Elixir\". \"Just $450 million, why that's practically free,\" he proclaims. \"That really pissed them off,\" says Lonegan. The news wasn't all dire for Young and his allies. On 22 October, a study was published by Rutgers economist Joseph Seneca and his research associate Will Irving. It had been requested by the legislature, and predicted that the $450-million investment in stem-cell research would return $2.2 billion in economic benefit to the state. Then on 23 October with shovel in hand before a crowd of photographers, Corzine broke ground in downtown New Brunswick for the Stem Cell Research Institute of New Jersey. Joined by the mother and brother of Christopher Reeve \u2014 after whom the building is slated to be named \u2014 the governor promised that the facility \"will serve as the nexus of cutting-edge scientific breakthroughs that will improve and save the lives of millions of our fellow citizens\". That brief moment was the end of the good public relations, though. From 22 October, the ads featuring McDonald, the paralysed police detective, had been playing all over the state. An Eagleton poll on 25 October showed voter support at 57%; but within a week, Young and his allies were hearing from unpublished sources that support had slipped to 48%. On 31 October,  The Record , a newspaper in liberal Bergen County, published an editorial urging voters not to put the state further into debt for a project that the private sector could and should finance. It concluded: \"We say yes to stem-cell research, but a loud no to public question no. 2.\" Around this time, Albowicz called Young in a panic. \"We're going to lose this unless we raise some money now,\" he said. But with less than $20,000 in the bank, Young says, \"We couldn't even get a direct mailing out.\" They hatched a plan and, together with a fundraiser dispatched from Corzine's office, pooled their phone lists and began making cold calls.  \n                Cash flow \n              On 30 October, Corzine sent the campaign $150,000 from his personal bank account. On 2 November, he kicked in another $50,000. That same day, Betsy Johnson, an elderly member of the clan that founded New Brunswick-based Johnson & Johnson added $100,000. Gordon Gund, the Princeton-based former owner of the Cleveland Cavaliers \u2014 an Ohio basketball team \u2014 sent in $200,000. The infusion allowed the campaign to buy several days of radio ads by Michael J. Fox, ageing rap group the Sugarhill Gang and New York hip-hop artist Styles P. They managed pre-recorded phone calls and one direct mailing to about 400,000 people (The state has 8.7 million.) From 31 October, they reported spending $567,000. They never made it onto television. On election night, supporters gathered at the West Orange headquarters of Senate president Richard Codey, a key backer of stem-cell research, to watch glumly as the results came in. Early on it was clear that voter turnout had been dismal, even for an off-year election. That was bad news in a battle in which opponents were more motivated to vote than the supporters. The death knell really sounded, however, when it became clear that even the counties of Middlesex and Somerset, where Rutgers and New Brunswick are located, had rejected the measure. In the end, the state's voters defeated the measure by some 79,000 votes, 53% to 47%, voting against it in three-quarters of the state's counties. Young drew up a post-mortem analysis shortly after the election: turnout in counties that voted against ballot question two was around 32%. Six of those counties opposed the measure by nearly two to one. By contrast, in the five counties that decisively approved the measure \u2014 essentially the Philadelphia and New York suburbs \u2014 turnout averaged 22% and dipped as low as 10%. Young calculated that if turnout in four of the five pro-stem cell counties had matched the 34% turnout recorded in the 2003 state elections, the measure would have passed easily. The fallout from the loss was immediate. The legislature's Joint Budget Oversight Committee, which had scheduled an 8 November meeting to release the first of the $150 million in construction money, cancelled it the day after the election and has not met since. A few weeks later, Corzine summoned Young and a handful of other key stem-cell advocates to breakfast at Drumthwacket, the governor's mansion in Princeton. \"He said that he would try very hard to ensure that the referendum goes on the ballot again\" next November, recalls Young. \"But he also explained to us what a difficult situation it was\", with the state so deeply in debt. Stem-cell research supporters in New Jersey clearly missed two key lessons from California: organize big and organize early. California proponents did so out of necessity: under that state's law, they had to gather 600,000 citizen signatures just to get the measure on the ballot. The result: by the time their opposition emerged, the proponents had built a huge, money-and-celebrity-studded machine that gathered more than $25 million, dwarfing the opponents' six-figure fundraising. They also profited from the high voter turnout characteristic in presidential election years.  \n                Relentless affront \n              Whether and when another ballot question two will confront New Jersey voters remains unclear. It could be as soon as this November \u2014 an idea that has grabbed supporters because of the political advantage that a presidential election is likely to generate. Either way, it won't be long after that, that a new occupant arrives in the White House, with the tantalizing possibility, depending on the winner, of an end to the restraints on federal funding. But in the current flat-funding climate at the National Institutes of Health (NIH), even the most liberal federal policy is unlikely to translate into a big boost. The NIH spent $37 million on human embryonic stem-cell research in 2007. And what funding there is will be the object of fierce competition. All of which leaves Young and his allies determined to press on. Nevertheless, in New Jersey, which is a liberal state, but also heavily taxed and undeniably pragmatic, a stem-cell friendly victory for the presidency could lead to disinterest in seeing state dollars, already stretched to their limit, being brought to bear on long-term basic biomedical research goals. Composed largely of small towns, small-town politics rule the day in New Jersey, and the lofty dreams of a few politicians and scientists can easily be swept under by more immediate matters such as property taxes, political scandals, and the tolls on the Jersey Turnpike. Young has steeled himself, however. He and his allies have met several times since the governor's breakfast, hatching plans to turn New Jersey for Hope into a permanent political action committee and lay out a strategy for the next nine months. \"I have come to the conclusion that to win next November we don't need $500,000, we need $5 million,\" Young says. \"Dozens\" of people, he adds, are hard at work to that end. Meredith Wadman writes for Nature from Washington DC. \n                     Nature Reports Stem Cells \n                   \n                     A stem-cell Glossary \n                   \n                     New Jersey for Hope \n                   \n                     New Jersey Right to Life \n                   \n                     New Jersey Americans for Prosperity \n                   \n                     HealthCare Institute of New Jersey \n                   Reprints and Permissions"},
{"file_id": "451390a", "url": "https://www.nature.com/articles/451390a", "year": 2008, "authors": [{"name": "Sharon Weinberger"}], "parsed_as_year": "2006_or_before", "body": "Half a century after its creation, the Pentagon's Defense Advanced Research Projects Agency is considered a paragon of government innovation. But some question whether it is still relevant. Sharon Weinberger reports. Last year's DARPATech conference could not have been held at a more fitting place than a Disneyland hotel in California. Run by the Pentagon's research arm ? the Defense Advanced Research Projects Agency (DARPA) ? the August meeting was meant to tout the agency's unparalleled record of far-out technological innovation. Next to the theme park, which features Buzz Lightyear Astro Blasters and elaborate virtual-reality displays, DARPA's message ? military technology meets science fiction ? took on a positively surreal form. On display were a robotic dog that can traverse rugged terrain and advanced human prosthetics that could someday be controlled by the brain. One presenter talked of putting micromechanical systems into larvae to create cyborg insects, and the agency's director, Anthony Tether (pictured, with flag), touted its upcoming robot car race, a competition that promised US$2 million to the team whose vehicle could drive itself fastest through a 97-kilometre 'urban challenge' course. Meanwhile, meeting organizers passed out chocolates emblazoned with DARPA slogans and jostled past the pink-clad army of Cookie Lee jewellery saleswomen attending their own conference elsewhere in the hotel. The event was in stark contrast to the buttoned-down DARPA of the early years. Since its creation in 1958, the agency has been a major player in science and technology challenges facing US national security. During the cold war, its experts worked on everything from the space race to the Vietnam conflict. Most famously, it created Arpanet, the precursor to the Internet. Today, in an era when federal agencies talk about becoming more like the private sector, DARPA is uniquely regarded as a model of success. ?It is the jewel in the defence-department crown,? says former defence secretary William Perry, who served as the Pentagon's director of defence research and engineering ? a position that oversees DARPA ? in the 1970s. But now, as DARPA turns 50, observers are asking whether the agency is still as relevant as it once was. The question is particularly pressing as other parts of the US government look to launch their own DARPA spin-offs, in fields from intelligence to homeland security to energy (see 'DARPA: the next generation').  \n                A lean, mean machine \n             With just 250 employees, most of whom serve a term of 4 to 6 years, DARPA is everything the federal bureaucracy is not: lean, innovative and dynamic. Its approximately $3-billion budget is directed at high-risk projects designed to provide the Pentagon with revolutionary advances. Unlike the military services, DARPA projects don't have to be tied to a specific need, and unlike grant agencies such as the National Science Foundation, it can fund risky ideas without going through peer review. That high-risk approach can lead to high pay-offs: the agency counts satellite navigation, unmanned aerial vehicles and radar-evading aircraft among its major accomplishments. But it's also had some flops: laser weapons are still elusive, and a decade-long strategic computing initiative never reached its goal of artificial intelligence. Even in its early days, DARPA struggled for purpose and legitimacy. President Dwight D. Eisenhower created the agency in response to the Soviet Union's launch of Sputnik. The aim was to jump-start space programmes by bypassing the traditional rivalry among the military services that had been developing competing and overlapping satellite technologies. But within a year, the Advanced Research Projects Agency ('Defense' was added to its name in 1972) lost its major mission when Eisenhower created the civilian space agency NASA. The defence department eventually transferred the remaining military space programmes back to the military services. But DARPA soon found its niche in other defence work. Its Project Vela, which began in 1959 and continued through the 1960s, developed seismic sensors and satellites with which to detect foreign nuclear detonations ? proving crucial in monitoring China's early nuclear tests and eventually enabling the United States to negotiate a weapons-testing treaty with the Soviet Union. And Defender, its 1960s anti-ballistic missile defence research programme, led the Pentagon to reject an army-built system in favour of DARPA-supported technology. The Vietnam war reinforced the agency's role as the place senior leaders would go looking for solutions. As US military involvement in the war escalated throughout the 1960s, DARPA became a focal point for counterinsurgency work through its Project Agile. The project incorporated an array of research efforts and experimental technologies from the serious ? the now-infamous Agent Orange, a defoliant widely used by the US military ? to the retrospectively silly ? a jet belt designed to propel individual soldiers on the battlefield. Jack Ruina, DARPA's director from 1961 to 1963, says that he never liked the Agile projects, calling them ?gimmickry and gadgetry?. But Charles Herzfeld, who served as director from 1965 to 1967, says that he embraced the agency's mission in Vietnam. DARPA's 'systems approach' to counterinsurgency has included finding ways to spot tunnels in Vietnam, tracing relationships between families in the Middle East and developing radar to patrol Iran's borders.  \n                Broad scope \n              At the same time, the agency continued its forays into areas such as computer and materials science. It fostered relationships with academia by funding long-term university research and by its sponsorship of the JASONs, a group of mostly university scientists that provides advice on national security. After a dispute over who got to choose JASON members, DARPA dropped the JASONs contract in 2002, and the group's sponsorship shifted to the director of defence research and engineering (see  Nature   416 , 353; 2002). Historically, DARPA directors also facilitated the agency's cooperation with universities and industry; they have all been engineers or physicists, many with close links to academia. As the director typically has sole discretion to fund or cancel a particular programme, the head of DARPA has great power to shape the agency's direction. Tether, an electrical engineer with a doctorate from Stanford University in California, spent much of his career in defence jobs in the private sector, working at companies such as Science Applications International Corporation in San Diego, California, one of the top Pentagon contractors. Tether says that the key to steering the agency is hiring creative managers ?who can generate ideas, unfettered sometimes by hard data? to direct its research programmes. ?I believe strongly that the best DARPA programme managers must have inside them the desire to be a science-fiction writer,? Tether says. H. G. Wells, he adds, would have been good at the job. And Tether thinks that this approach is working. Today, DARPA ?in all of its organizations is banging at the door of highly innovative ideas?, he says. ?I think anyone who attended DARPATech 2007 in Anaheim would agree.? Others aren't so sure that DARPATech is really the best reflection of that, however. The tightly scripted event excludes questions from the audience, and features little detailed discussion of current programmes. Stephen Lukasik, who was DARPA's director from 1971 to 1975, says he worries that DARPATech is now light on substance. ?It used to be a technical meeting,? he says, where programme managers and attendees had a forum to learn and argue about specific technologies. And the conference's penchant for Hollywood-style fantasy also underscores the differences between DARPA in the Vietnam era and today. In Vietnam, the Pentagon turned to DARPA to lead counterinsurgency research. But as violence ramped up in the Iraq conflict, it divided up counterinsurgency tasks among other organizations, such as the Joint Improvised Explosive Device Defeat Organization, an office established to fund technologies to defeat the ubiquitous homemade bombs seen in Iraq. Today, DARPA says that the closest thing it has to the Vietnam-era Project Agile is Persistent Operational Surface Surveillance and Engagement, a software system designed to track insurgents by integrating sensor data. Other DARPA technologies are being used in Iraq, such as unmanned aerial vehicles, translation devices and an anti-sniper system.  \n                Privacy issues \n              DARPA's highest-profile counterterrorism effort, though, proved politically disastrous. After the 11 September 2001 terrorist attacks, Tether ? then in his first year of the job ? created a new Information Awareness Office under the leadership of John Poindexter, a former national-security adviser and a key figure during the 1980s Iran?Contra scandal. Poindexter and his office proved to be more polarizing than expected; its Total Information Awareness programme (later changed to Terrorism Information Awareness), which aimed to sift through huge amounts of data to track terrorists, was attacked on privacy grounds, and Congress eventually cancelled it. ?That was a dishonest misuse of DARPA,? says Hans Mark, a former director of defence research and engineering now at the University of Texas at Austin. For its part, DARPA concedes that the office could have been created with more caution, but says that Tether believed that the right technology could have prevented 9/11, and that data mining could prevent similar events in the future. Although former agency directors and senior defence-department officials agree that DARPA is still highly innovative, they don't agree on whether it is as important as it once was to the Pentagon. In one early significant shift at the end of 1969, leaders moved DARPA out of the Pentagon building to make way for Vietnam analysts; DARPA's offices are now located in nearby Arlington, Virginia. Herzfeld calls the move the loss of ?a great gift?, breaking the immediate link between DARPA and senior Pentagon leadership. The agency also shifted, in the early 1970s, from issues directed by the president to more self-directed work, although Herb York, one of the agency's founding leaders, notes that the shift was more a product of the time than a change in DARPA's role. ?The fact that ARPA moved away from the White House,? he says, ?is not because of anything ARPA did ? it's because of the way the whole scene changed.? Some directors from DARPA's earlier era suggest that later agency directors have tended to choose more conservative projects, leading to less high-impact pay-offs. George Heilmeier focused during his tenure on projects such as radar-evading aircraft and acoustic detection of submarines; the maiden flight of the first stealth aircraft, a project managed and funded by DARPA, took place on Heilmeier's final day as director in 1977. ?Then things changed,? he says. ?Compensation became much less competitive with the outside high-tech world, and DARPA, without the close connection to the secretary of defence, became more conservative in its selection of new, technology-driven initiatives. To many, DARPA was losing its passion and excitement and seemed to be moving in the direction of a bureaucracy.? While Heilmeier questions DARPA's current commitment to bold research, others accuse the agency of moving away from supporting basic research in universities. In 2005, DARPA became the subject of a congressional hearing to discuss concerns that the agency was cutting off long-term support for universities and, in particular, computer-science departments. ?That simply is not correct,? Tether told the panel, saying that there had been no decline in support, merely a shift in priorities to interdisciplinary research. One DARPA-supported researcher attributed the change to the director's management style. DARPA's leader today doesn't agree that the agency is in decline. Tether points to a host of projects developed in the 1980s and later, such as unmanned aerial vehicles, as well as lesser-known advances such as research on solid-state photon detectors, which enabled night-vision goggles, and integrated circuit research that helped lead to modern mobile phones. Tether also points to miniature global-positioning system receivers and DARPA's funding of gallium arsenide research (for use in semiconductors at a time when the commercial industry viewed such an investment as too risky). Today, he says, nanotechnology is a key research area. Former directors, such as Lukasik and Herzfeld, also credit Tether for starting the Grand Challenge robot races that encouraged younger scientists and engineers to compete for a cash prize to develop a fully autonomous vehicle. The first two competitions were held in the California desert; the third, in November 2007, required the robots to operate in an urban area, obeying traffic laws and avoiding collisions. Although he hails that accomplishment, Lukasik warns that without presidential challenges, DARPA is in danger of working on problems that are technologically interesting but not important to the nation. ?Once you move in that direction,? he says, ?you move in the direction of more detail, and if that's the case, you run the risk of becoming irrelevant because your measure of survival is political adroitness rather than technical excellence and solving important problems.? \n               Sharon Weinberger is a freelance writer in Washington DC. \n               See Editorial,  page 374  , and Essay,  page 403  . \n                     DARPA \n                   \n                     Homeland Security?s directorate for science and technology \n                   \n                     Director of National Intelligence \n                   Reprints and Permissions"},
{"file_id": "451766a", "url": "https://www.nature.com/articles/451766a", "year": 2008, "authors": [{"name": "Jane Qiu"}], "parsed_as_year": "2006_or_before", "body": "Chinese authors are publishing more and more papers, but are they receiving due credit and recognition for their work? Not if their names get confused along the way. Jane Qiu reports. Jane Qiu writes for _Nature_ from Beijing. See also Correspondence \"Give south Indian authors their true names,\":http://www.nature.com/uidfinder/10.1038/452530d \"Name variations can hit citation rankings,\":http://www.nature.com/uidfinder/10.1038/453450a \"Names: dropped to avoid prejudice, now useful again\":http://www.nature.com/uidfinder/10.1038/453450b \n                     Nature Blogs: Web visibility \n                   \n                     Nature Network: What's in an Asian name? \n                   \n                     Demonstration page for Scopus identifier \n                   \n                     Thomson's Distinct Author Identification System \n                   \n                     Leydesdorff\u2019s paper in Scientometrics \n                   Reprints and Permissions"},
{"file_id": "451763a", "url": "https://www.nature.com/articles/451763a", "year": 2008, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "George Church has made a name for himself as an 'information exhibitionist'. Erika Check Hayden explores how the technological sage is turning his gaze to the next horizon - you. One day in the 1980s, while a graduate student at Harvard University, George Church decided to change the way he saw the world. He started with simple tools \u2014 scissors, cardboard, and sticky tape \u2014 and he ended up working at his bench with a pair of homemade blinkers affixed to his temples. Asked about the nearly forgotten episode, an older, wiser, now-tenured Church chuckles at his younger self, but he still defends the stunt's essential point. \u201cI was making a statement,\u201d he says, taking a break from running one of the largest, most eclectic biology labs in the United States. \u201cSometimes scientists put on blinders and don't see the big picture.\u201d Performance art's loss was science's gain, but Church has never lost sight of the point he was trying to make that day \u2014 indeed, he has spent his career scanning the horizon for the new, the different and the maybe-just-possible. As biology has become ever more specialized, Church has continued to widen the scope of his work. Yet he brings to each new project a depth and intensity of vision that suggests he knows when to put the blinkers on as well as when to take them off. Church spends most of his time dreaming up far-fetched ideas and doggedly pursuing them. His thoughts on DNA sequencing, first formulated decades ago, led to some of the high-output sequencing techniques now revolutionizing biology. His inquiries have led him to jointly found Codon Devices in Cambridge, Massachusetts, one of the first synthetic-biology firms. He has also founded two other companies, LS9 near San Francisco, California, which makes biofuels, and Knome in Cambridge, Massachusetts, which sells personal-genome sequencing services. Some of Church's ideas don't work out. But many do. And it's his willingness to take risks that colleagues say is his biggest contribution to science. \u201cHe has a certain breadth to his knowledge,\u201d says Robert Weiss, a genomics scientist at the University of Utah in Salt Lake City, who has known Church since the 1980s. \u201cHe can cross disciplines successfully and integrate ideas to move a field forward, and he has done that repeatedly.\u201d  \n                Open access \n              A tall and soft-spoken vegan whose salt-and-pepper beard lends him the aura of a mountain sage, Church's placid manner conceals a tendency to go to extremes to defend his convictions. Last year, when former Massachusetts Institute of Technology (MIT) biologist James Sherley felt that he was denied tenure due to racial discrimination, Church was among a handful of scientists who protested outside the MIT president's office in Cambridge. And Church has posted his birth date, mother's maiden name and signature on his web page to make the point that information wants to \u2014 and should be \u2014 free. His wife convinced him to remove his social security number, but other information, such as a map of the route he walks each morning to work, stayed up. Fritz Roth, a former student who now runs his own lab at Harvard Medical School in Boston, Massachusetts, calls him \u201cthe information exhibitionist\u201d. The title certainly seems apt for a current venture: the Personal Genome Project. Church has already begun sequencing portions of the genomes of ten volunteers: himself, Sherley, entrepreneur and futurist Esther Dyson and seven other individuals, six of whom have also revealed their identities. But unlike other large individual sequencing projects under way, such as the Chinese Yanhuang Project and the International 1,000 Genomes Project, Church will also collect information about his study subjects' 'phenotypes' \u2014 traits such as eye colour, height and medical conditions \u2014 and study how genes contribute to these phenotypes. Then he will publicly release the data in stages and study how this affects the participants, the practice of medicine, and society at large. Along the way, Church plans to continue refining DNA sequencing and analysis in the hope of attracting more volunteers \u2014 perhaps thousands or millions. The project is a culmination of everything that preoccupies Church: technology development, large-scale biology, data transparency and the public interpretation of science. Society at large, he says, doesn't really know how to handle most genetic information. \u201cI'm trying to make sure that there's enough information at a low enough risk so that all of the stakeholders in this technology, now or in the near future, will have a test set to work with,\u201d Church says. Church's project has roots going back decades. Church had had an aptitude for engineering even as a child. Yet he was inspired to apply that talent to biology by his second stepfather, a doctor Church refers to as his \u201cthird father\u201d. \u201cI figured that everything in his black bag, such as antibiotics, and in his office, such as ultrasound, was invented by someone \u2014 maybe like me,\u201d he says. In his twenties, Church typed all the known nucleic acid sequences into a computer in one afternoon to make predictions about how corresponding RNA sequences might fold \u2014 an exercise that made him wish it were possible to know the DNA sequence of every person on Earth. In 1976, he was kicked out of the biochemistry department of Duke University in Durham, North Carolina \u2014 he thought his time in graduate school was better spent publishing papers on RNA folding than on attending class \u2014 and found his feet at Harvard University in 1977. There, he sought out a like-minded mentor in Walter Gilbert, a biochemist who had developed one of the two standard methods for sequencing DNA. \u201cThere just weren't a lot of engineers in biology back then, and his lab seemed like an oasis where you could do technology and get away with it,\u201d Church says. In 1980, Gilbert's sequencing work earned him the Nobel Prize in Chemistry, which he shared with Paul Berg and Frederick Sanger. Sanger had developed the other foundational DNA sequencing strategy, which was \u2014 and still is \u2014 the basis for most sequencing projects. Not long afterwards, Church and Gilbert joined a series of discussions on the possibility of a Human Genome Project, an audacious plan at the time, considering not even a simple microorganism had been sequenced.  \n                You get what you pay for \n              Sequencing was a labour-intensive, error-prone process that cost US$10 per base \u2014 which meant that sequencing the whole human genome would cost $30 billion. The process needed to become a lot cheaper and faster, and the question was how \u2014 and if \u2014 that could happen. Church thought that the project should aim big. He advocated a strategy called 'multiplex' DNA analysis, one that he had developed and refined, partly with Gilbert's help. The strategy involved anchoring pooled DNA to a solid base, then exposing the DNA to a series of reagents to produce image readouts, all on the same machine \u2014 an approach that Church thought should save time, reagents, human effort, and money. Setting up his first lab in 1986, Church worked to develop his concept. He was up against tight competition. Other labs were working on multiplexing, and the company PerkinElmer, now based in Waltham, Massachusetts, was investing industrial dollars on a sequencing machine developed by Applied Biosystems in Foster City, California. The machine improved and automated the traditional Sanger sequencing method. In the mid-1990s, the US Department of Energy ran a comparison between the Applied Biosystems machines and other sequencing techniques, including Church's. Applied Biosystems won. Its sequencers were able to deliver finished genomes faster than multiplexing, and the company's machines became the workhorse of the Human Genome Project for the next decade. Church was disappointed but he didn't give up, predicting that Applied Biosystems' sequencer would never be cheap enough for mass use. \u201cYou tend to get what you ask for, and if you ask for a genome for $3 billion it will be very hard to do a thousand genomes with the technology that can do that,\u201d Church says. Plugging away throughout the 1990s, Church deployed some outlandish ideas. One project, for example, aimed to sequence DNA by passing it through a membrane protein channel while measuring changes in the flow of ions caused by differences in the size and charge of individual DNA bases. \u201cWhere many people would say this is practically not feasible, he would say, 'If you can't tell me what law of thermodynamics it violates, I don't see why we can't do it',\u201d says Roth, who took on the project as a student. \u201cI wouldn't say it failed utterly,\u201d he notes, pointing out that another Harvard lab is still working on the method. As the lab grew, Church gained a reputation for his cutting-edge ideas and eccentric personality. There was a rumour circulating around his department that for a few years as a graduate student, Church had eaten nothing except a nutrient broth he ordered from a laboratory reagent supplier. He confirmed it with a post on his web page. He got the idea after participating in an experiment on leucine deficiency for which he was forced to subsist on a \u201csemi-synthetic diet\u201d of \u201ccookie-like and jell-O-like 'foods'\u201d. \u201cHe smelled like yeast extract the whole time,\u201d remembers one scientist who knew him then.  \n                People magnet \n              Church's reputation became a magnet for equally unorthodox students. Robi Mitra, an electrical engineer looking to join the lab in 1997, recalls what Church told him: \u201cHe said, 'I like to believe that we get up every day and believe what we do is the most important thing we can be doing, and I'm looking for other people who believe that too'.\u201d The pep talk worked: Mitra signed up, and began working on what is Church's current concept for multiplex sequencing, dubbed 'polony' sequencing. In the polony technique, DNA molecules tethered to beads are amplified and processed in discrete units, or 'polymerase colonies' \u2014 hence the nickname polony. Separating the DNA into individual colonies makes it much easier to analyse the results of the sequencing reactions. In 1999, Mitra and Church published the proof of concept that the polony technology worked (R. Mitra & G. M. Church.  Nucleic Acids Res.   27,  e34, 1\u20136; 1999). Parts of the technology were licensed to the biotechnology company Agencourt in Beverly, Massachusetts. In 2006, Applied Biosystems \u2014 the very company that had beat Church's bid to power the human genome project \u2014 acquired part part of Agencourt, and took out a licence on Church's polony technology. Last year, Applied Biosystems began selling the 'SOLiD' system \u2014 a machine that uses some of the same concepts as Church's sequencing technology. It can sequence the amount of DNA contained in the human genome in less than a month. The SOLiD is only one of the 'next-generation' sequencers now available, other are made by 454 Life Sciences in Branford, Connecticut, and Illumina in San Diego, California, which have also licensed inventions from Church's lab in the past. So it seems that Church was eventually proven right: a single human genome is old news, and the scientific world is abuzz with projects to sequence more. In Church's Personal Genome Project, which began last year, his lab will sequence 1% of each participant's genome, mostly protein-coding regions. Church has also sent blood cells from each participant to a New Jersey company that will 'immortalize' the cells, turning them into stable lines for further study. The participants agreed to give extensive information related to appearance and mental and physical health. Last year, they met to discuss questions such as what to tell their families or the press \u2014 something that will become an issue when Church begins releasing this information to the world.  \n                Information release \n              The decision to release data \u2014 to participants and the public \u2014 is the most controversial aspect. The community norm for genetics studies, and the policy set by funding agencies such as the National Institutes of Health, is that study participants' identities must be kept private. Genetic information may be distributed to researchers, but it must first be 'delinked' from information that could identify its source. Church calls this a futile and counterproductive policy. As the amount of publicly available information about us increases, so does the likelihood that our identities can be linked to that information \u2014 whether or not we want it to be. Meanwhile, scientists can't predict what genetic information will be linked to any trait, so it makes more sense to provide a completely unbiased set of information. \u201cWhen you walk into a doctor's office you don't have a chador covering your face or a voice-masking device. You walk in freely exposing yourself so that every aspect of your being can be inspected, and that helps physicians make a diagnosis,\u201d Church argues. \u201cDe-identification creates a research tool that is impoverished relative to what a physician would see.\u201d The difference, however, is that once a doctor makes a diagnosis, the patient controls it, deciding who to tell and what to say. If a person's genetic predisposition to, say, Alzheimer's disease is available to anyone, he or she could face serious consequences \u2014 the loss of a job or health insurance, or distressing reactions from friends and family. Church says that he has already solicited a lot of advice about his project. And he argues that as genomic information becomes more available \u2014 as is already happening with the debut of personal genomics services offered by companies such as deCODE Genetics in Reykyavik, Iceland; 23andme (which Church advises) in Mountain View, California; and Navigenics in Redwood Shores, California \u2014 people may become less fearful of sharing genetic information with others. If everyone finds that he or she carries some genetic risk, as is likely to be the case, people may not think others' risks are such a big deal. Scientists can do a lot to minimize the blowback from genetic disclosure. For an example, Church points to the insurance industry. \u201cThey're trying to reduce costs, and sometimes they can reduce costs in a way that benefits both the patient and them,\u201d Church says. \u201cIt's up to researchers and insurance companies and, to some extent, patient advocacy groups, to be creative and figure out where those win\u2013win situations are.\u201d Church thinks carefully about the societal implications of his work and believes that openness can head off disaster. In 2004, for instance, he helped set up an international effort to head off malevolent uses of synthetic biology. But Church says that there should be restrictions on, or at least monitoring of, technologies that allow individual scientists to synthesize DNA, to minimize the threat that someone could cook up their own deadly microbes. In fact, Church's willingness to engage those outside the scientific community is rare among scientists. It is tempting to speculate that his willingness to deal with outsiders stems from his experience of going against the scientific grain. Although a number of luminaries in the field acknowledge his intelligence and abundance of ideas, several, who did not wish to be quoted for this story, say that they have been annoyed by his stubbornness \u2014 his refusal to go along with the 'community consensus', his unwillingness to be swayed from his convictions by other scientists' arguments, and his insistence on pursuing ideas that seem unworkable or impractical. Yet it is also true that courtesy is one of his defining traits. Because he doesn't engage in the sort of public sniping that has marked high-profile endeavours such as the Human Genome Project, many who disagree with Church still have tremendous respect for him. And Church's critics have to admit, he is remarkably persistent. Few would have pursued a project such as multiplexing for decades after such a discouraging start. Now, Church's determination has spawned a project that is moving genomics out of labs and into regular people's lives. Church's singular focus is remarkable for someone who has such a broad vision, as he himself admits. Thinking back on his graduate school stunt with homemade blinkers, Church jokes that he has always had a touch of a scientific attention deficit disorder (ADD). \u201cEven back then, I was entirely too broad for my own good,\u201d Church says. \u201cBut it seems as if society likes my flavour of ADD.\u201d\n See Editorial,  page 745.  . Erika Check Hayden writes for  Nature  from San Francisco. \n                     Genomics Supplement \n                   \n                     Nature Networks Boston \n                   \n                     George Church's Web page \n                   \n                     George Church on Edge.org \n                   Reprints and Permissions"},
{"file_id": "451884a", "url": "https://www.nature.com/articles/451884a", "year": 2008, "authors": [{"name": "Vicki Cleave"}], "parsed_as_year": "2006_or_before", "body": "The dream of perpetual flight without fuel has inspired pilots to take to the skies in solar-powered planes. Vicki Cleave looks at a mission to fly a solar plane through the night - and around the world. When the Wright brothers made their maiden flight in a powered aircraft on a windswept beach in 1903, it was a short hop, skip and jump into the record books. More than a century later another single-seater aircraft is on its way to making its own record-breaking hops, skips and jumps around the globe. Each of  Solar Impulse 's wings will cover more distance than Orville Wright's first flight; but the plane's 80-metre wingspan is not what's truly impressive about it. The remarkable thing is where it will get its power \u2014 and how little it will need. Driven solely by energy from the Sun, the plane will be carried aloft by solar cells that generate a total of around 9 kilowatts \u2014 roughly the same power available to the Wright Flyer from its single engine. In the Wright era, aircraft were dubbed 'heavier-than-air machines', reflecting the disbelief that they could leave the ground, let alone be successfully piloted. The history of manned solar aviation fosters similar scepticism (see  'Solar aviation highs and lows' ). Most solar planes move so slowly through the air, their ungainly frames buffeted by weather, that they challenge our expectations of modern-day flight. Yet the pilots who wish to fly  Solar Impulse  around the world plan on staying aloft for up to five days at a time, and flying through the Sun-starved night. The US$91-million  Solar Impulse  project is the vision of Bertrand Piccard, a Swiss aeronaut already in the record books as one of the pilots of the first non-stop round-the-world balloon flight in the Breitling Orbiter 3 in 1999. Piccard, who will also be one of the pilots on  Solar Impulse 's trip around the world, comes from a family of adventurers. His grandfather Auguste made a record-breaking balloon ascent to 23 kilometres in the 1930s and his father Jacques was one of two people to have reached the Challenger Deep in the Mariana Trench, the deepest surveyed point in Earth's oceans. Piccard says he first thought of the solar project after he stepped out of the Orbiter. \"The press was saying that my balloon flight around the world in 1999 was the last adventure that was still possible because everything else had been done,\" he says. \"I was 41 years old at that time, and I thought 'it's a pity if everything has been done.'\" He was also disappointed that the round-the-world balloon trip had used such massive amounts of fuel: having taken off with almost four tonnes of liquid propane, it landed with just 40 kilograms. \"We were really limited in fuel, in duration, and if the wind had been slower on the Atlantic we would have ditched and not made it,\" he says. \"I thought it would be great to have a vehicle that would fly day and night with no fuel, with no limit of duration.\" Piccard was not the first to be attracted by the notion of fuel-free flight. More than 25 years ago, US aeronautical engineer Paul MacCready from AeroVironment in Monrovia, California, built a plane light enough and slow enough to fly on the low power output of solar cells. His  Gossamer Penguin  weighed less than 31 kilograms without a pilot and its speed over ground was slower than a bicycle. The planned  Solar Impulse  will have a maximum weight of 2,000 kilograms, including the pilot, almost one-quarter of which will be from batteries for storing energy to fly through the night. If all goes well, the plane will fly at speeds of 50\u2013100 kilometres per hour on its round-the-world trip, landing five times along the way to swap pilots. The trip will be a no-frills experience for the solitary pilot. Inside his snug cockpit, which will protect him from temperature extremes of 80 \u00b0 C to \u221260 \u00b0C, the pilot will endure the same cramped position for up to five days at a time. \"We are not sure how we will sleep or cope with maintaining alertness,\" says Andr\u00e9 Borschberg, the other pilot and chief executive of the project. They will need to be at their most alert when flying the slow-moving craft through the hours of darkness. Although unmanned solar aircraft have made night flights before, no piloted solar plane has stayed aloft for more than 6 hours at a time. The  Solar Impulse  team plans to get through the nights with a mixture of gliding down to lower altitudes and using batteries for power. After dusk, the plane will descend from its maximum daytime altitude of 12 kilometres to just 1 kilometre. The air is denser at lower altitudes, slowing down the plane and reducing the amount of power consumed.  \n                Handle with care \n              But the biggest challenge will be the weather. Because of its light weight and slow speed, the craft can't handle strong winds or turbulence. \"This aircraft is the size of the largest transport aircraft, but it follows any gust that you have,\" says Borschberg. And despite having a wingspan slightly bigger than that of an Airbus A380, he expects  Solar Impulse  to fly a bit like a hang glider \u2014 rather like the Wright Flyer. When taking off, the pilot can choose the best weather window, but during the crucial overnight descent the plane will be at the mercy of winds and turbulence. \"If we have headwinds at night, the night gets longer. If the night is longer the batteries might not be sufficient any more,\" says Borschberg. Each dawn will look sweeter than the last, as pilot and plane run low on energy. Fellow solar-aviation experts \u2014 many of them pilots themselves \u2014 are upbeat about the project's chances. Piccard discussed his plans with MacCready before beginning the project, and his verdict was: \"It will take an elegantly crafted vehicle, flown in meteorological conditions that are hard to find, but it's doable.\" \"I'm just very sad he died before we could make our first flight,\" says Piccard. MacCready's view is echoed by Chris Kelleher, technical director of the Zephyr project, the record-holder for the longest unmanned solar flight. \"It's doable,\" he agrees. \"The issues are the altitudes and the speeds that it would fly at, and the weather conditions.\" As the on-the-ground pilot for the much smaller Zephyr, Kelleher explains that handling isn't a problem once Zephyr is high enough, above about 18 kilometres. \"At altitude, it handles like a big, commercial airliner because the gust sizes tend to be big features, and the aeroplane flies very slowly into the parcels of air.\" Because it will be manned,  Solar Impulse  won't be able to fly as high as Zephyr. So Kelleher thinks that weather forecasting will be crucial for achieving the mission. \"With stable conditions it's possible to predict the weather and come lower,\" he says. But how often will the team be able to count on stable conditions? The planned flight path will follow the Tropic of Cancer, which maximizes the plane's exposure to daylight while hopefully avoiding the worst tropical weather. Borschberg agrees that weather prediction is going to be one of the most important aspects of mission planning. \"At take-off it's not too difficult, because you decide when to take off. For landing it's more difficult because you cannot always plan exactly what happens.\" To test its weather-prediction systems, the  Solar Impulse  team has been conducting virtual flights since last May. The researchers used a simulator that mimics the performance of the aircraft and allows them to introduce meteorological data. \"You can have this aircraft basically flying in real conditions,\" says Borschberg. They have learnt some valuable lessons from the simulations. \"We learned that the flight could be longer than expected,\" says Piccard, and \"that we cannot just take off with the absolute certainty that the next five days will be OK.\" Avoiding bad weather systems means more unplanned diversions. \"When we made simulations from Hawaii to Miami, we had to land in Phoenix, Arizona, because there was a big thunderstorm on the Gulf of Mexico,\" says Piccard. \"So we learned to be more flexible.\"  \n                Fair-weather flyer \n              Earlier solar planes also faced turbulent weather \u2014 with mixed results.  Gossamer Penguin 's successor,  Solar Challenger , completed its flight across the English Channel in 1981 on a sunny day with white puffy clouds. But Bob Curtin, who has worked at AeroVironment since the 1980s, recalls that \"it was fairly turbulent actually, there were lots of clouds in the sky\". However,  Solar Challenger  handled more like a small, light aircraft compared with the giant  Solar Impulse . The ultralight unmanned Helios craft, built by AeroVironment and NASA, didn't handle turbulence so well on its final flight. Its huge 'flying wing' structure was designed to flex into a curved shape when flying, and was able to handle moderate turbulence. As part of NASA's mission to build high-altitude and long-endurance craft it flew at altitudes above 29 kilometres. But during an attempt to set a longer flight record in 2003, the curved wing started oscillating uncontrollably, and the structure broke up over the Pacific Ocean. Unlike Helios,  Solar Impulse 's design follows a classic rigid-wing structure, so rather than oscillate, the craft will get knocked around by the wind. Piccard says that the main reason for choosing this design, however, was the need to incorporate the cockpit. Given the unpredictable nature of the weather, the multinational team building  Solar Impulse  is perhaps wisely sticking to known technologies for the final design. A feasibility study done in 2003 predicted the technology improvements that were likely to be available in 3\u20134 years time, but didn't plan on any technological breakthroughs. For instance, the study predicted that monocrystalline solar cells would have efficiencies of 20% \u2014 they now provide around 22%. At just 130 micrometres thick, the solar cells are flexible enough that they can be integrated into the upper surface of the wings without shattering. The team also correctly predicted that the energy storage density of rechargeable lithium batteries would reach 200 watt hours per kilogram. The design is now frozen with these technologies, so the challenge is one of engineering rather than science. \"The technology is given, so you have to reduce energy consumption,\" says Borschberg.  \n                Testing times \n              Now that the project has three of four major sponsors in place, and two-thirds of the funding it needs, the team has started to build a smaller prototype. Test flights with the 61-metre prototype, scheduled for later this year, should give a better idea of how feasible overnight flights are. Once the lessons learned from the prototype have been fed back into the overall design, the team plans to build the full-size plane during 2009\u201310, with the round-the-world mission slated for 2011 if all goes well. When  Solar Impulse  finally gets airborne, its progress will be watched carefully. \"We're very interested in  Solar Impulse ,\" says Kelleher, who, as a stunt pilot himself, would love a chance to fly the plane. In its quest for energy efficiency and low weight, he sees  Solar Impulse  as \"the art of the possible\". But his company is more interested in unmanned solar planes as an alternative to satellite technology for the communications industry. Because of the cost of transmitting data from Earth to satellites, for example, Zephyr could provide a cheaper way to relay information. \"We don't expect it to replace satellites, but it may be able to do many of the jobs that satellites can't do so well, or do them much more cost effectively,\" Kelleher says. Curtin, now vice-president of business development at AeroVironment, also sees a future for solar-powered planes, although the company's solar research has been on hold since the Helios crash. He says that everything the firm learned about low-powered flight is being applied to its Global Observer project, an unmanned high-altitude aircraft for communications relay and observation. Global Observer will be fuelled by liquid hydrogen in an internal combustion engine, and will fly for a week at a time. The reason the company decided to go for hydrogen and not solar on this project, says Curtin, is that the payload of the Global Observer is large \u2014 around 181 kilograms \u2014 much bigger than the 30-kilogram Zephyr. Another limitation of solar-powered flight is that current technology requires long days and short nights \u2014 restricting the range of the craft to lower latitudes. But Curtin doesn't rule out a return to solar power as solar cells and battery technologies improve. \"The real future is probably in an unmanned vehicle,\" he says, \"because you're trying to make something fly perpetually, and if you do that, it doesn't make sense to have a human on board.\" Piccard is optimistic that his dreams of fuel-free flight, like MacCready's before him, will inspire others to change their thinking about energy consumption. When asked why they pursue such missions, both Piccard and MacCready cited the example of Charles Lindbergh's solo Atlantic crossing in 1927. \"Lindbergh was alone because the rest of the payload had to be gasoline,\" notes Piccard, yet 35 years later aircraft crossing the Atlantic were able to carry 300 passengers. This solo pilot is gambling he won't be alone forever.\n Vicki Cleave is a senior editor for  Nature Materials. \n                     Nature Insight: Materials for clean energy \n                   \n                     Solar Impulse's website \n                   \n                     QinetiQ's Zephyr project website \n                   \n                     AeroVironment's website \n                   Reprints and Permissions"},
{"file_id": "451012a", "url": "https://www.nature.com/articles/451012a", "year": 2008, "authors": [{"name": "Daniel Cressey"}], "parsed_as_year": "2006_or_before", "body": "As countries race to file claims to areas of the sea floor before a United Nations deadline, geologists and geophysicists are getting caught up in the frenzy. Daniel Cressey reports. Russia's twin Mir submersibles are perhaps the most- photographed explorers of the deep sea. Because they are manned, and can dive together to depths of 6,000 metres, film producers love sending them down for unprecedented footage. The 20-year-old Mirs have starred in a number of Hollywood adventures, including director James Cameron's exploration of the wreck of the  Titanic . So it is fitting that it was the flashy Mir-1 that planted a Russian flag on the sea floor at the North Pole last August, as part of a high-profile race to claim the riches of the Arctic Ocean. The stunt was, of course, purely symbolic ? Russia does not own the sea floor at the North Pole, at least not yet. But the flag-planting reignited nationalistic debate about who has rights to what in the Arctic. And scientists play a crucial part in resolving those debates. The latest area of interest is the Lomonosov ridge, a submerged rise about 1,800 kilometres long that runs from offshore Russia to offshore Greenland, crossing the geographic North Pole in the process. The ridge is also key to the ambitions of Russia, Denmark and Canada to lay claim to swaths of Arctic sea floor. The validity of their claims will be decided by a United Nations body, in accordance with the 1982 UN Convention on the Law of the Sea (UNCLOS). Under Article 76 of this treaty, a state can assert rights over the sea floor and its accompanying oil, gas and mineral wealth ? beyond the standard 'exclusive economic zone' that extends to 200 nautical miles off the coast. For this to happen, the country must prove that the claimed area is a ?natural prolongation? of its continental shelf (see 'How to split up the sea floor' ). Russia and Denmark both sent expeditions to the Lomonosov ridge last summer, but they are far from alone. A deadline of 2009 is approaching for some nations to stake claims under UNCLOS, and experts are poised for a rush of new applications worldwide. Most agree that the international body will eventually accept some of the claims. ?There's no doubt in my mind the map of the Arctic will change as a result of these actions,? says Ronald Macnab, a marine geophysicist formerly of the Geological Survey of Canada who now works as a consultant to those submitting claims. But redrawing maps will take time. ?There aren't going to be any easy claims under Article 76,? says Lindsay Parson, an oceanographer at the University of Southampton, UK. In the process, geology and geophysics will have a significant role ? as is already happening not only at the Lomonosov, but also in claims regarding the Bay of Biscay off France and Spain, the sea floor off New Zealand, and elsewhere.  \n                Surveying the deep \n              The Lomonosov ridge, named after the eighteenth-century Russian scientist and writer Mikhail Lomonosov, is ground zero for the hottest claims. It may seem strange that the sea floor in the middle of the Arctic Ocean, miles from any land, can be considered a ?natural extension? of Europe, Asia or North America. But the ridge's unique geology is behind this. In the early 1960s, sea-floor mappers Bruce Heezen and Maurice Ewing recognized that the mid-Atlantic ridge ? the chain of underwater mountains that runs up the centre of the Atlantic Ocean, marking where new sea floor is born ? extended into the Arctic, where it is known as the Gakkel ridge 1 . And in 1963, Canadian geologist J. Tuzo Wilson published a paper showing how the basin on the Siberian side of the Arctic Ocean could have opened up as the Gakkel produced new sea floor. That would have moved the Lomonosov ridge, which once formed a sliver along the edge of the Eurasian continent, over to run between Greenland and Russia 2 . In other words, the ridge was once part of Eurasia. In 1991, researchers aboard the German icebreaker  Polarstern  and the Swedish icebreaker  Oden  set out to test this idea. Geologist Yngve Kristoffersen, of the University of Bergen, Norway, was part of the research team. ?When we saw the first seismic survey I almost went through the roof,? he remembers. ?It was all there, just like a textbook.? On the Eurasian side of the ridge lay half-grabens, features formed by fault rifting as the ridge pulled away from the continent. On the other side lay deep sediments. Kristoffersen and his colleagues estimated that the ridge subsided below sea level between 64 million and 56 million years ago 3 . In 2004, the first cores were drilled from the Lomonosov ridge, providing the first 'ground truth' for its geology and the history of the Arctic 4 . ?Basically the bedrock is the same material from the Eurasian margin,? says Kate Moran, an oceanographer at the University of Rhode Island, Kingston, who co-led the Arctic Coring Expedition. ?There's full agreement that the ridge used to be connected to the Eurasian margin.? With the origin of the ridge not in doubt, the Russians must stake their claim on an interpretation of the brief passage in the UN rules, and must prove where exactly, if anywhere, the ridge is attached. ?That is the difficult part,? says Larry Mayer, director of the Center for Coastal and Ocean Mapping at the University of New Hampshire, Durham. ?The issue is this demonstration of a natural prolongation of the land mass.? Moran says that some theories suggest that the ridge might have 'sheared' at the Russian end, meaning it is not really attached in the traditional sense. But such questions are difficult to answer without more data ? hence the recent Russian and Danish expeditions. A 2001 Russian claim to UNCLOS for more Arctic territory was sent back with a request for more information. Exactly what was requested is not known, but this year's expedition included not only the Mir submersibles, which gathered water and sediment samples, but also aircraft running tests that included gravity surveys. Russian scientists are currently studying the data. For their part, Denmark and Sweden sent two icebreakers as part of the Lomonosov Ridge off Greenland (LOMROG) 2007 expedition. The project ran into extraordinarily thick ice at the southern end of the ridge, but managed to undertake geological coring and oceanographic sampling, among other studies. ?We wanted to see change in composition going from the shelf onto the ridge,? says Christian Marcussen, a senior adviser at the Geological Survey of Denmark and a principal investigator on the cruise. ?What we're looking for is some kind of crustal continuation from the continent to the ridge.? But that may not necessarily hold sway with the UN committee deciding on the question of natural prolongation. ?The geochemical affinity of rocks is not a defining factor in whether rocks are part of a nation's landmass,? says Vaughan Stagpoole, a geologist at the Institute of Geological & Nuclear Sciences in Lower Hutt, New Zealand. ?The key is in distinguishing such rocks from  in situ  'deep ocean floor with its oceanic ridges', which does not form part of the natural prolongation of a state,? he and his colleagues say in a statement to  Nature . Many of the LOMROG findings have not yet become public, as they are being prepared for publication in peer-review journals or for submission to the UN commission. But a few details emerged last month at a meeting of the American Geophysical Union, in which co-investigator Martin Jakobsson, from Stockholm University in Sweden, reported features such as ice scours on the ridge, probably caused by giant icebergs dragging across the ridge surface. The Danish part of the team, led by Marcussen, is still working on data from their underwater surveys, which aimed to delineate the foot of the continental slope to better inform any claim Denmark may make under UNCLOS.  \n                Small country, big claim \n              For its part, New Zealand submitted a claim in 2006 that encompasses roughly 1.7 million square kilometres surrounding its islands. The claim is based on a host of complex features, including plateaux, ridges, seamounts and trenches, and also involves the first claim of a major active subduction margin. Some of the areas in the claim may overlap with other countries' entitlements (see map opposite), and New Zealand is in ongoing negotiations with Fiji and Tonga regarding this. The New Zealand claim team had a budget of NZ$44 million (about US$30 million), underscoring the interest governments have in sea-floor claims. Under UNCLOS, nations get rights to exploit the oil, gas and mineral wealth in sea-floor areas awarded to them. The team, however, says its survey was motivated by the desire to know what might be claimable under Article 76, as opposed to what might lie within that potential claim. Other claims also seem to fall under the 'grab-it-now' mentality. Under UNCLOS, countries lose their rights to stake a claim once their deadline has passed. UNCLOS stipulates that countries have 10 years from the date they ratified the treaty to submit claims, with none coming before 2009. This gives deadlines of 2009 for Russia, 2013 for Canada and 2014 for Denmark ? not a huge amount of time considering that the Arctic, for instance, is accessible to researchers for only a few months each summer. In 2006, France, Spain, the United Kingdom and Ireland submitted a joint claim to a small area in the Bay of Biscay. The claim is based on a series of relatively small ridges that run out into the Atlantic, remnants from the period when the Iberian peninsula moved away from what is now France, creating the Bay of Biscay. ?The process of ocean basin formation is not always a clean break,? explains Parson, who is heavily involved in the UK claims. ?Very often there's a lot of stretching and twisting that goes on. You can think of it as similar to breaking a biscuit ? there are crumbs left over.? The Bay of Biscay claim is one of the less controversial under the UNCLOS process. ?It's relatively small but the total area is about the same as the land area of Ireland ? on that sort of scale it is significant,? says Peter Croker, head of the team for Ireland's submission. ?As submissions go it is relatively straightforward,? says Croker, who also serves as a member of the UN Law of the Sea commission on the limits of the continental shelf. Subgroups within the commission decide on the merits of each claim, and they do not include any members who might have a stake in the claim being discussed. More likely to be controversial are the expected claims around the hotly disputed 'island' of Rockall farther north in the Atlantic. The tiny, uninhabited outcrop that forms Rockall is frequently entirely swamped by waves and can itself support no claims; under UNCLOS, rocks that ?cannot sustain human habitation or economic life of their own? cannot be used to claim the sea floor around them. But the sea floor surrounding Rockall could potentially be claimed as an extension of Ireland, the United Kingdom, Iceland or Denmark's Faroe Islands under the UNCLOS process. The four countries have clashed before over Rockall. Ireland and the United Kingdom have agreed how to divide the continental shelf within their exclusive economic zones. But Iceland and the Faroes may also eventually claim part of the sea floor around Rockall ? and the disputes are likely to rule out any previously-agreed joint submission as in the Bay of Biscay. Other disputes may flare up farther south. The United Kingdom is expected to file a claim to the sea floor around what it calls the Falkland Islands ? over which it fought a war in the early 1980s with Argentina, which claims the land as the Malvinas. Britain is also preparing a claim for sea bed off the coast of Antarctica, contiguous with its designated Antarctic territory on land. Australia and New Zealand have submitted similar claims off their Antarctic territories.  \n                Flying blind \n              Controversies over claims are not helped by the secrecy of the UNCLOS process. Only summaries of submissions are published, and these need only contain a list of coordinates for the territory being claimed. ?The commission operates under pretty serious rules of confidentiality,? says Macnab. ?We don't know how it has dealt with other submissions that may have similar circumstances. The states are flying blind.? The insistence on confidentiality is written into UNCLOS. ?In an ideal world the whole process would be open,? admits Croker. ?The whole process is not transparent to the outside world. I understand why there's some frustration about it.? Inevitably, the criteria can lead to overlapping claims. For instance, Russia's Arctic claim in 2001 extended to, but not past, the North Pole, taking in the half of the Lomonosov ridge that extends from the pole towards Russia. Claims are expected from Denmark and Canada regarding the part of the Lomonosov on their side of the pole; the countries would have to resolve between them where to put a boundary between their claims. Indeed, the commission cannot even consider submissions on disputed areas without the permission of those involved in the dispute. Hence the joint submission for the Bay of Biscay, which involved researchers from all four countries on the survey team. If the claim is approved, the countries involved can then divide the land among themselves.  \n                Redrawing the maps \n              Even so, scientists working in the area reject media and public statements that such claims amount to a selfish 'land grab'. ?There is order in the oceans and that order is provided by the Law of the Sea,? says Croker. ?Terms such as 'land grab' are unfortunate to say the least.? But some researchers continue to stoke nationalistic fervour. Artur Chilingarov, who led the Russian expedition that planted the underwater flag, has been quoted as saying: ?The Arctic is Russian. We must prove the North Pole is an extension of the Russian coastal shelf.? Danish and Canadian politicians have made similar, if not quite so strident, statements. In the end, though, the decisions that are made by UNCLOS will massively redraw sea-floor maps. Article 76 holds that the commission's recommendations will be ?final and binding?. Still, questions are being raised as to what exactly that means. ?I've been at several meetings where people have questioned what it means, this 'final and binding',? says Macnab. ?The legal people themselves aren't quite sure what the terms mean.? So all the sea-floor wrangling may end up not even being permanent. ?Although most countries have ratified the Law of the Sea,? says Macnab, ?there's nothing to say you wouldn't have some rogue government saying it didn't agree with some past decision and they were going to do things differently. It wouldn't surprise me to see states say 'we want to reopen'.? In which case it is back to the drawing board.\n Daniel Cressey is a reporter in  Nature 's London offices. \n                     Commission on the Limits of the Continental Shelf \n                   \n                     UNEP shelf programme \n                   \n                     Center for Coastal and Ocean Mapping, University of New Hampshire \n                   Reprints and Permissions"},
{"file_id": "451008a", "url": "https://www.nature.com/articles/451008a", "year": 2008, "authors": [{"name": "Ehsan Masood"}], "parsed_as_year": "2006_or_before", "body": "Are think-tanks staffed by scientists a luxury that only rich nations can afford? Ehsan Masood meets the founders of four institutes that set out to help poorer nations to think for themselves. It began with Rio. At the 1992 United Nations Earth Summit in Rio de Janeiro in Brazil, 171 countries came together to hammer out global rules to slow down climate change and halt the loss of biodiversity. Then, as now, international meetings on environmental issues see developed countries pitched against developing ones. At such meetings it is all too easy for jaded commentators, and even participants, to conclude that little of consequence would result. Yet they can inspire individuals to take actions that have a lasting effect. \"Rio was a real turning point,\" says Saleemul Huq, plant biologist and founder of the Bangladesh Centre for Advanced Studies in Dhaka, and now head of the climate change group at the International Institute for Environment and Development in London. \"The Rio meeting acted as a wake-up call to developing countries that we needed to raise our game.\" (see  Huq ) During the heady days leading up to Rio, scientists and government officials from developing countries witnessed first-hand the power of scientific research in helping to inform and change policies. They saw, for example, how the talks on climate change needed a consensus, not just from politicians, but also from scientists. And they discovered that delegates from richer countries had access to environmental expertise from government, industry and academia. Huq recalls that before Rio, environmental problems were viewed as a direct result of pollution. And because Bangladesh had little industrial pollution in those days, he was told by politicians and policy-makers: \"Why the need for environmental policy research?\" That started to change, he says, with the floods of 1987\u201388 when they learned that natural causes alone had not caused the flooding. This led to a big debate about possible causes, including deforestation, and ultimately the human activities discussed at Rio. Rio was also a turning point for biologist Cristi\u00e1n Samper, founder of the Alexander von Humboldt Biological Resources Research Institute in Bogot\u00e1, Colombia. \"Rio is why we decided to establish a new national institute as a joint venture between the government, universities, non-governmental groups and the private sector,\" he explains. The Humboldt institute was set up in 1995 to foster scientific research in support of environmental policy. (see  Samper ) Today, these and other institutes owe their existence to fiercely driven individuals \u2014 each fired-up with a desire to harness the best available knowledge to solve their countries' environment and development problems. The success stories include Huq's centre in Dhaka, Samper's institute in Bogot\u00e1, the African Centre for Technology Studies (ACTS) in Nairobi, Kenya, and the Sustainable Development Policy Institute (SDPI) in Islamabad, Pakistan. The founders of these four institutions were all young \u2014 from 29 to 43 years in age. But they weren't idealistic hotheads unprepared for the realities of building independent science-policy institutions in Africa, Asia and Latin America. They had all received postgraduate education in the United Kingdom or the United States. They were also well-connected at home and abroad, and had good access to funding. They also needed resilience in the face of scepticism from international donors, from bureaucrats unable to see how good science could make for better policy and from scientific peers who were fearful of politics (especially those living in dictatorial regimes). Two decades later, their institutes are still in business \u2014 although, in every case, the founders have moved on to high-status positions elsewhere. As a model of sustainable development the attendees at Rio could hardly have wished for more.  \n                Launch fund \n              The Rio meeting was also a major event in the life of Kenyan-born Calestous Juma, now a professor at Harvard University's John F. Kennedy School of Government in Cambridge. Back then, he was working as an adviser to the team drafting the text for the United Nations convention on biodiversity. He saw first-hand the interplay of science and politics as nations argued, horse-traded and made compromises to agree a final text in Rio. The experience reinforced his wish for Africa to join other countries in making policies on the basis of evidence. (see  Juma ) After gaining a PhD in science policy at the University of Sussex, UK, Juma had returned to Kenya in 1987, when the country was under the one-party rule of President Daniel Arap Moi. \"The original idea was to work with existing centres,\" Juma recalls, \"but this turned out to be too difficult.\" He was forced to create a new institution to enable true cross-disciplinary policy research. Today, that institution \u2014 ACTS \u2014 is one of Africa's leading research institutions and a source of independent advice to governments and international institutions on biotechnology and agriculture. But 20 years ago, scepticism among international donors meant that the centre began life in its founder's spare bedroom. The launch funds came from the remainder of Juma's PhD scholarship, a US$2,500 donation from the Mennonite community of Nairobi and a $50,000 grant from the Ford Foundation. \"The Mennonites were my first real sponsors,\" says Juma. \"They have a reputation for being against the modern world, but what they oppose is consumption and they value innovation and creativity.\" Reasons enough, it seems, to support ACTS. \"Almost everyone else said that the idea would not fly,\" he recalls. I was told that policymakers don't read anything, so there was no reason to write for them. I was also told that there was no precedent for what I was doing in Kenya and that the political atmosphere at the time was too hostile \u2014 that I could go to prison, even.\" His disappointment at the hands of international agencies was shared by other founders. In Bangladesh, Huq says that the agencies would have been more receptive if they had said: 'We want to provide running water in rural villages.' \"They were less interested in helping developing countries to think for themselves,\" explains Huq. The prevailing practice at the time was that donors would hire expensive consultants to set the research framework. \"Our job as scientists in developing countries would have been to implement their vision,\" he says. Huq was fortunate in that one of his earliest supporters was his PhD supervisor, the ecologist Gordon Conway, now chief scientific adviser at Britain's Department for International Development. With Conway behind the idea, Huq was able to secure the backing of the Ford Foundation. Today, the Bangladesh centre is one of Asia's leading environmental policy think-tanks, advising governments and corporations on sustainable development, but Huq says that he will never forget the uncertainty of those early days. \"We had 40 or 50 families who depended on us. Sometimes I would sit in the office late into the night figuring out how people would be paid.\" Former US economics professor, Tariq Banuri, found it easier to attract the attention of foreign donors. As founding director of Pakistan's Sustainable Development Policy Institute (SDPI), he says, \"The vision was to create a trans-disciplinary research institution that was close to policy. In the late 1980s, such an institution would have been rare even in the developed West.\" But he was able to secure the backing of the governments of Canada, Norway and Switzerland, in part because the 1992 launch coincided with Pakistani thinking on an environment action plan. This helped to reassure donors that the SDPI would have some support from the state and was less of a freelance operation. (see  Banuri ) Banuri also had the backing of Aban Kabraji, a Pakistani conservationist who is a senior executive with the World Conservation Union (IUCN) and a well-respected figure. \"We did have a funding shortfall for a six-month period, during which many of us worked unpaid, but that was not to be repeated,\" Banuri recalls.  \n                Academic freedom \n              But friends in high places cannot alter one reality on the ground: a shallow pool of research talent in developing nations. Policy institutions need researchers who have an unusual blend of skills: in-depth knowledge of one or more scientific disciplines, along with knowledge or experience of how this applies in the policy arena. Developed countries have many opportunities for academics to hone such skills. But in the developing world, public universities have less experience of working with governments, of collaborating across disciplines or of working with other non-academic institutions. It is because of this that most of the founders decided to operate independently of public universities. In Pakistan, Banuri explains, \"university staff were poorly paid and seemed unhappy and depressed. This was not the kind of environment I wanted for something new and innovative.\" Banuri also wanted the freedom that independence from the state provides. This freedom proved crucial in the case of a 2002 research project to survey public attitudes to Pakistan's nuclear development. \"This project is unlikely to have happened had SDPI been linked to a public university,\" says project leader Haider Nizamani of the University of British Columbia. The survey's findings are due to be published in early 2008. However, Banuri now thinks that, with hindsight, the decision to go independent was perhaps a mistake. \"It would have been better for us to find a university, draw on its resources, but also help to build its capacity.\" Without building policy-research capacity in universities, his thinking goes, independent institutions will have less talent to draw on. Huq agrees that being based in a university would have been better in the long run. The Bangladesh education system never used to produce independent thinkers: \"You could get a masters' degree by memorizing a textbook.\" But he adds that \"things have changed in the past few years as Bangladesh higher education has opened up to private competition, which is raising the quality of graduates.\"  \n                Instant impact \n              Once up and running, all four institutions found themselves much in demand from policymakers. As there is much less competition for the ear of politicians in these regions than in the developed world, the work of new institutions gets noticed and can have an impact more quickly. \"The Humboldt Institute became a major force in science and environmental policy in Colombia in just a couple of years,\" Samper recalls. \"We helped to create the country's first environmental act and had the environment minister chair our board.\" In Nairobi, ACTS helped to draft Kenya's first industrial-property law in 1989, leading to the creation of the country's patent office. And in Dhaka, the Bangladesh centre helped to create a new environment ministry for the country and to write the country's first environmental action plan. But Sunita Narain, who heads the Centre for Science and Environment in Delhi, cautions that having close ties to government carries risks as well as rewards. Her centre has a policy of not doing contract-research for anyone, and has taken a more activist role. Because India has long had a more stable democracy, unlike the other four countries, think-tanks that combine policy and research are more common. Still, she says it is crucial for all institutions in the developing world to evolve in response to changing times. Now that environmental issues have become mainstream, for example, think-tanks need to find a niche to maintain their impact. International partners, too, bring some risks. Each of the founders wanted international contacts but without turning their institution into a contract-research centre for rich clients. For this reason, both the Bangladesh and Pakistan centres avoided bidding for lucrative overseas government contracts. But this was a form of research they were not always able to avoid, particularly when cash-flow from other projects was tight. Three of the centres formed a support network with other international partners. In Colombia, Samper says that such collaboration was seen as less of a priority. \"I did suggest this to our board, but they took a different view and felt we already had the world's top expertise in Colombia,\" he says. But he doesn't think the centre's work suffered: \"I would say that the best lessons and experience came from other developing countries.\" Social scientists have a phrase to describe what happens when the founder of an institution refuses to hand over leadership to a new generation, sometimes in the mistaken belief that the organization will collapse without them. They call it 'founder syndrome'. In the developing world, one of the most notorious examples of founder syndrome is that of the late Thomas Odhiambo, founding director of the International Centre for Insect Physiology and Ecology in Nairobi. Odhiambo, a charismatic figure, friend and adviser to generals and presidents, led the institution from 1970 until 1994 when he was forced out by the institution's donors and by its governing body. Odhiambo was determined that the centre should follow his vision. But the donors had other ideas and, today, the centre's website carries no reference to its founder.  \n                Letting go \n              Mindful of the dangers of clinging on, three of the founders established limits to their terms as directors. And all four organized the recruitment of a successor and then cleared their desks when their time was up. \"I was determined to avoid a situation where the founder sticks to his creation like a leech and refuses to let go,\" Banuri says. He adds, \"I wouldn't have minded an offer to sit on the board after I left, but I felt board members were too afraid that I would try to continue to run the organization.\" Banuri and Juma have since opted to return to working at the coal-face of research and are no longer responsible for the day-to-day administration of a large institution. By contrast, Samper and Huq are still active in the running of large organizations or collaborations. Huq remains non-executive chair of the board of the Bangladesh centre, and Samper is the acting head of the Smithsonian Institution in Washington DC. These different choices are perhaps explained by the contrasting styles of the founders. Both Banuri and Juma hold strong opinions and are not afraid to share them, whereas Samper and Huq would prefer to build a consensus. The founders offer similarly contrasting advice to anyone wanting to follow in their footsteps. As Juma puts it: \"If you have a good idea, don't consult too many people, as they will try to put you off.\" Samper, though, says it is important to: \"take your time to build a vision, focus on a few priorities and have the buy-in of key stakeholders.\" He recalls that the best advice came from his father, who founded several agricultural research institutions in Latin America. \"He said I should set a clear vision, hire a really good team and make sure I step down at the right time. The last of these is one of the hardest things, but I think it is crucial.\" Not surprisingly, given their successful career paths since leaving the running of their institutes to others, each of the founders feels the experience was worthwhile, and will admit to few if any regrets. Some even say that they would be willing to start over again, in an advisory role if not an executive one. As mentor for his next project, Juma believes in finding donors who see the value of learning from mistakes: \"This is critical for any new institution,\" he says, \"but in the eyes of a donor, mistakes are to be punished.\" And the most important lesson to pass on? There are no guarantees. In the same way that a research funding council cannot walk into a lab and demand the results of an experiment before it has even started, founding a new institute is a process of exploration without a certain outcome. In this sense, says Juma, \"International development is much like basic research.\"\n   See Editorial,  \n                     page 1 \n                    . Ehsan Masood writes about science in developing countries. \n                     African Centre for Technology Studies \n                   \n                     Bangladesh Centre for Advanced Studies \n                   \n                     Sustainable Development Policy Institute \n                   \n                     Alexander von Humboldt Institute \n                   \n                     Essay in Chronicle of Higher Education looking at SDPI's first year \n                   \n                     Articles from Saleemul Huq \n                   Reprints and Permissions"},
{"file_id": "4511042a", "url": "https://www.nature.com/articles/4511042a", "year": 2008, "authors": [{"name": "Brendan Maher"}], "parsed_as_year": "2006_or_before", "body": "After decades of work, a pioneering malaria vaccine may soon reach the final phase of clinical trials. In the first of two features on efforts against malaria, Brendan Maher reports on a vaccine that is far from perfect - but which may provide new direction and save thousands of lives. In 1987 Rip Ballou taped an ice-cream carton to his arm. The young US Army doctor was doing his bit for science; inside the carton five hungry mosquitoes set about doing theirs. It's an uncomfortable situation, Ballou remembers with a grimace, to have a bunch of the insects \u201cjust whaling away on your arm\u201d; all the more so when you know that they have been carefully infected with malaria parasites, and soon you will be too. Ballou and five colleagues at Walter Reed Army Institute of Research (WRAIR) in Silver Spring Maryland were testing a vaccine candidate, FSV-1. They'd first been injected with it a year earlier; now it was time for any immunity they might have developed as a result to be 'challenged'. Nine days after the mosquitoes had their meals, the first unvaccinated control tested positive for parasites and was given drugs to clear his system. The second control and three vaccinated volunteers followed suit in short order, apparently no more resistant. On the eleventh day another of the six volunteers fell ill \u2014 the tremors struck Stephen Hoffman, a WRAIR colleague in the Navy as he was giving a lecture in San Diego. On the twelfth day, while at a party, Ballou started to feel out of sorts. Not sure if it was the home-brewed beer he'd been drinking or parasites ravaging his red blood cells, he had his wife drive home. Soon he was cycling between chills and fevers and experiencing headaches of unprecedented intensity. \u201cI have never been so sick in my life,\u201d he says. \u201cIt gave me an extremely healthy respect for this disease.\u201d Five of them had succumbed. But the sixth vaccinee, Daniel Gordon, was still healthy more than four weeks later. For the first time a simple vaccine had protected someone from malaria 1 . That first glimmer of hope grew into an extensive collaboration between WRAIR and the drug company GlaxoSmithKline (GSK) that has turned a descendant of the FSV-1 vaccine, the oddly named RTS,S, into the world's 'most advanced' malaria vaccine. Most advanced means that phase III clinical trials, the final step before licensing the drug, could start by this September. If they go well the vaccine could be licensed and in use by 2011. To get this far speaks of determination, imagination and perseverance \u2014 qualities Ballou and his colleagues, like so many researchers into tropical medicine, show in abundance. It also speaks of something rarer in the field: a lot of money. All told, GSK and partners will have spent upwards of US$500 million by the end of the next set of trials. \u201cWe were convinced that if we develop the malaria vaccine it will be hard for society to refuse to pay for it,\u201d explains Jean St\u00e9phenne, president and general manager of GSK Biologicals, the unit that works on the vaccine at its facility in Rixensart, Belgium. And in recent years this conviction has begun to look warranted. Private donors and governments are funnelling hundreds of millions of dollars into projects such as the GAVI Alliance, which provides access to immunizations in the developing world; there is increasing pressure for countries to promise 'advanced market commitment', the setting aside of funds today to buy vaccines when they become available tomorrow. But if RTS,S is the first vaccine to be so bought, it will still fall short of the traditional goals for vaccines. It has improved since its meagre one-in-six showing in that first 1987 challenge, but not that much. RTS,S will offer at best partial protection, maybe 30%, against infection; some indicators predict it might diminish levels of severe malaria by as much as 50%. That may be enough to give infants and small children a better chance of surviving the scourge while they're most vulnerable. But it is a long way from the last word.  \n                Military origins \n              Malaria research has long been of interest to armies and navies fighting wars in hot wet climes. It was while embroiled in Vietnam that the US military began funding work at New York University on irradiated parasites, which turned out both to be non-infectious and to provide immunity in lab animals. The work culminated in a landmark 1973 paper describing long-lasting protective immunity in humans inoculated with irradiated sporozoites 2  \u2014 the form of the parasite that travels from the insects' salivary gland to the victim's liver. Irradiated sporozoites were not, however, seen as a practical way forward, because their production required live mosquitoes and the use of human blood (Hoffman is now revisiting that conclusion at Sanaria, a company that he has founded in Rockville, Maryland). Instead, researchers looked for what it was about the sporozoites that gave protection, and how the new technologies of genetic engineering might mass produce it. In the early 1980s antibodies against the sporozoites were used by researchers at the National Institutes of Health and WRAIR to identify the 'circumsporozoite' protein (CSP) and clone the relevant gene 3 . Working with WRAIR, the labs of what was then SmithKline Beckman in Philadelphia, Pennsylvania, zeroed in on a repeating pattern in the protein that seemed to be a major target of antibodies. This fragment, grown from genes cloned into bacteria, went on to become the main ingredient in FSV-1. In the coming years, Ballou and the SmithKline researchers in Philadelphia tested several variations on the theme. But SmithKline had been moving most of its vaccine development efforts to Rixensart, where they had recently acquired a small but dynamic laboratory, Recherche et Industrie Th\u00e9rapeutiques. Joe Cohen, who took over the project in Rixensart at the same time as Ballou and his colleagues were infecting themselves, had new plans for the CSP. Faced with turning the repeating fragment from the protein into a real vaccine, Cohen decided to use lessons that the company had learned from its successful development of a recombinant hepatitis B vaccine, Engerix-B. That vaccine consisted of a surface antigen protein from hepatitis grown in yeast; at high enough concentrations that protein spontaneously forms virus-like particles that have a greater effect on the antibody-making parts of the immune system than loose proteins could. Fusing the repeat region from the CSP to the hepatitis surface antigen protein, Cohen hoped, would make similar particles festooned with the CSP fragments and thus able to provoke the production of antibodies targeted at the sporozoites. Given a widespread suspicion that antibodies wouldn't be enough to elicit immunity, Cohen went on to add a fragment from the tail end of the CSP that was thought likely to interest the other arm of the immune system \u2014 the arm that prompts T cells to attack infected cells. The resultant \u201cdouble whammy\u201d, as Cohen calls it, was a gene for a protein containing the antibody-inducing repeat (R), the portion recognized by T cell white blood cells (T) and the hepatitis B surface antigen (S). With all these additions, though, the surface antigen protein lost its knack for self assembly. Through a lot of fine tuning, Cohen finally hit on a way to regain it: one part RTS to four parts plain old S. RTS,S was born.  \n                The vaccine factory \n              Cohen has shaggy grey hair and an endearing politeness. When showing visitors around the Rixensart facilities where RTS,S was developed, he holds the door for all those he has in tow, necessitating an awkward dance as he takes up the lead again. Although Ballou and others wax eloquent about the part he played in inventing RTS,S (the patent is in his name), Cohen shrugs the praise off, insisting it was a team effort. But the pride with which he opens a nondescript door in a nondescript building on GSK's Rixensart campus and ushers visitors into the production facilities is unmistakeable. Knee-high windows along one side of a long hallway look out over a fermentation room where yeast colonies from Petri dishes are scaled up to fill 1,600-litre vats, tended by technicians clad in clean-room paper suits. Through a second set of windows opposite, more technicians tend the bank of computers that monitor fermentation. They're in testing now, but Cohen says that once running at regular capacity, this facility could produce tens of millions of RTS,S doses per year. Already, there is more than enough capacity to provide the doses needed for the 16,000 infants he and his colleagues want to recruit into the imminent phase III trials, which will take place across 10 research sites in 7 African countries. It's just one indication of how quickly the project is moving. Twenty kilometres away, in Brussels, investigators from all over Africa are arriving at a moderately swanky Sheraton right near the Grand Place \u2014 people such as Salim Abdulla of Tanzania, who oversees the Ifakara Health Research and Development Centre, a research study site in Bagamoyo, and currently chairs the Clinical Trials Partnership Committee, the formal group of study-site leaders that meets once a year. His cackling laugh rings out repeatedly despite the 30 hours he spent travelling here and the knowledge that the next two days will be intense. High on the agenda is how the different study sites that might take part in phase III trials define severe malaria, a measure that has been difficult to standardize. \u201cThey go to the same school but they don't agree,\u201d Abdulla says. In the lobby, talk centres on the state of affairs at the two sites in Kenya, where violence has been spreading after a disputed election. Getting from a well designed virus-like particle to international politics and hundred-million-dollar trials was a demanding business. The first requirement was to find a way to get the biggest immunological bang possible for the RTS,S buck \u2014 which is to say choosing the right adjuvants. Adjuvants are, in the oft-quoted words of Yale immunologist Charles Janeway, the vaccine makers' \u201cdirty little secret\u201d. Ranging from inorganic chemicals such as aluminium hydroxide to fragments of bacterial cell wall, adjuvants boost the immune system's response to the vaccine they accompany, although only recently have researchers begun to understand why. In the 1980s one of the arguments that St\u00e9phenne used to convince SmithKline to invest heavily in Rixensart was that more sophisticated adjuvant formulations would lift hard-to-develop vaccines to the next level of efficacy. The malaria effort, aiming at something no other vaccine had ever achieved \u2014 immunity against a complex parasite \u2014 was in part an adjuvant stalking horse. In 1990 the first human challenge trials of RTS,S showed that adjuvants were crucial; with one adjuvant preparation the vaccine was worthless, but with another it protected two out of eight subjects \u2014 with a hint of the sought-after T-cell response. \u201cPeople were excited,\u201d says Gray Heppner, a major in Ballou's department at the time. Heppner, now a colonel at WRAIR, has an encyclopaedic knowledge of the army's malaria programmes and an enthusiasm for sharing that knowledge. Heppner cooked up a protocol to look for responses in Rhesus macaques, and once the difficulties of assessing the monkeys' responses to skin tests in a dimly lit monkey house were sorted out, he had a winner \u2014 an oil-in-water emulsion with monophosphoryl lipid A (MPL) and QS21, an extract from the Chilean soapbark tree  Quillaja saponaria . In 1996 the first human trial with this 'AS02' adjuvant provided protection for an impressive six-out-of-seven individuals 4 . \u201cThat was a huge a-ha moment for us,\u201d says Ballou. As with much of the celebration around RTS,S, though, there was sombre d\u00e9nouement. Six months after the first challenge, five volunteers who had been protected by RTS,S/AS02 took up their ice-cream containers again. This time all but one fell ill 5 . This pattern has, in general, held ever since; the vaccine's protection falls off quite steeply with time. Nevertheless, the results were deemed good enough to take into the field. In the summer of 1998, 250 men in the Gambia received three doses of either RTS,S/AS02 or a rabies vaccine and were followed up for 15 weeks. The unblinding, in which the differences \u2014 if any \u2014 between control and test group are revealed, took place the next year in Rixensart. These unblindings are long-drawn-out processes. Statisticians who know what the data show describe every aspect of the study \u2014 compliance rates, randomization, adverse events, immunogenicity and more \u2014 to the researchers who actually carried out the work but, because of the blinded structure of the study, didn't know what was going on. Sitting in the audience, Ballou was anxious. The results were difficult to interpret. During the surveillance period 81 of 131 men who had been given the RTS,S/AS02 vaccine exhibited measurable levels of parasite in their blood. In the control group 80 out of 119 tested positive 6 . That's 62% against 67%, which is not much of a margin. \u201cPeople really felt that this was way too low to be important,\u201d says Heppner. \u201cBut Dr Ballou said 'Wait a minute. Look, it works well for the first portion of the trial'.\u201d Indeed, during the first nine weeks of surveillance, the vaccine posted an impressive 70% efficacy, but that had tailed off to zero in the remaining six weeks. \u201cIt took some additional investigation and analysis by the statistician there to put that into context of what was happening,\u201d says Ballou. They were dealing with adults of different ages, many of whom had had malaria many times over and had a certain level of natural immunity to the diease. Further confounding things was that as individuals became infected, the pool of people from whom cases could be measured was shrinking.  \n                Booster benefit \n              SmithKline, WRAIR and their collaborators in the Gambia quickly made a decision to follow up with a booster dose for a portion of the men the next summer. That follow-up provided strong signals that the vaccine was acting in two ways; it was protecting against infection, and it was also weakening the symptoms in cases where the infection nevertheless took hold. SmithKline decided that a vaccine that could protect people in these ways when they were most vulnerable could save lives. And the most vulnerable are the young: children under five are thought to account for 75% of malaria mortality in Africa. So SmithKline decided to step down the age of the next trial participants to infancy; if it hadn't, Ballou says, the project would have died. But the company felt that at this stage, it needed help: Ballou remembers St\u00e9phenne telling him and Cohen that to move forward they would need a partner with more money. Ballou wrote a proposal to the William H. Gates Foundation asking for $25 million to help assume some of the risk that SmithKline was taking. In the end the Gates foundation gave $50 million to establish the Malaria Vaccination Initiative (MVI) through PATH, a non-profit organization based in Seattle, Washington. Ballou was asked to lead it, but instead took a job with Washington DC biotechnology firm MedImmune working on other vaccines, and Regina Rabinovitch, who had managed a network of vaccine initiatives at the National Institutes of Health, became the leader. With MVI on board, what was by now GSK began to collaborate with Pedro Alonso, a researcher at Barcelona Centre for International Health Research in Spain, who had developed a field research site in Saude de Manhi\u00e7a, Mozambique, in 1996. In addition to building health and research facilities, making such a site ready requires outreach to thousands of people in surrounding communities, training staff and developing reliable logistics, such as the 'cold chains' required to keep vaccines viable on their way to recipients. Alonso's site would be the setting for the biggest RTS,S trial so far, a study called Malaria 026 that eventually enrolled an unprecedented 2,022 children between the ages of 1 and 5 to test RTS,S/AS02. Melinda Moree, who succeeded Rabinovitch at the head of MVI, says that the study's tremendous scale was undertaken to make it definitive. It would make or break RTS,S. When Ballou rejoined the effort in 2003, having left MedImmune to join the GSK team in Rixensart, Malaria 026 was just ramping up and the mood, he says, was very positive. The unblinding was held in Maputo, the capital of Mozambique, on 9 August. For Ballou, Cohen and other veterans, the lengthy ritual was a gratifying one. In one cohort of 1,600 children, infection in the vaccine group was 37% lower. What's more, the vaccine seemed to reduce severe malaria infections by 65% (although that number would come down with further follow-up data) 7 . Cohen says that the news left \u201cA moment of silence that is very difficult to describe \u2026 I knew there and then that this was going to be a vaccine against malaria.\u201d If the investigators were elated, MVI policy-makers were a little less so. \u201cWe had sort of our go criteria and our no-go criteria. Then there was this whole area called the grey zone. We were hoping the results wouldn't be in the grey zone,\u201d says Moree. But they were. \u201cSo there was some amount of disappointment.\u201d Even so, Moree put more Gates Foundation money into the MVI project for further trials. \u201cI'd make that call again without hesitation,\u201d she says. Several smaller phase II studies ensued, including Malaria 038, a Manhi\u00e7a trial looking at safety and efficacy in infants vaccinated at 10 weeks, 14 weeks, and 18 weeks of age. Its results were published in October 2007. The vaccine was safe and seemed again to have efficacy on the order of 65% in the first 3 months and 35% in the first 6 (ref.  8 ). Across the continent, operations have expanded to encompass several more research sites, all in preparation for the big show, a clinical phase III trial enrolling 16,000 children. MVI has $107 million earmarked for the phase III trials, says Barbara Savarese, a senior programme director there, and intends \u201cto spend every dime of it\u201d. The investment is on a par with practically any modern vaccine initiative, says Ballou (who is in the process of moving again, this time to the Gates Foundation). But doubts persist. In a commentary that accompanied Malaria 038's results in  The Lancet , Judith Epstein of the US Military Malaria Vaccine Program took issue with the degree to which the RTS,S trials rely on what is known as time-to-event analysis rather than the overall number of cases in the two groups 9 . Although such analysis is appropriate, she says, on its own it may not communicate the whole picture to the people who would ultimately use the vaccine. Hoffman is more critical. \u201cIt's not as if it's inappropriate to evaluate a vaccine based on this time-to-event analysis. But the point is, what's the biological meaning of that type of an analysis, and the answer is, we don't know.\u201d Without knowing what exactly is going on, how can researchers explain to potential vaccinees what they should expect?  \n                Better than nothing \n             Dyann Wirth of the Harvard School of Public Health, who is considering a collaboration with RTS,S trial investigators, offers an explanation of what might be going on that draws on analogy to a previous intervention. \u201cThe sporozoite vaccine is like the effect of the bed net. It doesn't completely eliminate infection but it does reduce the onset of disease. And the interpretation of that could be that fewer sporozoites get through and establish infection.\u201d It is possible that these infecting sporozoites are, in a way, extending the work of the vaccine. A child may be inoculated with sporozoites by hungry mosquitoes dozens of times. If the vaccine means fewer of those inoculations lead to disease, even for a little while, the children may be able to build more of their own natural immunity before the vaccine's effects dissipate. They'll still get malaria \u2014 but not as debilitatingly. This line of argument offers some grounds for optimism. Clinical trials of insecticide-treated sleeping nets in the 1980s and 1990s posted modest, sometimes conflicting reductions in clinical malaria. But some areas with expanded programmes have now reported 50% or better reductions in child mortality (see  The big push ). Malaria control is already a multi-factorial undertaking, with spraying, drugs and bed nets playing what seem often to be reinforcing roles. Even a partially effective vaccine could be another useful component to the strategy. It could even, conceivably, be the one that tips it over onto a trajectory leading to the eradication of the disease \u2014 the Gates Foundation's long-term goal. The Malaria Vaccine Technology Roadmap put together in 2006 by a group from the malaria-vaccine community shows how partially effective vaccines are now playing a part in thinking on the disease. The group's goal for 2025 was a vaccine that provided four years of protection from clinical disease to 80% of users \u2014 the sort of target that traditional vaccines aim for, although not a very stringent one. But as a nearer-term goal it saw real advantages in a vaccine offering 50% protection from severe disease for a year. If indications of RTS,S's efficacy against severe malaria hold up, it might deliver that, which would be no small thing. \u201cIf tomorrow we can propose a vaccine in Africa that is going to have 50% efficacy against severe malaria,\u201d says Christian Loucq, the current director of the MVI, \u201cwe have a chance to save 1,000 to 1,500 lives daily.\u201d Adrian Hill at the University of Oxford's Jenner Institute, UK, isn't impressed, though. He is concerned with the reporting from Alonso's group on a second cohort in the Malaria 026 trial. In a 400-subject cohort at a second site with much higher transmission rates, the proportion protected at the end of 6 months seems to be just 11%. Moreover, the data showed no difference in clinical malaria. The investigators on the study maintain that clinical malaria was not a primary endpoint for that part of the study, which was designed to measure the time to the first infection. But Hill still wonders how a phase III roll out will look: \u201cWill it be like cohort 1 or cohort 2?\u201d Hill's doubts about the vaccine bolster a more general frustration with what he sees as GSK's go-it-alone approach, \u201cunconnected to the other 12 or 15 groups developing vaccines\u201d (see  table 1 ). He and others want different vaccines to be combined with RTS,S in Phase II testing, suspecting efficacy might be greatly enhanced. Heppner, for example, says that results of studies he and his colleagues have carried out on macaques indicate that combining an adenovirus-based vaccine made by the Netherlands biotech company CruCell with RTS,S would offer much better effects[10]: \u201cMy hope is that a way can be found to evaluate this clinically just as we've done for earlier improvements of RTS,S.\u201d Ballou says that although progress in studying this combination has been stalled for \u201cvarious business reasons\u201d, several collaborative efforts continue.  \n                Going it alone \n              Although mixed phase II trials are possible at some time, there's every likelihood that Phase IIIs of RTS,S on its own will go forward later this year. GSK is already looking beyond them to potential sales. It has made extensive inroads with government agencies and organizations that may purchase the vaccine for the developing world as those concerned attempt to assess the demand and the price per dose. (An interesting wrinkle here is that RTS,S, being very similar in molecular terms to the hepatitis B vaccine, provides immunity against that, too \u2014 a facet that some health officials say makes it more attractive.) GSK doesn't expect to turn huge profits. It can't. But getting a return on the investment will make it sustainable, potentially leading to better products. Bill Gates recently singled out the company's collaborative efforts as a model example of \u201ccreative capitalism\u201d. Meanwhile, the partnership has been building infrastructure and good will in Africa. At the meeting of site investigators in Brussels, there was a buzz of excitement about the coming trials. Abdulla is one of the many African doctors who will be seeing the RTS,S project through to its completion, some 25 years after Ballou first had a carton of mosquitoes taped to his arm. Abdulla projects a chipper but world-weary wisdom. His laugh comes easily when describing the maddening attitudes of some local politicians or when he is asked how many times he has had malaria himself and realizes he doesn't know \u2014 in the dozens at least. He doesn't worry about the results of phase III. And he doesn't worry that mothers in Africa will feel a false sense of security about their vaccinated children, as Epstein's article suggested 9 . They're more savvy than that. They know that all the various approaches are only partially effective, and that progress means using the right combination. With luck, Phase III trials will show that RTS,S might be part of that combination. What matters most to Abdulla, though, is that even if it isn't, the infrastructure for research is maintained, so that if after all its hard work GSK has to pull out, there is still a trials pipeline down which future candidates may flow. Whatever the phase IIIs of RTS,S show, there will be volunteers such as Ballou taping ice-cream cartons of mosquitoes to their arms for years to come in the expectation of a short, sharp introduction to one of the world's worst killers. And there will be researchers such as Abdulla, for whom repeated infection is a way of life, eager to take the fruits of those volunteers' labours. Creating a lasting link between them may offer as much lasting benefit as any single vaccine. Brendan Maher is a features editor at    Nature.    See Editorial,   page 1030 ,  and online at   http://www.nature.com/news/specials/malaria/index.html  . \n                     Malaria Special \n                   \n                     Nature Outlook Malaria  \n                   \n                     Nature Insight Malaria \n                   \n                     Walter Reed Army Institute of Research \n                   \n                     GSK Biologicals \n                   \n                     Malaria Vaccine Initiative \n                   Reprints and Permissions"},
{"file_id": "451124a", "url": "https://www.nature.com/articles/451124a", "year": 2008, "authors": [{"name": "Ewen Callaway"}], "parsed_as_year": "2006_or_before", "body": "Long dismissed as featureless, disorganized sacks, bacteria are now revealing a multitude of elegant internal structures. Ewen Callaway investigates a new field in cell biology. Nearly a decade ago Jeff Errington, a microbiologist at Newcastle University in England, was toying with a strange bacterial protein known as MreB. Take it away from microbes, and they lose their characteristic cylindrical shape. The protein's obvious role in structure and even its sequence suggested a shared ancestry with actin, a protein that produces vast, fibrous networks in complex cells, forming the framework of their internal structure, or cytoskeleton. But no one had ever seen MreB in action under the microscope until Errington found just the right combination of fluorescent labels and fixatives. In a 2001 paper, he presented MreB (orange in the  illustration ) fluorescing brilliantly and painting barbershop-pole stripes around the rod-shaped bacterium  Bacillus subtilis 1 . ?We got these amazing pictures. It was one of those few times in a scientific career when you do an experiment that completely changes your way of thinking,? says Errington. For more than a century, cell biology had been practised on 'proper' cells ? those of the eukaryotes (a category that includes animals, plants, protists and fungi). The defining characteristic of eukaryotic cells is their galaxy of internal structures: from the pore-studded nucleus that contains the genome, to the fatty sacs of the Golgi, to the myriad mitochondria, and of course the networks of protein highways that ferry things around the cell and give it shape and the capacity for movement. These elements form a catalogue of cell biology's greatest discoveries, and all of them are absent in bacteria. Hundreds to thousands of times smaller than their eukaryotic cousins, and seemingly featureless, bacteria were rarely invited to the cell biology party. But Errington's discovery has been part of a movement that is changing that. Dyche Mullins, a cell biologist at the University of California, San Francisco, had spent most of his career untangling the network of molecular cables and scaffolding that enforces order in the eukaryotic cell. With Errington's paper, Mullins saw the lowly bacterium anew. ?There was a lot of organization in bacterial cells we were just missing,? he says. He has since devoted much of his time to studying them. Last month, Mullins chaired the annual meeting of the American Society for Cell Biology in Washington DC. That he was chosen for the job is a clear indication that bacteria have made it on to the guest list. Lucy Shapiro, a microbiologist at Stanford University in California gave bacteria an hour-long tribute at the meeting. ?People more or less thought the bacterial cell was a swimming pool and the chromosome was this ball of spaghetti,? says Shapiro, whom many credit for launching the field of bacterial cell biology.  \n                External contractors \n              Busied by growth, propagation and little else, a bacterium's life can seem an endless cycle of fecundity. Well-fed bacteria in rich, sterile culture can divide every half hour or so. The cytoskeleton is the linchpin of efficient cell growth and division, but researchers are only starting to explain how. Take FtsZ (illustrated in  yellow ), a protein that ties a belt around the belly of nearly every species of bacterium. Without FtsZ ? a barely recognizable cousin of the eukaryotic protein tubulin ? rod-shaped bacteria called bacilli grow longer and longer without splitting in two. Somehow FtsZ cinches the dividing cell closed, says Harold Erickson, a cell biologist at Duke University Medical Center in Durham, North Carolina. Tubulin is involved in eukaryote cell division, but its role is completely different. Microtubules, formed from tubulin, pull chromosomes apart during cell division through a process that has been studied extensively. Erickson started out studying tubulin. But intrigued by the pictures of internal FtsZ structures coming out of other labs in the 1990s, he began reading up on FtsZ. When the time came to reapply for a grant, he devoted half of his proposal to the bacterial protein. ?I decided, 'I don't have any great ideas about what to do with tubulin',? he recalls. It took a couple of applications to get funding, but Erickson hasn't looked back. Squeezing two cells out of one is just one of the cytoskeleton's duties. When bacteria divide they need to resculpt a rigid cell wall built out of peptidoglycan, a polymer consisting of sugar and amino-acid bricks. Without the MreB protein wound around the shell of a bacillus, it grows spherical (see  'How bacteria get in shape' ). The protein directs the construction and destruction of the cell wall, says Zemer Gitai, a microbiologist at Princeton University in New Jersey. One theory is that MreB and its relatives build a protein scaffold inside the cytoplasm that tells the cell wall's enzyme contractors outside the cytoplasm where to lay new bricks. Because two layers of membrane separate the MreB helix from the cell wall, other proteins must forge the connection, says Gitai. Also, when a bacterium divides, each new cell must have its own DNA. Most of a bacterium's thousand or so genes sit on a long chromosome, but smaller rings of DNA called plasmids also help a cell by supplying antibiotic resistance and other perks. Mullins's lab studies a bacterial version of actin, called ParM, which ensures that as a cell splits in two, each receives a copy of a specific plasmid. Without the protein, many cells will invariably lose the plasmid and the drug resistance it provides. To avoid this fate, a strand of ParM molecules (shown in  green ) latches onto two freshly replicated plasmids ( purple ), like the chain to a pair of handcuffs. The two circles start close to one another, but as more ParM molecules leap onto the chain, the plasmids spread to opposite ends of the cell. Mullins's group found that the ParM chain grows pretty much on its own ? a startling contrast to our own actin, which requires other players to speed extension. Although related to actin, ParM works more like tubulin, constantly reinventing itself by adding and shedding units. ?That blows my mind,? Mullins says. His team is now looking at how other plasmids ensure their legacy, to say nothing of the bacterial chromosome, a DNA loop thousands of times longer than any individual plasmid. ?We know very little. For me, the most important unanswered question in cell biology is how bacteria segregate their chromosomes,? says Mullins. The wealth of questions and dearth of answers makes the field very attractive. Every time a new bacterium is sequenced, researchers have the opportunity to find new structural elements, often with surprising roles. One of the latest additions is an actin protein, MamK, found in bacteria endowed with iron-containing structures called magnetosomes. By sensing Earth's magnetic tug, the bacteria can position themselves in the environment best suited to their needs. For the compass to work, a cell's dozen or so magnetosomes need to line up in a row, and MamK forms their track 2 . Arash Komeili, a microbiologist at the University of California, Berkeley who first identified the protein's role says that by scouring genome databases he has found genes similar to MamK in bacteria with no magnetosomes.  \n                Seeing is believing \n              Although bacterial cell biologists such as Komeili can use genomics to hunt for new features of the cytoskeleton, pictures make a stronger case, he says. Advances in optics and microscopy are one reason the bacterial cell is only now getting its dues. At a few micrometres, bacteria are often not much longer than the limits of a light microscope, so even the best lens in the world won't bring any detail to a molecular cable a few nanometres thick. Peering deeper into a bacterial cell requires abandoning the light waves that obscure detail. Electrons, which have a far shorter wavelength than visible light, provide staggering insights into eukaryotic cell structure, such as the ribosome-studded endoplasmic reticulum or the perfectly arranged bundle of microtubules that build a cilium tail. In bacteria, the same electrons paint a blurry mush. Even the most recent edition of the hallowed text  Molecular Biology of the Cell  sees bacteria under the magnification of an electron microscope as chaotic vessels: ?This cell interior appears as a matrix of varying texture without any obvious organized internal structure,? the authors write. A more promising technology ? cryo-electron tomography ? might be the answer. Instead of coating cells with gold or dousing them in harsh fixatives, cryo-EM, as it is often called, takes pictures of flash-frozen samples. ?We're looking at cells in a nearly native state,? says Grant Jensen, a biologist at California Institute of Technology in Pasadena. The gentle treatment keeps the bacterial cytoskeleton intact. ?If you thawed them out, most of them would probably swim away.? Cryo-EM has the added benefit of allowing researchers to combine numerous angles of a cell into a three-dimensional picture, just like a computed tomography scan does. Recently, Jensen's lab collected images of rings of FtsZ lining the insides of a bacterium called  Caulobacter  and pinching its membrane ? a model predicted by others but never seen before. When early searches for bacterial genes resembling eukaryote scaffold-protein genes found nothing, scientists assumed that these proteins evolved after bacteria split from eukaryotes, some 1.5 billion to 2 billion years ago. The discovery of the bacterial cytoskeleton has turned that conclusion on its head. FtsZ may be the great-grandfather of cell division, says Erickson, whose lab recently showed that the protein makes rings inside microscopic droplets of oil, a stand-in for early life. Although cell division now is an elaborate choreography between dozens of players, the earliest cells may have needed just FtsZ to split in two. Erickson points out that the protein contains none of the amino acids, such as tryptophan and arginine, that some believe only to have shown up later in evolution. As cytoskeletons evolved, they took on new chores and snowballed in complexity. At some stage after eukaryotes branched off from bacteria, the eukaryote cytoskeleton seems to have frozen in time. From yeast through to people, its proteins do many of the same jobs, such as towing sister chromosomes to opposite ends of a dividing cell or making sure the endoplasmic reticulum nestles up against the nucleus. More complex eukaryotes might use actin to flex muscles and keratin to make hair, but those tasks are variations on a theme. Not so with bacteria, says Mullins. Actins that determine cell shape work differently across the bacterial world, and some rod-shaped bacteria, such as tuberculosis, don't even have them. Due to their vast numbers and unicellular lifestyle, ?bacteria can play around with fundamental mechanisms for doing things in a way that eukaryotes can't?, he says. But the shared trait of bacterial and eukaryotic cytoskeleton proteins ? self assembly ? means that bacteria can shed light on the workings of more complex species. For example, the molecular structure of MreB explained how actin molecules stick together. And in most cases, bacterial proteins yield to laboratory tinkering with less resistance than the eukaryotic kind. Turning up the expression of actin, for instance, kills many eukaryotic cells, but bacteria don't seem to mind. And bacteria, because they have few genes, are ideal for addressing fundamental questions about all cellular life. Although cytoskeletons seem to act as organizing centres in bacteria and eukaryotes, no one yet understands how these proteins travel to precise spots in a cell, to one end or the other or to the site where one cell splits in two. As well as being intellectually stimulating, probing the insides of bacteria has practical applications, and bacterial cell biologists recognize the need to remind funding agencies such as the National Institutes of Health of that. For example, a chemical named A22 slows bacterial growth by stopping MreB from forming into long cables, and without FtsZ many bacteria will die. No antibiotics yet target the bacterial cytoskeleton, but with drug resistance on the rise, structures such as the MreB helix and the FtsZ ring could prove to be chinks in the bacterial armour. But as researchers struggle to piece together the bacterial cell, cures for disease are far from the minds of most. For Mullins, the field's progress has vindicated his dive into the bacterial swimming pool, although he and others still haven't come close to its deep end. ?There's a lot of unexplored biology,? he says. Ewen Callaway recently completed an internship at  Nature 's Washington DC office. \n                     Newcastle University's Center for Bacterial Cell Biology \n                   \n                     Dyche Mullins' lab \n                   \n                     American Society for Cell Biology \n                   Reprints and Permissions"},
{"file_id": "451122a", "url": "https://www.nature.com/articles/451122a", "year": 2008, "authors": [{"name": "Mark Schrope"}], "parsed_as_year": "2006_or_before", "body": "A winning combination of isolation, local involvement and a broad ecological remit are making the management of the seas around Colombia's San Andr\u00e9s islands a model for other conservationists, reports Mark Schrope. The locals claim that the waters around the picture-postcard Caribbean island of Old Providence come in seven shades of blue; the surrounding corals come in a far greater range of hues and colours. Just a couple of kilometres offshore and 35 metres down, for instance, you can dive to Turtle Rock, a 12-metre megalith covered in red, yellow and neon green sponges with a 1.5-metre growth of bushy black coral at its peak. The seas of the San Andr\u00e9s archipelago, a part of Colombia about 800 kilometres north west of the country's coast (and in fact much closer to Nicaragua) boast many fine reefs, impressive for both their quantity ? Old Providence has the second largest barrier reef in the Americas ? and their quality. The archipelago's reefs, although not pristine, are some of the healthiest in the Caribbean. And the intriguing mixture of autonomy and local representation that forms the basis of their management may be able to keep them that way.  \n                Valuable lessons \n              In 2000 the San Andr\u00e9s archipelago ? Old Providence (also known by its Spanish name, Providencia), the more heavily populated San Andr\u00e9s and an assortment of other islands, rocks and reefs ? became the centrepiece of Colombia's Seaflower Biosphere Reserve, named after the ship that a group of Puritans used to reach the islands and settle there in 1631. The reserve covers 300,000 square kilometres ? a whopping 10% of the Caribbean Sea. In 2005, a core region of 65,000 square kilometres was designated a Marine Protected Area (MPA), with associated restrictions on fishing, tourism and the like. Marine ecologists widely agree that a global system of such MPAs is the last hope for many reef systems and their associated fisheries, but the step between designating MPAs and making them truly effective, both ecologically and bureaucratically, has been notoriously challenging. Lessons learned in Colombia could eventually affect marine resources elsewhere in the Caribbean and beyond. ?It seems like really exciting things are going on there,? says John Parks of the US National Ocean Service, who has worked on establishing MPAs with groups around the globe. ?The creative thinking that has gone into making this possible is something that other nations would very obviously benefit from tapping into, including the United States.? Two key factors in the MPA's apparent success are a historical lack of strong marine regulations in the area and a significant amount of local autonomy. Following the adoption of its new constitution in 1991, in 1993 Colombia's legislature transferred most of the control of, and responsibility for, environmental resources to 33 so-called regional autonomous corporations. The Corporation for the Sustainable Development of the Archipelago of San Andr\u00e9s, Old Providence and Santa Catalina, known as CORALINA, is responsible for environmental planning, management and research, with a hand in everything from training scuba divers in best practices for protecting the region's reefs to teaching school children the value of the region's resources. Like many remote areas of Colombia, the archipelago suffers from endemic unemployment, so CORALINA is working to develop vocational programmes. Some, but not all these, are tailored to fishermen affected by conservation efforts. One focus is to promote the islands' reefs and underwater cliffs such as Turtle Rock as attractions for scuba divers and other ecotourists. Because development is restricted, the islands have no five-star resorts, and nor will they ? the visitors CORALINA has in mind are interested in a purer remote-island experience, especially on Providence.  \n                Local involvement \n              Unlike most environmental agencies, CORALINA can single-handedly develop and implement management plans that cover both the land and the surrounding seas, and the law gives it the muscle it needs to enforce its regulations. Between 1995 and 1997, the agency issued multiple warnings to two hotels on San Andr\u00e9s, which with 60,000 inhabitants is by far the most populous island in the archipelago. The hotels were failing to comply with new sewage-treatment regulations CORALINA had imposed. When its warnings were ignored, CORALINA shut the hotels for two months. Not surprisingly, the hotels backed down. More surprisingly, perhaps, they were subsequently converted from beaten opponents to willing fellow travellers. ?Today one of these hotels is part of our environmental stars programme,? says Elizabeth Taylor, CORALINA's executive director. ?People have been really changing.? Observers say that one of the most essential elements of the CORALINA model is the mandated involvement of local people. This provides the sort of accountability that avoids regulations and restraints being easily dismissed as the impositions of outsiders. Taylor's position is elected locally, and the agency is overseen not by a government official, but by a board of directors that has to include elected members from local businesses as well as representatives from the national and local governments. At least 3 of the 14 board members have to be from the islands' native population ? descendants of planters and slaves from other Caribbean islands ? which makes up a significant minority on San Andr\u00e9s and an overwhelming majority on the much less populated Old Providence. Each board member's vote has the same weight. As well as playing a specific role in governing CORALINA, locals have been heavily involved in the process of tailoring regulations to specific places. Whereas some environmental measures ? such as a ban on fishing with nets ? apply to the whole of the biosphere reserve, others, such as measures needed to protect an important conch habitat, need to be more specific. These more restricted areas are parts of the MPA set up in 2005, which includes extensive no-fishing zones and even a no-access zone accessible only for research and monitoring. ?They have managed to designate a fairly significant portion of that large archipelago a no-take status, which we think is very important,? says Jack Sobel, with the Washington DC-based Ocean Conservancy and a member of CORALINA's international advisory committee. To delineate the reserve's various restricted zones, CORALINA sponsored hundreds of workshops, with various stakeholders mapping out what they knew about the best and most critical resources, and saying which areas they wanted to see protected. Later work included scientific surveys. Using a geographical information system to manage the various inputs, CORALINA proposed a variety of potential zoning schemes, which were further altered in consultation with locals. Hundreds of stakeholders signed on to the scheme before official enactment by CORALINA, and many locals have embraced the results. According to a survey commissioned by CORALINA, 96% of fishermen say they believe that the MPA will benefit them. Dairo Casanova, owner of a snorkelling and ecotourism centre on San Andr\u00e9s, talks enthusiastically about regulations that blocked spearfishing on some reefs that he depends on for snorkeling tours, turning the activity into an offence that can get its perpetrators a weekend in jail. The support is not universal. Some native islanders have charged that the national government is manipulating CORALINA to prevent proper native representation on the board. But overall, says Taylor, ?I think today we can say we have a lot of support from the local community, especially in San Andr\u00e9s. The people in Old Providence still have some resistance, but not the majority.? Giovanna Pe\u00f1aloza, who manages CORALINA's Old Providence office, says the fishermen there are not concerned so much about closed areas per se, but rather with enforcement. Foreign fishermen, they say, either don't know which areas are closed or don't care. CORALINA recognizes that the ultimate long-term success of the reserve and the MPA will require a number of changes ? especially stepping up enforcement, which is currently managed by Colombia's agricultural ministry. To that end, the group is now finishing a proposal for funds from the Inter-American Development Bank that would enable major improvements to the system. If the funds come through, the group would be able to purchase new boats for enforcement and monitoring work, set up buoy lines around protected zones and establish dedicated MPA offices on the islands instead of handling management out of CORALINA's limited existing facilities. ?So many MPAs in the Caribbean and elsewhere in the developing world have been put in place and end up being paper parks,? says Sobel, ?I think CORALINA is not likely to repeat those mistakes. It's a very talented group of people, and with the amount of planning and outreach they've done, I think they will be effective.? Parks says that as CORALINA expands its efforts, he hopes recognition of their work will also expand. ?We need these stories to be told. We need people to understand that we can do this, and that if we do it the right way, our children will have that future.? Mark Schrope is a Florida-based freelance writer. \n                     CORALINA \n                   \n                     International Coral Reef Initiative \n                   \n                     Thresholds and resilience of Caribbean coral reefs \n                   \n                     Review of ?Coral: a pessimist in paradise? by Steve Jones \n                   Reprints and Permissions"},
{"file_id": "451236a", "url": "https://www.nature.com/articles/451236a", "year": 2008, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "Physicists often borrow techniques from other fields. But how far can this get you? Geoff Brumfiel asks if simple table-top experiments can provide new insights into the early Universe. Take a look at water running in a sink and you'll see an intriguing everyday phenomenon. As water from the faucet strikes the basin, it will create a small saucer of moving water. The water entering this saucer from above flows smoothly and radially out; its even flow creates a ring of ripples which holds the more turbulent water in the rest of the sink at bay. Outside the ring, the water is full of waves and eddies, but on the inside, the water is moving out too fast for the ripples to penetrate ? no information from the rest of the sink can cross into the circle. One of the long term goals of the astronomical community is to produce images of the 'event horizons' that surround black holes ? the ultimate points, or rather surfaces, of no return. Theoretical physicists have spent decades calculating what happens at event horizons, and astronomers now want to spend decades more, and billions of dollars, trying to see what one actually looks like. However, other physicists think that they can get at least some of the answers to that question by studying those rippling fluid rims in the sink. The analogy between sink-saucer and black hole isn't perfect. For one thing, water flows out from the horizon line into the sink, while quite the reverse happens in a black hole. But according to Bill Unruh, a theoretical physicist at the University of British Columbia inVancouver, Canada, it is closer than you might think. In the early 1980s Unruh imagined a similar sort of flow as a thought experiment: a waterfall in which the falling water exceeded the speed at which sound waves could travel in the fluid 1 . In that system it is the point when water reaches the speed of sound that creates an 'event horizon' beyond which sound can never escape. ?If you set up the flow right,? he says, ?you could exactly mimic a black hole.?  \n                Getting the flow right \n              Since that time, a small coterie of physicists has devoted itself to simulations of esoteric phenomena such as black holes and the workings of the early Universe. But before anyone starts to think about saving billions of space-faring dollars with some cleverness in the kitchen sink, there are a few caveats. Getting ?the flow right?, as Unruh puts it, tends to mean using superfluid liquid helium only a fraction of a degree above absolute zero, or some even more esoteric system, such as a set of ultracool trapped atoms in a Bose?Einstein condensate ? another close-to-absolute-zero fluid with quantum properties. Most of the proposed setups haven't even made it off the drawing board; only a handful of experiments have been successfully carried out. And then there's the problem of what, if anything, such models actually tell you. If system B mimics system A in a set number of ways, and goes on to exhibit some other hitherto unlooked for activity, does that mean that system A does the same thing? Or does it mean that the two systems are not that similar after all? Despite these worries, kitchen-sink or table-top cosmology continues to generate excitement among a small but fervent group of physicists, mostly in Europe, where there is a small but steady stream of funding for such research. Much of the work involves superfluid helium, a good medium for studying phase transitions ? transitions from one state to another ? and quantum effects, both subjects of great importance in cosmology. Later this month, those interested in condensed matter and cosmology will gather at the Royal Society in London to discuss the future of their attempts to mimic ? and manipulate ? the otherwise unobservable. ?You're never going to do experiments  in situ, ? says Tanmay Vachaspati, a cosmologist at Case Western Reserve University in Cleveland, Ohio. ?It has to be in a laboratory setting.?  \n                Cosmic inflation \n              The field of condensed matter, which covers everything from waterfalls to semiconductors, has always been a useful source of inspiration for those interested in the origin of the cosmos, according to Paul Steinhardt, a cosmologist at Princeton University in New Jersey. In the mid-1980s, he was working on refining a theory known as cosmic inflation that postulates that the Universe underwent a period of extremely rapid expansion shortly after the Big Bang. The problem at the time, Steinhardt says, is that nobody knew how to explain how the transition from inflation to today's more slowly expanding Universe occurred. The dominant thinking then was that the present day Universe would have begun as bubbles in the inflationary cosmos. But the bubbles, according to calculations, would be nothing but vacuums ? matter and energy would never have developed under such conditions. Steinhardt himself was stuck until he read a description of unusual 'phase transitions' in a mixture of helium isotopes. Normal fluids change their phase ? from gas to liquid, say ? following a bubble regime similar to the one that theorists believed ended inflation. But the mixture of superfluid helium changed its properties in a completely smooth, uniform fashion. Applied to cosmology, the superfluid transition allowed the entire Universe to gently roll from inflation to the present-day conditions, says Steinhardt. Since Steinhardt's work, superfluid helium has emerged as the material of choice in these sorts of experiments. In particular, helium-3, an isotope of helium with two protons and one neutron, has very unusual properties, which make it an unusually good proxy for the cosmos. In addition to exotic phase transitions, helium-3 can undergo the phenomenon of 'symmetry breaking'. Normally, pairs of atoms in the liquid have their spin and orbital angular momentums aligned in random directions. But when cooled, the helium atoms will snap into a single alignment. The process is somewhat like iron filings lining up in a magnetic field, except that the helium arranges itself spontaneously ? creating order from chaos. Physicists believe that symmetry breaking in the early Universe led to the creation of every force except gravity. Taken together, the symmetries and phases of a helium-3 superfluid give the quantum liquid an important Universe-like quality, says Grisha Volovik, a condensed-matter theorist at the Helsinki University of Technology in Finland. ?All the ingredients are certainly there,? he says. So how far can such analogies can be trusted? And what if the cosmological theories being tested are themselves wrong? Around the time at which Steinhardt was refining his inflation theory, a theoretical physicist at Imperial College in London, Tom Kibble, was working on an alternative model. Kibble had a theory that the cooling of the early Universe as it expanded also created massive structural defects ? called cosmic strings ? that were the seeds of the large network of galaxies we see today. Kibble's hypothesis worked perfectly in helium-3, where rapid cooling led to a tangle of 'quantum vortices' that matched his theory. Unfortunately, he says, his cosmic strings theory of galactic structure failed to match up with astronomical observations of the cosmic background radiation left over from the Big Bang. After satellites designed to study the cosmic background delivered their results in the early 1990s, Kibble says: ?It became clear that the predictions of inflation were rather good, and the predictions of cosmic strings were completely wrong.? In other words, laboratory models had verified the theorists' equations, but they had provided absolutely no insight into whether those equations could be applied to the cosmos. That early failure left many experimentalists and theorists sceptical of any bench-top models of the early Universe. ?Frankly,? says Wolfgang Ketterle, a Nobel-Prize-winning condensed-matter physicist at the Massachusetts Institute of Technology in Cambridge, ?I don't think a table-top experiment will answer fundamental questions about the cosmos any time soon.?  \n                Stringing it together \n              Such concerns have not stopped Richard Haley of Lancaster University, UK, from pursuing lab analogues for string theory ? perhaps the most experimentally intractable theory of fundamental physics. String theory is controversial because it has evolved over the past two decades almost without reference to experiments or observations, and so some critics view it more as a branch of mathematics than of physics. Some versions of string theory postulate that our Universe may sit on a three-dimensional membrane, or 'brane', suspended in a higher-dimensional space, the way a two-dimensional sheet of paper sits in the three-dimensional world. In such models, string theory explains the end of the inflationary period through the collision of our brane with another similar brane. If it were true, the brane theory might explain why inflation ended when it did, a question left unanswered by Steinhardt's earlier work. To create colliding branes in the lab, Haley brought two phases of helium-3 together. His team used a magnetic field to create a helium-3 sandwich, with one part of the superfluid, the A-phase, as the filling and the other, the B-phase, as the bread. They then decreased the field strength and watched as the two B-phases collided 2 . Mathematically speaking, Haley says, the phases are good analogies for cosmic branes. In Haley's experiment the colliding phases did not merge smoothly into one uniform B-phase, but instead left behind structural defects ? most likely quantum vortices of the same sort predicted by Kibble. If these swirling vortices have analogies in the Universe, then they should be detectable as massive cosmic strings. Unlike Kibble's original idea, these strings would be a smaller fraction of the Universe's mass, but they should still be detectable by using ground and space-based interferometers to observe gravitational waves. Haley, meanwhile, says that he and his team are now working to further understand the different kinds of vortices created by the collision.  \n                Testing the untestable \n              Of course, cosmologists need to apply caution when interpreting such lab-based results. Steinhardt notes that string branes are flat and attract one another, whereas the helium-3 'branes' are curved and have no attractive force. The model is far from perfect. Still, in a field such as string theory where exotic mathematics reigns supreme, an experiment that makes any testable prediction could have a big impact, says Joe Polchinski, a string theorist at the Kavli Institute for Theoretical Physics in Santa Barbara, California. ?You never know what you might find,? he says. From the experimentalist's perspective, even a failed analogy can find another purpose. The quantum vortices first predicted by Kibble and his colleague Wojciech Zurek, a quantum theorist at the Los Alamos National Laboratory in New Mexico, are now being used to track the movement of helium-3 in other experiments, according to Matti Krusius of Helsinki University of Technology. ?This is a nice phenomenon,? he says. ?We use it to study turbulence.? Such crossover from cosmology into condensed matter is a common and overlooked benefit of these collaborations, says Ralf Sch\u00fctzhold, a quantum theorist at the Technical University of Dresden in Germany. Because the Universe has been expanding since the time of the Big Bang, cosmologists' equations that model this expansion can work well for systems that are changing. That makes them particularly useful for understanding phase transitions and other phenomenon. ?It's very nice to consider effects in condensed matter based on these beautiful equations from cosmology,? he says. Sch\u00fctzhold and his team are now working on a different cosmological analogue that could help to explain the origin of matter and energy in the Universe. Under normal circumstances, atoms are constantly moving, but when a single atom is chilled to near absolute zero, its real motion converts into 'virtual' quantum fluctuations, which are temporary changes in the amount of energy in a small volume of space. Following inflation, cosmologists believe that the Universe underwent the reverse of that process: virtual quantum fluctuations in the vacuum of space became real matter and energy. A laboratory experiment on a single atom, Sch\u00fctzhold says, could allow him and others to see how thermal noise and other real-world effects altered the fluctuations that created the cosmos we see today.  \n                Good vibrations \n              Controlling a single atom is no small task, but Tobias Sch\u00fctz, Sch\u00fctzhold's experimental partner at the Max Planck Institute for Quantum Optics in Garching, Germany, says that he is reasonably confident that it can be made to work. Even if it can't, he says, the project is likely to aid his work in quantum computing. ?We have to work in this direction anyway,? says Sch\u00fctz. That's just as well, because experiments to realize the quantum vibrations of an atom require exquisite control of the laser system used to cool it. ?It is really pushing experimental technique to its limits,? says Ketterle. Basing a career on such analogies would be ?scientific suicide?, he says, especially given their tentative link to actual cosmology. Experiments on the black-hole models that Unruh first described are even further off. Efforts to create a waterfall equivalent in helium-3 have been stymied by fluid turbulence. Other approaches are now in the works: some groups are working with Bose?Einstein condensates 3 , which can be studied at lower flow speeds than helium-3. Other techniques employ a series of light pulses in special fibre-optic cables 4 . Ultimately, a lab analogue that displays quantum behaviour is needed. Such a system could allow experimentalists to observe Hawking radiation ? a quantum-mechanically induced glow that theorists predict exists around the event horizon. The pay-off for theorists in this case promises to be tangible: an observation of Hawking radiation in such a system could inform debate about whether and how black holes 'evaporate' over time. So despite nearly two decades of waiting for his black-hole analogy to reach fruition, Unruh's enthusiasm for the project remains undimmed. ?It's a really neat idea and it would be great if it works,? he says, then adds: ?I'm astonished every time I see what these experimentalists can do.?\n Geoff Brumfiel is a senior reporter for  Nature  based in London. \n                     Ralf Sch\u00fctzhold's research \n                   \n                     More on superfluid helium-3 \n                   \n                     Helsinki University Low Temperature Laboratory \n                   Reprints and Permissions"},
{"file_id": "451240a", "url": "https://www.nature.com/articles/451240a", "year": 2008, "authors": [{"name": "Brendan Borrell"}], "parsed_as_year": "2006_or_before", "body": "A German physicist and a hedge-fund magnate are competing to push protein simulations into the realm of the millisecond. Brendan Borrell finds out what is at stake. For a while, Klaus Schulten did not mind the Godiva chocolates arriving in his team's mailboxes at the University of Illinois in Urbana-Champaign. Nor was Schulten, whose biophysics group boasted one of the fastest algorithms for simulating protein structures, much concerned when his programmers received e-mails heralding a job opportunity at an undisclosed Manhattan firm that aimed to ?fundamentally transform the process of drug discovery?. It was early 2004, and Schulten's 40-strong group was attracting close to $2 million a year in grant money. Nearly 20,000 users had downloaded his software, called NAMD for Nanoscale Molecular Dynamics, for use on computers running hundreds of parallel microprocessors to simulate how individual atoms behave in proteins and other large molecules. Schulten's group itself was working on a million-atom model of the satellite tobacco mosaic virus, which the researchers called ?the first all-atom simulation of an entire life form? 1 . But the German-born physicist got his wake-up call in 2006, when he saw a table of computing benchmarks in a report from that year's supercomputing conference in Tampa, Florida. A new program called Desmond, he saw, could calculate each step of a standard molecular-dynamics simulation ? the 23,558 atoms in a system involving the protein dihydrofolate reductase ? in a little over a thousandth of a second. NAMD was ten times slower. ?Suddenly,? Schulten says, ?we were not the best anymore.? The title had passed to the sender of the chocolates ? David Shaw, a hedge-fund magnate and computer expert who taught himself physical chemistry. Over the previous few years, he had recruited more than 50 scientists and engineers, including three former students from Schulten's group, and put them to work in his midtown Manhattan high-rise. In the paper from the supercomputing conference, Shaw's team wrote that Desmond ?is faster than NAMD at all levels of parallelism examined? 2 . And the group noted that on one simulation Desmond ran faster on 1,024 processors than NAMD ran on the 16,384 processors of IBM's Blue Gene/L ? the world's fastest supercomputer. The numbers shocked Schulten, who believed his team was on course to simulate molecular dynamics on the scale of milliseconds ? longer than anyone had previously achieved. Even with cutting-edge programs such as Desmond and NAMD, scientists have been able to glimpse only the fastest-folding proteins, such as the villin headpiece, which folds in about 10 microseconds. The number of possible configurations of atoms in larger molecules, over time and in three dimensions, is astronomical. If these kinds of simulation could be sped up 1,000-fold, which even then could take a month of computing time, the pay-off could be high. They might, for instance, reveal binding sites for new drugs to tackle a wide range of medical problems. Shaw and Schulten are now spending millions of dollars each to break the millisecond barrier. But some in the field aren't sure what the all-out push will come to. As Ross Walker, a computational biologist at the San Diego Supercomputing Center in California, puts it: ?A lot of what they are going to see are limitations on the underlying computational models.?  \n                Pushing the envelope \n              To make molecular-dynamics simulations feasible with today's computers, scientists have had to make a number of simplifying assumptions. Typical simulations calculate the forces acting on each atom from a century's worth of chemistry experiments on organic molecules much smaller than the proteins scientists wish to simulate. The simulated molecules are also pegged together like Tinkertoys; they can change shape during the simulation, but cannot react to form new molecules. The first software that sought to capture this world was developed at Harvard University in the late 1970s. In a paper in  Nature , a team led by Martin Karplus published its 458-atom simulation of a tiny protein on an IBM 370, a top-of-the-line supercomputer 3 . Today, development teams around the world continue to work on CHARMM, or Chemistry at Harvard Molecular Mechanics, even as other algorithms such as NAMD have risen to compete with it. One of the biggest factors limiting the development of molecular dynamics has always been computational power ? which is where Shaw comes in. Having stepped back from running his hedge fund around 2001 (see 'From science to finance and back again'), Shaw, who is also an adjunct professor of biomedical informatics at Columbia University in New York, returned to his first enthusiasm ? the architecture of massively parallel supercomputers. Predicting the motions of large systems of atoms requires finding the best way to communicate particle positions and forces among multiple processors. And on a scorching afternoon in June 2003, Shaw holed himself up at a friend's house and found a way to speed things up. In traditional parallel approaches, each processor calculates forces to update the position of all the particles in its own small box of simulated space. But to do so, it must import positional data from neighbouring boxes within a certain radius. Shaw's strategy, implemented in Desmond, changes the geometry of this import region from a hemisphere to a semicircular plate and a rectangular tower. As the number of processors available to Desmond grows, the volume of this import region shrinks more quickly than in the approaches used by NAMD and CHARMM. In one of the first studies to use Desmond, this speed-up gave Shaw and his collaborators an unprecedented view of the workings of an ion transporter that the bacterium  Escherichia coli  uses to maintain its salt and pH balance 4 . But Shaw knew that software alone could not obtain millisecond-long molecular simulations. His plan has been to build a supercomputer so dumb, he says, that it can do nothing except molecular dynamics. ?But,? he beams, ?it's really fast at that.? He calls it a computational microscope and has named it Anton, after Anton von Leeuwenhoek, the seventeenth-century Dutch scientist and builder of microscopes. The first segment of Anton is due to arrive in Shaw's lab at the end of the year.  \n                Need for speed \n              Anton uses a high-speed task pipeline to accelerate the most computationally intensive tasks of molecular dynamics ? modelling certain long-range interactions among atoms. But the chip does not have the ability to speed up software-based operations to the same extent, and the hard-wired pipeline may not be flexible enough to efficiently incorporate advances in the field. ?At this point, though, we placed our bets,? Shaw says. When Shaw began the work, he estimated that Anton would run molecular-dynamics simulations 1,000 times faster than previous parallel supercomputers. In recent months, he has stopped presenting the 1,000-fold estimate in talks, although he still believes Anton will run more than 100 times faster than today's machines. But with general-purpose hardware doubling in speed about every two years, many wonder how long Anton might maintain a lead. ?If you are a little bit of a sceptic,? says Schulten, ?you would say it is another attempt for a special-purpose processor that will be overrun by market forces.? The field is littered with what Gregory Voth, a computational chemist at the University of Utah in Salt Lake City, calls ?dead bodies?. In 1984, the late biochemist Cyrus Levinthal designed a molecular-dynamics computer called FASTRUN, but it took his group six years to get it running. During the past ten years, IBM and RIKEN, Japan's main research institute, have collaborated on several generations of chips intended for molecular-dynamics simulations, called MD-GRAPE, without producing any major breakthroughs in the field. At the National Institutes of Health in Bethesda, Maryland, in the late 1980s, Bernard Brooks abandoned his effort, dubbed Gemmstar, when Hewlett-Packard announced its blazingly fast 9000 series ? which could be had for as little as $12,000. Scientists are racing not just against each other, but against Silicon Valley. Schulten has played that game before. In Munich in the late 1980s, he built his own parallel supercomputer out of 60 processors mail-ordered from England. He carried his computer in a backpack to his new laboratory in Illinois, where he ran a 30,000-atom simulation of the bacteriorhodopsin protein, which drives the photosynthetic reaction that turns light into an electric charge. His simulation lasted 263 picoseconds ? less than a millionth of a millisecond ? and required more than two years of continuous computation 5 . By then, his machine was obsolete.  \n                Thinking big \n              In the past 15 years, Schulten's ambitions have grown: from 100,000 atoms in 1999, to 300,000 in 2003, and culminating with his million-atom simulation of the tobacco mosaic virus published in 2006. To match his models, Schulten developed software that could scale with advances in parallel computers, something CHARMM could not do at the time. Chemist Richard Hilderbrandt, who supported the early development of NAMD at the computing directorate of the US National Science Foundation, says that the idea ?was to take a large molecule and break it up into patches to distribute to processors. It was quite a bold step?. The drawback of Schulten's strategy was that it could not simulate the behaviour of smaller molecules significantly faster than it could large ones. ?If you have a protein of 500 atoms,? he says, ?it's very difficult to put it on a parallel computer with 5,000 processors.? Schulten emphasizes that his publicly funded group had to focus on ensuring that NAMD, which is freely distributed, would run on a wide range of platforms. Shaw's team, in contrast, could tune Desmond for its state-of-the-art computing cluster, about a year before similar clusters were available at National Science Foundation computing centres. Shaw says that profits are a long way off, and that he is working to share his team's technology as much as possible. But his proprietary algorithm will ultimately be sold to industry through an agreement with Schr\u00fcdinger, a biotechnology company founded by chemist Richard Friesner, a colleague of Shaw's at Columbia. Schulten had only inklings of Shaw's ambitions when he gave a seminar at D. E. Shaw Research in October 2004. ?At that time it was clear that there was a competition,? he says, ?but in a very civilized way.? Even so, he says, ?I wouldn't have told them about a great solution I had developed, and they wouldn't tell me their solution.? Although Schulten's software has been a boon to many researchers, with a development cost of $20 million it might also be considered a drain on their resource pool. Some scientists contend that the pursuit of speed has hindered alternative modes of inquiry. ?I think it's unfortunate that some of the researchers who use more established codes with a broader range of functionality are not getting the same access to national resources,? says computational chemist Charles Brooks of the Scripps Research Institute in La Jolla, California.  \n                Tough decisions \n              Some participants at a 2001 supercomputing conference recall Hilderbrandt telling the audience that users should switch from older programs such as CHARMM to modern parallelized packages, such as NAMD. Hilderbrandt, who is now at the Department of Energy, does not recall being so specific, but says he still believes NAMD is ?the program of choice? for most applications. Michael Crowley at the National Renewable Energy Laboratory in Golden, Colorado, doesn't buy that. He uses CHARMM to study biofuels and says: ?CHARMM has functionality that as far as I know, no other program comes near.? He says that when he has applied for supercomputing time from allocating agencies, ?you can almost expect that somebody is going to suggest you use NAMD?. There are deeper questions about the pursuit of ever-longer timescales. ?It's clear to me that what's emerging out of both Schulten's and Shaw's efforts are technological advances that are going to affect the entire community,? says Brooks. ?But whether an individual achievement of a millisecond timescale for any particular simulation is of great significance, I'm not entirely sure.? Vijay Pande, at Stanford University in California, has pioneered the folding@home distributed-computing project, which uses the personal computers and Sony PlayStations of more than 250,000 volunteers to study protein folding. ?The revolution that's going on,? he says, ?is people are now treating molecular dynamics in a much more sophisticated way, where they are running hundreds or thousands or millions of simulations and then data-mining those simulations.? Because a simulation may take a slightly different course each time, he notes, a single long simulation cannot provide the statistical information that must be gathered over many runs, such as the affinities for binding to a drug. Schulten and Shaw may also be pushing current models to their breaking point. Neither group is investing significant resources in improving fixed-charge force fields, which might turn out not to be accurate enough for lengthy simulations. For instance, when two atoms approach one another, the electron orbits of one can get sucked towards the positive charge generated by the other. This phenomenon, called polarizability, is cumbersome to model and slow to compute. Shaw estimates that it would slow down computation by roughly a factor of ten; Schulten thinks it may be only a factor of two. Yet these difficulties may be a reason for moving forward, not calling a halt. Longer simulations can show where the models are failing, and they can guide the distributed-computing approach. Shaw believes his group can make a meaningful contribution to the field, but he is well aware of the problems ahead. ?If you have something you're sure is going to work,? he says, ?you're not being ambitious enough.? Last year, Schulten's group started running a new version of NAMD that can handle smaller molecules faster. His team has also started programming the graphics accelerator chips prized by PC gamers ? an economical solution to the hardware problem that could further shrink Anton's expected lead. And, now that the team is up to speed with the University of Illinois's cluster, Abe, it has tailored a special version of NAMD to compete on equal terms with Desmond. Two months ago, Schulten was delighted to tell Shaw about a simulation of a 38,000-atom protein, in which NAMD had set a new personal best, computing a 0.1-microsecond simulation in the course of a day. ?We agreed, now the programs are pretty equal,? says Schulten. And for his part, Shaw may be starting to concede that each algorithm has its benefits. ?Schulten has made extraordinary strides in his NAMD code,? he says, ?so it's not obvious to me that Desmond will be significantly faster for all applications.?\n Brendan Borrell is a freelance science writer in New York City. \n                     Klaus Schulten \n                   \n                     D. E. Shaw Research \n                   Reprints and Permissions"}
]