[
{"file_id": "531127a", "url": "https://www.nature.com/articles/531127a", "year": 2016, "authors": [{"name": "Jeffrey M. Perkel"}], "parsed_as_year": "2006_or_before", "body": "A peer-to-peer website aims to disrupt the author-services industry. As Sebastian Eggert prepared to submit a conference article, he realized he had a problem: neither he nor his research adviser were native English speakers, and neither had much experience in writing and publishing research papers. But Eggert, a master's student in mechanical engineering at the Technical University of Munich in Germany, had heard of a website where he could purchase editing services from an expert: an online marketplace called  Peerwith . Launched in October 2015 and still in beta testing, Peerwith is a forum through which researchers can find and negotiate with service providers such as editors, translators, statisticians and illustrators to improve their research papers. The site boasts \u201chundreds of experts\u201d, most of them with expertise in the social sciences and humanities. Users post a job request detailing the subject area of the document, its length and the desired turnaround time. Experts then bid for the job, and both experts and users rate each other afterwards. Peerwith's business model is akin to freelance marketplaces such as  Upwork , says co-founder Joris van Rossum, who left the journal publisher Elsevier to start his firm, except with a strictly academic focus. A market for author services on research papers already exists; van Rossum estimates it at hundreds of million of dollars annually. It includes both large editing companies such as  American Journal Experts  (AJE),  Edanz ,  Editage  and  Macmillan Science Communication  (MSC, which is owned by  Nature 's parent company), and freelancers. But a peer-to-peer online marketplace, van Rossum says, makes services more affordable by cutting out the middleman and efficiently matching buyers and sellers. (Peerwith receives a cut of 10\u201320% for each transaction; the other firms would not comment on their margins). At the site, authors can review the experts who bid for work to identify the best fit, and can check to see how others have rated them. Val Kidd, an editor and translator based in the United Kingdom, earned \u20ac200 (US$223) on Peerwith to translate a presentation for Emanuel Rutten, a philosopher at the Free University, Amsterdam, in the Netherlands. The process, from job posting to completed document, took less than two weeks, Rutten says. \u201cIt's really smooth.\u201d For her part, Kidd says that the interaction with her client improved the final product. At most author-services companies Kidd works with, she says, editors and translators cannot contact the author should they have questions \u2014 the client interacts with the service, which identifies a freelancer to handle the job. Peerwith doesn't vet its service providers, says Anna Sharman, founder of Cofactor, a London-based author-services consultancy. So, unlike her own and other such companies, there is no guarantee that the 'experts' really are qualified. Editors at Cofactor undergo a rigorous recruitment process, Sharman says, and she double-checks their work before it is returned to the client. Sharman says that she could see Peerwith as a marketing channel for her business, but is concerned that it may foster a \u201crace to the bottom\u201d in pricing. She says that when she created an account, the only request she saw was from someone who wanted a 5,000-word article edited for US$9, \u201ca ridiculously small amount\u201d. Sharman charges \u00a360 ($87) per 1,000 words at Cofactor. At Editage, a 6,000-word article with 1-week turnaround costs $350 at the company's 'premium' price, and AJE charges $594. And for 'extensive' scientific editing at MSC (by a panel of at least four editors with experience at high-impact journals), a typical 5,000-word article with a 17-day turnaround costs $2,860. Peerwith is still getting up to speed, van Rossum says. But ultimately, a community-based marketplace could succeed \u201cif there's the right balance of price and quality\u201d, says Deni Auclair, a lead analyst for the media, information and technology consulting firm Outsell, headquartered in Burlingame, California. The larger editorial service providers might be left to target institutions more than individuals, she suggests. As for Eggert, he received one bid to his job posting, and paid \u20ac100 for style and content edits to his 2,500-word paper, which he negotiated down from \u20ac120. He says he would use the service again, and recommend it to others \u2014 assuming the price is right. \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     Does it take too long to publish research? 2016-Feb-10 \n                   \n                     Rate that journal 2015-Mar-30 \n                   \n                     Publishing: Foreign tongues 2012-Jul-04 \n                   \n                     Nature  special: The future of publishing \n                   \n                     Peerwith \n                   \n                     Cofactor Ltd. \n                   \n                     Macmillan Science Communication Scientific Editing \n                   \n                     Edanz Editing \n                   \n                     American Journal Experts \n                   \n                     Editage \n                   Reprints and Permissions"},
{"file_id": "536113a", "url": "https://www.nature.com/articles/536113a", "year": 2016, "authors": [{"name": "Jeffrey Perkel"}], "parsed_as_year": "2006_or_before", "body": "Increasingly popular social-media tool says it can maximize reach and impact of research. Few people have heard of Michele Tobias's research field \u2014 and even fewer study it. Tobias is an environmental scientist at California State Parks in Sacramento, but in her spare time she is a biogeomorphologist \u2014 someone who investigates the effects of organisms on landscapes. In her case, she studies how plants shape the development of sand dunes on the California coast. \u201cI'm pretty much, as far as I know, the only person who has been studying this phenomenon,\u201d she says. But Tobias thinks those who make decisions about California's coastal management should care about her work. She has become adept at pushing her message on social media to draw as many eyes to her papers as possible. \u201cBeing someone who's fairly young in the field of scientific publication, I need to spread out and find an audience. I can't just publish something and expect it to go somewhere,\u201d she says. It's not the job of researchers to become experts in public relations \u2014 that's why universities have press offices, says Matt Shipman, research communications lead at North Carolina State University in Raleigh. But he recommends scientists toot their own horns as well. Increasingly, researchers across the scientific spectrum are coming to the same conclusion. That demand has led to the emergence of an online tool for managing the practice: a free site called  Kudos , which aims to help researchers maximize the reach and impact of their papers on social media, and measure the effects of their efforts. Tobias is a fan, and so are many others. Since its launch in May 2014, the website's user base has grown to more than 100,000 users with around 4,000 sign-ups each month, says co-founder Charlie Rapple, who \u2014 like most of Kudos's staff \u2014 is based in Oxford, UK. \n               Reaching a wider audience \n             \u201cWith so much more research being undertaken and published, the current system of dissemination can no longer guarantee that your work will find its audience,\u201d says Rapple. Kudos, she says, aims \u201cto make research more discoverable\u201d and to \u201chelp researchers get more credit for what they do and achieve more with their work\u201d. Kudos gives each research paper its own profile page, which users can flesh out with a plain-language summary, external resources, reviews, presentations and more. Tobias says that more than 830 individuals have viewed a summary of one of her papers on the site. She also  linked one of her studies  to a YouTube video in which members of her contemporary dance company represented the role of plants in sand dune formation through improvisational dance. \u201cIt's kind of an interesting experiment, but it's related to the paper so I linked it in the resources section.\u201d Another Kudos user, Antony Williams of the National Center for Computational Toxicology in Research Triangle Park, North Carolina, likes that the site provides a forum for linking to post-publication commentary and related articles \u2014 which he calls \u201cforward citation\u201d. Users also create their own overall profile page, which can automatically pull in data from research-profiling services such as ORCID. This material isn't only visible on Kudos's site: it can also appear in a small box next to a paper on external sites thanks to 'widgets' that Kudos provides to third parties such as research publishers. The International Union of Crystallography, for instance,  displays Kudos profiles next to papers  in the structural-biology journal  Acta Crystallographica Section D . To help researchers promote their work to others, Kudos also provides its users with trackable URLs, which they can send out by e-mail, Twitter, Facebook or other social-media platforms. Kudos tallies the resulting clicks \u2014 allowing researchers, as well as institutional press offices and funding agencies, to look at which outlets produce the best return on investment, says Rapple. The site also aggregates data from Altmetric.com, which finds mentions of papers on mainstream and social media, as well as citation figures from Thomson Reuters and article-download counts from publishers. Some of the services Kudos provides are available elsewhere, notes Greg Tananbaum, who owns the California-based ScholarNext consultancy and focuses on scholarly communication and academic technology issues. But integrating them into one site is unique, he says. In particular, Kudos makes it easier for mid-to-late-career academics, who often are wary of social media, to engage on those platforms and measure the impact of that activity. \u201cCreating a mechanism that makes it easier to onboard them into that world, is novel,\u201d he says. The site is free for academics because scholarly institutions, societies, publishers and other commercial clients pay for its upkeep. Kudos helps these customers to track and evaluate their researchers (or, in the case of publishers, their authors) and foster a stronger relationship with them, explains Rapple. By encouraging researchers to do outreach, the site also indirectly builds the profile of their institution or their journal, she adds. And Rapple hopes that publishers and institutions can build up valuable intelligence from the Kudos database about the effects of different kinds of outreach. The site has established partnerships with some 65 publishers so far, including well-known firms such as Wiley and Taylor & Francis. \n               Papers in plain language \n             Kudos's support for displaying plain-language summaries, which researchers can write to make their articles accessible to a wider audience, is particularly useful, Tananbaum says. The family members of someone with cancer, for example, may be better able to understand the context and significance of a medical study from a simple description than from the published abstract, he says. Tobias, similarly, says she hopes that the summary she has written for one of her papers will bring it to the attention of policymakers and resource managers. But even researchers within one's immediate field can benefit from such summaries, says Matthew Bowler, a scientist at the European Molecular Biology Laboratory in Grenoble, France, and a Kudos user. The plain language makes it easier for them to understand a study's context and significance, and to find research papers through general keyword searches. That said, it takes time and dedication to make such resources pay off, Shipman says. It isn't easy to translate research findings into something accessible to other disciplines, let alone to the general public. \u201cHaving access to a platform like Kudos is great in that it gives a place for your research story to live. But it does not make you a better story teller, and it does not help you reach and find an audience for that story,\u201d he says. A poorly worded plain-language summary would be little better than the article abstract. Some research journals already provide plain-language summaries for non-specialist readers, but these are often written by editors, not the authors themselves.  Nature  is one, and in June it started a trial of author-written summaries (although they are still edited by journal staff before publication). Over June and July, the journal released 500-word summaries of 12 previously published articles, says  Nature  editorial director Ritu Dhand. Meanwhile,  eLife , which hires writers to produce plain-language summaries, as well as writing some in-house, posts around 10% of its 'digests' on the popular blogging platform Medium to broaden the journal's audience, says Stuart King, associate features editor at  eLife . \n               Personal decision \n             Each researcher must make a personal choice about how much time to spend promoting their work on social media. But judicious use of self-promotion, says Shipman, leads to visibility, which in turn can lead to increased citations and attract talented graduate students and postdocs to the lab. Yet scientists cannot simply flit in and out of the social-media landscape and hope to make a significant impact, Shipman adds. \u201cLike any other relationship, it takes time and effort to  build and sustain an online network .\u201d For researchers who might feel daunted by the breadth of the social-media landscape, Williams, who has given seminars in the United States and internationally on social media for scientists, offers simple advice: choose two or three social-media platforms, invest the time to get them set up, and then spend perhaps two hours a month keeping them current. If nothing else, he says, build a LinkedIn profile as an online CV, claim and update an  ORCID ID , and log peer-review activities on  Publons.com . That said, a research paper is itself the end product of an extraordinary investment of time and energy. It takes thousands of hours of research, data analysis, writing and peer review, he says. \u201cShouldn't you put at least 10 to 20 hours of work into making sure that you can get the message out to relevant people?\u201d \n                     Science and sexism: In the eye of the Twitterstorm 2015-Nov-11 \n                   \n                     Social media: A network boost 2015-Feb-11 \n                   \n                     Online collaboration: Scientists and the social network 2014-Aug-13 \n                   \n                     Nature Toolbox \n                   \n                     Kudos \n                   Reprints and Permissions"},
{"file_id": "nature.2016.19542", "url": "https://www.nature.com/articles/nature.2016.19542", "year": 2016, "authors": [{"name": "Dalmeet Singh Chawla"}], "parsed_as_year": "2006_or_before", "body": "Open Data Button launched to encourage public sharing of data sets. A  free web-based tool  that promises to help its users ask authors of research papers to publicly share their data \u2014 and to make such requests publicly trackable \u2014 launched in beta version on 7 March. The Open Data Button \u2014 a downloadable web-browser extension \u2014 can be clicked when a reader is looking at a research paper and wants to see its underlying data, says Joseph McArthur, who is co-leading the project and is assistant director of the policy advocacy group The Right to Research Coalition (R2RC) in London. The button is currently available only for Google Chrome users. When clicked, the button generates a template e-mail which the user can send to the paper\u2019s authors. It asks them to share the data supporting the paper, explains how to do so and \u2014 if the user has typed in the information \u2014 states why the data would be useful. All requests are simultaneously publicly posted on the  Open Data Button  website, where anyone can comment on existing entries to note that they want access to the same data sets. If the author replies \u2014 either with a weblink or attached data file \u2014 \u201cwe ask our users to affirm that this is the data they wanted\u201d, says McArthur. If so, the button e-mails a digital badge to the author as a reward for his or her commitment to data sharing. The badge is also hosted on the Open Data Button site\u2019s  \u2018requests\u2019 page . And, when a request is made using the button, past requests are trawled so that authors don\u2019t get repeatedly asked for data sets that they have already provided. The tool will chase authors once a week for the first four weeks after a request is filed, says McArthur, after which entries will be marked as \"failed\". \n               Making open the norm \n             The project is mostly funded by a US$25,000 grant from the non-profit Centre for Open Science in Charlottesville, Virginia \u2014 which has also promised to host any data files sent in reply by authors on its own  Open Science Framework  repository. It is also part-funded by the  Open Society Foundations  in New York, and is supported by a group of volunteers who \u2014 at least in the tool\u2019s early stages \u2014 will moderate e-mail requests. The aim is to encourage open data sharing, which is still  far from the norm in research , McArthur notes, even though many journals are now asking authors to publish their data alongside their research papers. \u201cIt is good to draw attention to an important problem such as data availability with a one-click gadget,\u201d says Bernd Pulverer, chief editor of  The EMBO Journal  in Heidelberg, Germany. But a button by itself \u201cis probably not quite going to cause the revolution\u201d, he says. There is still a reluctancy \u2014 especially amongst biologists \u2014 to share their data openly, because of fears of being scooped by competition, and because of the extra work required in making data sets open, he notes. The beta version of the tool has limitations: users have to manually enter the author\u2019s e-mail address when sending requests, and the application doesn\u2019t have any way of checking whether requested data sets are already freely available somewhere else. But McArthur hopes to automate these functions and to extend the tool to Mozilla Firefox and other web browsers. \n               Way to go \n             The Open Data Button is an extension of a related project, the  Open Access Button , which invited users to click a button if they could not find freely available versions of papers behind pay walls. For the moment, that tool \u2014 which  launched in October 2014  and has around 5,000 users \u2014 simply trawls through public repositories when clicked. Next month, McArthur says that it will allow users to e-mail a paper\u2019s author, as with the Open Data Button. Ultimately, McArthur hopes that neither button will be needed as it becomes the norm to share data openly and make versions of papers open to read. But progress on both fronts could be slow \u2014 so McArthur thinks that the tools can expect a long and useful lifespan. \n                     Benefits of sharing 2016-Feb-10 \n                   \n                     Data sharing: An open mind on open data 2016-Jan-06 \n                   \n                     Digital badges aim to clear up politics of authorship 2015-Sep-28 \n                   \n                     Confusion over publisher\u2019s pioneering open-data rules 2014-Nov-26 \n                   \n                     Scientists losing data at a rapid rate 2013-Dec-19 \n                   \n                     Data-sharing: Everything on display 2013-Aug-07 \n                   \n                     Open Data Button \n                   Reprints and Permissions"},
{"file_id": "534139a", "url": "https://www.nature.com/articles/534139a", "year": 2016, "authors": [{"name": "Mark Wolverton"}], "parsed_as_year": "2006_or_before", "body": "Archivists are borrowing and adapting techniques used in criminal investigations to access data and files created in now-obsolete systems. When archivists at California's Stanford University received the collected papers of  the late palaeontologist Stephen Jay Gould  in 2004, they knew right away they had a problem. Many of the 'papers' were actually on computer disks of various kinds, in the form of 52 megabytes of data spread across more than 1,100 files \u2014 all from long-outdated systems. \u201cIt was a large collection, as you can imagine,\u201d says Michael Olson, service manager for the Born Digital/Forensics Lab at Stanford University Libraries. \u201cHe used a lot of early word processing for his writing, lots of disks and diskettes in different formats.\u201d After considerable effort the Stanford archivists did get Gould's papers into order \u2014 first by finding hardware that could read the obsolete disks, and then by deciphering what they found there. \u201cWe had some challenges finding old applications to figure out what word processor he used, that sort of thing,\u201d says Olson. The Gould papers were an early indication of an issue that's been rapidly worsening: four decades after the personal-computer revolution brought word processing and number crunching to the desktop, the first generation of early adopters is retiring or dying. So how do archivists recover and preserve what's left behind? \u201cPeople around the world have information stored on disks that are less readable with every passing day,\u201d says Christopher Lee, a researcher in the School of Information and Library Science at the University of North Carolina (UNC) in Chapel Hill. \u201cThis includes floppies, Zip disks, CDs, DVDs, flash drives, hard drives and a variety of other media.\u201d Many files can be accessed only with long-obsolete hardware, and all are subject to physical deterioration that will ultimately make them unreadable by any means. By now, many libraries, archives and museums have accumulated shelves full of such material, stashed away in the hope that if it's ever needed, somebody, somewhere will be able to figure out how to access it. \n               Digital inspiration \n             Increasingly, archivists are finding inspiration in the field of digital forensics: the art of extracting evidence about illicit activity from computer drives, smartphones, tablets or even GPS devices. \u201cIt turned out that law-enforcement and computer-security people were dealing with essentially the same problems of stabilizing and recovering data from digital media,\u201d says Matthew Kirschenbaum at the University of Maryland in College Park. And many of their solutions were directly applicable to the archivists' needs. In law enforcement, for example, a top priority is to preserve material in its original form. This is often harder than it sounds: almost anything done on a computer, even something as innocuous as plugging in a USB drive, leaves a faint digital trace. So digital-forensics practitioners have developed techniques for creating an artefact-free 'disk image' that duplicates everything, down to the unused and hidden disk space. They can then preserve the integrity of the original for evidentiary purposes in court while doing all their forensic analysis on a perfect copy. Institutions working to decipher collections have the same need, although in their case, the object is to maintain the provenance of the original for future researchers. Creating forensic copies of the data was a relatively fringe idea 8 or 10 years ago, Lee says. \u201cIt's now quite common in library and archive settings.\u201d Unfortunately for archivists, however, disk imaging is usually done through commercial software packages such as the  Forensic Toolkit  made by Access Data in Lindon, Utah, or by  EnCase , which is developed by Guidance Software in Pasadena, California. Because these packages are designed for criminal investigators, they include tools for file carving (assembling complete files from fragmentary data); cracking passwords; accessing encrypted files; advanced searching; and generating reports for use in court \u2014 tasks that tend to be less important for archival purposes. These packages also come with licensing costs in the thousands of dollars, which would strain the budget of many collecting institutions. So in 2011, Lee and his colleagues launched  BitCurator , a platform designed for the archival field, with funding from the Andrew W. Mellon Foundation, and with continued support from a consortium that currently encompasses 25 member institutions, including Harvard University, the Massachusetts Institute of Technology, Stanford University, Emory University and the British Library. BitCurator has the advantage of being open source and freely available for download ( wiki.bitcurator.net ). \u201cIt's a combination of third party open-source tools and our own work,\u201d says Kam Woods, a research scientist at UNC's School of Information and Library Science and co-principal investigator with Lee on the project. On the basis of the turnout at training sessions and other BitCurator events, Lee estimates that several dozen institutions now use the package actively, and several hundred more use it at least occasionally. BitCurator not only handles disk imaging, but a number of other issues that criminal investigators don't have to worry about. One example is redaction: editing out confidential material before publication. That's an alien concept in the criminal investigations, says Olson. \u201cWhy would you ever want to redact evidence from a case? But from an archival or library standpoint, you wouldn't want to make somebody's health records available.\u201d So BitCurator has to have methods for access control that don't really exist in the forensics field. Another speciality of BitCurator is its ability to read long-outdated disks \u2014 an essential tool for archivists who are faced with stacks of old floppies or even reels of magnetic tape. Although digital-forensics investigators usually deal with newer-generation systems, their techniques can still be quite useful for recovery, says Lee. \u201cTaking a forensic approach, you can still create a safe copy of the data, even if you don't know what the file system is or you can't read it,\u201d he says. \u201cAs long as you can attach a drive and get the bits off of it, you can create an image.\u201d Archivists can then experiment on different ways to retrieve the files, safe in the knowledge that the original is not in danger. Some advantages to the forensics-based approach transcend technical considerations, says Olson. With the Gould archives, for example, \u201cyou can get timestamps from different word-processing files to see how he actually wrote something, a particular order that he wrote, a way that he edited. That's really nifty if you're a researcher that wants to know how his mind worked.\u201d \n               Search and rescue \n             The same techniques can be used for other purposes besides archiving. At Stanford, Olson's lab is increasingly helping faculty members and students who need to access work that was born on now-outdated computer systems. \u201cI had a graduate student about a year ago that came to us with an astrophysics data set on a Zip disk,\u201d he says. \u201cIt was something that their professor had created, that they weren't able to read and needed to get to because it was part of their research. And nobody had really shepherded that to a new modern system.\u201d The library was able to help the student do just that. Another recent example is Stanford's long-running ME310 engineering course, which had a server full of design studies, presentation slides and videos that students had completed over the years as part of their graduate work. \u201cThe people running the programme wanted to preserve all the data from these projects,\u201d says Olson, \u201cbut they needed help to recover the data, organize it and also get permission from the students to actually make this available.\u201d Data are already being lost to science at a rapid rate. One study, for example, found that  as little as 20% of data for ecology papers published in the early 1990s is still available  (T. H. Vines  et al .  Curr. Biol.   6 , 94\u201397; 2014 ). Co-author Tim Vines, who now runs a peer-review service called Axios Review in Vancouver, Canada, says that the best way for scientists to preserve their data for future generations is to upload it into library-maintained archives or open online repositories, such as  Dryad  or  Figshare . \u201cPutting it into the hands of an organization committed to preserving it is far better than putting it on a shelf\u201d, he says. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Eight ways to clean a digital library 2015-Nov-02 \n                   \n                     The trouble with reference rot 2015-May-04 \n                   \n                     Scientific writing: the online cooperative 2014-Oct-01 \n                   \n                     Scientists losing data at a rapid rate 2013-Dec-19 \n                   \n                     Publishing frontiers: The library reboot 2013-Mar-27 \n                   \n                     BitCurator \n                   \n                     Stanford Born Digital/Forensics Lab \n                   \n                     Forensic Toolkit \n                   \n                     EnCase Forensic \n                   Reprints and Permissions"},
{"file_id": "537123a", "url": "https://www.nature.com/articles/537123a", "year": 2016, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "Software tools that digitize and annotate underwater images are transforming marine ecology. Blue spires seem to pop out of the photograph in one place; a patch of bushy forms of pinkish-purple in another. To the untrained eye, each is distinct and clearly a coral. Then, Manuel Gonz\u00e1lez-Rivero points to a third cluster. The bulbous shapes look like coral, except the smooth grey surface isn't quite right. \u201cThe texture and colour suggest that you are probably not looking at a coral,\u201d he says. \u201cMore likely you are looking at a crustose coralline alga.\u201d The image is a high-resolution panoramic photograph collected by the XL Catlin Seaview Survey, a scientific initiative that began to catalogue the world's reefs in 2012. To understand how coral reefs are responding to overfishing, pollution, global warming and ocean acidification, the Catlin team \u2014 ecologist Gonz\u00e1lez-Rivero among them \u2014 is documenting coral abundance, health, structure and biodiversity in millions of underwater snapshots. It would take decades to go through all these images manually, even with Gonz\u00e1lez-Rivero's expert eyes. But the Catlin team is using a neural-networking algorithm: a  deep-learning system  in which a computer learns to classify what it sees in coral-reef pictures. The project was led by computer scientist Oscar Beijbom at the University of California, Berkeley, and the software can zip through Catlin's gigantic photo album \u2014 currently around one million photographs \u2014 in a matter of months. The software is just one example of how coral researchers are embracing advances in computer science and software to speed up under-sea mapping of reefs around the world. Combined with high-quality imagery and sensors that collect standardized biological data about reefs, these tools could unleash an era of semi-automated data collection and monitoring, freeing up ecologists to spend less time processing data and more time doing research. \u201cIt's a tremendous step forward,\u201d says Mark Eakin, who manages the  Coral Reef Watch programme  for the US National Oceanic and Atmospheric Administration (NOAA) in College Park, Maryland. \u201cWhen you aren't limited by the speed of people going through and manually processing images, the yield of information is just so much greater.\u201d \n               Ocean of data \n             Coral researchers' entry into the world of big data comes none too soon. Long limited by the size of their diving fins and the capacity of their oxygen tanks, marine ecologists are racing to expand their surveys to document and understand the  longer-term impacts of rising ocean temperatures and acidification .  The bleaching of corals around the world that has accompanied  the  epic 2015\u201316 El Ni\u00f1o warming event  in the tropical Pacific Ocean has only heightened concerns. Gonz\u00e1lez-Rivero's goal is to cover as much territory as possible to get a sense of how different corals and reefs are responding to these stresses. Computers will never replace the human eye, nor will they obviate the need for detailed underwater investigations and laboratory research, but they can speed up the basic surveys, he says. \u201cWhat we are trying to do is find a compromise where we get enough information to understand the reef, but at a much faster pace and in a much cheaper way.\u201d The quality does not have to be compromised: according to Beijbom's unpublished results, the deep-learning system agrees with the human eye on features in coral photos about 81% of the time \u2014 impressive considering that even two experts are likely to agree only 84% of the time. Beijbom plans to launch the algorithm in a few months' time for anyone submitting pictures to his website  CoralNet , which already uses computer-assisted systems to help the automated analysis of images. The service is free thanks to funding from NOAA, and 420 users from a variety of institutions, including NOAA, have already uploaded nearly 269,000 images to the site. The best results seem to come from use of a semi-automated program in which the computer does simple analyses and alerts human experts to cases that it's not confident about, Beijbom says. In many ways, Gonz\u00e1lez-Rivero says, marine science is catching up with the terrestrial sciences, which have been developing tools to gather and process copious amounts of data from satellites and aircraft for decades. The software and hardware can't be directly translated to analysing seas, however: the ocean swallows light, so it is difficult to study anything but the shallowest reefs from above. That has pushed coral researchers to adapt the tools. At Michigan State University in East Lansing, for example, biophysicists David Kramer and Atsuko Kanazawa have modified a handheld sensor originally designed for agricultural research. When used on land, the sensor measures information such as fluorescence in plants, the carbon content of the soil, the temperature of the air and the humidity. Around 300 sensors are in use in 18 countries, and every time a researcher or a government official takes a reading, the data are uploaded to a central server for analysis. The modified system, dubbed  CoralspeQ , pings reefs with different kinds of light and records the returning spectral signal in 256 wavelengths, from ultraviolet to infrared. These data can be used to measure a reef's photosynthetic activity, for instance, by measuring the fluorescence of chlorophyll in symbiotic algae that provide their host corals with oxygen and nutrients. Knowing how much photosynthetic activity is taking place, and where, could help researchers to identify stressed systems, Kramer says. The devices use commercially available sensors and are built with the help of 3D printers. Kramer and Kanazawa hope to bring down the cost of the underwater version from its current US$500 and get it into the hands of as many scientists as possible. \u201cWe need an army of people making high-quality measurements,\u201d Kramer says. \n               Computer-assisted vision \n             Marine microbiologist Arjun Chennu has developed an  underwater imaging system  called HyperDiver to collect even more detailed data across a greater radiation spectrum. Coral ecologists then annotate the images, and the information is fed into an algorithm that is based on open-source machine-learning software and is similar to the one currently used by CoralNet. The machine's 'hyperspectrum' means that it can capture much more information than can the human eye, says Chennu, who works at the Max Planck Institute for Marine Microbiology in Bremen, Germany. This makes it easier to differentiate between corals that look similar in standard images. \u201cFor example, we resolve the often-used 'other coral' categories into their proper taxonomic types, and also include sponges, macroalgae and seagrass in our predictions,\u201d he says. Others have adapted  commercially available software  that is already used to map landscapes and analyse landslides by overlapping 2D images into 3D models. PhD student John Burns at the University of Hawaii at Manoa's Institute of Marine Biology uses a program called Agisoft PhotoScan, which costs $549 for an educational licence for the professional edition. Free software is available, but it is less sophisticated, Burns says. The models \u2014 which can achieve a resolution of just 1 millimetre when used with good cameras \u2014 can be analysed by people or computers to identify coral species and quantify reef coverage. But, because they're 3D, they can also be used to track structural changes as reefs bleach and break down owing to high ocean temperatures \u2014 a new kind of ecological information. For Burns, the beauty of the method is its simplicity: data can be collected quickly and with minimal training. \u201cThis method just lets you take hundreds of thousands of single-lens images with your camera, and then you are essentially stitching them together,\u201d he says. \n               Standardized store \n             Sophisticated technologies aren't the only answer, says Emily Darling, a marine ecologist with the Wildlife Conservation Society in New York City. Because separate research efforts are collecting ever-greater quantities of data on coral reefs, it is important that they collect standardized data sets and store them in a repository that can be accessed by the entire community. In an effort to collect systematic data on the recent global bleaching event, for example, Darling and her colleagues came up with a very simple technology \u2014 an Excel spreadsheet that scientists around the world can use to register various data on reef conditions. The value is that when scientists come out of the water, they can immediately import and analyse their data, and Darling now has uniform results from more than 61,000 reef colonies in 13 countries. Roughly 58% showed bleaching. Ultimately, Darling says, coral ecologists need to converge on some kind of a central repository for the full suite of information that they are collecting around the world. \u201cWe need places where data are accessible, where they are telling stories, and where people can go and figure out whether conservation actions are working or not,\u201d she says. \u201cWe need to be able to answer those questions a lot faster.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Conservation: Fishing for lessons on coral reefs 2016-Jul-20 \n                   \n                     Mass coral death drives efforts to identify resilient reefs 2016-Jun-15 \n                   \n                     Bright spots among the world\u2019s coral reefs 2016-Jun-15 \n                   \n                     Marine ecologists take to the skies to study coral reefs 2016-May-31 \n                   \n                     Coral crisis: Great Barrier Reef bleaching is \u201cthe worst we\u2019ve ever seen\u201d 2016-Apr-13 \n                   \n                     Epic El Ni\u00f1o yields massive data trove 2016-Mar-02 \n                   \n                     Corals worldwide hit by bleaching 2015-Oct-08 \n                   \n                     CoralNet \n                   \n                     NOAA Coral Reef Watch \n                   Reprints and Permissions"},
{"file_id": "532135a", "url": "https://www.nature.com/articles/532135a", "year": 2016, "authors": [{"name": "Helen Shen"}], "parsed_as_year": "2006_or_before", "body": "Neuroscientists hope to turn the delicate art of eavesdropping on brain cells into an automated technique. Clamping an electrode to the brain cell of a living animal to record its electrical chatter is a task that demands finesse and patience. Known as \u2018whole-cell patch-clamping\u2019, it is reputedly the \u201cfinest art in neuroscience\u201d, says neurobiologist Edward Boyden, and one that only a few dozen laboratories around the world specialize in. But researchers are trying to demystify this art by turning it into a streamlined, automated technique that any laboratory could attempt, using robotics and downloadable source code. \u201cPatch-clamping provides a unique view into neural circuits, and it\u2019s a very exciting technique but is really underused,\u201d says neuroscientist Karel Svoboda at the Howard Hughes Medical Institute\u2019s Janelia Research Campus in Ashburn, Virginia. \u201cThat\u2019s why automation is a really, really exciting direction.\u201d On 3 March, Boyden, at the Massachusetts Institute of Technology in Cambridge, and his colleagues published detailed instructions on how to assemble and operate an automated system for whole-cell patch-clamping 1 , a  concept that they first described in 2012 2 . The guide represents the latest fruits of Boyden\u2019s partnership with the laboratory of Craig Forest, a mechanical engineer at the Georgia Institute of Technology in Atlanta who specializes in robotic automation for research. Most neural recordings involve inserting an electrode in the space between cells to pick up electrical volleys between neurons. Such \u2018extracellular recording\u2019 detects outgoing signals but misses the electrical activity inside the cells that determines whether they will fire. This is where whole-cell patch-clamping comes in, a technique that can tap into a neuron\u2019s innards. The delicate procedure \u201chas a very steep learning curve, and even then some people never really get it to work\u201d, says Svoboda. Whole-cell patch-clamping involves pushing a tiny glass pipette containing a wire electrode through the brain. In the most common, \u2018blind\u2019 version, researchers do this without being able to see the neurons. The scientist must continually apply pressure to push brain matter away from the pipette, but when a rise in electrode resistance indicates that a cell is nearby, they must switch to suction at just the right moment to seal a tiny patch of the neuron\u2019s membrane against the pipette\u2019s super-thin tip. With an additional burst of suction, the researcher can then make a tiny hole in the cell membrane to record the neuron\u2019s activity. Hitting the neuron at the wrong angle, misregulating the pressure and numerous other variables often derail recordings. \u201cEvery step has a certain failure rate, and these multiply throughout the process,\u201d says Boyden. Experienced practitioners report success rates of between 20% and 60%. Boyden and Forest decided to automate this tricky technique. Their robot does not outperform human experts yet, but its average success rate is around 33% in tests on mice. The device, which runs on the commercial programming platform LabVIEW, only requires researchers to position the animal and the pipette. A computer algorithm then controls the pipette\u2019s internal pressure and its progression through the brain. A company called Neuromatic Devices in Atlanta, Georgia, offers machines based on Boyden and Forest\u2019s technology, but did not disclose pricing or sales figures to  Nature . At the University of Texas at Austin, researchers have created a similar auto-patching system that is controlled in the MATLAB computing environment. This system uses a slightly different algorithm to decide when to start suctioning, and it succeeds in patch-clamping cells in mice about 17% of the time 3 . Neuroscientist Niraj Desai, who led the team, says that he hopes to incorporate more-sophisticated algorithms. Some researchers question whether the recording robots will ever surpass the best human experts. \u201cThe elements that go into the human\u2019s decisions may be richer than can be captured by the machine,\u201d says neuroscientist Michael Hausser at University College London. But he adds that the technologies could still be a huge boon to novices. Others suggest that the robots could help users of all skill levels in lengthy or complex experiments, in which human fatigue becomes a limiting factor. At the Allen Institute for Brain Science in Seattle, Washington, researchers have developed an automated system to assist in the even more challenging \u2018image-guided\u2019 variant of the technique. In this version, instead of blindly bumping into neurons with a pipette, scientists target specific neurons near the brain\u2019s surface using a two-photon microscope. The procedure requires more coordination than blind patch-clamping because the scientist must constantly focus the microscope in addition to guiding the pipette and adjusting its internal pressure. \u201cThis is a technique that ideally would benefit from having three hands,\u201d says Hausser, an expert in image-guided patching. The automated system constructs 3D images of the brain region of interest and allows users to digitally select the neuron that they want to record. Then, with the coordinates locked in, the device navigates the pipette into place. For now, researchers still need to patch onto the cell by hand, but Allen Institute neuroscientist and joint team leader Lu Li says that eventually they hope to fully automate the procedure. Whether these automation systems will be taken up widely by the neuroscience community remains to be seen. Each of the teams has made their code freely available for people to download: Boyden\u2019s group at  autopatcher.org ; Desai\u2019s team at  clm.utexas.edu/robotpatch ; and Li\u2019s team at the GitHub repository ( go.nature.com/sgjpab ). \u201cOur hope is that we can help as many people as possible to answer questions about how neurons compute,\u201d Boyden says. \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     Injectable brain implant spies on individual neurons 2015-Jun-08 \n                   \n                     Ambitious plans for BRAIN project unveiled 2014-Jun-06 \n                   \n                     Brain-mapping projects to join forces 2014-Mar-18 \n                   \n                     Robots hunt neurons to record brain activity 2012-Aug-28 \n                   \n                     Nano-hairpin peeks into cells 2010-Aug-12 \n                   \n                     Lab automation and robotics: Automation on the move 2003-Feb-06 \n                   \n                     Autopatcher \n                   \n                     Robotpatch \n                   \n                     smartACT \n                   Reprints and Permissions"},
{"file_id": "533131a", "url": "https://www.nature.com/articles/533131a", "year": 2016, "authors": [{"name": "Jeffrey M. Perkel"}], "parsed_as_year": "2006_or_before", "body": "Experiments that generate millions of images have forced scientists to find new ways to store and share terabytes of experimental data. As the fruit-fly larva wriggles forwards in the video, a crackle of neural activity shoots up its half-millimetre-long body. When it wriggles backwards, the surge undulates the other way. The 11-second clip, which has been watched more than 100,000\u00a0times on YouTube, shows  the larva\u2019s central nervous system  at a resolution that almost captures single neurons. And the experiment that created it produced several million images and terabytes of data. For developmental biologist Philipp Keller, whose team produced the video at the Howard Hughes Medical Institute\u2019s Janelia Research Campus in Ashburn, Virginia, such image-heavy experiments create huge logistical challenges. \u201cWe\u2019ve spent probably about 40% of our time during the past 5\u00a0years simply investing in computational methods for data handling,\u201d he says. The problem isn\u2019t so much storing images \u2014 data storage is cheap \u2014 but organizing and processing the images so that other scientists can make sense of them and retrieve what they need. The \u2018image glut\u2019 challenge is becoming an increasing burden for researchers across the biological and physical sciences. Here, Keller and scientists in two other fields \u2014 astronomy and structural biology \u2014 explain to  Nature  how they are tackling the problem. \n               Mapping the Sun \n             Somewhere in geosynchronous orbit above Las Cruces in New Mexico, the  Solar Dynamics Observatory (SDO)  traces a figure-of-eight in the sky. The satellite keeps a constant watch on the Sun, recording its every hiccup and burp with an array of three instruments that photograph the Sun through ten filters, record its ultraviolet output and track its seismic activity. Those data are then beamed to a ground station below. The SDO produces \u201csomething like 1.5\u00a0terabytes of image data a day\u201d, says Jack Ireland, a solar scientist at ADNET Systems, a NASA contractor in Bethesda, Maryland. According to NASA, this amount of data is equivalent to about 500,000\u00a0iTunes songs. To help researchers to stay on top of those images,  the ADNET team at NASA, with the European Space Agency , developed the  Helioviewer website  for browsing SDO images \u2014 rather like Google Maps for the Sun, says Ireland \u2014 as well as a  downloadable application . Researchers and astronomy enthusiasts using these tools view not the original data, but instead a lower-resolution representation of them. \u201cWe have images of the data,\u201d Ireland explains, \u201cnot the data itself.\u201d The original SDO scientific images are each 4,096\u2009\u00d7\u20094,096 pixels square and about 12\u2009megabytes (MB) in size. They are taken every 12\u00a0seconds, and tens of millions have been collected \u2014 a data archive of several petabytes (PB), and growing (1\u00a0PB is 1\u00a0billion\u00a0MB, or 1,000\u00a0TB). To make images accessible to users, every third image is compressed to 1\u00a0MB and made available through Helioviewer. Users can jump to any particular time since the SDO launched in 2010, select a colour filter and retrieve the data. They can then zoom in, pan around and crop the images, and string them together into movies to visualize solar dynamics. Users create about 1,000 movies a day on average, Ireland says, and since 2011, at least 70,000 have been  uploaded to YouTube . Once they have selected an individual image or cropped area, such as the region around a particular solar flare, users can still download it in its original high resolution. They can also download the complete archive of smaller 1-MB images if they want: but at 60\u2009TB and counting, that process could take weeks. \n               Faster file formats \n             For Keller\u2019s developmental-biology group at the Janelia Research Campus, posting their data online for outsiders to access isn\u2019t such a concern. If others request it, the team can share images using  specialist file-transfer tools , or simply by shipping hard drives. First, however, the team must manage and sort through images that stream off the lab\u2019s microscopes at the rate of a gigabyte each second. \u201cIt\u2019s a huge challenge,\u201d Keller says. Keller\u2019s lab uses microscopes that fire sheets of light into the brains and embryos of small organisms such as  fruit flies ,  zebrafish  and mice. These have been genetically modified so that their cells fluoresce in response \u2014 allowing the team to image and track each cell in 3D for hours. To store its data, the lab has spent around US$140,000 on file servers that provide about 1\u00a0PB of storage. The highly structured organization of the millions of images on those servers keeps the team sane. Each microscope stores its data in its own directory; files are arrayed in a tree that describes the date a given experiment was done, what model organism was used, its developmental stage, the fluorescently tagged protein used to visualize the cells, and the time that each frame was taken. The lab\u2019s custom data-processing pipeline was constructed to act on that organization, Keller says. Yet the directories don\u2019t contain the JPEG image files with which most microscopists are familiar. The JPEG format compresses image file sizes, making them easier to process and transfer, but it is relatively slow at reading and writing those data to disk, and is inefficient for 3D data. Keller\u2019s microscopes collect images so fast that he needed a file format that could compress images as efficiently as JPEG, but that could be written and read much faster. And because the lab often works on isolated subsets of the data, Keller needed a simple way to extract specific spatial locations or time points. Enter the  Keller Lab Block (KLB) file format , developed by Keller and his team. This chops up image data into chunks (\u2018blocks\u2019), which are compressed in parallel by multiple computer processors 1 . That triples the speed at which files can be read and written, so KLB can compress file sizes just as well as the JPEG format, if not better. In theory, Keller says, KLB files could be used on commercial digital cameras or on any system that requires rapid data access. KLB source code is freely available, and the lab has made tools and file converters for the MATLAB programming environment and for an open-source image-analysis package called ImageJ, as well as for some commercial packages. Researchers using commercial microscopes could employ the format too, says Keller; he calls it \u201cstraightforward\u201d to convert data to KLB files for long-term storage and use. \n               Sharing raw data \n             Biologists who take pictures to determine molecular structures also generate vast amounts of image data. And one technique that is growing in popularity \u2014 and hence, generating more data \u2014 is  cryoelectron microscopy (cryoEM) . CryoEM users fire electron beams at a flash-frozen solution of proteins, collect thousands of images and combine these to reconstruct a 3D model of a protein with near-atomic resolution. Most of these reconstructions are less than 10\u2009GB in size, and researchers deposit them in the  Electron Microscopy Data Bank (EMDB)  \u2014 but not the raw data used to create them, which are some two orders of magnitude larger than the resulting models. The EMDB simply was not set up to handle them, says Ardan Patwardhan, who leads the EMDB project for the Protein Data Bank in Europe (PDBe) at the European Bioinformatics Institute (EBI) near Cambridge, UK. As a result, reproducibility suffers, Patwardhan says: without access to raw data, researchers can neither validate others\u2019 experiments nor develop new analysis tools. In October 2014, the PDBe launched a pilot solution: a database of raw cryoEM data called the  Electron Microscopy Pilot Image Archive (EMPIAR) , also led by Patwardhan. Only data sets for structures deposited in the EMDB are allowed, he says; otherwise, users might be tempted to use the database as a data dump. EMPIAR currently contains 49\u00a0entries averaging 700\u2009GB apiece. The largest is more than 12\u2009TB, and the total collection weighs in at about 34\u2009TB. \u201cWe have space available to grow into the petabyte range,\u201d Patwardhan says. Users download about 15\u2009TB of data per month in total. Downloading such large amounts of data presents its own problems: the standard protocol used to transfer files between computers, called FTP, struggles with large data sets; connection loss is common, and download times can slow significantly over long distances. Instead, the EBI has paid for EMPIAR users to access two high-speed file-transfer services,  Aspera  and  Globus Online , both of which transfer data at the rates of \u201ca few terabytes per 24 hours\u201d, Patwardhan says. The EBI \u2014 which also uses these services to transfer large genomics data sets \u2014 pays for its side of the transaction. The cost to the EBI of providing Aspera can be many tens of thousands of dollars per year, he says. The EMPIAR raw data has already proved its worth. Edward Egelman, a structural biologist at the University of Virginia in Charlottesville, co-authored a study 2  of the structure of an aggregated, filament-like protein called MAVS \u2014 which was at odds with another, earlier model of the protein 3 . Egelman proved the earlier structure was incorrect by downloading and reprocessing the raw data set 4 . EMPIAR\u2019s grant runs out in 2017, but Patwardhan says that cryoEM researchers have told him they already consider EMPIAR a necessity, and want \u2018pilot\u2019 taken out of the archive\u2019s name. \u201cThey feel that this should be considered a vital archive for the community \u2014 which is nice to hear,\u201d he says.\n \n                     Fly larvae brains filmed in action 2015-Aug-11 \n                   \n                     Genome researchers raise alarm over big data 2015-Jul-07 \n                   \n                     Neuroscience: Solving the brain 2013-Jul-17 \n                   \n                     High-energy physics: Down the petabyte highway 2011-Jan-19 \n                   \n                     Helioviewer \n                   \n                     Philipp Keller\u2019s laboratory \n                   \n                     Electron Microscopy Pilot Image Archive \n                   Reprints and Permissions"},
{"file_id": "535187a", "url": "https://www.nature.com/articles/535187a", "year": 2016, "authors": [{"name": "Ewen Callaway"}], "parsed_as_year": "2006_or_before", "body": "Inventive graphic design and abstract models are helping researchers to make sense of a glut of data. A smart visualization can transform biologists' understanding of their data. And now that it's possible to sequence every RNA molecule in a cell or fill a hard drive in a day with microscopy images, life scientists are increasingly seeking inventive visual ways of making sense of the glut of raw data that they collect. Some of the visualizations that are currently exciting biologists were presented at a conference at the European Molecular Biology Laboratory in Heidelberg, Germany, in March. Called Visualizing Biological Data (VIZBI), the meeting was co-organized by Se\u00e1n O'Donoghue, a bioinformatician at the Garvan Institute of Medical Research in Sydney, Australia. The gathering attracts an eclectic mix of lab researchers, computer scientists and designers and is now in its seventh year. Here,  Nature  highlights some of O'Donoghue's picks of the visualizations that are set to transform biology. \n               Streamlined cells \n             Bioinformatics postdoc Nico Scherf watches cells shift paths to form different germ layers and then organs in developing zebrafish embryos using 'light-sheet microscopy' techniques developed by his supervisor at the Max Planck Institute of Molecular Cell Biology and Genetics in Dresden, Germany. But, he says, when tracing the path of every single zebrafish cell, \u201cyou end up with a hairball\u201d of tracks. To get some meaning out of these hairballs, Scherf borrowed some fluid-dynamics approaches used to analyse atmospheric and ocean currents. \u201cYou only plot the major streamlines, which gives you the highways of cellular motion,\u201d he says. To achieve this, Scherf wrote some software to analyse the images, and will share it with others on request. So far, his approach has revealed that a mutation that causes abnormal organ development alters the movement of cells only very early in zebrafish development. And he thinks that people who are studying the development of other organisms could benefit from getting into the flow of things, too. \n               Abstract connections \n             Jasmin Imran Alsous, a developmental biologist at Princeton University in New Jersey, took inspiration from Picasso when trying to make sense of microscopy images of a fruit fly's egg chamber, a torpedo-shaped cluster that forms when a germ cell goes through four incomplete and asymmetrical divisions. The final result is a network of 16 interconnected cells that constitute both the developing embryo and the surrounding cells that nourish it. Alsous's adviser had sent her an article about Picasso lithographs that depicted increasingly abstract renderings of a bull. She thought the same principle could apply to depictions of the egg chamber. She transformed fluorescent microscope images of the chamber into a string of numbers that unambiguously represent how each cell connects to the others. Using this abstraction, she has found that some of the 72 possible configurations of the egg chamber are much more common than others. She is now testing whether the different configurations affect how fruit-fly embryos grow and develop. \n               A better model of the cell \n             O'Donoghue says that his first attempt to visualize how fat cells respond to insulin ended up as a hairball of criss-crossing molecular pathways. A colleague had measured how hundreds of different protein types in a cell are phosphorylated (which tends to activate them) in response to insulin over the course of an hour, when the cell stops burning fat to produce energy and starts bringing in sugars and storing fats. To tame the hairball, O'Donoghue found inspiration in a famous chart created by the nineteenth-century French civil engineer Charles Joseph Minard. The image charted Napoleon's disastrous invasion of Russia, and integrated six kinds of data \u2014 including troop numbers and geography \u2014 in two dimensions. O'Donoghue's 'Minardo' chart visualizes an insulin-treated cell like a clock, with consecutive phosphorylation events moving clockwise around the cell. It also depicts a protein's location in a cell and its relationship to other molecular players. One of the major insights from the visualization, O'Donoghue says, is how quickly the cell responds to insulin, with many changes occurring in the first 15 seconds. \u201cA lot of people in the community were quite shocked by the suddenness.\u201d He is eager for others to use the approach to map other dynamic events, such as the cell cycle, and has created an online guide for doing so. But, for now, he says, \u201cyou have to do a lot of manual tweaking\u201d. \n               Inside out \n             Illustrator Graham Johnson is used to depicting the internal life of cells by hand. Now director of the Animated Cell project at the Allen Institute for Cell Science in Seattle, Washington, Johnson got his start doing illustrations for a cell-biology textbook. \u201cDespite painstaking efforts to be accurate, it was always easy to make mistakes,\u201d he says \u2014 particularly when depicting the relative size of cellular components. \u201cWhen you're creating a visualization, you're establishing what will be the mental model for many current and future biologists,\u201d so accuracy is key, he adds. To make cellular model-making more systematic, Johnson developed a tool called cellPACK. To use it, researchers use experimental data to create a series of physical rules (a 'recipe') by which defined cellular components such as proteins, lipids and nucleic acids (the 'ingredients') fill a space. Johnson would like to create a platform such that the models are automatically updated when new data are generated. But despite lots of interest from other researchers, most life scientists find that the tool requires too much time and effort to be very practical. \u201cIt's months of research to generate a recipe from scratch,\u201d says Johnson, who plans to release a more streamlined web version of the software later this year. The tool isn't just for making visually striking models, he emphasizes. It can also help scientists to come up with testable hypotheses. His team created a model of the internal structure of HIV and used it to predict how the protein that forms the outer shell interacts with an internal protein. Johnson says that a virologist recently got in touch with him to say his conclusions gleaned from cellPACK checked out experimentally. \u201cHe has a bunch of new data, and he wants to work with us to build new models.\u201d \n               Computer vision \n             Biologists who are studying life at the cellular and molecular level aren\u2019t the only ones with too much data on their hands. By some estimates, more than 80% of the planet\u2019s organisms haven\u2019t been named, and pretty much any taxonomist who goes looking for new forms of life \u2014 be it in a tropical rainforest, the ocean floor or a basement herbarium \u2014 will find some. John La Salle, an entomologist at the Commonwealth Scientific and Industrial Research Organisation in Canberra and the director of the Atlas of Living Australia, is among a disparate group of scientists who are employing a 3D modelling technology called computer vision. The software was originally developed with factory robots and Mars rovers in mind and uses digital photographic images of something, such as a beetle or dinosaur bone, to create a digital model. From the 3D models, such as of a dragon-fly wing (see image), a computer can automatically measure traits of a specimen that can be used to determine how the organism is related to other species and to members of the same species. La Salle\u2019s team is using computer vision to better characterize Australia\u2019s biodiversity. And others are using the technique to digitize the collections of herbaria and natural-history museums. \u201cIf our goal is to change the way we discover biodiversity on the planet, we have to go outside the box,\u201d says La Salle. \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n                 Wechat \n                 Follow @ewencallaway \n               \n                     Data visualization: Science on the map 2015-Mar-04 \n                   \n                     Programming tools: Adventures with R 2014-Dec-29 \n                   \n                     Nature Toolbox \n                   Reprints and Permissions"},
{"file_id": "538127a", "url": "https://www.nature.com/articles/538127a", "year": 2016, "authors": [{"name": "Jeffrey Perkel"}], "parsed_as_year": "2006_or_before", "body": "Scientists are turning to a software\u2013development site to share data and code. When the Ebola outbreak in West Africa picked up pace in July 2014, Caitlin Rivers started to collect data on the people affected. Rivers, then a PhD student in computational epidemiology, wanted to model the outbreak\u2019s spread. So every day she downloaded PDF updates released by the ministries of health of the virus-stricken countries, and converted the numbers into computer-readable tables. Rather than keeping these files to herself, she  posted them  to  GitHub.com , a hugely popular website for collaborative work on software code. Rivers thought the postings might attract those interested in up-to-date information from the Ebola outbreak. \u201cI figured if I needed it, other people would, too,\u201d she says. Rivers was right. Other researchers began to download the data and contribute to the project. On some days, third parties would download and convert the ministries\u2019 data before her, and load them into the GitHub repository. Others created programming scripts to do simple error-checks on the data, such as ensuring that the daily patient counts made sense. At the time, GitHub was \u201creally the only place on the Internet that you could interact with these data as data, and not as a PDF\u201d, says Rivers, who was at Virginia Polytechnic Institute and State University in Blacksburg when she began the project, and is now an epidemiologist at the US Army Public Health Center in Edgewood, Maryland. Launched in 2008 to assist software developers, GitHub now boasts some 15 million users and is an increasingly popular site for researchers to share, maintain and update scientific data sets and code (see \u2018Growing influence of GitHub\u2019). GitHub is \u201cthe biggest revelation in my workflow ... since I started writing code\u201d, says Daniel Falster, a postdoctoral researcher in ecology at Macquarie University in Sydney, Australia. \u201cWhen we started using GitHub, it was just amazing. We now use it in everything that we do.\u201d Falster\u2019s  Biomass and Allometry Database , which aggregates various measures of plant size from 176 studies, is stored on the site. So is the  Open Tree of Life  project, which aims to compile different published phylogenies to build one master \u2018tree of life\u2019. It  uses GitHub  to store data files and publication records, and to accept new data sets from third parties. Plenty of websites are dedicated to sharing data. But GitHub is specifically designed for transparent, open collaboration because it uses version-control software to track every change made to code or data. This means that large, distributed teams of programmers can work together on a project online, and users can scroll back in time through a file\u2019s version history, seeing each change, when it was made, by whom and for what purpose. Programmers can copy (\u2018fork\u2019) a repository to experiment with new ideas; useful changes can be folded into the main project, while others can be ignored or rolled back later. For instance, anyone can visit the GitHub-based  Open Exoplanet Catalogue  \u2014 a growing database of the thousands of known planets outside the Solar System \u2014 and submit new information through their browser. As with the Open Tree of Life, the project\u2019s main website doesn\u2019t have github.com in the URL address, so casual visitors wouldn\u2019t necessarily know that they are interacting with version-control software \u2014 but the files are openly available  in a GitHub repository  for more sophisticated users. Making an edit alerts the project\u2019s developers, including Hanno Rein, an astrophysicist at the University of Toronto in Canada, to review the suggested change. GitHub, says Rein, allows for a \u201cway more democratic system\u201d than would a static online catalogue of exoplanets, because any user can suggest changes and can even customize a version of the data set to their own specifications. Some 100 people have forked the project\u2019s repository, and Rein\u2019s smartphone app Exoplanet, which runs off the same database, has attracted around 10 million downloads. \n               From Linux to the lab \n             The software tool that GitHub relies on is called  Git . It was created in 2005 by coder Linus Torvalds to manage development of the open-source operating system Linux \u2014 a huge project that involved thousands of independent programmers. \u201cGit is a technology that's designed for very fine-grained, line-by-line monitoring of changes in source code,\u201d says Arfon Smith, a program manager for GitHub in Seattle, Washington. It is not the only version-control software available (another option is  Mercurial ), but it is one of the most popular. Many programmers use Git on their own computers. For scientist coders, the tool works like a laboratory notebook for scientific computing, says Katy Huff, a nuclear engineer at the University of Illinois at Urbana\u2013Champaign: just like a lab notebook, it keeps a lasting record of events. But its syntax and workflow are notoriously confusing. \u201cI\u2019m comfortable saying that the interface is unnecessarily non-intuitive,\u201d Huff says. GitHub\u2019s prettier browser interface softens some of Git\u2019s hard edges, making it easier for novices to contribute. The site now hosts millions of projects, some personal, some massively collaborative, and is free for open-source projects. (Users and organizations that want to keep their files private pay US$7 per month and up. A related service called  Bitbucket , which also runs on Git, offers unlimited free public and private repositories for up to five users; larger collaborations cost from $10 per month.) Not every kind of data set works well with Git software. The tool records line by line how files have changed, rather than by maintaining multiple versions of files. It works well with text files such as source code, XML files, manuscripts written in Markdown or LaTeX, and CSV files (which can be exported from Excel, for instance). But it cannot effectively keep track of changes in non-human-readable \u2018binary\u2019 files, such as Microsoft Office documents and images, because the program\u2018s \u2018diff\u2019ing\u2019 function, which identifies how files change from version to version, cannot interpret such data. \u201cAs soon as you introduce a binary format that isn\u2018t line-oriented, Git does a terrible, terrible job of versioning that content,\u201d Smith says. GitHub also imposes file limitations; it has a hard limit of 100 megabytes per file, and a \u2018soft\u2019 cap of a gigabyte per repository. (A plug-in called  Large File Storage  allows Git and GitHub to more effectively handle larger files, although it still cannot report the differences between binary versions.) \n               Fast and flexible \n             GitHub makes most sense for those researchers working with relatively small, text-based data sets that are being actively updated, curated and maintained by groups of scientists \u2014 such as Rivers\u2019 Ebola-virus project. Nick Loman, a microbial genomicist and bioinformatician at the University of Birmingham, UK, has also used the site to drive fast-paced studies of pathogens. Loman is a member of the  ZiBRA (Zika in Brazil Real Time Analysis) project , an ongoing Brazilian surveillance effort that collects Zika-virus samples across the country and sequences and analyses them in real time. Traditionally, Loman says, DNA sequence data go to archives such as GenBank \u2014 and these data will too. But it can take time for those sites to release data to the public. GitHub, he says, provided a faster and more flexible way to disseminate  draft data sets , rather like tweeting a research finding in advance of publication. Because data sets on GitHub can be changed or deleted by their authors, the site doesn\u2019t guarantee a permanently citable archive, warns Smith. Those interested in creating a long-term, permanent record of their data set as it exists at a particular point in time \u2014 for example, when a paper is published \u2014 should consider storing the relevant version of their data on dedicated scientific sites, such as  Zenodo  and  Figshare . Both of these sites allow GitHub users to archive snapshots of their repositories, and will  provide a citable Digital Object Identifier (DOI)  for the data set. According to Smith, some 8,000 GitHub users have done so. Another data-sharing option is  Dat , a general-purpose tool for sharing and syncing data between different computers. According to lead programmer Max Ogden in Portland, Oregon, Dat provides versioning in a similar way to Git for collaborative work, but includes a peer-to-peer file-sharing system for distributing data files. Ogden says that Dat is more adept at handling large binary files because it breaks them into chunks and transfers only those pieces that have changed. Data sharing is a key requirement of open science, and researchers can share data sets anywhere they wish. But even if they don\u2019t use GitHub.com, scientists should consider using Git or a comparable tool to record changes to data sets and data-processing scripts, says Tracy Teal, executive director of  Data Carpentry , a non-profit organization that trains researchers in working with data. Researchers interested in learning to use Git and GitHub have many online resources to turn to: Codecademy offers a free interactive tutorial, as does GitHub ( try.github.io ). Greg Wilson, founder of the research-computing skills site  Software Carpentry , co-authored a how-to guide in January (J. D. Blischak  et al .  PLoS Comput. Biol.   12 , e1004668; 2016 ). And many programmers and bioinformaticians use Git \u2014 so they, too, can always be asked for help. Despite their steep learning curves, Git and GitHub have a loyal fan base among scientists. Emily Jane McTavish, an evolutionary biologist at the University of California, Merced, and a member of the Open Tree of Life project, says it's an essential resource. \u201cI don\u2019t know how I lived without it.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Researchers debate whether female computer coders face bias 2016-Feb-15 \n                   \n                     The unsung heroes of scientific software 2016-Jan-04 \n                   \n                     My digital toolbox: Nuclear engineer Katy Huff on version-control systems 2014-Sep-29 \n                   \n                     'Boot camps' teach scientists computing skills 2014-Sep-03 \n                   \n                     Science projects on GitHub \n                   \n                     Open Exoplanet Catalogue \n                   \n                     Caitlin Rivers' Ebola Virus data \n                   \n                     Open Tree of Life \n                   Reprints and Permissions"},
{"file_id": "539125a", "url": "https://www.nature.com/articles/539125a", "year": 2016, "authors": [{"name": "Anna Nowogrodzki"}], "parsed_as_year": "2006_or_before", "body": "The creators of the Open Syllabus Project hope that sharing data can both improve and reward teaching. Despite a growing movement to glean insights from scholarly materials that are available online \u2014 from articles and data sets to conference presentations and lectures \u2014 one kind of academic document remains little examined. And that is the syllabus: a document that lays out the reading materials, topics and expectations of college courses. That, at least, was the case until January this year, when data scientists, sociologists and digital-humanities researchers at Columbia University in New York City launched a tool called the  Open Syllabus Explorer . This integrates more than 1 million publicly available syllabuses and lays open their data in a conveniently searchable format. A version containing three times as many syllabuses is scheduled to launch in January 2017. The team behind the tool, the Open Syllabus Project (OSP), hope to nudge universities towards making more syllabuses public. They argue that doing so could aid textbook authors, instructors and course developers, and would reward the design of effective teaching materials, which is largely overlooked by conventional measures of academic effort. \u201cSyllabuses are among the most important documents written by scholars which are not yet widely shared, and they ought to be,\u201d says Peter Suber, director of the Harvard Open Access Project and the Harvard Office for Scholarly Communication in Cambridge, Massachusetts, who serves on the OSP advisory board. \u201cThey reflect serious scholarly judgements about what's worth teaching.\u201d Such judgements can be welcome news to textbook authors. Stuart Russell, a computer scientist at the University of California, Berkeley, didn't realize until  Nature  interviewed him for this article that his 1995 book  Artificial Intelligence  (Prentice Hall), co-authored with Peter Norvig, was the most highly assigned text in the field of computer science. \u201cI was definitely surprised,\u201d he says. Beyond stoking professional pride, such information could strengthen tenure and promotion packages. Authoring a textbook, no matter how useful and informative it might be, generally yields few citations in scholarly papers, so its academic impact is likely to be low. The OSP could help to shift the balance. \u201cWe're at a point in time when I think faculty have to take more ownership of their whole record of scholarship, of impact, of influence,\u201d says Amy Brand, director of the MIT Press. Hard data on syllabus usage, she says, could empower faculty members \u201cto tell their own story about what their work is doing in the world\u201d. At present, Open Syllabus Explorer searches more than 1 million syllabuses dating back to 2000, cross-referenced with 20 million texts, to produce data on how often a text is taught. Users can search those data by author, title, institution and academic discipline. The tool also reports which textbooks are commonly used together, and ranks each text on how frequently it is taught (see \u2018Top texts\u2019). \n               Top texts \n               An updated version, due to become available on 21 January 2017, the Explorer's first anniversary, will feature 3 million syllabuses cross-referenced with about 150 million texts; these will include titles from the arXiv preprint server, CrossRef and the Virtual International Authority File \u2014 which links together identical bibliographic records from different national-library catalogues. The update will include new search options, such as the ability to search by date or type of institution, says Joe Karaganis, the OSP's project director. The new version will also incorporate better Canadian and UK data, information about where to find materials and, eventually, full-text syllabuses, if the authors have given permission to reproduce them. \u201cWe have some big ambitions,\u201d Karaganis says. \u201cAll the techniques are very crude at present but they're all improvable, and the data science is only getting better.\u201d \n               Fishing for citations \n             The OSP is based at the American Assembly, a public-policy institute at Columbia University, and is funded by the Sloan Foundation and the Arcadia Fund. It was inspired by a search engine called Syllabus Finder, which scraped the public web for syllabuses from 2002 (the year it was built) until 2009. That tool was created by Dan Cohen, then a historian at George Mason University in Fairfax, Virginia, who is now executive director of the Digital Public Library of America. It amassed what Cohen says was then the largest collection of syllabuses ever assembled, comprising about 1 million documents. He  released the URLs as a database in 2011 . Unlike the OSP, Cohen's tool provided links to the full text of each syllabus. But it included only courses run up to 2009, when he had to retire the tool because of changes to Google's programming interface \u2014 a move that vexed Cohen's colleagues, including his wife, an early-childhood educator. \u201cI still get e-mails begging me to turn the Syllabus Finder back on,\u201d he says. When the OSP began in 2014, the team built tools to scrape the public Internet \u2014 including the links used by Cohen, who had lost a portion of the data owing to a coding error. But, as Cohen was, the team is limited to publicly accessible syllabuses: about 6 million of an estimated 80 million to 120 million syllabuses in the United States alone, by Karaganis's reckoning. Syllabuses sealed behind the walls of private course-management software, such as Blackboard, remain out of reach. \u201cColumbia, for example, is sitting on 80,000 syllabuses from the last 12 or 13 years,\u201d says Karaganis. \u201cA large state school could have two, three times that.\u201d The OSP team then had to build tools to extract what those syllabuses contained. Citations, for instance, had no consistent structure, says David McClure, the project's technical director. The tool searched for titles by cross-referencing each syllabus against a database of 20 million titles \u2014 11 million from Harvard LibraryCloud and 9 million from JSTOR. A matching title and author counted as a citation. \u201cWe built in different techniques for allowing fuzziness, like allowing the word 'by' in between the author and title,\u201d says McClure. \n               A new metric \n             The OSP distils those data down to a single metric called the teaching score, which indicates how often a text is assigned in syllabuses. It can take any value from 1 (rarely taught) to 100 (frequently taught). According to Suber, teaching scores are an alternative to conventional metrics of scholarly impact. They reflect the burgeoning 'alternative metrics' ethos, which aims to quantify the whole of a person's research output. \u201cI think this teaching score can take part in the new alt-metrics movement and give us a more sensitive measurement of the impact of texts,\u201d he says. Already, a handful of researchers and universities are using the data to do just that. The University of Kentucky in Lexington issued a  press release  when it discovered that a paper by Edward Morris, one of its faculty members, ranked 46 out of 13,225 sociology-related texts. It now ranks 371 out of 53,177, and Morris plans to use the figure to support his promotion to full professor. US universities aren't the only ones paying attention. Most of the roughly 1,000 visits to the OSP each day are from the United States, says Karaganis, but significant traffic comes from Ukraine, Russia and Egypt as well. Other researchers have used the data to compile  lists of widely taught graphic novels and comics , for instance, or to quantify  the fraction of frequently taught sociology texts authored by women . Melanie Martin, a postdoc at Yale University in New Haven, Connecticut, used the Syllabus Explorer to identify the most commonly taught texts in her field,  evolutionary anthropology . But, because there is no way to search the database by subfield \u2014 for instance, limiting biology results to such subdisciplines as neuroscience or genomics \u2014 she had to scan the 16,000 anthropology titles manually. \u201cWithout better filtering, I think it's limited,\u201d she says. \n               Building on peer expertise \n             Another possible application of OSP data involves course design. By enabling faculty members \u2014 particularly junior ones \u2014 to build on the knowledge of their peers, the OSP could help them to teach more creatively, such as by identifying new ways to present teaching material. \u201cThis could go a long way to improving the quality of instruction,\u201d says Russell. It would also improve efficiency, leaving faculty members more time for other activities such as research and mentoring. However, it is important not to over-interpret the data, says Lisa Janicke Hinchliffe, a specialist in information literacy at the University of Illinois at Urbana\u2013Champaign. The project's sample set might not be a good proxy for all syllabuses, even at a particular institution. For instance, the second most-assigned text at Harvard, according to the Explorer, is 'Letter from Birmingham Jail' by Martin Luther King Jr. But about 80% of the OSP's Harvard syllabuses come from the John F. Kennedy School of Government, Karaganis says (although the OSP doesn't publicly list its sources in this much detail). So it's not possible to conclude how popular this text is at Harvard overall. For Hinchliffe, the value of the OSP lies in its ability to reveal the breadth of resources that instructors use. \u201cI don't need a definitive 'These are the top-six taught books,'\u201d she says. \u201cI want to see the variety.\u201d Such information could go a long way towards simplifying course design, a notoriously time-consuming process. Just ask Suber, who has been teaching philosophy for 21 years. \u201cWhenever I knew a new course was coming, I would try to start preparing it at least a year in advance,\u201d he says. \u201cWriting 40 lectures is a huge job; it's harder than writing a book.\u201d The OSP's data could ease that burden. Plus, says Suber, the data are fun to explore, sometimes revealing unexpected pairings. His legal philosophy text,  The Case of the Speluncean Explorers  (Routledge, 1998), for instance, has been taught alongside Sappho's lyric poetry. \u201cThere are partners or juxtapositions that I never would have guessed,\u201d he says. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Time to remodel the journal impact factor 2016-Jul-27 \n                   \n                     Beat it, impact factor! Publishing elite turns against controversial metric 2016-Jul-08 \n                   \n                     NIH metric that assesses article impact stirs debate 2015-Nov-06 \n                   \n                     We need a measured approach to metrics 2015-Jul-08 \n                   \n                     Q&A: Peter Atkins on writing textbooks 2010-Feb-03 \n                   \n                     Nature  special: Science metrics \n                   \n                     Open Syllabus Project \n                   \n                     Open Syllabus Explorer \n                   \n                     Analysis of the Open Syllabus Explorer (Medium.com) \n                   Reprints and Permissions"},
{"file_id": "540151a", "url": "https://www.nature.com/articles/540151a", "year": 2016, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "Researchers debate whether using a program to automatically detect inconsistencies in papers improves the literature, or raises false alarms. Mich\u00e8le Nuijten and her colleagues found rampant inconsistencies when they unleased statcheck on the psychological literature. The program scans articles for statistical results, redoes the calculations and checks that the numbers match. It went through 30,717 papers to identify 16,695 that tested hypotheses using statistics. In half of those,  it found at least one potential error  ( M. B. Nuijten  et al. Behav. Res. Methods   48,  1205\u20131226; 2016 ). Nuijten did not alert the papers' authors. But this August, her co-author Chris Hartgerink, a fellow methodologist at Tilburg University in the Netherlands, moved the focus from the literature in general to specific papers. He set statcheck to work on more than 50,000 papers, and posted its reports on PubPeer, an online forum in which scientists often dispute papers. That has prompted a sometimes testy debate about how such tools should be used.  The program is still very immature, but in the long run could keep scientists honest.  Hartgerink predicted that the posts would inform readers and authors about potential errors and \u201cbenefit the field more directly than just dumping a data set\u201d. Not everyone agreed. On 20 October, the German Psychological Association warned that posting false findings of error could damage researchers' reputations. And later that month, a former president of the Association for Psychological Science in Washington DC decried the rise of \u201cuncurated, unfiltered denigration\u201d through blogs and social media, and implied that posts from statcheck-like programs could be seen as harassment. Others foresee a positive change in the culture. Hartgerink and Nuijten have each received awards from organizations promoting open science. And  in a PubPeer comment  on the original statcheck paper, psychology researcher Nick Brown of the University of Groningen in the Netherlands wrote that science might benefit if researchers stopped assuming that posts on the forum indicated that there was \u201csomething naughty\u201d in a paper, and instead thought, \u201cThere's a note on PubPeer, I will read it and evaluate it like a scientist.\u201d An automated tool makes researchers more likely to double-check their work, which is good for psychology, argues Simine Vazire, who studies self-perception at the University of California, Davis. \u201cIt will catch mistakes, but even more importantly it will make us more careful.\" That seems to appeal. Several thousand people have downloaded the  free statcheck program , which works in the programming language R, or visited the web-based  statcheck.io , which requires no programming knowledge. (Researchers who want to check selected results rather than whole papers can use online calculators such as  ShinyApps .) \n               Technical check \n             Most psychology papers report statistical tests in a standardized format, with related parameters that can be checked for inconsistencies. Statcheck \u2014 which so far works only for papers in this format \u2014 identifies and inspects a few common tests that calculate  P  values, a measure of how likely results are to arise by chance if, for instance, no real difference exists between two groups (see 'What statcheck looks for'). Although statisticians  have warned against it , a  P  value below 0.05 is often used as an arbitrary determiner of 'statistical significance', allowing results to be taken seriously and published. Most of the errors that statcheck catches seem to be  typos or copy-and-pasting mistakes , says Daniel Lakens, a cognitive psychologist at Eindhoven University of Technology in the Netherlands. After reading the statcheck paper, he decided to analyse the errors it reported that changed a result's statistical significance. He found three main categories. Often, a researcher had inserted an incorrect sign, such as  P  < 0.05 instead of  P  = 0.05. In other cases, the calculations were set up to detect only particular relationships, such as positive or negative correlation, which was not always made explicit. Optimistic rounding was also common:  P  values of 0.055 reported as  P  \u2264 0.05 made up 10% of detected errors that changed statistical significance, a rate that Lakens calls depressingly high. But statcheck itself makes errors, says Thomas Schmidt, an experimental psychologist at the University of Kaiserslautern in Germany, who wrote a critique of the program ( T.SchmidtPreprintathttp://arxiv.org/abs/1610.01010;2016 ) after it flagged two of his papers. For example, it does not always recognize necessary statistical adjustments. When statcheck does detect an error, it cannot distinguish whether it is the  P  value or a related parameter that is incorrect. Schmidt says that, across the two of his papers that it scanned, statcheck failed to detect 43  P  values, checked 137 and noted 35 \u201cpotentially incorrect statistical results\u201d. Of those, 2 reflected  P -value errors that did not change significance, 3 reflected errors in other parameters but did not affect  P  values, and 30 were improperly flagged. Nuijten admits that statcheck can sometimes misidentify tests and overlook adjusted  P  values, but she notes that, in her original paper, it found similar rates of error to manual checks. Nuijten and Hartgerink have been working hard, mostly successfully, to keep conversations amiable. Nuijten has posted detailed explanations about how statcheck works, with smiley emoji and friendly exclamation marks. Hartgerink is updating PubPeer posts with an improved version of the software. Both note that anyone can add comments on PubPeer to explain statcheck's results, and that the posts state that results are not definitive. \u201cThe one thing I try to repeat over and over is that statcheck is automated software that will never be as accurate as a manual check,\u201d says Nuijten. Much of what statcheck flags up is trivial, but when authors do not respond, matters are left unresolved, says Elkan Aky\u00fcrek, a psychologist at the University of Groningen. \u201cContent-based discussion is getting a bit flooded.\u201d Thought leaders such as  neuropsychologist Dorothy Bishop  of the University of Oxford, UK, worry that posts could distract from more serious discussions, or alienate people and make them less receptive to efforts to improve reproducibility. Heiko Hecht, a psychologist at Johannes Gutenberg University in Mainz, Germany, thinks it might have the opposite effect: \u201cThe program is still very immature, but in the long run could keep scientists honest.\u201d Besides, he adds, if researchers made raw data available, anyone could check the results. Some authors have expressed gratitude for a chance to correct mistakes, although several have said that they should have the chance to review posts before they are made public. At least three have responded on PubPeer to explain errors. Two of them told  Nature  that the errors were typos that did not affect  P  values and were too trivial to justify a formal correction. As for Vazire, she hopes that automated reports will help researchers to get used to post-publication commentary. \u201cI think it will help desensitize us to criticism,\u201d she says. \n               Editor's helper \n             In July this year, the journal  Psychological Science  began running statcheck on submissions that got favourable first reviews, and discussing flagged inconsistencies with the authors. \u201cI thought there might be some blowback or resistance,\u201d says editor-in-chief Stephen Lindsay. \u201cReaction has been almost non-existent.\u201d Of the few dozen runs so far, none of the errors has been egregious, he says, although there have been at least two instances in which authors have reported a  P  value as 0.05 when it was 0.054. Lindsay says that statcheck reports are too confusing to share with authors directly. (For example, the program flags potential errors with the word TRUE.) Nuijten says that an upcoming version will be much more comprehensible to non-programmers. Meanwhile, she says, her team has been talking to publishers Elsevier and PLOS about adopting the program at their titles. And statcheck may soon have company: a more-comprehensive commercial program called StatReviewer is under development by other researchers. It is designed to analyse papers from a variety of fields, not just to double-check calculations but also to ensure that reporting requirements are followed. Lindsay hopes that statcheck's utility will fade over time as researchers stop manually entering statistical outcomes into their manuscripts; instead, the values would be directly inserted by the programs that produced them, and linked to their scripts. \u201cThe methodological leaders are using things like  R markdown ,\u201d he says. As for Schmidt, he thinks that statcheck could be useful in manuscript preparation, but it is not for beginners. \u201cThe greatest risk during prepublication is that unsophisticated users overestimate the program, relying blindly on its output.\u201d Lakens is sticking to a manual system: one author of a paper does the analyses, and another checks them. That can detect errors that statcheck will not, such as transposing results. That approach makes sense to Nuijten. Her goal was never to fix statistical analysis. Statcheck is more like a standard spellchecker, she says: \u201ca handy tool that sometimes says stupid things\u201d. People laugh at the absurdities, but still use the tool to correct mistakes. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Smart software spots statistical errors in psychology papers 2015-Oct-28 \n                   \n                     Statistics: P values are just the tip of the iceberg 2015-Apr-28 \n                   \n                     Psychology journal bans P values 2015-Feb-26 \n                   \n                     Science joins push to screen statistics in papers 2014-Jul-03 \n                   \n                     Scientific method: Statistical errors 2014-Feb-12 \n                   \n                     Nature Web Collection: Statistics for biologists \n                   \n                     Statcheck R package \n                   \n                     Statcheck on the Web \n                   \n                     ShinyApps \n                   Reprints and Permissions"},
{"file_id": "nature.2015.19102", "url": "https://www.nature.com/articles/nature.2015.19102", "year": 2016, "authors": [{"name": "Elizabeth Gibney"}], "parsed_as_year": "2006_or_before", "body": "Peer-review platforms built around online pre-print repositories spread to astrophysics. An astrophysicist has launched a low-cost community peer-review platform that circumvents traditional scientific publishing \u2014 and by making its software open-source, he is encouraging scientists in other fields to do the same. The  \n Open Journal of Astrophysics \n  works in tandem with manuscripts posted on the pre-print server arXiv. Researchers submit their papers from arXiv directly to the journal, which evaluates them by conventional peer review. Accepted versions of the papers are then re-posted to arXiv and assigned a DOI, and the journal publishes links to them. By piggybacking on or 'overlaying' the arXiv repository, the journal should operate at a fraction of the cost of traditional publishers and will be free for both readers and authors, says journal founder and editor-in-chief, Peter Coles, an astrophysicist at the University of Sussex in Brighton, UK. He  announced  on 22 December that the journal was open for submissions. It will go live later this month, once its first papers have undergone review. Development of the software that powers the journal's peer-review system was led by Arfon Smith, chief scientist at the popular code repository GitHub. Because the software is open-source and  available at GitHub,  Coles hopes that researchers in other fields will adopt the same platform to create their own open journals. \u201cJust cross out 'astrophysics' and write 'condensed matter' or anything else, and you\u2019ve got your open journal,\u201d he says. Similar overlay journals already exist in computer science and mathematics; Tim Gowers, a mathematician at the University of Cambridge, in the UK,  launched one high-profile example,\u00a0 Discrete Analysis , in September . But the  Open Journal of Astrophysics  is thought to be the first of its type in physics. Gowers says he is excited that the platform behind it is open-source \"since it potentially reduces the costs for others even further\". \n               The overlay model \n             Coles believes that traditional journals and their associated costs are no longer needed in fields such as astrophysics and cosmology, because most researchers already both submit their work to arXiv and read papers on it. \u201cThe only objection to just putting things on arXiv is that it\u2019s not peer reviewed, so why not have a community-based effort that provides a peer-review service for the arXiv?\" he says \u2014 pointing out that academics already carry out peer review for scientific publishers, usually at no cost. Coles himself covered the costs of developing the software platform for the journal, amounting to a few thousand pounds, he says. ( Discrete Analysis  licenses different software and is helped by a grant from the University of Cambridge, UK.) GitHub is covering the costs of hosting the platform, so the only remaining expense is editors\u2019 and reviewers\u2019 time, which they give up voluntarily, says Coles. If the experiment proves successful and the volume of papers balloons, the journal may eventually have to charge authors a handling fee of a few tens of pounds, he adds. (The journal also relies on the continued existence of arXiv, whose running costs amount to less than $10 per paper). The journal does not have the resources to offer services provided by conventional journals, such as heavy editing of papers. Instead, poorly written articles will be rejected and the authors referred to a list of professional copy-editing services, Coles says. \n               Gaining traction \n             Gowers welcomes the new journal; the arXiv overlay model is much more likely to succeed, he says, if many examples of it can be seen to be working. The journal has amassed an editorial board with high-profile physicists including Pedro Ferreira, a theorist at the University of Oxford, UK, and Andrew Jaffe, a cosmologist at Imperial College London. But astrophysicists will not necessarily jump to publish in Coles' journal. Ewine van Dishoeck, an astrophysicist at the Leiden Observatory in the Netherlands, says she, for one, is unlikely to submit her work there. \"We have a small number of well established and high quality journals in astronomy that everyone respects,\" she says. Papers in astrophysics are effectively open already, van Dishoeck points out, because anyone can view pre-print manuscripts immediately on the arXiv, while journals in the field make final accepted versions open after a delay \u2014 typically 12 months after publication. An issue for researchers can be slow peer-review of papers, she adds, but the  Open Journal of Astrophysics  has yet to prove it can be faster. Whatever their costs, the main problems facing all new journals hoping to achieve traction among researchers are ensuring speed and editorial fairness, adds Andrew King, a cosmologist at the University of Leicester, UK. \"Reliability \u2014 and particularly\u00a0fairness \u2014 are very hard to guarantee,\" he says, pointing out that the backing of long-lived organizations with a stake in the future of a field, such as learned societies, is often crucial to a journal's success. \n                 Tweet \n                 Follow @NatureNews \n                 Follow @LizzieGibney \n               \n                     Leading mathematician launches arXiv 'overlay' journal 2015-Sep-15 \n                   \n                     Open access: The true cost of science publishing 2013-Mar-27 \n                   \n                     Mathematicians aim to take publishers out of publishing 2013-Jan-17 \n                   Reprints and Permissions"},
{"file_id": "529115a", "url": "https://www.nature.com/articles/529115a", "year": 2016, "authors": [{"name": "Dalmeet Singh Chawla"}], "parsed_as_year": "2006_or_before", "body": "Creators of computer programs that underpin experiments don\u2019t always get their due\u00a0\u2014 so the website Depsy is trying to track the impact of research code. For researchers who code, academic norms for tracking the value of their work seem grossly unfair. They can spend hours contributing to software that underpins research, but if that work does not result in the authorship of a research paper and accompanying citations, there is little way to measure its impact. Take Klaus Schliep, a postdoctoral researcher who is studying evolutionary biology at the University of Massachusetts in Boston. His Google Scholar page lists the papers that he has authored \u2014 including his top-cited work, an article describing phylogenetics software called phangorn \u2014 but it does not take into account contributions that he has made to other people\u2019s software. \u201cCompared to writing papers, coding is treated as a second-class activity in science,\u201d Schliep says. Enter  Depsy , a free website launched in November 2015 that aims to \u201cmeasure the value of software that powers science\u201d. Schliep\u2019s profile on that site shows that he has contributed in part to seven software packages, and that he shares 34% of the credit for phangorn. Those packages have together received more than 2,600 downloads, have been cited in 89 open-access research papers and have been heavily recycled for use in other software \u2014 putting Schliep in the 99th percentile of all coders on the site by impact. \u201cDepsy does a good job in finding all my software contributions,\u201d says Schliep. Depsy\u2019s creators hope that their platform will provide a transparent and meaningful way to track the impact of software built by academics. The technology behind it was developed by Impactstory, a non-profit firm based in Vancouver, Canada, that was founded four years ago to help scientists to track the impact of their online output. That includes not just papers but also blog posts, data sets and software, and measuring impact by diverse metrics such as tweets, views, downloads and code reuse, as well as by conventional citations. In effect, Depsy recognizes the \u201cunsung heroes\u201d of scientific software, says Jason Priem, co-founder of Impactstory, which is funded by the US National Science Foundation and various philanthropic foundations. Such a tool is needed, notes Neil Chue Hong, founding director of the Software Sustainability Institute in Edinburgh, UK, because there are few ways to credit scientists for their software. Young researchers are enthusiastic about coding, he says. Last year, he ran  a survey of 1,000 randomly selected UK scientists , which suggested that more than 50% of researchers develop their own code. Even so, few UK academics listed code or software as one of their research outputs in the nation\u2019s latest research quality audit (the \u2018Research Excellence Framework\u2019) even in disciplines such as computer science that rely heavily on software. \u201cThere is a culture that reinforces the idea that producing and publishing code has no perceived benefit to the researcher,\u201d Hong says. \n               Tracking software use \n             The usual way to track academic impact \u2014 by counting citations \u2014 still has some relevance to software. Researchers can write papers that describe their software, as Schliep has done for his phangorn package, so that anyone who uses the program can cite it in subsequent papers. But counting citations is an imperfect measure. Researchers may not know which paper to cite, argues Priem, because software packages often have multiple articles associated with them \u2014 and some pivotal software projects, he says, such as the GDAL Python library, are not linked to a canonical paper. If software has no associated paper, there is no universally recognized way to cite it. Still, it is now quite common for coders to assign digital object identifiers (DOIs) to their code, and increasingly to their data sets as well, notes Martin Fenner, technical director of the online repository DataCite in Hanover, Germany. Software is often first stored in the popular code repository GitHub, from which a copy can be automatically archived on scholarly focused repositories such as Zenodo or Figshare, which allocate DOIs to software and thus make it a citable object. Other initiatives are trying to ensure that research papers cite software in a standardized format \u2014 such as by using the  Research Resource Identifier . But counting citations of software DOIs, papers or any other standard format does not reveal the full impact of coders on science, because software so often goes uncited. A  2015 analysis of 90 random biology papers  found that two-thirds informally mentioned the use of software, but fewer than half of those papers actually cited the package. Depsy searches through research papers to discover both citations and informal mentions of software \u2014 of which, unsurprisingly, it has found many, says Priem, such as in the acknowledgement sections or the main text of academic papers. But a limitation of the site, Priem admits, is that it currently searches only open-access research papers \u2014 missing the vast bulk of paywalled scholarly content. Impactstory will, however, negotiate with publishers for permission to mine the text of paid-access literature. Mentions in research papers are one of three ways in which Depsy tracks the impact of software, Priem says. Second, the site tracks how code is reused by others. The name Depsy originates from \u2018dependency network\u2019 \u2014 an overarching term for a map of factors that depend on each other, such as software packages that recycle code from other packages. Depsy calculates the extent to which code is recycled by using Google\u2019s PageRank algorithm, which gives weight to reuse by more-prominent software. From the view of measuring impact, an example of code reuse may be more meaningful than a citation in the literature, Priem notes. And third, the site gathers download statistics on code packages by trawling through CRAN and PyPI, which are the main repositories for software written in the popular R and Python programming languages, respectively. \n               Focus on research \n             Other websites do some of what Depsy offers.  Crantastic , for example, is a review site that tracks the most popular R packages, and  PyPI ranking  lists the most popular Python modules by tracking downloads from PyPI. In addition, a few commercial services such as  VersionEye  and  Libraries.io  track dependency networks, explaining which software depends on which other packages. But Depsy is unconventional in its focus on research software, which it distinguishes from other code by identifying key words and the descriptions and titles of software \u2014 although the classification process is imperfect, Priem says. The site tracks other code, but it includes research software only when it calculates the percentile impact rankings for academics such as Schliep. Depsy apportions fractional credit to each participant who has contributed to a software package by counting the percentage of code that they have contributed or edited \u2014 known in the programming world as a person\u2019s \u2018commits\u2019. Fingerprints of each commit are saved in the code, making it easy to track down the originator. But not every edit has the same impact, and Depsy currently cannot distinguish between important contributions and trivial ones. The tool may be adapted to attempt this distinction \u2014 by tracking the influence of individual commits \u2014 in the future, says Priem. Depsy also enables users to determine the software with the highest impact in specific disciplines. A  search on Depsy for \u2018astrophysics\u2019 , for instance, yields 11 software packages, of which an analysis and visualization toolkit for astrophysical simulations called \u2018yt\u2019 has the greatest impact; it lies in the 97th percentile of all packages. \n               Obstacles to progress \n             One of Depsy\u2019s restrictions, notes Hong, is that it only tracks code that is available in public repositories \u2014 so it cannot show the impact of commercial software. Moreover, the site tracks software in only two coding languages: R and Python. But Depsy\u2019s creators aim to eventually include other coding languages, and to add a fourth way to measure impact: a social-influence metric that would take into account the number of stars that software packages receive from other GitHub users, and how many times a piece of software is discussed online. The site\u2019s code-reuse metrics have their limitations, too. Researchers often reuse their own code, but might \u2018game\u2019 Depsy by repeatedly doing so to garner better profile scores \u2014 the software equivalent of citing your own paper. Another way for researchers to game the site might be to start lots of projects but not to finish them, Fenner warns, leaving others to refine them instead; the project originator could then claim credit after the fine-tuned versions of their software become prominent. \u201cI would love to get to the place where people are trying to game Depsy, because it would mean people are taking software reuse seriously,\u201d Priem says. Ultimately, transparent metrics that demonstrate the impact of code might enable software creators to secure larger funds during grant reviews, Hong hopes. Science\u2019s coders deserve more funding and support, he says \u2014 but getting to that point requires a culture change from everyone involved in scientific research. \u201cThe real irony is that by not rewarding the use of software, we\u2019re actually putting roadblocks in the way of science,\u201d Hong says. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Motion studies: See how they run 2015-Sep-01 \n                   \n                     Researchers argue for standard format to cite lab resources 2015-May-29 \n                   \n                     Open science decoded 2015-Apr-30 \n                   \n                     Programming: Pick up Python 2015-Feb-04 \n                   \n                     My digital toolbox: Climate scientist Damien Irving on Python libraries 2015-Jan-30 \n                   \n                     Programming tools: Adventures with R 2014-Dec-29 \n                   \n                     Blog post: Get more out of your research data \n                   \n                     More about Depsy \n                   \n                     Software Sustainability Institute \n                   \n                     Blog about Depsy \n                   \n                     More about Depsy \n                   Reprints and Permissions"},
{"file_id": "531128a", "url": "https://www.nature.com/articles/531128a", "year": 2016, "authors": [{"name": "Daniel Cressey"}], "parsed_as_year": "2006_or_before", "body": "\u2018Experimental Design Assistant\u2019 helps to give feedback on research plans. A  free online tool  that visualizes the design of animal experiments and gives critical feedback could save scientists from embarking on poorly designed research, the software's developers hope. Over the past few years, researchers have picked out  numerous flaws in the design and reporting of published animal experiments , which, they warn, could lead to bias. In response, hundreds of journals have agreed to  voluntary guidelines for reporting animal studies : checklists of best practice, such as what statistical calculations to use to ward off error. But these lists kick in after scientists submit a paper, says Nathalie Percie du Sert, who specializes in experimental design at the National Centre for the Replacement, Refinement and Reduction of Animals in Research (NC3Rs) in London. \u201cWhen you get to the reporting stage, that's a bit too late,\u201d she says. \u201cWe want researchers to think about these issues at the design stage.\u201d Percie du Sert's solution is a programme called the  Experimental Design Assistant  (EDA), which launched in October 2015. She hopes that it will help to improve the quality of animal research and perhaps even become an integral part of the conduct of animal studies. The EDA allows scientists to create a visual representation of an experiment by laying out its key elements \u2014 hypothesis, experimental method and planned analysis \u2014 in logically connected, coloured boxes. The software then uses a built-in set of rules to spot potential problems, and suggests refinements. These may be simple \u2014 the researcher hasn't specified how to randomize animals to the control or treatment arm \u2014 or more complex: there are potential confounding variables in the control and trial arms. The tool can also assist scientists with calculating the sample size needed to ensure a statistically robust result, or with randomization. There's nothing fundamentally new in the EDA, says Percie du Sert. It builds on existing knowledge of good experimental design. But it can aid scientists who have little training in the area, she says, and teach them design choices. Since the EDA's launch, around 400 accounts have been created to use it, producing between 50 and 100 experimental diagrams in total each month, says Percie du Sert. She does not have access to detailed information about its users; the sensitivities around animal research and the need to protect researchers mean that data on who is using it, and how, are secured. The Wellcome Trust's Sanger Institute, a genome-research centre in Cambridge, UK, is rolling out an internal training programme that includes lessons on design and use of the EDA; the agency is encouraging staff to use the software to present experiments to ethical-review committees, says Natasha Karp, a biostatistician at the institute. Karp took part in the working group that oversaw the tool's development, and says that she uses it to visualize the experiments of the biologists whom she supports. The EDA is not the only software that aims to improve research quality and reproducibility. Other tools check manuscripts before publication for issues such as errors in formatting or omission of  P  values. These include  Penelope , a paid-for service aimed at journal publishers; another tool called WebCONSORT (which is not yet freely available) is  being tested as a way to improve reporting of clinical trials .  Protocol Navigator , a free web application created by scientists at Cardiff University, UK, also produces visual experiment maps that can be shared. But the EDA specifically targets animal research, and as such, is unique in its ability to give a rapid overview of the design and analysis of animal experiments, says Karp. \u201cThere isn't anything else quite like this system.\u201d Percie du Sert hopes that a visual representation of experiments could become common practice, used in research papers or lab meeting presentations. Eventually, the EDA might even produce time-stamped versions to prove that an experiment was conducted and analysed as designed, she adds, rather than being the product of a scientist searching for meaning in data after the fact \u2014 a frowned-upon practice sometimes called  HARKing ('hypothesizing after the results are known') . The online tool can seem a little complicated, says Jeffrey Mogil, who studies pain at McGill University in Montreal, Canada. \u201cBut I actually think that people might get a big kick out of using this,\u201d he says. \u201cIt looks like a cool way to break in new grad students or teach the scientific method to undergrads.\u201d \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     Surge in support for animal-research guidelines 2016-Feb-01 \n                   \n                     UK funders demand strong statistics for animal studies 2015-Apr-15 \n                   \n                     Nature Toolbox \n                   \n                     Experimental Design Assistant (EDA) \n                   Reprints and Permissions"},
{"file_id": "nature.2016.21191", "url": "https://www.nature.com/articles/nature.2016.21191", "year": 2016, "authors": [{"name": "Kendall Powell"}], "parsed_as_year": "2006_or_before", "body": "Recommendation engine Instrumentl aims to speed grant searching. When Angela Braren met Katharine Corriveau in 2012 the two former scientists realized that they had a lot in common. Both had left research, and partly for the same reason: they spent much more time fundraising for their science than actually doing it. Shared frustration quickly morphed into a shared vision. The two women joined up with computer scientist Gauri Manglik in San Francisco, California, and this August the team launched  Instrumentl , a web- and e-mail-based service to match researchers with funding opportunities. Originally, scientists had to pay for a subscription to access the site after a two-week trial, but an updated version of the software with a free account option went live on 28 November. Since then, around 1,000 scientists a week have signed up, says Braren, a former fisheries biologist and Instrumentl's chief executive officer. The software has more than 10,000 users, the majority of whom have free access. \u201cWe realized that bringing technology to this grants space \u2014 which hasn\u2019t really seen much innovation \u2014 could make a big impact,\u201d says Corriveau, a former agriculture researcher who is now chief operating officer at Instrumentl. Individual investigators, who typically default to browsing large, national funding-agency websites or Google, needed better tools to sift through the millions of dollars on offer, she says. \n             Music to their ears \n           Early users are enthusiastic, likening the website's approach to music recommendation engines. \u201cThe concept of a sort of Pandora for grants is fantastic,\u201d says Erika Wright, a current user and research administrator at Humboldt State University in Arcata, California. As a recommendation engine, Instrumentl \u2018learns\u2019 user preferences, and refines subsequent suggestions accordingly. Wright typically searches through websites such as  grants.gov , or databases such as  SPIN,  to find programmes for faculty researchers at Humboldt State. \u201cInstrumentl takes something that currently takes a lot of time and effort to do manually and automates it,\u201d she says. Ecology doctoral student Lauren Satterfield, who signed up for a free account in late November, describes Instrumentl as \u201ca dating site for researchers and grant agencies\u201d. It took her about five minutes to set up her profile with demographic information and details about her field of study, specific research project and budget requirements, she says; within 24 hours, Instrumentl had sent 40 grant matches to her inbox. \u201cIt\u2019s hard enough to find a list of grants, but Instrumentl gives you a list of grants that match your project. Then I can pull from there,\u201d says Satterfield, who studies wolf\u2013cougar interactions at the University of Washington in Seattle. Each summary profile includes basics about a grant programme \u2014 amount offered, deadlines, programme overview, eligibility, and preferences. Users can click \u2018Save\u2019, \u2018Not sure yet\u2019 or \u2018Not interested\u2019 and Instrumentl promises that its matching algorithm will incorporate their feedback in future suggestions. \n             Affordable options \n           At present, Instrumentl covers non-human biology (including zoology, botany and marine science), ecology, evolution, agriculture, soil science, Earth science, palaeontology and environmental science. Its database includes 7,000 funding opportunities, mostly from US-based funders; these include national, state and local governments, corporations, clubs, societies and private foundations. The Free Basic plan suits one researcher looking for grants that have application deadlines within the next 90 days, for one research project. An Individual plan (US$16 per month) supports one researcher looking for grants with application deadlines within the next 12 months for two projects, and a Group plan ($49 per month) supports multiple researchers and up to five projects. Databases such as SPIN and  Pivot  provide broader coverage: SPIN includes 40,000 funding opportunities. Yet these services are largely designed for use by institutions. Access is costly and simple keyword searches often return long lists of poorly curated hits. Combing those lists for appropriate matches takes time that most researchers don\u2019t have, says David Trinkle, director of research development at the University of California, Berkeley. Trinkle says that, compared with many grant databases that he mines, the output from Instrumentl is easier to understand and therefore likely to increase researchers\u2019 efficiency. And Instrumentl\u2019s \u201c21st-century approach\u201d exposes investigators to non-conventional funding opportunities that they might not otherwise see, he adds. However, Trinkle would like to see the tool developed to cover more disciplines and opportunities. Braren says the service plans to add human-biology fields in 2017 and, eventually, all natural sciences. Also absent are details on grant success rates and funding priorities, says Zoe Donaldson, an assistant professor of neuroscience at the University of Colorado Boulder. Donaldson does not use Instrumentl and relies for her grant searches on e-mails from the free funding-opportunity aggregator service  Trialect , as well as on departmental listings and her social network. Still, Instrumentl has the potential to save researchers\u2019 time and boost the number of grant submissions they make, Wright says. Both these benefits represent strong returns on an investment in today\u2019s ultra-competitive funding environment. Wright encourages faculty members to sign up for free accounts to try out the product. But it\u2019s too early to tell how many researchers will migrate to fee-based tiers. \u201cAfter all,\u201d says Donaldson, \u201cI can\u2019t pay for that out of my grant funds.\u201d \n                   Hard work, little reward: Nature readers reveal working hours and research challenges 2016-Nov-04 \n                 \n                   Quest for the holy grant 2016-Jun-29 \n                 \n                   Column: It takes time and a team to win grants 2014-Nov-05 \n                 \n                   Nature special: philanthropy in science \n                 Reprints and Permissions"}
]