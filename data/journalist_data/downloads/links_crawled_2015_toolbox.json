[
{"file_id": "522115a", "url": "https://www.nature.com/articles/522115a", "year": 2015, "authors": [{"name": "Nadia Drake"}], "parsed_as_year": "2006_or_before", "body": "Why cloud computing is attracting scientists \u2014 and advice from experienced researchers on how to get started. In February, computer scientist Mark Howison was preparing to analyse RNA extracted from two dozen siphonophores \u2014 marine animals closely related to jellyfish and coral. But the local high-performance computer at Brown University in Providence, Rhode Island, was not back up to full reliability after maintenance. So Howison fired up Amazon's Elastic Compute Cloud and bid on a few 'spot instances' \u2014 vacant computing capacity that Amazon offers to bidders at a discounted price. After about two hours of fiddling, he had configured a virtual machine to run his software, and had uploaded the siphonophore sequences. Fourteen hours and US$61 later, the analysis was done. Researchers such as Howison are increasingly renting computing resources over the Internet from commercial providers such as Amazon, Google and Microsoft \u2014 and not just for emergency backup. As noted in a 2013 report sponsored by the US National Science Foundation (NSF) in Arlington, Virginia, the cloud provides labs with access to computing capabilities that they might not otherwise have (see  go.nature.com/mxh4xy ). Scientists who need bursts of computing power \u2014 such as seismologists combing through data from sensors after an earthquake or astronomers processing observations from space telescopes \u2014 can rent extra capacity as needed, instead of paying for permanent hardware. Scientists can configure their cloud environment to suit their requirements. Although cloud computing cannot handle analyses that require a state-of-the-art supercomputer or quick communication between machines, it may be just right for projects that are too big to tackle on a desktop, but too small to merit a high-performance supercomputer. And working online makes it easy for teams to collaborate by sharing virtual snapshots of their data, software and computing configuration. But shifting science into the cloud is not a trivial task. \u201cYou need a technical background. It's not really designed for an end user like a scientist,\u201d says Howison. Although the activation energy might be high, there are recommended routes for scientists who want to try setting up a cloud environment for their own research group or lab. \n               A DIY guide to cloud computing \n             Most cloud platforms require users to have some basic computing skills, such as an understanding of how to work in the command line, and a familiarity with operating systems and file structures. Once researchers have a strong foundation, the next step is to try working in a cloud. The most user-friendly cloud for scientists, says plant biologist Andreas Madlung, could be the platform Atmosphere, which was created as part of a collaborative cyber infrastructure project called iPlant. Funded by the NSF and led by three US universities and the Cold Spring Harbor Laboratory in Long Island, New York, iPlant has been helping scientists to share software and run free analyses in the cloud since 2008. Designed with scientists in mind, the platform's interface comes with pre-loaded software, a suite of practice data sets and discussion forums for users to help each other to tackle problems. Madlung, at the University of Puget Sound in Tacoma, Washington, teaches an undergraduate bioinformatics course that includes a section on cloud computing. He first introduces his students to the Unix operating system, then has them use that knowledge to analyse RNA sequence data on Atmosphere. Those who sign up with iPlant are automatically given what equates to around 168 hours of processing time a month, and can request more if needed. Users can load up virtual machines with any extra software that they need, and if a job is too much for standard equipment to handle, tasks can be offloaded to a supercomputer at the Texas Advanced Computing Center in Austin, where iPlant has a guaranteed allocation. Biologist Mike Covington of the University of California, Davis, shifted his lab's computing work to iPlant after its servers kept crashing because they were overloaded. He has also made copies ('images') of his own virtual machine, so that his collaborators \u2014 and any iPlant user \u2014 can log in and access the same software, data and computing configuration. \u201cIf I spend several hours setting up my virtual machine perfectly for  de novo  genome assembly [reconstructing full-length sequences from short fragments of DNA], I can quickly and easily make it available to any other scientist in the world that wants to do  de novo  assembly with their own data,\u201d Covington says. Such virtual snapshots may become standard for projects that require computational work. Anyone who wants to reproduce, for example, the microbial-genome analysis described in one paper can access a snapshot of the authors' virtual machine on the Amazon cloud, simply by paying for Amazon computing time (B. Ragan-Kelley  et al .  ISME J.   7 , 461\u2013464; 2013 ). \n               Pick a cloud \n             For some researchers, choosing a cloud is straightforward. Scientists at CERN, Europe's particle-physics laboratory near Geneva, Switzerland, have had access to a massive internal cloud running on the software platform OpenStack since 2013. A handful of institutions, such as Cornell University in New York and the University of Notre Dame in Indiana, have developed computing clouds, too. Some, including Notre Dame, outsource their clouds to companies such as Rackspace Private Cloud, a multi-national firm in San Antonio, Texas, that sets up and manages cloud services for users. But for scientists who are not at an institution with a fully functional campus cloud, bushwhacking through the jungle of cloud options can be a frustrating adventure (see \u2018A guide for the perplexed\u2019). Cloud system set-up can vary, and proficiency with one provider does not guarantee an easy transition to others. \n               boxed-text \n             Casey Dunn, an evolutionary biologist who works with Howison at Brown University, prefers to train students on commercial platforms. \u201cWhen they go on to a postdoc somewhere else or start their own lab, they'll still be able to log into Amazon,\u201d he says. Somalee Datta, the director of bioinformatics at Stanford University's Center for Genomics and Personalized Medicine in California, is using Google's cloud platform to support the centre's enormous amount of genomics data and computing demand, rather than relying only on the servers available at Stanford. She chose Google, she says, for several reasons: the company's developers were actively making tools available for genomics researchers, Google had demonstrated interest in health-care research \u2014 and the price was right. \n               Cloud concerns \n             For Datta and others, one key issue surrounding cloud computing is security. \u201cIt's a big concern,\u201d she says. \u201cHackers understand where the value is, and they will turn their attention towards that.\u201d Still, Datta thinks that clouds are no more or less secure than any other computer network. A university cloud system, for example, is only as solid as the university's firewall. \u201cIf I were working on my own or at a small college or company, I would probably feel more secure with Google's cloud,\u201d Datta says (although Stanford has its own army of engineers watching security). The truth is, anyone working with extremely sensitive data might be better off keeping it away from the Internet altogether. Another key issue for researchers who are venturing into cloud computing is the level of tech support needed. Getting software to run on a new system can take days, and determining how much computing power or memory a virtual machine needs can be an exercise in trial and error. All cloud providers offer training and tutorials, but dedicated support staff are more commonly found at universities with campus clouds. Despite the challenges, cloud computing is increasingly appealing to scientists, says Darrin Hanson, vice-president of Rackspace Private Cloud. \u201cThe last few years have been mostly people who are absolutely out on the bleeding edge,\u201d he says. \u201cBut now we're starting to see an influx of adopters.\u201d That isn't too surprising, Dunn says \u2014 the cloud is not as foreign as it can sometimes sound. \u201cNearly all consumer computer products now have a cloud component, be it mobile apps, content-streaming services like Netflix or desktop tools like Dropbox,\u201d he says. \u201cResearch computing is not on the vanguard of some crazy and risky unknown frontier \u2014 we are just undergoing the same transitions that are already well under way in industry and the consumer marketplace.\u201d \n                     Programming: Pick up Python 2015-Feb-04 \n                   \n                     Programming tools: Adventures with R 2014-Dec-29 \n                   \n                     Interactive notebooks: Sharing the code 2014-Nov-05 \n                   \n                     Cloud computing beckons scientists 2014-May-27 \n                   \n                     Head in the clouds 2007-Oct-24 \n                   \n                     Nature Toolbox \n                   \n                     Interactive Demo: iPython Notebook \n                   \n                     2013 XSEDE Cloud Survey Report \n                   Reprints and Permissions"},
{"file_id": "528153a", "url": "https://www.nature.com/articles/528153a", "year": 2015, "authors": [{"name": "Jeffrey M. Perkel"}], "parsed_as_year": "2006_or_before", "body": "Scientific publishers are forging links with an organization that wants scientists to scribble comments over online research papers. Would researchers scrawl notes, critiques and comments across online research papers if software made the annotation easy for them? Dan Whaley, founder of the non-profit organization  Hypothes.is , certainly thinks so. Whaley's start-up company has built an open-source software platform for web annotations that allows users to highlight text or to comment on any web page or PDF file. And on 1 December, Hypothes.is  announced partnerships  with more than 40 publishers, technology firms and scholarly websites, including Wiley, CrossRef, PLOS, Project Jupyter, HighWire and arXiv. Whaley hopes that the partnerships will encourage researchers to start annotating the world's online scholarship. Scientists could scribble comments on research papers and share them publicly or privately, and educators could use annotation to build interactive classroom lessons, he says. If the idea takes off, some enthusiasts suggest that the ability to annotate research papers online might even change the way that papers are written, peer reviewed and published. Hypothes.is, which was founded in 2011 in San Francisco, California, and is supported by philanthropic grants, has a bold mission: \u201cTo enable conversations over the world's knowledge.\u201d But the concept it implements, online annotation, is as old as the web itself. The idea of permitting readers of web pages to annotate them dates back to 1993; an early version of the Mosaic web browser had this functionality. Yet the feature was ultimately discarded. A few websites today have inserted code that allows annotations to be made on their pages by default, including the blog platform  Medium , the scholarly reference-management system  F1000 Workspace  and the news site  Quartz . However, annotations are visible only to users on those sites. Other annotation services, such as  A.nnotate  or Google Docs, require users to upload documents to cloud-computing servers to make shared annotations and comments on them. Hypothes.is is not the only service that wants to make it easy for users to leave annotations across the entire web. A competing offering is a web annotation service from  Genius , a start-up firm that began as a site for annotating rap lyrics. In April, it launched services such as browser plugins to help users to annotate any web page. But unlike Hypothes.is, the Genius code is not open-source, its service doesn't work on PDFs, and it is not working with the scholarly community. On the scholarly side, the reference-management tool ReadCube makes it possible for users to annotate PDFs of papers viewed on a ReadCube web reader \u2014 but that software is proprietary. (ReadCube is owned by Digital Science, a firm operated by the Holtzbrinck Publishing Group, which also has a share in  Nature 's publisher.) By contrast, the open-source nature of the Hypothes.is platform means that anyone could use it to create their own annotation reader or writer \u2014 just as anyone can create their own web browser using standards-based technology. The company is also a member of a working group within the World Wide Web Consortium, the standards body for the web, which is developing a universal standard for annotations and how they are communicated. The hope is that web pages that allow annotations would all adopt the same underlying code and protocols (as they do with hyperlinks, for example), making the function easier to use and interact with. The working group has released a draft version of its standard, which is expected to be finalized by the end of 2016. \n               How it works \n             For now, Hypothes.is users have several options for creating and viewing annotations. These include bookmarklets (a simple program within a browser bookmark), browser plugins or adding 'via.hypothes.is/' to the start of any URL. [To see public annotations on this article in hypothes.is, for example, visit  https://via.hypothes.is/http://www.nature.com/news/annotating-the-scholarly-web-1.18900 .] When a Hypothes.is user opens a page \u2014 a scholarly article, for instance \u2014 the web browser shows any annotations to which the user has access. These appear as highlighted words and comments on top of the text, like an overlaid transparency. Users can then add their own comments, similar to a student marking up a textbook. These are public by default but can be made private, and, following an update added on 3 November, annotations can be shared with private groups. That should enable the tool to be used for journal clubs, classroom exercises and even peer review. If a page has been altered since an annotation was made, the software uses 'fuzzy' logic to map annotations to their approximate original location. The system can also map annotations from HTML to PDF and back again (for instance, if a user annotates the web version of an article and subsequently views a PDF of the same document). Annotations are stored on a dedicated Hypothes.is server, which Whaley says looks set to log around 250,000 comments from some 10,000 users in 2015. For instance, after Hurricane Patricia in October, climate scientists left comments and highlighted text on a widely shared  mashable.com  article (see  go.nature.com/rcsesf ). But publishers that wish to host annotations for their own content, or companies that want to annotate corporate documents behind a firewall, could run their own server using the same software platform, Whaley adds. \n               Publisher partnerships \n             A Hypothes.is user can already annotate any web page \u2014 including research papers and pay-to-view articles to which they have access. But the formal partnership announced this week sees some publishers working harder to encourage annotation, including tackling content that annotation systems stumble over, such as page frames and embedded page readers. The digital library JSTOR, for example, is developing a custom Hypothes.is tool for its educational project with the Poetry Foundation, a literary organization and publisher in Chicago, Illinois. Alex Humphreys, who is director of JSTOR Labs in New York City, says that teachers will be able to use the tool to annotate poems with their classes. An instructor selects the poem to be annotated, sets up a dedicated page with a copy of it, and restricts access to their class only. Students can then create personal notes or share them with the group; an extra annotation layer finds the scholarly resources in JSTOR that quote each line of poetry and provides links out to those resources. The tool is slated to launch in mid-December, Humphreys says. The scientific publisher eLife in Cambridge, UK, has been testing the feasibility of using Hypothes.is to replace its peer-review commenting system, says Ian Mulvany, who heads technology at the firm. The publisher plans to incorporate the annotation platform in a site redesign instead of its current commenting system, Disqus. At a minimum, says Mulvany, Hypothes.is provides a mechanism for more-targeted commentary \u2014 the equivalent of moving comments up from the bottom of a web page into the main body of the article itself. Another partner, the arXiv preprint service run by Cornell University Library in Ithaca, New York, has been working on making annotations flow across multiple article versions, says information scientist Simeon Warner, who leads technology development for arXiv. To jump-start interest in the annotation program, arXiv has been converting mentions of its articles in external blog posts (called trackbacks) into annotations that are visible on an article's abstract page  when using Hypothes.is . \n               Not just graffiti \n             Hypothes.is plans improvements to its platform that include a way to validate the identities of commenters, by incorporating researchers' unique ORCID digital profiles. That could go a long way towards improving adoption of the system among scholars, by facilitating expert commentary on published works and filtering out unwanted marginalia, says Paul Ginsparg, the founder of arXiv and a physicist at Cornell University. \u201cIf people start looking at articles and they see the equivalent of graffiti, then people will turn off the comments and the experiment will fail,\u201d he says. If it takes off, online annotation could represent a fundamental shift in the way scholarly communication is done, adds Cameron Neylon, part of the research team at the Centre for Culture and Technology at Curtin University in Perth, Australia, who formerly worked at PLOS. At the moment, Neylon explains, the scholarly publishing process involves ferrying a document from place to place. Researchers prepare manuscripts, share them with colleagues, fold in comments and submit them to journals. Journal editors send copies to peer reviewers, returning their comments to the author, who goes back and forth with the editor to finalize the text. After publication, readers weigh in with commentary of their own. With an open-source annotation platform, Neylon says, the document is the centre of attention. Different contributors act on the content simply by changing who has access to it and its comments, with the document becoming richer over time. \u201cYou can think of this as a fabric that allows those comments to move freely both in time and [across] versions in a way that we've never been able to do before,\u201d he says. But as Ginsparg points out, it is not clear that researchers \u2014 who have proved reluctant in repeated trials to comment on published articles \u2014 will take to annotation, even if they can share their comments privately. \u201cThere's no incentive structure for people to comment extensively, because it can take time to write a thoughtful comment, and one currently doesn't get credit for it,\u201d he says. \u201cBut it's an experiment that needs to be done.\u201d \n                 Tweet \n                 Follow @NatureNews \n               \n                     Social network launches bid to get academics chattering about papers online 2015-Sep-25 \n                   \n                     PubMed opens for comment 2013-Oct-24 \n                   \n                     Company offers portable peer review 2013-Feb-12 \n                   \n                     In search of credit 2013-Jan-02 \n                   \n                     Peer reviews: some are already public 2011-Jun-15 \n                   \n                     Nature  Toolbox hub \n                   \n                     Hypothes.is project \n                   \n                     Web annotation working group \n                   \n                     Mashable.com article on Hurricane Patricia \n                   Reprints and Permissions"},
{"file_id": "524125a", "url": "https://www.nature.com/articles/524125a", "year": 2015, "authors": [{"name": "Jeffrey M. Perkel"}], "parsed_as_year": "2006_or_before", "body": "Inventory-tracking systems range from paper filing to custom-made databases. Using the right system can save researchers time, money and frustration. When Marilyn Goudreault received a request for plasmids stored in the repository of the laboratory she manages at the Lunenfeld\u2013Tanenbaum Research Institute in Toronto, Canada, there was never any question whether she would honour it. Reagent sharing is typically a precondition of publication in peer-reviewed journals, and is fundamental to the scientific process. But first, Goudreault would have to find the plasmids \u2014 circular strings of DNA. In many labs, the task might have required a tortuous search through old notebooks, out-of-date spreadsheets and frost-encrusted freezer boxes. But in Goudreault's lab, reagents are tracked with  OpenFreezer : a free, web-based system designed to document data such as the location, source, creator and biological properties of every reagent in a user's possession \u2014 including not just plasmids, but also antibodies and stretches of DNA, RNA and protein. Goudreault needed only to run a quick search for the materials, then retrieve the indicated boxes from storage. \u201cI had everything within 15 minutes,\u201d she says. OpenFreezer is one of a number of computerized inventory systems developed to simplify lab management. They range from simple homespun databases for individual labs to enterprise-level systems, and accommodate a range of budgets. Some are designed for documenting frozen samples; others for tracking chemicals or lab animals. Some facilitate purchasing and equipment scheduling; others are limited to simple descriptions. But in all cases, the goal is to ensure that lab workers know what resources are available to them, and where to find them. Many labs track their inventories with nothing more than sheets of paper in a binder or entries in an Excel spreadsheet. But some are using more-sophisticated database software. In the late 1990s, for example, virologist Joe Mymryk created a Microsoft Access database to track key reagents when he set up his lab at Western University in London, Canada. In 2007, his graduate student Ahmed Yousef joined Ibrahim Baggili, a computer-science graduate student then at Purdue University in West Lafayette, Indiana, to develop a friendlier, Windows-based interface to the system, called  LINA  (Laboratory Inventory Network Application; A. F.Yousef  et al .  J. Lab. Automat.   16 , 82\u201389; 2011 ). LINA draws from a series of Access databases \u2014 one for each class of reagent, including bacterial and yeast strains and short sequences of DNA and RNA known as oligonucleotides. As new reagents are developed or acquired, they are logged in the system, which assigns each one a unique identifier. Samples are then organized in freezer boxes according to those numbers and users can search the database by keyword, source and function. \n               Search and rescue \n             For Mymryk, LINA's most useful feature is a tool to search and compare DNA sequences. This means that he can enter a gene sequence and check whether the library contains any oligonucleotides that could be used to amplify it, rather than just ordering new ones. \u201cThe oligo thing has really saved my bacon,\u201d he says. LINA is free and simple to use, which makes it particularly attractive for small molecular-biology labs. But more-advanced options are also available at no cost. Marie Ebersole, who manages the chemistry preparation room at Wellesley College in Massachusetts, opted to upgrade her Excel-based system to  Quartzy , a free cloud-based system that allows her to track purchases for her 1,000-reagent collection. Quartzy's zero cost figured prominently in her decision. \u201cI didn't have to have 'buy-in' from 12 different people in 3 departments, and I could upload my existing spreadsheets,\u201d she says. Ebersole uses Quartzy mainly for tracking dry and liquid chemicals. But it can also track freezer boxes, so that users know precisely what each slot of a given container holds. When stocks run low, users click on a button to reorder, and the system automatically alerts the manager so that she or he can track the order's status. (The system is able to offer its service for free because it incorporates catalogues from several reagent vendors, and suggests those products when orders are placed.) Other features include support for tracking barcodes attached to individual samples, as well as equipment scheduling and document management for maintaining lab manuals and the like. For Ebersole, Quartzy's features not only improve the efficiency of the lab, they cut down on her costs. \u201cI've saved about a third of my budget,\u201d she says. In part, that is because there is less waste: by knowing precisely what chemicals she has to hand, Ebersole can use up old reagents before buying fresh ones. And when she does buy new chemicals, she says, she can do so in smaller quantities than before. Another option is  StrainControl , which has been developed by DNA Globe of Ume\u00e5, Sweden. The software is free for individual researchers in small labs; a professional licence for 10 users costs US$79.95; and a 50-user licence costs $649.95. Both of the paid versions allow the software to be used on a computer network or cloud-based service. Although its name evokes images of fruit flies and mice, StrainControl can accommodate the resources of most wet-lab biologists, says Kristoffer Lindell, DNA Globe's external-relations manager. The software, which has some 15,000 users, provides support for different lab-organism strains, proteins, plasmids, antibodies and chemicals, and like some other tools, is compatible with sample barcoding. Users can rename any of the fields to suit their needs, Lindell says; as a result, StrainControl can be used to catalogue anything, whether lab-related or not. An imminent update will allow users to add one or two custom modules to the database (not just reconfigure existing ones), for tasks such as tracking references. Other systems are more specialized. A lab information-management system (LIMS) called  mLIMS , developed by BioInfoRx in Madison, Wisconsin, is designed to track rodent colonies, for instance. Some also offer connectivity to electronic notebooks.  Labguru , for instance, is a cloud-based application that tracks plasmids, bacteria, antibodies, plants, rodents and proteins and has a built-in electronic notebook, says product specialist Xavier Armand. Developed with investment from  Nature 's parent company, Holtzbrinck Publishing Group in Stuttgart, Germany, Labguru costs $120 per user per year for academics and $450 per user per year for industry labs and is produced by BioData in Cambridge, Massachusetts. Usually, Armand explains, inventories and electronic lab notebooks connect details about each sample to experimental results, so users can track which reagents were used in which experiments. \u201cWe believe that providing high-fidelity metadata for reagents and methods coupled with linkage to experimental data will help to improve the reproducibility problem,\u201d he explains. It should make it easier for researchers to duplicate the findings of their own and other labs' experiments. Similarly,  Freezer Web Access  and  Lab Inventory , both from ATGC Labs in Potomac, Maryland, allow users to link their reagents to a LIMS. Software developer Pavel Bolotov says that both applications cost $150 per user and $350 per server installation \u2014 plus $1,000\u201320,000 for customization. \n               Linked-up approach \n             Some research institutes and companies centralize inventory management at a large scale. The office of Environmental Health & Radiation Safety (EHRS) at the University of Pennsylvania in Philadelphia has spent the past several years moving its 700 lab groups to the unified system  CISPro , which is developed by BIOVIA (formerly Accelrys) in San Diego, California. According to EHRS lab-safety specialist Kimberly Bush, institution-wide tracking facilitates three key tasks: compliance reporting (for example, whether the university is meeting building-code limits on flammable materials), cross-lab material sharing and university-wide reagent monitoring. \u201cThose are difficult or impossible to accomplish if there are 700 standalone inventory systems,\u201d she says. In 2011, the EHRS received $50,000 from the Penn Green Fund, a university sustainability initiative, to implement CISPro as part of an effort to reduce waste and consolidate inventory management. Today, only about one in eight labs is on-board. The roll-out has been anything but smooth, Bush says, and exemplifies the challenges of inventory tracking. Because the university's chemical purchases do not go through a central office, each lab has to be trained to create and upload its own inventories. And the process of creating the database is cumbersome and error-prone. For instance, a chemical might have multiple names, and inconsistencies in database set-up and material logging can make the chemical difficult to recall at a later date, leading to unnecessary reordering. Thus, she notes, some users actually maintain two systems, \u201cbut that's duplicate effort\u201d. Furthermore, CISPro is designed to give every chemical container a unique barcode. But for users that consume bottle after bottle of a given solvent, the repetitive logging can become tedious. In that case, says Bush, users might reserve and reuse a handful of barcodes on the door of the flammables cabinet. \u201cTo keep an inventory as accurate as possible you have to consider both the chemicals and the users' workflow,\u201d she advises. Whichever tracking system researchers choose, they can be confident at least of this: they need never be at a loss for their lab's resources again. If nothing else, says Mymryk, that could save researchers some awkward moments: \u201cThere's nothing more embarrassing than having to ask for the same reagent twice.\u201d \n                     Researchers argue for standard format to cite lab resources 2015-May-29 \n                   \n                     Going paperless: The digital lab 2012-Jan-25 \n                   \n                     Electronic notebooks: A new leaf 2005-Jul-06 \n                   \n                     BIOVIA CISPro \n                   \n                     Freezer Web Access \n                   \n                     Freezerworks \n                   \n                     LabGuru \n                   \n                     LINA: Laboratory Inventory Network Application \n                   \n                     mLIMS \n                   \n                     OpenFreezer \n                   \n                     Quartzy \n                   \n                     StrainControl \n                   Reprints and Permissions"},
{"file_id": "nature.2015.18351", "url": "https://www.nature.com/articles/nature.2015.18351", "year": 2015, "authors": [{"name": "Philip Ball"}], "parsed_as_year": "2006_or_before", "body": "Journal that reviews papers from preprint server aims to return publishing to the hands of academics. \n               Update: The journal  \n               Discrete Analysis \n                officially launched on 28 February 2016. \n             New journals spring up with overwhelming, almost tiresome, frequency these days. But  \n                   Discrete Analysis  \n                 is different. This journal is online only \u2014 but it will contain no papers. Rather, it will provide links to mathematics papers hosted on the preprint server arXiv. Researchers will submit their papers directly from arXiv to the journal, which will evaluate them by conventional peer review. With no charges for contributors or readers,  Discrete Analysis  will avoid the  commercial pressures  that some feel are distorting the scientific literature, in part by reducing its accessibility, says the journal's managing editor Timothy Gowers, a mathematician at the University of Cambridge, UK, and a winner of the prestigious Fields Medal.  \u201cPart of the motivation for starting the journal is, of course, to challenge existing models of academic publishing and to contribute in a small way to creating an alternative and much cheaper system,\u201d he explained in a  10 September blogpost  announcing the journal. \u201cIf you trust authors to do their own typesetting and copy-editing to a satisfactory standard, with the help of suggestions from referees, then the cost of running a mathematics journal can be at least two orders of magnitude lower than the cost incurred by traditional publishers.\u201d Discrete Analysis'  costs are only $10 per submitted paper, says Gowers; money required to make use of Scholastica, software that was developed at the University of Chicago in Illinois for managing peer review and for setting up journal websites. (The journal also relies on the continued existence of arXiv, whose  running costs  amount to less than $10 per paper). A grant from the University of Cambridge will cover the cost of the first 500 or so submissions, after which Gower hopes to find additional funding or ask researchers for a submission fee. \n               Overlay journals \n             The idea of an 'overlay' journal that links to papers hosted on a preprint server is not new. There are arXiv overlay journals in maths already, such as  SIGMA  ( Symmetry, Integrability and Geometry: Methods and Applications ) and  Logical Methods in Computer Science . But Gowers\u2019 announcement is likely to widen interest in the idea because of his influence in the mathematics community \u2014 and outside it. Three years ago, for instance, a blogpost announcing Gowers' personal boycott of the Dutch publishing giant Elsevier, helped to  spark the 'Cost of Knowledge' movement , which has seen more than 15,000 researchers variously pledging not to publish with, referee for or do editorial work for Elsevier. And in 2013, Gowers announced his involvement with an initiative called the  Episciences project , in which mathematicians decided to launch a series of overlay journals. That uses the multidisciplinary archive HAL, a preprint server that mirrors arXiv and is hosted in Lyon, France. One of its leaders, mathematician Jean-Pierre Demailly of the University of Grenoble in France, admits that progress has been sluggish. \u201cThings have been slower than what we dreamed about three years ago \u2014 the technical development of the Episciences platform took about a year and a half longer than initially envisioned,\u201d he says. \u201cHowever things are now coming along nicely.\u201d\u00a0The initiative now has five or six staff, Demailly says, and operates three computer-sciences journals and one in maths, which charge nothing to publish.  Episciences would have been a suitable platform to support  Discrete Analysis  too, Gowers says, but he happened to have sufficient funds to use the Scholastica software, and opted for that instead. \u201cI hope that in due course people will get used to this publication model,\u201d he adds, and that \u201cthe main interest in the journal will be the mathematics it contains\u201d.  Discrete Analysis  will publish in a family of subjects related to additive combinatorics, including topics such as analytical and combinatorial number theory and the mathematical aspects of theoretical computer science. A temporary website has been created on the Scholastica platform to receive submissions, before the journal launches early next year. Gowers says that the model could be extended to other scientific fields. \u201cFor many subjects, where articles are almost purely text and nearly all authors know how to produce nice documents in LaTeX [a  typesetting system commonly used by researchers ], this model should work,\u201d he says. The question, perhaps, is how readily researchers will embrace the model. \u201cApart from being an arXiv overlay journal, our journal is very conventional, which I think is important so that mathematicians won't feel it is too risky to publish in it,\u201d says Gowers. \u201cBut if the model becomes widespread, then I personally would very much like to see more-radical ideas tried out as well\u201d \u2014 for example, post-publication review and non-anonymous referees. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Open access: The true cost of science publishing 2013-Mar-27 \n                   \n                     Mathematicians aim to take publishers out of publishing 2013-Jan-17 \n                   \n                     Open-access deal for particle physics 2012-Sep-24 \n                   \n                     Timothy Gowers' blog on  Discrete Analysis \n                   \n                     Discrete Analysis temporary website \n                   \n                     Episciences project \n                   Reprints and Permissions"},
{"file_id": "527123a", "url": "https://www.nature.com/articles/527123a", "year": 2015, "authors": [{"name": "Jeffrey M. Perkel"}], "parsed_as_year": "2006_or_before", "body": "Scientists have a surfeit of options to choose from in the competitive market of reference-management software. Adam Rocker didn\u2019t expect the software that managed his digital reference library to flag up better ways he could be doing his research. But his electronic filing system of choice, ReadCube, periodically scans his library and suggests related papers, rather as some music-file-management programs highlight recommended tunes. And that feature, he says, has brought up some unexpected gems. As a graduate student, Rocker, who is now studying medicine at the University of Ottawa, was researching bacterial infections in zebrafish. ReadCube highlighted a paper that described a way to entrap the fish using microfluidics \u2014 a field whose literature he would not normally read \u2014 that was much easier than his own method. Being alerted to the research was \u201creally rewarding\u201d, Rocker says, although he was ultimately too invested in his own project to adopt the alternative approach. As Rocker discovered, today\u2019s reference-management tools go above and beyond simple electronic filing. Rather like a Swiss-army knife, each tool now appeals to customers by offering an ever-evolving set of extra features. This article focuses on eight tools \u2014  colwiz ,  EndNote ,  F1000Workspace ,  Mendeley ,  Papers ,  ReadCube ,  RefME  and  Zotero  \u2014 all competing in the reference-management market (see \u2018Reference-management software\u2019 or download this Excel spreadsheet for a  fuller comparison of the software  ). Some excel at streamlining the process of browsing and building literature libraries, whereas others focus on creating bibliographies, aiding collaboration through the use of shared workspaces or recommending papers. (One, ReadCube, is owned by Digital Science, a firm operated by the Holtzbrinck Publishing Group, which also has a share in  Nature \u2019s publisher.) Each tool exists to help researchers to tame the digital flotsam and jetsam of scattered, downloaded PDFs. Most scientists can relate to that problem: as they grab PDFs from journal websites \u2014 where they are often assigned impenetrable alphanumeric codes as filenames \u2014 and dump them into any convenient folder, chaos can quickly take hold, with multiple copies of files spread across hard disks. \n               Reference-management software \n               \u201cIn science, or at least in my experience, we tend to end up with a folder in the desktop with 3,000 really weirdly named PDF files, which we can never find when we need them,\u201d says Ra\u00fal Delgado-Morales, a neuroscientist at the Bellvitge Biomedical Research Institute in Barcelona, Spain. Reference-management tools address that confusion by indexing a hard disk. Typically, the process of dragging and dropping a PDF into an application window triggers the software to try to identify it using the DOI or title, and to retrieve relevant metadata (such as title, keyword and author names) from online servers. Researchers can also assign software to monitor specific folders into which they drop their files. They can then find PDFs through a simple search for author name, keyword or, in some cases, their own notes. Delgado-Morales solved his problem, for example, by organizing his literature library with Papers, a user-friendly application that automatically renames files according to any scheme he chooses. Other tools offer similar functions, except for RefME \u2014 a website and mobile app \u2014 which stores only lists of references and not the PDFs themselves. \n               Core functions \n             Most of the tools help researchers to import literature from a variety of online sources. Many offer in-app searching of external databases such as PubMed and Google Scholar, as well as web-browser plugins that grab reference data (and sometimes, associated PDFs) from journal websites and other pages. Zotero \u2014 a free, open-source software project \u2014 was founded ten years ago specifically to tackle the problem of extracting information from a web browser, says project director Sean Takats of George Mason University in Fairfax, Virginia. \u201cThat\u2019s the key feature of Zotero, and remains one of its strongest compared to other reference managers,\u201d he says. RefME offers the unusual option of adding references by scanning a barcode with a smartphone camera. One of the best-known features of reference-management software is the ability to insert in-text references in a research paper and to create bibliographies in any format. EndNote, a widely used commercial package, has offered this feature for decades, but now faces competition from many modern tools. Many tools interface with common word-processing software (usually Microsoft Word, but sometimes OpenOffice and related freeware suites as well) so that a user typing up a research article need only select the papers that they want to mention and click a button to have codes inserted into the document to mark the in-text reference. Later, the user can create a bibliography and in-text citations according to several thousand journal styles, picking his or her choice from a pull-down list. Most tools include built-in PDF readers for reading and annotating articles \u2014 typically allowing users to search through comments and notes \u2014 as well as cloud-based capabilities for syncing those comments (and the PDFs themselves) between, for example, an iPad and a desktop computer. But ReadCube and colwiz try to offer richer PDF reading experiences. In ReadCube, for instance, in-line citations and author names in PDFs are rendered as active hyperlinks to provide direct access to cited articles and publication lists. The same functionality is available when viewing and annotating PDFs on the websites of partnering publishers (including, for ReadCube,  Nature  and Wiley; and, for colwiz, Taylor & Francis). Many of these tools can identify articles related to specific items in a library, or recommend articles on the basis of the library\u2019s content overall. F1000Workspace \u2014 like ReadCube \u2014 uses an algorithm to do this. It also taps into recommendations made by a community of 10,000 or so specialists. However, many other stand-alone software products also recommend papers (see   Nature 513, 129\u2013130; 2014 ). \n               Set to share \n             Many tools now allow researchers to set up group libraries or share key papers with distant collaborators, although this process is carefully managed to prevent violation of publishers\u2019 copyright. Those in public groups using Mendeley, for instance, can share only information about a paper \u2014 the equivalent of a library-catalogue entry. Only users in private groups can share and modify PDFs (and groups must upgrade to a paid account to add more than three individuals). Brenton Wiernik, an organizational-psychology PhD candidate at the University of Minnesota in Minneapolis, uses a shared library in Zotero for collaborative projects involving systematic reviews and meta-analyses of the literature in his field. Such efforts might involve 15\u201320 people, he says: some downloading articles into a shared library; others reading them; still more adding annotations and tags and logging key data. According to Wiernik, the process is akin to using a shared Dropbox folder, with the added benefit that Zotero tracks and maintains metadata, notes and annotations. For instance, researchers can use a dedicated tag to indicate that they are processing an article, thereby signalling to collaborators that they should work on a different article to avoid duplicated effort. F1000Workspace and colwiz both extend sharing to include features for preparing manuscripts and managing projects. With F1000Workspace, researchers can use a plugin to upload Microsoft Word manuscripts to a secure location, thereby enabling team members to comment on the shared copy \u2014 although the text cannot be edited in the browser, says Jo\u00e3o Peres, the company\u2019s product-development manager. Peres plans to implement a \u2018one-click\u2019 article-submission feature that sends papers directly from F1000Workspace to journal editors, starting with the journal  F1000Research . And colwiz also permits users to share documents to an online drive for team members to view and comment on. Given the highly overlapping feature sets of these tools, a user\u2019s choice often comes down to particular individual priorities. Richard Karnesky, a materials scientist at the Sandia National Laboratories in Livermore, California, supports Zotero for its open-source ethos, for example. Perhaps the best reason for using a reference manager is the technology\u2019s ability to provide a form of searchable memory. Imagine, says Boyd Steere, a senior research scientist at pharmaceutical firm Eli Lilly in Indianapolis, Indiana, a desk piled high with printed papers: Post-it notes hanging out, writing in the margins, doodles, notations, arrows and more. Today\u2019s PDF-filled, digital folders are in many ways no easier to navigate. With a digital reference manager, however, buried knowledge is just a keyword search away. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Interactive notebooks: Sharing the code 2014-Nov-05 \n                   \n                     Scientific writing: the online cooperative 2014-Oct-01 \n                   \n                     How to tame the flood of literature 2014-Sep-03 \n                   \n                     Nature  Toolbox hub \n                   \n                     colwiz \n                   \n                     EndNote \n                   \n                     F1000Workspace \n                   \n                     Mendeley \n                   \n                     Papers \n                   \n                     ReadCube \n                   \n                     RefME \n                   \n                     Zotero \n                   Reprints and Permissions"},
{"file_id": "526145a", "url": "https://www.nature.com/articles/526145a", "year": 2015, "authors": [{"name": "Dalmeet Singh Chawla"}], "parsed_as_year": "2006_or_before", "body": "Machine-readable system seeks to clearly explain who did what for a research paper. An initiative that uses  colourful \u2018digital badges\u2019  to denote different contributions to research aims to standardize and simplify the often-fraught business of detailing who did what on a scientific paper. Two publishers have begun assigning authors any of 14 badges that delineate the parts they played in a study: from a magenta \u2018Resources\u2019 one (for providing study reagents or instruments) to a red one for writing the initial draft. The badges, says Amye Kenall, associate publisher at BioMed Central in London, could help to minimize the politics of authorship lists, in which supervisors can gain credit for work done by their doctoral students. The project also aims to enhance collaboration by clearly demarcating each contributor\u2019s specialities, she says. On 28\u00a0September, the BioMed Central journal  GigaScience  added the badges to two of its  published   papers . Readers can click to see co-authors listed under multiple badges; the information is also coded in a format that allows computer programs to extract it, which makes it linkable to other online author profiles (such as the researcher identification system ORCID). Another London-based publisher, Ubiquity Press, is also adding badges to two of its  published   papers . \u201cIn order for information around contribution to be meaningful and useful, it needs to be standardized,\u201d Kenall explains. Many papers include author-contributions sections, but their formats vary, and they can be a vehicle for ambiguity \u2014 or insider jokes. In one of the papers with badges, author Keith Bradnam, a bioinformatician at the University of California, Davis, is described in the contributions section as having \u201cherded goats\u201d. The concept has been developed by collaborating publishers, research funders and software firms (which have used digital badges for a few years as a visual sign of achievement). Several other publishers have expressed interest in implementing them, Kenall says, and initial feedback from researchers has been positive. The  14 categories  come from a related  \u2018digital taxonomies\u2019 project , which last year brought together journal editors, funders and researchers to classify authors\u2019 contributions as a set of standard roles. \u201cWe think it\u2019s timely to have a bit more granularity around contributions to scholarly published work,\u201d says Liz Allen, a co-founder of the digital taxonomies project. Accurately determining co-authors\u2019 roles might also help with grant-funding applications, she adds, because applicants could be more explicit about research contributions. The taxonomy is still in a rough format, but the badges project is not alone in implementing it, Allen adds.  Cell Press , for instance, now offers researchers the  option to use the taxonomy  when submitting papers; so far, it has published two papers that do so \u2014 although without the badges. But contributions to scholarly products may be too varied to be captured with a 14-part taxonomy, says Melissa Haendel, who develops systems for querying and classifying biological data at Oregon Health & Science University in Portland. Haendel co-chairs a working group as part of FORCE11, a community-driven initiative that aims to improve scholarly communication technologies and policies. The group is mapping out author roles, in part by using computer programs to search the text of author-contributions sections on papers. A January workshop in Oxford, UK, listed more than 500\u00a0tasks that authors might want to be credited for, she says; examples include developing experimental protocols, taking photographs, developing validated surveys or providing lab reagents. The badges that a researcher might collect could easily be extended, Kenall notes; extra categories could credit peer reviewers, for example. But for now, BioMed Central is focusing on collecting data about how often people click on the badges, before advancing conversations with funders, publishers and researchers about their practicality, and rolling out the badges to other journals. \u201cUnlike a CV or author-contributions section, badges provide a method of credit and transparency around contribution fit for purpose for a digital world,\u201d Kenall says. Editor\u2019s note: Dalmeet Singh Chawla worked at BioMed Central until June 2014, but had no involvement with the badges project. \n                 Tweet \n                 Follow @NatureNews \n               \n                     Physics paper sets record with more than 5,000 authors 2015-May-15 \n                   \n                     Fruit-fly paper has 1,000 authors 2015-May-13 \n                   \n                     Publishing: Credit where credit is due 2014-Apr-16 \n                   \n                     Collaborations: The rise of research networks 2012-Oct-17 \n                   \n                     Seven days: 3\u20139 August 2012 2012-Aug-08 \n                   \n                     The digital badges \n                   \n                     Project CRediT \n                   Reprints and Permissions"},
{"file_id": "525145a", "url": "https://www.nature.com/articles/525145a", "year": 2015, "authors": [{"name": "Boer Deng"}], "parsed_as_year": "2006_or_before", "body": "Software tools that track how animals move are helping researchers to do everything from diagnosing neurological conditions to illuminating evolution. Palaeontologist Stephen Gatesy wants to bring extinct creatures to life \u2014 virtually speaking. When he pores over the fossilized skeletons of dinosaurs and other long-dead beasts, he tries to imagine how they walked, ran or flew, and how those movements evolved into the gaits of their modern descendents. \u201cI'm a very visual guy,\u201d he says. But fossils are lifeless and static, and can only tell Gatesy so much. So instead, he relies on XROMM, a software package that he developed with his colleagues at Brown University in Providence, Rhode Island. XROMM (X-ray Reconstruction of Moving Morphology) borrows from the technology of motion capture, in which multiple cameras film a moving object from different angles, and markers on the object are rendered into 3D by a computer program. The difference is that XROMM uses not cameras, but X-ray machines that make videos of bones and joints moving inside live creatures such as pigs, ducks and fish. Understanding how the movements relate to the animals' bone structure can help palaeontologists to determine what movements would have been possible for fossilized creatures. \u201cIt's a completely different approach\u201d to studying evolution, says Gatesy. XROMM, released to the public in 2008 as an open-source package, is one of a number of software tools that are expanding what researchers know about how animals and humans walk, crawl and, in some cases, fly (see \u2018Movement from inside and out\u2019). That has given the centuries-old science of animal motion relevance to a wide range of fields, from studying biodiversity to designing leg braces, prostheses and other assistive medical devices.\u201cWe're in an intense period of using camera-based and computer-based approaches to expand the questions we can ask about motion,\u201d says Michael Dickinson, a neuroscientist at the California Institute of Technology in Pasadena. To use and develop effective software, however, scientists must learn how to adapt broad, open-source tools to their own needs \u2014 and when to build their own. \n               boxed-text \n             \n               A visual history \n             The boom in motion-tracking tools has come about in part because of improvements in what researchers can see and measure. The first studies of animal and human motion, dating back to Aristotle, relied on naked-eye observations, anatomy and detailed pictures drawn by hand. In the nineteenth century, the science of biomechanics was boosted by photography \u2014 perhaps most famously in a series of images of a galloping horse taken by British photographer Eadweard Muybridge, and published in his  Animal Locomotion  collection in 1887. Higher-speed cameras eventually improved what could be captured. But movement studies still needed a person to look through the results frame by frame, laboriously tracing the arc of each step, arm swing or wing flap to extract information about angles and forces. Much of that tedium can now be relieved by computers or other measuring tools. But such tools are often expensive, and even today, many researchers do without them. Gatesy recalls a graduate student's surprise at the low-tech approach that was used to study gait in rodents a few years ago: \u201cIt wasn't uncommon just to dip their feet into some ink, have them leave some tracks and take measurements from those,\u201d he says. Lately, however, scientists have been coming up with methods that are much more sophisticated without being too expensive. In July, developmental biologists Richard Mann and C\u00e9sar Mendes at Columbia University in New York City and their colleagues published a paper on MouseWalker: a system they have built to automatically analyse changes in a mouse's gait (C. S. Mendes  et al .  BMC Biol.   13 , 50; 2015 ). It involves an inexpensive set-up in which a mouse walks on a transparent surface over a high-speed camera that records the animal's footfalls. An analytical technology called machine vision allows the MouseWalker software to discern details such as the position of each step relative to the mouse's body. Mendes says that this information can be used to detect when something goes wrong with gait, as can happen with the onset of neurological illnesses such as Parkinson's disease. MouseWalker was adapted from FlyWalker, a system that Mendes and his team helped to develop to let neuroscientists track how fruit flies walk after their neurons have been manipulated. Both MouseWalker and FlyWalker are open source: the authors hope that making the software available for free will help to attract users who can add parameters that they had not thought of. \n               Building user communities \n             The desire to share tools is common to many developers, so motion-tracking software is finding applications in a number of fields \u2014 sometimes in unexpected ways. \u201cOne of the things we hope for is that people will use what we develop and go in a new direction with it,\u201d says Jen Hicks, an engineer at Stanford University in California, who helps to manage OpenSim \u2014 an open-source software package that allows users to model joints, muscles and how they move. OpenSim has more than 20,000 users, and part of Hicks's job is to organize workshops and tutorials to guide this growing community. The OpenSim community serves as an exemplar of what newer programs such as XROMM or MouseWalker could become. The software models musculoskeletal systems, and researchers have used it to simulate everything from the potential outcomes of surgery to the muscular forces of goats. Since the first version of OpenSim was released in 2007, the package has gone through dozens of upgrades that have added features and improved the algorithms used for calculations. It has been downloaded more than 100,000 times. \u201cIt's amazing how much the community has grown,\u201d says mechanical engineer Katherine Steele of the University of Washington in Seattle, who first began using OpenSim while studying cerebral palsy as a graduate student at Stanford. Serving an ever-larger crowd requires careful planning to make the program accessible, says Hicks. Through grants from the US National Institutes of Health, she and her colleagues keep manuals up to date with the new releases. Ensuring that the software can be tailored to a researcher's particular needs has helped new users to embrace it, she says. \n               The limits of broad platforms \n             XROMM's developers are in the middle of building up the infrastructure to make the software accessible to a wider community, for instance setting up a site to host the newest open-source version, XMA Lab, which became available in December 2014. The team has tried to make the latest versions of the software easier for new users. For example, says Elizabeth Brainerd, a colleague of Gatesy at Brown, \u201cThere used to be about 20 pieces of information you had to keep track of,\u201d including items such as calibration measurements. \u201cBut now it's all integrated\u201d. It is important not to make things too easy, says Steele: if the software does too much of the work, there is a risk that the researchers will misunderstand the data that it spits out. However, as an open-source program develops, understanding its architecture can get very complicated. \u201cSometimes the software can get so big that it becomes black-box-ish. Then it might be better for you to build your own,\u201d she says. Dickinson agrees, and says that sometimes, modifying open-source tools is not enough. \u201cAs science is becoming more quantitative, we're all working on finer slices of the pie,\u201d he says. \u201cIf you only got to use a microscope that someone else built, so to speak, you won't be able to get as far.\u201d Regardless of what tools are available, researchers intend to keep expanding the applications of motion tracking. Hicks anticipates seeing more people using the tools to explore neural control and robotics designs. And she expects the software to keep improving. \u201cWe're finding ways to learn from even messier motion data, like from accelerometers in your phone,\u201d she says. \u201cBringing together more machine learning and biomechanics \u2014 that will be the next step.\u201d\n \n               \n                 Tweet \n                 Follow @NatureNews \n               \n                     Wildlife energy: Survival of the fittest 2014-Sep-10 \n                   \n                     Wing and fin motions share universal principles 2014-Feb-18 \n                   \n                     Behaviour: Flies on film 2009-Dec-02 \n                   \n                     X-Ray Reconstruction of Moving Morphology \n                   \n                     OpenSim \n                   Reprints and Permissions"},
{"file_id": "523115a", "url": "https://www.nature.com/articles/523115a", "year": 2015, "authors": [{"name": "Ewen Callaway"}], "parsed_as_year": "2006_or_before", "body": "Palaeontologists hope that software can construct fossil databases directly from research papers. For a field whose raison d'\u00eatre is to chronicle the deep past, palaeontology is remarkably forward-looking when it comes to organizing its data. Victorian natural history museums meticulously organized their collections with handwritten cards that survive to this day. And over the past 15 years, researchers have collectively entered records of more than a million fossils into an online database, allowing them to track broad trends in the history of life. Now, palaeontologists are exploring the use of machine algorithms to pull fossil data from their research papers automatically. \u201cI'm fairly convinced that this is the future, for sure,\u201d says Shanan Peters, a palaeontologist at the University of Wisconsin\u2013Madison (UW Madison) who is co-leading an effort to use software to extract information from tens of thousands of palaeontology papers. \u201cBuilding a database, per se, will be a thing of the past. Those databases will be dynamically generated based on the questions you're interested in, and the machine will do the heavy lifting.\u201d Peters should know. He is the principal investigator of the Paleobiology Database (PBDB;  paleobiodb.org ), which details the age, location and identity of some 1.2 million fossils. Since it was started in 1998, researchers have spent about 80,000 hours \u2014 the equivalent of 9 continuous years \u2014 entering and opining over data from original field research and around 40,000 articles. The PBDB has produced hundreds of papers and has allowed palaeontologists to address questions that would have been otherwise unanswerable, on topics ranging from epoch-wide extinction rates to the disappearance of certain dinosaurs. The PBDB is a database created by experts: around 380 scientists have uploaded some 560,000 published opinions on the classifications of 320,000 taxonomic names. But Peters was curious to know whether such a database could be compiled automatically by computer. So in 2013 he started a collaboration with Miron Livny and Chris R\u00e9, then data scientists at UW Madison (R\u00e9 has since moved on to Stanford University in California). R\u00e9 had developed software called DeepDive, which mines written text (such as words in a research paper) and pulls out facts. Text mining \u2014 or content mining \u2014 is now a commonplace tool in computer science and is slowly beginning to find uses in research fields from genomics to drug discovery. Text mining palaeontology literature appealed to R\u00e9, partly because the PBDB offers a human-curated database with which to compare a computer-generated counterpart. \n               Parsing the past \n             DeepDive begins by parsing research papers in a manner that would be familiar to anyone who remembers their early grammar lessons. \u201cIt's taking those papers and converting them into text,\u201d says R\u00e9: it is trying to determine the answer to questions such as, \u201cWhat's a noun, what's a verb and how do you diagram a sentence?\u201d Next, DeepDive attempts to predict the concepts that are stored in those sentences (such as, for palaeontology, the names of fossils and the places where they were found) and assigns a probability to each assertion. The result is software \u201cwhich is usually imperfect in a lot of ways\u201d, says R\u00e9. \u201cThat's where you get the domain scientist involved.\u201d Peters spent about a year refining the first-pass software so that, for instance, it knows where to look in palaeontology papers for the names of new species and the geographic locations in which they were discovered. R\u00e9 describes this process as a \u201cback and forth\u201d with Peters that required R\u00e9's team of data scientists to come up with custom computing solutions to make the requests feasible. \u201cI would love to say the answer is people can press a button and use it and run it and they don't need us,\u201d R\u00e9 says \u2014 but that is a goal that his team has not yet reached. As a proof of principle, Peters and R\u00e9 used custom software that they called PaleoDeepDive to create a text-mined, scaled-down version of the PBDB that incorporated around 12,000 papers. In some ways the computer-generated database outshines the PBDB, Peters says, because all the information in it comes with a probability assigned to it and is linked back to the original text. \u201cThe machine is really clear about uncertainty, when there's ambiguity, or differences between documents and authors,\u201d Peters says. PaleoDeepDive also managed to extract 192,000 opinions on the classification of taxonomic names from the papers, whereas the PBDB's human curators found only 80,000. PaleoDeepDive did not do such a bad job at organizing that information either. In a December 2014 paper, R\u00e9 and Peters report that from a random sample of 100 statements drawn from the computer-generated database, 92% were correct \u2014 which they say was similar to the accuracy of the PBDB (S. E. Peters  et al .  PLoS ONE   9 , e113523; 2014 ). The two databases also scored similarly in a second experiment, when scientists were presented with five documents and asked to score the accuracy of facts that had been mined from them by the PBDB and by the computer. And perhaps most impressively, PaleoDeepDive was used to estimate species diversity and extinction rates over the past 500 million years, coming up with measures similar to those determined by the PBDB. \u201cIt's a little scary, the machines are getting that good. That's just something that we're going to have to get used to,\u201d says Mark Uhen, a palaeontologist at George Mason University in Fairfax, Virginia, who is on the PBDB's executive council. \u201cI think it's one of the best innovations that palaeontology has had in a very long time,\u201d says Jonathan Tennant, a palaeontologist at Imperial College London. He uses the PBDB every day and thinks that text mining could serve as a useful way to collect a large amount of data for later manual inspection \u2014 but not as a full-on replacement for human-curated databases such as the PBDB. \u201cI don't see machines replacing humans. I think it's important that we retain the human aspect of the analytics,\u201d he says. John Alroy, a palaeontologist at Macquarie University in Sydney, Australia, who co-founded the PBDB but is no longer affiliated with it, is less bullish on text mining. He says that DeepDive tends to overestimate the period during which species existed, leading to mistaken estimates of species diversity. He sees speed as the only advantage of text mining. \u201cBut there is no need to be fast in this case because the PBDB is already extremely comprehensive, so pretty much any question you might want to ask can already be answered with it. That explains why it has generated so many publications,\u201d Alroy says. \n               Text-mining frustrations \n             Peters says that he will be using the computer-generated database as a supplement to the human-generated PBDB but adds that, for now, the limited number of documents it works from make it of little added use to palaeontologists. He wanted to let PaleoDeepDive loose on a bigger set of documents, but he did not have legal permission. As other text miners have discovered, many publishers of paywalled articles are cautious about allowing researchers to text mine their papers, even if they have lawful access to the literature; publishers tend to place limits on how the results of text mining can be published and reused, and often limit the number of papers a scientist can download at any one time (see  Nature   483 , 134\u2013135; 2012 ). \u201cI can't think of any single palaeontologist who has 40,000 papers in their own stash, at least legally acquired,\u201d says Tennant. Peters and Livny spent months brokering a deal with one scientific publisher, Elsevier, to gain access to thousands of papers. \u201cThis is just the frustrating reality of things right now: advanced capabilities in machine reading and learning are coming out, and the bottleneck in progress is now getting documents together in one place for analysis,\u201d Peters says. He and his colleagues are working on amassing and parsing documents to feed into PaleoDeepDive and a related software tool for the geosciences literature called GeoDeepDive. R\u00e9, meanwhile, is working with experts in other fields to apply DeepDive to drug development, genomics and human trafficking. Many palaeontologists also want to make it easier to find the data buried in their papers, so they are calling for research papers to be described more systematically in the future. \u201cIf we start having publication where everything is standard, then it will be much easier to read and process that data,\u201d says Tennant. Uhen adds, \u201cI think there's a sort of cultural shift going on in palaeontology, where people are interested in data aggregation, and getting more insistent about being crystal clear about where you're finding your fossils.\u201d Despite these challenges, many palaeontologists see text mining as the way forward for their field. \u201cIt's a huge waste of time for grad students and postdocs to manually re-enter already published information into a structured database,\u201d says Ross Mounce, a palaeontologist at the Natural History Museum in London who is using text mining to track how the museum's 80-million-specimen collection is used in research papers. Peters hopes that efforts such as PaleoDeepDive will allow him and his colleagues more time to generate data instead of spending their days organizing data they already have. \u201cI see these machine reading systems as liberating our efforts a little bit, and shifting our work back into the field and back into the museums.\u201d \n                     Text-mining offers clues to success 2014-May-20 \n                   \n                     Gold in the text? 2012-Mar-07 \n                   \n                     Trouble at the text mine 2012-Mar-07 \n                   \n                     Literature mining: Speed reading 2010-Jan-27 \n                   \n                     DeepDive \n                   \n                     The Paleobiology Database \n                   \n                     PaleoDeepDive \n                   Reprints and Permissions"},
{"file_id": "518125a", "url": "https://www.nature.com/articles/518125a", "year": 2015, "authors": [{"name": "Jeffrey M. Perkel"}], "parsed_as_year": "2006_or_before", "body": "A powerful programming language with huge community support. Last month, Adina Howe took up a post at Iowa State University in Ames. Officially, she is an assistant professor of agricultural and biosystems engineering. But she works not in the greenhouse, but in front of a keyboard. Howe is a programmer, and a key part of her job is as a 'data professor' \u2014 developing curricula to teach the next generation of graduates about the mechanics and importance of scientific programming. Howe does not have a degree in computer science, nor does she have years of formal training. She had a PhD in environmental engineering and expertise in running enzyme assays when she joined the laboratory of Titus Brown at Michigan State University in East Lansing. Brown specializes in bioinformatics and uses computation to extract meaning from genomic data sets, and Howe had to get up to speed on the computational side. Brown's recommendation: learn Python. Among the host of computer-programming languages that scientists might choose to pick up, Python, first released in 1991 by Dutch programmer Guido van Rossum, is an increasingly popular (and free) recommendation. It combines simple syntax, abundant online resources and a rich ecosystem of scientifically focused toolkits with a heavy emphasis on community. \n               Hello, world \n             With the explosive growth of 'big data' in disciplines such as bioinformatics, neuroscience and astronomy, programming know-how is becoming ever more crucial. Researchers who can write code in Python can deftly manage their data sets, and work much more efficiently on a whole host of research-related tasks \u2014 from crunching numbers to cleaning up, analysing and visualizing data. Whereas some programming languages, such as MATLAB and R, focus on mathematical and statistical operations, Python is a general-purpose language, along the lines of C and C++ (the languages in which much commercial software and operating systems are written). As such, it is perhaps more complicated, Brown says, but also more capable: it is amenable to everything from automating small sets of instructions, to building websites, to fully fledged applications. Jessica Hamrick, a psychology PhD student at the University of California, Berkeley, has been programming in Python since 2008 and uses it in all phases of her research. In a study investigating how people manipulate geometric objects in their minds, for instance, she used the language (as well as JavaScript) to generate different shapes, present those to study participants, record their choices and analyse the data. Despite its general-purpose power, Python is considered less painful for beginners to learn than other options. That accessibility is a function of both the language itself and the resources that have been built up around it (see \u2018A Python toolkit\u2019). For example, software execution can be interactive \u2014 type a command, get a response \u2014 whereas in C, a compilation step is required to translate the code into an executable file, which complicates the process for neophytes. The language is also generally easier to handle; users do not have to predefine whether a variable will hold numbers or text, for instance. The classic programming exercise of printing 'Hello, world!' to the screen is as simple as it can be in Python \u2014 just type print(\u201cHello, world!\u201d) at a Python prompt and hit Enter. \u201cIt's easier to teach novice programmers how to get things done in Python than in C++ or C,\u201d says Brown, now at the University of California, Davis. Python is in fact a popular choice for introductory programming classes in general. \n               boxed-text \n             The community aspect is particularly important to Python's growing adoption. Programming languages are popular only if new people are learning them and using them in diverse contexts, says Jessica McKellar, a software-engineering manager at the file-storage service Dropbox and a director of the Python Software Foundation, the non-profit organization that promotes and advances the language. That kind of use sets up a \u201cvirtuous cycle\u201d, McKellar says: new users extend the language into new areas, which in turn attracts still more users. The community seems especially dedicated to encouraging women, Brown notes. There are numerous women-centric resources available, including workshops offered by the Hackbright Academy in San Francisco, the non-profit organization Ladies Learning Code in Toronto, Canada, and the global mentorship group PyLadies. As a master's student at McGill University in Montreal, Canada, Emily Irvine picked up Python to help her make sense of neuronal electrophysiology data. She was attracted to the language because of its \u201csimple syntax\u201d and \u201cmassive amount of online support\u201d. But just as important was the wider Python community, says Irvine, who will start a PhD in neuroscience at Dartmouth College in Hanover, New Hampshire, this autumn. At the PyCon conference last April in Montreal, \u201cthey just had such a welcoming atmosphere, especially towards women and scientists\u201d. Educational resources also abound. The Software Carpentry Foundation runs a series of two-day workshops that focus on scientific programming, and many of its educational resources are available online. Online classes are also available through Coursera in Mountain View, California, and Edx in Cambridge, Massachusetts, as are do-it-yourself tutorials, such as those hosted by Codecademy in New York City. (Because Python is named in honour of Monty Python, these tutorials often work references to the British comedy troupe into their exercises: one Codecademy exercise, for example, is to capitalize and calculate the length of the phrase 'the ministry of silly walks'.) Irvine taught herself to code using online courses and a healthy dose of the programming Q&A site stackoverflow.com. Today, she says, she considers herself somewhere between a beginner and an intermediate Python programmer, or 'pythonista', as they are sometimes called. \n               The full monty \n             Of course, user-friendliness is meaningless if researchers cannot write the software they need. That is where Python's packages, which extend the language with new functionality, come into play. \u201cPython was developed as a language with a philosophy that it was 'batteries included',\u201d McKellar says \u2014 it has built-in capabilities that make it easy to get started right out of the box. But, \u201cit also has a very mature package ecosystem around it. Anything that you could possibly write code to solve, people have written libraries to make that easier for you.\u201d Scientific programmers, irrespective of their discipline, routinely use a small set of core packages: NumPy (mathematical arrays), SciPy (linear algebra, differential equations, signal processing and more), SymPy (symbolic mathematics), matplotlib (graph plotting) and Pandas (data analysis). Another popular tool, Cython, addresses Python's relatively slow execution speed. Cython optimizes certain aspects of Python code, such as 'for' loops (used to instruct a program to repeatedly run a specific block of code) that are notoriously slow, essentially by converting them into C. \u201cYou can get speed-ups that are up to 1,000 times faster than standard Python,\u201d says Paul Nation, a theoretical physicist at Korea University in Seoul. The IPython Notebook is another popular package \u2014 Howe terms it \u201ca coder's lab notebook\u201d \u2014 that allows users to interleave data, code and explanatory text in a single browser-based page, rather than in separate files (see  Nature   515 , 151\u2013152; 2014 ). Beyond the core packages, software packages exist for just about every scientific discipline, including scikit-Learn for machine learning, Biopython for bioinformatics, PsychoPy for psychology and neuroscience and Astropy for astronomers. Thomas Robitaille, a coordinator of the Astropy project and a researcher at the Max Planck Institute for Astronomy in Heidelberg, Germany, says that Astropy was created to reduce duplicated effort between research groups. It gives users a core set of abilities, such as ways to convert coordinates from one astronomical mapping system to another, and a unified interface for reading and writing different data file formats, manipulating images and carrying out cosmological calculations. QuTip, another Python package, enables researchers working on quantum mechanics to define a system and then simulate how it behaves. The project was launched in 2010 by Nation and Robert Johansson, a postdoctoral fellow in RIKEN's Interdisciplinary Theoretical Science Research Group in Wako, Japan, to adapt into Python a MATLAB package that Nation was using. Such packages are key enablers of McKellar's 'virtuous cycle'. But researchers could probably do their work using any language, provided they put in the time to learn it. (Indeed, in many languages, including Python, it is possible to run algorithms written in a different language, thereby allowing researchers to reuse their old code.) The difficult part of learning to program lies with the fundamentals, says Brown \u2014 once a researcher has those nailed down, adapting to a new language is just a matter of syntax. What matters most in the early stages is having a good support network. \u201cPick the programming language based on what people around you are using,\u201d Brown advises. Increasingly, that language is Python. \n                     My digital toolbox: Climate scientist Damien Irving on Python libraries 2015-Jan-30 \n                   \n                     Programming tools: Adventures with R 2014-Dec-29 \n                   \n                     My digital toolbox: Nuclear engineer Katy Huff on version-control systems 2014-Sep-29 \n                   \n                     'Boot camps' teach scientists computing skills 2014-Sep-03 \n                   \n                     Nature Toolbox \n                   \n                     Python Software Foundation \n                   \n                     SciPy (Python tools for science) \n                   Reprints and Permissions"},
{"file_id": "519119a", "url": "https://www.nature.com/articles/519119a", "year": 2015, "authors": [{"name": "Mark Zastrow"}], "parsed_as_year": "2006_or_before", "body": "Easy-to-use mapping tools give researchers the power to create beautiful visualizations of geographic data. When linguist Lauren Gawne roams the valleys of Nepal documenting endangered Tibetan languages, she takes pains to distinguish each dialect's geographical origin. But when it came to producing maps of her results, for many years her cartographic methods were somewhat crude. \u201cMy old maps were [made] using MS Paint on top of some copyrighted map that I really shouldn't have been using,\u201d she says. Her next solution wasn't much better: \u201cMy mum tracing a map off an atlas so that I had something a bit cleaner to work with.\u201d The one after that \u2014 \u201cusing Google Earth and dropping pins on it\u201d \u2014 was generic, ugly and \u201clooks horrible in a PowerPoint\u201d. So in 2013, she jumped at the chance to join a workshop on mapping and visualization at the University of Melbourne in Australia, where she was working on her PhD. There she discovered the free, open-source program  TileMill , created by the company Mapbox, which has offices in San Francisco, California, and in Washington DC. It lets users create maps from their data and pre-existing online cartographic databases. TileMill is just one tool in the emerging field of customized mapping, where a bevy of open-source technologies and start-ups have given rise to an abundance of offerings for researchers and enthusiasts (see \u2018Get on the map\u2019). These tools are more approachable for novices than the conventional geographic information systems (GISs) that geographers have long used for analysis of geospatial data sets. They allow non-specialists to easily visualize, manipulate and share their data in formats that are as slickly browsable as Google Maps but with greater power and flexibility. \u201cTileMill allows you to be a complete control freak,\u201d says Gawne, now at Nanyang Technological University in Singapore. From line styles to font spacing and kerning, \u201cI can really manipulate all the variables quite easily.\u201d \n               boxed-text \n             Until recently, Google, which is based in Mountain View, California, itself had staked the biggest claim in this space, providing various ways to access and decorate its maps through application programming interfaces (APIs). But as demand grew, the tech giant began limiting public access to its APIs in 2011 \u2014 and this allowed slightly more sophisticated open-source tools to flourish, says Oliver O'Brien, a geographer at University College London. Today, a fully fledged ecosystem of start-ups with open-source technology at their core offer platforms that many say have surpassed Google's offerings. \u201cGoogle really nailed down having maps on the web,\u201d says Javier de la Torre, a founder and current chief executive of one of Google's emerging rivals,  CartoDB  of New York City. \u201cWhat I think they didn't see coming was that there was going to be this explosion of new mapmakers.\u201d \n               The new mapping landscape \n             In 2011, de la Torre was part of a team researching biodiversity informatics. The group was seeking an online platform to make a map of all known species on the planet. \u201cThere wasn't technology for doing that,\u201d he says \u2014 no tool could handle the amount of data, nor visualize how they changed over time. The researchers decided to develop the tool themselves and created what became the open-source platform CartoDB. The company offers free and paid plans for hosting and visualizing data through its website. Unlike TileMill, which is primarily intended for drawing and designing static maps, CartoDB specializes in visualizing dynamic layers of data on top of basemaps. Users can import their geo-located data into CartoDB's web-based interface and then filter or cluster data points, change the colour or size of symbols, and animate data changes over time. \u201cCartoDB wants to be a place where your data lives,\u201d says Steve Bennett, a research-oriented technologist at the University of Melbourne who takes workshops on mapping, including the one that Gawne attended. Peter Desmet, who collaborates with a bird-tracking research team at the Research Institute for Nature and Forest in Brussels, was a colleague of de la Torre and became an early adopter of CartoDB. \u201cI was never a desktop GIS person,\u201d he says. But in CartoDB, \u201cyou can create and share a visualization in literally minutes\u201d. Being able to simply send a link to the map online also makes it much faster to point out data-quality issues to colleagues, he says. Another strength of CartoDB is its selection of global basemaps \u2014 ranging from familiar geopolitical and satellite-image formats to more stylish black-and-white and even pencil- and watercolour-themed renditions. Some are produced by TileMill's maker Mapbox, which boasts a growing list of corporate and media clients \u2014 in many cases supplanting Google in a growing 'battle of the basemaps'. Mapbox first released TileMill in 2011. The team took a powerful but complex open-source cartographic renderer called Mapnik, built an easy-to-use interface around it and created a simple styling language, CartoCSS, to customize the maps' appearance. \u201cTileMill was a game-changer, absolutely,\u201d says Bennett. It allowed non-experts to produce professional-looking maps \u2014 either for publication as static figures or for use as basemaps in other visualization tools \u2014 without the need for more-complicated GIS programs. The landscape continues to shift rapidly. In January, Google announced that it would shut down some premium and paid forms of Google Maps and focus on its basic Maps API. In response, CartoDB introduced tools to help users migrate their data to CartoDB, while still allowing them to integrate the Google Maps APIs. Mapbox, for its part, has shifted development from TileMill to its intended replacement, Mapbox Studio. Cost of data storage is a potential stumbling block for scientists with large data sets \u2014 although CartoDB is open source, its convenience comes in large part from using it on the company's hosted web service. The firm offers 75 megabytes of storage for free, but to store more than 1 gigabyte of data, the price rises quickly to hundreds of US dollars per month. CartoDB also charges to keep data and maps private on the site. \u201cWe've had real problems,\u201d says Bennett. \u201cIf you're a PhD student with no funding, it just doesn't work.\u201d Mapbox works with a similar pricing model for hosting maps on its servers, although TileMill itself is a free, downloadable program. However, CartoDB does work with academic users to try to find a solution, says de la Torre, and awards grants of up to US$3,500 to researchers studying the impacts of climate change, in recognition of the company's environmental roots. Power users can daisy-chain these tools together: for example, one could create a basemap in TileMill and data layers in CartoDB, then wrap them in an online interface using Leaflet, a mobile-friendly visualization package that runs in the program JavaScript and meshes with other JavaScript visualization packages such as D3. Duncan Smith, a geographer at University College London, has made one such combination: an online map of UK census data called  LuminoCity  that uses Leaflet to display the map data over basemaps produced in TileMill, and a variant of D3 called Dimple to show graphs of the data onscreen. \n               Storage hubs \n             Researchers can also store their data sets in a CartoDB account, then access them (using the ubiquitous SQL database language) for other online applications, notes Desmet. For one project, he used D3 to build a map depicting radar observations of bird migration as wind-like flowing curves. The source code is stored in the repository GitHub, but the map pulls the scientific data from his CartoDB account. Despite the visual sophistication of these tools, the level of computational analysis they provide is limited. But after using these programs to get to grips with the basic principles, researchers can progress to more-powerful GIS platforms. Many scientists \u2014 including those involved in public policy, such as urban planning and crisis mapping \u2014 use  arcGIS , a suite of products maintained by Esri, based in Redlands, California. But there is also an open-source alternative:  QGIS , a project of the Open Source Geospatial Foundation. Researchers who already write code as part of their work can use programming languages such as  Python  and  R , which already have capable mapping packages that users may not even be aware of, points out astronomer James Davenport of the University of Washington in Seattle. He says that astronomers often \u201cend up bastardizing scientific visualization software to make maps\u201d. He now uses the Python package matplotlib in tandem with the rest of his Python-based analysis to project his infrared observations onto maps of the sky. Even researchers who would rather not touch a line of code can accomplish a lot with the help of CartoDB and TileMill. \u201cYou don't have to be particularly technically competent,\u201d says Gawne, who produced the Tibetan-language maps for her thesis in TileMill and now teaches mapping workshops herself. \u201cYou have to be not afraid to try it.\u201d \n                     Humanity's cultural history captured in 5-minute film 2014-Jul-31 \n                   \n                     Digital atlas shows oceans' iron levels 2014-Feb-25 \n                   \n                     Mapping the H7N9 avian flu outbreaks 2013-Apr-24 \n                   \n                     Visualization: Picturing science 2012-Jul-25 \n                   \n                     Nature Toolbox \n                   \n                     TileMill \n                   \n                     CartoDB \n                   \n                     Tableau \n                   \n                     ArcGIS \n                   \n                     SimpleMappr \n                   Reprints and Permissions"},
{"file_id": "520119a", "url": "https://www.nature.com/articles/520119a", "year": 2015, "authors": [{"name": "Jeffrey M. Perkel"}], "parsed_as_year": "2006_or_before", "body": "Consumer-oriented websites allow researchers to compare the merits of scientific journals and review their publishing experiences. What if scientific journals were like hotels, restaurants and holiday operators \u2014 easy to compare online and reviewed by those who use them? That thought occurred to conservation biologist Neal Haddaway two years ago: frustrated by a bad experience publishing his work with a journal he prefers not to name, he decided to launch  Journalysis.org , a journal-review site that he likens to TripAdvisor. \u201cI wanted to basically reward the journals that were doing a good job and, within reason, name and shame the ones that weren't doing so well,\u201d he says. Haddaway, now a project manager at the Mistra Council for Evidence-based Environmental Management in Stockholm, was not alone in his thinking. His site is one of a handful of comparison websites that have sprung up in the past few years. Those developing these tools say that, although the practice of rating journals online has been slow to take hold, the sites help authors to become discriminating consumers of publishing services, choosing the journals that suit them and dodging questionable operators. Journal-comparison tools allow authors to search or filter journals by various dimensions of performance, from prestige to publishing speed. Many of these tools are free to use, created by consultancy firms that make their money from related services for researchers, such as English-language editing and advice on publishing. Among those websites are  Journal Selector , created by the London-based firm Cofactor, collating several hundred journals;  JournalGuide , from Research Square in Durham, North Carolina, covering more than 46,000 journals; and the  Edanz Journal Selector , a site designed by the Edanz Group in Fukuoka, Japan, compiling some 28,000 titles. These comparison websites might seem unnecessary: researchers tend to identify the best destinations for their work by checking where studies they admire have been published, or by asking colleagues for advice. But Keith Collier, chief operating officer at Research Square, says that his company sees a market in researchers who may be unfamiliar with English-language journals, especially those located outside the United States and Western Europe. Even Western researchers might feel overwhelmed by the rapid growth in the scientific literature, finding it hard to keep track of the number of journals sprouting up. Online comparison tools could help them to select the best journal for interdisciplinary work \u2014 or steer them away from predatory publishers that take researchers' money, but offer little in return (see 'The right one for me'). \n               boxed-text \n             Most sites provide an indicator of prestige, such as a score that denotes how many citations on average an article in that journal accrues. But there is much more to picking journals than this one measure, says James Maclaurin, a philosopher at the University of Otago in Dunedin, New Zealand. He has developed a mobile app called  HelpMePublish  (available on Apple's iOS operating system, with an Android version in development), which indexes more than 6,000 journals. \n               The consumer psyche \n             Some researchers want only open-access journals; others care more about the acceptance rate, fees and time to publication. For Maclaurin, a killer detail is whether a journal allows 'double-blind' peer review, in which the authors' and peer reviewers' identities are withheld from each other to prevent prejudice. Often, sites are simply aggregating data from individual journals' webpages. But the relevant information is not always available: Maclaurin randomly selected 300 journals from his database, and found that nearly half of their websites made no mention of peer-review policies and only one specified the journal's acceptance rate. To get information for his database, he surveys journal editors, giving him access to details not available online. Maclaurin's app allows people to search for free, but charges for access to certain data such as a journal's acceptance rate: individual subscriptions are US$4.99 a year, and institutional ones $1,250. He says that the app has been downloaded by \u201cthousands of people\u201d from 28 countries. JournalGuide shows similar success, hosting, at present, 13,500 users per month, according to product-management director Laura Stemmle. The emergence of such tools reflects a shift in the dynamic between publisher and researcher, argues Peter Binfield, co-founder and publisher of the open-access journal  PeerJ . As long as an outlet is of reasonable quality, he says, researchers are starting to recognize that the content of their article matters more than esteem garnered from the reputation of the journal. \u201cIt's not where you publish; it's what you publish,\u201d he says. As such, authors are shifting towards a more transactional, consumer-like attitude to publishing, Binfield thinks \u2014 they are looking for the best deals on fees and time to publication (even though many still also hanker after prestige). If that is true, consumer-oriented researchers might relish the chance to read reviews and leave ratings of their own \u2014 \u201ca Yelp restaurant review for journals\u201d, as Binfield puts it. HelpMePublish restricts users to numerical ratings \u2014 on a scale of 1 to 5 \u2014 on topics such as refereeing practice and communication. But some websites enhance the experience even further. Journalysis and  SciRev  \u2014 both of which are free \u2014 provide space for free-form comments. Created by economists Jeroen Smits of Radboud University in Nijmegen, the Netherlands, and Janine Huisman of the Centre for International Development Issues Nijmegen, SciRev boasts some 14,000 journals in its database and has received more than 1,000 user reviews in its one year of operation. In theory, a lot of bad reviews might push publishers to change their procedures. But scientists have been slow to embrace the feature. Inexplicably, some publishers have received many reviews on SciRev, whereas others have received few. For example, the  Open Access Macedonian Journal of Medical Sciences  has 38 reviews (all accompanied by positive ratings), yet  Science  has 6 and  Nature  only 2, none of which includes ratings. JournalGuide initially accepted user reviews, but dropped them due to a poor response rate. Journalysis is faring little better; few researchers have left ratings. \u201cThat's where we're all falling down really \u2014 users aren't submitting enough data,\u201d Haddaway says. This is despite the fact that reviewing sites allow user anonymity. Typically, these tools require registration only with a validated academic e-mail address. \u201cI think there is just a reluctance to say something about a journal you may need to go back and try to submit something later on,\u201d says Collier. If that is true, perhaps it is not surprising that review features have yet to achieve critical mass. Even so, the lack of engagement so far has not jolted Haddaway's conviction that the tools are needed. \u201cI think there needs to be more transparency,\u201d he says. \n                     Double-blind peer review 2015-Mar-06 \n                   \n                     Open-access website gets tough 2014-Aug-06 \n                   \n                     Science publishing: The golden club 2013-Oct-16 \n                   \n                     Publishing: Open to possibilities 2013-Mar-27 \n                   \n                     Open access: The true cost of science publishing 2013-Mar-27 \n                   \n                     Cofactor Journal Selector Tool \n                   \n                     Edanz Journal Selector \n                   \n                     HelpMePublish \n                   \n                     JANE \n                   \n                     Journal Guide \n                   \n                     Journalysis \n                   \n                     SciRev \n                   \n                     Scholarly Open Access \n                   \n                     Directory of Open Access Journal \n                   Reprints and Permissions"},
{"file_id": "521111a", "url": "https://www.nature.com/articles/521111a", "year": 2015, "authors": [{"name": "Jeffrey M. Perkel"}], "parsed_as_year": "2006_or_before", "body": "Computer scientists are trying to shore up broken links in the scholarly literature. The scholarly literature is meant to be a permanent record of science. So it is an embarrassing state of affairs that many of the web references in research papers are broken: click on them, and there's a fair chance they will point nowhere or to a site that may have altered since the paper referred to it. Herbert Van de Sompel, an information scientist at the Los Alamos National Laboratory Research Library in New Mexico, quantified the alarming extent of this 'link rot' and 'content drift' (together, 'reference rot') in a paper published last December (M. Klein  et al .  PLoS ONE   9 , e115253; 2014 ). With a group of researchers under the auspices of the Hiberlink project ( http://hiberlink.org ), he analysed more than 1 million 'web-at-large' links (defined as those beginning with 'http://' that point to sites other than research articles) in some 3.5 million articles published between 1997 and 2012. The Hiberlink team found that in articles from 2012, 13% of hyperlinks in arXiv papers and 22% of hyperlinks in papers from Elsevier journals were rotten (the proportion rises in older articles), and overall some 75% of links were not cached on any Internet archiving site within two weeks of the article's publication date, meaning their content might no longer reflect the citing author's original intent \u2014 although the reader may not know this. Hyperlinks to web-at-large content were present in only one-quarter of the 2012 scholarly articles, but some four-fifths of those papers that did contain a link suffered from reference rot, the team found \u2014 that is, at least one reference to web-at-large content was either dead or not archived. Van de Sompel terms the situation \u201crather dramatic\u201d. Because the content of servers can change, or they can 'go dark' or change hands, researchers following up links to online data sets, software or other resources might have nowhere to turn. \u201cYou've lost a trace to the evidence that was used in the research,\u201d he says. \n               Snapshots of the web \n             Fortunately, online archiving services, such as the Internet Archive, make it possible for researchers to store permanent copies of a web page as they see it when preparing their manuscripts \u2014 a practice Van de Sompel recommends. He urges researchers to include their cached link and its creation date in their manuscripts (or for publishers to take a snapshot of referenced material when articles are submitted). The Harvard Law School Library in Cambridge, Massachusetts, has developed a web-archiving service called Perma.cc ( https://perma.cc ): enter a hyperlink here and the site spits back a new hyperlink for a page that contains links to both the original web source and an archived version. Van de Sompel and others have in the past few weeks rolled out a complementary approach. It relies on a service that Van de Sompel has co-developed called Memento, which he dubs \u201ctime travel for the web\u201d. The Memento infrastructure provides a single interface for myriad online archives, allowing users access to all of the saved versions of a given web page. This infrastructure could potentially allow access to web-at-large links in any scholarly article, even if the linked sites go down. Publishers would have to incorporate a small piece of extra computer code in their articles, and the standard single weblinks would have to be replaced with three pieces of information \u2014 the live link, a cached link and its creation date \u2014 all wrapped in Van de Sompel's proposed machine-readable tags. \n               Storage block \n             Van de Sompel says that he is \u201cunbelievably enthusiastic\u201d about the team's approach. But the solution depends on the cooperation of authors and publishers \u2014 who may be disinclined to help. Another issue is that web-page owners who hold copyright over content can demand that archives remove copies of it. They can also disallow archiving of their sites by including a file or line of code that prevents computer programs from 'crawling' over or capturing content \u2014 and many do. If Perma.cc, for instance, encounters such an exclusion code, it preserves the content in a 'dark archive'; to access a web page in a dark archive, the reader must contact a library participating in the Perma.cc project and request to see the site. Scholarly articles that are behind a paywall routinely exclude such crawling, too \u2014 although publishers have introduced the DOI system to ensure that scientists can confidently cite a persistent hyperlink to the right version of an online research article, even if the publisher changes its local web addresses. (In January, however, the system that redirects DOI links went down, showing that it is not immune to failure.) Publishing companies also guard against link rot by automatically preserving articles in archives; the articles can be released if the company folds. But not all companies are archiving, says David Rosenthal, a staff member at the library of Stanford University in California; analysis of data from a monitoring service called The Keepers Registry shows that \u201cat most 50% of articles are preserved\u201d, Rosenthal writes on his blog ( go.nature.com/jrwqo4 ). So for both web-at-large hyperlinks and scholarly articles, the Memento team's mission to solve reference rot may be \u201cexcessively optimistic\u201d, he says. \n                     Scientists losing data at a rapid rate 2013-Dec-19 \n                   \n                     Publishing frontiers: The library reboot 2013-Mar-27 \n                   \n                     The future of publishing: A new page 2013-Mar-27 \n                   \n                     The Memento Project \n                   \n                     Hiberlink \n                   \n                     David S H Rosenthal's blog \n                   Reprints and Permissions"},
{"file_id": "nature.2015.17382", "url": "https://www.nature.com/articles/nature.2015.17382", "year": 2015, "authors": [{"name": "Dalmeet Singh Chawla"}], "parsed_as_year": "2006_or_before", "body": "Published chart integrates data from outside scientists. In July last year, neurobiologist Bj\u00f6rn Brembs published a paper about how fruit flies walk. Nine months on, his paper looks different: another group has fed its data into the article, altering one of the figures. The update \u2014 to  figure 4  \u2014 marks the debut of what the paper\u2019s London-based publisher, Faculty of 1000 (F1000), is calling a living figure, a concept that it hopes will catch on in other articles. Brembs, at the University of Regensburg in Germany, says that three other groups have so far agreed to add their data, using software he wrote that automatically redraws the figure as new data come in.\u00a0 His article, written with Julien Colomb, chief executive of the start-up firm Drososhare in Berlin, finds behavioural differences within a strain of fruit fly: the Canton Special, or CS strain ( J. Colomb and B. Brembs  F1000Research    3,  176; 2014 ). Although there are substrains, researchers usually regard CS flies as so similar that they do not distinguish between the substrains in their analyses, but Brembs and Colomb report that the flies exhibit three types of walking behaviour. This might betoken other differences in behaviour and therefore confound experiments in which CS flies are used as a control group, he says. Having sequenced the genomes of the flies, Brembs thinks that the behaviours have a genetic origin and will not be explained away by environmental variations between labs. The addition of data by other labs could help to test whether his theory is correct. \n               Iterative publishing \n             The living figure concept fits within a central tenet of F1000\u2019s publishing philosophy, that papers can be continually updated. The online-only open-access site publishes articles immediately with the status \u2018Awaiting Peer Review\u2019, then invites scientists to review them. Authors can then update their articles with new versions. The process is like adding pieces of paper to the top of an existing pile, the publisher says. Allowing outside researchers to post their data into a paper simply takes the idea a step further, says Rebecca Lawrence, managing director of the publishing platform  F1000Research . \u201cThe idea is that it better mirrors the way science is conducted,\u201d she says. Other laboratories\u2019 information confirms or challenges the published research in an incremental process. In addition to updating work, living figures may allow systematic reviews to be updated rather than published afresh each time, Lawrence adds. They should also help to address the issue of lack of reproducibility, she argues, because it provides a way for laboratories to release confirmatory data, which can be hard to get published. Of course, by adding data to someone else\u2019s article, scientists are giving up the chance to publish a paper of their own \u2014 a potential hurdle, because publications are the lifeblood of reputations in academia. But Gregg Roman from the University of Houston, Texas, the first outside author to add data to Brembs\u2019s paper after publication, says that he accepts that. \u201cWe\u2019re sacrificing a bit of recognition,\u201d he says, but \u201cit\u2019s a more accessible way for scientists to get the answer than if we publish separately\u201d. New contributors\u2019 names do, however, appear in the legend of updated figures; and the updated data set and paper get their own DOIs. Alternatively, contributors can choose to gain a formal publication by submitting what  F1000Research  calls a Data Note that links to the original updated paper. If the new contributors\u2019 methods or results differ significantly from the original paper\u2019s, then they can publish a Research Note, Lawrence says. They can also request that the original authors update their article. An updated paper would be peer reviewed again. Lawrence says that many research groups have shown interest in publishing living figures. And the concept could work with traditional pre-publication review, too, notes Peter Binfield, co-founder of the open-access journal\u00a0 PeerJ.\u00a0 \u201cAs long as the full version history of the article is available, and it\u2019s clear which version of the article was reviewed, and in what way, it should be possible to publish updates,\u201d he says. As for Brembs\u2019s work, Roman says that his data seem to support the general trend, but with smaller differences between the flies. The question may be resolved only as figure\u00a04 evolves. \n                     Programming: Pick up Python 2015-Feb-04 \n                   \n                     My digital toolbox: Climate scientist Damien Irving on Python libraries 2015-Jan-30 \n                   \n                     Scholarship: Beyond the paper 2013-Mar-27 \n                   \n                     Beyond the PDF 2013-Jan-30 \n                   \n                     Blog post: F1000 launches fast, open science publishing for biology and medicine \n                   \n                     Blog Post: Has this research paper been updated? Click here to find out. \n                   \n                     F1000Research \n                   \n                     Bjorn Bremb's fly paper: latest version \n                   Reprints and Permissions"},
{"file_id": "nature.2015.17652", "url": "https://www.nature.com/articles/nature.2015.17652", "year": 2015, "authors": [{"name": "Dalmeet Singh Chawla"}], "parsed_as_year": "2006_or_before", "body": "Research Resource Identifier (RRID) aims to clean up poorly referenced data. A single, standard format to identify resources such as model organisms and reagents used in published experiments is catching on with researchers and journals, its originators claim. The biosciences would benefit from a universal way of citing research materials, they say. An article published before peer review at  F1000Research 1  describes the early fortunes of the Research Resource Identifier (RRID) \u2014 a citation format used in papers for referencing antibodies, model organisms and software. The RRID was launched in February 2014 as a pilot study sponsored by the US National Institutes of Health (NIH) in Bethesda, Maryland, and has since made its way into hundreds of papers, says Anita Bandrowski at the University of California, San Diego, who is coordinating the project. The scientific literature is full of references to reagents, antibodies, tissue samples and software tools, but these are often referenced so poorly that it is hard for other researchers to pinpoint the exact materials used. One study 2  found that 54% of resources are not uniquely identifiable in publications. A standard citation format does exist for many types of resource. Researchers can reference the type of  Drosophila  they used by citing the correct ID from the Bloomington  Drosophila  Stock Center at Indiana University; for mice, a similar identifier can be found at the database Mouse Genome Informatics (MGI). But in practice, Bandrowski says, researchers get confused when they have to go to different repositories to find identifiers for each resource, and often cite resources improperly. \n             One format to rule them all \n           The new ID format simply adds the prefix \u2018RRID:\u2019 to existing identifiers. Each RRID is stored at a  central website , where authors can search for their resource in any of 10 repositories or databases (including the BDSC and the MGI), retrieve the relevant ID, and click a \u2018cite this\u2019 button, which presents correctly formatted text that authors can copy into their papers. If identifiers don\u2019t yet exist for a resource, the portal does not mint fresh ones itself, but helps researchers to create one at in a suitable outpost (such as the  Drosophila  database FlyBase), by clicking on \u2018Add a Resource\u2019 at the central portal. Once created, the portal presents the identifier to the author with the RRID wrapper. \u00a0 After more than a year, the number of published papers with RRIDs has reached more than 350 \u2014 but almost 90% of them are in neuroscience journals. Bandrowski says that the identifiers will break out into other disciplines, and are being adopted by broad-scope journals such as  PLoS ONE.  \u201cThey\u2019ll be useful if they catch widespread attention and uptake,\u201d says Dan MacLean, a bioinformatician at the Sainsbury Laboratory in Norwich, UK. Because RRIDs are computer-readable, it is possible to automatically pull out lists of resources cited in articles that use the identifiers. Publishing giant Elsevier, for example, has included a box showing \u2018Antibody data cited for this article\u2019 alongside the online version of J.G. Doria  et al. 3 . The publisher is looking to extend the idea to cover software tools and model organisms too. \u201cThe concept behind the RRID project is nothing revolutionary, but is critical for science to be done accurately and reproducibly,\u201d says Joanne Berghout, who coordinates outreach for MGI. Whereas RRIDs are focused on standardizing citations of antibodies, model organisms and software, it is only one system in a sea of different standard formats for permanently identifying online scholarly content \u2014 acronyms for citation that include DOI, URI, PURL, and hdl (Handle), for example. \u201cThere is no single authority that is bringing these together,\u201d says Susanna Assunta-Sansone, who works on ways to curate and share reproducible research at the University of Oxford e-Research Centre, UK. Sansone says that heterogeneity should be embraced, and that any identifiers for research objects with aspirations of being universal should build on existing community practices. Bandrowski believes that introducing RRIDs for biological cell lines is \u201cthe next logical step\u201d. She says the team is currently liaising with the NIH to map out all entities that are worth giving identifiers to. The team may also turn to governmental bodies or the commercial sector for support with specific resource types, she says. \n                   Reproducibility crisis: Blame it on the antibodies 2015-May-19 \n                 \n                   Reproducibility: Standardize antibodies used in research 2015-Feb-04 \n                 \n                   Journals unite for reproducibility 2014-Nov-05 \n                 \n                   Policy: NIH plans to enhance reproducibility 2014-Jan-27 \n                 \n                   Announcement: Reducing our irreproducibility 2013-Apr-24 \n                 \n                   The RRID portal \n                 \n                   The Resource Identification Initiative \n                 Reprints and Permissions"}
]