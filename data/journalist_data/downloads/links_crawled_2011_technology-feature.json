[
{"file_id": "480133a", "url": "https://www.nature.com/articles/480133a", "year": 2011, "authors": [{"name": "Charlotte Schubert"}], "parsed_as_year": "2006_or_before", "body": "To understand biological heterogeneity, researchers are learning how to profile the molecular contents of individual cells. James Eberwine, a neuroscientist with a penchant for invention, helped to pioneer a technique that is now routine. In the early 1990s, he sucked the contents out of a single cell with a pipette, and examined the expression of a handful of genes using molecular techniques that amplify RNA. His data verified a long-held assumption \u2014 that electrical activity in a single neuron simultaneously changes the abundance of multiple RNAs inside it 1 . But other researchers were sceptical. At the time, just about the only way to detect RNA in a single cell was by labelling molecules using fluorescence  in situ  hybridization. \u201cPeople were used to microscopy to look at RNA; they wanted to see it,\u201d says Eberwine, who works at the University of Pennsylvania in Philadelphia. Things have changed since then. Gene-expression analyses leaped forward in the mid-1990s with the invention of the microarray. And the rise of high-throughput RNA sequencing, or RNA-seq, which spits out the sequences of thousands of cellular RNAs at once, has enabled researchers to reveal the collection of active genes in a cell in a single readout. Eberwine and others are using sequencing, and techniques such as microfluidics and flow cytometry, to profile single cells \u2014 cataloguing RNA molecules, sequencing DNA and even profiling metabolites and peptides. Studies indicate how strongly cells can show their individuality. Brain cells may express as few as 65% of the same genes as their neighbours, according to an unpublished analysis by Eberwine. In the immune system, cells placed in the same category on the basis of surface markers can express different sets of genes, and have different responses to vaccines 2 . And as tumour cells evolve, their genomes quickly become twisted in unique ways. Single-cell techniques let researchers track and catalogue this heterogeneity. They may be the only way to get at some fundamental questions, such as what makes individual cells different biochemically and functionally. How much is each cell influenced by its microenvironment, and what is the role of stochasticity \u2014 random 'noise' in the behaviour of cellular molecules? These questions are getting more attention (see  page 139 ). In 2009, Eberwine co-organized a meeting on single-cell analysis at Cold Spring Harbor Laboratory in New York, along with Sunney Xie, a single-cell biochemist at Harvard University in Cambridge, Massachusetts. The meeting drew 47 attendees. This July, 120 people went to the second such meeting. And the US National Institutes of Health (NIH) has launched an initiative to support single-cell techniques (see  'The NIH gets singular' ). But single-cell analysis is still an emerging field. Many researchers say that protocols from academic labs are often superior to commercial kits. \u201cWith any nascent field, there are lots of different approaches,\u201d says Eberwine. \u201cPeople are trying lots of things to see if they can make the techniques more sensitive, more representative of the state of a cell, easier and cheaper.\u201d \n               Profiling heterogeneity \n             The classical biochemical approach is limited, say single-cell researchers. Grinding up and analysing the contents of large pools of cells \u2014 a procedure undertaken in thousands of labs every day \u2014 averages out the results, says Timm Schroeder, director of the Institute of Stem Cell Research at the German Center for Environmental Health in Munich. But, says Schroeder, \u201cit's the individual cell that makes a decision\u201d such as whether to fire an electrical impulse, migrate or differentiate into a new cell type. It's the individual cell that makes a decision. Looking at single cells to uncover the impetus for such decisions means doing fussy experiments on a very small object: an average cell might span about 10 micrometres and contain less than 1 picolitre (1 \u00d7 10 \u221212  litres) of cytoplasm. And some key regulatory molecules are scarce \u2014 just a few, hard-to-detect RNAs can exert a big effect on a cell. Many established techniques are only now being applied to single cells. Fluorescent tagging and microscopy can be used to analyse molecules that have already been characterized. To profile previously unexamined molecules, there is transcriptome analysis \u2014 cataloguing the set of RNAs expressed in a cell \u2014 as well as high-throughput methods based on microfluidics or flow cytometry. But getting such techniques to work on single cells is not easy, says Schroeder, whose own research involves long-term imaging of single bone-marrow cells. Single-cell applications are, he says, \u201cat least one level more demanding and complex than the conventional approaches\u201d. And the unexplored biological terrain is vast. \u201cWe don't even know what we are getting into in terms of heterogeneity,\u201d says Sherman Weissman, a geneticist at Yale University in New Haven, Connecticut. Whatever the study, the first step is generally getting hold of the cells. When Eberwine first struggled with studying gene expression in a single neuron 20 years ago, it was difficult even to get intact RNA out of a single cell. Eberwine solved the problem by capturing the material in the same pipette used to measure electrical activity. Now, researchers can use a variety of techniques to pick out single cells, from enzymatic digestion, which releases cells from tissues, to laser-capture microdissection. But it is still tricky, says Eberwine. \u201cA major technical issue is how do you do that initial capture.\u201d Eberwine has used his pipette-capture system to study individual warm-sensitive neurons 3 , which regulate core body temperature and underlie fever. Along with Tamas Bartfai, a neuroscientist at the Scripps Research Institute in La Jolla, California, and his colleagues, Eberwine examined the cells' transcriptomes. The researchers identified transcripts for G-protein-coupled receptors \u2014 potential drug targets \u2014 that went undetected in screens of pooled cells. As techniques improve, they are letting researchers explore heterogeneity within a cell. By cutting branches, or dendrites, off neurons, Eberwine and his colleagues have discovered that RNAs in dendrites retain nucleotide sequences that target the RNA to that location 4 . They could not have found such information by analysing the RNA of an entire neuron. Developing technology will produce even more fresh data. Most biologists will need to work closely with computational biologists to evaluate the huge data sets that will result from cataloguing thousands of molecules in numerous single-cell experiments. \n               Extra-loud amplification \n             Perhaps the best-known single-cell profiling technique is transcriptomics. Azim Surani, a developmental biologist at the University of Cambridge, UK, uses this method to examine cells of the early embryo, which are hard to study in large batches because they are so rare. He is tracing how such cells, when cultured, turn into pluripotent embryonic stem cells. Surani has adapted a single-cell protocol for the polymerase chain reaction (PCR) to work with RNA-seq. To do this, he has collaborated with technical experts such as Kaiqin Lao, a principal scientist at Applied Biosystems in Foster City, California (a subsidiary of Life Technologies in Carlsbad). In the cells of the early mouse embryo, the team detected the expression of some 12,300 genes \u2014 75% more than were detected by microarray techniques 5 . Lao says he can now get his PCR technique to work with 1 picogram of RNA \u2014 one-tenth the amount of RNA in a typical cell. The published protocol amplifies only molecules that are 3 kilobases long or smaller, so it misses about 40% of transcripts, says Lao. He and his colleagues are using different enzymes to increase that; Lao can now amplify 10-kilobase transcripts, corresponding to about 99% of transcription, he says. Another technique to amplify a cell's RNA is antisense RNA (aRNA), an  in vitro  transcription technique from Eberwine and his colleagues 6 , in which a cell's RNA is copied into a stable DNA library, with each DNA molecule containing a short sequence recognized by an RNA polymerase. The polymerase uses the DNA library to make multiple copies of the RNA. Each approach has its advantages, and its proponents. Bias can be introduced to PCR when certain sequences dominate during amplification, so approaches based on this technique are less quantitative than aRNA. But aRNA is less efficient than PCR, and can take days, notes Weissman. Commercially available aRNA kits include TargetAmp from Epicentre Biotechnologies of Madison, Wisconsin (owned by Illumina of San Diego, California), and MessageAmp from Ambion of Austin, Texas, which is owned by Life Technologies. Both can work for single cells, says Eberwine. Companies such as NuGEN in San Carlos, California, and Sigma-Aldrich in St Louis, Missouri (in partnership with Rubicon Genomics of Ann Arbor, Michigan), have products designed for low amounts of RNA, and some say that their systems can work for single cells. It is unclear when Life Technologies might release a product based on Lao's method, but both Eberwine and Lao report that their single-cell techniques are being used successfully in other researchers' labs. \n               The genome gap \n             Many researchers want to analyse not just the transcriptome of a single cell, but the underlying genome. This would be particularly relevant for cancer cells, with their warped DNA, and Life Technologies is offering a US$1-million prize to the first researchers who sequence the entire genome and RNA content of a single cancer cell using the company's technology. Nicholas Navin, a geneticist at the MD Anderson Cancer Center in Houston, Texas, is one of only a handful of researchers who have successfully sequenced the genomes of single cells from eukaryotic organisms. This year, in collaboration with Michael Wigler, a geneticist at Cold Spring Harbor, and his colleagues, Navin sequenced the DNA of 100 individual cells in each of two human breast tumours, tracing how cancer evolves 7 . It took several years and cost about $2,000 per cell; the cost has since fallen to about $200 per cell, he says. In the end, Navin was able to reliably cover about 6% of the genome of a single cell \u2014 enough to assess some larger copy number aberrations, but not to look at the accumulation of point mutations during tumour evolution. Every neuron is probably different from every other neuron. The limitation, say Navin and other researchers, is the technique used to amplify the DNA: whole-genome amplification, which relies on an enzyme that preferentially copies some genomic regions but skips others. By tweaking this step, Navin says, he is now exceeding 50% total coverage from the genome of a single human cell, although his work has not yet been published. Navin is not the only one tackling this problem. At the Cold Spring Harbor Meeting this year, Xie said that he and his colleagues are able to sequence 85% of the genome of a single mammalian cell. The paper describing it has not yet been published \u2014 but researchers who have seen the data are impressed. \u201cSunney nailed it,\u201d says Lao. This is welcome news to researchers such as Fred Gage, a neuroscientist at the Salk Institute for Biological Sciences in San Diego, California, who wants to sequence individual neurons. He has found that long interspersed elements (LINEs) \u2014 DNA sequences that can move around in the genome \u2014 form new insertions when neurons are born from neuronal stem cells 8 . Every neuron probably contains unique LINE insertions, with most cells having between 80 and 300. \u201cEvery neuron is probably different from every other neuron,\u201d says Gage. \n               One device, lots of information \n             Questions on single cells often lead researchers to difficult experimental terrain. For help in navigating such tricky territory, Gage recommends collaborating with the best technical experts; he is working with Roger Lasken, a leader in sequencing single unculturable microbes at the J. Craig Venter Institute in San Diego. But many researchers venturing into single-cell analysis will be on their own, so techniques will have to become more automated, integrated and kit-like, says Jonathan Sweedler, a chemist at the University of Illinois at Urbana\u2013Champaign. \u201cResearchers will be able to buy a device that has 48 steps incorporated into one platform,\u201d he says. Widespread uptake of single-cell analysis will also require high-throughput analyses of dozens to thousands of cells to tease out measurement errors from real heterogeneity. Several companies are working on these goals, offering miniature devices that integrate multiple steps for the high-throughput analysis of single cells. Fluidigm of South San Francisco, California, markets a microfluidic system that can simultaneously analyse 96 genes in 96 individual cells using quantitative PCR. Fluidigm systems have been deployed to uncover previously unrecognized subsets of immune cells 2 , and to examine variability in the response of single cells to cytokine signalling 9 . RainDance Technologies of Lexington, Massachusetts, also sells microscale kits to analyse single cells. Usability and high throughput are a boon for miniature devices. Integrated steps also help to conserve precious samples, and small volumes can aid the dynamics of biochemical reactions \u2014 for instance, they can reduce amplification bias in PCR reactions, notes Stephen Quake, a bioengineer at Stanford University in California, and co-founder of Fluidigm. \u201cWorking with small volumes gives you some real technical advantages,\u201d says Quake, whose lab is harnessing microfluidics to develop a technique for single-cell transcriptomics, and has created a device to isolate and sequence single chromosomes 10 . \n               Putting it all together \n             But high-throughput techniques will be limited if what they measure is too simple. To grasp how a cell works, \u201cyou need to understand not just chemistry, but spatial and temporal information\u201d, says Daniel Chiu, a chemist at the University of Washington in Seattle. To integrate these analyses, his lab combines microfluidics, nanomaterials and optics. Chiu's team has developed a technique for single-cell nanosurgery using a 'vortex trap', an optical method that can manipulate organelles or liquid droplets. The group has isolated single mitochondria from cells and prepared them for analysis on a 'droplet nanolab', which deploys a vortex trap to fuse droplets and change the concentration of reagents 11 . Chiu's lab has also developed microfluidic devices for quantifying fluorescently tagged molecules, and for detecting and analysing cells that are rare in a population, such as circulating tumour cells (see  'Beyond amplification' ). Ultimately, a combination of techniques will be necessary for researchers to attain their goal of measuring multiple parameters in a single, living cell. \u201cThe more parameters you can define \u2014 the transcriptome, the peptide-ome, how a cell looks, how it responds to drugs \u2014 the more information you are going to get out,\u201d says Eberwine. Eberwine is confident that these methods will emerge, even if it takes years. \u201cI think we will be successful,\u201d he says, \u201cand if we are not, somebody else will be.\u201d \n                     Single-cell analysis supplement - Nature Methods \n                   \n                     A closer look at the single cell \n                   Reprints and Permissions"},
{"file_id": "473403a", "url": "https://www.nature.com/articles/473403a", "year": 2011, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "Biologists have copied an existing genetic code, but haven't yet commercialized it or written their own. What will it take for a tour de force to reach industrial force? A year ago this week, headlines trumpeted that humans had created artificial life. Scientists at the J. Craig Venter Institute in Rockville, Maryland, had chemically synthesized DNA and placed it inside a bacterial cell emptied of its own genetic material. Tests a few days after the insertion showed that the 1-million-base-pair-long synthetic genome was able to run the cellular machinery 1 . Whole-genome engineering could one day create cells unbound by biochemistry as we know it, says George Church, a geneticist at Harvard Medical School in Boston, Massachusetts. Researchers might even be able to design a new genetic code, one that could incorporate more than the 20 or so amino acids used by natural living systems. That achievement is \u201cgoing to be more than an increment\u201d, says Church, \u201cthat's going to be a game-changer\u201d. But current reality is more prosaic. As Venter Institute staff celebrated their cell's first birthday with a chocolate-and-spice layer cake topped by a miniature microscope made of sugar, they were well aware that the era of synthetic genomes still faces plenty of growing pains.  Breathless headlines notwithstanding, the Venter Institute team did not create life so much as copy an existing plan. In this case, they acted more like scribes than authors. Synthetic biologists are also working on changing DNA sequences \u2014 trying to engineer microbes for practical applications such as decontaminating toxic waste, tracking down tumours or secreting biofuels \u2014 but few work with more than ten genes at a time. The story of the field so far is, \u201ccan write DNA, nothing to say\u201d, says Drew Endy, a synthetic biologist at Stanford University in California. \u201cWe can compile megabases of DNA, but no one is designing beyond the kilobase scale.\u201d \u201cMost of us are still working on a small scale because there are interesting questions there and because that's what we have the technology to build,\u201d says James Collins, a biomedical engineer at Boston University in Massachusetts. \u201cWe frankly don't understand biology well enough to start designing genomes  de novo .\u201d Many technologies must fall into place before researchers will be able to routinely work with even tens of genes at a time. Putting together huge DNA molecules is time-consuming and expensive, and designing biological components to perform a particular task is a challenge for parts of genes, let alone whole genomes. Transplanting DNA molecules into cells is not easy, nor is getting the DNA to 'boot up' once it is in place. And because the genomes will be far from perfect, researchers will need ways to tweak and test many variants. No one expects fast results, and much of the work will be tedious. The Venter Institute spent 15 years and US$40 million creating the technology to build and transplant a genome. The 2010 paper lists two dozen authors. \u201cThis was a debugging process from the beginning,\u201d says Craig Venter, founder of the institute. \u201c99% of our experiments failed.\u201d And failed experiments were costly: a single error in a million base pairs set the project back months. Not counting scientists and their equipment, four species were involved in the genome transplant:  Mycoplasma mycoides  to provide the source code,  Escherichia coli  to copy DNA pieces, baker's yeast ( Saccharomyces cerevisiae ) to assemble them into a million-base-pair circle and  Mycoplasma capricolum  to provide the recipient shell. No wonder more synthetic biologists are thinking about parts of genes than are dreaming of constructing whole genomes. \n               Learning to write genomes \n             Synthetic biology often adopts the language of engineers: rather than talking about genes, networks and biosynthetic pathways, practitioners prefer to talk about parts, devices and modules. 'Parts' refer to the protein-coding section of a gene and sundry regulatory sequences that tune gene expression. A 'device' is an assembly of parts that together perform a particular function, often turning a protein's production on or off. And a 'module' or pathway is a collection of devices that carry out more-complex functions, such as coordinating a chemical synthesis or shunting cells between 'growth' and 'production' modes. Jeff Hasty, a bioengineer at the University of California, San Diego, used three genes to make bacteria light up in sync 2 . Each gene is indirectly activated by the same small molecule: one controls the production of the molecule, another directs its degradation and the third makes the fluorescent protein that causes the cell to flash. The molecule diffuses between cells and coordinates bursts of protein production. Another example of bioengineering involves a dozen or so genes from multiple species. Jay Keasling, a biologist at the University of California, Berkeley, engineered  E. coli  and yeast cells to make a precursor of the malaria drug artemisinin at one-tenth of the cost incurred by the conventional method of production: extracting the natural product from sweet wormwood 3 . (More importantly, microbes grow faster than the plants, which are in limited supply.) Sanofi-aventis in Paris and the Institute of One World Health in South San Francisco, California, plan to start distributing the synthetic form of artemisinin next year.  But Keasling's achievement is an object lesson in the time and expense involved. He and his colleagues began work a decade ago and had a $43-million grant from the Bill & Melinda Gates Foundation in Seattle, Washington, to Keasling's lab and to Amyris Biotechnologies in Emeryville, California, a company that Keasling co-founded in 2003. Researchers had to track down a previously unidentified enzyme, and engineered a dozen further yeast enzymes not just to work in  E. coli , but also to operate at the right levels to move chemical intermediates towards a desired product without poisoning the cell or wasting resources. To speed up such projects, the Massachusetts Institute of Technology (MIT) in Cambridge maintains a Registry of Standard Biological Parts ( http://partsregistry.org ) that lists thousands of components. However, descriptions of these parts are often incomplete, and they don't all work as described. To address this issue, Endy and bioengineers from the University of California, Berkeley, launched the International Open Facility Advancing Biotechnology (BioFab) in Emeryville in 2009, with a grant from the National Science Foundation. The BioFab aims to boost the supply of working parts, both by optimizing the parts themselves and by developing systems to swiftly design genetic constructs. The goal, says Endy, is to create a set of genetic regulatory elements to precisely control the rates and levels of protein production. The BioFab currently provides 350 promoters, grouped into ten levels of protein production. Having a range of options is important, says Endy, because using the same sequences multiple times makes genetic constructs unstable. The team is assessing how these elements behave in systems and under different  E. coli  growth conditions. Eventually, the researchers hope to create vast libraries combining variants of different parts. This will let them compare the parts' performances, and pick the best ones. Computer analysis will then be used to model how different sequences affect gene expression, which can in turn predict how new combinations of parts will function. But the  in silico  design process can go only so far. \u201cModels are not yet as predictive as they could be,\u201d says Adam Arkin, a bioengineer at Berkeley and co-director of the BioFab. \u201cIn almost all cases of real application we are faced with some tinkering,\u201d he adds. And the more parts are combined, the more unpredictable are the results. \n               Some assembly required \n             The biological parts are generally easy to come by \u2014 short stretches of DNA can be ordered from a variety of companies (see  'Making DNA on the cheap' ) \u2014 but physically assembling multiple parts can be cumbersome and expensive. DNA molecules are either designed using complementary DNA sequences or mixed in with DNA complementary to the opposing ends of the molecules that are to be joined. These are combined with enzymes that cut and join DNA. Researchers can link elements using a system called BioBricks, in which sequences are cut out of circular genetic elements called plasmids by restriction enzymes specific to a particular series of nucleotides at the start and end of the sequence. The desirable parts are then stitched together into larger plasmids by other enzymes. (New England BioLabs in Ipswich, Massachusetts, sells a kit of the necessary enzymes and buffers.) Assembled sequences can then be replicated in bacteria. Each assembled DNA piece starts and ends with the same sequences as the component parts, theoretically allowing larger and larger components to be assembled sequentially. But only three elements can be put together in a single reaction, which generally takes a couple of days. Reactions are also less successful with longer molecules, discouraging long assemblies. Tom Knight, a computer scientist and co-founder of start-up company Gingko BioWorks in Boston, invented BioBricks while working at MIT and has redesigned the system for industrial applications. The proprietary version can assemble up to ten parts in a single reaction, says company co-founder Barry Canton. This allows researchers to work on DNA molecules with as many as 100,000 base pairs, although most of the pathways that Gingko is working on are half that size. Just as importantly, most assembly steps can be performed by liquid-handling robots. For example, rather than bands of DNA being isolated from a gel, as in most methods, DNA molecules are collected onto and separated by suspended magnetic beads. Such automation speeds assembly and frees up lab technicians for more complicated tasks. But BioBricks-type methods are limited by their use of restriction enzymes. Because the enzymes cut DNA whenever they encounter a particular series of nucleotides, there are 'forbidden sequences' that must be excluded from the genetic construct to avoid errant cutting. The larger a construct becomes, the harder it is to avoid such sequences. To circumvent this problem, researchers have developed assembly 'overlap' methods, in which opposite ends of molecules are joined as DNA is copied. Dozens of separate pieces of DNA can be assembled in the same reaction, often totalling a few thousand nucleotides. These methods have their own drawbacks, however. Most copy DNA using the polymerase chain reaction (PCR), which can introduce errors.  There is a bewildering array of overlap assembly techniques. 'Gibson assembly', invented by Daniel Gibson and his colleagues at the Venter Institute, allows many sequences to be assembled in parallel, and can even stitch together entire genomes 4 . In one demonstration, the team started with six hundred '60-mers' (oligonucleotides 60 base pairs long), and went on to assemble the 16.3-kilobase mouse mitochondrial genome 5 . Other methods include Golden Gate Shuffling, sequence- and ligation-independent cloning (SLIC), splicing by overlapping extension (SOEing), enzymatic inverse PCR (EIPCR), overlap extension and more 6 . Some commercial kits are available: In-Fusion, from Clontech in Mountain View, California, has a mix of enzymes that can assemble 15-base-pair overlaps of any desired sequence. Life Technologies in Carlsbad, California, sells a plasmid-construction kit, MultiSite Gateway, that can join molecules with specific overlap sequences; it also markets the GeneArt High-Order Genetic Assembly System, which can assemble 10 DNA molecules, totalling up to 110 kilobases. Researchers also design their own assembly reactions. To help this, the Joint BioEnergy Insttiute in Berkeley has invented a design tool, dubbed j5, that let researchers work with several DNA assembly protocols. It determines which overlap sequences to use, recommends the sequences to order from vendors and can instruct liquid-handling robots. Synthetic Genomics in La Jolla, California, which was co-founded by Venter, plans to start offering fee-for-assembly services later this year. Assemblies larger than about 100 kilobases may be best put together inside cells, because big DNA molecules are fragile and difficult to manipulate.  In vitro  replication is also less accurate than cells' machinery. The Venter Institute team managed to assemble a 583-kilobase genome  in vitro 7 , but it ultimately developed an  in vivo  assembly system for its synthetic genome. Larger genomes than that of  M. mycoides  have been assembled inside cells, albeit not from synthetic starting points. In 2005, Mitsuhiro Itaya, a biochemist now at Keio University in Tsuroka, Japan, and his colleagues constructed a 3,500-kilobase cyanobacterium genome 8 . They cut the genome of the bacterium  Synechocystis  PCC6803 into large chunks and propagated them in specially prepared plasmids in  E. coli . The plasmids were then transferred into a third species,  Bacillus subtilis , where the DNA was stitched together.  Assembly methods aren't interchangeable. Overlap sequences that work for one method often don't work for others, so researchers who run into problems with one technique have to start from scratch, says Tom Ellis, a synthetic biologist at Imperial College London. Ellis is working with Geoff Baldwin, a biochemist also at Imperial, and other colleagues to develop rules to find out which sequences will work with multiple overlap techniques, including recombination in yeast and  Bacillus . That way, if one technique doesn't work, researchers can try others quickly. These standards will also allow researchers to assemble DNA pieces in any order, says Ellis. Although a dictated order of assembly is fine for copying an existing genome, it does not let synthetic biologists test multiple possibilities. That issue is going to become more important as researchers move from working with thousands of base pairs to tens of thousands (see 'Sizing up synthetic DNA'). If researchers start building genomes or even large parts of genomes, they will have to think about how the DNA will wrap up on itself, and how they can place genes in chromosomes so that they end up in the right places, says Ellis. \u201cIt's a whole other aspect we'll have to uncover if we're going to do genome engineering.\u201d  \n               Editing is essential \n             Jef Boeke, a molecular biologist at Johns Hopkins Medical Institute in Baltimore, Maryland, believes that genome-scale engineering is coming more quickly than many think. He is building artificial yeast chromosomes, each about the same size as the  M. mycoides  genome. Although he hasn't yet been able to design an entire new genome, he has developed techniques to make systematic alterations in existing genetic codes. \u201cIt opens the door to a lot of imaginative change at the genome scale that wasn't possible before,\u201d he says. For example, one systematic study in 2008 deleted introns (regions within genes that don't code for protein) from many yeast genes individually, and found that the procedure had surprisingly little effect on the growth and fitness of cells 9 . Boeke wants to use his techniques to find out what will happen if all introns are removed from the genome at once. But new possibilities introduce new problems. For the next few years, large genome assemblies are going to take months to build. With every assembly, researchers will detect unanticipated errors or realize after the fact that another sequence should work better, predicts Ellis. Then they will need to decide whether to assemble the whole genome again, or just edit it. \u201cThere has not been widespread acknowledgement in the synthetic-biology community that this is going to be an issue as we go into bigger assemblies,\u201d he says. The problem has already made itself felt: a quotation that the Venter Institute had incorporated into its synthetic genome turned out to contain a mistake, and is going to be altered. Another use of editing is to produce and compare many gene variants. In a colourful demonstration in 2009, Church and his colleagues described a high-throughput editing system. Multiplex-automated genome engineering (MAGE) mixes bacteria with synthesized stretches of DNA that are designed to target many areas in the genome; carefully timed jolts of electricity cause the bacteria to take up the DNA as they grow in culture. Church used MAGE to alter 24 genes in  E. coli  at once, focusing on those involved in making lycopene, an antioxidant and pigment found in tomatoes. Within three days, some bacterial cells were making five times more of the red stuff than cells in the starting population 10 . The need for custom equipment and the difficulty of purifying transformed cells has kept researchers from widely adopting the technique, but the sheer number of genetic possibilities that can be tested using MAGE is a huge advantage, says Church. As many as 4 billion  E. coli  genomes were produced in the course of one experiment. \u201cYou're not resting on the outcome of one construct,\u201d he adds. Mutagenesis and directed evolution of existing genomes could also help synthetic biologists to make up for current gaps in knowledge, says Collins. As more genes are brought into the system, he says, \u201cuncertainty goes up exponentially, and you run up against the limits of what you can do modelling-wise\u201d. And although computational approaches are not yet sophisticated enough to design new genomes, they are good at modelling existing ones, he says. This understanding could help researchers to co-opt existing cellular networks to perform desirable tasks. \u201cWe are starting to see labs recognize that there is a lot to be exploited inside the cell,\u201d says Collins (see  'The useful genome' ). \n               Biology matters \n             The most difficult problem may well be one of the least discussed: putting the genome to work. Although Itaya has synthesized large genomes inside cells, the introduced genomes do not go on to produce proteins. Venter's group had originally chosen  Mycoplasma genitalium  for the synthesis project because its genome was, at the time, the smallest known: only 583 kilobases. But  M. genitalium  grows so slowly that the team switched to its faster-growing cousin, even though its genome is twice the size. Making the DNA is not the rate-limiting step, says Venter. \u201cIt's much more dealing with the complexity of biology versus the chemical synthesis,\u201d he says. In fact, Venter thinks that adapting genomes to work in different cell types may be one of the most difficult tasks. The creation of the first synthetic cell is illustrative: the team had to remove certain enzymes from recipient cells to keep them from cutting up the foreign DNA. And moving to other species is going to be even more difficult. Unlike  Mycoplasma , many microbes contain tough cell walls that resist the introduction of DNA. \u201cIt's quite likely that transplantation will be the unique step for each species,\u201d says Venter. Like a child learning to write, researchers must be able to copy natural genomes before they can create new ones. One day, geneticists will be able to design code on large scales, fuelling as-yet-undreamed-of applications, says Venter. \u201cAfter we sequenced the genome, analysts were arguing that there was no more need for sequencing, and I argued that this was the starting point.\u201d The question of whether whole-genome synthesis will be useful will prove foolish in time, Venter believes. \u201cIt's like asking, 'why would you want to invent an airplane when people already have horses?'\u201d \n                     Five hard truths for synthetic biology \n                   \n                     Synthetic-biology competition launches \n                   \n                     Researchers start up cell with synthetic genome \n                   \n                     Sizing up the 'synthetic cell' \n                   \n                     Hymn to Gibson assembly \n                   \n                     International Meeting on Synthetic Biology \n                   Reprints and Permissions"},
{"file_id": "470289a", "url": "https://www.nature.com/articles/470289a", "year": 2011, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "A DNA sequence isn't enough; to understand the workings of the genome, we must study chromosome structure. The next frontier of genomics is space: the three-dimensional structures of chromosomes coiled in the nucleus. Far from being the random result of packing 2 metres of DNA into a sphere perhaps 10 micrometres across, the structures vary across cell types and exert an as-yet-mysterious influence on gene expression. Efforts to decipher the effects of structure face many difficulties, not least that researchers are still trying to find out how chromosomes shift as cells change, says Thomas Cremer, a geneticist at the Ludwig Maximilian University of Munich in Germany, who has studied the spatial organization of the genome since the 1970s. \u201cThe nucleus is still an uncharted landscape and it is embarrassing how little undoubtedly proven knowledge we have about its dynamic topography,\u201d he says. The basics have been known for decades: DNA double helices coil around proteins called histones, forming 'chromatin' strands that in turn are bundled into chromosomes. But when it came to the twisting and turning of chromosomes themselves, \u201cit wasn't clear what role genome organization was playing or even if there was that much organization\u201d, says Peter Fraser, a genome biologist at the Babraham Institute in Cambridge, UK. Long-range interactions seemed implausible. \u201cPeople assumed that sequences 50 kilobases away couldn't find each other in the nucleus,\u201d he says. These days, scientists know that such interactions happen all the time. In 2002, Fraser's laboratory was among the first to detect 'long-range looping interactions' that bring gene sequences into physical contact with far-off regulatory elements 1 . More-global changes also occur. For example, inactive chromatin is generally shunted to the nuclear periphery, but that arrangement is inverted in mouse retinal cells, allowing more light to reach photoreceptors 2 . That the spatial organization of the genome is important is also demonstrated by the havoc that alterations can wreak. A cancer of the lymphatic system called Burkitt's lymphoma occurs after a chunk of chromosome 8 ends up on chromosome 14 and vice versa. This happens because of the way that chromosomes arrange themselves in white blood cells 3  \u2014 translocations occur more often between genes that physically come together during transcription 4 . Various types of cancer have been found to be connected with mutations in proteins that affect chromatin structure, and researchers have speculated that long-range interactions can be altered by disease-associated mutations in stretches of DNA that do not code for genes. \n               Answers in the structure \n             Researchers have long known that DNA sequences and histones are tagged with chemical modifications that turn genes on and off; the cataloguing of such 'epigenetic' modifications is well under way. It is now becoming clear that the three-dimensional organization of chromatin reflects a higher order of epigenetic regulation, says Yijun Ruan, a biologist at the Genome Institute of Singapore, who has developed techniques to find long-range interactions mediated by specific proteins 5 . Instead of assuming that gene activity is determined entirely by chemical attachments along a linear DNA sequence, researchers are looking for answers in the ways that chromatin folds, moves and communicates. Discussions are beginning to include phrases such as 'chromatin network', 'chromosome interactome' and 'spatial epigenetics'. A suite of technological innovations is starting to reveal the significance of such concepts. New microscopes are letting researchers look more closely at more nuclei, for example, and experiments are allowing researchers to identify interacting sequences or to locate sequences within the nucleus. But challenges remain: chromosomal movements are dynamic and non-deterministic, so detecting what is where, and when, is difficult. Even more difficult is figuring out when and how genome architecture affects gene activity.  Until the beginning of this century, nearly all techniques that were used to study chromosome arrangements relied on microscopy. Researchers could label certain DNA sequences or DNA-associated molecules, and see where the labelled areas were inside the nucleus. But a strand of chromatin is only about 10 nanometres thick, and conventional fluorescence microscopy has a resolution at best of 200 nanometres. Thus, microscopy can reveal that two loci are close to each other, but not whether they come into contact. Moreover, if an interaction is fragile or short-lived, microscopy can miss it altogether. When Job Dekker was a postdoctoral researcher studying the mechanics of cell division at Harvard University in Cambridge, Massachusetts, he wanted to map the DNA sequences that mediated interactions between chromosomes. One day, while commuting to his lab, he hit on the idea of capturing an interaction by chemically snagging two strands of chromatin that approached one another, then fusing the DNA from both into a single molecule. \u201cYou start out with a difficult problem \u2014 where are two loci in three dimensions \u2014 and you convert it through a series of molecular steps to a simple problem, just sequencing a piece of DNA,\u201d says Dekker, now a genome biologist at the University of Massachusetts Medical School in Worcester.  Dekker's idea became a technique, described in the literature in 2002, known as chromosome conformation capture (3C; ref.  6 ). It has since spawned many variations (see  'Investigating the architecture' ), but the basic principles are the same. Protocols begin with 'cross-linking': dousing cells with formaldehyde to glue the DNA to its associated proteins, and those proteins to each other. Then the DNA is cut up with restriction enzymes or sheared by sonication, leaving behind 'hairballs' of tangled DNA and protein. The next steps vary between protocols, but all combine free strands of DNA to create hybrid molecules: ligation products of DNA strands that had been close together on the same hairball. Researchers interested in genes that are associated with a particular transcription factor or other DNA-associated protein use specially designed antibodies to capture the relevant hairballs. In some techniques, chemically modified nucleotides are incorporated into hybrid molecules to ease purification, whereas in others, judicious application of PCR amplifies DNA sequences near loci of interest. \n               The medium matters \n             No matter which technique is used, researchers need to be careful when choosing their restriction enzymes. For example, those that cut at sites made up of 6-base-pair sequences produce large fragments that may not capture important interactions, whereas enzymes that recognize sequences of 4 base pairs may produce more and smaller fragments, perhaps generating so much background information that real interactions cannot be detected. Researchers also need to keep in mind that most of the hybrid DNA molecules produced by this technique are the result of random interactions, particularly between loci that are just a few kilobases apart on the same chromosome; separating the signal from the background noise requires involved bioinformatics and replicated experiments. \u201cIt used to be, even two years ago, that getting the data would be an endpoint of the project. Now it's the start,\u201d says Dekker. On the plus side, preparing libraries of ligation products requires only very general reagents: formaldehyde, a variety of buffers and the enzymes that cut DNA and join it back together. Moreover, all the necessary reagents can be purchased from established companies: Life Technologies of Carlsbad, California; New England Biolabs of Ipswich, Massachusetts; QIAGEN of Hilden, Germany; Sigma-Aldrich of St Louis, Missouri; and Thermo Fisher Scientific of Waltham, Massachusetts. Researchers can also order specially synthesized primers for DNA amplification or ligation from a large range of (generally smaller) providers. Different techniques generate different information. A million sequenced molecules (or 'reads') for Hi-C (high-throughput 3C) provides a low-resolution map of the whole human genome, whereas a million reads for 4C (circular 3C) produces a detailed interaction map for a gene of interest, and in ChIA-PET (chromatin interaction analysis by paired-end tag sequencing) the same amount of data indicates which transcription-factor binding sites interact with which gene promoters. This summer, Life Technologies plans to launch a kit that bundles together reagents for 3C experiments. The kit would allow researchers to monitor and optimize digestion, use less of the sample for ligation and produce a library of ligation products in 1.5 days, says Shoulian Dong, a technology developer at Life Technologies. But perhaps the most important factor for throughput is the increasing availability of next-generation sequencers from companies such as Applied Biosystems of Carlsbad, California, and Illumina of San Diego, California, which can quickly sequence the hundreds of thousands of short hybrid DNA molecules produced in these experiments. \n               From sequences to ideas \n             The ability to detect specific interacting loci is already revealing previously unknown biology. Last September, researchers led by Richard Young, a molecular biologist at the Massachusetts Institute of Technology in Cambridge, described evidence for a biological system that juxtaposes separate stretches of DNA. Together, these stretches control gene expression. The team found that a 'mediator' protein complex was often bound to enhancer sequences and core promoters of genes transcribed in embryonic stem cells 7 . Another protein, cohesin, which can connect two DNA segments, was bound along with mediator, and purified with it. Follow-up 3C studies on four genes showed increased interactions between promoter and enhancer sequences in stem cells, but not in another type of cell in which the genes were inactive 7 .  For Wouter de Laat, a genome biologist at the Hubrecht Institute in Utrecht, the Netherlands, who showed how 3C can be used to match a gene with its regulatory elements 8 , the most exciting applications of chromosome capture technology are global: working out which sites interact with which genes in different tissues. \u201cThere are many more sites with regulatory potential than we have genes, and the only way to know which site is acting on which gene is to get three-dimensional,\u201d he says. \u201cThat's the next level of what we need in functional genomics.\u201d Current techniques are not powerful enough to match regulatory elements and genes across the genome, but de Laat and other labs are working on more far-reaching methods, which they hope to describe in the literature this year. It is useful to ask genome-wide questions because, otherwise, researchers tend to interpret their results only in the context of the gene they happen to be studying, says de Laat. But because every gene is part of a chromosome, those observations could have less to do with the gene under study than with its neighbours. Adding to the challenge is another signal-to-noise problem: all the current techniques have to be carried out on between 10 million and 20 million cells at once, which means that the observed interactions represent an averaged reading. No one believes that all the interactions identified by sequencing technologies occur in any one cell, says Tom Misteli, who studies the cell biology of genomes at the US National Cancer Institute in Bethesda, Maryland. \u201cAny interaction that happens will appear as a signal, but it doesn't tell you how often it happens in cells,\u201d he adds. \u201cThat makes the interpretation of the sequencing data a little bit complicated.\u201d  \n               Seeing is believing \n             To find out how often interactions occur, researchers have to count labelled cells under a microscope. For live-cell imaging, they can insert genes for fluorescent proteins that bind to desired DNA sites into the cell, but the technique is labour-intensive and tedious. A fixed-cell technique, fluorescence  in situ  hybridization (FISH), is more common. Nuclei are treated with formaldehyde, then denatured just enough to allow the entry of DNA probes that fluorescently label certain sequences. In general, interactions identified by chromosome conformation studies are observed in only about one in ten cells under the microscope, says Misteli. That doesn't mean that the interaction isn't real; randomly selected loci are seen near each other even less often. Instead, such rates show just how dynamic and varied chromosome arrangements are, and how difficult they can be to study. Last year, Fraser and his colleagues combined chromosome capture technology with microscopy to show that a single transcription factor, Klf1, helps to bring target genes from distant loci into a cluster in a common space 9 . Such studies of 'transcription interactomics' could reveal secrets of cell differentiation and stability, but mastering the necessary technologies is a formidable task. To separate relevant hybrid molecules from background signals, the researchers made significant tweaks to the 4C technique. And to show that multiple loci came together at the same time, lead author Stefan Schoenfelder looked at some 50,000 cells under a microscope: the equivalent, Fraser says, of spending half a year in a dark room. That situation is familiar to Misteli, who in 2009 used FISH to show how genes reposition themselves in cancer 10 ; such knowledge could aid diagnosis. Genes generally move from the periphery of the nucleus towards the centre when they become active, but individual genes move in unpredictable ways. No one has yet been able to look at gene positioning comprehensively, to discover how it might vary across different cell types, says Misteli. \u201cIt's all based on small sample numbers and people's favourite genes. So you want to look at more genes and that's simply not possible.\u201d Technologies are improving, letting researchers look at more cells; Fraser says that currently available microscopes with faster autofocus and more-agile robotic stages would now let Schoenfelder perform the same number of experiments in a month or less. Platforms are available: PerkinElmer in Waltham, Massachusetts, sells the Opera high-content screening system, which keeps the objective lens immersed in water. This allows it to work at the high resolutions required to determine where sequences are in the nucleus. The instrument automatically moves along wells on a plate to collect the necessary data, and its four different-coloured lasers can light up several probes in each cell. The Opera instrument can examine loci in hundreds of cells a minute \u2014 considerably faster than stand-alone microscopes \u2014 and can make difficult techniques more accessible to non-experts, says Achim von Leoprechting, vice-president of imaging at PerkinElmer. \u201cWe're seeing FISH moving out of specialized labs,\u201d he says, \u201cso from an imaging standpoint we need to make sure they can use these platforms and get high-quality data without being trained as microscopists.\u201d Researchers who are already studying the position of genes in the nucleus are particularly keen to examine more cell types under different conditions, says Aaron Risinger, a specialist in high-content screening at PerkinElmer. \u201cFor individuals who were doing one-off experiments, the natural progression is to move to high-throughput,\u201d he says. In fact, Misteli is doing just that by incorporating the platform into a new US National Cancer Institute facility aimed at ultra-high-throughput cell biological imaging.  Lower-throughput techniques also have their advocates. Ana Pombo, a cell biologist at Imperial College London, has developed the cryoFISH technique: rather than fixing and denaturing intact cells, researchers embed cells in a sugar solution, carefully freeze them, cut them into thin slices, then add DNA probes 11 . The process is technically demanding but produces fewer artefacts and better resolution than standard FISH because the probes don't need to move through an entire nucleus. Pombo has used cryoFISH to show that chromosomes keep largely to their own 'territories' but intermingle extensively 9 . Electron microscopy has very high resolution, but the staining and imaging of cells can take days. In the past three years, researchers have turned to super-resolution optical microscopy, which uses techniques such as synchronized laser pulses to focus on structures as small as 15\u201320 nanometres \u2014 well below the 200-nanometre resolution limit of conventional optical microscopy \u2014 even in living cells. Companies selling these new microscopes include Applied Precision of Issaquah, Washington; Leica of Wetzlar, Germany; Nikon of Shinjuku, Japan; and Zeiss of Oberkochen, Germany, but the instruments have not yet reached most laboratories. \n               A third way \n             Ultimately, all microscopy is a coarse detection technique, says Rolf Ohlsson, an epigeneticist at the Karolinska Institute in Stockholm. Standard fluorescence microscopy cannot distinguish between loci that are near each other and those that are in contact; even super-resolution microscopy cannot do so definitively. On the other hand, sequencing techniques cannot show which interactions occur together, says Ohlsson. \u201cSomewhere between DNA FISH and chromosome conformation capture is the truth,\u201d he adds. But even accurate representations will not be enough: ascertaining that an interaction occurs is far easier than showing that it affects function. \u201cIs what you see an interaction?\u201d asks Ohlsson. \u201cOr just a collision?\u201d Several groups are attempting to use conformation capture to build computational models that show the positions of chromosomes in different cell types and at different stages of the cell cycle. To construct these models, researchers do not actually measure distances between two loci; instead, they use algorithms to process captured DNA sequences. The programs produce 'proximity profiles' from sequencing data by measuring how frequently regions of the genome are observed to interact with one another, and comparing that with what would be predicted from chance. In 2009, Dekker and his colleagues constructed a model of human cells that breaks the 3-billion-base-pair genome into 3,000 pieces and maps long-range interactions 12 . That resolution is too poor to show individual genes, let alone predict which binding sites might help to generate a particular conformation, but creating a more detailed picture is difficult. Constructing the interaction map required some 30 million reads of fused DNA molecules; improving resolution by a factor of 10 (to 100-kilobase pieces) would require some 3 billion reads, because the number of reads required increases exponentially as the resolution improves linearly. Even so, Dekker and his colleagues' maps agreed with established ideas about chromosome territories, indicating that gene-rich areas lie close together.  \n               Whole-Genome models \n             This year, researchers led by Dekker and Marc Marti-Renom, a bioinformatician at the Prince Felipe Research Centre in Valencia, Spain, published the results of 3C carbon copy (5C) performed on two different types of cell. They used the data to build a three-dimensional model of a 500-kilobase region of human chromosome 16 (ref.  13 ). This region contains a cluster of housekeeping genes active in most cell types, and another set of genes active in only some cells. Using interaction-frequency maps, the researchers generated chromatin models for both cell types. These predicted the existence of compact chromatin structures in which active genes were clustered. In the cells in which both sets of genes were active, the chromatin in the model folded into two 'globules'. In cells in which only the housekeeping genes were active, only one globule formed. FISH experiments confirmed the overall size and shape of this region of chromatin in individual cells. It is possible to construct genome-wide models at higher resolution, by starting with smaller genomes. Last year, Ken-ichi Noma, who studies gene expression at the Wistar Institute in Philadelphia, Pennsylvania, and his colleagues took this approach, generating a very high-resolution genome-wide model of the fission yeast  Schizosaccharomyces pombe , which has only three chromosomes, containing a total of about 14 million base pairs and 5,000 genes 14 . The researchers calculated how close different pieces of chromatin were to each other by dividing the genome into sections of just 20,000 base pairs, and confirmed several results with microscopy. Earlier that year, a multilaboratory team had built a kilobase-resolution model of the genome of the budding yeast  Saccharomyces cerevisiae , which has 16 chromosomes 15 . The challenge starts with gathering reliable data: picking out real interactions from background reads. \u201cThe hardest step was going from sequence data to a set of interactions we could trust and interpret functionally. We had the data in hand for a year before the paper was published,\u201d says William Noble, a genome biologist at the University of Washington, Seattle, who leads one of four labs that produced the budding yeast model. The structure provides a visual interpretation that the human brain can understand, says Noble, but that interpretation can be taken only so far. \u201cThe structure isn't introduced until the very end because we didn't want to base any of our conclusions on the structure itself,\u201d he says. Other researchers acknowledge that such models could be useful, but worry that they could be misleading. \u201cWhen you say that two points are folded together, what's in between? We don't have the physical parameters to predict what's really happening there,\u201d says Ruan. The distance estimates from high-throughput data represent an \u201cunrealistic average\u201d that does not take into account that chromatin is in constant, often non-directed, motion, says Pombo. \u201cYou make protein structures when you crystallize a protein,\u201d she says. \u201cNuclei are not like that.\u201d Model builders reply that in future, representations will reflect the dynamic, semi-random movements of chromosomes, and that current versions can still be valuable, by showing overall tendencies. \u201cBy imaging you highlight the variability. By chromosome capture you highlight the commonalities,\u201d says Dekker. But Cremer suggests that researchers should spend at least as much time with their microscopes as with their computers. Before people can really understand what high-throughput sequencing data tell us about higher-order chromosome arrangements, he says, the field needs many more descriptive studies. \u201cOne has to be very careful about making generalizations at this moment, and we need a lot more data.\u201d \n                     Genomics tools for unraveling chromosome architecture \n                   \n                     Ten years of genetics and genomics: what have we achieved and where are we heading? \n                   \n                     Networking the nucleus \n                   \n                     Genomic Architecture in Harvard Magazine \n                   Reprints and Permissions"},
{"file_id": "471661a", "url": "https://www.nature.com/articles/471661a", "year": 2011, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "For years, scientists have struggled to reconstruct tissues and organs by combining cells and nanotechnology. These devices are now edging from cool concept to practical application. For more than a decade, researchers have been etching grooves into silicon and plastic wafers, filling the spaces with living cells, and hoping that the resulting devices will mimic biological systems such as the liver or gut. Scientists at the Wyss Institute for Biologically Inspired Engineering at Harvard University in Boston, Massachusetts, have created one of the most sophisticated devices so far: a lung on a chip that represents several types of tissue 1 . \u201cWe started with the simplest embodiment of human airway and capillary cells, and then introduced immune cells,\u201d says Donald Ingber, head of the institute. The chip holds a pair of microchannels separated by a flexible, porous 10-micrometre membrane. One channel contains air and a layer of epithelial cells such as those lining the tiniest air sacs in the lung; the other holds the type of cell that lines capillaries, along with flowing liquid to simulate blood. The set-up even models breathing: vacuum chambers attached to the channels simulate the mechanical forces that cells encounter as a person's chest expands and contracts. The chip showed that the cells' behaviour changes when they are stretched. To model the effects of air pollution on the lungs, Ingber's team placed toxic nanoparticles on the surface of the air-sac cells. More particles moved across the membrane from the air channel to the blood channel when the vacuum-controlled 'breathing' apparatus was operating than when the 'lung' was at rest, indicating that toxicity tests on static cells underestimate the detrimental effects of airborne particulates. More-complex behaviours could also be monitored: when substances known to provoke an immune response were introduced into the air channel, white blood cells migrated across the membrane, simulating what occurs in actual inflamed lungs. The goal, says Ingber, is not to make replacement organs for transplant, but to replicate enough of a lung's functions to make the chips useful in testing substances for therapeutic and toxic effects. \u201cWe are not making a lung,\u201d he says. \u201cWe are inspired by design principles of what makes a lung relevant physiologically.\u201d Although researchers have many ways to study isolated proteins and cultured cells, experimenting on tissues generally requires whole animals or freshly dissected body parts. Such experiments are costly and often unreliable, and can raise ethical issues. Organs on chips are still very much a work in progress, but advances in culturing cells and manufacturing nanomaterials mean that they could eventually supplement or supplant animal studies. A little-appreciated advantage is that the chips are more consistent than whole mice, says Judith Swain, executive director of the Singapore Institute for Clinical Sciences. \u201cPeople may say it's halfway between  in vitro  models and animal models,\u201d she says, \u201cbut it goes past that. It endeavours to create the smallest functional unit so that you can control things and you're not confounded by variability.\u201d Chips need further validation before they can move from research project to research tool. \u201cWe think this is tremendously exciting, but it has a good way to go before it can substitute for some of these animal tests,\u201d says Jesse Goodman, chief scientist at the US Food and Drug Administration (FDA). Nonetheless, the agency is preparing guidelines on how to replace animal tests with chips or related technologies, including computational and cell-based screening. Even if the FDA does not end up considering experiments on chips to make its decisions, says Goodman, the technology can still make drug discovery more efficient by helping companies to decide which drug candidates to prioritize for animal studies. The chips will be especially useful, he says, if they can be used to study toxicity over several days of repeated exposure, or if they can be seeded with cells from different patient groups to reflect varying responses to drugs. The simulated lung will need to be even more complex than it is at the moment, says Alan Ezekowitz, an immunologist at Merck Research Laboratories in Rahway, New Jersey. \u201cThe lung on a chip is the beginning; it's a very simple prototype,\u201d he says. Modelling how absorption changes as cells stretch is impressive, but Ezekowitz would like a way to model the lungs' muscles too, so that screens can assess what might cause effects such as spasms in the bronchial tubes. And the Wyss Institute's current chip includes only one kind of white blood cell \u2014 neutrophils \u2014 when in fact the lung is monitored by several types, including dendritic cells, lymphocytes and macrophages, all modulating each other's effects. Ingber is adding more types of cell. He foresees the lung on a chip eventually being seeded with cells derived from people with conditions such as asthma, being customized for different assays, or being used to gauge rates of pulmonary scarring or absorption of inhaled drugs. Chips won't replace animal testing, says Ingber, but they could reduce it and provide options for diseases for which no good animal models exist. \n               Tackling the whole animal \n             Given the difficulty of recreating a single organ, representing the entire body on a chip sounds impossible \u2014 but it was actually one of the first biology-on-a-chip projects to be tackled. Michael Shuler, a bioengineer at Cornell University in Ithaca, New York, is credited with coining the phrase 'animal on a chip' in the late 1990s, after he and a colleague, Gregory Baxter, began etching silicon wafers to form tiny compartments that would hold gut, liver and fat cells, all linked by microfluidic channels. The approach, which Shuler calls a \u201cmicroscale cell-culture analogue\u201d, is a physical manifestation of mathematical models used to predict how drugs move through and accumulate in various organs 2 . Frank Sonntag, a biosystems technologist at the Fraunhofer Institute for Material and Beam Technology in Dresden, Germany, leads a group that is trying to predict systemic toxicity using what Sonntag calls a chip-based multi-micro-organoid culture system 3 . His chips hold six identical micro-bioreactors, each containing cells chosen to mimic the liver, brain and bone marrow. A third team, led by Kiichi Sato, a bioanalytical chemist at the University of Tokyo, has created a chip 4  to test how cell lines representing breast cancer, liver and intestine interact with drugs. One difficulty with the chips is the complexity of modelling the proportion and sequence of blood flow to each 'organ'. Shuler says that some devices capture blood distribution at least as well as mathematical models, but they do not model other aspects, such as how blood flows within an organ. The greater issue, however, is that current devices rely on cell lines that grow readily in culture, rather than the more-finicky cells that better represent organ function. Chips will become more predictive in the next few years as researchers learn to cultivate \u201cmore authentic\u201d cells, says Shuler. \n               From animals to organs \n             Shuler is now working on reconstructing better models of the organs through which drugs move. The intestines, a barrier that must be passed by all swallowed drugs, seem surprisingly easy to model (see  'Just cells' ): using cell lines representing only the gut epithelium, mucin-secreting cells and lymphocytes, Shuler and his colleagues have been able to recreate the mucoid layer in the gut 5 . With the help of an absorbent polymer gel that can be used to build microscale scaffolding, the team has even crafted a collagen structure to represent the villi that line the intestinal wall 6 . Meanwhile, the Wyss team is developing a model of the gut that mimics peristalsis using vacuum chambers similar to those in the lung chip. This model allows researchers to observe molecules passing from the gut chamber into the blood chamber, says Ingber. Several companies are developing chips that can be used as miniature testing systems. Myomics in Providence, Rhode Island, for example, grows models of skeletal muscle in multi-well plates. It is collaborating with pharmaceutical partners to screen drugs that might harm muscles, as well as one that could be used to treat muscle disorders. It can be difficult to create systems that are robust enough to be shipped and simple enough for most scientists to use, says Robert Freedman, chief executive of Hurel in New Brunswick, New Jersey. The company was co-founded by Baxter in 2005 and is developing chips to investigate liver toxicity and skin allergies. Part of the product-development process, says Freedman, was switching from opaque silicon chips to transparent plastic ones, to enable microscopy studies. Company researchers also had to put chips packed with living cells on an aeroplane to make sure that they could withstand pressure changes during shipping. The company's most important task is picking systems that scientists want to buy. For example, a European Union directive to phase out animal testing for cosmetics from 2009 has created a market for  in vitro  evaluation of skin irritants, so Hurel is working with the world's largest cosmetics company, L'Or\u00e9al in Paris, to develop a replacement for a test in which a potential allergen is rubbed behind a mouse's ear. The 'allergy test on a chip' holds skin and immune cells. \u201cOnce you work out all the kinks, it will be better than the animal test because you'll use all-human materials,\u201d says Martin Yarmush, chief scientific adviser at Hurel. \n               Learning about the liver \n             Liver toxicity is among the most common biological reasons for drug candidates to be pulled from clinical development, so it is important to be able to predict it. Even if a molecule does not harm the liver, that organ's detoxifying actions may harm the molecule, rendering potential drugs ineffective. Compounds can be tested in 'primary' cultures of liver cells, which have been gathered from cadavers, but these are in short supply. Moreover, the cells behave differently and die quickly when grown flat in a dish. Consequently, several companies and academic labs are developing liver platforms with an eye to drug screening. Hurel plans to launch its liver-cell chips later this year. In 2007, RegeneMed in San Diego, California, began selling three-dimensional liver co-culture plates and screening services as an outgrowth of previous efforts to develop artificial organs for transplantation. Each of the 96 wells in a co-culture plate is set up with what Dawn Applegate, RegeneMed's chief executive, calls a \u201cjungle gym\u201d: nylon scaffolding with openings the right size for cells to pass through. The cells grow over the scaffolding to simulate tissue. \u201cCells need a third plane to express the extracellular-matrix proteins and growth factors that they would express in the body,\u201d says Applegate. Reconstituted tissue can live for up to six months, and the technology supports liver cells from several species, so it can help to resolve conflicting results obtained in different animal models. The plates contain not only hepatocytes, the most common type of cell in the liver, but all the other types as well, says Applegate. Although hepatocytes carry out most drug metabolism, chips must model interaction between different cell types to provide an idea of full liver function. In March, Hepregen of Medford, Massachusetts, launched HepatoPac: a liver platform based on microfabrication technology developed by Sangeeta Bhatia, a bioengineer at the Massachusetts Institute of Technology (MIT) in Cambridge. A substrate is dotted with collagen, which keeps different types of liver cell in their places and holds colonies of hepatocytes surrounded by supportive cells; the cells can remain functional for 4\u20136 weeks, says Bhatia. The platform is being developed through a partnership with companies including Boehringer Ingelheim Pharmaceuticals in Ridgefield, Connecticut. At a toxicology meeting this month, scientists from Hepregen and Alnylam Pharmaceuticals in Cambridge, Massachusetts, presented results showing that HepatoPac predicted liver damage from repeated doses of fialuridine, a potential treatment for hepatitis B that failed clinical trials in the early 1990s because it was found to cause severe toxicity in humans \u2014 an effect that had not been predicted in animal studies. In January, CellASIC in Hayward, California, began selling a 96-well sample plate riddled with channels that provide oxygen and a continuous flow of media to hepatocytes in the wells, simulating how blood delivers drugs and toxins to the liver. The cells are assembled in 60-micrometre tubes imprinted with an artificial structure that mimics the effects of cell\u2013cell interactions, and the hepatocytes retain a suite of liver-specific activities for more than four weeks, says Philip Lee, who co-founded CellASIC with Paul Hung in 2005. The two had developed the technology while working in the laboratory of Luke Lee, a bioengineer at the University of California, Berkeley. The microfluidics technology in the plates relies on gravity rather than a pump system to pull media and test compounds from an inlet well, past the cells and into an outlet well, where the liquid can be collected and analysed for metabolites and other cell products. The goal, says Lee, was to create a robust product that can run on an automated system, minimizing operator-to-operator variability. Researchers can study cells directly by imaging, or collect them and break them up to study gene expression or the induction and inhibition of drug-metabolizing enzymes. Other systems are still in academic laboratories. Linda Griffith, a bioengineer at MIT, has built silicon scaffolds less than 2 centimetres across and filled them with wells that allow liver cells to grow in three dimensions 7 . These structures are placed inside multiwell plates. Micropumps maintain oxygen and nutrient gradients \u2014 similar to those found in the body \u2014 between the wells in the silicon scaffolds. Currently, Griffith is comparing how three-dimensional liver tissue containing several cell types compares with flat hepatocyte cultures in predicting drug toxicity. The goal is to get the most information possible from the simplest culture possible, she says. \u201cYou may be able to use the simple cultures as an early screen. More complex cultures are needed for more complex questions.\u201d \n               Putting it together \n             Creating more complex cultures is getting easier, says Shuichi Takayama, a bioengineer at the University of Michigan, Ann Arbor, who has constructed chips that represent bone, liver and lung. The cell types needed for such devices are becoming more accessible, as are the growth factors and extracellular-matrix proteins needed to keep the cells healthy. But for tissues more than 3 millimetres thick, the chips also need to provide a circulatory system, and many will need to supply some sort of mechanical perturbation: tension on skin and muscles, flow in blood vessels, compression on bone and so on. \u201cAnything that requires dynamic control rather than just static control is a challenge,\u201d says Takayama. And of course, each organ represents its own set of challenges: to simulate beating heart tissue, for example, muscle fibres must be aligned on a chip that does not interfere with the mechanical and electrical activity of cells (see  'The real McCoy' ). There are other challenges associated with the logistics of the chips, says Shuler: for example, the effects of polymers and microfluidics on cell behaviour are still poorly understood. The very small sample volumes involved make collecting and analysing drug metabolites difficult, and some materials used to build the devices may actually absorb drugs. Not surprisingly, many chips require considerable expertise to operate and troubleshoot, limiting the ease with which they can be adopted by inexperienced labs. Still, progress is real, says Ali Khademhosseini, a bioengineer at MIT, who is developing ways to create artificial circulatory systems that can keep engineered tissue alive. \u201cThe perception of chips being just cute little things is changing, and there is now more of the view that they can make a significant impact,\u201d he says. Reprints and Permissions"},
{"file_id": "475123a", "url": "https://www.nature.com/articles/475123a", "year": 2011, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "Monitoring technologies and genetic engineering are producing a growing array of animal models for psychiatric disorders, but researchers are still learning how best to use them. Anyone familiar with Rett syndrome will recognize the symptoms. Mouse models \u2014 animals that carry genetic mutations similar to those that cause the condition in humans \u2014 wring their paws, walk awkwardly and learn poorly. Other human brain disorders have animal models, too. Mice with extra copies of the genome regions duplicated in Down's syndrome show motor problems and learning deficits. A developmental disorder known as fragile X syndrome arises when humans lack a working copy of the gene  FMR1 ; mice without the gene show learning deficits and hyperactivity similar to the symptoms of the human disorder.  But those are conditions with discrete, recognized causes. Other neurocognitive disorders, such as autism, depression and schizophrenia, have multiple and often mysterious causes, so mimicking them is more complicated. Studies have implicated dozens of genetic variants in producing the disorders, and environmental factors from traumatic life experiences to  in utero  conditions also contribute. Even when researchers have decided which genes or factors to study, it is not always clear how to assess animal models: how can a researcher use a mouse to study diseases diagnosed by hallucinations or an inability to understand figurative language? \n               From behaviour to biology \n             Craig Powell, a neuroscientist at the University of Texas Southwestern Medical Center in Dallas, says that the goal of tweaking genes is usually to uncover disease mechanisms. \u201cSo you make a mouse with a mutation that you know causes autism in humans, and you see that it has behaviours that resemble autism,\u201d he says. Powell's next step is to look at slices of the mouse's brain, to see how it differs from normal mice. At that point, behaviour can help researchers to home in on what really matters. \u201cWe might find a hundred things wrong with the brain function, but only one or two cause changes in behaviour, so we want to fix things and see what changes the behaviour,\u201d says Powell. In work that has become a touchstone for the field, Mark Bear, a neuroscientist at the Massachusetts Institute of Technology (MIT) in Cambridge, and his colleagues showed that reducing expression of a particular receptor in mice ameliorated the effects of a mutation that causes fragile X syndrome 1 . Several physiological abnormalities were reversed, and engineered mice had fewer seizures and better memories. In another example, Adrian Bird, a geneticist at the Wellcome Trust Centre for Cell Biology in Edinburgh, UK, and his colleagues reversed symptoms resembling Rett syndrome in mice 2 . They crafted a version of the defective gene that could be restored to normal activity with a supplement to a mouse's diet, but administered the supplement only after mice carrying the gene began to exhibit symptoms. Activating the gene at this point was expected to have little effect, but the mice showed marked improvement, raising hopes that recovery might also be possible in humans. A similar study 3  into spinal muscular atrophy found that restoring a defective gene's function four days after birth essentially eliminated signs of the disease; doing so ten days after birth had little effect. Such studies could help researchers to predict which patients are most likely to benefit in clinical trials.  Research into behaviour can also probe how genetic variants interact with each other and with environmental factors. In one study published this year 4 , wild-type males from five mouse strains were bred with females carrying a mutation that causes symptoms resembling autism and developmental disorders. Tests on the offspring showed that the symptomatic behaviours recurred in only some of the genetic backgrounds. In another study 5 , mice from a strain displaying a range of autism-relevant symptoms were reared by and with mice from a strain known for high sociability. The fostered mice showed no social deficits as adults, but other relevant symptoms, such as repetitive grooming, were not reduced. And when a mutant version of  DISC1 , the first gene to be implicated in schizophrenia, is present in mice whose mother's immune system has been stressed during pregnancy, the offspring exhibit symptoms of affective disorders and autism 6 . Without the environmental stressors, they show symptoms of schizophrenia.  \n               Hidden symptoms \n             Even the cleverest assays cannot capture some important aspects of human disease, such as the paranoid delusions common in schizophrenia. (In fact, because the models will always be imperfect, most behavioural researchers object to phrases such as 'schizophrenic mice'.) Mikhail Pletnikov, a neurobehaviourologist at Johns Hopkins University in Baltimore, Maryland, is one of many scientists hoping to complement behavioural tests with more readily measured biomarkers. People with schizophrenia exhibit a wide range of behavioural symptoms, but their lateral ventricles \u2014 fluid-filled cavities on either side of the brain \u2014 tend to be larger than average, so Pletnikov is using brain scans to measure these structures in mice. It is not always necessary to see a behavioural change to probe a diease's biology, he says. \u201cWith humility, you can use mice or rats or even worms.\u201d Unexpected behaviours have revealed unanticipated biology. Several years ago, Guoping Feng, a neuroscientist now at MIT, was trying to work out the function of various proteins found on either side of the synapses that connect neurons. Knocking out one such protein, SAPAP3, had no apparent effect on brain function: the mice walked and learned normally 7 . They did, however, seem to have something wrong with their skin: open sores appeared on their faces. After tests showed nothing abnormal, video surveillance revealed that the mice groomed themselves excessively, literally rubbing through their fur. Further work showed abnormalities in a region of the brain linked to obsessive\u2013compulsive disorder (OCD). Although SAPAP3 had not previously been implicated in the condition, drugs that eased OCD symptoms in humans reduced the grooming in mice. Studies 8  of proteins that interact with SAPAP3 revealed that they, too, had links to OCD and autism.  As human genetic studies reveal gene variants with increasingly smaller impacts on disease, there is increasing demand for new behavioural tests to assess them. Results of assays are highly variable, and they may not measure the most meaningful symptoms, says Jeffrey Mogil, a neuroscientist at McGill University in Montreal, Canada. \u201cWe've made a lot of advances in making ever-fancier mice, but at the end of the day the question is, what's your assay and what's your measure and are they relevant?\u201d he says. \u201cThe slow link in the chain, the messy link in the chain, has always been the behavioural assays.\u201d Current tests of animal behaviour are blunt tools. The Morris water-navigation task evaluates an animal's cognitive ability by assessing how it learns to use spatial cues to swim to an underwater platform that it can't see. Changes in the animal's performance can be used to measure learning and memory. Tests for anxiety include the open-field test, which measures the time a mouse spends in enclosed spaces or along the edges of its cage; nervous animals avoid exposed areas. For both, psychiatric drugs effective in humans change the outcomes. Such tests can be effective at screening new drugs that act by the same mechanisms as existing ones. But they are less useful for conditions for which no effective drugs exist, or for gaining insight into pathology. So researchers are trying to develop tests that capture more-specific components of human disorders. \u201cWe talk more to the clinical researchers,\u201d says Jacqueline Crawley, chief of behavioural neuroscience at the US National Institute of Mental Health in Bethesda, Maryland. \u201cThere are opportunities for us to sit down and say, 'what do these diseases look like, what is their variability in the real world, and what do you consider the fundamental core symptoms?'\u201d  Crawley's own studies of children with autism inspired her to develop a mouse test for analogous behaviour. \u201cYou'll see a group of children without autism playing together and the ones with autism being off to the side, playing with a train or a computer,\u201d she recalls. So Crawley designed a task that would assess whether a mouse chose to spend time with a social partner or an inanimate object. The assay is now used in many laboratories. Clinical tests have inspired other animal counterparts. Mogil and his colleagues produced the mouse-grimace scale for pain assessment 9 , based on a scale that used facial expressions to determine pain in infants and other humans incapable of speaking. Mogil believes his scale will prove a more reliable measure of chronic pain than commonly used assays such as the tail-flick test, which measures how quickly a mouse moves its tail out of a beam of light. It should also help researchers to design more-humane experiments. Other scientists have developed a mouse version 10  of the Wisconsin card-sorting test, in which participants are presented with cards displaying, say, three red circles or four blue squares. Once humans recognize that rewards come for, say, matching cards by colour, the reward criteria are changed to matching by shape or number. The test is used to study disorders including autism and schizophrenia. The mouse version relies on scents such as cinnamon and garlic alongside textures such as gravel and cotton balls. Feng is currently evaluating the assay on mouse models of autism. Tim Bussey and Lisa Saksida, neuroscientists at the University of Cambridge, UK, have developed a mouse version of a touchscreen interface originally developed for humans and primates. It uses high-contrast images, tailored to mouse eyesight; and instead of pressing a lever or poking its nose into a hole in the wall as in most mouse-testing systems, the animal touches a screen with its nose or a paw. The technology, commercialized by Campden Instruments in Loughborough, UK, last year, could assess many cognitive abilities in rodents; one battery of tests assesses functions typically impaired in patients with schizophrenia, including visual perception, working memory and pattern-learning 11 . Bussey estimates that the system is now being used in more than 30 labs. \n               Ironing out the kinks \n             Confounding variables are the bane of behavioural testing, says Douglas Wahlsten, a neuroscientist at the University of North Carolina at Greensboro and the author of a handbook on the topic. For example, if a mouse seizes up with fear and stands still in the centre of an open chamber, the time it spends there could be misinterpreted as demonstrating reduced anxiety. And the walls of water-maze tanks are often so high above the water that mice cannot see much of the room, which makes it hard for them to use spatial cues to find the platform. Genetic manipulation increases the scope for artefacts in the data, because tinkering with genes could alter how mice perform at tasks for reasons that have little to do with the parameters being tested (see  'Assessing the assays' ). If learning assessments are based on an animals' ability to associate a sound with a mild electric shock, for example, researchers should make sure that the animals have normal hearing and sensitivity to pain. The biggest confounding variable may simply be moving the mouse from its cage to the area where tests are performed. \u201cIt's really rare that a small rodent would be lifted up by another animal and survive,\u201d says Laurence Tecott, a neurobiologist at the University of California, San Francisco. \u201cWe scare the hell out of the animal, then ask it if it's anxious and how it can learn,\u201d he says. \u201cWe do that routinely.\u201d To reduce distress, researchers are working on ways not just to observe behaviour, but to do so without physically transporting animals first. In IntelliCage, an observation system from NewBehavior in Zurich, Switzerland, each animal is radiochipped. The system monitors when each animal drinks and eats, and how it performs at various stations in its enclosure. Tecott, working with colleagues Evan Goulding and Katrin Schenk, has developed an inexpensive system that can monitor animals around the clock 12 . A cage sits on a weight-detecting platform that measures an animal's location 50 times a second. Self-correcting informatics organize the animal's movements into 'bouts' of activity and can keep track of a mouse as it eats, defecates and moves its bedding. As part of the Mouse Phenome Project, an international collaboration to collect phenotypic data on mouse strains used in labs, Tecott is developing a lifestyle database for 16 strains. Even in preliminary results, strains can be distinguished by their distinct patterns of activity.  Tecott has also used his monitoring system on two lines of mice genetically engineered for obesity:  ob / ob  mice, which lack the gene to make one of the hormones that regulates appetite; and  htr2c  mutant mice, which lack a receptor for the neurotransmitter serotonin 12 . The mice act like couch potatoes and midnight snackers respectively, says Tecott. The  ob / ob  animals eat just slightly more than mice of normal weight, but spend only about one-fifth as much time walking around their cages. The  htr2c  mutants have normal activity and feeding most of the time, but leave their burrows in the middle of their resting periods for a series of snacks. Without automated analysis, such insights into the behavioural components of obesity would be hard to detect.  More-expensive video-tracking systems, already widely used for many behavioural tests, can also be used to monitor animals in their home cages, automatically detecting and categorizing behaviours. As more molecular biologists want to monitor genetically engineered mice, demand for and applications of automated systems are increasing, says Lucas Noldus, chief executive of Noldus Information Technology in Wageningen, the Netherlands. Noldus describes one of his company's latest systems as \u201can instrumented home cage that can be configured in a variety of ways, from a very bare cage to very rich stimuli. Depending on the cage conformation you can perform all sorts of tests: an anxiety test with a light spot, or a memory test with automated pellet dispenser.\u201d The monitoring component doesn't interfere with the animal, says Vikrant Kobla, vice-president of business development at Clever Sys in Reston, Virginia. \u201cYou're taking the same cage and putting a camera in front of it.\u201d His company's systems recognize more than two dozen behaviours, including head bobbing, grooming and standing on hind legs, and the repertoire is expanding. \u201cWe have so many modules we've developed that we can adapt it to deal with new behaviours,\u201d says Kobla. \n               Constant vigilance \n             \u201cIf you want a rich detail of measure from many kinds of tests, you really need to use video tracking,\u201d says Wahlsten. Nonetheless, he says, accuracy cannot be taken for granted, even for standard tests of animal behaviour. Tracking two animals at once is particularly difficult. It is not uncommon for software to confuse an animal's nose with its tail, for example. Bad lighting wrecks many experiments, particularly if a mouse is moving from a lighter area to a darker one, and different artefacts occur with white, brown, black and patchy mice. Infrared backlighting vastly reduces these problems and is commercially available, but is rarely used, owing to both lack of awareness and the cost of converting existing equipment. But even the best systems require considerable effort to avoid false readings, warns Powell. \u201cIt's hard to sum up even in a book what the pitfalls are,\u201d he says. \u201cDon't just take the Excel spreadsheet at the end and analyse it. Watch what's going on during the experiment.\u201d  Even when behaviours can be observed accurately, a mouse's activity may not be robust or subtle enough to reflect the effects of tweaking a gene or the environment. Rats show more complex behaviours; for example, littermates wrestle with each other, a behaviour that is considered social play. The most established behavioural tests were designed for rats, so many researchers are interested in modelling disease using genetically modified rats. Rats lacking genes implicated in schizophrenia, Parkinson's disease and autism are among the very first strains being produced by Sigma-Aldrich in Saint Louis, Missouri, which began offering a suite of ready-made knockout rats earlier this year. Sigma and other companies also take on custom projects to genetically engineer rats. Richard Paylor, an autism researcher at Baylor College of Medicine in Houston, Texas, has just begun testing on knockout rats, investigating behaviours such as how the rats interact and vocalize in social situations, and how they respond to social odours. He hopes to report results by the end of the year. It is too early to say anything definitive, he says, but rats should allow finer behavioural assessments than mice. In particular, it may be possible to use them to quantify the effects of potential treatments for social and communication disorders, something that has proved particularly difficult with mice. Rats' larger size also makes it easier to take electrophysiological recordings, brain images and tissue samples. Paylor predicts that labs working with mice will find it difficult to switch to rats, which are expensive to buy and maintain, needing more space and different equipment. \u201cWe will be sort of a test case for labs that may want to study both mice and rats,\u201d says Shannon Hamilton, a postdoc in Paylor's laboratory. But whether testing rats or mice, says Crawley, researchers must remember that the goal of an animal model is not perfection but utility. \u201cAn animal model may not be 100% translatable, but maybe 80% is good enough to test for possible treatments.\u201d \n                     The knockout rat pack \n                   \n                     Animalgesic effects \n                   \n                     Animal models of neuropsychiatric disorders \n                   \n                     Autism linked to hundreds of spontaneous genetic mutations \n                   Reprints and Permissions"},
{"file_id": "478137a", "url": "https://www.nature.com/articles/478137a", "year": 2011, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "The tiniest structures in cells can be seen only using sophisticated instrumentation and informatics, but what biologists really need are improved fluorescent probes. Scientists love to decorate their favourite biomolecules with fluorescent tags. Attaching light-emitting labels to a protein can reveal when and where in a cell it functions, but usually the details are fuzzy. Optical microscopes use light with wavelengths between 350 and 750 nanometres, and structures smaller than about 200 nm cannot be seen clearly. That is much bigger than the thickness of a cell membrane and is about half as long as the mitochondria that supply cells' energy. At this scale, many cellular secrets are invisible. The protein machinery that allows a virus to invade a cell is blurry, as are the synapses across which neurons communicate. The past few years have seen the rise of a suite of techniques, collectively known as super-resolution microscopy, that can use light to reveal structures much smaller than the theoretical limit. The trick is to control fluorescent labels, or fluorophores, so that not all of them signal at once. Light from each individual fluorophore creates a blur, but as long as blurs don't overlap, they can be resolved into individual points at their centres. This allows the position of the fluorophore to be identified precisely, revealing features as small as 20 nm. \u201cThe super-resolution that we have developed doesn't rely on changing the wave nature of light,\u201d says Stefan Hell, director of nanobiophotonics at the Max Planck Institute for Biophysical Chemistry in G\u00f6ttingen, Germany. \u201cIt relies on turning dyes on and off.\u201d  Although advances in instrumentation and informatics should not be overlooked, many researchers believe that it is better-performing fluorescent labels that will allow super-resolution microscopy to continue to move forward. \u201cThat's an area where the field will see the biggest advances,\u201d says Jan Liphardt, a biophysicist at the University of California, Berkeley. \u201cThat's been limiting all of us.\u201d Electron microscopes can resolve features less than a nanometre long \u2014 even smaller than super-resolution. But electron microscopy requires elaborate preparation of samples: usually, cells must be 'fixed' with preservatives and then embedded in resin or frozen. By contrast, many forms of super-resolution microscopy can be done with live cells. And with fixed cells, labels for optical microscopy can identify proteins more specifically than can those available for electron microscopy.  Most super-resolution techniques fall into two categories. In one, sometimes called illumination-based super-resolution, precise geometric patterns of light shine repeatedly across a sample to control which fluorophores are active. In the other, sometimes called probe-based super-resolution, conditions are tuned so that just a few fluorophores emit light at a time. Whereas illumination-based super-resolution microscopy requires specialized optical equipment, probe-based techniques do not. Experiments using the latter technique are relatively easy to set up (see  'Starting up in super-resolution' ). However, only a few dozen of the hundreds of extant fluorescent proteins and dyes have the requisite properties for probe-based super-resolution microscopy: the ability to change from one 'spectral state' to another when exposed to certain wavelengths of light (see  'Fluorescent proteins for super-resolution microscopy' ). Some remain dark until they are activated; others go from one colour to another. \n               Acronym uproar \n             Three labs independently developed the first probe-based techniques in 2006: fluorescence photoactivation localization microscopy (fPALM) was described 1  by Sam Hess, a physicist at the University of Maine in Orono; photoactivated localization microscopy (PALM) was described 2  by Eric Betzig and Harald Hess, physicists at the Howard Hughes Medical Institute's Janelia Farm Research Campus in Ashburn, Virginia; and stochastic optical reconstruction microscopy (STORM) was described 3  by Xiaowei Zhuang, a physicist at Harvard University in Cambridge, Massachusetts. Perhaps one of the most confusing aspects of these and other probe-based techniques is what to call them. Commonly used terms include fPALM/STORM and variations such as single-molecule localization microscopy (SMLM) and single-molecule active-control microscopy (SMACM). But the underlying concepts behind all the probe-based techniques are the same, says Sam Hess. \u201cYou somehow control the molecules so you only have a few visible at a time, you find their position, you cycle through a whole bunch of molecules, and you make a map of where the molecules were. That's your image.\u201d  These techniques require more control over fluorophores than most scientists are used to, says Michael Davidson, director of the optical microscopy division at the National High Magnetic Laboratory in Tallahassee, Florida. \u201cA lot of people are jumping into this. I get the question of what probes to use probably ten times a week,\u201d he adds. Fluorescent proteins are commonly used for super-resolution microscopy. The genes that code for them, often taken from jellyfish or other sea creatures, are fused with the genes for the proteins being studied, so that when the proteins are produced, they too are joined. Thus, when the fluorophore probe lights up, it allows researchers to locate the studied protein. The most popular protein for probe-based super-resolution microscopy is probably mEos2 (ref.  4 ). When first expressed, it fluoresces green, but a burst of ultraviolet light turns it red. Such 'photoconvertible' fluorophores offer certain advantages over those that start out in a dark, non-fluorescent state: they allow researchers to image the protein before experiments begin, and so more easily pick out healthy cells that are producing high levels of the labelled protein. What is more, newly produced proteins are different colours from those that have already been imaged, so researchers can follow pools of proteins over time and get a sense of their rates of production and destruction (see  'How to build a fluorescent protein' ). \n               Everything is illuminated \n             Often, though, one fluorophore per experiment is not enough. \u201cMost of the outstanding questions that people want nanometric accuracy for are in the relationship of two or more different proteins relative to each other,\u201d explains Jennifer Lippincott-Schwartz, a cell biologist at the National Institutes of Health in Bethesda, Maryland, and part of the team that invented PALM. \u201cThe only way that you can address that is using different markers at the same time.\u201d Gleb Shtengel, a physicist at Janelia Farm, says that getting two labels to work together inside a cell is difficult, partly because the optimal conditions for each are not always the same. Fluorophores always prove trickier to work with than the imaging apparatus. \u201cYou have to add another laser, but that's the simplest part,\u201d says Shtengel. Putting the brighter label on the less-expressed protein can help to make sure that enough data can be collected on each of the proteins of interest to fix their locations definitively; expression levels must also be sufficient and reliable for both proteins. And then there are the spectral considerations. If researchers want to use a second label alongside mEos2, for example, they have to find one unaffected by both red and green wavelengths of light. A protein described 5  this year could be a big help: it converts from orange to far-red, a much desired colour that is distinct from both the natural fluorescence of cells and that of other popular fluorophores. \u201cThe palette is so small right now that any addition is a big step forward, especially if you add a colour in part of the spectrum that's empty,\u201d says Shtengel. But researchers are succeeding in using two-colour super-resolution microscopy. That has allowed them to address questions such as whether cell-surface receptors implicated in cancer are randomly assorted or are co-localized on the plasma membrane. Lippincott-Schwartz has described 6 , 7  a general technique that allows researchers to quantify how proteins cluster together on plasma membranes, and to assess the size, abundance and density of clusters. Even illumination-based techniques are benefiting from new fluorophores. A technique called stimulated emission depletion works by pairing lasers: one excites a spot to fluoresce, and the other shrinks the area of fluorescence by further exciting fluorophores on its periphery into a special dark state. To collect an image, the paired laser beams scan across the sample, repeatedly applying intense beams of light that force fluorophores into the appropriate state but can also damage cells. Hell and his colleagues last month described 8  a fluorescent protein that can enable illumination-based super-resolution miscroscopy in extremely low light levels. Although most fluorescent proteins bleach out, or lose their fluorescence, with repeated imaging, this new protein can be switched on and off more than 1,000 times. The researchers were able to image dendritic spines (signal-receiving outgrowths on neurons) at light levels one million times lower than had previously been documented, and the technique can work with a standard confocal microscope, says Hell.  Lippincott-Schwartz and others are working out ways to make conventional fluorophores amenable to probe-based super-resolution microscopy. Instead of lighting up just a few molecules at once, they activate an entire population and wait for the fluorescent proteins to slowly turn off. The analysis identifies the loss of signal, she explains. \u201cAs they bleach, molecules switch off and leave a hole that can be fit to determine where the molecule was.\u201d Data for localizing dark holes are much noisier than those for localizing bright points, but the technique allows researchers to work with several labels at once. In unpublished work, Lippincott-Schwartz has been able to visualize as many as four fluorophores in a single fixed sample, and she thinks that the technique can also be made to work in live cells. \n               Desirable dyes \n             To many cell biologists, the term fluorescent label is synonymous with fluorescent protein, but there are also small-molecule fluorescent dyes. Dyes tend to be more photostable than fluorescent proteins, so they can emit an order of magnitude more photons, which means that dye molecules can be detected and pinpointed more reliably, explains Markus Sauer, a biophysicist at the University of W\u00fcrzburg in Germany. \u201cThe higher photon yield goes in hand with higher localization precision and thus a higher optical resolution,\u201d he says.  The speed at which dyes turn on and off is also an advantage. In the first demonstration of live-cell, three-dimensional STORM, Zhuang used six different probes: four dyes and two proteins 9 . One of the dyes, Alexa 647, allowed an image to be taken in one second; proteins required substantially longer, at 30 seconds per image. Collecting more images in less time is a practical advantage for all samples, particularly for live cells, says Zhuang. \u201cIf you can't switch the probes fast, you can only image slow processes,\u201d she adds. The problem is that dyes are often less convenient than fluorescent proteins. Whereas researchers can label proteins with fluorescent proteins by introducing genes into cultured cells, dyes have to be attached in a separate step. The most common technique is to combine them with antibodies against a protein of interest. Usually, researchers label 'secondary antibodies', which themselves attach to antibodies against the protein of interest \u2014 a practice that allows the same reagents to be used in multiple experiments. However, because it is the antibody rather than the protein that is visualized, the dyes are somewhat distant from the protein of interest. Antibodies can usually be used only on fixed cells, but they do have advantages. Relevant techniques are in common use, and antibodies work in samples that can't be transfected, such as human biopsies. What is more, the target proteins are produced naturally, rather than from introduced genes, which can have aberrant expression. Last year, Zhuang and her colleagues reported 10  that they had used labelled antibodies with STORM to interrogate the locations of ten different proteins within synapses, distinguishing which occurred on the signal-sending (pre-synaptic) side and signal-receiving (post-synaptic) side \u2014 something that would be impossible in conventional microscopy because the synapse is so small. There are also ways to use dyes without antibodies: 'soluble ligands', or secreted proteins that attach to cell surfaces can be produced, labelled and then added to cell cultures directly. Intracellular proteins can be labelled using a 'hybrid-fusion' approach. Instead of being fused to a fluorescent protein, a protein of interest is joined to a 'protein hook' that can attach to the dye molecules. A variety of tags are in use, and the technique can even work with commercially available chemical-tag kits made for conventional microscopy 11 . But the dye can sometimes attach to biomolecules other than the target, says Robert Campbell, a protein engineer at the University of Alberta in Edmonton. \u201cThat raises up background fluorescence, and that limits the level at which you can see the protein.\u201d Improved analysis will also help scientists to get more from their labels. Researchers led by Sam Hess showed 12  that three fluorophores that all emit in the orange\u2013red wavelengths could be distinguished from each other. Conventional microscopy would not be able to separate, say, a greenish-yellow label that emits two green photons for each red one from an orangish-yellow label that emits one green photon for each red one, but super-resolution microscopy can distinguish such signals because emitted photons are attributed to individual proteins. In such a case, \u201cit's okay to have the emission spectrum overlap because you are imaging individual molecules\u201d, says Hess. His team was able to use three labels with overlapping spectra to simultaneously image two membrane proteins and a cytoskeleton protein, showing how these different components of the cell interact. \n               Better resolution through computation \n             Better analysis and more sophisticated algorithms should also help researchers who are using only one label at a time. To speed imaging, researchers would like to increase the number of fluoropohores that emit light at any given time. But if too many fluorophores emit too close together, their signals overlap and cannot be resolved into individual points. Several groups are working on software that lets scientists image more labels in smaller spaces. For example, researchers at the University of Oxford, UK, adapted 13  an algorithm originally developed to study crowded star systems, and used it in probe-based super-resolution microscopy. They showed that it could detect more fluorophores than could two imaging algorithms commonly used in microscopy.  Aleksandra Radenovic, a biophysicist at the Swiss Federal Institute of Technology in Lausanne, has designed computational approaches to mitigate artefacts caused when 'bleached' proteins, which have supposedly lost their fluorescence permanently, revert to a state in which they can be activated 14 . The effort grew out of another project, exploring dense protein clusters on the cell membrane. After a protein fragment chosen as a negative control displayed unexpectedly high levels of clustering, Radenovic and her co-workers studied the activation times of individual molecules of mEos2. The data showed that signals from similar locations clustered together in time. The sequence of signalling molecules should be random across a sample, so these results indicated that the same protein was signalling more than once and was being misinterpreted as multiple proteins, explains Radenovic. \u201cJust looking at the time domain, you can get rid of those artefacts,\u201d she says. Although probe-based super-resolution microscopy can be done using standard fluorescence microscopes, several manufacturers offer systems built specifically for this purpose, along with software for analysing the data. Such microscopes are designed to optimize the activation of probes. Licensing agreements restrict which acronyms each manufacturer uses in marketing, but a machine that works for one form of probe-based microscopy generally works for other forms as well. Tokyo-based company Nikon has installed its system in dozens of labs; Leica Microsystems of Wetzlar and Zeiss of Oberkochen, both in Germany, have also introduced systems. And small start-up companies, such as Vutara in Salt Lake City, Utah, are getting into the market as well. Applied Precision of Issaquah, Washington (acquired in April by GE Healthcare of Fairfield, Connecticut), plans to roll out its probe-based super-resolution system, Monet, later this year. Most of these companies also make instruments for illumination-based microscopy, which require specialized components. With or without dedicated instruments, researchers are keen to try their hand at super-resolution microscopy. So far, most papers demonstrate proof of principle for microscope methods rather than fundamental new biology uncovered by the techniques, but the balance is shifting, says Davidson. \u201cIt's going to be an explosive field. It's just now raising its head, and it's about to take off like a bat out of hell.\u201d \n                     Microscopic marvels: The glorious resolution \n                   \n                     Super-resolution microscopy: breaking the limits \n                   \n                     Microscopy: Ever-increasing resolution \n                   Reprints and Permissions"}
]