[
{"file_id": "480437a", "url": "https://www.nature.com/articles/480437a", "year": 2011, "authors": [{"name": "Declan Butler"}, {"name": "Ewen Callaway"}, {"name": "Erika Check Hayden"}, {"name": "David Cyranoski"}, {"name": "Eric Hand"}, {"name": "Nicola Nosengo"}, {"name": "Eugenie Samuel Reich"}, {"name": "Jeff Tollefson"}, {"name": "Mohammed Yahia"}], "parsed_as_year": "2006_or_before", "body": "Ten people who mattered this year. \n               Dario Autiero: Relativity challenger \n             The shy experimentalist whose team claims to have found faster-than-light neutrinos is happy for the work to stand or fall . \n               By Nicola Nosengo \n             Dario Autiero can hardly keep track of his e-mails any more: hundreds keep pouring in from the media and his fellow physicists across the globe. \u201cBut the real problem is science amateurs,\u201d says Autiero, who works at the Institute of Nuclear Physics in Lyons, France. \u201cThey send e-mail upon e-mail saying that they had predicted it all.\u201d Autiero has been at the centre of this media storm, scientific scepticism and amateur theorizing since 23 September, when he and his colleagues at the international Oscillation Project with Emulsion-Tracking Apparatus (OPERA) experiment announced results that seemed to remove a cornerstone of modern physics. At a seminar at CERN, the particle-physics laboratory near Geneva, Switzerland, and in a paper posted on the arXiv.org website ( http://lanl.arxiv.org/abs/1109.4897 ), the OPERA team described how neutrinos \u2014 fundamental particles with no electrical charge and very low mass \u2014 seemed to make the 730-kilometre journey from CERN to an underground laboratory at Gran Sasso in Italy some 60 nanoseconds faster than the speed of light would allow. If true, the result will challenge Albert Einstein's theory of special relativity and force theoretical physicists to rewrite their textbooks. Autiero was caught off guard by the frenzy that ensued. \u201cThe media have a completely different timescale,\u201d says Autiero, a quiet, shy man who is clearly uncomfortable in the spotlight. \u201cThey want answers right away, whereas as a scientist I am used to spending years looking for them.\u201d \n               boxed-text \n             The experiment's day job is to study how the three known types of neutrino 'oscillate', or switch between different identities, as they travel between Geneva and Gran Sasso. But Autiero realized that it also offered a chance to measure the particles' speed with unprecedented precision. This would require complicated statistics to match the neutrinos detected in Italy to the proton collisions that generated them in Switzerland, as well as extreme accuracy in measuring the distance between the particles' points of departure and arrival. In March, the team thought it had finally solved these problems \u2014 but that left them facing a much bigger one. \n               boxed-text \n             \u201cWhen I first saw the data I was sure there was some mistake,\u201d Autiero recalls. The team spent the following months reanalysing the statistics, recalibrating the instruments and having them checked by independent institutes. The 60-nanosecond discrepancy refused to go away. \u201cAt that point, we had done all we could do with the data,\u201d Autiero says. \u201cIt was our duty to release them, so that others could look into them.\u201d The experiment's advisory board felt the same. Still, a few OPERA members refused to sign the paper, and some scientists have criticized the team for telling reporters that the non-refereed preprint would be posted on arXiv.org. Astrophysicist Martin Rees of the University of Cambridge, UK, called the OPERA result an \u201cembarrassment\u201d and quoted the late astronomer Carl Sagan's remark that \u201cextraordinary claims require extraordinary evidence\u201d. But Autiero finds the criticism baffling. \u201cIf you think your experiment is done properly,\u201d he says, \u201cyou should treat unexpected results as you would treat expected ones.\u201d Even more puzzling to him was the flood of papers about the experiment posted on arXiv in the following weeks. Some of them \u201cmade valuable critiques and forced us to refine our work\u201d, he says. \u201cBut mostly they pointed at trivial problems we had solved months ago.\u201d A hardline experimentalist who admits to being \u201creligious\u201d when it comes to the scientific method, he felt offended by some of those papers. Autiero thinks that the matter will be settled by the end of 2012. His team is repeating the experiment with a variation in how the neutrinos are generated at CERN, to rule out that process as a possible source of errors. On 17 November, the group released an updated arXiv preprint that included an analysis of 20 neutrinos from the repeated experiment, and simultaneously submitted the paper for publication in the  Journal of High Energy Physics . This new version confirmed the initial result, and won back many of the collaborators who had refused to sign the initial paper. But the true test, Autiero says, will come from two neutrino experiments that are attempting to replicate the results: the Main Injector Neutrino Oscillation Search, in which neutrinos fired from Fermilab near Chicago, Illinois, travel 720 kilometres to a detector in a mine in Minnesota; and T2K, which sends a beam from Tokai on the east coast of Japan to a detector in Kamioka, 300 kilometres to the north. Even if they prove the OPERA result wrong, Autiero says that he would not consider the experiment a failure. \u201cWhatever happens, our most important contribution is not the result \u2014 it's the way we did the measuring,\u201d he insists. \u201cWe have refined a method others can now use, and this is what science is about.\u201d \n               Sara Seager: Planet seeker \n             In a year of exoplanet excitement, one astronomer is already asking what comes next . \n               By Eric Hand \n             Hundreds of years from now, says Sara Seager, \u201cpeople will look back at us, and they won't remember me or you. They'll remember us as the generation of people who first found the Earth-like worlds\u201d outside our Solar System. This year, scientists have come tantalizingly close, says Seager, an astronomer at the Massachusetts Institute of Technology (MIT) in Cambridge. NASA's Kepler space telescope, which monitors thousands of stars for dimming caused by an orbiting planet, has found 28 confirmed exoplanets this year, including one that is slightly bigger than Earth and in its star's habitable zone. More than 2,000 await verification. But Seager, a member of the Kepler science team, wants to do better. Kepler can reveal a planet's size and orbital radius, she explains. But to find out whether such a planet is Earth-like \u2014 with free oxygen or other signs of biological activity in its atmosphere \u2014 astronomers need a spectrum of the parent star's light reflected from or transmitted through the atmosphere. Because the stars in Kepler's field of view are up to 920 parsecs (3,000 light years) away, they are too dim for that. \n               boxed-text \n             Seager wants to search for Earth-like planets no more than 30 parsecs away, close enough that their atmospheres could be studied. Her tool would be a 10 \u00d7 10 \u00d7 30-centimetre space telescope designed to watch a single star for a planetary transit. Such an 'ExoplanetSat' would not be able to analyse spectra by itself. For that, Seager will need an orbiting telescope such as the Terrestrial Planet Finder, an ambitious concept that NASA put on ice in 2006. But a fleet of ExoplanetSats could provide a resurrected planet finder with a map of where to look. Each ExoplanetSat would cost less than US$1 million. Rather than a telescope mirror, it would rely on a modified, $1,300 commercial lens. And dozens could be launched very cheaply, piggybacking on rockets carrying other missions. \n               boxed-text \n             \u201cI'm trying to do new things,\u201d says Seager, who is teaching herself the necessary engineering and is aiming for a 2013 launch. Her group has received roughly $3 million in funding from MIT, Draper Laboratory in Cambridge and elsewhere. Geoff Marcy, an astronomer at the University of California, Berkeley, and one of the first exoplanet seekers, lauds Seager's creativity in unfamiliar fields. \u201cThere are thousands of scientists working on exoplanets,\u201d he says. \u201cShe's looking for something different.\u201d \n               Lisa Jackson: Pollution cop \n             The top US environment official has faced relentless attacks on the country's pollution regulations . \n               By Jeff Tollefson \n             Lisa Jackson knew from the start of 2011 that she would face a rough year as head of the US Environmental Protection Agency (EPA). Republican conservatives had just swept the board at the 2010 congressional elections, and they took over the House of Representatives vowing to handcuff the EPA, which they viewed as a prime example of big government run amok. One hostile lawmaker advised Jackson to get her own parking space on Capitol Hill because Congress would be calling her in regularly to defend her agency and the environmental agenda of President Barack Obama. Jackson has testified before Congress 11 times this year. The Republican hostility has its roots in one of Jackson's first major decisions after taking the helm of the EPA in 2009. A chemical engineer turned public servant, Jackson issued a scientific assessment formally declaring that greenhouse gases pose a threat to human health and welfare. That ruling set the stage for the EPA to regulate greenhouse-gas emissions under the Clean Air Act, and Jackson immediately went to work. Looking back, she says the ruling represents a long-awaited triumph of science after years of delay and obfuscation about climate research under the previous administration. \u201cRestoring that science I hope will be among the hallmarks of the Obama EPA,\u201d she says. The ruling has led to a series of decisions that have provoked Republican ire. The Obama administration has worked with automobile makers over the past three years to establish fuel-efficiency and greenhouse-gas-emissions standards, the president's signature environmental achievement thus far. The EPA has also begun rolling out requirements for major industrial facilities and, despite delays, is preparing to issue the first-ever greenhouse-gas standard for US power plants and refineries. \n               boxed-text \n             Jackson's EPA has also targeted smog-forming pollutants as well as mercury and other toxic chemicals from industrial facilities and power plants. In October, the agency announced plans to regulate the waste water generated by shale-gas development, which involves injecting water and chemicals at high pressure into gas-bearing rocks, an activity that many fear could pollute groundwater resources. \n               boxed-text \n             The Republican Congress has fought these efforts, arguing that pollution regulations cost jobs and resources at a time when the country is short of both. As of November, by Jackson's count, Republicans had brought more than 170 attacks on basic environmental protections up for vote this year, although none of their efforts to weaken existing regulations was successful. Even Jackson's critics acknowledge that she holds up well under fire and has succeeded in pushing through some significant environmental regulations.\u201cShe's been very effective,\u201d says Jeff Holmstead, an attorney at Bracewell and Giuliani in Washington DC, who headed the EPA's air-quality programme under former president George W. Bush. Environmentalists, however, were hoping for much more from the Obama administration. Already angry about the slow pace of greenhouse-gas regulations, they were incensed when Obama overruled Jackson and quashed the EPA's proposed standards for ozone pollution. But most realize that an EPA chief has limited power, says Frank O'Donnell, who heads the advocacy group Clean Air Watch in Washington DC. He adds, \u201cJackson herself has done as strong a job as anybody possibly could under the circumstances\u201d. \n               Essam Sharaf: Science revolutionary \n             An engineer was catapulted from Tahrir Square to Egypt's parliament and fought to rebuild science . \n               By Mohammed Yahia \n             As academics joined the millions protesting in Egypt's streets this spring, the voice of one engineer soon began leading chants. Essam Sharaf was in the thick of demonstrations in January, and he became the first prime minister of a post-revolution cabinet in March \u2014 promoting science as a solution to the country's woes. But by November, he had resigned amid a second surge of popular protest. The 59-year-old Sharaf was born in Egypt and earned degrees in engineering from Cairo University and Purdue University in West Lafayette, Indiana. By 2010, he was an academic engineer at Cairo University and a fierce critic of Egyptian President Hosni Mubarak's regime. Sharaf's stance during the uprising made him popular with the young revolutionaries. He was high on their list of candidates to lead the new transition government, along with Nobel laureate Ahmed Zewail, a chemist from the California Institute of Technology in Pasadena. When Sharaf was chosen, hundreds of thousands of revolutionaries gathered to greet him in Tahrir Square. \u201cIf I can't bring the change you want, then I will return to the lines with you,\u201d he told them. \n               boxed-text \n             Once in office, Sharaf said that science could solve many of Egypt's dire developmental problems, ranging from water security to energy. His cabinet began drafting plans to improve the education system and, in June, he approved a long-standing proposal by Zewail to build a university of basic and applied research. \u201cCountries do not move forward except with scientific research,\u201d said Sharaf at the launch of the project, to be called the Zewail City for Science and Technology. \u201cWithout question, he has been a visionary engineer,\u201d says Kumares Sinha, a professor of civil engineering at Purdue University who once supervised Sharaf. \u201cOver the years he has been a strong advocate for science and engineering in the Arab world as a way to advance and reform the society.\u201d \n               boxed-text \n             But that success in advocating for science did not carry over to politics. Critics accused Sharaf's cabinet of weakness, believing that all real power lay in the hands of the military junta. When this discontent erupted into violent protests in November, Sharaf and his cabinet resigned. Elections took place in November for a new parliament, which should be in place early next year, and a presidential election will be held in June. For now, the future of science in Egypt is as uncertain as that of the reinvented country itself. \n               Diederik Stapel: Fallen star \n             A psychologist's spectacular fraud became an example in open investigation . \n               By Ewen Callaway \n             Dutch social psychologist Diederik Stapel had been called one of the 'bright thrusting young stars' of the field before his career imploded this autumn over fraudulent research. In prominent studies that explored prejudices and stereotypes, Stapel didn't just fudge data, he fabricated entire experiments \u2014 seemingly for much of his career, according to a preliminary report issued on 31 October by the three university committees investigating his work. They are still sifting through data from approximately 150 published papers to catalogue Stapel's misdeeds for a final report to be issued next year. Stapel's case is unusual not only for its scale, but also for the speed and transparency of the investigation. Officials at Tilburg University in the Netherlands, where Stapel was dean of the School of Social and Behavioural Sciences, announced his suspension and their plans to investigate in early September, just days after students alerted them to irregularities in his published data. Pim Levelt, director emeritus of the Max Planck Institute for Psycholinguistics in Nijmegen, the Netherlands, headed the Tilburg investigation and says that the committee acted rapidly and openly in the interests of Stapel's former students and collaborators. \u201cThere are co-authors who are losing most of their publications, and that is really a disaster when you are starting a science career,\u201d he says. \n               boxed-text \n             Levelt says that he drew lessons from other investigations, including the high-profile case of Marc Hauser, the former Harvard University psychologist who resigned this year after being found guilty of eight counts of research misconduct. The details of what Hauser did, however, are still unclear. \u201cI was not very pleased by the Hauser case. It was a bit secretive, the final report was very short, and it took them three years,\u201d Levelt says. \n               boxed-text \n             But Daniele Fanelli at the University of Edinburgh, UK, who studies research misconduct, says that the committees investigating Stapel had much more freedom. Stapel quickly confessed and even identified papers that contained fabricated data. (He has since returned his PhD to the University of Amsterdam and reportedly sought mental-health care.) In cases in which accused scientists dispute the charges, Fanelli says, universities have to keep the investigations under wraps. \n               Rosie Redfield: Critical enquirer \n             A blogger's quest to replicate 'arsenic life' led to a remarkable experiment in open science . \n               By Erika Check Hayden \n             She appeared like a shot out of the blogosphere: a wild-haired Canadian microbiologist with a propensity to say what was on her mind. And on 4 December 2010, what was on Rosie Redfield's mind was arsenic \u2014 specifically, a paper published two days earlier in  Science , in which researchers funded by NASA claimed to have found bacteria that could incorporate arsenic into their DNA in place of phosphorus ( F. Wolfe-Simon  et al .  Science   332,  1163\u20131166; 2010 ). If true, the finding showed that life could be supported by a form of biochemistry radically different from the one we know. But Redfield's blog entry on the paper pulled no punches. \u201cBasically, it doesn't present ANY convincing evidence that arsenic has been incorporated into DNA,\u201d she wrote. Redfield kicked off a frenzy of criticism of the 'arsenic-life' paper in the blogosphere and the media. In June,  Science  published eight critiques of the paper. But that month, Redfield took matters into her own hands: she began attempting to replicate the work in her lab at the University of British Columbia in Vancouver, and documenting her progress on her blog ( http://rrresearch.fieldofscience.com ). The result has been a fascinating story of open science unfolding over the year. Redfield's blog has become a virtual lab meeting, in which scientists from around the world help to troubleshoot her attempts to grow and study the GFAJ-1 bacteria \u2014 the strain isolated by Felisa Wolfe-Simon, lead author of the  Science  paper and a microbiologist who worked in the lab of Ronald Oremland at the US Geological Survey in Menlo Park, California. For months, Redfield could not get the GFAJ-1 bacteria to grow reproducibly on a medium containing arsenic. Finally, in November, the bacteria took off. Redfield now plans to check whether they have incorporated arsenic into their DNA, but it will take even more work to show that they can survive without any phosphorus. \n               boxed-text \n             To Redfield, the exercise has shown how social media tools are binding science into a community closer than it has been since the early twentieth century, when it was possible for scientists to personally know everyone in their field. \u201cScientists are much more able to communicate with people we don't know, and to learn from people we've never met,\u201d she says. \n               boxed-text \n             Before arsenic life, Redfield was known in the evolutionary-microbiology community for championing the relatively unpopular idea that bacteria evolved the ability to take up DNA from their environment for the purposes of nutrition, not sex. So although much has been made of Redfield's criticism of Wolfe-Simon \u2014 a blogger on Gizmodo.com, for instance, photoshopped a picture of the two glaring lightning bolts at each other \u2014 Redfield says that her lonely defence of her own unpopular hypothesis has made her sympathetic to Wolfe-Simon. \u201cI'm in that position; it's not that people doubt my experiments; they doubt my interpretation,\u201d she says. Like Redfield, many scientists now believe that the conclusions of Wolfe-Simon and her team were incorrect because the researchers didn't rule out the possibility that their cultures were contaminated with phosphorus. Wolfe-Simon's team has argued that any background phosphorus was insufficient to support the bacterial growth it observed. Ford Doolittle, a biochemist who hired Redfield for her first faculty job and is now at Dalhousie University in Halifax, Canada, says that Redfield's work has proved a point by showing how science is supposed to work. \u201cScience is way too uncritical of itself,\u201d says Doolittle. \u201cWe need more Rosies out there.\u201d \n               Danica May Comacho: Child of the times \n             The '7-billionth baby' was born into a world that is approaching a population plateau . \n               By Declan Butler \n             On 31 October, the United Nations declared, the world's population reached 7 billion. Danica May Camacho, born on that day in the Philippines, was the first of several babies acclaimed for having nudged the population over the threshold. The choices were symbolic: the United Nations can at best only estimate that humanity will pass 7 billion sometime this year or next. But putting a face to the number drew attention to the challenges of absorbing a larger population and prompted a slew of Malthusian doomsayers to lament an overpopulated world. Yet the underlying population story is not that there are now 7 billion people, nor that humanity's numbers will rise to somewhere around 10 billion by 2050. It is the dramatic slowing of population growth. The raw data reveal that the number of annual births, which had been growing for centuries, peaked around 1990 at roughly 135 million, and has declined since then. \u201cThe world reached peak child before peak oil,\u201d says Hans Rosling, an epidemiologist at the Karolinska Institute in Stockholm. That is mostly because family size in the majority of poorer nations has been shrinking for decades, thanks to economic growth, improved family planning and decreased child mortality. Much of the developing world is closing in on the population-replacement fertility rate, about two children per woman. \n               boxed-text \n             Annual population growth has already dropped from 1.8% around 1950 to 1.1% in 2010 and is expected to reach zero around 2060\u201380. This has knock-on effects: in Africa, where the population has been growing fastest of late, the median age is set to increase, bringing most of the population into working age. This demographic dividend is likely to spur economic growth. \n               boxed-text \n             Even if the population problem is abating, the sheer number poses challenges for humanity and the planet. The remedies needed in the next decades are much the same as those needed today: reduce poverty to tackle the root cause of hunger and to accelerate the fall in population growth; develop sustainable agricultural practices that increase food production without gobbling up extra land, water and other resources; develop renewable energy sources and boost energy efficiency to deliver the power that the world will need while avoiding more global warming. We may have defused the population bomb, but 1 billion people remain hungry and the planet's resources are stretched thin. \n               Mike Lamont: The Higgs mechanic \n             The engineer who keeps the Large Hadron Collider running brought physics closer than ever to completing the particle zoo . \n               By Eugenie Samuel Reich \n             It was standing room only in the auditorium at CERN on 13 December, when physicists at the Large Hadron Collider (LHC) presented the best indications yet that the long-sought-after Higgs boson might have been found. But the man who keeps the particles colliding was absent. Mike Lamont heads the LHC operations group, which was holding its own workshop on the machine's function. The group of about 80 engineers staffs the LHC control room in shifts and coordinates hundreds of others working on the superconducting magnets, accelerating chambers and other equipment arrayed around the LHC's 27-kilometre ring near Geneva, Switzerland, all to keep the data flowing without overtaxing the machine. \n               boxed-text \n             This year, the LHC produced 500 trillion proton\u2013proton collisions, 100 times more than in 2010, generating a torrent of data that has allowed scientists to collect suggestive \u2014 but not definitive \u2014 indications of a Higgs boson with a mass of about 125 gigaelectronvolts. Lamont had spent the year carefully edging up the number of collisions by injecting more protons into each of the bunches speeding around the ring, packing the bunches more tightly and tightening the colliding beams to an ever-smaller focus. He also had to solve day-to-day problems, such as radiation damage to beam-line electronics and 'UFOs' \u2014 unidentified falling objects \u2014 that occasionally plummet into the path of the beam and interrupt experiments. \n               boxed-text \n             Lamont, a physicist who joined the LHC preparation team in 2001, was among those who worked frantically to restart the machine after an accident in 2008 shut it down. He insists that getting the collider back online was all down to teamwork. His goal for the next year is to deliver three times more data than in 2011 \u2014 hopefully enough either to confirm that the hint of the Higgs was real, or to rule it out. \u201cThe experiments are like hungry chicks. They always want more,\u201d he says. \n               Tatsuhiko Kodama: Fukushima's gadfly \n             The emotional academic who challenged his government and took nuclear clean-up into his own hands . \n               By David Cyranoski \n             Tatsuhiko Kodama began his testimony calmly. But a few minutes into his speech before the Japanese parliament's health and welfare committee on 27 July, the biologist's tone grew sharp \u2014 and then downright angry \u2014 as he blasted the Japanese government for not accurately reporting the amount of radiation that had leaked from the Fukushima Daiichi nuclear power plant after the earthquake on 11 March. \u201cThis is clear negligence on the part of the government,\u201d he shouted. \u201cWith 70,000 people wandering around, unable to go home, what is the government doing?\u201d The 16-minute rant has since been viewed around one million times on YouTube, and Kodama, who is head of the Radioisotope Center of the University of Tokyo, quickly became known as the 'emotional scientist' spokesman for the victims of the Fukushima disaster. Journalists and local governments sought his advice on how to deal with the ongoing nuclear crisis, and he helped local governments to initiate some evacuation and decontamination efforts \u2014 all while the central government dawdled. \u201cHe's been a real driving force,\u201d says Kaname Tajima, a ruling-party politician in charge of nuclear-disaster measures, who was stationed in Fukushima from June to September. \n               boxed-text \n             Sitting in his office in October, Kodama quietly recounts what led to his impassioned rant. He says that the information breakdown started in the first days after the disaster, when the government decided not to release data from an \u00a511-billion (US$141-million) system created to forecast how radiation spreads after a nuclear accident. The government claimed that because data were sparse, the system might cause unwarranted panic. That suggestion angers Kodama, who says that the Nuclear Safety Commission \u201cshould not be worried about confusion or panic. The specialist committee's major mistake was trying to act as politicians rather than scientists.\u201d Without the simulation results, which turned out to match later reports of a plume of radiation stretching to the northwest of the plant, evacuees from one high-radiation area fled to another. \n               boxed-text \n             Then, the Nuclear Safety Commission and the parliament bickered over whether safety levels should be set at 20 millisieverts or 1 millisievert, delaying decontamination efforts and further confusing citizens. \u201cWhile these committees were arguing, the situation was getting worse and worse. That's another thing that makes me mad,\u201d says Kodama. The academic eventually got the government's ear. The week after his rant, Tajima visited him. The following week, Kodama met then prime minister Naoto Kan, and advised him to get more data from the worst-affected areas. Kodama, who is responsible for the safe operation and maintenance of the University of Tokyo's 27 isotope facilities, was already advising local officials about contamination. In late May, he started working in Minamisoma, a coastal city with around 70,000 inhabitants that straddles the border of the 20-kilometre mandatory evacuation zone. On his counsel, the local government encouraged pregnant women and children, who face an increased risk from radiation exposure, to evacuate from those areas outside the exclusion zone that had elevated radiation. Later, such advisories became common in the wider affected region. Kodama also started emergency decontamination efforts in Minamisoma, teaching town administrators how to measure radiation and look for micro-hot-spots. \u201cYou can't just measure, you have to look for the source,\u201d says Yoshiaki Yokota, a member of the local school board whom Kodama taught. \u201cBefore he showed me, I knew nothing about radiation.\u201d Kodama's frustration continues. He says that the government is still not doing enough to help the victims, and he opposes plans to build a state-of-the-art \u00a5100-billion hospital in Fukushima city, arguing that support should be spread out more widely. He also says that the government is still not releasing enough information. A ban on entry to the exclusion zone has kept scientists from sizing up the true situation in the area and has hampered the work of journalists. Kodama calls it a \u201ccensorship that is quite unusual in democratic countries\u201d. \u201cThe lesson from Chernobyl for the Japanese government is that there is so much psychological scarring, so we can never put too much information out there,\u201d he adds. \n               John Rogers: Tech exec \n             From flexible circuitry to miniature solar cells, this engineer has a knack for turning physics into technology . \n               By Eric Hand \n             The bulky crate in the back of the delivery truck didn't look like much, remembers John Rogers, thinking back to 1996 and his time as a postdoctoral fellow in physical chemistry at Harvard University in Cambridge, Massachusetts. But inside was his first commercial product, the InSite 300. It was designed to shine laser light on thin films and listen for an acoustic echo that would reveal their thickness and elastic properties without touching them. Active Impulse Systems, the start-up company he had co-founded a year earlier, had just sold its first InSite 300 for US$400,000 to a California firm that made semiconductor tools. In that moment, as Rogers posed with his colleagues for a jubilant snapshot in the back of the truck, he knew what he wanted to do with his life. \u201cPhysics for physics' sake is great,\u201d he says. \u201cBut if you can do physics for the sake of technology, I think that might be the way to go.\u201d That attitude goes a long way towards explaining how Rogers came to win this year's Lemelson\u2013MIT award for innovation \u2014 not to mention a MacArthur Foundation 'genius' fellowship in 2009. Now the head of a 40-person lab at the University of Illinois at Urbana\u2013Champaign, Rogers still freely combines techniques from physics, chemistry, materials science and even bioengineering \u2014 and continues to pile up patents and spin-off companies. \u201cThat transition from a scientific idea to an engineering prototype \u2014 he's unbelievably good at it,\u201d says George Whitesides, who mentored Rogers during his postdoc. \n               boxed-text \n             A case in point is Rogers's work on flexible electronics, a crowded field that he joined about five years ago. The goal is to make electronic devices that can be worn rather than held \u2014 woven into clothes, say, or moulded to the body. But whereas many of his materials-science colleagues were working with organic materials, Rogers gave brittle silicon another chance. He found that ultra-thin silicon circuits printed onto an elastic surface could be highly flexible \u2014 and retain the benefits of silicon's low cost and high performance. \n               boxed-text \n             One result of this work is a spin-off company based in Cambridge, Massachusetts, called mc10, that is working with the sporting-goods giant Reebok to roll out a product in 2012 that, Rogers says, will measure an athlete's \u201ckinetic health and well-being\u201d. He wants to get inside the body, as well \u2014 mc10 is in the process of developing membranes studded with electrodes that can wrap around the brain or heart to provide neurologists and cardiologists with vastly improved diagnostic maps ( J. Viventi et al. Nature Neurosci. 14, 1599\u20131605; 2011 ). Another spin-off co-founded by Rogers \u2014 Semprius, based in Durham, North Carolina \u2014 aims to make photovoltaic arrays that produce solar energy for less than 10\u00a2 per kilowatt-hour, which would make the arrays competitive with coal or gas technologies. The company relies on a transfer-printing technique developed by Rogers to peel tiny, high-efficiency solar cells off gallium arsenide wafers and put them onto arrays ( J. Yoon et al. Nature 465, 329\u2013333; 2010 ). For all the razzle-dazzle of his ideas, however, Rogers in person is laid-back, agreeable and modest. It is telling that he is spending his MacArthur and Lemelson\u2013MIT prize money not on himself, but on supporting students whose ideas might be too risky for government grants. \u201cLife is short,\u201d he says. \u201cI want to be able to point to a few things where we were able to have an impact.\u201d \n                     2011 review of the year 2011-Dec-21 \n                   \n                     Detectors home in on Higgs boson 2011-Dec-13 \n                   \n                     Report finds massive fraud at Dutch universities 2011-Nov-01 \n                   \n                     Seven billion and counting 2011-Oct-19 \n                   \n                     Speedy neutrinos challenge physicists 2011-Sep-27 \n                   \n                     A paler shade of green 2011-Sep-14 \n                   \n                     Fukushima impact is still hazy 2011-Sep-07 \n                   \n                     Open research casts doubt on arsenic life 2011-Aug-09 \n                   \n                     Egypt invests in its science 2011-Jun-15 \n                   \n                     Stretchy electronics promise speedier heart surgery 2011-Mar-07 \n                   \n                     Astronomy: Exoplanets on the cheap 2011-Feb-02 \n                   Reprints and Permissions"},
{"file_id": "479464a", "url": "https://www.nature.com/articles/479464a", "year": 2011, "authors": [{"name": "Jo Marchant"}], "parsed_as_year": "2006_or_before", "body": "As the country struggles to refashion its government, archaeologists are looking warily towards the future. In a secluded stretch of desert about 300 kilometres south of Cairo, hundreds of bodies lie buried in the sand. Wrapped in linen and rolled up in stiff mats made of sticks, they are little more than bones. But their ornate plaited hair styles and simple personal possessions help to reveal details about the individuals in each grave. The bodies date from around 3,300 years ago, when the Pharaoh Akhenaten renounced Egypt's traditional polytheistic religion and moved his capital to remote Amarna, to worship just one god: the Sun disc Aten. The cemetery offers a window on a unique episode in Egyptian history, a revolution that some see as the birth of monotheism. Barry Kemp, an archaeologist at the University of Cambridge, UK, and director of the Amarna Project, has been working with his colleagues to excavate the skeletons, and says that they are starting to reveal \u201can alarming picture of a stressful life\u201d. Many Amarnans died young, with retarded growth and signs of multiple injuries. Some young men had marks where their shoulder blades had been pierced, perhaps as part of a brutal ritual. Kemp can't say much more about the skeletons because he had to flee the site in January, putting his team on flights out of the country and walling up his storehouses as a present-day revolution sent the country into chaos (see 'Archaeology in turmoil'). Although the situation soon calmed \u2014 in fact, Amarna did not suffer a single episode of looting \u2014 Kemp has spent months waiting for permission to resume excavations. Other teams working in the country tell a similar story. \u201cWe've lost a year,\u201d says Frank R\u00fchli, a palaeopathologist from the Centre for Evolutionary Medicine at the University of Zurich, Switzerland, who was scheduled to start work in February on human remains at the pyramids of Saqqara, near Cairo, and in the Valley of the Kings near Luxor. The block on excavations has been the latest in a series of obstacles for archaeologists working in Egypt \u2014 the home of perhaps one-third of the world's antiquities, which reveal a vanished culture in unmatched detail (see \u2018New research in an ancient land\u2019). \n               boxed-text \n             Egyptian officials have said that their reluctance to allow work to restart stems from security concerns; they are now starting to grant permits for excavations. But a broader problem is that Egypt's Supreme Council of Antiquities (SCA), which coordinates all conservation and excavation activities in the country, has been mostly paralysed since the departure of its charismatic but controversial leader, Zahi Hawass. An ally of Egypt's deposed president, Hosni Mubarak, Hawass was forced to leave office in July. Since then, the agency has gained and lost three heads in quick succession, with the latest secretary-general, Mustafa Amin, appointed at the start of October. The uncertainty dashed hopes of a swift return to normality for archaeological research, and unrest this week adds new concerns. \u201cEverything is up in the air,\u201d said Kim Duistermaat, director of the Netherlands\u2013Flemish Institute in Cairo, last month. As Egypt struggles to determine its future without Mubarak, archaeologists are wondering what their field might look like without Hawass. \n               Rise of the pharaoh \n             The antiquities service was set up in 1858 to stem a different kind of chaos: the loss of artefacts. Early Egyptologists were little more than treasure hunters, who carted off everything from jewellery to entire monuments. Now, the SCA conducts its own excavations and approves and supervises foreign archaeological missions, as well as conserving and managing the country's wealth of antiquities and archaeological sites. The service was initially led by French scholars, and did not have an Egyptian head until the 1950s. After becoming secretary-general of the SCA in 2002, Hawass catapulted what had been a fairly anonymous position into the limelight. He mixed with celebrities from Diana, Princess of Wales, to US President Barack Obama; fronted big-budget television documentaries; and even starred in his own reality show,  Chasing Mummies . The image of Hawass enthusiastically unearthing treasures in his Indiana Jones-style hat became a familiar sight, and it gave Egyptology its first Egyptian face. Even as he raised his own profile, Hawass did the same for archaeology in Egypt. His efforts attracted tourists and raised millions of dollars from international touring exhibitions of Tutankhamun's treasures. He fought hard \u2014 some felt too hard \u2014 for repatriation of artefacts, and pushed for Egyptian teams to conduct high-profile science (see   Nature 472, 404\u2013406; 2011 ). He raised money for state-of-the-art facilities in Egypt, notably persuading National Geographic in Washington DC to donate a US$3-million scanner to the SCA in return for filming a project to scan Tutankhamun and other royal mummies; US broadcaster the Discovery Channel built two ancient-DNA labs in Cairo and donated $250,000 towards testing the mummies' DNA. Hawass also tackled corruption and supported projects to develop archaeological sites, including building a suite of museums and dealing with rising groundwater that is threatening to damage sites across the country, including Giza's famous pyramids. But critics claim that Hawass had a darker side: that as the years went on, he exerted excessive control and sought mainly to boost his own fame at the expense of other researchers and of high-quality science. Under Hawass, they complain, archaeologists were prevented from announcing their own discoveries. \u201cThis focus on him was something that really bothered people,\u201d says Duistermaat. \u201cEven for foreign missions, you had to wait, even for weeks, until Zahi would come down and 'excavate' it.\u201d Many archaeologists working in Egypt are reluctant to speak about Hawass on the record out of fear that he could regain influence in the country. But in private, several researchers say that Hawass was intolerant of opposition and blocked excavation permits to those who published results or theories that clashed with his own. Megan Rowland of the University of Cambridge, who has just completed a master's of philosophy degree on the political significance of Egypt's antiquities during the revolution, says that researchers who crossed Hawass became targets of intense criticism or had their permits revoked. \u201cEgyptological research is subject to very heavy censorship,\u201d she argues. In media interviews over the years, Hawass has accused several well known archaeologists of smuggling, scientific fraud or other improprieties. One researcher targeted by Hawass was Joann Fletcher, an Egyptologist from the University of York, UK. In a 2003 television documentary she suggested that a particular mummy was Queen Nefertiti, wife of Akhenaten \u2014 a finding that Hawass says he did not vet, and which was at odds with his own ideas. Hawass told the Australian television programme  60 Minutes , \u201cIt is clear [Fletcher] made all this up because she wants to be famous.\u201d Fletcher was temporarily blocked from excavating in Egypt. She challenges Hawass's account and maintains that she did not break any rules. Researchers also face restrictions when they seek to analyse artefacts. Despite Hawass's efforts, Egypt still has only limited capacity for sophisticated testing, such as carbon-14 dating and DNA analysis. But it is illegal to remove any archaeological artefact \u2014 even mud or pollen samples \u2014 from the country for analysis. Although some see this as an understandable response to the history of artefacts being illicitly exported, others complain that it is devastating for archaeological science. \u201cThis is what makes us look like fools at international conferences,\u201d says one researcher, based in Cairo, who does not wish to be named. Just a year ago, it seemed impossible to imagine any change in this situation. The position of SCA secretary-general has traditionally been temporary, held for just two to three years. But Hawass had the support of Mubarak, who extended his appointment. The revolution changed all that. Hawass's hold on power started to slip when he denied, incorrectly, that any objects were missing after Cairo's Egyptian Museum was looted on 28 January. It was further eroded when he underestimated the extent of looting at important sites, despite reports that it was severe, and repeatedly voiced support for Mubarak. When Mubarak fell, Hawass's days were numbered. After resigning and being reappointed in March, Hawass finally left office in July. He has barely appeared in public since, and has been under investigation by the Office of the Attorney General for a range of alleged offenses including stealing artefacts and diverting money from a touring Tutankhamun exhibition to a private charity owned by Mubarak's wife, Suzanne. \u201cIt's laughable,\u201d says Salima Ikram, an Egyptologist at the American University in Cairo, who has worked in Egypt for 18 years. \u201cZahi would never steal antiquities.\u201d Today, Egypt's most famous archaeologist can be found tucked away on the ninth floor of a faded apartment block in the Mohandessin district of Cairo. Forbidden to leave Egypt while the investigation is ongoing, Hawass spends his days writing books in this modest office, surrounded by trophies, medals and photos of himself with celebrities. When  Nature  visits, he is charming and full of energy, bouncing up from his desk every few minutes to locate objects that will illustrate a point: his sweat-stained hat; his handwritten manuscripts; and a tall pile of stuffed envelopes that he says will prove his innocence in the attorney-general's investigation. Hawass denies having close ties to Mubarak and calls the charges against him \u201cridiculous and untrue\u201d. Almost all of them have been dismissed, and the rest will soon be resolved, he says. Regarding his leadership style and appearances on television, Hawass says that it was important for him to maintain a high profile \u201cto Egyptianize Egyptian antiquities\u201d. He denies taking credit for others' discoveries, arguing that he was required to scrutinize all results before they were announced to the media, to prevent unscrupulous archaeologists from making false claims. \u201cMany people announce wrong information to get money,\u201d he says. He acknowledges that people have been banned from working in Egypt, but says that such decisions were made by a 60-person committee of the SCA and the sanctions were imposed only when researchers did not have proper credentials or broke SCA rules, such as announcing findings without approval. Rather than harming Egyptian science, Hawass says that he raised standards, cleaned up corruption and trained a new generation of researchers. Hawass sees his work \u2014 and his ability to extract money from foreign television companies \u2014 as a high-profile success for Egyptian Egyptology. \u201cI'm very proud of the results,\u201d he says, describing the paper reporting DNA analysis of Tutankhamun ( Z. Hawass  et al .  J. Am. Med. Assoc.   303,  638\u2013647; 2010 ) as \u201can incredible article\u201d. High-profile projects like that, he says, help to \u201craise the global interest in Egyptology\u201d. But foreign researchers have criticized the studies, complaining that raw data were not shared, making it impossible for them to assess the quality of \u2014 let alone repeat \u2014 the work. Some complain that the research was carried out purely for television audiences, whereas less glamorous projects might have had greater scientific value. \n               Back to business \n             Love him or hate him, Hawass's departure has unnerved Egyptologists. Asked what they're hoping for from his successor, many researchers say that they want more open discussion of ideas, more sharing of data and collaborations between Egyptian and foreign teams. But first, the SCA needs to get back on its feet. Researchers had hoped to resume work as soon as the security situation calmed. But the agency has been dogged by protests since the revolution, and Hawass's departure left it in chaos. None of his successors at the SCA has yet managed to last more than two months, and researchers say that progress has stalled. \u201cThis is the first time in the course of five administrators I've lived through as an adult Egyptologist that it can't function,\u201d Ikram said last month. As researchers waited through the summer, permits were left unsigned and decisions unmade. When  Nature  visited in October, the agency's headquarters in Zamalek, Cairo, was a hive of inactivity, with dozens of men milling around its halls and the waiting room filled with bored employees watching the clock until it was time to go home. \u201cWe've been sitting here for six months,\u201d said one, clearly frustrated. Everyone now hopes that Amin, the SCA's latest secretary-general, can get things started again. He holds a PhD in Islamic antiquities, and was previously head of the SCA's Islamic and Coptic department. Researchers say it is too early to comment on his leadership style, but because he does not specialize in Egyptology, it seems unlikely that he will share Hawass's one-man approach \u2014 or front documentaries about the pharaohs. \u201cHe'll need people beside him,\u201d says Atef Abu El-Dahab, the affable head of the SCA's Egyptian antiquities sector. \u201cFirst of all, me.\u201d Amin has some huge problems to address before even thinking about boosting the quality of research. His first priority is the security of Egypt's sites and museums. Some looting is still going on, and the full extent of the losses isn't known, says Tarek El Awady, director of the Egyptian Museum in Cairo. \u201cWe're still waiting for the inventories,\u201d he notes. But the most serious challenge is illegal building, with locals trying to claim archaeological land at several sites. El Awady says the underlying issue is that local people don't appreciate the importance of the country's archaeological heritage. Rowland blames this alienation on the Mubarak regime's \u201chighly politicized approach to heritage management\u201d. She argues that Hawass had absolute power and focused on foreign audiences, which left local people with no sense of ownership of their own antiquities. But El Awady defends his former boss. \u201cHe played an important role in increasing people's knowledge of Egyptian heritage,\u201d he says. Still, he adds, the looting shows the importance of \u201cbuilding bridges between museums, sites and local communities\u201d. \n               Lost millions \n             The second major problem facing Amin is funding. The SCA had a healthy income during Hawass's tenure, but the coffers are now empty, despite the extra millions of dollars that should have come in from the travelling exhibitions. \u201cWe have no money,\u201d confirms El-Dahab. He says that all conservation and excavation projects have been halted, and the agency is now borrowing millions of dollars from banks and the government just to pay salaries. There is no shortage of conspiracy theories as to what might have happened to the cash, but El-Dahab says that it has gone to the many projects that Hawass championed, including the construction of 22 local museums, conservation and restoration work at important sites, and his efforts to deal with rising groundwater. Hawass denies any impropriety and defends his record. \u201cI spent 1 billion Egyptian pounds [US$167 million] a year\u201d in support of Egyptian archaeology, he says proudly. He adds that he had planned to bring in more funds through tourism and travelling exhibitions, and blames the political situation \u2014 which has drastically cut the number of foreign visitors \u2014 for the SCA's financial crisis. To make matters worse, many of the agency's employees have been angrily protesting since the revolution for better pay and conditions, blockading SCA buildings and obstructing tourists. The agency has a huge staff \u2014 a spokesperson refused to even guess how many, but Egyptologists estimate that there are perhaps 40,000 permanent employees and another 15,000 or so on temporary contracts. But the SCA doesn't have the money to pay them, or enough work for them all to do. El Awady says that a large proportion of SCA staff should be let go: \u201cWe don't need all these workers.\u201d However, it seems certain that there will not be large numbers of layoffs. The protesters forced out Amin's three predecessors, and Amin will need to keep employees on his side. He is now negotiating with the government for the funds to provide them all with permanent contracts. Amin also announced in October that he will carry out a comprehensive inventory of all SCA-owned land, selling or leasing any areas declared free of monuments and artefacts in order to raise money. He promises to revive restoration work at the pyramid of Djoser, Egypt's oldest surviving stone building, and other major projects \u2014 if the government gives him the money. Meanwhile, permissions for foreign research are starting to come through. Kemp's group finally returned to the field earlier this month. Ultimately, however, the future of archaeology in Egypt depends not just on Amin, but on the outcome of Egypt's first democratic elections in decades, scheduled to begin on 28 November. Researchers are wondering whether the new political regime will take a nationalistic approach that favours Egyptian researchers, or become more open to foreign researchers and international collaborations. And there is one more move that the new government could make. Egypt is reliant on funds from the millions of tourists who come to see its antiquities each year, and although visitor numbers have picked up slightly since the revolution, they are still low. El-Dahab says that the number of tourists visiting the country in September 2011 was only one-quarter of what would normally be expected. If there was one thing that Hawass was good at, it was bringing in tourists, keen to visit after watching his exploits on television, or marvelling at Tutankhamun's travelling treasures. So it is not inconceivable that a new leader might yet invite the charismatic archaeologist back to the SCA. Hawass has previously denied any interest in returning to his old job, but now seems to be repositioning himself. \u201cI'm sorry to say it, but I'm the only one who can bring the tourists back,\u201d he told  Nature . So would he offer his services, if asked? \u201cI will never come back unless there is a stable government,\u201d he says. If the upcoming elections can deliver that, the man in the hat might yet rise again. \n                 See Editorial \n                 p.445 \n               \n                     Egypt invests in its science Wed Jun 15 00:00:00 EDT 2011 \n                   \n                     Ancient DNA: Curse of the Pharaoh's DNA Wed Apr 27 00:00:00 EDT 2011 \n                   \n                     A unifying cause Wed Mar 23 00:00:00 EDT 2011 \n                   \n                     Egypt's outgoing antiquities chief warns heritage is at risk Mon Mar 07 00:00:00 EST 2011 \n                   \n                     Security focus hinders progress in Arab world Thu Feb 10 00:00:00 EST 2011 \n                   \n                     Zahi Hawass Thu Feb 10 00:00:00 EST 2011 \n                   \n                     Supreme Council of Antiquities Thu Feb 10 00:00:00 EST 2011 \n                   \n                     Netherlands-Flemish Institute in Cairo Thu Feb 10 00:00:00 EST 2011 \n                   \n                     Salima Ikram Thu Feb 10 00:00:00 EST 2011 \n                   \n                     Amarna Project Thu Feb 10 00:00:00 EST 2011 \n                   Reprints and Permissions"},
{"file_id": "479287a", "url": "https://www.nature.com/articles/479287a", "year": 2011, "authors": [{"name": "Roff Smith"}], "parsed_as_year": "2006_or_before", "body": "Did a giant impact 200 million years ago trigger a mass extinction and pave the way for the dinosaurs? It takes a little fiddling, a few missed turns on the old, labyrinthine lanes and a good deal of folding and unfolding of an Ordnance Survey map bought that morning, but eventually Paul Olsen and Dennis Kent manage to locate the unmarked access path that leads through the woods to a desolate stretch of shoreline on Lavernock Point, a wild, cliff-lined promontory south of Cardiff in Wales, UK. Olsen and Kent park their rental car in a muddy lay-by. A slow rain begins to fall and low peals of thunder can be heard in the distance. Grumbling good-naturedly about the British climate, the two geoscientists shrug into their parkas, sling their rucksacks over their shoulders and start down the slick path for another afternoon of getting cold, wet and muddy in the search for clues to mysterious events that wiped out much of life on Earth 200 million years ago and allowed the dinosaurs to take over the world. How the dinosaurs' mighty reign ended has been fairly well established: a catastrophic asteroid impact near Chicxulub on the Yucatan Peninsula in Mexico, just over 65 million years ago, is widely credited with bringing the age of the dinosaurs to a close and ushering in the age of mammals. Olsen and Kent, who both work at Columbia University's Lamont-Doherty Earth Observatory in Palisades, New York, have long speculated that with Chicxulub, history might have been repeating itself: that another asteroid, 135 million years earlier, could have wiped out, or at least had a hand in wiping out, much of the late-Triassic flora and fauna. This would have allowed dinosaurs to spread around the globe during the subsequent Jurassic period (200 million to 145 million years ago), evolve enormous bodies and dominate the planet until the next great impact catastrophe. \n               Sudden death \n             It is certain that something drastic did happen at the end of the Triassic, because in the space of a few thousand years, half of all genera known to have existed at the time suddenly vanish from the fossil record. At sea, 20% of all families abruptly disappear, including an entire class of creatures \u2014 the eel-shaped conodonts. On land, the death toll was even higher. It was one of the greatest mass extinctions in Earth's history and, at this vast distance in time, one of the least understood. \u201cThe only thing anyone can say with any certainty about the Triassic\u2013Jurassic mass extinction is that it happened,\u201d says Olsen. \u201cWhatever it was that caused it, it happened so swiftly that most life forms never had time to adapt and evolve to meet the changes.\u201d The prevailing view among scientists these days is that the extinctions were caused by massive volcanic activity associated with the break-up of the super-continent Pangaea. The series of eruptions created a vast geologic formation called the Central Atlantic magmatic province (CAMP; see 'End of an era'). \u201cWe are talking here about volcanic activity on a scale many thousands of times greater than anything ever witnessed by humans,\u201d says Gregory McHone, an independent Canadian geologist who has spent much of his career investigating the CAMP volcanic event and building a convincing case for its involvement in the Triassic\u2013Jurassic mass extinction. The flood-basalt eruption of the Icelandic volcano Laki in 1783 provides researchers with a scaled-down model of just how bad things might have been in the late Triassic. An outpouring of sulphurous gases from Laki created haze that cooled the planet and caused widespread crop failures and famine. It ultimately contributed to the deaths of an estimated 6 million people, says McHone. But disastrous as Laki's eruption was, it belched up just 15 cubic kilometres of basalt. The CAMP events produced 2 million cubic kilometres or more, in a series of pulses that alternated between cooling the climate with sulphurous haze and warming it with massive emissions of carbon dioxide and methane. The oceans grew acidic and parts became starved of oxygen, while on land a surge in lightning sparked extensive fires 1 . Many of Earth's life forms simply couldn't recover from that succession of body blows, says McHone. It's a plausible theory, as Olsen and Kent both readily concede \u2014 even, perhaps, the most likely one. All the same, the CAMP theory leaves a lot of unanswered questions, not least of which is the suddenness of the extinctions. The late-Triassic eruptions span hundreds of thousands of years, but the die-offs seem swift in the fossil record. And what of the 'fern spike'? Late-Triassic sediments on the US east coast contain huge quantities of fossilized fern spores. Ferns are usually the first plant to appear after a natural disaster, says Kent. \u201cIf you look in the fossil record you see a sudden massive spike in ferns just around the time of the extinctions, but demonstrably before the great basalt flows \u2014 at least in our part of the United States.\u201d In other spots, the extinctions seem to coincide with the oldest basalt layers, within the errors of the dating conducted so far. \u201cThe only way we are ever going to unravel this mystery is to work out a timeline, as precise as we can make it, of all the various events around the world that led up to it,\u201d says Kent. Pursuit of that timeline has taken Olsen and Kent on a global quest, from North Carolina to Nova Scotia, Canada; China to Germany, Italy, Austria and the High Atlas Mountains in Morocco; and now to Wales. \n               Just in time \n             Olsen has been thinking about the possibility of an impact connection to the Triassic\u2013Jurassic extinctions for more than 20 years 2 , 3 , but one of the biggest drawbacks to the asteroid theory is that until recently nobody had found any evidence of such a catastrophe occurring around the time of the extinctions. Then, last year, French and German research teams re-dated a badly eroded structure left by a massive impact near Rochechouart, in western France 4 . Previous work had put the impact at around 214 million years ago, long before the extinction event. But the revised date of 199 million to 203 million years ago overlaps with the extinctions, which have been dated to 201.4 million years ago 5 . The authors of the paper also suggested that the enormous shock waves generated by the 2-kilometre asteroid, as it slammed to Earth at more than 25 kilometres per second, could account for unexplained ripples and disturbances in the late Triassic limestone and shale beds in western Britain \u2014 sedimentary beds that coincide with the mass-extinction event. \u201cWhen I read that,\u201d says Olsen, \u201cI decided it was time to come over here and take a much closer look.\u201d As Olsen scrambles along the base of the Welsh cliffs, his stories bring to life the Triassic world. There were the monkey lizards with their beaky faces, long arms and grasping tails; crocodilian creatures that trotted along like dogs; and shallow tropical seas teeming with sharks, right where this shingle beach lies today. \u201cThen suddenly it all came crashing to an end,\u201d says Olsen. Olsen's fascination with this lost world goes back to 1968, when he was a 14-year-old in Livingston, New Jersey, and heard that dinosaur footprints had been found in the rocks of the nearby Roseland quarry. He and a school friend hopped on their bikes, pedalled over to the quarry and found that there were fossils everywhere. By the time the two friends were in their second year of high school, they had catalogued thousands of fossils and tracks of reptiles from the late Triassic and early Jurassic. They became so involved that when the quarry and its treasures were to be sold off and developed into housing units, the teens mounted a publicity campaign to save it. Soon,  Life  magazine was on the phone and their campaign had captured national attention. Olsen even made a cast of a footprint left by a fearsome three-toed beast, and sent the fibreglass model to then-US president Richard Nixon. The publicity garnered by this sheer chutzpah helped to protect the fossil-rich site, and the fibreglass footprint went on to find a place in Richard Nixon's presidential library. Olsen's trip through western Britain with Kent retains some of that sense of schoolboy enterprise and science on the human scale: the two researchers bicker amiably over which way to go at the crossroads; stop off in the local supermarket to pick up more plastic sandwich bags for their rock specimens; use a self-modified, battery-powered drill to take core samples; and even get scolded by the waspish landlady at their guest house when they clomp in with muddy boots at the end of the day. \n               Explosive force \n             At Lavernock Point, the scientists ignore the rain and thunder as they set about their work. Olsen makes his way to the base of the cliffs and points to a layer of buff-coloured limestone at about waist height. \u201cRight there is where it happened: there's the extinction line,\u201d he says. To a layman, it is innocuous: just another of the alternating bands of limestone and shale that are brightened here and there by clumps of flowering purple valerian. But close examination reveals sand-filled cracks and deposits of grainy, irregularly sized material \u2014 disturbances that might have been the work of a tsunami or an extraordinary earthquake. The disturbed layers here and at other late-Triassic sites in the United Kingdom have a distinctive orientation, all angled as if the source of the unrest was in the vicinity of Rochechouart, says Olsen. It certainly would have been an awful day in this part of the world when the Rochechouart crater formed. The researchers who re-dated it estimate 4  that the impact would have generated an earthquake up to magnitude 11.5 \u2014 100 times more powerful than any quake in recorded history. Gareth Collins, a geoscientist at Imperial College London, says that the quake would probably have been much smaller, although still massive. Collins and researchers from Purdue University in West Lafayette, Indiana, have developed an online calculator to model the effects of impacts. Their algorithm paints a vivid picture of what would have been going on at this spot in Wales after the asteroid hit with the explosive force of more than one million megatonnes of trinitrotoluene (TNT). Even at a distance of more than 600 kilometres, the beach would have endured hurricane-force winds and a hail of debris. That debris would have carried a chemical signature of the impact that might still reside in the layers of sedimentary rocks. Stratigrapher Stephen Hesselbo and geochemist Ken Amor, both at the University of Oxford, UK, have joined Olsen and Kent on their outing, and are taking samples to analyse with a mass spectrometer. They will measure the ratios of chromium isotopes, and look for one that is characteristic of meteorites. Olsen will also send samples from this location to another lab, to be searched for other tell-tale impact markers. If those tests detect an extraterrestrial signal in the late-Triassic sedimentary layers in Wales and elsewhere, it will be the first substantive link between an impact and the Triassic\u2013Jurassic mass-extinction event. But a coincidence in time won't prove that an impact was the cause. \u201cThe two things might occur at approximately the same moment but have nothing whatever to do with each other,\u201d says Olsen. More troublingly, the Rochechouart impact doesn't seem to have been anywhere near big enough to have accounted for the mass extinctions around the globe \u2014 at least, not on its own. The 25-kilometre-wide buried crater may have been 40 to 50 kilometres across originally, but it is just a pockmark in comparison with the 180-kilometre-wide scar at Chicxulub. \u201cBased on our estimates, Rochechouart is quite small in terms of global environmental consequences,\u201d says Collins. \n               Picking up the pieces \n             For now, says Olsen, it is too early to make any definitive statements about what Rochechouart did or didn't do. That will take much more data from late-Triassic sites around the world. This year alone, he has crossed the Atlantic three times to collect samples from the United Kingdom and Morocco. He is back in the Atlas Mountains this week, to examine yet more sites. He and his colleagues are not only looking for signs of an extraterrestrial impact in the sediments, but are also searching for other clues, such as the extinction layer and a chemical signature that could be linked to the CAMP eruptions. All those data, says Olsen, will help the team to sort out the relative timing of events around the world, and to create a fuller picture of what happened and how life responded. He suspects that Rochechouart may have been \u201ca piece of a much bigger puzzle\u201d. Perhaps it was one of a series of asteroids that hit around the same time. Alternatively, a lone French crash might have been the final straw for a world already reeling from volcanic eruptions. Or the impact may have come first, weakening ecosystems enough that when the eruptions started, life took a nosedive. Teasing out the answer will take some time, says Olsen, as he trudges, wet and muddy, back up the trail at the end of another wearying afternoon in the cold Welsh rain. \u201cThere is no easy way to do this,\u201d he says. \u201cBut I believe that eventually we will be able to put the pieces together and know what happened and why.\u201d \n                     Move over Eoraptor Thu Jan 13 00:00:00 EST 2011 \n                   \n                     Continental split boosted big mammals Thu Sep 29 00:00:00 EDT 2005 \n                   \n                     Asteroid let dinosaurs rule Fri May 17 00:00:00 EDT 2002 \n                   \n                     Dinos were super-lucky-o-sauruses Fri May 17 00:00:00 EDT 2002 \n                   \n                     Nature Geoscience   Fri May 17 00:00:00 EDT 2002 \n                   \n                     Paul Olsen Fri May 17 00:00:00 EDT 2002 \n                   \n                     Gregory McHone's business homepage Fri May 17 00:00:00 EDT 2002 \n                   Reprints and Permissions"},
{"file_id": "479460a", "url": "https://www.nature.com/articles/479460a", "year": 2011, "authors": [{"name": "Eric Hand"}], "parsed_as_year": "2006_or_before", "body": "How the reclusive Mike Malin changed the way that scientists view Mars. It is sometimes said that Mike Malin knows Mars better than anyone else on Earth. A more verifiable statement is that Malin has seen more of Mars than anyone on Earth. His company, Malin Space Science Systems (MSSS) in San Diego, California, has designed cameras for every one of NASA's Mars-orbiting missions since Viking in 1975. Later this week, Malin will see the start of his ninth mission to the red planet, when a launch window opens on 25 November for Curiosity, the US$2.5-billion NASA rover that is carrying three of his camera systems. Malin's devices are the eyes of the rover, the most costly and complicated Mars mission in a generation. The pictures taken by his cameras will help engineers to steer the machine; they will also be central to the scientific aim of the project: to determine whether Mars had suitable conditions for life billions of years ago. When the images start streaming back from Curiosity, some nine months after launch, the best shots are likely to end up in Malin's hall of fame, the library inside his company's two-storey office building. A sanctuary in which the reclusive scientist can work alone, the library is strewn with copies of  Aviation Week & Space Technology , which Malin says he has subscribed to since he was 14. In racks along the walls are poster-sized images of Mars taken from orbit. They display the wildly variegated terrain that has challenged scientists' understanding of the planet: scorched plains, dead volcanoes, mountainous dunes, chiselled canyons and the massive holes in the ground left by asteroid strikes. Everywhere he looks, Malin sees a land carved by wind, water and time. \u201cI like these big prints,\u201d he says. \u201cYou see much more in them.\u201d At 61, Malin no longer has the physical stamina for the geological field trips that took him to the ends of the Earth to help understand analogous Martian landforms. But he still knows how to aggressively interrogate an image. He gets down on his hands and knees and, with a magnifying glass, puts his eyes inches above a Mars image he has laid flat. \u201cWhat you can do with pictures is pretty limited,\u201d he says. \u201cBut it's mostly limited by your imagination.\u201d Prickly and driven, Malin is a unique force in the world of planetary science. Almost without exception, the instruments that NASA sends to other planets come from big government research centres and leading universities. But Malin's company of 30 people has managed to corner the market both in the cameras that get sent to Mars and the discoveries that they provide. \u201cHere's this little company that's been trusted to do it,\u201d says Phil Christensen at Arizona State University in Tempe. \u201cIt's amazing.\u201d Yet Malin feels that he struggles to gain respect \u2014 and to get his cameras onto missions. And he complains that other researchers have not sufficiently adopted his vision of Mars \u2014 as a planet with layers of sedimentary rock created and sculpted by water and wind. Yet the leaders of the Curiosity mission chose its landing spot, the Gale crater, precisely because it will allow them to study the sedimentary patterns there. Anybody but Malin would see that as a vindication of his work. \n               The whole picture \n             Malin's cameras on Curiosity are sometimes overlooked amid the scientific gadgetry packed onto the 900-kilogram rover. One of those instruments is a laser that can vaporize distant rocks and, in the flash of light, look for tell-tale chemistry. Another will prepare thin samples for X-ray diffraction, a challenge even in a laboratory on Earth. Perhaps the most important instrument is the Sample Analysis at Mars, which will ingest rock samples and tease out their molecular make-up using gas chromatography and mass spectrometry in the hope of spotting an organic molecule \u2014 a sign that Mars once had a habitable environment. The cameras are slightly more prosaic. They are much like modern digital cameras, but built to withstand the rigours of an interplanetary journey and years of dust storms and frigid temperatures on Mars' surface. The largest of Malin's systems are the Mastcams: two cameras on the rover's neck that stand 2 metres off the ground and will survey the terrain. A second system, a microscopic imager fixed to the end of a robotic arm, will have sufficient resolution to see, for the first time, grains of silt. And a third, attached to the underside of the rover's chassis, will monitor Curiosity's descent and landing. All these devices come from a man who once wanted to be a trombonist. Born in 1950 to a shoe-store manager and a secretary, Malin spent a large part of his teenage years obsessively practising the instrument. The extraordinary effort, he says, was triggered by a music teacher who suggested that he didn't have the chops for it. He eventually became good enough to play with the Los Angeles Philharmonic and considered attending a conservatory. \u201cIf you ever want Mike to do something, tell him he can't,\u201d says Christensen. \u201cHe very much has an 'I'll show you' personality.\u201d Nevertheless, Malin ended up choosing space science over the arts. He did his graduate work in the early 1970s at the California Institute of Technology (Caltech) in Pasadena, and was mentored by Bob Sharp, considered by many to be the father of planetary geology. Afterwards, Malin had a stint at NASA's Jet Propulsion Laboratory at Caltech, a place with which he continues to have a love\u2013hate relationship. He admires the lab's planetary missions and its starry-eyed romanticism for the cosmos. But he disliked the bureaucracy, and often butted heads with managers. \u201cI have a personality that chafes at large institutions,\u201d he says. Malin ran up against similar constraints at Arizona State University, which he joined in 1979. He thrived as a researcher, making a name through his geological studies in Antarctica \u2014 an analogue for Mars. But he grew to hate the committees and collaborations that are a part of so many university jobs. Malin is not necessarily antisocial, but he does do his best work alone. Christensen recalls a time when Malin invited him over for dinner, but the only furniture in Malin's bachelor pad was an easy chair and a television. They ate from trays in front of the television, Malin sitting in the chair and Christensen on the floor. \u201cIt never occurred to him to have more than one chair in his living room,\u201d he says. And even though Malin now lives alone in the up-market coastal community of La Jolla, California, he doesn't bother going to the beach. In the mid-1980s, NASA began talking about a flagship Mars mission, the first since Viking, to be called Mars Observer. The many instruments on the orbiter \u2014 radiometers, spectrometers and laser altimeters \u2014 were expected to revolutionize the study of the planet's surface, atmosphere and magnetic fields. Strangely, Malin's call for an optical imaging camera drew little support. Few, he says, saw a need for mapping at a resolution sharper than the average of 50 metres per pixel that the Viking orbiters achieved. \u201cThere was this idea that we had mapped Mars with Viking, and there was no need to map it further,\u201d he says. But based on his fieldwork on Earth, Malin was convinced that greater visual detail would reveal the processes that played a part in shaping the surface. It was only after a last-minute intervention from NASA headquarters that Malin was given a contract to build and operate the Mars Orbiter Camera (MOC). But where was he actually going to build it? The answer came in 1987, when Malin won a MacArthur 'genius' award and used the $250,000 prize as seed money to found MSSS. There he developed a camera design for Mars Orbiter whose innovations would be copied on many subsequent missions. Up to that point, Viking and other planetary missions had used framing cameras, which worked in the conventional way: a shutter opened and allowed light to hit an array of sensors. But Malin designed what is known as a push-broom camera. It had a permanently open aperture that funnelled light to a single line of charge-coupled devices (CCDs), at the time a new technology. The continuous movement of the orbiting spacecraft provided the second dimension. Each time that data were read out from the line of CCDs, the spacecraft had moved forward a bit; an image was thus built up over time, one line at a time. This design eliminated the need for any moving parts, such as shutters, in the camera \u2014 always a liability on a space mission \u2014 and allowed resolutions as good as 1.5 metres per pixel. Before the camera could prove itself, though, the mission came to an abrupt end. Just a few days before Mars Observer was supposed to enter Martian orbit in August 1993, a fuel line ruptured and the craft flew off into space. Malin suddenly had to lay off half the employees in his fledgling business. But he soon earned a second chance. A replica of the MOC became the centrepiece of the 1996 Mars Global Surveyor mission, which entered Mars orbit in 1998. And as he prepared for the deluge of data that would come from the camera, the famously individualistic Malin realized that he would need a scientific sidekick. He found one in Ken Edgett, who was just finishing his PhD under Christensen at Arizona State University. Today, Edgett works in an office near Malin's second-floor library. A large, laid-back man, he slouches in his office chair with his shirt untucked. At the mention of a particular Mars image, he leans into his workstation and types in an MOC number: FHA-1858. \u201cI know this one by heart,\u201d he says. That image changed everything in Mars science, Edgett says. It showed a region, called Candor Chasma, that had been carved into notched canyons and ziggurat-like steps 1  (see 'Focus on Mars') . The terracing had an enormous implication: these were once layers of sediment, probably laid down by water. And to get so many layers, water must have been at work for millions of years. It was the first solid evidence of a water-sculpted Mars. The picture soon ended up on the cover of  Science . It was one of four such covers for Malin and the MOC team, highlighting papers that touched on everything from the possibility of near-surface water flowing in large gullies today 2 , to the ever-changing Swiss cheese of frozen carbon dioxide at the poles that indicates ongoing climate change 3 . The MOC images also revealed the substantial sedimentary layering within a mysterious mound at the centre of the 154-kilometre-wide Gale crater \u2014 evidence that would eventually lead researchers to choose that site as the target for the Curiosity rover. All those discoveries vindicated Malin, and his long push to get something like the MOC to fly. \u201cIt really changed our views of the planet,\u201d says Jim Bell, a geologist at Arizona State University who is the principal investigator for the cameras on Spirit and Opportunity, small rovers that landed on Mars in 2004. \u201cIt set the tone and the goals for the Mars programme that we're still pursuing today.\u201d Not that Malin sought the limelight or found comfort in the recognition. He had to be dragged to press conferences, and cajoled into releasing data and writing up papers. He also demanded an unusual amount of control. With the MOC, Malin and Edgett personally chose the targets for nearly half of the 243,227 images it took. Because the MOC swivelled independently, the two researchers did not have to worry about any other instrument on the Mars Global Surveyor spacecraft. But on Curiosity, Malin's mast cameras will serve as the eyes for the other instruments. He will have to balance pursuing his own scientific goals with doing the reconnaissance needed to guide the sampling by the other instruments. Some are worried that Malin and Edgett will find it hard to defer to the needs of others. \u201cThese are guys who like to do it their way,\u201d says Christensen. \u201cThey're not necessarily team players.\u201d The members of Curiosity's science team will need to work together if they are to crack the mystery at the centre of the Gale crater: a 5-kilometre-tall mountain of stacked sediments that reaches above the crater's rim. At some point, a massive amount of material must have filled the crater and buried it completely. Then erosive forces must have stripped most of the sediment away, leaving only the mound behind. Curiosity is supposed to find out how sediment got into the hole, and how it got out. The working hypothesis is water and wind, acting over immense stretches of time. Researchers judge that the oldest sediments at the bottom were deposited 3.8 billion years ago, when Mars was thought to be warm and wet, and that the rock at the top of the mound represents a vastly different era some 200 million to 300 million years later, when Mars was growing colder and dryer 4 . As it climbs upwards from the bottom of the mound, the rover will provide geologists with an unparallelled opportunity to study Martian history up close. By comparison, the Opportunity rover has traversed just 100 metres of strata in its nearly six years of exploration, according to John Grotzinger, Curiosity's project scientist at Caltech. \u201cThat is the gift of Gale,\u201d he says. \u201cYou know you're going to get a walk through time.\u201d Although no one is willing to admit it, all on the team secretly hope that the rover, lasting a decade or more with its nuclear power source, could somehow make it to the top. A panoramic image taken from that vantage would surely be one for the ages \u2014 and yet it would be a bittersweet achievement for the man whose cameras would capture it. For although almost all the instruments on Curiosity are big steps up in capability from the sensors on Spirit and Opportunity, the Mastcams offer only an incremental improvement. That is not the way Malin had wanted the story to go. \n               Lost focus \n             Mike Ravine enters a storage room at the MSSS offices, flicks on the lights and considers two plastic boxes. He unlocks them and opens their lids \u2014 but does not touch the cameras inside, \u201cThe only time I've ever not delivered something was that,\u201d he says, stabbing his finger at the cameras. \u201cIt's disturbing to have been so close.\u201d The advanced-projects manager at MSSS, Ravine is another employee cut from the Malin cloth: clever, driven and a bit combustible. As well as working for MSSS, Ravine has launched an Internet start-up company, produced a movie and cultivated Hollywood contacts including the director James Cameron, whom he convinced to join the Curiosity camera team. They planned to develop twin 'zoom' cameras, which could switch rapidly between wide-angle panoramas and narrow, high-resolution close-ups. With high-definition and near-video capabilities on the cameras, Cameron would make three-dimensional movies for the public. But with Curiosity running over budget in 2007, NASA demanded that the mission trim $39 million from its instrument packages. The zoom cams had to be scrapped, even though Malin says that they were neither over budget nor behind schedule. Malin's team quickly designed two cheaper, fixed-focus cameras instead, one with a wide field of view, and one a narrow. Ravine had one last chance to rescue the zoom cams when problems with the rover's motors forced NASA to delay the mission's launch from 2009 to 2011. He and Cameron flew to Washington to see NASA administrator Charles Bolden and convinced him to put the zooms back. But by then, time was too short. There were problems with the alignment of the lenses and Malin's team could not fix them quickly enough. In March 2011, with most of the rover assembled and ready to go, NASA ordered the conventional cameras to be put on. Malin is still full of regret. He wonders how it would have gone had his team been able to work on the zoom cams all along. \u201cThey should never have been de-scoped,\u201d he says. In February, just a few weeks before NASA shelved the zoom cameras for good, Malin had a heart attack. He'd already had one, two decades ago, and a stroke six years ago. But neither deterred him from long days and nights of work. This time was different. Malin needed a quadruple bypass. As he recovered from surgery, his absence at the office was palpable. His employees \u2014 the closest thing Malin has to a family \u2014 wondered about their future. Six months later, Malin is back, tethered to an iPhone that reminds him to take his medicine. Doctors have told him to curtail his workaholic hours. But on this September day at the office, Malin is as intense as ever. He enters an empty, cavernous room about the size of a basketball court, and offers a glimpse of his vision for the future of Mars exploration. \n               Mars on Earth \n             \u201cAs a kid I thought I'd be the first person on Mars,\u201d he says. \u201cThat ain't gonna happen.\u201d Instead, he wants to bring Mars to Earth, or at least to use his cameras to give people a feel for the richness of another world. Malin paces off a circle in his empty room, winking at an imaginary point in the centre to mimic the snapshots he would like his Mastcams to take on Mars. In this painstaking way, he would build a three-dimensional data set of a rock or other object of interest to scientists. Then, with the help of a virtual-reality helmet, Malin could recreate the view back in this room in San Diego. Geologists could peer in close, look behind a rock, or even explore it virtually using the rover's arm. And, ever the business man, Malin says that people might pay for the virtual trip to Gale crater. \u201cBasically, you'd walk around \u2014 and you'd be on Mars!\u201d he says. Malin has told some of his colleagues about the idea. Although they've offered murmurs of support, he knows that it will be up to him to make things happen. It will certainly be tougher without the zoom cameras or the full support of an Oscar-winning director. But never count Malin out. It took him years to convince his colleagues of the importance of a sedimentary Mars \u2014 and now they're sending a multibillion-dollar rover to the largest mound of stacked sediment in the Solar System. Standing in the vacant room, Malin surveys the space. But you know that, in his mind, it's already a Martian landscape. \u201cIt's not real to people because they don't see it,\u201d he says. \u201cI'm going to make it real.\u201d \n                 See Editorial \n                 page 446 \n               \n                     NASA picks Mars landing site Wed Jul 27 00:00:00 EDT 2011 \n                   \n                     Gale Crater on target to become next Mars landing site Thu Jun 23 00:00:00 EDT 2011 \n                   \n                     US Mars mission takes pole position Tue Mar 08 00:00:00 EST 2011 \n                   \n                     Mars exploration: Phoenix: a race against time Wed Dec 10 00:00:00 EST 2008 \n                   \n                     Mars rover's debut delayed Thu Dec 04 00:00:00 EST 2008 \n                   \n                     JPL's Mars Science Laboratory Thu Dec 04 00:00:00 EST 2008 \n                   \n                     Malin Space Science Systems Thu Dec 04 00:00:00 EST 2008 \n                   Reprints and Permissions"},
{"file_id": "479284a", "url": "https://www.nature.com/articles/479284a", "year": 2011, "authors": [{"name": "M. Mitchell Waldrop"}], "parsed_as_year": "2006_or_before", "body": "Five years in, has a lofty experiment in interdisciplinary research paid off? Gerald Rubin points to three jumbo coffee urns that stand near a dining area in the Janelia Farm main laboratory building, an elegant ribbon of glass and concrete overlooking the Potomac River valley near Ashburn, Virginia. \u201cWe figured it was worth spending $20,000 a year to provide high-quality coffee for free,\u201d Rubin explains, \u201cbecause that way, people won't be tempted to make it in their labs.\u201d It is true that the coffee is good. But then, everything about this US$300-million facility testifies to the deep pockets of the Howard Hughes Medical Institute (HHMI), the not-for-profit research-funding organization in nearby Chevy Chase, Maryland, that opened the Janelia Farm Research Campus five years ago as its first-ever intramural research facility. More significantly, the campus embodies what Rubin calls \u201can experiment in scientific culture\u201d 1 . Put world-class researchers in an environment that makes it easy to interact across disciplines \u2014 right down to chance encounters around the coffee urns (see 'Cultivating collaboration'). Then put them to work on a handful of grand scientific challenges \u2014 long-term, high-risk, high-payoff research that addresses some of the biggest questions in neuroscience. And use the HHMI's ample chequebook to free them from the distractions of conventional academic life. No administrative work, no teaching duties, no chasing tenure, no writing of grants. \u201cThis really is the ivory tower,\u201d says Rubin, who was named director of the campus in 2003 and has been involved with the facility since its inception. The hypothesis is that this $100-million-a-year experiment will produce uncommonly great science. To Rubin, this means being much more than simply excellent. \u201cI'd consider us a failure if, in 20 years, we come back and say, 'We recreated the Salk Institute',\u201d he says, hastening to add that he considers the Salk, located in La Jolla, California, to be one of the best free-standing research facilities in the world. \u201cThe point is that we didn't need to build Janelia Farm to do that,\u201d he says. A geneticist through and through, he has proposed that Janelia must eventually be able to pass the \u201cdeletion test\u201d: just as knocking out a gene can reveal its function, removing Janelia from the future scientific landscape should reveal the vital importance of its contributions to biology. \n               Early days \n             Five years into Janelia Farm's life, however, Rubin admits that he has no hard evidence that it will ever pass that exceedingly ambitious test. Given the campus's long-term research focus \u2014 and the difficulties of creating a brand-new institute from scratch on former farmland \u2014 Rubin says that he does not expect the facility to start producing its best discoveries for another five or even ten years. So far, the experiment has proved only that the promise of well-financed, unfettered academic freedom can indeed entice high-quality researchers to move to a new facility. Still, the researchers' presence is reflected in the steady increase in publications from Janelia Farm labs (see 'Publications on the rise'), and in the growing respect Janelia is commanding from investigators who were initially sceptical of its lofty ambitions. Getting even this far has been a major accomplishment, says Eric Kandel, a neuroscientist at Columbia University in New York who was one such sceptic. \u201cNo one has hit a home run yet,\u201d he says. \u201cBut the team is in place.\u201d The idea behind Janelia Farm originated in 1999, when Thomas Cech, a biochemist at the University of Colorado in Boulder, had just agreed to become president of the HHMI and was looking to try something new. The HHMI was already funding hundreds of investigators at universities around the world. And with its endowment booming, Cech thought there must be some way for the organization to have a bigger impact on science than just funding a few hundred more. To help him work out how to do this, he recruited Rubin, then a geneticist at the University of California, Berkeley, as HHMI vice-president for biomedical research. Rubin began by looking back at some of the wonderful experiences in his own career: summers as an undergraduate at the Cold Spring Harbor Laboratory in New York; a PhD at the Medical Research Council Laboratory of Molecular Biology (LMB) at the University of Cambridge, UK; and three years at the Carnegie Institution for Science Department of Embryology in Baltimore, Maryland. He wondered: what had made these places feel so great? Rubin posed the question to many people, including veterans of non-biological institutes \u2014 notably Bell Labs in Murray Hill, New Jersey, which had been an innovation powerhouse before the 1984 break-up of its parent company, AT&T. The answers from all these places were surprisingly consistent, he says. Research groups were small, which promoted communication and mentoring. Group leaders were active bench scientists, not administrators or fund-raisers. Research was funded from within, so there was no need to chase grants. And no one got tenure, so that researchers could rotate through and ideas could stay fresh. Many institutes had implemented some of these principles. Bell Labs and the LMB, despite widely divergent remits in applied physics and molecular biology, had implemented all of them in their glory days. But no one was doing so at the time, says Rubin \u2014 especially not the internal-funding part. And that, he argued, was the HHMI's great opportunity. Cech and the HHMI trustees were sold on the idea from the beginning. But others were not. \u201cI didn't see what was so special about recreating the LMB,\u201d says Kandel, recalling his early scepticism. \u201cIt wasn't clear to me what problems would be solved there, or even what fields they would be working in,\u201d he says. Many university investigators funded by the HHMI worried that this new initiative would end up cutting into their money. And most of them doubted that the HHMI would be able to persuade top-quality people to forgo tenure in established research centres and move to a farm outside Ashburn, a dormitory suburb an hour's drive from Washington DC. Also controversial was the initiative's proposed research strategy of focusing on a handful of grand challenges instead of tackling a wide range of biomedical problems. In 2004, the HHMI held a series of five workshops to determine what those grand challenges would be. One topic they settled on fairly quickly was technologies for biological imaging. \u201cIt was a great problem for us,\u201d says Rubin. Not only would it bring together physics, chemistry, biology and many other disciplines, he says, \u201cit was going to be an enabling technology for so many areas, the way [DNA] sequencing had been\u201d. A second major topic was understanding neural circuits and how they give rise to behaviour. This promised to fill a tremendous gap in neuroscience, says Kandel. \u201cIt was clear that we now understood neurons very well,\u201d he explains. \u201cAnd we had imaging techniques to see how large areas of the brain are interconnected. But there was nothing to link the two.\u201d A new generation of techniques was poised to aid in this quest \u2014 most notably optogenetics, in which the activity of specific neurons can be tracked and manipulated using light, allowing researchers to work out the function of those neurons and how they connect. But at the time, the application of optogenetics to neuroscience was still in its infancy. And there was always the chance that Janelia Farm researchers would spend 20 years deciphering the neural circuitry of, say, the fruitfly  Drosophila melanogaster , only to discover that it had nothing to do with the human brain. Most people thought that the underlying principles and logic of the circuits would have been conserved throughout evolution, says Kandel, but still, \u201cit was a very ballsy move\u201d. \n               Fertile soil \n             In October 2006, the HHMI officially opened Janelia Farm's main laboratory facility, dubbed the Landscape building for the way it winds for some 300 metres along the vast, open 'S'-shaped curve of a hillside. It's not exactly a warm and cosy place. The hallways on the upper two floors, where the inner glass walls open onto row upon row of laboratory benches, are so big and full of light that they feel a bit like airport concourses. They also seem to be constantly vanishing around the next bend, which can produce the disconcerting sense that one is stepping off into infinity. But none of that bothered Julie Simpson when she arrived in the summer of 2006, fresh from a postdoctoral appointment in neuroscience at the University of Wisconsin\u2013Madison. She had been eager to get to Janelia Farm ever since hearing Cech give a talk about its philosophy \u2014 so eager, she says, that when she moved in, \u201cI had to wear a hard hat because they didn't even have finished floors in my lab yet!\u201d And she hasn't been disappointed. \u201cI really like being part of a community focused on a particular problem,\u201d she says. \u201cEvery talk is relevant, and every colleague is interested in the same basic thing\u201d \u2014 albeit with different perspectives and lots of productive arguments. Simpson leads a group using optogenetics to trace the neural circuitry that gives rise to specific, hard-wired responses \u2014 grooming behaviour, for example \u2014 in  D. melanogaster . Comparing the detailed structure of many such circuits, she says, should begin to reveal the general principles behind them 2 . Another early recruit was Eric Betzig, a physicist who had begun working at Bell Labs during the late 1980s, when the facility was still investigating everything from neuroscience to antimatter. Impatient with the steadily declining role of basic research at the labs, Betzig left to work at his father's machine-tool company in 1994. Soon after deciding to get back into research, he joined Janelia Farm in 2006 to work on a broad range of imaging technologies. \u201cWhat attracted me was the same thing that had attracted me about Bell,\u201d he says. \u201cAll the resources you need, and no pressure to publish. I won't do science if I can't do it under those terms.\u201d Before arriving at Janelia, Betzig had been developing an imaging technique, called photoactivated localization microscopy, or PALM, that he created in collaboration with Harald Hess, a physicist who is now also at Janelia Farm. It uses image-processing algorithms pioneered by astronomers to detect the position of single fluorescent molecules with nanometre accuracy 3 . Since starting at Janelia, Betzig has developed three more imaging techniques designed to help biologists peer deeper than before into the layers of living cells, with higher resolution and with less damage to them in the process. Janelia Farm is definitely not for everybody, says Rubin. \u201cI've tried to make it irresistible for a small fraction of people,\u201d he says \u2014 researchers who are confident enough to go without tenure, who don't mind the remote location or the six-person limit on group size, and who do want to work with their own hands. Plenty of researchers do seem to fit that description. At present, Janelia Farm has 20 research-group leaders, who are evaluated for renewal on a five-year cycle. The first round of evaluations begins next spring \u2014 a process Simpson calls \u201cterrifying\u201d, if only because it's new and no one knows for sure how it will work. In addition, 26 fellows at various stages of non-renewable five-year stints are working at the facility, as are more than 100 visiting scientists. With additional group members and support staff, the total number of employees at Janelia Farm comes to 424. Yet the place can still feel almost empty. \u201cThat's the first thing you notice,\u201d says Robert Tjian, a biochemist who was once Rubin's colleague at Berkeley and who succeeded Cech as HHMI president in 2009. \u201cWe probably have another 100 to 150 scientists to put in the building before it's even close to being full.\u201d \n               Keeping creative \n             Those spaces are empty in part because it has taken longer to recruit scientists than Rubin initially thought. The most common problem for prospective recruits is the six-member limit for research groups, he says. The remote location has been a smaller hurdle than feared, but it has been a factor. Rubin and his colleagues have done their best to fight that isolation. Janelia's visitor programme brings scientists in for weeks or even months at a time, and the campus hosts about a dozen scientific conferences every year. Nonetheless, the remoteness of the campus is a major ongoing challenge, says Carla Shatz, a neuroscientist who directs the interdisciplinary Bio-X programme at Stanford University in California, and who serves on the Janelia Farm advisory committee. \u201cJanelia has to think about how it can create excitement and creativity and innovation in a location that doesn't have ready access to a medical school, an engineering school, biology, physics and chemistry departments or an undergraduate student body,\u201d she says. A related challenge is that of keeping the research programme intellectually fresh. The tight focus on imaging and neurocircuitry has been useful in the initial phase, says Tjian, \u201cso that everyone understood what Janelia Farm was\u201d. But in the long run, he says, the place will stagnate unless it can broaden out and give scientists the freedom to follow new opportunities as they arise. That is another reason Janelia still has so many empty spaces: they allow for expansion. On the basis of a series of planning workshops held earlier this year, Rubin has begun to hire group leaders in the fields of cell biology, evolutionary biology, structural biology and chemistry \u2014 each of which has some overlap with neurocircuitry but will also extend the campus's programme in new directions. Looking back over Janelia Farm's first five years, Rubin says he is very satisfied. \u201cWe showed that we could go from an empty building to a functioning lab, that we could hire first-rate people, and that they could come here and work on interesting problems,\u201d he says. \u201cAll the things people said we were certain to fail at, we accomplished.\u201d But what happens next? Can Janelia Farm do 'great science' during the next 5 to 10 years? Will it pass Rubin's deletion test? Can it rewrite the introductory biology texts (Cech's favourite definition of great science), or foster \u201ca couple of programmes that create a whole new direction\u201d (Tjian's favourite)? That is the great unanswerable question. As Simpson says, \u201cyou can't engineer great science. You just have to create the conditions that make it possible, and see what happens.\u201d \n                 See Careers \n                 page 433 \n               \n                     Microscopy: Bright light, better labels Wed Oct 05 00:00:00 EDT 2011 \n                   \n                     Cell biology: Seeing cells with sheets of light Wed Mar 16 00:00:00 EDT 2011 \n                   \n                     Neuroscience: Illuminating the brain Wed May 05 00:00:00 EDT 2010 \n                   \n                     The future for Howard Hughes Wed Oct 08 00:00:00 EDT 2008 \n                   \n                     An indifference to boundaries Wed Feb 20 00:00:00 EST 2008 \n                   \n                     Monuments and instruments Wed Nov 28 00:00:00 EST 2007 \n                   \n                     Biomedical philanthropy: State of the donation Wed May 16 00:00:00 EDT 2007 \n                   \n                     Biomedical research gets a fresh twist down on the farm Wed Sep 13 00:00:00 EDT 2006 \n                   \n                     Capital collaboration Washington DC Wed Feb 09 00:00:00 EST 2005 \n                   \n                     Capital collaboration Washington DC Wed Feb 09 00:00:00 EST 2005 \n                   \n                     Lab architecture: Do you want to work here? Thu Aug 14 00:00:00 EDT 2003 \n                   \n                     Janelia Farm Thu Aug 14 00:00:00 EDT 2003 \n                   \n                     Howard Hughes Medical Institute Thu Aug 14 00:00:00 EDT 2003 \n                   Reprints and Permissions"},
{"file_id": "479164ax", "url": "https://www.nature.com/articles/479164ax", "year": 2011, "authors": [{"name": "Sharon Levy"}], "parsed_as_year": "2006_or_before", "body": "Pollinating insects are in crisis. Understanding bees' relationships with introduced species could help. Bees thrum among bright red blossoms on a spring day on Mount Diablo, near San Francisco Bay. Alexandra Harmon-Threatt, a young ecologist just finishing her doctorate at the University of California, Berkeley, lovingly identifies an array of native pollinators. She points out three species of bumblebee, each with a unique pattern of black and yellow stripes. There are bee-flies, members of the fly family covered in soft brown fur, which look and act like bees. Among the native insects are plenty of honeybees ( Apis mellifera ), the species raised by beekeepers worldwide and introduced to the Americas by English settlers in the seventeenth century. All these insects are drawn to a clump of red vetch ( Vicia villosa ), an invasive weed. Just down the road is a patch of native lupins, laden with purple blossoms. But the lupins bloom in silence: no bees attend them. For the past three years, Harmon-Threatt has been studying the ways in which the native yellow-faced bumblebee ( Bombus vosnesenskii ) uses the plants growing in the area. By capturing bees as they visit plants and then sampling the pollen they carry, she has confirmed in unpublished work that they get much of their food from introduced plants. And by analysing the amino-acid content of pollen, Harmon-Threatt has shown that bee foraging behaviour can be driven by a craving for nutrients rather than an evolved attachment to a specific plant. Although many conservationists assume that introduced plants are always destructive, her work shows that it's not necessarily so from a bee's point of view. What matters to most bee species is the abundance and quality of pollen \u2014 and if an introduced plant, such as the red vetch, offers more protein-rich food than the natives around it, the bees will collect its pollen. Harmon-Threatt is one of a growing group of scientists studying the evolving relationships between native bees and introduced plants. Their work is critical in a world where human actions have dramatically shifted the distributions of plants and are forcing a pollinator crisis. Most flowering plants need animal pollinators in order to reproduce, and bees serve that role for many important crops \u2014 including fruits, pulses, some vegetables and alfalfa \u2014 many of which were themselves introduced to the United States. Yet stocks of the domesticated honeybee have been declining in the United States and Europe: the number of managed hives in the United States, for example, has dropped from nearly 6 million in the 1940s to 2.3 million in 2008 (see  'Sting in the tale' ). Habitat loss, pesticide poisoning, viruses and parasitic mites, any or all of which may be behind the mysterious syndrome called colony collapse disorder, have taken their toll on the domesticated bees, leaving farmers increasingly dependent on native bees. But they, too, are suffering from the effects of pesticides, disease and changes in land use. \n               boxed-text \n             What bees need most, the new pollination studies have shown, is a diverse community of flowering plants that bloom throughout the spring and summer. Abundance and diversity matter more than whether species are native or exotic. These findings could inform conservation strategies used by farmers and other land managers. Park managers tend to target invasive weeds such as red vetch with herbicides because they can outcompete native plants. But for bees, \"just taking all the vetch out might not be the best idea\", says Harmon-Threatt. \"It might take ten to fifteen different species of native plants to support this array of pollinators.\" Stories of exquisitely specialized pollination systems \u2014 such as those of yuccas, which are pollinated only by coevolved moth species \u2014 can give the impression that pollination is an exclusive, highly choreographed dance. \"Until the past five or ten years, people thought that exclusive pollination relationships were more common,\" says Rachael Winfree, a pollination biologist at Rutgers University in New Brunswick, New Jersey. By studying entire networks of pollinators and plants, however, biologists have learned that most native bees are far less picky than was imagined. Winfree and her colleagues have investigated the ways in which bees use flowers growing in agricultural, urban and natural areas \u2014 ranging from woodland to farm fields and suburban gardens \u2014 in central California and southern New Jersey. The study, led by Neal Williams at the University of California, Davis, and published earlier this year 1 , found that bees collect pollen from both alien and native plants in proportion to a plant's abundance in the landscape. In highly disturbed habitats, bees make greater use of alien plants \u2014 not because the bees prefer them, but simply because introduced plants are more common where people have transformed the landscape. That makes sense to Winfree. \"I don't see why bees would know or care whether a plant was native or exotic,\" she says. But not all altered landscapes are equal for bees: modern agriculture has taken a severe toll on wild bee numbers. Vast monocultures \u2014 such as the almond orchards of central California and the soybean fields of Argentina \u2014 bloom for only three or four weeks each season, offering no food for bees the rest of the time. \"The expansion of these crops destroys habitat for bees,\" says Marcelo Aizen, a pollination biologist at the National University of Comahue in San Carlos de Bariloche, Argentina. Claire Kremen, a conservation biologist at the University of California, Berkeley (and Harmon-Threatt's mentor), has shown that the diversity of pollinators drops with increasing distance from wild habitat, as does the number of visits by wild bees to flowering crops 2 , 3 . This cuts crop yields. A study by Aizen and his colleagues, published in April this year, documented a drop in the yield per acre of pollinator-dependent crops since 1961, even as total global production has increased 4 . Falling yields have prompted farmers to put more land under cultivation, further eroding bee habitat. Modern agriculture seems locked in a vicious circle of pollinator destruction. Yet Kremen and her colleagues showed in 2004 that crop pollination by native bees increases dramatically when natural habitat exists within 1\u20132.5 kilometres of farm fields 3 . Farms where just 30% of the surrounding landscape is covered in wild vegetation are completely pollinated by native bees, and flourish without help from domesticated honeybees. As most crops in California's Central Valley are far from patches of wild habitat, Kremen and Williams have been experimenting by growing hedgerows of diverse flowering plants in orchards and fields. They now have a list of native California plants, such as redbud ( Cercis occidentalis ) and wild asters, which can be combined to create ideal hedgerows, providing pollen-rich blooms from early spring to late autumn. The results are not yet published, but Kremen says it is already clear that the hedges boost the diversity of native bees, and they are being adopted by farmers. The burning question now, says Kremen, is \"how much hedgerows can contribute to long-term population persistence of individual bee species\".  \n                Weeds will do \n              Winfree finds that bees don't even need pristine hedges \u2014 weeds will do. She studies bee communities in parts of New Jersey and Pennsylvania where native bees pollinate about 90% of the crops. In one study, she and her team watched 6,187 bee visits to watermelon and tomato crops on 23 farms 5 . Both computer modelling and observation suggest that these crops are fully pollinated by wild bees. That's possible, Winfree explains, because the wet climate encourages the growth of weedy plants that spring up at the field edges, and bees use these scraps of habitat to nest and forage. There's another crucial difference from California: in Winfree's study area most farmers plant a variety of crops rather than monocultures. In a study of New Jersey pine\u2013oak forest 6 , Winfree was surprised to find that bee populations are more abundant and diverse near sites of human disturbance \u2014 where backyard gardens or farm fields add to the range of blossoms available. But the picture is likely to vary from one area to the next. In a recent review of the literature, Winfree and her colleagues concluded that land-use changes such as urbanization and deforestation can affect native pollinators differently, depending on whether they increase or reduce the numbers and diversity of flowering plants 7 . There's yet another complication: although some exotic plants can feed native pollinators, such plants can also fuel the growth of alien bee populations. Aizen and his colleagues have analysed webs of plants and pollinators in the southern Andes and on islands in the North Atlantic and the Indian Ocean 8 . They found that, in some cases, exotic plants and pollinators team up to dominate resources, to the detriment of native bees and native plants. \"You cannot generalize and say it is good or bad to have alien plants,\" says Aizen. Problems arise when the alien plants become so widespread in an ecosystem that they lower the diversity of species. \"It takes a diverse assemblage of plants to support a diverse assemblage of bees. That is the lesson,\" he says. There are still many lessons to learn. Winfree notes the relatively primitive state of pollination ecology: most research on bee diversity has simply counted the number of species, without tracking their fates over time. Her current work examines which bee species are most vulnerable to human disturbance, and explores in more detail whether both rare native bees and efficient pollination services can be restored by increasing the diversity of flowering plants. Still, a new awareness of the vital role of native bees is spreading. Bruce Rominger, who farms onions in Yolo County, California, has interlaced his crops with hedgerows of native plants, including buckwheat and willow. Now, strolling through his fields on a spring day, he recognizes a variety of insects visiting the blossoms \u2014 from plump bumblebees to slender, iridescent solitary bees. Hedgerows are becoming common among Yolo County farms. \"The more native pollinators we have,\" says Rominger, \"the better off we'll be.\" Sharon Levy  is a writer based in   Arcata, California,  and the author of   Once and Future Giants. \n                     Winfree lab \n                   \n                     Claire Kremen \n                   Reprints and Permissions"},
{"file_id": "479164a", "url": "https://www.nature.com/articles/479164a", "year": 2011, "authors": [{"name": "Sharon Levy"}], "parsed_as_year": "2006_or_before", "body": "Pollinating insects are in crisis. Understanding bees' relationships with introduced species could help. Bees thrum among bright red blossoms on a spring day on Mount Diablo, near San Francisco Bay. Alexandra Harmon-Threatt, a young ecologist just finishing her doctorate at the University of California, Berkeley, lovingly identifies an array of native pollinators. She points out three species of bumblebee, each with a unique pattern of black and yellow stripes. There are bee-flies, members of the fly family covered in soft brown fur, which look and act like bees. Among the native insects are plenty of honeybees ( Apis mellifera ), the species raised by beekeepers worldwide and introduced to the Americas by English settlers in the seventeenth century. All these insects are drawn to a clump of red vetch ( Vicia villosa ), an invasive weed. Just down the road is a patch of native lupins, laden with purple blossoms. But the lupins bloom in silence: no bees attend them. For the past three years, Harmon-Threatt has been studying the ways in which the native yellow-faced bumblebee ( Bombus vosnesenskii ) uses the plants growing in the area. By capturing bees as they visit plants and then sampling the pollen they carry, she has confirmed in unpublished work that they get much of their food from introduced plants. And by analysing the amino-acid content of pollen, Harmon-Threatt has shown that bee foraging behaviour can be driven by a craving for nutrients rather than an evolved attachment to a specific plant. Although many conservationists assume that introduced plants are always destructive, her work shows that it's not necessarily so from a bee's point of view. What matters to most bee species is the abundance and quality of pollen \u2014 and if an introduced plant, such as the red vetch, offers more protein-rich food than the natives around it, the bees will collect its pollen. Harmon-Threatt is one of a growing group of scientists studying the evolving relationships between native bees and introduced plants. Their work is critical in a world where human actions have dramatically shifted the distributions of plants and are forcing a pollinator crisis. Most flowering plants need animal pollinators in order to reproduce, and bees serve that role for many important crops \u2014 including fruits, pulses, some vegetables and alfalfa \u2014 many of which were themselves introduced to the United States. Yet stocks of the domesticated honeybee have been declining in the United States and Europe: the number of managed hives in the United States, for example, has dropped from nearly 6 million in the 1940s to 2.3 million in 2008 (see 'Sting in the tale'). Habitat loss, pesticide poisoning, viruses and parasitic mites, any or all of which may be behind the mysterious syndrome called colony collapse disorder, have taken their toll on the domesticated bees, leaving farmers increasingly dependent on native bees. But they, too, are suffering from the effects of pesticides, disease and changes in land use. What bees need most, the new pollination studies have shown, is a diverse community of flowering plants that bloom throughout the spring and summer. Abundance and diversity matter more than whether species are native or exotic. These findings could inform conservation strategies used by farmers and other land managers. Park managers tend to target invasive weeds such as red vetch with herbicides because they can outcompete native plants. But for bees, \u201cjust taking all the vetch out might not be the best idea\u201d, says Harmon-Threatt. \u201cIt might take ten to fifteen different species of native plants to support this array of pollinators.\u201d Stories of exquisitely specialized pollination systems \u2014 such as those of yuccas, which are pollinated only by coevolved moth species \u2014 can give the impression that pollination is an exclusive, highly choreographed dance. \u201cUntil the past five or ten years, people thought that exclusive pollination relationships were more common,\u201d says Rachael Winfree, a pollination biologist at Rutgers University in New Brunswick, New Jersey. By studying entire networks of pollinators and plants, however, biologists have learned that most native bees are far less picky than was imagined. Winfree and her colleagues have investigated the ways in which bees use flowers growing in agricultural, urban and natural areas \u2014 ranging from woodland to farm fields and suburban gardens \u2014 in central California and southern New Jersey. The study, led by Neal Williams at the University of California, Davis, and published earlier this year 1 , found that bees collect pollen from both alien and native plants in proportion to a plant's abundance in the landscape. In highly disturbed habitats, bees make greater use of alien plants \u2014 not because the bees prefer them, but simply because introduced plants are more common where people have transformed the landscape. That makes sense to Winfree. \u201cI don't see why bees would know or care whether a plant was native or exotic,\u201d she says. But not all altered landscapes are equal for bees: modern agriculture has taken a severe toll on wild bee numbers. Vast monocultures \u2014 such as the almond orchards of central California and the soybean fields of Argentina \u2014 bloom for only three or four weeks each season, offering no food for bees the rest of the time. \u201cThe expansion of these crops destroys habitat for bees,\u201d says Marcelo Aizen, a pollination biologist at the National University of Comahue in San Carlos de Bariloche, Argentina. Claire Kremen, a conservation biologist at the University of California, Berkeley (and Harmon-Threatt's mentor), has shown that the diversity of pollinators drops with increasing distance from wild habitat, as does the number of visits by wild bees to flowering crops 2 , 3 . This cuts crop yields. A study by Aizen and his colleagues, published in April this year, documented a drop in the yield per acre of pollinator-dependent crops since 1961, even as total global production has increased 4 . Falling yields have prompted farmers to put more land under cultivation, further eroding bee habitat. Modern agriculture seems locked in a vicious circle of pollinator destruction. Yet Kremen and her colleagues showed in 2004 that crop pollination by native bees increases dramatically when natural habitat exists within 1\u20132.5 kilometres of farm fields 3 . Farms where just 30% of the surrounding landscape is covered in wild vegetation are completely pollinated by native bees, and flourish without help from domesticated honeybees. As most crops in California's Central Valley are far from patches of wild habitat, Kremen and Williams have been experimenting by growing hedgerows of diverse flowering plants in orchards and fields. They now have a list of native California plants, such as redbud ( Cercis occidentalis ) and wild asters, which can be combined to create ideal hedgerows, providing pollen-rich blooms from early spring to late autumn. The results are not yet published, but Kremen says it is already clear that the hedges boost the diversity of native bees, and they are being adopted by farmers. The burning question now, says Kremen, is \u201chow much hedgerows can contribute to long-term population persistence of individual bee species\u201d. \n               Weeds will do \n             Winfree finds that bees don't even need pristine hedges \u2014 weeds will do. She studies bee communities in parts of New Jersey and Pennsylvania where native bees pollinate about 90% of the crops. In one study, she and her team watched 6,187 bee visits to watermelon and tomato crops on 23 farms 5 . Both computer modelling and observation suggest that these crops are fully pollinated by wild bees. That's possible, Winfree explains, because the wet climate encourages the growth of weedy plants that spring up at the field edges, and bees use these scraps of habitat to nest and forage. There's another crucial difference from California: in Winfree's study area most farmers plant a variety of crops rather than monocultures. In a study of New Jersey pine\u2013oak forest 6 , Winfree was surprised to find that bee populations are more abundant and diverse near sites of human disturbance \u2014 where backyard gardens or farm fields add to the range of blossoms available. But the picture is likely to vary from one area to the next. In a recent review of the literature, Winfree and her colleagues concluded that land-use changes such as urbanization and deforestation can affect native pollinators differently, depending on whether they increase or reduce the numbers and diversity of flowering plants 7 . There's yet another complication: although some exotic plants can feed native pollinators, such plants can also fuel the growth of alien bee populations. Aizen and his colleagues have analysed webs of plants and pollinators in the southern Andes and on islands in the North Atlantic and the Indian Ocean 8 . They found that, in some cases, exotic plants and pollinators team up to dominate resources, to the detriment of native bees and native plants. \u201cYou cannot generalize and say it is good or bad to have alien plants,\u201d says Aizen. Problems arise when the alien plants become so widespread in an ecosystem that they lower the diversity of species. \u201cIt takes a diverse assemblage of plants to support a diverse assemblage of bees. That is the lesson,\u201d he says. There are still many lessons to learn. Winfree notes the relatively primitive state of pollination ecology: most research on bee diversity has simply counted the number of species, without tracking their fates over time. Her current work examines which bee species are most vulnerable to human disturbance, and explores in more detail whether both rare native bees and efficient pollination services can be restored by increasing the diversity of flowering plants. Still, a new awareness of the vital role of native bees is spreading. Bruce Rominger, who farms onions in Yolo County, California, has interlaced his crops with hedgerows of native plants, including buckwheat and willow. Now, strolling through his fields on a spring day, he recognizes a variety of insects visiting the blossoms \u2014 from plump bumblebees to slender, iridescent solitary bees. Hedgerows are becoming common among Yolo County farms. \u201cThe more native pollinators we have,\u201d says Rominger, \u201cthe better off we'll be.\u201d \n                     Geneticists bid to build a better bee Tue May 17 00:00:00 EDT 2011 \n                   \n                     Conservation: The trouble with bumblebees Wed Jan 12 00:00:00 EST 2011 \n                   \n                     Plight of the bumblebee Wed Nov 17 00:00:00 EST 2010 \n                   \n                      Wed Nov 17 00:00:00 EST 2010 \n                   \n                     Winfree lab Wed Nov 17 00:00:00 EST 2010 \n                   \n                     Claire Kremen Wed Nov 17 00:00:00 EST 2010 \n                   Reprints and Permissions"},
{"file_id": "479166a", "url": "https://www.nature.com/articles/479166a", "year": 2011, "authors": [{"name": "Richard Monastersky"}], "parsed_as_year": "2006_or_before", "body": "The US government says that a huge earthquake risk lurks in the heart of the country, where a series of large shocks hit 200 years ago. Seth Stein says that kind of warning is dead wrong. The lethal fault cuts through the middle of a Tennessee bean field and then ducks beneath the Mississippi River, making a beeline for New Madrid, Missouri. Named the Reelfoot fault, this geological crack combined with neighbouring faults two centuries ago to unleash a series of devastating earthquakes that have been called the biggest to strike the contiguous United States in recorded history. On government hazard maps, the New Madrid region stands out as a red bull's eye. This spot in the middle of the continent \u2014 far from the plate boundaries that produce Earth's greatest quakes \u2014 would seem to be every bit as dangerous as San Francisco or Los Angeles. Seth Stein poses for a picture on top of the Reelfoot fault, but the geology just doesn't cooperate. For something supposedly so dangerous, the quake-maker is hardly noticeable: the only sign is a 3-metre-high bump. \"You don't see much of a fault,\" snorts Stein, a geophysicist at Northwestern University in Evanston, Illinois, who has studied the region for the past 20 years. To Stein, the lack of anything substantial to photograph is one of several pieces of evidence suggesting that the US government and many scientists are wrong about the hazard here. If the Reelfoot fault had been popping off giant earthquakes repeatedly, for a long geological time, it would have built up a noticeable scarp, like the impressive topography of a mountain range. But there's nothing like that here. And when Stein and his team surveyed the region with Global Positioning System (GPS) instruments, they found no evidence that the ground was warping in preparation for a repeat of the New Madrid quakes. The seismic-hazard maps that show the New Madrid region to be so dangerous, he says, are sort of \"an emperor's-new-clothes business. Our role was to point out the emperor has no clothes.\" The consequence, says Stein, is that the government and businesses in cities such as Memphis, Tennessee, are misspending vast sums \u2014 potentially billions of dollars \u2014 to construct buildings strong enough to withstand a giant quake that will never show up. \"Building the Midwest to California construction codes is a colossal waste of money,\" he says. Stein and his group have published papers in  Nature, Science   and other journals presenting evidence to support that idea, and last year Stein took his case to the public with a book called  Disaster Deferred , which accused the US Geological Survey (USGS) of inflating the quake risk in the region, known as the New Madrid Seismic Zone. That charge, coupled with Stein's habit of ridiculing people and ideas he finds absurd, has irritated many geoscientists. But his work has forced them to confront the question of whether the United States is over-preparing for earthquakes in New Madrid, while under-preparing elsewhere. The magnitude-5.6 quake in Oklahoma this week and a 5.8 in Virginia in August point to the potential for strong shocks throughout central and eastern United States. As he tours the New Madrid region with  Nature , Stein lays out his case. The evidence emerging from New Madrid, he says, suggests that the faults involved in the quakes of 1811\u201312 are shutting down, shifting the hazard to other faults perhaps hundreds of kilometres away. Stein has hypothesized that the centres of other continents behave in a similar way, with patches of earthquake activity that harass one locale for thousands of years and then jump to other regions. Some of his arguments are gaining ground, although few researchers go as far as Stein in playing down the dangers in the New Madrid area. \"What he's doing here is good; he's raising issues and making hypotheses that could well be true,\" says John Vidale, a seismologist at the University of Washington in Seattle, who this year led a federal panel that investigated the hazard in New Madrid. \"The only thing that angers the community is when he argues that he's right and everybody else is wrong.\" New Madrid is a quiet outpost on the banks of the Mississippi, where the mighty river meanders briefly north and then makes a complete U-turn back south. Throughout much of its history, the town fought \u2014 and lost \u2014 epic battles with the ever-shifting river, until levees were built in the early twentieth century. The New Madrid Historical Museum sits next to the levee and honours the town's seismic history with a detailed exhibition, currently being upgraded in advance of the quakes' bicentennial next month. At the time of the shocks, New Madrid was a bustling port \u2014 one of the most important on the lower half of the Mississippi River. The devastating seismic sequence started with a thunderous bang at around 2 a.m. on 16 December 1811. According to one resident: \"We were awakened by a most tremendous noise, while the house danced about and seemed as if it would fall on our heads.\" The quake and its immediate aftershocks scared many of the residents deeply, but few were killed. \"Several men, I am informed, on the night of the first shock deserted their families, and have not been heard of since,\" wrote the New Madrid resident. Over the next few weeks, hundreds of aftershocks rattled the region, followed by two more major shocks on 23 January and 7 February 1812. The February quake caused the land southwest of the Reelfoot fault to jerk up, whereas New Madrid and the rest of the land northeast of the fault dropped by a combined total of about 3 metres or more. The quake destroyed the town and the Mississippi soon swallowed the land, forcing residents to rebuild farther from the river. The vibrations from the quakes were felt across an area of more than 2.6 million square kilometres \u2014 about one-third of the area of the contiguous United States (see  'Earthquake central' ). \n               boxed-text \n             Because the quakes happened before the advent of modern seismometers \u2014 and in such a sparsely settled part of the young country \u2014 researchers have struggled over the years to determine just how big they were. In 1996, Arch Johnston, a geophysicist at the University of Memphis, used historical accounts of the quakes to estimate that the first shock, which he thought was the largest, had a magnitude somewhere between 7.8 and 8.4 \u2014 a truly monstrous earthquake, bigger than any in the history of the United States outside of Alaska 1 . The events sound even worse in the museum, where a film calls the largest shock \"the worst earthquake in recorded history\". Virtually the entire North American continent experienced some shaking, claims the film. A museum pamphlet cites a study, sponsored by the Federal Emergency Management Agency, projecting that 3,500 people would die if a magnitude-7.7 earthquake were to strike the New Madrid region today. The economic losses, according to the pamphlet, could exceed US$300 billion. Stein grimaces at the film and scoffs at what he considers hazard-mongering. \"The scary thing is how much of this is completely wrong,\" he says. \"The hazard here is wildly overestimated.\"  \n                Finding fault \n              It is that kind of pronouncement that wins Stein few points with his rivals. In debates, his attacks on positions he feels are unsupportable can be hyperbolic. And when his rants reach full steam, he seems like a hyper-intelligent version of George Costanza, the ever-complaining character from the television show  Seinfeld , who, like Stein, also happens to be short and bald with glasses. But Stein has a fertile imagination and has made several important advances in geophysics. As a young faculty member at Northwestern in the 1980s, he and his colleagues developed a widely used model quantifying how Earth's plates have moved over the past 3 million years. The model assumes that each plate is a rigid patch of Earth's outer shell, with not much movement in its interior. The vast majority of earthquakes, after all, happen at the edges of plates, where they crunch together and warp into mountain ranges. But Stein wondered how much the plates were deforming in their centres. The US system of GPS satellites gave him a means of finding out. In 1991, he began a study of the New Madrid area to track the warping of the crust in the centre of the North American plate. Stein was certain that the crust was deforming to some degree there because geologists had been finding evidence of a series of large quakes hundreds of years before the cluster in 1811\u201312. The energy for those quakes must have come from forces straining the crust, perhaps transmitted all the way from the edges of the North American plate. But before Stein could get results from his GPS study, a rival group beat him to the finish by taking a shortcut. The team compared one set of GPS measurements with land-surveying data from the 1950s and found that the New Madrid area was deforming at a rapid rate, about one-third as fast as the San Andreas fault in California 2 . Because they were relying on GPS data alone to catch any movement, Stein and his team had to wait several years and then resurvey the suite of sites around New Madrid, which they did in 1993 and 1997. When Andrew Newman, a graduate student working with Stein, finally crunched the numbers, he found something odd. There was little \u2014 or possibly no \u2014 warping of the crust 3 . \"I was utterly flabbergasted that we did not see anything,\" says Stein. For several years, research teams battled in the scientific literature about whether the crust was deforming around New Madrid. But as more data piled up, they corroborated Stein's original finding. \"As we get better and better at measuring, we're finding this motion is slower and slower,\" admits Robert Smalley Jr, a geophysicist at the University of Memphis, who reported in 2005 that the region was straining at a rapid rate 4 . When Stein saw Newman's GPS analysis, he knew something was wrong with the whole New Madrid story. If the crust was not warping in any noticeable way, he couldn't see how it could store up enough energy to fuel earthquakes as strong as magnitude 8 every 500 years or so \u2014 the accepted wisdom among geoscientists at the time. So Stein and his team proposed that the quakes of 1811\u201312 were much smaller than previously estimated, perhaps no larger than magnitude 7, which, because the scale is logarithmic, would produce just one twenty-fifth the shaking of a magnitude-8.4 quake. As with the GPS results, other research has since backed up Stein's estimate. When Susan Hough, a seismologist at the USGS in Pasadena, and her colleagues went back to witnesses' reports of the 1811\u201312 quakes, they estimated that the biggest shock was between magnitude 7.4 and 7.5 (ref.  5 ). This year, she downgraded the size even further 6 , suggesting that somewhere between 6.8 and 7.0 would fit the data best, which is around what Stein had suggested more than a decade earlier. Yet even before all the confirming evidence emerged, Stein had pushed his GPS data to a controversial conclusion. The New Madrid zone might well be turning off, after a geologically brief spell of activity, he claimed \u2014 and large quakes like the 1811\u201312 sequence might never happen there again.  \n                Under pressure \n              Like most faults across the stable part of North America, the ones in the New Madrid area are hundreds of millions of years old and have remained quiet for most of their lives. But something must have activated them in the recent geological past \u2014 within the past 20,000 years \u2014 and geoscientists have been struggling to understand what that was. A potential answer can be found in the tawny bluffs along Pawpaw Creek in northwest Tennessee. On a visit there, Stein and geologist Roy Van Arsdale of the University of Memphis examine a 25-metre cliff that provides a superb view of the sedimentary strata that are normally hidden beneath the surface. Farther west of this site, in the Mississippi River Valley, parts of this layer-cake sequence are missing. The river stripped away 12 metres of sediment at the end of the last ice age, between 16,000 and 10,000 years ago. In a study of the sedimentary sequence throughout the area, Van Arsdale and his colleagues found that the erosion of those sediments happened right above the faults that have been implicated in the New Madrid earthquakes. They proposed that the removal of all that weight altered the stresses in the subsurface rocks enough to reactivate the New Madrid faults 7 . To test that idea, Van Arsdale and Stein worked with Eric Calais and Andrew Freed at Purdue University in West Lafayette, Indiana, to model the effects of removing so much weight from above the faults. They calculated that the erosion unclamped the faults enough for the ones on the verge of an earthquake to fail, releasing energy that had accumulated over the past \u2014 perhaps millions of years earlier during events such as the growth of mountains in western North America 8 . But once a particular fault has broken, the scientists suggested, there would not be enough stress in the region to trigger another big quake there. So the large prehistoric quakes in that region must have happened on different fault segments from the ones that broke two centuries ago. Rather than seeing the faults around New Madrid in isolation, Stein suggests that they are part of a broad system of interacting faults, and that seismic activity shifts from one set of faults to others over hundreds or thousands of years. He sees a similar pattern in Europe, Australia and China, where earthquakes often do not recur in the same spots. If the New Madrid area were to shut down, the seismic activity would eventually transfer somewhere else, says Stein, probably to the southwest or to the Wabash Valley fault system in Indiana and Illinois, which had a major earthquake 6,000 years ago. Van Arsdale is concerned about the possibility of quakes shifting south towards Memphis, along the edge of the Reelfoot rift, where the North American continent began, briefly, to pull apart some 500 million years ago. Stein, Van Arsdale and others would now like to get GPS measurements and construct a detailed earthquake history of a much broader region around New Madrid, to determine how seismic patterns have moved among the different faults in the central United States. \"Compared with ten years ago, we've made a hell of a lot of progress,\" says Stein. \"I think we're now poised to make significant advances.\" The evidence already available, however, is enough to undermine the government's assessment of the earthquake danger in the New Madrid area, Stein says. \"How on Earth did the survey make this the most dangerous place in North America?\" he asks. \"It's a relatively small hazard.\" This year, the National Earthquake Prediction Evaluation Council, which advises the USGS, convened a panel of independent scientists and engineers to examine the issue. Vidale, who led the panel, said they were motivated in part by Stein's persistent criticism as well as by the impending bicentennial of the quakes. The panel in general supported the survey's most recent assessment from 2008, which showed that the hazard in parts of the New Madrid region surpasses that in Los Angeles or San Francisco, although it said \"it is likely that the estimated New Madrid Seismic Zone hazard may decline moderately in the next hazard assessment due to improved knowledge of past earthquakes and current deformation\" (see  'Do maps magnify the earthquake hazard?' ). According to Vidale, the main problem is that the region remains poorly understood. \"We basically found that there's a lot of uncertainty. Because we don't know exactly what's going on, we need to be prepared for large earthquakes.\" But he also acknowledges the possibility that a large earthquake might never come. About Stein, Vidale says: \"None of his ideas are bad. It's just that he presents them in an extreme way and doesn't allow for the possibility that he might not be right.\" Stein's overall argument gets little backing even from Hough, who has argued strongly that the earthquakes 200 years ago were much smaller than once thought. \"Seth argues that the zone is dead because you don't see strain. And I don't think that's a valid conclusion,\" she says. \"There's enough strain around that could produce a magnitude 7.\" Stein, though, denies that he has been so dogmatic. He says that, in his book and in papers, he presents the idea of New Madrid turning off as a possibility \u2014 not a certainty.  \n                Reality check \n              These issues all come to a head in downtown Memphis, which is full of office buildings, apartments and hotels constructed during the first third of the twentieth century. Many of them are unreinforced masonry \u2014 made of concrete blocks or bricks without any metal skeleton. Engineers consider this class of structure to be one of the most vulnerable during an earthquake. If a quake near magnitude 8 were to strike within the next few decades, it could topple many of these buildings. But if no large quake comes for 50 years or more, most of these buildings will probably be long gone, replaced by newer structures. The Federal Emergency Management Agency has been pushing municipalities in the Memphis area to adopt a standard International Building Code (IBC) that would require new buildings to withstand 3 to 4 times stronger shaking than was required by earlier codes, says Joseph Tomasello, an engineer with the Reaves Firm in Memphis who has collaborated with Stein. But many people in the area have baulked at such stringent requirements, and the city is allowing owners to build to a less restrictive code until it adopts the 2009 version of the IBC, which is expected to happen around the end of this year. Some engineers suggest that the code's enhanced seismic provisions would increase building costs only marginally, but Tomasello contends that they could add an extra 10\u201315%. If all new construction were required to meet the newer code, building costs in the Memphis metropolitan area would be increased by $200 million a year, he estimates. \"When you look at the hazard, it just doesn't make any sense,\" says Tomasello. To Stein, building to a lower level of protection makes sense. The faults that produced the 1811\u201312 quakes are still crackling with aftershocks, and there is a chance that one of them could be as large as magnitude 6.7. But building to the new IBC in Memphis would be a waste of money, he says. \"There's a good chance that all the money you would spend would save no lives at all. But if you spend that money on schools or health care, you're almost guaranteed to save lives.\" The most important thing, Stein says, is for the government to do a rigorous analysis of the costs and benefits of seismic building codes, before forcing people to foot the bill for expensive protections. The next day, as Stein walks through Memphis airport on his way back home, he brings up the issue again. It is 13 September, just two days after the tenth anniversary of the 2001 terrorist attacks on the United States. In the intervening decade, the country has spent $1.3 trillion waging two wars and has instituted time-consuming security provisions at airports, such as herding travellers through scanning machines capable of peering through clothes. To Stein, these are all examples of misunderstood risks and squandered resources. \"I can't do anything about the thousand things the government does that I'm unhappy about,\" he says in a quiet moment. But with New Madrid, \"I can do something about that\". \n                 Richard Monastersky is a features editor for Nature in Washington DC.  \n               \n                 Listen to the  \n                 \n                     Nature Podcast \n                   \n                  for more on the quake hazard.  \n               \n                     Slideshow: the New Madrid quake hazard \n                   \n                     Nature Podcast on the quake hazard \n                   \n                     Nature Geoscience \n                   \n                     US Geological Survey seismic hazard assessments \n                   \n                     Seth Stein \n                   \n                     US Geological Survey on the New Madrid earthquakes \n                   \n                     New Madrid earthquakes bicentennial page \n                   Reprints and Permissions"},
{"file_id": "480022a", "url": "https://www.nature.com/articles/480022a", "year": 2011, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "To save the Amazon, Bruce Babbitt wants to isolate islands of oil and gas production amid a sea of trees. Bruce Babbitt ambles down a walkway flanked by manicured lawns, gleaming office trailers and tidy rows of housing. The drone of vehicles competes with Latin music playing softly in the distance. Only the occasional birdsong or glimpse of macaws serves to remind that this encampment is smack in the middle of the Peruvian Amazon. Babbitt looks into the distance, where the separation towers of a natural-gas facility rise above the trees and shimmer with industrial splendour in the evening sun. \u201cIt's an amazing sight,\u201d he says. The Malvinas natural-gas plant might seem the ultimate insult to a largely unspoiled tropical paradise, particularly for a lifelong conservationist such as Babbitt, who served as Secretary of the Interior \u2014 responsible for managing much of the United States' federal land and natural resources \u2014 under US President Bill Clinton from 1993 until 2001. But where others see blight, Babbitt sees a vision of the future. He looks past the pipes and pollution and focuses instead on what makes this project stand out: seen from the sky, Malvinas is an island of industrial activity in a sea of trees. There are no roads into the site; everything that enters or leaves Malvinas, including gas, rubbish, food and people, does so by plane, boat or underground pipeline. The design is called an offshore\u2013inland development, and Babbitt thinks it might be the western Amazon's only hope. Historically, roads have paved the way for uncontrolled development throughout the region. Without them, there can be no associated logging, squatting or large-scale invasion of the forest, as has happened around other oil and gas operations in the Amazon. \u201cThere's a huge rush all across this region, and the place is going to be destroyed unless this model is embedded in all future discovery and operations,\u201d Babbitt says. \u201cThis is a message that curiously does not have a messenger.\u201d It does now, in Babbitt. In partnership with the non-profit Blue Moon Fund based in Charlottesville, Virginia, Babbitt has used his prominence to urge governments in the region to require that any oil and gas development in the Amazon basin follow the offshore\u2013inland model. His efforts run counter to industry, which fears regulation, and to some environmentalists, who want to avoid selling out to oil and gas producers that have a poor track record in the Amazon. But Babbitt's campaign is an extension of the work he did in the US government, where he forged innovative policies that balanced business and conservation interests on issues ranging from endangered species to energy development in Alaska. Now 73 years old, Babbitt is dedicating himself to protecting the Amazon. Working with Enrique Ortiz, a programme officer with Blue Moon, Babbitt has been shuttling back and forth between Peru and his home in Washington DC to promote the offshore\u2013inland model and a broader goal of smart, controlled development in the Amazon. The heart of their campaign is Malvinas, which began processing gas from the surrounding Camisea natural-gas field (see 'Islands in the jungle') in southeast Peru in 2004. Apart from one field in Brazil, Camisea is the only example of offshore\u2013inland development in the Amazon. The roadless plan was implemented under pressure from environmental groups and the Inter-American Development Bank (IDB), but the idea received scant attention, in part because Camisea has been beset by environmental problems such as pipeline leaks. Babbitt and Ortiz, however, see potential here. They are promoting the idea of roadless development at a crucial time, as energy companies look to expand their operations in the Amazon. Earlier this year, Babbitt and Blue Moon were instrumental in blocking a plan to build a new pipeline from the Camisea field that could have spurred widespread road construction and deforestation. \u201cI'm not a very emotional guy, but I'll tell you I got emotional about this,\u201d Babbitt says, after delivering his first public speech promoting his vision for roadless development at an energy conference in Lima in September. \u201cIt was important to make an issue of it, to demonstrate that it was possible to mobilize opposition.\u201d \n               Damage control \n             Malvinas's isolation comes into focus as Babbitt's helicopter rises into the sky and arcs out to the east, across the jungle. The Urubamba River borders the plant on the west, and a sea of green surrounds it on all other sides. Babbitt and Ortiz are accompanying a crew of six workers to San Martin Number 1, a platform for multiple wells tapping natural gas, propane, butane and other light hydrocarbons trapped in a sandstone formation thousands of metres below ground. En route, they pass over a clearing dotted with thatch-roofed huts near the Camisea River. It is home to one of several indigenous communities that have leased their land to a consortium led by the exploration and production firm Pluspetrol, based in Buenos Aires. 'Uncontacted' tribes that have yet to establish any ties with modern society roam the rolling hills on the horizon. The helicopter circles the production site and gives Babbitt and Ortiz a good view of the operation. Apart from a few small structures containing production equipment, the main feature is an open grass field roughly the size of a football pitch, which is used for setting up equipment. Several pipes rise up out of the ground on one edge of the clearing then dive back below the surface at the base of a hill, marking the beginning of an underground journey to Malvinas, where gas will be separated from liquids then piped to the coast for distribution. Workers cleared a narrow path through the trees when they buried the pipeline, but the jungle has since reclaimed the land, completely camouflaging the route. For the two environmentalists, Malvinas is a proof of principle. \u201cThis is far from an idyllic forest, but what I see is something that is contained,\u201d Ortiz says. \u201cAs long as there is no oil or gas in the river, it's just not that bad.\u201d Pluspetrol has several isolated production platforms tapping the eastern end of the Camisea field, and is expanding production with several platforms to the northwest. At each of these platforms, the company used horizontal-drilling techniques to create multiple wells that veer off underground and tap different regions, helping to limit the amount of forest that needed to be cleared. The project's remote location has made it difficult to assess what might have happened had roads been allowed into Camisea, but historically, deforestation has followed closely on the heels of road building 1 . A lack of roads has kept the western Amazon relatively intact. To the south, a classic fish-bone pattern of secondary roads can be seen branching off the InterOceanic Highway, and roads have accompanied oil and gas operations in Ecuador and northern Peru 2 . One modelling study 3  suggests that the southwestern Amazon, around the confluence of Peru, Brazil and Bolivia, could lose two-thirds of its forest cover by the middle of this century if roads continue to be built without additional protections. \n               Circle of life \n             Babbitt started working full time on Amazon conservation issues just two years ago, after he retired from his law practice in Washington DC. He is not, however, new to the region. He made his first trip there in the summer of 1962, as a master's student in geophysics at Britain's Newcastle University. His professor, Keith Runcorn, pioneered the palaeomagnetic mapping that bolstered the case for plate tectonics, and Babbitt's job was to collect rocks for palaeomagnetic analyses from transects up and down the eastern side of the Andes in Bolivia. He spent much of his time in camps run by Gulf Oil, a company based in Framingham, Massachusetts, that was busy doing early oil and gas reconnaissance.  This is far from an idyllic forest, but what i see is something that is contained.  Babbitt ended up abandoning rocks for law school and then politics, but his geological introduction to the eastern Amazon still comes in handy. \u201cThere's a circular quality to life,\u201d he says. \u201cThat first summer made an impression on me, and I made a commitment to get back.\u201d As a lawyer, Babbitt moved up through the ranks to attorney-general of Arizona, then to governor in the 1980s. He failed to win the Democratic nomination for the US presidency in 1988, but in 1993, Clinton appointed Babbitt to serve as secretary of the Department of the Interior, which put him in charge of roughly 200 million hectares of federal land. It was then that he began to think more deeply about low-impact ways to develop oil and gas, beginning with the North Slope in Alaska. In 1998, the interior department opened up around 1.6 million hectares of wilderness there for drilling but required the use of 'ice roads', which are created each winter to give heavy equipment access to the region. That meant that industrial activity was confined to the winter months and helped to prevent damage to the fragile tundra. Babbitt started to focus on Camisea drilling in 2002, when he chaired a blue-ribbon commission established to review environmental policies at the IDB. At the time, a consortium was seeking US$75 million in loans from the bank to build a pipeline from Camisea to the coast. Under pressure from environmentalists, the IDB imposed several requirements, including roadless development, collaboration with native communities and a programme to monitor biodiversity. It was an important time for Babbitt. The Camisea example showed how international lending institutions could impose social and environmental constraints on such projects. And despite a litany of complaints about Pluspetrol's actions in Camisea, Babbitt and Ortiz say that the roadless requirements have held up \u2014 so far. Their optimism is not shared by all. For many environmental and social activists, Camisea is a deeply flawed project. It has been rocked by scandal, from ruptured pipelines in the early years to questions about how the government distributes revenue from the project. Activist groups such as Amazon Watch in San Francisco, California, argue that Pluspetrol could have built fewer platforms by taking additional steps to consolidate their drilling, and that the pipeline has caused extensive erosion along the route carrying gas south out of the basin from Malvinas. C\u00e9sar Gamboa, political director for the Law, Environment and Natural Resources centre in Lima, supports the offshore\u2013inland model, and his group has received funding from Blue Moon to work on legal issues associated with oil and gas development and their impact on indigenous cultures. But Gamboa sees the model as just a first, small step and argues that companies need to do much more to limit the environmental and cultural impact of operations in the Amazon. Gamboa says that the main issue is what happens to indigenous communities as development moves forward. Communities that have secured title to their lands can gain access to health and education services through development. As oil and gas production expands, however, it will encroach on the uncontacted tribes, which have no formal rights or title to the land they live on. \u201cIt's an issue of human rights,\u201d says Gamboa. What is clear is that the operations in southeast Peru are just the beginning. Pluspetrol has its eyes on indigenous territories to the east of Camisea, and other companies are moving into the region. The Spanish company Repsol is exploring to the north, and Brazil's government-controlled energy giant Petrobras is working to the west and south. The Peruvian government has already granted exploration rights for roughly half of its Amazon territory, and Babbitt says that it is just a matter of time before oil and gas companies seek access to the crown jewel of the Peruvian Amazon, Manu National Park, the country's first major conservation initiative and an area of extreme biodiversity. In addition to funding Peruvian non-governmental organizations to conduct research and draft legislative proposals, Babbitt and Ortiz are busy spreading the word within industry and government. Babbitt spoke about the offshore\u2013inland idea during a conference this autumn on Latin American energy investments in New York, and he is now taking the message to investor groups that promote corporate responsibility. He and Ortiz are also expanding their audience within South America by initiating talks with the Brazilian Development Bank and the Brazilian industrial giant Odebrecht Group, based in Salvador, which is working on a big pipeline into the Camisea region with Petrobras, based in Rio de Janeiro. Babbitt acknowledges that it is a tough sell, even among the companies that have developed Camisea and proudly tout its environmentally friendly attributes. Two years ago, the consortium running the pipeline sought to build a second pipeline, which was to be routed through the lower Urubamba River valley, and through an important environmental sanctuary. That plan raised alarm because to construct a pipeline, a temporary road would need to be built to move equipment around. Settlers often follow roads, so temporary roads become permanent ones, leading to deforestation and expanded development. That was not a risk with the first pipeline because the tortuous path it follows up and down hills is not attractive to loggers, miners and others seeking new territory. When Babbitt and Ortiz learned about the plans for a new pipeline, they immediately moved to block it. Ortiz mobilized \u2014 and in some cases funded \u2014 environmental groups in Peru while Babbitt lobbied old contacts on Capitol Hill and at the IDB. They also made their case to Thomas Lovejoy, an Amazon ecologist at George Mason University in Fairfax, Virginia, who is on the advisory board of Hunt Oil in Dallas, Texas \u2014 one of Pluspetrol's partners in the Camisea project and in the pipeline consortium. Hunt Oil officials were sympathetic to Babbitt and Ortiz's case and pushed for a new pipeline route. In the face of growing opposition from environmental and indigenous groups, the consortium this year redirected the pipeline away from the sanctuary and up into the hills. Despite Amazon Watch's reservations about Camisea, its executive director, Atossa Soltani, gives Babbitt and Ortiz credit for helping to persuade the companies to rethink the pipeline. \u201cThey took a tactical approach,\u201d she says, \u201cand they were really effective.\u201d Officials at Pluspetrol, however, do not hold up Camisea as a model. Nelson Soto Fuentes, the firm's director of environmental issues and community relations, warns that roadless development is both expensive and difficult, and that some communities might actually welcome roads. Others are more open to the idea. Carlos del Solar, a consultant for Hunt Oil, says that companies are increasingly looking to this kind of development as a way to avoid the social conflicts that afflict many major infrastructure projects in the region. \u201cIt's expensive, but in the long run it pays off,\u201d del Solar says, although he is not sure that government regulation is necessary. As he prepares to leave Malvinas, Babbitt puts Camisea in a historical context, comparing the Amazon of today to America's lawless Wild West. Although the struggle between conservation and development continues across the West, including in Babbitt's Arizona, the US government has imposed the rule of law and set up a process to manage public lands. Babbitt hopes that the same will happen in the Amazon. His goal is to slow down the most destructive road building and development until that time comes. The offshore\u2013inland model is not a complete answer, he says, \u201cbut it starts the process of thinking about development in the right way\u201d. \n                     A struggle for power Wed Nov 09 00:00:00 EST 2011 \n                   \n                     Worth a dam? Tue Jun 21 00:00:00 EDT 2011 \n                   \n                     Climate: Counting carbon in the Amazon Wed Oct 21 00:00:00 EDT 2009 \n                   \n                     The Camisea Project Wed Oct 21 00:00:00 EDT 2009 \n                   \n                     Inter-American Development Bank on Camisea Wed Oct 21 00:00:00 EDT 2009 \n                   \n                     Blue Moon Fund Wed Oct 21 00:00:00 EDT 2009 \n                   Reprints and Permissions"},
{"file_id": "480026a", "url": "https://www.nature.com/articles/480026a", "year": 2011, "authors": [{"name": "Roberta Kwok"}], "parsed_as_year": "2006_or_before", "body": "A menagerie of intriguing cell structures, some long-neglected and others newly discovered, is keeping biologists glued to their microscopes. In 2008, Chalongrat Noree faced an unenviable task: manually surveying hundreds of yeast strains under a microscope. Each strain had a different protein tagged with a fluorescent label, and Noree, a graduate student at the University of California, San Diego, was looking for interesting structures in the cells. But it wasn't long until Noree's labour yielded results: within a month, he began finding a wide variety of proteins assembling into clusters or long strands. \u201cImagine every week you found a new intracellular structure,\u201d says Jim Wilhelm, a cell biologist and Noree's adviser. \u201cIf it were a slot machine, it would be paying off every other time you pulled the handle.\u201d These days, textbook diagrams of cell structures such as the nucleus, mitochondrion, ribosome and Golgi apparatus are beginning to seem out of date. New imaging techniques, genome data, interest from disciplines outside cell biology and a bit of serendipity are drawing attention to an intricate landscape of tubes, sacs, clumps, strands and capsules that may be involved in everything from intercellular communication to metabolic efficiency. Some could even be harnessed for use in drug delivery or in synthesis of industrial products, such as biofuels. Some of these structures have been known for decades, whereas others have only recently come to light. Wilhelm's team, for instance, has found six kinds of filament that either had never been described, or had been largely passed over. \u201cYou figure, how many structures could have been missed in the cell?\u201d says Wilhelm. \u201cApparently, a lot more than you would imagine.\u201d \n               Lines of communication \n             One structure that is receiving fresh scrutiny is the membrane nanotube: a thin thread of membrane suspended between cells. In 2000, Amin Rustom, then a graduate student at Heidelberg University in Germany, was using a newly acquired dye to look at rat tumour cells under a fluorescence microscope. But he decided to skip some washing steps in the protocol. \u201cHe said, 'I saw something \u2014 I don't know what it is, but it looks interesting',\u201d recalls his former adviser, Hans-Hermann Gerdes, a cell biologist now at the University of Bergen in Norway. The tubes that Rustom had noticed were so straight that Gerdes initially wondered if they were scratches on the dish. The team concluded in a 2004 study 1  that the structures, which could span the distance of several cells, were channels that could transport small cellular organelles. That same year, Daniel Davis, a molecular immunologist at Imperial College London, and his colleagues proposed that immune cells might send signals to each other along such tubes 2 . At the time, Davis recalls, \u201cThere would always be people in the audience who would say, 'I saw those strands in the late 1970s or 80s'.\u201d But earlier observers paid little heed to the tubes. The 2004 reports prompted more studies, which have found nanotubes in many types of mammalian cell. Davis's team found that nanotubes could help certain white blood cells to kill cancer cells, either by acting as a tether that draws the cancer cell close or by providing a conduit for delivering lethal signals 3 . Nanotubes can also conduct electrical signals, which might enable cells to coordinate during migration or wound healing, according to a 2010 study by Gerdes and his colleagues 4 . HIV and prions \u2014 infectious, misfolded proteins \u2014 may even travel along the tubes 5 , 6 . Some researchers are sceptical that nanotubes can form open channels. \u201cIt's not clear that there's a real continuous tunnel,\u201d says Jennifer Lippincott-Schwartz, a cell biologist at the US National Institutes of Health in Bethesda, Maryland. And so far, nanotubes have been studied mainly in cell culture. Blocking nanotube formation in living organisms might give clues to their importance, says Davis. But such manipulations often disturb other crucial processes. \n               Productivity hotspots \n             Researchers have long puzzled over how some metabolic processes work so efficiently. If the proteins involved are not close together, intermediate molecules could get lost in the \u201cbewildering mass of enzymes in the cell\u201d, says Stephen Benkovic, a chemical biologist at Pennsylvania State University in University Park. Proteins often assemble to carry out a particular task \u2014 a large complex is required to copy DNA, for example \u2014 but Benkovic and others have wondered whether metabolic enzymes might cluster together in a multistep assembly line, passing sometimes-unstable molecules from one 'worker' to the next. Benkovic's group found evidence that this clustering does occur in enzymes that produce a precursor of purine nucleotides, which are components of DNA and RNA. The team tagged each enzyme with a fluorescent label and observed them in living cells under the microscope. When a cell was deprived of purines, the enzymes grouped together in a cluster, which the team called the 'purinosome' 7 . Last year, the team reported that purinosomes are nestled in a mesh of protein fibres called microtubules, like berries in a bramble bush 8 . The molecules produced by purinosomes can be converted to the cellular fuel adenosine triphosphate, so Benkovic speculates that purinosomes may help power the transport of organelles and materials around the cell on microtubule tracks. Edward Marcotte, a systems biologist at the University of Texas at Austin, advises caution in interpreting these results, however. He and his colleagues have seen enzyme clusters as well: in 2009, they reported that they had found 180 types of protein forming clumps in starved yeast cells 9 . But it is not clear whether the clumps serve a useful purpose \u2014 such as improving metabolic efficiency or acting as storage depots \u2014 or are a result of cellular failures brought on by starvation, says Marcotte. \n               Tiny factories \n             Some researchers are taking a closer look at elegant bacterial protein containers called microcompartments. First seen about 50 years ago, these polyhedron-shaped protein capsules resemble the outer shell of a virus 10 . But unlike viruses, which package genetic material, microcompartments contain enzymes that carry out important reactions, such as converting carbon dioxide into a form of carbon that is usable by the cell. Scientists suspect that the shells make reactions more efficient, keep toxic intermediate products away from the rest of the cell and protect enzymes from molecules that could hinder their performance. In 2005, protein crystallographers helped to reveal the capsules' finer details. Microcompartments \u201csimply hadn't attracted the attention yet of structural biologists\u201d, says Todd Yeates, a structural biologist himself at the University of California, Los Angeles. He and his colleagues found that some shell proteins assemble into six-sided tiles that come together to form the sides of a microcompartment 11 . Each tile has a hole in the centre that could allow molecules to pass through. In addition to having an orderly structure, microcompartments can also line up in neat rows. Pamela Silver, a synthetic biologist at Harvard Medical School in Boston, Massachusetts, and her colleagues reported 12  last year that in cyanobacteria, certain microcompartments called carboxysomes \u201cmore or less stayed in a line down the centre of the cell\u201d, says Silver. This tidy arrangement allows cells to allot carboxysomes evenly to daughter cells when dividing. Biologists are now eager to exploit these capsules for industrial uses by loading them with different enzymes. For instance, Yeates and his team are planning to try engineering microcompartments to produce biofuel. Some researchers have managed to package fluorescent proteins or enzymes from other species into the shells, suggesting that it is possible to modify the capsules' contents. Microcompartments still offer plenty of unexplored territory. Scientists aren't sure, for instance, exactly how enzymes are organized inside the capsules, says Cheryl Kerfeld, a structural biologist at Lawrence Berkeley National Laboratory in Berkeley, California. \u201cWe don't really know what it looks like in there.\u201d \n               Cargo containers \n             Other subcellular packages drawing attention are exosomes \u2014 tiny membrane-enclosed sacs that form inside the cell and are later spat out. These nanoscale vessels were discovered in the 1980s and then ignored for about a decade \u2014 considered a way of bagging up cellular rubbish. \u201cPeople thought they were junk, basically,\u201d says Jan L\u00f6tvall, a clinical allergist at the University of Gothenburg in Sweden. Interest in exosomes picked up in 1996, when Gra\u00e7a Raposo, a cell biologist now at the Curie Institute and the National Centre for Scientific Research in Paris, and her colleagues scrutinized exosomes spat out by B cells, a type of white blood cell. Although the technology to examine them \u2014 electron microscopy \u2014 wasn't new, it wasn't very popular at the time because \u201cit was just old-fashioned\u201d, says Raposo. Using it and other techniques, the team reported that the humble vessels might do something useful: display scraps of pathogen protein on their surfaces, spurring immune cells to mount defences against an infection 13 . Scientists became even more intrigued when L\u00f6tvall's team reported in 2007 that exosomes could carry messenger RNA 14 , some of which could be picked up and translated in a recipient cell. This suggested that the shipments might allow cells to affect protein production in their neighbours. The study \u201creally showed that exosomes were a vehicle of communicating important information between cells\u201d, says Clotilde Th\u00e9ry, a cell biologist who is also at the Curie Institute. Researchers are now trying to use exosomes to deliver drugs to specific parts of the body \u2014 with the hope that, because exosomes are 'natural', they might be less likely to be toxic or provoke an immune response than other vessels, such as artificial lipid sacs or protein shells. This year, Matthew Wood, a neuroscientist at the University of Oxford, UK, and his colleagues reported 15  an attempt in mice: the team loaded exosomes with artificial RNA intended to hinder production of a protein involved in Alzheimer's disease and tagged them with a molecule directing them to neurons and the blood\u2013brain barrier. The exosomes successfully delivered their cargo and reduced production of the protein with no obvious ill effects, the team found. Other scientists are trying to fish exosomes out of body fluids and analyse their contents to diagnose cancer or deploy exosomes to provoke immune responses against tumours. \n               Cell serpents \n             Finally, Wilhelm's group and others have found filaments that string together enzymes by the hundreds or thousands \u2014 enough, in some cases, to span nearly the entire cell. One of the filament-forming enzymes Wilhelm's team found was CTP synthase, which makes a building block for DNA and RNA 16 . Two other teams discovered the same filaments in fruitflies and bacteria at around the same time 17 , 18 . One researcher, Ji-Long Liu, a cell biologist at the Medical Research Council Functional Genomics Unit at the University of Oxford, named them cytoophidia (or 'cell serpents') because of their snake-like shapes in fly cells. Wilhelm suspects that researchers found the same filaments in the 1980s but never identified the protein. These structures could allow the cell to turn enzymes on and off en masse, suggests Wilhelm. For instance, if the enzymes in a filament are inactive, the cell could activate all of them by dissolving the strand. In some bacteria, enzyme filaments also seem to serve a structural purpose, somewhat like the actin filaments that are part of the cytoskeleton in more complex cells. When Zemer Gitai, a cell biologist at Princeton University in New Jersey, and his colleagues studied the structures in a comma-shaped bacterium called  Caulobacter crescentus , they found that CTP-synthase filaments kept the cells' curvature in check. If there was too little of the enzyme, the cells curled up tightly; if there was too much, they straightened out 18 . It is not clear why curvature is important for the bacterium, says Gitai, but the findings suggest that the cells may have co-opted enzyme filaments to preserve cell shape. Researchers already suspect that actin is related to the enzyme hexokinase. It is possible that the cytoskeleton arose from filaments that originally formed to regulate the cell's metabolism, Gitai says. Although the purpose and importance of some of these emerging structures is not yet clear, the research illustrates that the act of simply observing cells and their contents is alive and well. \u201cA key aspect of doing great science is exploration,\u201d says Davis. \u201cI think that there's a tremendous amount that we learn just by watching.\u201d \n                     Nanotubes help cells pass messages Mon Sep 20 00:00:00 EDT 2010 \n                   \n                     Making a cellular menagerie Fri Oct 23 00:00:00 EDT 2009 \n                   \n                     Cell biology: Ahead of the curve Wed Jul 15 00:00:00 EDT 2009 \n                   \n                     Cell biology: Bacteria's new bones Wed Jan 09 00:00:00 EST 2008 \n                   \n                     Hans-Hermann Gerdes Wed Jan 09 00:00:00 EST 2008 \n                   \n                     Daniel Davis Wed Jan 09 00:00:00 EST 2008 \n                   \n                     Facebook page for exosomes, microvesicles and other secreted vesicles Wed Jan 09 00:00:00 EST 2008 \n                   \n                     Graca Raposo Wed Jan 09 00:00:00 EST 2008 \n                   \n                     Stephen Benkovic Wed Jan 09 00:00:00 EST 2008 \n                   \n                     Edward Marcotte Wed Jan 09 00:00:00 EST 2008 \n                   \n                     Jim Wilhelm Wed Jan 09 00:00:00 EST 2008 \n                   \n                     Ji-Long Liu Wed Jan 09 00:00:00 EST 2008 \n                   \n                     Zemer Gitai Wed Jan 09 00:00:00 EST 2008 \n                   Reprints and Permissions"},
{"file_id": "480310a", "url": "https://www.nature.com/articles/480310a", "year": 2011, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Oliver Br\u00fcstle fought for more than a decade to pursue and patent human embryonic stem-cell research in Germany. Now his efforts have backfired. It was overcast and unseasonably sultry in Luxembourg, home to the European Court of Justice, on the morning of 18 October 2011. But German neuroscientist Oliver Br\u00fcstle wasn't sweating when the 13 judges entered the court room in their flowing crimson robes. Not, that is, until they delivered their verdict, which spelt the end of his lengthy fight to defend his patent on human embryonic stem (ES) cells from attack by Greenpeace. It took barely two minutes for the court's president to summarize aloud the four pages of the judgment. The ruling, which cannot be appealed, upheld Greenpeace's position. It declared that any patent depending even indirectly on human ES-cell lines is outlawed on moral grounds throughout the European Union (EU). Unexpectedly, it added that any research using such cell lines was similarly immoral. For a few moments Br\u00fcstle was so shocked he could barely draw breath. Outside, the clouds burst and rain poured down. \u201cWhat hurt most personally was the accusation that scientists who work on human ES-cell lines are somehow immoral,\u201d says Br\u00fcstle, who says he never doubted that he would win the case. \u201cBut now I accept it's the end of the road.\u201d That road has been a long and twisted one for Br\u00fcstle, one of Europe's leading stem-cell researchers, based at the University of Bonn in Germany. Germany is a scientific powerhouse, but it has always been slow to accept any form of new genetic technology. Long before the patent battle, Br\u00fcstle was fighting to pursue research using human ES cells \u2014 which can differentiate into many types of mature cell \u2014 while the German government vacillated about its legitimacy. The latest dispute centred on the ethics of patenting discoveries that stem from the use of human embryos. Br\u00fcstle argued that patenting was crucial because, without it, industry would not be interested in pursuing what he saw as potentially life-saving therapies. And whereas some people charge that patenting could actually impede research, Br\u00fcstle argues that patents held by academics are normally free for other researchers to use. Br\u00fcstle admits his battles were lonely, and took their toll on him and his family, who at one time even found themselves under police protection. Yet he says that he never once considered pulling out. \u201cI just didn't think it right that an environmental organization, rather than a government, should define research agendas,\u201d he says. Br\u00fcstle's single-mindedness is not a reflection of an aggressive personality: his aura is calm, not domineering. \u201cI think he's just been driven \u2014 rightly \u2014 by strong scientific convictions,\u201d says stem-cell biologist Elena Cattaneo at the University of Milan, Italy, who has worked with him as part of scientific networks funded by the European Commission. \u201cOliver has been an important figure in the ethical debate and he deserves a lot of credit,\u201d adds Ronald McKay, a stem-cell researcher at the Lieber Institute for Brain Development in Baltimore, Maryland, and a former mentor of Br\u00fcstle's. The reality, however, is that although those convictions have helped to further human ES-cell work in Germany a little, only a handful of research groups have ever tried to work with such cell lines. And Br\u00fcstle's patent fight may now have backfired, because the new ruling could hinder researchers working on human ES cells throughout Europe. By declaring such research to be immoral, many fear, the court ruling could prompt funding agencies to stop funding it \u2014 and it certainly means that no patent, foreign or home-grown, that involves human ES cells can be defended in Europe. Even Christoph Then, an equally driven one-man crusader against biopatenting who spearheaded Greenpeace's campaign, expressed surprise at the court's wide-reaching condemnation of human ES-cell research. \u201cI don't think anyone has the moral right to patent any sort of life, so this is a happy victory for us,\u201d he says. \u201cBut our intention was never to stop basic research in the area.\u201d \u201cIt's easy, in hindsight, to question if I was right to take the patent fight so far,\u201d Br\u00fcstle says now, \u201cand I'm sorry that there has been collateral damage for my colleagues in other countries.\u201d But, he adds, he would do exactly the same if he had his time again. \u201cNo one could have predicted that what started with a goal to have more liberal rules in Germany would have made Europe ultra-restrictive.\u201d As a medical student at the University of Ulm in southern Germany, Br\u00fcstle fell in love with the brain. He set a career trajectory to become a neurosurgeon, but in the early months of his residency at the University of Erlangen, he felt the irresistible draw of research. \u201cI wanted to do more than just remove brain tumours,\u201d he says. \u201cI wanted to find out how brain disease could be avoided.\u201d He interrupted his residency in 1993 for a research break in McKay's lab at the National Institute for Neurological Disorders and Stroke in Bethesda, Maryland. There he studied how human fetal brain cells behaved after they were grafted into the brains of embryonic rats 1 . \u201cWe joked in the lab that we wanted to see if it would be the American way of adapting to a new environment or the European way of not adapting,\u201d he recalls. The cells adapted. When, in the mid-1990s, several research groups developed procedures for turning mouse ES cells into heart cells using a cocktail of growth factors, Br\u00fcstle adapted the method to generate neural cells 2 . \u201cSuddenly, stem-cell biology was at a point where reconstructive therapy was not just a pipe dream,\u201d he says. Br\u00fcstle wanted to be part of the global research effort that would realize it. Like his stem cells, Br\u00fcstle had adapted well to his new home, but he began thinking about returning to Germany to establish an independent scientific career. He was drawn by the newly dynamized research environment, where scientists were being strongly encouraged by universities and the government to think about how their basic research could be commercialized or otherwise put to use for the public good. Patenting in particular was being encouraged. \u201cNewspaper articles had been saying that German scientists were too stupid to patent, unlike American scientists who were helping the US economy \u2014 it was a constant lament,\u201d Br\u00fcstle recalls. He returned to Bonn in September 1997, and shortly thereafter submitted his fateful patent on procedures for generating neural precursors from ES cells to the German Patent Office. It was based on his mouse work, but its claims covered most species, including primates, both non-human and human. He thought that a method to derive endless neural precursors would be bound to prove useful in medicine. \u201cI wasn't thinking in concrete terms about commercialization,\u201d he says. \u201cI just had an idea that a company might want to license it at some point and that would help translation to the clinic.\u201d The patent was granted in 1999. The year before, James Thomson, then at the University of Wisconsin\u2013Madison, reported his team's success in generating the first stem-cell lines from human embryos 3 . Like many scientists, Br\u00fcstle was keen to get hold of the cell lines and translate his animal work to humans. Knowing that Thomson had been swamped with requests for collaboration, he simply booked a flight to Madison. \u201cI told Jamie I would just stay in the hotel until he had time to meet me and talk about research we could do together.\u201d They met, and hatched a plan to work together on ways to generate neural precursor cells from Thomson's new cell lines. For Br\u00fcstle, this decision also marked the start of his fight for acceptance of human ES-cell work in Germany. He put together a grant application to work on the methods he had devised with Thomson, and in August 2000 submitted it to the DFG, Germany's main university granting agency. \u201cI had a gut feeling though that its passage may not be smooth,\u201d says Br\u00fcstle. \u201cIt just wasn't clear whether the DFG would be allowed to fund such work.\u201d The country's 1990 Embryo Protection Act forbids research on human embryos but does not refer to research on established ES-cell lines, which didn't exist when it was written. In 1999, the DFG stated that a 'societal debate' was needed on the issue and in the meantime it would fund just adult stem-cell research. Br\u00fcstle's application was a direct challenge to this stance. Br\u00fcstle, who is a practising Catholic, had thought hard about his own moral position. He disagrees with the creation of human embryos specifically for research. But almost all human ES-cell lines have been derived from embryos leftover from fertility treatment that would otherwise have been destroyed. Br\u00fcstle maintains that using them for biomedical research rather than discarding them is the moral imperative. Ernst-Ludwig Winnacker, a biochemist and then-president of the DFG, decided that Br\u00fcstle's application should be peer reviewed. It was highly rated. In May 2001, the DFG opined that human ES-cell research on imported cell lines was legal, morally justified and scientifically necessary. It was ready to fund Br\u00fcstle. This was a point of explosion. Within days the government intervened, asking the DFG to defer giving Br\u00fcstle his grant to allow more societal debate. That debate, which grew progressively hotter, took the form of local and national panel discussions, newspaper articles and parliamentary hearings. Some sections of the media, the churches and other groups attacked Br\u00fcstle \u2014 and, in turn, Br\u00fcstle appeared in newspapers and television shows to state his case. His four children had to deal with remarks from schoolmates. \u201cIt was a very stressful time,\u201d he says. Elsewhere, other countries were devising their own solutions, amid similarly polarized debates. Some conservative governments, such as Ireland's, tightly restrict human ES-cell research. Bolder ones have been more permissive: in the United Kingdom, for example, the government decided to allow new human ES-cell lines to be derived from spare embryos, and in some cases to allow human embryos to be created for research purposes. In the end, the German government fudged the decision. On 30 January 2002, it passed a law that forbade projects involving newly derived lines but allowed research on cell lines that existed before a specific date. That same year, Br\u00fcstle finally got his grant. \n               Moral challenges \n             The small victory didn't stop the debate. Some groups argued vehemently that the use of any human ES-cell lines was wrong. Br\u00fcstle became such a target of vitriol that undercover police officers had to mingle with the audience at his talks. Given the climate, Br\u00fcstle was not particularly surprised when Greenpeace challenged his patent in 2005. The organization invoked the 'ordre public' clause in the EU's Directive on the Legal Protection of Biotechnological Inventions, to which all national patent law must defer. The directive states that inventions whose commercial use could lead to a breach of morality cannot be patented, and uses the commercialization of the human embryo as an example. A year later, Br\u00fcstle arrived at the Federal Patent Court in Munich in an armoured limousine with three police escorts. The judge found in favour of Greenpeace and ordered claims for human ES-cell lines to be removed from his patent. Still, Br\u00fcstle was certain that he would be able to overturn the decision on appeal. What actually ensued, however, was a series of court setbacks and delays. When the Federal Patent Court ruled against Br\u00fcstle on appeal, he took the case to the country's Supreme Court. At a hearing in November 2009, the Supreme Court seemed to accept Br\u00fcstle's arguments, but, before making its final ruling, decided to refer the case to the European Court of Justice to clarify ambiguous points of patent law regarding the definition and potential commercial use of human embryos and stem cells derived from them. At that stage, even Greenpeace's Christoph Then told the press that he thought his case was lost. It wasn't. In March 2011, Judge Yves Bot, who was responsible for working the case, delivered his preliminary opinion for the European court. He argued that an invention that had involved destruction of an embryo would be immoral. Br\u00fcstle was still confident that the Grand Chamber, which comprises all 13 judges, would rule the other way. He had read the statements provided by the European Commission and many of the EU member states. None of them \u2014 even from the most conservative of countries \u2014 asked for patents based on human ES-cell lines to be outlawed. When the Grand Chamber ruled in October, however, there was no place for more optimism. The judges took the preliminary ruling further, making it clear that even inventions that indirectly involved destruction of an embryo were immoral and could not be patented. Br\u00fcstle walked from the courtroom into a swarm of journalists, and spent the next five hours telling them why he thought the judgment was wrong. \u201cHow can a European court rule immoral what national governments have allowed and which national research agencies have funded?\u201d He then drove the two hours home, he says, \u201ctoo exhausted to feel anything\u201d.  How can a European court rule immoral what national governments have allowed?  Elsewhere, the response to the ruling was muted. German scientific organizations remained silent \u2014 although they eventually published a joint statement condemning the judgment on 7 December. \u201cI was quite shocked that no one spoke up after the judgment,\u201d says Winnacker, who is now head of the international Human Frontier Science Programme based in Strasbourg, France. A handful of patent lawyers, companies and scientists in other countries pronounced that patents were not so important anyway and that clarity in the matter has its own advantages. \u201cIndustry also wanted clarity, even if that result was tough,\u201d says Johan Hyllner, chief executive of Cellartis in Gothenburg, Sweden, which derives cell lines from human ES-cell lines for drug discovery. \u201cNow we know how to proceed and we can always choose to keep trade secrets instead of patenting.\u201d But Br\u00fcstle and some other stem-cell scientists and legal experts view those pronouncements as an effort to play down the damage. \u201cMany were afraid that investors or funding agencies would pull out of the area,\u201d says stem-cell researcher Christine Mummery at Leiden University Medical Centre in the Netherlands. Since the ruling, scientists and funding agencies across Europe have been trying to analyse its impact. If academic scientists using human ES cells want to found a biotechnology start-up company, they'll now find it hard, admits Hyllner. \u201cVenture capitalists like to see a strong patent portfolio, and this won't be possible in Europe now.\u201d And there are hints that research organizations are getting cold feet about funding the work. On 23 November, the European Parliament's official cross-party working group on bioethics urged zero funding of human ES-cell work in the next multi-billion-euro EU framework research programme, Horizon 2020, on the grounds that the work is now legally challengeable. But a week later, Europe's research commissioner M\u00e1ire Geoghegan-Quinn said that she wanted to continue funding such research. Scientists expect a fight to ensue. Through all this, Br\u00fcstle says that colleagues using human ES cells have been supportive. No one has publicly expressed regret that Br\u00fcstle pushed his case so far. Br\u00fcstle is taking a break from battle, however. Methods for reprogramming adult cells to make stem cells capable of differentiating into many cell types look encouraging, he says, and his own lab now spends 80% of its time in this area. Last month, he published a paper 4  showing how his team reprogrammed skin cells taken from patients with Machado\u2013Joseph disease into such 'induced pluripotent' stem cells, work that helps explain how the rare disorder attacks neurons. \u201cBut we don't yet know if those induced pluripotent cells will work as well \u2014 we still need to use human ES cells in research,\u201d he says. On reflection, Br\u00fcstle says, the thing he just can't understand is that the tough decision on human ES-cell research and patenting has not been made through reasoned debate by law- and policy-makers, as it has in many other countries. \u201cIt's been made instead by a struggle between a single researcher and an environmental organization: was that quite right?\u201d \n                 See Editorial \n                 page 291 \n               \n                     European ban on stem-cell patents has a silver lining 2011-Oct-24 \n                   \n                     European court bans patents based on embryonic stem cells 2011-Oct-18 \n                   \n                     Europe rules against stem-cell patents 2011-Mar-16 \n                   \n                     European Court of Justice rejects stem-cell patents 2011-Mar-10 \n                   \n                     Fresh hope for German stem-cell patent case 2009-Nov-16 \n                   \n                     Germany eases ban on embryonic stem-cell lines 2008-Apr-16 \n                   \n                     Insight: Regenerative Medicine \n                   \n                     Directive of the European Parliament and of the Council on the legal protection of biotechnological inventions \n                   \n                     European Court of Justice Ruling \n                   \n                     Oliver Br\u00fcstle \n                   Reprints and Permissions"},
{"file_id": "480308a", "url": "https://www.nature.com/articles/480308a", "year": 2011, "authors": [{"name": "Michael Cherry"}], "parsed_as_year": "2006_or_before", "body": "South Africa is vying fiercely with Australia to host a giant radio telescope that may never be built \u2014 but the competition itself is changing the country's science landscape. The 90-kilometre drive starts in the sheep-ranching town of Carnarvon in western South Africa, and runs northwest through the semi-desert plains of the Karoo, where the only sign of human settlement is an occasional sheep farm. But as the road begins to pass north of the flat hills of the Losberg mountains, it suddenly comes upon seven huge white radio dishes. This is the Karoo Array Telescope (KAT-7), a government-funded facility sheltered by distance and the Losberg from human-generated radio interference. Commissioned just this year, KAT-7 is both an operational radio telescope and a prototype for the much more ambitious MeerKAT project, a 64-antenna array that will be the largest radio-astronomy facility in the Southern Hemisphere when it is completed at this site in 2018. But KAT-7 also symbolizes an even bigger ambition: South Africa hopes that by 2024, this remote expanse will be the centre of a radio facility spanning the continent. The Square Kilometre Array (SKA) \u2014 named after the total collecting area of its 3,000 or so antennas \u2014 would be by far the largest radio telescope ever built (see 'The biggest array'), and would need international funding to cover its estimated US$2.1-billion cost. It would allow astronomers to see how primordial gas formed the first stars and galaxies; how mysterious 'dark energy' has shaped galactic clusters; and how Earth-like planets form around young stars. It might even allow researchers to eavesdrop on the faint radio emissions of extraterrestrial life.array Or maybe not: South Africa's quest faces long odds. The most immediate hurdle is a fierce competition with Australia to host the facility. The two nations submitted their sealed bids in September to the international SKA Program Development Office at the University of Manchester, UK, and a decision is expected by March 2012. \n               Real big dish \n             A more serious question is whether the SKA will be built at all. Getting cash-strapped governments to invest in radio astronomy is a daunting prospect in the current economic climate. And even more daunting is the computational challenge: the number of calculations required to correlate the antennas' radio signals and forge them into a single image increases massively with the number of dishes. The full-scale SKA would need thousands of times more processing power than is currently available in the fastest supercomputers \u2014 a big reason why US researchers didn't rank the SKA when drawing up astronomy priorities last year 1 . \u201cIt is not clear that the technology for the required correlators is in hand, or even over the horizon,\u201d says Adam Burrows, chair of the US National Research Council\u2019s physics and astronomy board. \u201cIt just may require another decade to establish a credible financial and technological plan for such a massive undertaking.\u201d But the SKA's proponents in South Africa are undeterred. The act of competing has given the country's astronomy programme a major boost and, they argue, could do the same for science and technology across the board. \u201cBy giving the country a major opportunity to raise its level, and to inspire and train a new generation, the SKA could represent a new dawn for science in Africa,\u201d says Roy Maartens, a cosmologist at the University of the Western Cape in Cape Town. The prospect of an astronomy renaissance was a big draw for Bernard Fanaroff, the director of South Africa's SKA effort. A radio astronomer by training, Fanaroff left academia in the 1970s to pursue racial and economic justice in South Africa \u2014 first with the country's nascent trade-union movement, and then as a senior civil servant in the reconstruction and development programme of the first post-apartheid government. After leaving government in the early 2000s, Fanaroff seized on the SKA's potential for inspiration as yet another way to pursue change. International planning had been under way for a decade by that point. But the country still had a fighting chance. All of the site-selection criteria, starting with the need for a good view of the centre of our Galaxy, favoured the Southern Hemisphere. Fanaroff soon found an ally in Rob Adam, then director-general of the South Africa Department of Science and Technology. Adam wanted to address the chronic lack of maths and science skills in the black majority of the country's population \u2014 a legacy of both apartheid and a stagnant education system. And he believed that high-profile science projects such as the SKA were the best way to excite young people and draw them into research 2 . \n               Bidding war \n             With Adam's backing, Fanaroff prepared a bid for the SKA and submitted it in 2004. Bids also came in from Australia, China and Argentina. But only when the field was narrowed to South Africa and Australia in 2006 did the race really begin. Ever since, the two countries have chased the SKA with as much fervour as they strive against each other in cricket and rugby. In 2007, for example, Australia vowed to build the Australian SKA Pathfinder (ASKAP) at its proposed site near Murchison, in the arid west of the country. The 36-dish facility would demonstrate antenna designs and other technologies, and together with associated infrastructure and human-capacity development would cost an estimated US$490 million (ref.  3 ). In 2008, despite the global recession, South Africa countered with a commitment to its own pathfinder, MeerKAT. (The name refers to  meer , Afrikaans for 'more,' as well as to a famously photogenic mammal found in the Karoo.) The $275-million estimated cost of the South African scheme represents the country's largest-ever investment in a pure-science project \u2014 and a source of major irritation for researchers in more tightly funded disciplines 4 . But officials say that the project is good for economic development: not only were MeerKAT's antennas designed by Land Systems Dynamics, a subsidiary of British Aerospace Engineering based in Pretoria, but they will be manufactured and assembled mainly in South Africa. The ruling government, which was elected in 2009, is determined to protect the bid. This year, when oil company Shell applied to drill for natural gas just south of the MeerKAT site, for example, Naledi Pandor, the science and technology minister, made it clear that she would invoke the country's Astronomy Geographic Advantage Act to block interference with the array, whether it came from radio activity or seismic disturbances 5 . Pandor has also moved to resolve a major embarrassment: last year's suspension and subsequent exoneration of Phil Charles, then director of the South African Astronomical Observatory in Cape Town, who had tangled with his bosses in the National Research Foundation for reasons that are still unclear 6 . The incident left many South African astronomers mistrustful of the foundation. In October, Pandor announced that control of the country's astronomical facilities would eventually be taken from the agency and vested in a new organization. The South African and Australian bids for the SKA are being evaluated this month by a site advisory committee. The committee's recommendation will be sent to the directors of the SKA organization before its final decision. \n               Down to the wire \n             It could be a close call. Radio interference is minimal at both sites, and other observing conditions are equivalent, so the SKA directors will have to look at other criteria, such as antenna design. The technical differences boil down to a trade-off, says Tony Foley, head of science operations at MeerKAT. \u201cThe Australians can catch a larger patch of sky\u201d at any one time, he says, \u201cwhereas we are operating at higher sensitivity\u201d \u2014 meaning that the South African antennas will look farther into the Universe with a shorter observation time. Another factor will be the strength of the host country's astronomy community. Australia has a longer and stronger tradition of radio astronomy, but South Africa has poured money into closing the gap. The country's roster of radio astronomers currently numbers about 65, many of whom have moved from elsewhere during the past decade. Then there is what some see as the moral imperative. \u201cThere is a need for the first world to help the third world build up science,\u201d says George Ellis, a cosmologist at the University of Cape Town. \u201cThis is an ideal opportunity.\u201d How the international SKA organization will weigh these factors is hard to predict. But astronomy isn't rugby; it could be that both sides will win. In July, recognizing the bleak prospects for international funding and the computational challenges, European astronomers proposed a cheaper alternative: fund modest expansions of both ASKAP and MeerKAT, then combine their signals as if they were a single instrument. The combined array wouldn't have nearly the same sensitivity to very faint signals as the full-scale SKA. But it may well be the most realistic option. Win, lose or draw, the competition has already given South Africa a symbol of national unity. From government officials to cab drivers, everyone there seems to know about the SKA. The Department of Science and Technology sponsors a monthly performing-arts festival in support of the bid. And South Africa's financial daily,  Business Day , often runs front-page advertisements sponsored by the SKA project, featuring a full-colour photo of the KAT-7 antennas with the boastful headline: \u201cSouth Africa is ready to host the SKA.\u201d \n                     Australia: Astronomy in the outback 2011-Aug-10 \n                   \n                     South African astronomer reinstated amid recriminations 2010-Mar-17 \n                   \n                     South African science: black, white and grey 2010-Feb-10 \n                   \n                     Star of the south 2005-Nov-02 \n                   \n                     The international SKA organization \n                   \n                     South Africa's KAT-7, MeerKAT and SKA projects \n                   \n                     The Australian SKA project \n                   Reprints and Permissions"},
{"file_id": "480168a", "url": "https://www.nature.com/articles/480168a", "year": 2011, "authors": [{"name": "Ed Yong"}], "parsed_as_year": "2006_or_before", "body": "Henrik Ehrsson uses mannequins, rubber arms and virtual reality to create body illusions, all in the name of neuroscience. It is not every day that you are separated from your body and then stabbed in the chest with a kitchen knife. But such experiences are routine in the lab of Henrik Ehrsson, a neuroscientist at the Karolinska Institute in Stockholm, who uses illusions to probe, stretch and displace people's sense of self. Today, using little more than a video camera, goggles and two sticks, he has convinced me that I am floating a few metres behind my own body. As I see a knife plunging towards my virtual chest, I flinch. Two electrodes on my fingers record the sweat that automatically erupts on my skin, and a nearby laptop plots my spiking fear on a graph. Out-of-body experiences are just part of Ehrsson's repertoire. He has convinced people that they have swapped bodies with another person 1 , gained a third arm 2 , shrunk to the size of a doll or grown to giant proportions 3 . The storeroom in his lab is stuffed with mannequins of various sizes, disembodied dolls' heads, fake hands, cameras, knives and hammers. It looks like a serial killer's basement. \u201cThe other neuroscientists think we're a little crazy,\u201d Ehrsson admits. But Ehrsson's unorthodox apparatus amount to more than cheap trickery. They are part of his quest to understand how people come to experience a sense of self, located within their own bodies. The feeling of body ownership is so ingrained that few people ever think about it \u2014 and those scientists and philosophers who do have assumed that it was unassailable. \u201cG. E. Moore said that if there's something you can be certain of in this world, it's that your hand is your hand,\u201d says Ehrsson. Yet Ehrsson's illusions have shown that such certainties, built on a lifetime of experience, can be disrupted with just ten seconds of visual and tactile deception. This surprising malleability suggests that the brain continuously constructs its feeling of body ownership using information from the senses \u2014 a finding that has earned Ehrsson publications in  Science  and other top journals, along with the attention of other neuroscientists. \u201cA lot of people thought the sense of self was hard-wired, but it's not at all. It can be changed very quickly, and that's very intriguing,\u201d says Miguel Nicolelis, a neurobiologist at Duke University Medical Center in Durham, North Carolina. Ehrsson's work also intrigues neuroscientists and philosophers because it turns a slippery, metaphysical construct \u2014 the self \u2014 into something that scientists can dissect. \u201cWe can say if we wobble the signals this way, our conscious experience wobbles in this way,\u201d says David Eagleman, a neuroscientist who studies perception at Baylor College of Medicine in Houston, Texas. \u201cThat's a lever we didn't have before.\u201d \u201cThere are things like selfhood that people think cannot be touched by the hard sciences,\u201d says Thomas Metzinger, director of the Theoretical Philosophy Group at Johannes Gutenberg University of Mainz, Germany. \u201cThey are now demonstrably tractable. That's what I think is valuable about Henrik's contribution.\u201d \n               Daydream believer \n             Ehrsson was born in 1972 in a suburb of Stockholm. His father was a chemist, his grandfather a dentist, and both stoked an interest in science and the human body that led him to study medicine at the Karolinska Institute. But in long anatomy classes, Ehrsson often found himself bored. \u201cDuring lectures, I kept on thinking that if my eyes were floating over there and I was looking at myself from that perspective, where would my consciousness be?\u201d After a pause: \u201cI was not an A student.\u201d After graduating, Ehrsson left medicine to start a PhD at the Karolinska, using brain scanners to study how people grasp objects. At the same time, he was becoming deeply fascinated by physical illusions. Some of these are well established: Aristotle discovered that crossing the index and middle fingers and touching the nose can \u2014 in some people \u2014 create a feeling of having two noses. Ehrsson also heard about the rubber-hand illusion, a trick devised in the late 1990s by researchers in the United States. They could convince people that a fake hand was their own by hiding their real hand under a table, placing a rubber one in front of them, and stroking both in the same way 4 . \u201cIt really worked on me,\u201d Ehrsson says. \u201cIt was fantastic and surreal.\u201d Ehrsson started investigating illusions on the side. By the time he completed his postdoc at University College London and returned to the Karolinska Institute to start his own lab, illusions had become his main focus. He knew that many scientists use visual illusions to glean the basics of perception. \u201cThere are conferences on visual illusions, and yearly contests to create the best new ones,\u201d he says. \u201cBut there aren't many body illusions. The body has never been a main focus of psychology.\u201d It was these tricks, such as the rubber-hand illusion, that Ehrsson was keen to explore. He wanted to test how easily the sense of body ownership could be distorted. Ehrsson set about creating more illusions based on the same principles as the rubber hand one. Headsets, cameras or fake body parts fooled the eyes, and synchronous strokes and prods added a tactile clincher. In 2007, he reported that he had used such props to convince subjects that they had left their own bodies 5  \u2014 a stunt that attracted headlines around the world. At the time, some scientists and members of the public were openly sceptical that the illusion really worked. But on a trip to Ehrsson's lab this September, I was convinced. The goggles I wore displayed the view from a camera pointing at my back (see 'Out-of-body experience'). Ehrsson tapped my chest with one plastic rod while using a second one to synchronously prod at the camera. I saw and felt my chest being prodded at the same time as I saw a picture of myself from behind. Within ten seconds, I felt as if I was being pulled out of my real body and was floating several feet behind it. A year after removing his subjects from their own bodies, Ehrsson learned how to trick them into acquiring new ones. This time, the volunteers' goggles showed them the view from a camera on the head of a mannequin looking at its own plastic torso. Simultaneously poking the arm or stomach of the mannequin and the volunteer a few times was enough to convince the subjects that they were the dummy. They could even stare at their old bodies from their new ones and shake hands with their old self, all without breaking the spell 1 . \u201cIt really is very intense and incredibly fast,\u201d says Mark Hallett, a neurologist from the National Institutes of Health in Bethesda, Maryland, who experienced it first hand. In his latest trick, published in May 3 , Ehrsson convinced people that they had jumped into a tiny Barbie doll. When he prodded the doll's legs, the volunteers thought they were being prodded by giant objects. And when Ehrsson tested the illusion on himself and a colleague touched his cheek, he says, he looked up and \u201cfelt as if I was back in my childhood and looking at my mother\u201d. Not everyone succumbs. Ehrsson suspects that people who can expertly localize their limbs without sight, such as dancers or musicians, would be less susceptible than the students with whom he normally works. But typically, the illusions work for around four out of five people. Ehrsson confirms the effects by asking his volunteers about their experiences and by threatening their disembodied, shrunken or plastic forms with a knife. If the illusion has worked, then volunteers show a reflexive burst of nervous sweating (as I did). Even knowing the knife is coming is no defence: as Ehrsson puts it, the illusions are \u201ccognitively impregnable\u201d. Earlier this year, Ehrsson even tweaked the rubber-hand illusion and convinced people they owned a third hand 2 . \u201cHe has taken those basic ideas and investigated how far you can push them,\u201d says neuroscientist Matthew Botvinick, one of the creators of the rubber-hand illusion, at Princeton University in New Jersey. \u201cHe's shown how extreme and how malleable the body representation can be.\u201d \n               Self delusion \n             Ehrsson's next challenge is to work out what these illusions reveal about the brain. According to textbook wisdom, people build up a perception of their bodies using 'proprioception' \u2014 signals from the skin, muscles and joints that indicate the relative position of body parts. But Ehrsson's illusions show that vision and touch are also a crucial part of the mix, and that the brain builds a sense of self by constantly compiling information from all these senses. Proprioception may be telling the brain that the body is seated in a chair, but the carefully timed vision and touch signals in Ehrsson's illusion convince the brain that it is somewhere else entirely. Ehrsson thinks such illusions depend on 'multisensory' neurons, which have been studied mostly in monkeys and allow the animals to successfully interact with objects by combining vision and touch. \u201cWe think these circuits are important, not just for representing external objects,\u201d says Ehrsson, \u201cbut representing your own body and the boundary between your body and the world.\u201d He thinks that the neurons integrate information from across the senses to create a cohesive representation of the body. During an illusion, he simply changes the data flooding into the neurons to manipulate that representation. For now, this is a working hypothesis. \u201cThe details of the multisensory integration have never been formally worked out,\u201d says Botvinick. \u201cThat's the missing link for me.\u201d Ehrsson and others are busy trying to see where these multisensory neurons reside in the human brain, by inducing illusions while volunteers sit inside a functional magnetic resonance imaging machine. The results differ. Ehrsson has found that the ventral premotor cortex \u2014 known to be involved in visual guidance of movements \u2014 is particularly active when people experience the full body-swap illusion 6 . Olaf Blanke at the University of Lausanne in Switzerland, one of only a handful of other researchers in this field, has shown that the nearby temporoparietal junction lights up when people experience out-of-body illusions 7 . He points out that brain damage or tumours in that area can induce disembodied feelings. \u201cIt's difficult to judge which is right, because we just have a thin neuroscientific layer of data at the moment,\u201d says Blanke. \n               Soul searching \n             Ehrsson's interests in the self and body ownership bleed into the rest of his life. He is drawn towards experimental theatre and surrealist art. He comes across as thoughtful and remote, easily forgetting the names of colleagues and often lapsing into silence. He prefers working in his lab to going to conferences, he says. He also occasionally gets angry letters from people who have had out-of-body experiences themselves. \u201cThey believe that their souls have left their bodies, and they feel threatened that a similar experience can be induced in a lab,\u201d says Ehrsson. He offers a diplomatic response, saying that he has \u201cno way of disproving their ideas\u201d. Metzinger is more forthright. \u201cHenrik's work speaks to the idea that there is no such thing as a soul or a self that's independent of the brain,\u201d he says. Ehrsson is now working to turn his illusions towards a practical goal: developing better artificial limbs. Many people who have lost arms still feel the presence of their missing limbs, yet replacement hands feel alien. \u201cWe think that people might find it easier to use their prostheses, or develop a better body image, if we can elicit the illusion of real ownership,\u201d says Ehrsson. To achieve this, he has adapted the rubber-hand illusion. With amputees, Ehrsson stimulates small spots on the stump that trigger the feeling of phantom fingers. By stroking these spots in time with the corresponding parts of a robotic 'cyberhand', Ehrsson and his colleagues have successfully convinced amputees that they owned the metal limb 8 . The effect disappeared about 10 to 15 seconds after the stroking stopped, though, so the stimulation would have to be continuous to keep the illusion going. That is what Ehrsson is working towards now. \u201cThe idea is to have an advanced prosthetic hand with sensors in the fingertips and stimulators in the stump,\u201d he says. Although other groups are working on similar sensory devices, Ehrsson says that his is different because he is attempting to precisely match up the feelings from the artificial fingers with stimulation in the corresponding part of the stump to generate the illusion of ownership. Ehrsson has even bigger ambitions. Ownership illusions could help people to take control of entire alien bodies, both virtual and robotic, in a way that would afford a finer degree of control than the joysticks and other controllers used to steer robots and avatars today. People who control robots, for example, would see from their machine's perspective using goggles, steer using motion-capture suits and receive tactile feedback from gloves connected to sensors in the robot's hands. As long as signals are transferred between human and machine within 100 milliseconds, Ehrsson predicts that \u201cthe full-body illusion will kick in\u201d. And size should not matter. A surgeon could control a microscopic robot in a patient's body. Giant robots could repair broken oil rigs or fractured nuclear power stations. Ehrsson smiles as he considers the possibilities. There is one illusion that even Ehrsson is unsure if he can pull off: splitting the sense of self, so that a person can commandeer two bodies. Ehrsson thinks that the trick's feasibility may depend on how the brain integrates information from the senses. \u201cMaybe it's statistical in nature. Maybe instead of using absolute coordinates, the brain says it's most likely that my hand is here,\u201d he says. \u201cIf it works like that, maybe you could trick the brain into thinking that this body and that body are equally likely to be me.\u201d It sounds far-fetched, but so does most of what Ehrsson has achieved so far. \u201cWe're working on it,\u201d he says. \u201cThen again, it might be impossible.\u201d \n                     Neuroscience vs philosophy: Taking aim at free will 2011-Oct-01 \n                   \n                     Neuroscience vs philosophy: Taking aim at free will 2011-Aug-31 \n                   \n                     Neuroscience: Opening up brain surgery 2009-Oct-14 \n                   \n                     Illusion mimics out-of-body experiences 2007-Aug-23 \n                   \n                     Mind trick 'whittles the waist' 2005-Nov-29 \n                   \n                     Henrik Ehrsson's lab \n                   Reprints and Permissions"},
{"file_id": "480166a", "url": "https://www.nature.com/articles/480166a", "year": 2011, "authors": [{"name": "Daniel Cressey"}], "parsed_as_year": "2006_or_before", "body": "Nations are racing to establish marine protected areas, but it's not clear whether many are living up to the name. Former US President George W. Bush did not garner much applause from environmentalists during his eight years in the White House, but on 15 June 2006, he gave them something to cheer about. Bush signed an order to create the Papah\u0101naumoku\u0101kea Marine National Monument in Hawaii, then the world's largest ocean conservation area. Spanning about 360,000 square kilometres of the Pacific Ocean, the reserve is designed to safeguard 7,000 marine species including the rare Hawaiian monk seal ( Monachus schauinslandi ), hawksbill ( Eretmochelys imbricata ) and leatherback turtles ( Dermochelys coriacea ) and nearly two dozen other animals on the US endangered species list. It also started a race between other nations to follow suit with giant reserves of their own. Since then, Australia, the United Kingdom and the Pacific nation of Kiribati have created still larger ocean conservation zones. And nations are discussing plans to make other huge reserves that will encompass much of the Southern Ocean and the Sargasso Sea. Worldwide, about 4 million square kilometres have now been set aside in marine protected areas (MPAs). Despite that rapid expansion, many of the MPAs fall well short of the grand plans that governments once trumpeted to protect the world's oceans and the species that live in them. The 2002 Convention on Biological Diversity called on the world to protect 10% of the oceans by 2012, but the MPAs created so far cover little more than 1%. Studies show that some of the MPAs, such as Australia's Great Barrier Reef Marine Park, have provided strong protection for crucial ecosystems 1 . But scientists fret that many MPAs exist only on paper and offer no real benefits to the ecosystems or the people within or near them. Some have design flaws, such as protecting the wrong habitats. In others, there are no efforts to enforce limits on fishing or other activities within the protected areas. \u201cThe biggest thing we can do wrong is to claim to be protecting, when in fact the design of the place or the nature of the management is such that there is no effective protection,\u201d says Peter Sale, a marine scientist and professor emeritus at Canada's University of Windsor in Port Carling, Ontario. These problems are likely to grow as countries increasingly join forces to develop larger and more expensive international protected areas on the high seas. Enforcement and monitoring will be more difficult in these places than they are in coastal waters. Some scientists worry that these concerns are not being openly discussed. \u201cThere's almost a religion, which is nurtured by non-governmental organizations and some members of the science community, to the extent that critical examination of MPAs is exceedingly difficult to get published,\u201d says Sale. \n               A sea change \n             The term MPA can cover a vast range of entities, from tiny reserves less than a square kilometre in size to Britain's 544,000-square-kilometre Chagos Marine Protected Area, which wraps around 55 small islands in the Indian Ocean (see 'Fencing the sea'). 'No-take' MPAs ban fishing outright, but others permit certain types of fishing in specific zones. One of the forces driving the expansion of larger MPAs has been the Global Oceans Legacy programme of the Pew Environment Group in Washington DC. In the mid-2000s, Pew identified a handful of sites as suitable for mega-MPAs because they had relatively low economic value and were owned by countries that had a history of conservation and the ability to enforce protections. The United States has established MPAs in two of these areas: the Papah\u0101naumoku\u0101kea protected zone and one in the Mariana Trench in the western Pacific. In late November, Australia proposed plans for a 990,000-square-kilometre reserve in the Coral Sea off the country's northeast coast. Another of the sites highlighted by Pew was New Zealand's Kermadec Islands, where protections are currently in place for the sea floor, and conservationists would like to extend them to the entire water column. Jay Nelson, director of the global ocean legacy programme at Pew, sees the Papah\u0101naumoku\u0101kea MPA as the marine equivalent of Yellowstone National Park, the first big parcel of land to be set aside as a reserve. The creation of that park in 1872, he says, set off a chain reaction, prompting other countries to establish large parks of their own. \u201cThat had a similar effect to what Papah\u0101naumoku\u0101kea has had in the oceans.\u201d Countries have picked up the pace of establishing reserves as the deadline approaches for the 2012 target. In 2008, Daniel Pauly, a fisheries biologist at the University of British Columbia in Vancouver, Canada, estimated that 2.35 million square kilometres of ocean had been protected 2 . In late 2010, a report from the United Nations Environment Programme (UNEP) and the International Union for Conservation of Nature and Nature Conservancy charities put the area at 4 million square kilometres, with the number of MPAs nearing 5,900. That area represents just 1.17% of the world's seas. So the deadline for reaching 10% was extended to 2020 at last year's meeting of countries that have signed up to the UN Convention on Biological Diversity. The recent spike in protected areas has Pauly feeling hopeful, despite the failure to meet the 2012 target. \u201cMaybe vast areas of the ocean will be put under protection in time to prevent bad things from happening,\u201d he says. \n               Reserves work \n             Scientists and conservationists agree that a good MPA can make a difference. In 2009, Sarah Lester, a marine ecologist at the University of California, Santa Barbara, and her colleagues examined published peer-reviewed reports on 124 no-take marine reserves in 29 countries 3 . They found documented increases in characteristics such as biomass, species richness and population size within the boundaries of the reserves. \u201cBy and large, everyone agrees that MPAs can achieve their conservation goals if they are enforced,\u201d says Benjamin Halpern, a co-author on the study and director of the Center for Marine Assessment and Planning at Santa Barbara. But many MPAs are worth little more than the paper they are written on because nothing is done to enforce the rules or monitor the area. Earlier this year, Peter Jones, an MPA researcher at University College London, and his colleagues produced a report 1  for UNEP analysing the governance of 20 MPAs, including those in iconic regions such as the Great Barrier Reef and the Galapagos. The team found that none of the MPAs rated more than three out of five points for their effectiveness at meeting key objectives, such as preserving biodiversity and fostering sustainable use of resources. \u201cThree of the 20 MPAs we've said had no effectiveness whatsoever \u2014 completely paper parks,\u201d Jones says. Of these, Baleie Franca in Brazil and the Cres-Lo\u0161inj reserve in Croatia scored one. Pirajuba\u00e9 in Brazil scored zero, in part because the government approved a highway in the area that harmed coastal habitats and fishing grounds, the report says. Even countries that do try to regulate their MPAs can find it hard to fund them. \u201cWe don't have the resources that we need to actually monitor, enforce and understand these areas,\u201d says Jane Lubchenco, head of the US National Oceanic and Atmospheric Administration. Poor planning can also hamper an MPA. Mexico, for example, set aside parts of the Gulf of California to protect the endangered vaquita \u2014  Phocoena sinus , a type of porpoise \u2014 but did not include a large part of the species' range. The Australian government has also been criticized over its efforts at marine zoning. In August, a group of 173 scientists published an open letter warning that a proposed 320,000-square-kilometre reserve off the southwestern coast \u201cfails on the most basic test\u201d by not protecting a representative selection of habitats in the region. Some suggest that Australia's proposal for a Coral Sea MPA faces similar issues (see Nature 480, 14\u201315; 2011). In an e-mail to  Nature  about the southwestern reserve, a spokesperson from the Australian Department of Sustainability, Environment, Water, Population and Communities said that the government has a responsibility to balance social and economic factors along with ecological ones in decisions about conservation. \u201cThe department does not accept that there is a lack of protection for valuable habitat in the proposed marine reserves network,\u201d said the spokesperson. Researchers say that there are ways to improve reserve planning. Many nations are now setting up MPAs using a software called Marxan, which uses an algorithm to calculate the areas that should be included in order to protect the most important habitat while minimizing the cost of the reserve. One of the driving forces behind the software, Hugh Possingham, head of the Spatial Ecology Lab at the University of Queensland in Brisbane, Australia, says that the best approach basically amounts to saying \u201cget me 10% of everything and annoy as few people as possible\u201d. Australia used Marxan to design the Great Barrier Reef MPA, which sets aside large fractions of some 70 types of habitat. Looking forward, researchers want to investigate how best to monitor and manage the increasing numbers of MPAs. As they assess the effectiveness of reserves, scientists sometimes find surprises, such as the fate of the monk seals in Hawaii's Papah\u0101naumoku\u0101kea MPA. The seal population has increased outside the protected region but declined inside it, perhaps because sharks have thrived there 4 . Ashley McCrea Strub, a postdoctoral fellow working with Pauly's lab, says that there are very few data on the cost effectiveness of reserves and it will be vital to fill that gap. \u201cThat's really the future of where we need to go,\u201d she says. \u201cThat's an enormously important question.\u201d Some scientists have argued for doing more studies on MPAs before moving forward with major plans for new ones. But Possingham says the benefits in most cases are clear enough. \u201cIf you wait till you have perfect data,\u201d he says, \u201cthen you'll be waiting forever.\u201d \n                 See Editorial \n                 page 151 \n               \n                     Australia\u2019s marine plans questioned Tue Nov 29 00:00:00 EST 2011 \n                   \n                     Seals slide towards extinction in Hawaiian reserve Mon Sep 12 00:00:00 EDT 2011 \n                   \n                     Little Mexican reserve boasts big recovery Fri Aug 12 00:00:00 EDT 2011 \n                   \n                     Marine protection goes large Mon May 16 00:00:00 EDT 2011 \n                   \n                     Plans for marine protection highlight science gap Mon Jan 10 00:00:00 EST 2011 \n                   \n                     Endangered-porpoise numbers fall to just 250 Tue Jun 08 00:00:00 EDT 2010 \n                   \n                     World Database on Protected Areas Tue Jun 08 00:00:00 EDT 2010 \n                   \n                     Protected Planet Tue Jun 08 00:00:00 EDT 2010 \n                   \n                     Papah\u0101naumoku\u0101kea Marine National Monument Tue Jun 08 00:00:00 EDT 2010 \n                   \n                     Great Barrier Reef Marine Park Tue Jun 08 00:00:00 EDT 2010 \n                   Reprints and Permissions"},
{"file_id": "478444a", "url": "https://www.nature.com/articles/478444a", "year": 2011, "authors": [{"name": "Ewen Callaway"}], "parsed_as_year": "2006_or_before", "body": "The genome of a 660-year-old bacterium is revealing secrets from one of Europe's darkest chapters. s word of a brutal pestilence raging across Europe reached London, its residents started digging. In 1348, Ralph Stratford, Bishop of London, dedicated acres of land that had been purchased to bury the legions of Black Death victims who would overwhelm existing churchyard cemeteries. Within two years, one-third to one-half of the city's 40,000\u2013100,000 residents succumbed, and many thousands were buried in two newly dug cemeteries at East and West Smithfield. At the height of the scourge, 200 bodies were interred each day. East Smithfield, originally called the Churchyard of the Holy Trinity, is one of a handful of burial sites known to have been used only during the Black Death. In the 1980s, excavation of this 'plague pit' turned up nearly a third of the 2,400 bodies estimated to be buried there, some piled five deep. Despite the urgency of the time, the bodies were placed purposefully, oriented east to west, some with charcoal, possibly to absorb the fluids released during putrefaction, and many with coins and trinkets of their former lives. Such foresight not only helped keep corpses from piling up in the streets, but also, it seems, afforded some Black Death victims a dignified Christian burial. Six-and-a-half centuries later, it would also give scientists the opportunity to dissect the disease that laid waste to Europe (see \u2018Death on the march\u2019). \n               boxed-text \n             This month, geneticists reported that they have reconstructed the genome of  Yersinia pestis , the bacterium that causes bubonic plague, recovered from remains at East Smithfield 1 . The sequence \u2014 the first from an ancient bacterial pathogen \u2014 may help to explain how a disease could wreak so much havoc. It also marks a renaissance in genetic studies of ancient diseases, a field that has suffered a controversial history but that is now being revitalized. \"There will be a race now for all the ancient pathogens,\" says Hendrik Poinar, a palaeogeneticist at McMaster University in Hamilton, Canada, who co-led the sequencing efforts. \n               Plagued with disbelief \n             When Alexandre Yersin linked  Y.\u00a0pestis   to bubonic plague in 1894, many scientists surmised that the pathogen was behind not only the Black Death, but also a spate of earlier mass die-offs. The sixth-century Justinian plague devastated Constantinople and killed millions in Europe and the Near East. Plagues reared their heads periodically for the next two centuries. Black Death itself reappeared several times, even into the nineteenth century. Clues tying  Y.\u00a0pestis   to these outbreaks came largely from historical accounts of their symptoms, such as Giovanni Boccaccio's description of the Black Death in  The Decameron , written around 1350: \"It first betrayed itself by the emergence of certain tumours in the groin or the armpits, some of which grew as large as a common apple, others as an egg.\" But some modern historians and scientists came to doubt that  Y.\u00a0pestis   caused these ancient outbreaks. Bubonic plague epidemics known to have been caused by  Y.\u00a0pestis   in the past century seemed too mild to have been caused by the same culprit as the Black Death: they killed fewer people and spread more slowly. Some 'plague revisionists' have argued that fleas, which spread  Y.\u00a0pestis   to humans, would have struggled to survive the cold temperatures reported during the Black Death. And there was the speed with which it killed \u2014 Boccaccio reported that death often occurred within three days of the first symptoms appearing. Anthrax or a haemorrhagic-fever-causing virus similar to Ebola would be more likely than plague to cause such a rapid demise, say critics. DNA evidence would seem to offer a definitive answer. In 2000, a team led by Didier Raoult, a microbiologist at the University of the Mediterranean in Marseilles, France, said it had proved the link between the bacterium and the disease. The researchers reported that they had successfully recovered  Y.\u00a0pestis   DNA from the teeth of a child and two adults dug up from a fourteenth-century mass burial site in Montpellier. The team identified the bacterium using a sensitive technique called the polymerase chain reaction (PCR) to amplify a portion of a gene from  Y.\u00a0pestis   called  pla . \"We believe that we can end the controversy,\" the team wrote 2 . \"Medieval Black Death was plague.\" But several critics raised concerns about contamination. The PCR might instead have amplified DNA from modern  Y.\u00a0pestis   used previously in the lab, or possibly the sequences from a closely related soil-dwelling bacterium. \"I could never, ever replicate it,\" says Thomas Gilbert, an evolutionary geneticist at the University of Copenhagen in Denmark. In 2004, Gilbert and his colleagues reported no trace of  Y. pestis   DNA in 108 teeth from 61 individuals found in plague pits in France, Denmark and England (including East Smithfield) 3 . Raoult says that there was no contamination and that Gilbert's methods did not accurately replicate his 4 . Still, those who were already sceptical of the suggestion that  Y. pestis   caused the Black Death latched on to Gilbert's study. Other studies of microbial DNA extracted from ancient human remains \u2014 including those affected by tuberculosis, syphilis and malaria \u2014 were also being scrutinized. In several cases, researchers could not replicate results, or they found methodological shortcomings. Critics said that DNA from these samples was too degraded by heat, moisture and time to detect, and the field soon divided into believers and sceptics. \"There was a complete schism,\" says Ian Barnes, a palaeogeneticist at Royal Holloway University of London, who says he spent two-and-a-half years trying \u2014 unsuccessfully \u2014 to find DNA evidence of syphilis or tuberculosis in bones dating from the nineteenth and early twentieth centuries 5 . \"People largely ignored each other,\" he says. \n               Digging up answers \n             Although Poinar was dubious of claims about ancient microbial DNA, he was intrigued by the bones from East Smithfield. Nearly all of the remains are from Black Death victims, many of whom were cut down during the prime of their lives. In a bright ground-floor laboratory of the Museum of London, a short walk from East Smithfield, osteoarchaeologist Jelena Bekvalac examines the nearly complete skeleton of one of the plague pit's former residents. Wearing a black silk scarf dotted with white skull-prints, Bekvalac handles a pelvic bone and determines that it belonged to a man who died in his late teens or early twenties. Apart from some plaque on his teeth and a gash in his skull that shows some signs of healing, the man's skeleton offers no outward evidence of Black Death. His remains, and those from hundreds of others, represent a snapshot of life and death in London during the epidemic. Since the site's excavation, researchers have descended on the bones in search of information. In the late 1990s, Poinar met Sharon DeWitte, then a graduate student at Pennsylvania State University in State College, who was working on a demographic analysis of the remains suggesting that Black Death preferentially killed those who were already frail. The two considered drilling into teeth and bones to find  Y. pestis   DNA, but Poinar wasn't satisfied with the available detection tools, which were still based on PCR. \"We sort of sat on the samples for a few years waiting for all the stars to align,\" says DeWitte, now at the University of South Carolina in Columbia. That alignment came from next-generation DNA sequencers, machines that read short snippets of DNA. The technology was perfect for sequencing DNA that has been damaged by spending hundreds of years underground. The sequencers allowed Svante P\u00e4\u00e4bo, a palaeogeneticist at the Max Planck Institute for Evolutionary Anthropology in Leipzig, Germany, and his team to sequence a draft of the Neanderthal genome 6 . But finding and sequencing ancient pathogens in a human skeleton is much harder \u2014 like finding \"needles in the football field\", Poinar says \u2014 because their genomes are 1,000 times shorter than that of the Neanderthal and closely resemble those of soil microbes that have infiltrated the bones. Another technology helped narrow the search. P\u00e4\u00e4bo and his team developed a technique, called targeted capture, in which they used lab-synthesized 'bait' DNA to snag ancient DNA strands from a bone sample 7 , leaving soil-microbe and other sequences behind. \"It's pretty much like fishing in a pond,\" says Johannes Krause, a palaeogeneticist at the University of T\u00fcbingen in Germany, who worked with P\u00e4\u00e4bo on the Neanderthal genome and co-led the Black Death project with Poinar. In a proof-of-principle experiment published in August of this year, Krause and Poinar's team used sequences from a contemporary plague strain to fish out  Y.\u00a0pestis   DNA from the teeth of victims buried at East Smithfield. From this, they sequenced a short loop of DNA, called the pPCP1 plasmid, that is partially responsible for bubonic plague's ability to infect humans. Their results 8 , along with a paper published last year 9  that found  Y.\u00a0pestis   sequences in different Black Death bone samples, have convinced most scientists that bubonic plague was involved in the Black Death. In their most recent paper 1 , Poinar and Krause completed the ancient genome and showed that it sits at the root of an evolutionary tree that comprises 17 contemporary strains of  Y.\u00a0pestis . This indicates that the Black Death strain spawned many of the forms of  Y.\u00a0pestis   that infect humans today. This strain, Krause adds, probably emerged not long before the Black Death started its rampage across western Asia and Europe in the fourteenth century. \"That, for me, was the biggest surprise,\" he says. It suggests, the authors argue, that earlier plagues were caused by either a now-extinct strain of  Y.\u00a0pestis   or by an entirely different pathogen. Mark Achtman, a plague-evolution expert at University College Cork in Ireland, calls this interpretation \"absolute nonsense\". Krause and Poinar's team did not consider a number of modern plague strains found in central and east Asia, which are thought to have earlier origins than the East Smithfield strain, Achtman says. Genome sequences for these strains were not available to his team, says Krause, but he is eager to see how they are related. \n               Mysterious scourge \n             Just as puzzling, however, is that  Y.\u00a0pestis   seems to have changed very little over the past 660 years. The genome of the Black Death strain differs from that of the modern  Y.\u00a0pestis   'reference' strain by about 100 nucleotides, but each of these genetic differences can be found in at least one contemporary strain. \"We can't find anything that makes the Black Death special,\" Krause says. The team is now looking for other genetic changes that could account for the Black Death's ferocity, such as rearrangements in the genome, which are difficult to determine from the short fragments of DNA available. To better understand how the plague worked, researchers could try to resurrect the Black Death pathogen by modifying the genomes of contemporary  Y. pestis   strains. Although this might sound alarming, research on  Y. pestis   is already carefully controlled, and even an accidental infection with such a strain could be easily treated with modern antibiotics. Moreover, Poinar says, the Black Death was not just about the bacterium. Environmental and epidemiological factors must have aided in its vicious tear through Europe. Sick soldiers returning to Europe from Caffa, the Black Sea port that was the plague's gateway from Asia, unleashed the disease on a population that would have been weakened by malnourishment and years of cold, wet weather, he says. Achtman says that it is possible that Black Death was not spread by rat-dwelling fleas, as  Y.\u00a0pestis   is today, but by other animals, which could have enhanced transmission. Or another circulating pathogen could have contributed, as in the 'Spanish flu' pandemic that killed up to 100 million people worldwide in 1918\u201319, often with the help of bacterial pneumonia. Whatever questions remain about the Black Death, scientists are now keen to apply the latest sequencing methods to other ancient epidemics. \"I've completely gone from thinking, 'ancient pathogens are a load of crap,' to 'hold on, maybe some of this stuff works',\" says Gilbert, whose team has started to sequence DNA from pathogens that plagued ancient crops. Researchers could identify ancient microbes and chart their spread and their evolutionary relationships with contemporary strains. For example, Europeans who travelled to the New World may have introduced new forms of tuberculosis to North America and brought syphilis back to Europe. Ancient pathogens may help scientists understand current and future outbreaks, says Terry Brown, a biomolecular archaeologist at the University of Manchester, UK. He and Charlotte Roberts, of Durham University, UK, are charting the evolution of tuberculosis strains in Britain and Europe. \"By looking over the past 1,000 years of disease in British cities, we can understand problems occurring in the Third World, where more and more people are crowding into cities,\" he says. Similarly, the sequencing and resurrection of the influenza strain responsible for the 1918 pandemic 10  has helped researchers to interpret the sequences of contemporary flu strains. For all its ferocity, the Black Death left few visible marks on London. Today, the plague pit at East Smithfield is in the heart of London's financial district, buried under modern office suites and the old Royal Mint building. The only visible remnants are the crumbled ruins of St Mary Graces, a Cistercian abbey built near the site in 1350. London may have seen its last significant bubonic plague outbreak, but catastrophic epidemics are a rule of human history, not an exception. Centuries from now, what traces will the next great scourge leave? Future archaeologists chronicling its history may find memorials, graves and probably even the bodies of victims. But another story will also lurk in its DNA, just waiting to be read. Ewen Callaway writes for Nature from London. \n                     Ancient DNA: Curse of the Pharaoh's DNA 2011-Apr-27 \n                   \n                     European and Asian genomes have traces of Neanderthal 2010-May-06 \n                   \n                     King Tut's death explained? 2010-Feb-16 \n                   \n                     Did Black Death boost HIV immunity in Europe? 2005-Mar-11 \n                   \n                     Black Death's DNA 2001-Oct-04 \n                   \n                     Pied piper a plague risk? 2000-Oct-19 \n                   \n                     Nature video on the Black Death genome \n                   \n                     Johannes Krause \n                   \n                     Hendrik Poinar \n                   Reprints and Permissions"},
{"file_id": "478442a", "url": "https://www.nature.com/articles/478442a", "year": 2011, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Asking parents to donate a child's brain to research is emotionally fraught. Some researchers say that it is time to put aside the taboos. David Amaral wanted to watch the young brain take shape. He thought that studying post-mortem brains under the microscope would help him to work out why children with autism often have abnormalities in the key structures that drive emotion and behaviour. But he soon found that existing brain banks couldn't give him what he needed. \"It's just too hard to get high-quality tissue,\" he says. The banks may contain hundreds or even thousands of brains \u2014 but not from children, and not necessarily in the best condition. Amaral, who is director of research at the MIND (Medical Investigation of Neurodevelopmental Disorders) Institute at the University of California, Davis, is not the only scientist eager for access to brains from children. The crucial stages of brain development span early fetal life through to the end of the teenage years; and destructive neurodevelopmental disorders such as autism and schizophrenia are thought to arise partly because of faulty connections laid down during this time. Many researchers want to apply new technologies, including increasingly sensitive molecular analyses and ever smarter microscopy, to developing brains to create a dynamic picture of what goes wrong. When they succeed, the results can be breathtaking, says neuropathologist Joel Kleinman at the National Institute of Mental Health (NIMH) in Bethesda, Maryland. In work reported in this week's  Nature 1 , he and his colleagues applied genomic technologies to 269 brains spanning the human lifetime and revealed an extraordinary wave of changes in gene expression that occur as the human brain develops. \"It's like I witnessed the poetry of birth,\" he says. But experiences such as Kleinman's are rare, owing to the challenges of collecting and storing children's brains. Parents must give permission shortly after their child has died, a time of inconsolable grief, and fetal brains are available only after an abortion \u2014 an incendiary political issue as well as an emotionally painful one for the women involved. Biomedical organizations have been tiptoeing around the delicacies for a decade or more. The solution, according to Amaral, is not complicated. Outreach programmes could be aimed at the coroners who conduct autopsies as well as at the families of children with brain disorders. They could explain the research value of donated brains and encourage families to sign up to a donor registry. A network of brain-collection centres around the United States could ensure that brains are preserved quickly. And centralized governance of the banks could direct tissue from each donated brain towards as much high-quality research as possible. \"All it needs is for someone to take ownership of the issue,\" Amaral says. That ownership may now be emerging from advocacy groups for neurodevelopmental disorders. \"I know there has been a lot of talk and no action till now,\" says neuroscientist Robert Ring, vice-president of translational research at Autism Speaks, a research and advocacy organization based in New York. So Ring is pushing forward plans for a bank along the lines Amaral suggests. \"Give us one year and we'll have developed a collaborative model with the scientific community,\" he says. Only two major brain banks store brains from children or fetuses and distribute them to the research community at large. One is run by the National Institute of Child Health and Human Development (NICHD) and held at the University of Maryland School of Medicine in Baltimore; the other, called the Autism Tissue Program, is run by Autism Speaks and is hosted at the Harvard Brain Tissue Resource Center in Belmont, Massachusetts. At most brain banks, including the NICHD's, personnel typically call the local coroner's office each morning. If a child is to be autopsied, they ask the office's permission to contact the family and request the brain for research. But the few coroner's offices involved can collect only a small amount of tissue. The Autism Tissue Program depends more on families that get in touch when they experience such a bereavement. Experts then go out to retrieve and prepare the brain. As the programme collects brains from across the United States, this often means a long journey. Ideally, though, the brain should be acquired quickly after death to minimize the breakdown of proteins and other molecules that researchers might wish to study. Other factors also influence tissue integrity, such as how soon after death a body is refrigerated and whether the person died slowly and painfully, as scientists have shown that this alters gene expression in the brain, making it less useful for research. Collecting fetal brains is also hard. Brains from spontaneous abortions can't be used for research because the fetus has generally been dead for many hours before it is expelled. In fact, brains can be collected from abortions only when labour has been induced medically, because surgical procedures tend to damage the tissue. Neither the NICHD bank nor the Autism Tissue Program bank \u2014 which together hold nearly 1,300 brains from people aged 19 and under \u2014 can meet the demand from researchers. Neuroscientist H. Ronald Zielke, director of the NICHD bank, says that he turns down 20% of requests for tissue because of a lack of material. In particular, this and other brain banks are running critically short \u2014 or have run out \u2014 of the brain areas that are the most interesting for research into developmental disorders, says Zielke. That includes the amygdala, which processes emotion, and the prefrontal cortex, which processes other cognitive and social behaviours. A brain bank, like any tissue repository, is also very expensive to run \u2014 the annual direct costs for the NICHD bank come to US$900,000. \n               Click here for larger image \n               To get around the shortage, some researchers have built up collections for their own use. Kleinman's research on gene expression drew on a collection that he heads at the NIMH. A similar study in this week's  Nature 2 , led by Nenad \u0160estan from the Yale University School of Medicine in New Haven, Connecticut, and with Kleinman as a co-author, drew in part on a collection that \u0160estan has generated at Yale. Their study showed the dramatic changes in gene expression that occur before and shortly after birth (see  'Brain waves' ). Neonatologist David Rowitch at the University of California, San Francisco, began a collection of brains at his hospital, which led to a paper published in last week's  Nature 3  showing that the migration of 'progenitor' cells between two brain structures seen in infants slows down after the age of 18 months and has almost disappeared by adulthood. He began collecting brains in 2008 with the support of the Howard Hughes Medical Institute in Chevy Chase, Maryland, and now has more than 100, most of which are from very young babies. These studies show how valuable such collections can be, but both Rowitch and \u0160estan describe the process of creating and running their own banks as \"a big headache\" because of the bureaucracy associated with handling human material. \u0160estan says that he would feel \"much more comfortable\" if the National Institutes of Health (NIH) were to run his collection. \"It's a huge effort for a small group and the NIH could do something on a larger scale,\" he says.  \n                Putting brains together \n              In fact, neuroscientists have been proposing for years that the NIH take a leading role in establishing a network of collection centres and standardizing methods for brain collection and preservation. In July last year, Autism Speaks and the other major US foundation that funds autism work, the Simons Foundation in New York, made a formal proposal to the NIMH for a public\u2013private partnership to collect brains from children with and without autism. The idea is that the advocacy groups would engage in intensive outreach efforts to potential donors, particularly families who have a child with autism, and the NIH would fund and manage the bank. The NIH, though, has been slow to commit. Ring, who moved from the drug giant Pfizer to Autism Speaks in June this year and has the can-do air of someone used to industry deadlines, sees \"a unique opportunity for the foundations to take on a leadership role\". His organization and the Simons Foundation are now in discussions with scientists to get agreement on scientific standards for the bank. He says that multiple collection centres will help to overcome geographical logistics, shortening the time from death to collection, for example. Thomas Insel, director of the NIMH, says that the NIH already supports 11 brain banks related to different neurological disorders, and would like to adopt \"a rational overall strategy rather than simply adding another boutique brain bank to the list\". He says that the NIH has now agreed in principle, at least, to create a 'neurobiobank' that would include both adult and children's brains. Although no firm plans have been released, the bank would probably have multiple collection points (the agency's existing tissue banks would become 'nodes'), but centralized oversight and tissue distribution. That is essentially what the advocacy groups want. However the banks are organized, the agonizing task of approaching bereaved families will remain. Yet autism researcher Cynthia Schumann, who earlier this year became director of an effort by the MIND Institute to start a bank of its own, says that her first encounters with families who choose to donate were eye-opening. \"I have been blown away by how parents have thanked us \u2014 for helping them to handle grief with the opportunity to give something back to help autism research,\" she says. Schumann, like counsellors at Autism Speaks, has also spent time educating affected families about autism research. \"Parents often agree to sign up to a registry, and to encourage other families to sign up too,\" she says. So the reluctance to ask parents about acquiring their children's brains, she thinks, may be ill-founded. That seems to be reflected in the experience of Valerie Hund, who donated the brain of her 16-year-old son, Grayson, to the MIND Institute after he died in January. Grayson had autism and epilepsy, and had died during a seizure. Hund says that a neighbour was a board member of the MIND Institute, and that her elder daughter had thought to call him shortly after Grayson died. The donation, says Hund, \"helped me to cope through the process. I'm happy that Grayson is a pioneer in this.\" Hund says she thinks that the institute's programme for raising awareness on brain and tissue banking is important. \"It would have been easier for us if we had thought about donation in advance \u2014 but that is the last thing on your mind.\" \n                 See Editorial  \n                 page 427 \n               Alison Abbott is Nature's senior European correspondent. \n                     Schizophrenia special \n                   \n                     NICHD brain and tissue bank for neurodevelopmental disorders \n                   \n                     Autism Tissue Program \n                   \n                     Autism Speaks \n                   \n                     Simons Foundation \n                   Reprints and Permissions"},
{"file_id": "479022a", "url": "https://www.nature.com/articles/479022a", "year": 2011, "authors": [{"name": "Karen Weintraub"}], "parsed_as_year": "2006_or_before", "body": "Shifting diagnoses and heightened awareness explain only part of the apparent rise in autism. Scientists are struggling to explain the rest. When Leo Kanner first described autism in 1943, he based his observations on 11 children with severe communication problems, repetitive behaviours such as rocking and an acute lack of social interaction. The physician and psychiatrist at Johns Hopkins University in Baltimore, Maryland, predicted that there were probably many more cases than he or anyone else had noticed 1 . \"These characteristics form a unique 'syndrome', not heretofore reported,\" he wrote, \"which seems to be rare enough, yet is probably more frequent than is indicated by the paucity of observed cases.\" Kanner's prophecy has been more than fulfilled. An early study 2 , in 1966, examined eight- to ten-year-old schoolchildren in Middlesex, UK, and estimated a prevalence of 4.5 cases per 10,000 children. By 1992, 19 in every 10,000 six-year-old Americans were being diagnosed as autistic 3 . Numbers skyrocketed in the first decade of the twenty-first century, according to data from the US Centers for Disease Control and Prevention (CDC) in Atlanta, Georgia. Surveying what is now known as autism spectrum disorder (ASD), the CDC found that by 2006, more than 90 in 10,000 eight-year-olds in the United States had autism 4 . Put another way, autism was now affecting 1 in every 110 children \u2014 a figure that strengthened public fears that an 'epidemic' was afoot (see  'Diagnosis: rising' ). \n               Click here for larger image \n               For the most part, research into autism's prevalence had explained away the increase. Studies attributed it to greater awareness of the condition, the wider diagnostic criteria for ASD, more frequent diagnosis of children with mental retardation as also having autism and diagnosis at younger ages. But by the mid-2000s, researchers started to note that these explanations were coming up short. \"A true risk due to some, as yet to be identified, environmental risk factor cannot be ruled out\", read one study from 2005 (ref.  5 ). That shift is important. If the rise in autism can be explained mainly by increased awareness, diagnosis and social factors, then the contributing environmental factors will always have been present \u2014 perhaps an ill-timed infection in pregnancy or some kind of nutritional deficit. If the increase can't be explained away \u2014 and at least part of the rise is 'real' \u2014 then new factors must be causing it, and scientists urgently need to find them. The subject is sensitive. Parents of children with autism agonize over whether they could have done something to prevent it. Researchers have been wary of invoking environmental triggers because that harkens back to a long-discarded idea that cold, unloving 'refrigerator' mothers were the source of their children's problems. And the increase in prevalence has been used to support more recently debunked hypotheses such as the idea that vaccines cause autism. Thomas Insel, director of the National Institute of Mental Health in Bethesda, Maryland, says it is time to get past these legacies. \"This whole idea of whether the prevalence is increasing is so contentious for autism, but not for asthma, type 1 diabetes, food allergies \u2014 lots of other areas where people kind of accept the fact that there are more kids affected.\" To him, it is clear that there is a real increase in autism, and researchers need more funding and encouragement to look at possible environmental causes. During the past decade, the US federal government has spent about US$1 billion researching the genetics of autism and only about $40 million on studies of possible environmental factors. Not everyone agrees with Insel's assessment. Some argue that the current data aren't strong enough to say for certain that the increase in autism diagnoses represents a true change in its prevalence. \"It feels like the numbers are going up. It really does,\" says Richard Grinker, an anthropologist at George Washington University in Washington DC. But \"when I look at the science, that doesn't stand up\", he says. \"You simply can't take prevalence estimates of autism as if they are the kind of hard scientific evidence that you would get from mapping out the increase in a virus.\"  \n                Changing criteria \n              No one knows for sure what causes autism, although genes and environment both seem to be involved. The brain's white matter may grow too fast in the first two years of life, leaving its networks jumbled. Synapses, the junctions between neurons, might not be functioning normally. Or other physiological processes could be involved: autism has been variously linked to epilepsy, digestive problems, immune or hormonal dysfunction, mitochondrial function and more. The diagnostic criteria for autism have changed over time. In 1952, autism defined by Kanner's narrow description was diagnosed as 'early-onset schizophrenia'; it was renamed 'infantile autism' in 1980 and then 'autism disorder' in 1987. In the past decade, the common name autism has covered a wider range of behavioural, communication and social disorders also referred to by the umbrella term ASD, which includes autistic disorder, Asperger's syndrome and other related conditions. Diagnoses of autism are also subjective. Social skills vary widely in the general population, as do other behaviours associated with autism. When does lack of spontaneity or an inability to make eye contact become a problem worthy of a medical label? And the frequency of diagnosis often reflects how eager parents are to receive one. When there's a stigma attached, diagnoses are likely to fall; when public support rises, so will cases. A diagnosis is mutable, says Grinker. \"It is a framework for a set of symptoms. And it's a framework that works at a particular point in time with a certain society and a certain health-care system and education system, and that will change as society changes.\" Such considerations help to explain the startlingly high prevalence of autism that Grinker found in South Korea in a study published this year 6 . In the 1980s, he had found Korean families generally unwilling to admit that anything might be wrong with their children, because of the stigma attached 7 . But when he undertook the latest study, attitudes had changed. Families in Ilsan, a stable, residential community on the outskirts of Seoul, welcomed information about autism, which in this study was offered confidentially. His team screened more than 55,000 children born between 1993 and 1999, and came up with an estimated prevalence for ASD of 1 in 38 (ref.  6 ). Grinker says that this is perhaps an overestimate, but it's the best his team could produce. Current US prevalence figures for autism are likely to be too low, Grinker says, because they don't look at the entire population. Many US studies are based on diagnosed cases of autism, either in the California school district \u2014 the nation's largest \u2014 or in the CDC's Autism and Developmental Disabilities Monitoring Network. But the California data count only children old enough to be in school and disabled enough to get a diagnosis or need services. The CDC surveillance also only picks up children with a documented developmental disorder. These methods probably miss children at the milder end of the spectrum. Some research suggests that the prevalence has always been high. In a study published this year 8 , a team led by Terry Brugha, a psychiatrist at the University of Leicester, UK, looked into autism's past by counting adults with the disorder. His team knocked on more than 7,000 doors across England. And although Brugha expected to uncover a very low prevalence of autism in adults, he and his colleagues calculated it as 9.8 in 1,000 \u2014 close to the frequency found in US children. Brugha says that the research needs to be repeated in different groups, but the implication is that autism prevalence is stable. \"If this is confirmed in other studies, it means we should also be looking for causes of autism that have always been there, and not just for causes that have developed in recent years or decades,\" he says. Christopher Gillberg, who studies child and adolescent psychiatry at the University of Gothenburg in Sweden, has been finding much the same thing since he first started counting cases of autism in the 1970s. He found a prevalence of autism of 0.7% among seven-year-old Swedish children in 1983 (ref.  9 ) and 1% in 1999 (ref.  10 ). \"I've always felt that this hype about it being an epidemic is not really very likely,\" he says.  \n                Filling the gap \n              Nevertheless, with numbers rising fast, many expect to see some sort of smoking gun in the environment. Peter Bearman, a sociologist at Columbia University in New York, has been trying to figure out how much of the increase is driven by social forces. He analysed nearly 5 million California birth records and 20,000 records from the state's department of developmental services. By linking birth with detailed diagnostic data he was able to generate a rich picture of the demographics and life history of those with autism, which yielded clues to the social factors that influence diagnosis. So far, Bearman says, he can account for just more than 50% of the observed increase (see  'Reasons: unclear' ). Around 25% of the rise in autism over the past two decades can be attributed to what he calls \"diagnostic accretion\". He could see from the medical records that some children who would have been diagnosed as mentally retarded a decade ago are now given a diagnosis of both mental retardation and autism 11 . Another 15% can be accounted for by the growing awareness of autism \u2014 more parents and paediatricians know about it 12 . \n               Click here for larger image \n               Geographic clustering accounts for 4%, Bearman says. The most fascinating cluster lies in and around the hills of Hollywood, California. Children living in a 900-square-kilometre area centred on West Hollywood are four times more likely to be diagnosed with autism than are those living elsewhere in the state 12 . Some residents worried that something in the water was triggering autism \u2014 perhaps the legacy of a 1959 nuclear accident at the Santa Susana Field Laboratory in nearby Simi Valley \u2014 but Hollywood shares its water supply with Los Angeles, where autism rates are not uniformly high. Moreover, rates are high whether families have lived in Hollywood for years or have just moved there, Bearman says. He suspects that the real reason for the cluster has to do with neighbourliness: a parent explains to a neighbour over the back fence where to find help and how to navigate the medical and educational systems. Once a cluster of informed, involved parents builds up, specialists are more likely to settle in that area, diagnosing and treating even more kids, Bearman says. Another 10% of the increase may be explained by a social change with biological implications: people having children when they are older. Some research has found that children born to parents older than 35 have a higher risk of being diagnosed with autism. Studies are divided about whether the mother's age or the father's has the strongest influence, but Bearman's work on parents older than 40 suggests that the mother's age matters more 13 . The fact that he still cannot explain 46% of the increase in autism doesn't mean that this 'extra' must be caused by new environmental pollutants, Bearman says. He just hasn't come up with a solid explanation yet. \"There are lots of things that could be driving that in addition to the things we've identified,\" he says. But many researchers now say that at least part of the rise in autism is real and caused by something in the environment. Rather than quibbling over recounts they are focusing on finding the causes. Since autism was first identified, ideas about its cause have swung to and fro between nature and nurture. The early focus on 'refrigerator' mothers resulted in a backlash and a stronger focus on genetics. The pendulum now seems to have settled somewhere in the middle, which is where many think it should be. \"The bulk of the autism research that's occurred has only looked at genetics,\" says Lisa Croen, director of the autism research programme at the health-insurance provider Kaiser Permanente, in Oakland, California. \"We've learned a lot but we haven't found the magic bullet. I think that's because part of the picture has been missing.\" Several major federally funded trials, together with other smaller ones, are now under way in the United States and elsewhere to try to fish out what makes a child autistic. They are hoping to uncover unknown risk factors and markers for autism by monitoring environmental exposures and taking regular biological samples from children and their parents. In 2007, for example, the Study to Explore Early Development (SEED), under the auspices of the CDC, began recruiting about 2,700 children aged two to five. The study includes developmental evaluations, questionnaires, a review of medical records and analysis of blood, cheek-cell and hair samples to examine genetic make-up and exposures to environmental chemicals. The Early Autism Risk Longitudinal Investigation (EARLI), funded by the National Institutes of Health, is enrolling up to 1,200 families that have a child with autism and are preparing to have another baby. The study intends to look for any interplay between environmental factors and genetic susceptibility that might contribute to autism risk in their next child. \"These studies are really going to fundamentally change the landscape,\" says Croen, who is a lead investigator on SEED. She and others expect a dramatic improvement in the understanding of autism and its prevalence over the next five to ten years. Craig Newschaffer, an epidemiologist at Drexel University in Philadelphia, Pennsylvania and an investigator with EARLI, says that a focus on the rise in diagnoses may be less important than figuring out what is causing autism in the first place. \"If it is an environmental cause that's contributing to an increase,\" he says, \"we certainly want to find it.\" It may be time to move on from the question of whether or not autism is truly rising, \"I think it's probably a nearly intractable question to answer.\" Karen Weintraub is a freelance writer in Cambridge, Massachusetts. \n                     Autism special \n                   \n                     CDC Autism and Developmental Disabilities Monitoring Network \n                   \n                     SEED \n                   \n                     MARBLES \n                   Reprints and Permissions"},
{"file_id": "479025a", "url": "https://www.nature.com/articles/479025a", "year": 2011, "authors": [{"name": "Lizzie Buchen"}], "parsed_as_year": "2006_or_before", "body": "Psychologist Simon Baron-Cohen thinks scientists and engineers could be more likely to have a child with autism. Some researchers say the proof isn't there. In the opening scene of  The Social Network , Jesse Eisenberg portrays a cold Mark Zuckerberg getting dumped by his girlfriend, who is exasperated by the future Facebook founder's socially oblivious and obsessive personality. Eisenberg's Zuckerberg is the stereotypical Silicon Valley geek \u2014 brilliant with technology, pathologically bereft of social graces. Or, in the parlance of the Valley: 'on the spectrum'. Few scientists think that the leaders of the tech world actually have an autism spectrum disorder (ASD), which can range from the profound social, language and behavioural problems that are characteristic of autistic disorder, to the milder Asperger's syndrome. But according to an idea that is creeping into the popular psyche, they and many others in professions such as science and engineering may display some of the characteristics of autism, and have an increased risk of having children with the full-blown disorder. The roots of this idea can largely be traced to psychologist Simon Baron-Cohen at the University of Cambridge, UK. According to a theory he has been building over the past 15 years, the parents of autistic children, and the children themselves, have an aptitude for understanding and analysing predictable, rule-based systems \u2014 think machines, mathematics or computer programs. And the genes that endow parents with minds suited to technical tasks, he hypothesizes, could lead to autism when passed on to their children, especially when combined with a dose of similar genes from a like-minded mate 1 . The notion has an intuitive plausibility. In the public mind, it meshes with the stereotype of the scientist or computer geek as smart but socially awkward. (Baron-Cohen has speculated that luminaries such as Albert Einstein and Isaac Newton had Asperger's syndrome.) And in scientific circles, many accept that certain autistic traits \u2014 social difficulties, narrow interests, problems with communication \u2014 form a continuum across the general population, with autism at one extreme. As most experts believe that genes have an important role in autism, it's also plausible that two parents with milder, 'autistic-like' traits could be more likely to have a child with autism. It also fits at least some clinicians' experiences. \"I see deep geeks of all sorts,\" says Bryna Siegel, a clinical psychologist who runs the autism clinic at the University of California, San Francisco, referring to the parents of children with autism. \"They don't make great eye contact, all their clothing is from the Intel shop, they don't have a lot of social understanding. I do think that when these geeks marry each other, that's bad news for the offspring.\" But critics of Baron-Cohen's theories aren't hard to find. Autism researchers say that his work has focused primarily on a subset of people with 'high-functioning' autism \u2014 such as Asperger's syndrome \u2014 who have good language capabilities and at-least average intelligence. They say that the data are insufficient to support his theories and that many experiments cry out for independent replication. \"They're some really good hypotheses to think about, but they need to be tested,\" says John Constantino, a psychiatrist at Washington University in St Louis. \"There's not a lot of data.\" Some critics are also rankled by Baron-Cohen's history of headline-grabbing theories \u2014 particularly one that autism is an 'extreme male' brain state. They worry that his theory about technically minded parents may be giving the public wrong ideas, including the impression that autism is linked to being a 'geek'. Baron-Cohen acknowledges that \"there is a problem that there are too few attempts at replication\" of his studies, and says that he remains \"open minded about these hypotheses until there are sufficient data to evaluate them\". But he says he doesn't see a problem with introducing theories before definitive evidence has been collected. \"I would see it as a positive contribution rather than a concern that scientists move from preliminary evidence to formulate the more general theory, especially when the theory is highly testable, since this is how science advances,\" he says.  \n                Bucking the system \n              In the 1990s, while most research on autism focused on problems with social interaction, Baron-Cohen became fascinated by the obsessive, narrow interests and repetitive behaviours that also characterize the condition. He noticed that children with autism were drawn to things such as machines, numbers, calendars and spinning objects 2 . One child might memorize the technical specifications of gadgets; another would flip light switches on and off incessantly. \"The old view was that [such behaviours] were lacking in purpose, they just did it,\" says Baron-Cohen. But he started seeing these eccentricities from a new perspective. \"They're figuring out how the family DVD player works, or understanding the electrical circuitry of the house. The child is doing it to understand the system.\" In autism, he theorizes, the brain has an average or superior ability to understand predictable systems, or 'hypersystemize', coupled with an inability to empathize, or understand other people's intentions and feelings. Baron-Cohen cites several lines of evidence in support of his theory. In a 2003 study 3 , for example, he found that people with autism scored highly on the 'systemizing quotient', a questionnaire he designed. In a survey of undergraduates at the University of Cambridge, he found that those studying mathematics were more likely to have been diagnosed with autism than were students majoring in medicine, law or social science 4 . And, using another questionnaire called the autism quotient, he found that students in science and maths had higher scores on measures of autistic traits than did students in the humanities and social sciences 5 . Baron-Cohen says that although these surveys do not measure systemizing ability directly, they demonstrate that systemizing is a trait of autism, and also part of the 'broader autistic phenotype' that includes some of the wider population. Baron-Cohen's critics, however, are sceptical of these surveys, in which subjects answer questions about themselves such as: 'I notice patterns in things all the time' and 'I would rather go to a library than to a party'. \"Whether those self-perceptions, as with any of our self-perceptions, are accurate is questionable,\" says Francesca Happ\u00e9, a cognitive neuroscientist at King's College London. It would be more objective, say Happ\u00e9 and others, to test children with and without autism on their abilities to understand systems, and then compare the scores. \"Rigorous studies are still missing,\" says Uta Frith, a developmental psychologist at University College London. \"At the moment, he has people saying, 'yes, I'm a person interested in details', as opposed to actually observing them on tasks.\" Baron-Cohen says that his lab is doing such follow-up work. He says that questionnaires can be advantageous because data can be collected quickly, and that even though biases can creep in, \"you do find consistent patterns\". He also points to a 2001 study 6  in which he showed that children with Asperger's syndrome can outperform typical children at figuring out how simple mechanical systems work. But critics counter that the children with Asperger's were selected on the basis of having average or above average IQs, whereas the typical children were selected at random. Similarly, critics point out that the Cambridge students with autism are highly unusual because they function well enough to attend one of the top universities in the world. This is a common complaint about Baron-Cohen's work. \"He's tended to focus on very bright individuals with ASD,\" says Catherine Lord, a clinical psychologist and autism researcher at Weill Cornell Medical College in New York. \"A lot of the things he might say in describing those individuals are pretty irrelevant for most people with ASD.\" Baron-Cohen acknowledges that \"some of the psychological research is focused on high-functioning children with autism\", because, he says, they have the language capabilities to perform the tests. \"But my thought is that it could apply across the system,\" he says of the systemizing theory, to all children who have some form of the disorder. Earlier this year, Liz Pellicano, a developmental psychologist at the Institute of Education in London, tested how a group of children with a wider range of ASD compare with a control group in figuring out a system. Her team designed a small room in which the floor was arrayed with 16 identical green lights. The children were asked to find the one light that, when pressed, would turn from green to red. The target light was on the same side of the room 80% of the time. Children with autism, including Asperger's syndrome, were much worse at figuring out this system than the children in the control group 7 . \"They weren't systematic,\" says Pellicano. \"When they were searching, they were unbelievably haphazard.\" In her view, she says, studies such as this show that Baron-Cohen's theory \"isn't standing up to empirical tests\". Baron-Cohen says he is not sure that Pellicano's paradigm was testing the same sort of systemizing he describes. But, he says, he's \"glad that at least people are starting to look at systemizing\". So far, most work on the subject has come out of his lab. \"I think our published studies are rigorous, but there are still too few studies into systemizing,\" he says. \"It is still way too early to be able to look across dozens or hundreds of studies to evaluate that theory.\"  \n                Like father, like son? \n              Baron-Cohen proposes that systemizing ability can be inherited \u2014 and that in information-technology (IT) enclaves such as Silicon Valley, where hypersystemizers are more likely to meet, pair off and have children, the result is a higher incidence of autism. Back in 1997, for example, he concluded that fathers of children with autism were more than twice as likely to be engineers as were fathers of non-autistic children 8 . But autism researchers Christopher Jarrold and David Routh at the University of Bristol, UK, pointed out that Baron-Cohen reported the analysis of data only for engineers, not for the other occupations surveyed. After analysing the same data 9 , they found that fathers of children with autism were more likely to work in medicine, science and accountancy, as well as engineering, and less likely to have manual occupations. They suggested that these fathers were simply more likely to have reached a higher level of education. Baron-Cohen says that when he reanalysed the data and controlled for education level, he found that fathers of children with autism were still more likely to be engineers, although the difference was smaller. One of Baron-Cohen's most recent studies comes from the town of Eindhoven, a technology hub in the Netherlands. By examining school records, he found that children living in the town were 2\u20134 times more likely to be diagnosed with autism than were children living in two other Dutch towns of similar size 10  \u2014 evidence that he takes as support for the idea that parents who are strong systemizers could be more likely to have a child with autism. But, he says, he chose to study Eindhoven after parents contacted him about an autism epidemic there, rather than, as some researchers may prefer, comparing the prevalence of autism in randomly selected IT regions with that of non-IT regions with similar demographics. And the Eindhoven school records did not disclose parental age or level of education \u2014 both of which are positively correlated with autism diagnoses \u2014 or whether the parents worked in the IT industry. Indeed, researchers say that several other factors could explain the seeming correlation between autism and science or engineering. A 2010 analysis of autism diagnoses in California 11  did not find that autism clustered preferentially around areas rich in IT industry. Instead, it found that clusters tended to occur in areas where parents were older and educated to a higher level than were parents in surrounding areas. \"Virtually all of these clusters were also clusters of higher education,\" says lead author Irva Hertz-Picciotto, an epidemiologist at the University of California, Davis. People who have progressed further in education tend to have children later in life, and at least some evidence suggests that older parents are at higher risk of having children with autism. Parents who are more educated are also more likely to be aware of the symptoms of autism and to seek a diagnosis, which can open the door to support and education services. One Silicon Valley school for children with learning disabilities costs US$30,000 per student per year, but if a child has been diagnosed with autism, the school district may pick up the tab. In response to criticisms of his Eindhoven study, Baron-Cohen says he plans to follow up by looking at the age, occupation and other details of the parents, and that he'd also like to examine autism rates in other IT centres, such as Silicon Valley. He's putting together a large online survey ( go.nature.com/umyv61 ) to gather detailed information about the general population \u2014 including age, education, occupation and hobbies \u2014 to explore whether these factors correlate with having a child with autism. He says that Hertz-Picciotto's study didn't support his hypothesis because it \"was not designed to look at autism in IT-rich regions. What I'm doing is coming at it in a different way,\" he says. Despite the criticisms of Baron-Cohen's experiments, most of his colleagues commend him for putting his theories forward, and many are open to the possibility that parts of them could prove correct. \"He does try to address big questions that many of us would be too wimpy to take on,\" says Lord. Constantino is testing related ideas. He has developed the 'social responsiveness scale' \u2014 a questionnaire to measure autistic-like traits in the general population. He found hints that parents with more autistic-like traits tend to partner with each other, and that when they do, their children have even more of those traits than their parents 12 . Those children, however, are not more likely to be diagnosed with autism 13 . What is needed now, Constantino says, is a large study that determines whether having two parents with autistic-like traits is more common among people with autism than in the general population. \"Those are the kind of data one needs,\" he says, \"rather than to infer, from an epidemiological cluster in a place where people tend to be a little nerdier, that that's why you've got more autism there.\" For now, the idea that technical brilliance requires a dash of autism seems to have taken root, at least in some tech and science hubs. It's a trend that, for Happ\u00e9, provokes mixed feelings. \"On the one hand, I'm glad that 'geek chic' has some kudos in our current society, because a lot of people with AS or ASD have a jolly tough and unpleasant life, and if people can recognize their talents a little more, I'm glad for that.\" On the other hand, she says, \"a large number of children with autism have significant intellectual disabilities and no speech. For their parents to be surrounded by people spotting all these famous people and saying they have autism, it must be absolutely infuriating.\" Lizzie Buchen is a freelance writer based in San Francisco. \n                     Autism special \n                   \n                     Simon Baron-Cohen's lab \n                   \n                     Systemizing quotient and autism quotient tests \n                   Reprints and Permissions"},
{"file_id": "479028a", "url": "https://www.nature.com/articles/479028a", "year": 2011, "authors": [{"name": "Meredith Wadman"}], "parsed_as_year": "2006_or_before", "body": "Convinced by the evidence that vaccines do not cause autism, Alison Singer started a research foundation that pledges to put science first. The e-mail that ended one career for Alison Singer, but started another, arrived as she was cooking dinner for her daughters one evening in January 2009. Singer was preoccupied. At a committee meeting she was due to attend in Washington DC the next day, she and others were set to vote on a plan that would direct much of the United States' spending on autism research for the next year. Singer, who had her laptop perched on the kitchen counter, immediately noticed the e-mail from another committee member \u2014 a mother who was convinced that vaccines had caused her son's autism. The message proposed last-minute language for inclusion in the plan, endorsing more research into whether vaccines can trigger the disorder of communication and movement. Singer knew immediately that this would cause her serious difficulties. Having read the literature and talked to numerous scientists, she was convinced that no studies supported a link between autism and vaccines. But she was also the top communications executive at Autism Speaks in New York, autism's most prominent research and advocacy group. The organization supports vaccine-related research, and Singer knew that her bosses would expect her to vote for more studies of vaccines as a possible cause of the condition. At 11:10 p.m., Singer hit 'send' on an e-mail of her own, to Bob and Suzanne Wright, the co-founders of Autism Speaks. \"I've concluded that as a matter of personal conscience, I cannot vote in favor of dedicating more funds to vaccine research that has already been undertaken and which I and many others find conclusive,\" her message read. \"I feel compelled to offer my resignation.\" With that, Singer became a solo operator in the world of autism-research funders. Within months, she would launch the Autism Science Foundation (ASF), a tiny New York-based charity with a relentless focus on rigorous science, a niche supporting the youngest researchers and a guiding principle that \"vaccines save lives; they do not cause autism\". The ASF is scarcely a blip on the big screen of autism-research spending: in this, its second full year of operations, it is awarding US$220,000 in grants to young researchers; last year, it spent $180,000. The two major non-governmental players in US autism research, the Simons Foundation in New York and Autism Speaks, last year spent $54 million and $21 million, respectively (see  'All change for autism' ). \n               Click here for larger image \n               But Singer's charity is drawing notice as much for the aims and quality of its work as for its magnitude. In August, GuideStar, a major charity-rating group based in Washington DC, singled out the ASF as a \"promising start-up\", calling it \"a shining star to those interested in real science and evidence based interventions\". The fledgling foundation has also won endorsements from leaders at the American Academy of Pediatrics in Elk Grove Village, Illinois; the National Institutes of Health (NIH) in Bethesda, Maryland; and the Centers for Disease Control and Prevention (CDC) in Atlanta, Georgia. \"The Autism Science Foundation is an important voice for scientific direction in the autism community,\" says Coleen Boyle, the director of the National Center on Birth Defects and Developmental Disabilities at the CDC. The jury is still out on whether Singer and her tiny organization can do much to combat the perception of a link between autism and vaccines that has become cemented in many minds. But if anyone is going to do it, Singer is a strong candidate. She \"is a force of nature\", says Thomas Insel, director of the National Institute of Mental Health in Bethesda, which spends more than $80 million on autism each year \u2014 more than half of the NIH's autism budget. \"I have enormous respect for her abilities.\" \"She is one of the most strong-willed people that I have ever met,\" adds Christie Buchovecky, an ASF-funded graduate student at Baylor College of Medicine in Houston, Texas, who has a 12-year-old cousin with autism. \"And she somehow manages to do that while still being extremely caring and supportive. She knows where the parents are coming from.\"  \n                All in the family \n              On Saturdays 40 years ago, when Singer (then Alison Tepper) was 5 years old and wanted to be in ballet class, she and her parents would set out on a very different errand: visiting her autistic 7-year-old brother, Steven, at the Willowbrook State School, an institution on Staten Island, New York, that housed more than 5,000 people diagnosed with mental retardation and developmental disabilities. Singer's parents had placed Steven in Willowbrook out of fear for their children's safety. Steven could be self-injurious and a danger to others; once, he had brought a whole wall unit, including a television, crashing to the floor. Willowbrook was a Dickensian institution that would eventually come to symbolize everything that was wrong with the way America cared for its mentally ill. In 1965, New York Senator Robert Kennedy alluded to the facility as \"a snake pit\". In 1972, an undercover expos\u00e9 by local television channel WABC-TV showed residents sitting and staring vacantly; rocking, often naked, in overcrowded, barren rooms; or abandoned on bathroom floors. When Singer and her family visited Steven, they would see him only in the visitors' room. Still, the stench of urine bothered Singer. So did the noise. \"There was a lot of screaming,\" she recalls. \"I didn't like it.\" In 1971, shortly before the documentary aired, the Tepper family moved from the New York borough of Queens to the suburbs, and transferred Steven to a nearby facility called Letchworth Village. In time, the family \u2014 Singer also has a younger brother \u2014 began visiting Steven less often. Singer's mother, who had been told she was to blame for Steven's condition, instructed her daughter to tell people that she had just one, younger, brother. \"People didn't talk about it then. No one talked about it,\" Singer says. \"There was a tremendous stigma associated with autism.\" Singer carried her secret out of childhood as she set out to become a journalist. In 1993, having graduated with a degree in economics from Yale University in New Haven, Connecticut, and a master's from Harvard Business School in Boston, Massachusetts, she was hired as vice-president of programming for the cable division of television network NBC. Singer was charged with bringing news content to desktop computers through the new phenomenon called the Internet. She married in 1994; in 1997, her first daughter, Jodie, was born. Singer thought something was wrong, she says, \"from the day she was born\". Jodie cried constantly. She didn't want to eat. She wouldn't sleep. One diagnosis followed another: failure to thrive; early colic; late colic. \"I just thought I didn't have the mommy gene,\" Singer recalls. She started questioning her mother over and over. \"'Did Steven ever do this? Did Steven ever behave like this?' My mom would say: 'No, Steven never did that'.\" Singer also took heart from a conversation with an obstetrician who assured her that autism is \"not genetic\". Still, as Jodie became a toddler, Singer remained deeply troubled by her inability to communicate. \"She used to recite the  Madeline   book from beginning to end perfectly,\" says Singer. \"But if I said, 'Do you want juice?' she couldn't answer yes or no.\" Singer called the state department of health several times to schedule an assessment for Jodie. Each time, in a rush of ambivalence, she would call back and cancel. Finally, she made a firm appointment. When a psychologist and a case worker visited her at home, Jodie was two years, eight months old. (Singer had just given birth to Jodie's healthy younger sister, Lauren.) Singer expected them to say that nothing was wrong. Instead, they diagnosed Jodie as severely autistic, telling Singer gravely, \"Don't worry, we're going to get her help.\" When they were gone, says Singer, \"I just bawled\". Then, she says, \"I pulled myself together\".  \n                Driving on \n              'Together' seems an apt word for Singer. She comes across as organized, articulate and, above all, driven \u2014 although not without a sense of humour. On a September day, Singer showed this reporter around what she jokingly referred to as \"the worldwide headquarters\" of the ASF: a cubicle on the fourth floor of a lower Manhattan office building. There, Singer's one paid member of staff \u2014 Jonathan Carter, a recent college graduate with an autistic brother \u2014 was labouring over the just-relaunched ASF website. Singer herself does not draw a salary. It's a long way from the high-flying position she held at Autism Speaks, where in her last year she made $187,000. Bob and Suzanne Wright had made Singer their first employee when they launched the group in 2005, shortly after their grandson was diagnosed with autism. They were well placed to make things happen: Bob Wright was chief executive of media conglomerate NBC Universal and vice-chairman of General Electric, NBC's parent company. During the group's first months, when Singer served as interim chief executive, she, Suzanne Wright and an assistant constituted the entire staff. Working with a shared passion and purpose, from an office on the 51st floor of NBC Universal's headquarters, Singer and Suzanne Wright became fast friends. \"Suzanne doesn't take no for an answer and neither do I. So we got a lot done,\" Singer recalls. Their completed to-do list included landing coverage for Autism Speaks on television programmes  Oprah   and  Larry King Live , and in the pages of  The Wall Street Journal . Singer turned the organization's website into the most highly trafficked in the autism world. The group, she later wrote in her resignation e-mail, \"elevated 'autism' to the global vocabulary\". In 2008, Singer's final year with Autism Speaks, the charity raised more than $70 million. But there was friction, too. The Wright's daughter, Katie, fervently believed that vaccines were the cause of her son's autism, and was pushing her parents to fund more vaccine research. This didn't sit well with Singer. Although she had been open to such research when Jodie was first diagnosed, Singer had followed the growing number of studies that debunked any link between autism and the once-suspect measles, mumps and rubella vaccine, or between autism and thimerosal, a mercury-containing preservative that was used in some early-childhood vaccines until 2001. For the past several years, she says, the data have been very clear. \"There were no studies indicating a link between autism and vaccines.\" As time went on, Singer became increasingly uncomfortable with her organization's continued funding of vaccine-related research. She was even more disturbed by its failure to state, publicly and unequivocally, that all the data show that vaccines do not cause autism. The committee meeting in January 2009 brought matters to a head. The Interagency Autism Coordinating Committee, a group of federal-agency and public representatives, was charged with developing an annual strategic plan for autism research, which would be used to guide work done by federal agencies, both internally and in collaboration with private foundations. Singer refused to support a plan that would broadly sanction vaccine studies. \"She knew that her resignation would be seen as a desertion,\" says Paul Offit, chief of infectious diseases at the Children's Hospital of Philadelphia in Pennsylvania, and now a close colleague of Singer's and a member of the ASF's board of directors. \"It was a very, very hard thing for her to do.\" The day after she resigned, Singer spoke at the committee meeting, letting members know how she was going to vote, and why. There were two votes related to vaccine research at the meeting. The first, on whether to add the language that had circulated the previous evening, failed. The committee also voted on whether to cut two existing vaccine-research objectives from the plan. All the other members representing advocacy groups or affected families opposed removing the objectives; Singer and the federal-agency scientists did not, and the objectives remained. Autism Speaks immediately put out a press release distancing itself from Singer and withdrawing its support for federal autism-research plan. Suzanne Wright has not contacted Singer, or replied to e-mails from her, since Singer resigned. But Singer did receive many supportive messages. \"I understand your reasons completely and agree with you wholeheartedly,\" wrote one supporter, Mark Krinsky, whose son, now 25, has autism. \"What are your plans? Any group would be lucky to get you but maybe you should start a new one. I will volunteer.\" It was letters and phone calls such as these, says Singer, \"that got me thinking about starting a new advocacy group\". She launched the ASF in March 2009.  \n                Personal challenge \n              Singer, who drives a blue Honda minivan decorated with bumper stickers reading \"Autism Science Foundation\" and \"Vaccinate your baby\", spends two days a week working from her home in Scarsdale, an upmarket northern suburb of New York. This allows her to juggle a mother's duties with her work. One Monday afternoon, she arrives home from the office to find Lauren, now 11, downstairs finishing her homework, and Jodie, 14, upstairs with Keith Amerson, one of her therapists. Amerson, who is an expert in a leading autism therapy called applied behaviour analysis (ABA), has been working with Jodie since she was diagnosed. Today, Jodie is sitting beside Amerson at a table, in front of two plastic plates. The yellow one has one piece of toy food on it; the green one has several. \"Jodie, which one has more?\" asks Amerson. Jodie points to the yellow plate. When her mother comes in, Jodie lights up. The two bounce on the bed in Jodie's bedroom and sing a song, that, Singer says, they have sung \"a thousand times\". Jodie: \"Say: 'I love my Jodie.'\" Singer: \"I love my Jodie.\" But Jodie is challenging. She sleeps for as little as four hours a night. Every door and window in the house has had an alarm ever since, in the wee hours one winter morning, shoeless and clad in pyjamas, she set off down the driveway in search of an egg roll at a Chinese restaurant. Luckily, Singer was awake and stopped her. When Amerson pushes Jodie to find the prices beside pictures of root beer and cake on a sheet of paper, and enter them in a calculator, Jodie cries \"Oh no!\" and runs her flapping, nervous hands through her brown pony tail. \"Do you need some help?\" asks Amerson. \"Keith!\" replies Jodie. \"Say: 'I need some help,'\" counters Amerson. \"I need some help,\" Jodie says. The goal of ABA therapy, says Amerson, \"is to make her as independent as possible and to improve the quality of her life\". A task as quotidian as writing '$1.74' legibly, with the decimal point in the right place, is a large challenge, its achievement a victory. \"It's very tough work,\" he says.  \n                Young brains \n             On 8 February 2010, almost a year after it launched, the ASF awarded its first grants, splitting $180,000 between six PhD students. Singer has chosen to focus on the youngest scientists because, she says, \"it fuels the pipeline for the future. You are encouraging that person to pursue autism research as opposed to research in some other disease.\" The foundation boosted its funding by more than 20% this year, handing out a further six awards to graduate students and postdoctoral researchers. Chosen by a ten-member scientific advisory board, the awards run the gamut from basic science to diagnosis and treatment. One 2011 recipient, Jill Locke at the University of Pennsylvania and the Children's Hospital of Philadelphia, is working to translate behavioural techniques proven in the research setting to Philadelphia's public schools. She is training school personnel to improve the social interactions between children with autism and their playground peers. Another recipient is Haley Speed, a postdoctoral fellow at the University of Texas Southwestern Medical Center in Dallas, who is building on work that she recently co-authored in  Cell , describing a mouse with a mutation in the  Shank3   gene, which is linked to autism in humans ( M. A. Bangash  et al. Cell    145,   758\u2013772; 2011 ). In the past, Speed had failed to win NIH postdoctoral awards, and she believes that the riskiness of her current research \u2014 in which she hopes to find compounds that could correct the animals' abnormal neuron-to-neuron communication and restore normal behaviour \u2014 would not help her case with conservative study sections at the biomedical agency. \"The ASF had faith in us, whereas the NIH did not,\" she says. Speed adds that it inspires her to know that $10 and $20 donations from affected families are paying for her work. \"On a bad day, if your equipment breaks or your experiment fails \u2014 which they all do \u2014 it gives you an extra boost as you to go pick yourself up and do it all again.\" Indeed, many donations to the ASF come in small cheques from affected families and their friends, or larger ones from the proceeds of football goal-a-thons and charity motorbike rides. (A significant amount has also come from large donors, including Offit, who has donated all the royalties from the sales of his books challenging the anti-vaccine movement to ASF \u2014 an amount he estimates in tens of thousands of dollars.) Singer does not think that her opposition to vaccine research limits the group's ability to raise funds \u2014 far from it. She says the organization has mobilized the thousands of families who were sick of the autism story being hijacked by the vaccine hypothesis. \"ASF is their voice.\" The ASF is still dwarfed by the size and reach of other organizations supporting autism research. The Simons Foundation does not fund vaccine research, but Autism Speaks spends about 2% of its budget \u2014 nearly twice as much as the ASF spent on all research this year \u2014 on studies that are relevant to vaccines, says Geri Dawson, chief science officer for Autism Speaks and a professor of psychiatry at the University of North Carolina at Chapel Hill. \"We are not funding any studies that directly address whether vaccines cause autism,\" says Dawson. \"The evidence strongly suggests that there is not a link between autism and vaccines. What we are trying to understand through our research funding is the role of the immune system in autism, which certainly could be relevant to the question of vaccines.\" For instance, Dawson says, Autism Speaks is funding studies investigating the idea that disorders in the cellular powerhouses, mitochondria, might influence responses to immune challenges such as infection and immunization. \"We are willing to leave the door open for the possibility of rare cases in which an immunization may have triggered the onset of autism symptoms due to an underlying medical or genetic condition.\" Singer calls Dawson's words \"a carefully crafted 'big tent' political statement designed to appeal to both sides of the issue\" and an attempt \"not to offend any potential donors on either side\". Her concern, she says, is that statements suggesting that the jury is still out on autism and vaccines put children \"at risk for vaccine-preventable disease\". In an e-mail, Katie Wright wrote that she \"totally respects\" Singer's beliefs about Jodie. \"However, Alison doesn't live my life, doesn't know what caused my son's autism and wasn't with me the night he had a febrile seizure or the day he stopped talking. It is important not to tell other parents you know better than they do about their child's autism. We all love our kids very much, Alison and I certainly agree on that.\"  \n                The long haul \n              Although the world of autism research is expanding, horizons for many of those with the condition have scarcely changed. Singer's brother, Steven, lives in a group home in Rockland County, not far from Scarsdale. Now 47, he attends a day programme and helps to deliver Meals on Wheels three mornings a week. Still, he has never spoken, and needs round-the-clock supervision. \"If he had had early intervention when he was two, when his brain was more malleable, then who knows where he would be today,\" says Singer, who visits him several times a year. \"But I'm a big believer that you never stop trying.\" Singer is not deterred by the sometimes agonizingly slow pace of science, or by the stark fact that the genetics and aetiology research that her foundation is funding is unlikely to ever directly benefit Steven or Jodie. Singer now believes that her daughter's autism was largely caused by genes, but genetic testing when she was first diagnosed revealed no known pathogenic deletions or duplications in her genome. Singer had Jodie retested earlier this year, after a glut of studies identified mutations implicated in autism. The tests still came up empty. Nonetheless, Singer believes that the ASF's research into treatment will improve the lives of people who have autism today \u2014 including Jodie. She also focuses on her other daughter, Lauren, who has already asked whether she, too, will have a child with autism. Says Singer: \"I would like to be able to answer her with: 'If you do, we will know how to help.'\" \n                 See Editorial  \n                 page 5 \n               Meredith Wadman is a reporter for Nature in Washington DC. \n                     Epidemiology: How common is autism? \n                   \n                     Autism Science Foundation \n                   \n                     Autism Speaks \n                   \n                     Interagency Autism Coordinating Committee \n                   Reprints and Permissions"},
{"file_id": "479021a", "url": "https://www.nature.com/articles/479021a", "year": 2011, "authors": [], "parsed_as_year": "2006_or_before", "body": "Diagnoses and research funding are rising, but much about autism remains a puzzle.  Nature   seeks some truths. Everything about autism spectrum disorder conspires to make it hard to understand. It takes diverse forms, from profound communication and behavioural problems to social difficulties coupled with normal language and even precocious talents. (Here,  Nature   will refer to them all as autism.) The prevalence of autism is rising \u2014 by some counts, steeply \u2014 but the reasons for that are unclear. Causes of the condition include a complicated mixture of genetic and environmental factors, most unknown (see  page 5 ). Its roots lie in the development of the human brain, a process that, despite huge leaps in neuroscience, remains mysterious. So as awareness rises and parents clamour for answers, scientists can offer few certainties. Hearsay and unsubstantiated theories sometimes fill the void. This week,  Nature   searches for some truths about autism. Some researchers have evidence to combat the notion that the rise in prevalence can all be explained by changes in how the condition is diagnosed (see  page 22 ). Others are debating the idea that some scientists and engineers are themselves 'on the spectrum', and are at high risk of having a child with autism (see  page 25 ). At the same time, researchers are learning that although autism is a clearly disability, certain characteristics of it could be an advantage in science (see  page 33 ). A debunked link between vaccines and autism still clouds the public discussion, but some advocates have taken a firm stand in favour of rigorous science, and the answers it will eventually provide (see  page 28 ). Much more content can be found at nature.com/autism. Even before fundamental problems are solved, research is revealing better ways to support people with autism. If the condition is diagnosed early, a growing repertoire of evidence-based therapies can be applied to give children the best possible chance of living full lives. Meanwhile, the spotlight on autism is helping to reduce stigma. The complexities that make autism hard to understand are a magnet for researchers \u2014 and this should lead to a future with less fiction and more much-needed fact. \n                     Autism special \n                   \n                     National Institute of Mental Health: Autism Spectrum Disorders \n                   Reprints and Permissions"},
{"file_id": "478302a", "url": "https://www.nature.com/articles/478302a", "year": 2011, "authors": [{"name": "Zeeya Merali"}], "parsed_as_year": "2006_or_before", "body": "The exotic theory of everything could shed light on the behaviour of real materials, thanks to an unexpected mathematical connection with condensed-matter physics. \"On one side,\" says Jan Zaanen, \"you have this refined, almost other-worldly intellectual \u2014 the perfectionist obsessed with detail, barely interested in earthly pleasures. On the other, you have the loud, boisterous, sometimes aggressive, business-savvy character who knows how to get his hands dirty.\" It might almost be a description of the misfit roommates known on stage, screen and television as  The Odd Couple . But Zaanen, a condensed-matter physicist at the University of Leiden in the Netherlands, is actually describing the pairing of two groups of scientists: string theorists, who spend their days pursuing a rarefied, highly mathematical 'theory of everything', and his own colleagues \u2014 a considerably more grounded bunch who prefer to focus on how real-world materials behave in the laboratory. The scientists trying to bridge these disciplines are motivated by the discovery of a startling coincidence: suitably interpreted, the equations of string theory can be a powerful tool for analysing some exotic states of matter, ranging from super-hot balls of quarks and gluons to ultracold atoms. The past year alone has seen at least four international workshops designed to stimulate collaborations across the disciplinary divide, including one hosted by Zaanen in Leiden. Sceptics still question whether this strange alliance will actually lead to new insights, or whether it is just a marriage of convenience. String theory does hint at the existence of many new states of matter, for example. But those predictions will be difficult to verify, and decisive experimental tests are only now in the planning stages. For the time being, the advantage to both partners is clear. String theory, long criticized for having lost touch with reality, gets experimental credibility. And condensed-matter physics, never the media darling that string theory has been, gets a new mathematical tool \u2014 and a chance to bask in new-found glamour. The match-making began a dozen years ago with the reunion of Dam Thanh Son and Andrei Starinets, who had been undergraduates and dorm-mates at Moscow State University in the 1980s. The friends had lost touch with each other when they left Russia after the fall of communism in 1991. But in 1999, Son got a job at Columbia University in New York City, and heard that Starinets was doing a PhD in string theory just a few kilometres away at New York University. So Son went to pay Starinets a visit. Collaboration was the farthest thing from his mind. String theory is mathematically rich and has an undeniable aesthetic appeal. But it is all about what physics might be like at scales of 10 \u221235  metres \u2014 the idea being that seemingly point-like elementary particles such as quarks and electrons will actually turn out to be tiny, vibrating threads of energy when viewed at such scales. But these strings would be about 20 orders of magnitude smaller than a proton, putting the theory hopelessly beyond the reach of any direct experimental test. Son's speciality, by contrast, was firmly rooted in experiment: he was trying to understand the properties of quark\u2013gluon plasmas, the short-lived, super-hot fireballs that form when heavy nuclei such as gold are smashed together in accelerators. All this stringy stuff seemed utterly alien. Except that, when Son saw the string-theory calculations that Starinets had been working on with fellow PhD student Giuseppe Policastro, he recognized the equations as the same ones he had been using to analyse the plasma. Son immediately had to know what was going on, and Starinets began to explain. Starinets and Policastro had been working on an idea proposed in 1997 by Juan Maldacena, a physicist at Harvard University in Cambridge, Massachusetts. Maldacena, now at the Institute for Advanced Study in Princeton, New Jersey, had realized that string theory predicts a mathematical equivalence between two hypothetical universes, one of which would be similar to our own. It would have the same three dimensions of space and one dimension of time, for example, and be filled with much the same types of elementary particle, which would, in turn, obey familiar-looking (to physicists) quantum-field equations. But it would not contain strings \u2014 or gravity. The other universe would be the opposite: it would contain both strings and gravity \u2014 indeed, the gravity could get strong enough to form black holes \u2014 but no elementary particles. It would also have an additional dimension of space. Maldacena's insight was simple, if audacious: take any process involving particles and fields in the first universe, he said, and it could equally well be described as a process involving gravity, black holes and strings in the second universe \u2014 and vice versa 1 . The equations might look very different. But the fundamental physics would be exactly the same. That was why Son was seeing quark\u2013gluon equations in a string-theory calculation, Starinets explained: they were the three-dimensional equivalent of the gravitational fields that he and Policastro had been studying in the four-dimensional universe.  \n                Marriage of convenience \n              All this jumping back and forth between universes was weird even by string-theory standards (and even weirder for non-string theorists, as Maldacena had showed that the mapping worked not just between three and four dimensions of space, but also between four and five, five and six and so on). But as Son and Starinets talked, they began to see that Maldacena's mapping might be a powerful problem-solving strategy. They could start with a messy set of quantum-field calculations in our real, three-dimensional world \u2014 the quark\u2013gluon plasma equations, say \u2014 then map those into the four-dimensional world, in which the equations tend to be much easier to solve. Then they could map the results back to the three-dimensional world and read off the answer. It worked. \"We turned the calculation on its head to give us a prediction for the value of the shear viscosity of a plasma,\" says Son, referring to a key parameter of the quark\u2013gluon fireball 2 . \"A friend of mine in nuclear physics joked that ours was the first useful paper to come out of string theory,\" he says. In 2008, the team's predictions were confirmed 3  at the Relativistic Heavy Ion Collider at Brookhaven National Laboratory in Upton, New York. \"These were strong quantitative results, and they still stand today as the best results achieved by the programme to relate string theory to experiment,\" says Steve Gubser, a string theorist at Princeton University, and one of the early champions of applying the principle to real-world problems. The team's success also caught the attention of Subir Sachdev, a condensed-matter theorist at Harvard. Just as Son had seen a plasma reflected back at him in Starinets' equations, Sachdev saw quantum critical-phase transitions \u2014 the changes of state that occur in materials when they near absolute zero, when quantum-mechanical effects begin to dominate. \"They were using different words,\" he says, \"but it was the same physics.\" Sachdev hoped that Maldacena's idea could provide him and his fellow theorists with some much-needed help in exploring this chilly realm. Over the decades, experimentalists had discovered a long list of exotic, quantum-dominated states \u2014 including superconductors that allow current to flow without resistance; superfluids that have no viscosity and can creep up the walls of beakers; Bose\u2013Einstein condensates made up of atoms moving in step like a single 'super atom'; and 'strange' metals that behave in ways subtly different from ordinary metals. But physicists still have no way to predict what will turn up in the lab next. \"We can't even answer the fundamental question of how many phases of matter exist,\" says Sean Hartnoll, a string theorist at Stanford University in California. \n               Click here for larger image \n               Sachdev's first efforts to apply Maldacena's idea to laboratory materials had resulted in two papers he co-authored in 2007, one with Son and his colleagues 4 , and another with a team that included Hartnoll 5 . Since then, Sachdev and his collaborators have built up a recipe for mapping the conductivity of strange metals into the properties of black holes in the string theorists' four-dimensional universe \u2014 a strategy that string theorist John McGreevy at the Massachusetts Institute of Technology in Cambridge 6  and others are also pursuing. These groups get answers that broadly reproduce the peculiar low-temperature behaviour of the metals. They have also mapped the behaviour of four-dimensional black holes in string theory to the conditions at which many materials will change phase into states other than the familiar solid, liquid and gas 7 . \"We now have a whole new hammer for attacking the problems I have been working on for 20 years,\" says Sachdev (see  'An unexpected link' ). Sachdev's involvement, in turn, has helped to ignite the interest of other condensed-matter physicists. \"A lot of us got into this field because of the force of Subir's personality and his reputation \u2014 we realized that if he was taking this seriously, maybe we should too,\" says Andrew Green, a condensed-matter physicist at the University of St Andrews, UK, who co-organized a workshop on the correspondence at Imperial College London in January. The condensed-matter results also got the string theorists excited \u2014 eventually. The field had been generally unenthusiastic about following up on the quark\u2013gluon plasma calculations, says Clifford Johnson, a string theorist at the University of Southern California in Los Angeles. And at least part of the reason, he suspects, was a bias against sullying string theory's purity. \"There was a snobbery among some towards what was termed 'mere applications',\" he says. But in 2006, string theory took a public battering in two popular books:  Not Even Wrong   by Peter Woit, a mathematician at Columbia, and  The Trouble With Physics   by Lee Smolin, a physicist at the Perimeter Institute for Theoretical Physics in Waterloo, Canada. Both books excoriated the theory's isolation from experiment. \"It's hard to say whether the interest in condensed-matter applications is a direct response to those books because that's really a psychological question,\" says Joseph Polchinski, a string theorist at the Kavli Institute for Theoretical Physics in Santa Barbara. \"But certainly string theorists started to long for some connection to reality.\" The condensed-matter partnership seemed perfect for that. If nothing else, it promised to make a virtue out of string theory's embarrassment of riches \u2014 the roughly 10 500  solutions to its basic equations, each of which describes a possible universe with its own size, shape, dimensionality and physical laws. Through Maldacena's idea, says string theorist Jerome Gauntlett at Imperial College London, \"each solution can be expressed in the countless materials yet to be discovered\". The rewards are mutual, says Zaanen. \"If I talk about superconductors and black holes in a colloquium, folk are attracted to it like bees to honey,\" he says. \"It's now bringing young blood to condensed-matter physics, as their first choice.\" The flurry of workshops promoting the partnership have been very productive, agrees Polchinski. He co-organized a meeting at the Kavli Institute last year that sparked seven new collaborations, and he is running another from August through November of this year. \"It is unique to try to actively make two groups of physicists collaborate so quickly \u2014 I haven't personally seen any other similar drive over the course of my career,\" says Gauntlett, who co-organized the London workshop. Just as with the fictional odd couple, however, this partnership still has plenty of friction. Everyone agrees, for example, that condensed-matter physicists are much more hesitant about pairing up than their string-theory counterparts. \"I have been remarkably unsuccessful at getting condensed-matter physicists to let string theorists speak at their big meetings,\" says Zaanen. \"They fear that they will need to learn string theory to talk to them. It's as though I am asking them to have coffee with aliens.\" Polchinski admits that the condensed-matter sceptics have a point. \"I don't think that string theorists have yet come up with anything that condensed-matter theorists don't already know,\" he says. The quantitative results tend to be re-derivations of answers that condensed-matter theorists had already calculated using more mundane methods. To make matters worse, some of the testable predictions from string theory look a tad bizarre from the condensed-matter viewpoint. For example, the calculations suggest that when some crystalline materials are cooled towards absolute zero, they will end up in one of many lowest-energy ground states. But that violates the third law of thermodynamics, which insists that these materials should have just one ground state. \"That's the gorilla in the room that should be keeping people awake at night,\" says Gubser. To win over sceptics, theorists are busily searching for testable predictions that will lead to killer evidence that the collaborations are worthwhile. Gauntlett's group, and others, are hunting for black-hole configurations in the string-theory universe that map to undiscovered phase transitions 8 . The trick is to figure out which materials might exhibit those transitions. \"Right now that involves going round asking, 'have you seen something like this?',\" says Gauntlett. \"But the hope is that as techniques advance, experimenters will be able to engineer materials with the properties we predict.\" Sachdev is applying string theory to an existing challenge: calculating how conductance should change with temperature as ultracold atoms transition from a superfluid state to an insulating one 7 . He thinks that it should be possible to test his predictions in the next couple of years. Even if the programme is successful, there are limits to how much the relationship can benefit either partner. String theory can offer a handbook of properties to look for, and predictions for how they should change in experiments, says Green. But it will never be able to provide a theory of how these properties emerge from the behaviour of electrons. Similarly, experimental verification of string theory's predictions about condensed matter will not prove that strings themselves are an accurate description of reality. But perhaps, Green argues, the connection to materials will show that people have fundamentally misunderstood what string theory is. \"Maybe string theory is not a unique theory of reality, but something deeper \u2014 a set of mathematical principles that can be used to relate all physical theories,\" says Green. \"Maybe string theory is the new calculus.\"\n Zeeya Merali is a freelance writer based in London. \n                     Nature Materials \n                   \n                     Jan Zaanen \n                   \n                     Subir Sachdev \n                   \n                     Jerome Gauntlett \n                   Reprints and Permissions"},
{"file_id": "476270a", "url": "https://www.nature.com/articles/476270a", "year": 2011, "authors": [{"name": "Alison McCook"}], "parsed_as_year": "2006_or_before", "body": "For decades, the Armed Forces Institute of Pathology has been a leader in disease diagnosis. Now it is closing, and its legacy is in jeopardy. The news took William Travis's breath away. The e-mail he had just opened revealed that the organization where he had worked for nearly 15 years \u2014 the Armed Forces Institute of Pathology (AFIP) in Washington DC \u2014 was to close. \"I had cold chills,\" recalls Travis, now a thoracic pathologist at the Memorial Sloan-Kettering Cancer Center in New York City. \"I was shocked. I thought, 'How could an institution like this be allowed to go away?' I had trouble breathing.\" That was in the spring of 2005, just five months after Travis had left the institute. The storied medical centre had become a casualty in a wave of cost-cutting closures and consolidations at the US Department of Defense. And on 15 September, less than a year shy of its 150th birthday, the AFIP will shut its doors for good. In its long history, the AFIP has become a stalwart of the international biomedical community. With its vast library of tissue samples and expertise at analysing tissues for the diagnosis of disease, it has been a valuable resource for researchers and clinicians alike. Every year, the AFIP received at least 50,000 requests for second opinions on difficult cases from external pathologists. The nearly 800 employees \u2014 including experts in many areas of human and animal pathology \u2014 made major or minor changes to roughly half of the cases they acted on. The military has yet to decide whether academic scientists will continue to have access to the AFIP's unique tissue repository. The largest in the world, it holds 55 million glass slides, 31 million paraffin blocks and more than 500,000 wet tissue samples. Scientists had only just begun to apply modern molecular techniques, such as DNA and RNA sequencing, to the collection. \"That repository is an international treasury. And that has got to be available to the community at large,\" says John Madewell, a former AFIP radiologist now based at the M. D. Anderson Cancer Center in Houston, Texas. So even as scientists mourn the loss of the institution, many are wondering what will become of the fruits of its labours. \"I am very concerned,\" says Madewell.  \n                Humble beginnings \n             From the start, the AFIP had a mission that extended beyond military strategy. In the spring of 1862, three dried and varnished tissue specimens were placed on a shelf in the Washington DC office of Brigadier General William Hammond, the Army surgeon-general. It was the beginning of the Army Medical Museum ([slideshow 1 left] 'A diary of death & disease'). Hammond wanted to collect and catalogue the specimens that had been accumulating from men fighting in the American Civil War. He wanted a collection that would \"embrace all forms of injuries and diseases, so that eventually it would become a general pathological museum, accessible for study to all medical men who are prosecuting original inquiries\". The museum became increasingly important to the military during the two world wars, says Michael Rhode, an archivist and one of the few remaining AFIP employees, as soldiers came back with strange new diseases such as gas gangrene, in which bacteria produce tissue-killing toxins inside the body. The army decided it needed a central location to collect and learn from unique cases and in 1944, the museum established the Army Institute of Pathology. Five years later, the institute became the central laboratory of pathology for all branches of the armed forces, and adopted its current name. Rhode says that \"there was a strong tradition of involving civilians\". It was in the military's interests to collect difficult cases from around the world, to learn more about the diseases troops could face. As the AFIP didn't initially charge for second opinions on difficult cases, the samples flooded in. The institute's final home, on 16th Street in Washington DC, was built in the 1950s to accommodate the growing repository and the pathologists who wanted to work at the AFIP, drawn to the collection and the stream of unusual cases. Within the first five years in the new building, AFIP scientists conducted more than 200 investigations of misunderstood diseases. AFIP pathologists also began producing the  Atlas of Tumor Pathology , a set of frequently updated volumes that are considered \"bibles\" of the field, says Chris Kelly, a former spokesperson for the AFIP. \"There is probably not a bookshelf in a pathology office that does not contain at least one of these.\" But many clinicians would say that the AFIP had the most impact with its consultation service, in which resident experts provided a second opinion on cases submitted by external pathologists in which, for whatever reason, the original diagnosis was uncertain. For instance, says former AFIP pathologist Susan Abbondanzo, two high-grade lymphomas \u2014 lymphoblastic and Burkitt's lymphoma \u2014 can be difficult to distinguish using the typical tissue-staining method used by most pathologists. But each has a very different treatment, so an incorrect diagnosis could kill the patient. The AFIP could routinely diagnose these cases, relying on experts in conjunction with a molecular and immunohistochemical laboratory, which many small hospitals lack. \"I'm not going to say every diagnosis is life-saving,\" says Abbondanzo. \"But many of the things you see there are, potentially.\"  \n                A pathologist's goldmine \n              When pathologists extol the value of the AFIP, the conversation always returns to the tissue repository \u2014 all told nearly 90 million tissue samples, including some of the most rare and difficult cases ever encountered in the history of the field. A general pathologist might see one or a few cases of a rare tissue abnormality during a career. The repository often contains many, enabling researchers to categorize the diseased tissues and standardize diagnosis. Abbondanzo joined the AFIP's department of haematopathology in 1990, and the first research project she collaborated on was investigating whether the anti-seizure drug Dilantin caused tumours in lymph nodes. \"It was a huge concern,\" she says. Abbondanzo had been a pathologist for six years, but had never seen a case of enlarged lymph nodes in people taking Dilantin. When she arrived at the AFIP, there were 25, which allowed Abbondanzo and her colleagues to determine that the vast majority of cases were benign, not cancerous \u2014 showing there was no association between the drug and the cancer 1 . And in the 1990s, Jeffery Taubenberger (then chair of the AFIP's department of molecular pathology) and his colleagues began applying molecular techniques to the repository's paraffin-embedded tissue blocks containing lung samples from soldiers killed by Spanish flu during the 1918 pandemic 2 . From these samples, among others, they were able to analyse the genome of the virus and investigate why it was so deadly. They traced its virulence to multiple genes, and found that it triggers a dramatic inflammatory response.  \n                Out of favour \n              Abbondanzo, who became the chair of the haematopathology department in 1994, began to suspect that the AFIP was in trouble during the first US war with Iraq in the early 1990s, when the Department of Defense began to take a closer look at its budget. Suddenly, research projects that would have been approved and encouraged in the past were being questioned. Abbondanzo and her colleagues had submitted a proposal to study follicular lymphoma, a type of cancer that is rare in children. The AFIP repository contained at least 20 childhood cases, providing an unprecedented opportunity to characterize the condition. But the board that approved proposals rejected the project, saying it had \"no military relevancy\", Abbondanzo says. During this same period, she attended a meeting at which someone from the department referred to the AFIP as an \"obscure little agency\". Travis, too, was worried about the AFIP's future. In 2003, he and others asked pathologists from overseas to write letters urging the military to continue supporting the AFIP, and he worked with the office of Senator Edward Kennedy (Democrat, Massachusetts), a long-time supporter of the institute, which tried to get financial support from other agencies. \"I spent countless hours,\" Travis says. \"All those efforts were not going anywhere. It was very clear I had to look out for myself and my family.\" When a position opened at Memorial Sloan-Kettering, he applied. Despite rumours that some services might be cut, the closure announcement in 2005 came as a shock. \"We learned of it when everyone else did,\" says Kelly. Rhode, who has worked at the AFIP since the 1980s, heard the news in an auditorium full of other staff. \"I don't remember people talking much. It seemed like we kind of just spilled out and went back to our offices in silence.\" The AFIP was on a list of base closures and changes designed to save more than US$30 billion over 20 years. But the institute's budget in fiscal year 2004 was only $93 million \u2014 slightly more than 0.02% of the defence department's budget request for 2004 of $380 billion. The military had welcomed civilian pathology cases for decades. Clearly, something had changed. A spokesperson for the Department of Defense said the decision to \"disestablish\" the AFIP was \"based on capacity, military value and scenario development after analysis of military and non-military workload, services available in the civilian sector and cost savings\". What really caused the demise of the AFIP, speculates Abbondanzo, is that the vast majority of its work had limited direct benefit to the military. \"It's been a long time coming,\" she says.  \n                Life after death \n              Not everything will disappear. A new entity, the Joint Pathology Center, has been created in Silver Spring, Maryland, to carry on the AFIP's military duties, including consulting on pathology cases for the military and other federal agencies. The AFIP's museum, the National Museum of Health and Medicine, which includes such exhibits as the bullet that killed US President Abraham Lincoln, is being packed up and moved to Fort Detrick's Forest Glen Annex in Silver Spring \u2014 a \"stressful\" process, says Rhode. And Madewell has helped to move the radiological pathology training that the AFIP provided for the majority of US radiology residents to the American College of Radiology based in Reston, Virginia. A 2007 report by the US Government Accountability Office concluded that the change in the AFIP's consultation and other services would have \"minimal impact\", because there are alternative sources of pathology expertise. In recent decades, some centres \u2014 such as the Mayo Clinic in Rochester, Minnesota, and Johns Hopkins University in Baltimore, Maryland \u2014 have developed strengths in particular fields of pathology, and have become sources for second opinions in difficult cases, says Colonel Vernon Armbrustmacher, one of the AFIP's former directors. \"The world of pathology will survive without the AFIP,\" says Armbrustmacher. \"I hate to say that, but it will.\" Even so, these services could be more expensive than those of the AFIP, which charged from around $20 to just over $2,000 for consultations, depending on the procedure. Some pathologists will feel the hit more than others \u2014 especially those in developing countries, who frequently lack local services to diagnose emerging tropical diseases and can't afford to pay high fees. These were a minority of cases \u2014 perhaps between 5% and 10% of consultations, estimates Armbrustmacher. But \"they were unbelievable cases\", Travis recalls. \"Really challenging, difficult problems.\" And even when pathologists can afford second opinions from academic centres, what they receive may not be of the same calibre as what they would have got from the AFIP, says Timothy O'Leary, director of Clinical Science Research and Development at the US Veterans Health Administration in Washington DC, and a pathologist at the AFIP until 2004. \"There are cases that are just not common. And at the AFIP, we saw a lot of those unusual cases, far more than you do at academic centres.\" It may only be \"a few hundred, a thousand people each year\" whose diagnoses might be significantly affected, he estimates. \"But for those people, it can be a matter of life and death.\" The fate of the tissue repository, which is now under the control of the Joint Pathology Center, remains unknown. It has been moved to two renovated buildings at Forest Glen Annex, one of which used to serve as the laundry facility for the AFIP and the Walter Reed Army Medical Center, which is also being relocated this year. Officials have asked the Institute of Medicine to recommend how best to use the repository, including who should have access to it, says the Joint Pathology Center's interim director Colonel Thomas Baker. Those recommendations are due in June 2012. What's most important, many say, is that civilian pathologists continue to have access to the repository. New techniques in high-throughput genomics and proteomics could \"now or soon\" reveal even more clues about the deadly diseases preserved there, says Taubenberger, now at the National Institute of Allergy and Infectious Diseases in Bethesda, Maryland. The repository \"is only of value to people who know what's in it\", says Travis. If the top pathologists can't access the material, an untold amount of valuable knowledge will never be uncovered, he says. Inside the AFIP, the mood is decidedly sombre as people watch the lights go out, says Rhode. \"Every day, you see a little bit more furniture moving down the hallway.\" Little by little, more offices are emptied, cleaned, locked and fitted with 'do not enter' signs. Florabel Mullick, the last director of the institute, declined to be interviewed for this article. But in the AFIP's final newsletter last winter, she lamented watching the institution become a \"shell of its former self\" and wrote that closing the AFIP \"has been one of the most painful experiences of my life \u2026 How does one, after all, watch missions that have benefitted so many people fade away?\" \"It is the passing of one of the greatest institutions in the entire history of medicine,\" agrees Travis. \"And it is very painful to see that happen. But we all have to move on.\" Alison McCook is a Comment editor at Nature. \n                     Modern Pathology \n                   \n                     Armed Forces Institute of Pathology \n                   \n                     Joint Pathology Center \n                   \n                     National Museum of Health and Medicine \n                   Reprints and Permissions"},
{"file_id": "476387a", "url": "https://www.nature.com/articles/476387a", "year": 2011, "authors": [{"name": "Lizzie Buchen"}], "parsed_as_year": "2006_or_before", "body": "For more than 20 years, Brian Kobilka worked to create a portrait of a key cell receptor. Sometimes, the slow, steady approach wins. Brian Kobilka was exhausted when he stepped off the 12-hour red-eye flight from China to San Francisco, California, last May. But after a quick nap at home, he headed straight back to the airport and crammed his long frame into another plane, this time bound for Chicago, Illinois. Once there, he drove to the Advanced Photon Source at Argonne National Laboratory, a source of powerful X-ray beams used for analysing protein structures. Kobilka, a biochemist at Stanford University in California, was desperate to see the latest data from his lab's effort to make the first atomic-scale, three-dimensional image of a key cell-surface receptor locked with its protein partner. The image marked the last leg of an intellectual journey that he had started some 20 years before. Nearly every function of the human body, from sight and smell to heart rate and neuronal communication, depends on G-protein-coupled receptors (GPCRs). Lodged in the fatty membranes that surround cells, they detect hormones, odours, chemical neurotransmitters and other signals outside the cell, and then convey their messages to the interior by activating one of several types of G protein. The G protein, in turn, triggers a plethora of other events. The receptors make up one of the largest families of human proteins and are the targets of one-third to one-half of drugs. Working out their atomic structure will help researchers to understand how this central cellular-communication system works, and could help drug-makers to design more effective treatments. The structure and workings of GPCRs have been overriding obsessions of Kobilka for most of his professional life. For much of this, he had little company, as the proteins were widely considered too complex and unwieldy to be stabilized as crystals, a prerequisite for structural analysis by X-ray crystallography. But his determination has finally started to pay off. In 2007, his team solved the first high-resolution structure of a GPCR that binds to a hormone 1 \u2013 3 ; in January this year he did the same for the receptor poised to activate its G-protein 4 ; and last month, he published the results from Chicago, the first structure of any GPCR in the act of turning on its G protein 5 . The latest accomplishment has many in the field buzzing about a Nobel prize. \"But if they do give it to him, it'll be the devil to get him to Stockholm,\" says Henry Bourne, a professor emeritus at the University of California, San Francisco, who worked on G proteins and has known Kobilka since the 1980s. Kobilka loathes the limelight, and is renowned as much for his shy modesty as his ability to crack seemingly impossible protein structures. When I met Kobilka at his Stanford office, he accepted a quick handshake, averting his eyes, and reluctantly offered me a seat across from him at his desk. He stared anxiously at the glowing red light on my voice recorder. He is \"desperately fearful\" of talking to the press, he says, his voice breaking, and only agreed to talk because he \"wanted to make sure that the contributions of my lab and collaborators are recognized\". He is so fearful, in fact, that it is almost impossible to draw much out from him. When asked why he is so captivated by GPCRs, Kobilka struggles for an answer. \"I'm just inherently fascinated by these proteins. I don't know. I just want to know how they work,\" he says. The only time he becomes animated is when describing the conformation of the receptor gripping its G protein. \"It's a fantastic structure,\" he says, a grin across his face. \"It's just amazing. I really enjoy talking about it.\" Bourne says that \"Brian is a fascinating character. He's so driven, and enormously intense. But you don't get any feeling of \u2014 nor is there any \u2014 self-aggrandizing, pushiness, show-offiness. There is absolutely none of that. It's quite refreshing and rare.\" Perhaps Kobilka's rare reticence and determination are exactly what make him so well-suited for the task of protein crystallization \u2014 a pursuit in which a complete atomic structure is the finish line, and there are few intermediate rewards. \"What he did was, step by step, sneak up on it to make it work,\" says Bourne.  \n                Baking to biology \n              Kobilka is from a small, rural community in central Minnesota. His grandfather and father were bakers; his mother decorated the cakes. Kobilka studied biology and chemistry at the University of Minnesota in Duluth, where he met his future wife, Tong Sun Thian, in biology class. \"He always topped the curve,\" she says. \"But he was also so modest and quiet, you'd never know.\" Kobilka got his first taste of research when he and Tong Sun worked in a developmental biology lab at the university, and built a tissue-culture hood for the lab using scrap heavy plastic from his father's bakery. He enjoyed research, but went to medical school anyway \u2014 in rural Minnesota, he says, people who were interested in biology became doctors. Studying at Yale University in New Haven, Connecticut, Kobilka developed an interest in intensive-care medicine and the drugs used in life-or-death situations that act on GPCRs \u2014 particularly the receptors for adrenaline and noradrenaline, which open the airways and boost heart rate. In 1984, after his residency, Kobilka applied for a fellowship with Robert Lefkowitz at Duke University in Durham, North Carolina. It was the premier lab studying receptors for adrenaline, which had become a model system for all hormone receptors. When Kobilka joined, the lab was just starting to think about how to clone the gene for the \u03b2 2 -adrenergic receptor (\u03b2 2 AR) and determine its genetic sequence. But the receptor was produced in such small amounts that the team was only able to collect enough protein to work out a few scraps of its likely genetic sequence. Kobilka then displayed \"the first of many flashes of technological innovation\", says Lefkowitz: he decided to construct a library of mammalian genomic sequences and screen it with the scraps of sequences they had. This would pull out longer clones that could be pieced together to reveal the full sequence. The plan worked. And when the team stitched together the receptor sequence 6 , it held a surprise: several strings of amino acids that are typically found in cell membranes showed that the receptor snaked through the membrane seven times. It was just like rhodopsin, the light-detecting receptor in the retina that was also known to activate a G protein. At the time, there was no reason to think that these receptors were going to look the same \u2014 especially when one was turned on by light, and the other by a hormone. \"It was a real eureka moment,\" recalls Lefkowitz. At the time, about 30 proteins were known to turn on G proteins. \"We realized, oh my god, they're all going to look like this. It's a whole family!\" That family became known as seven-transmembrane receptors, or GPCRs \u2014 and is now known to have nearly 800 members in humans. Kobilka describes the watershed cloning project with humility. \"It was exciting just to be involved,\" he says. \"It was the result of a lot of heroic work by the Lefkowitz lab.\" Sometimes that self-effacement has held Kobilka back. After he left Duke, he was interviewed for a faculty position in the pharmacology department at the University of California, San Francisco. \"We had him and another guy,\" says Bourne, who was then head of the department. \"It was very clear to everyone that Brian was smart, no question. But he didn't sparkle in the slightest. He was this shy, pale, Scandinavian-looking guy. I thought, who is this guy? He's very strange, so modest, so quiet.\" The other guy got the job, and Stanford snapped up Kobilka, says Bourne. \"We should have hired both of them.\" After his success with \u03b2 2 AR, Kobilka was hooked. He wanted to see what the receptor looked like in three dimensions using X-ray crystallography, in which a beam of X-rays is fired at a protein crystal and the resultant diffraction pattern is used to reveal the arrangement of its atoms. It was an audacious goal. To produce an intelligible X-ray diffraction pattern, Kobilka first needed to crystallize the receptor \u2014 a formidable process of packing millions of identical copies of protein so tightly that they form a solid that looks like a microscopic shard of glass. Working out the conditions that will allow a protein to crystallize can take years, and membrane proteins such as GPCRs are the hardest of all: they must be coaxed out of the membrane intact, but it is the membrane that holds them in shape. GPCRs are also constantly shifting into various states, and most are expressed in very low quantities. To collect enough protein, Kobilka would need to express the \u03b2 2 AR at about 100\u20131,000 times the levels at which it is normally produced in a cell. At that time, no one was close to crystallizing a GPCR and few were trying. Crystallizing the receptor became Kobilka's pet project because he thought it was too high risk for a postdoc or graduate student. \"We used to joke that he'd come running into the lab from teaching or a meeting with a column in his hand, trying to do a binding assay to see if his latest purification had worked before he went home,\" says Mark von Zastrow, one of Kobilka's first postdocs, who now studies GPCR trafficking at the University of California, San Francisco. Kobilka had reason to rush through the workday \u2014 he had two young children at home, Tong Sun was at medical school and they had a \"tremendous\" mortgage. To make ends meet, he moonlighted as a doctor in the emergency department at weekends. Von Zastrow recalls poking fun at Kobilka for his seemingly futile crystallography project. \"He told us, 'You'll see. The crystals are going to be so big, I'm going to make a ring for Tong Sun out of them.'\" As the years rolled by, Kobilka's lab was carrying out various biochemistry and biophysics experiments aimed at getting to know the \u03b2 2 AR more intimately, and he was inching forwards in expressing and purifying the protein. But he wasn't noticeably closer to getting the structure. \"People viewed what he was doing as dotting i's and crossing t's,\" says Bourne. \"And for a while it was sort of that. He wasn't getting published in fancy journals.\" The team showed that GPCRs have big, floppy loops inside and outside the cell, and that the receptor writhes and squirms, adopting a variety of levels of activation 7 . The work only made crystallization look more impossible. In the meantime, another GPCR crystallography effort raced ahead. In 2000, Krzysztof Palczewski and his postdoc Tetsuji Okada, then at the University of Washington in Seattle, published the crystal structure of rhodopsin, the light-sensing GPCR in the retina 8 . It was a significant accomplishment, but of little help to Kobilka. Rhodopsin is plentiful \u2014 a bucketful of cow eyes from the slaughterhouse gives enough pure protein for crystallography. It is also simpler and more stable than other GPCRs, and its structure was thought to be different. In 2001, Kobilka got disheartening news. His main funding, from the Howard Hughes Medical Institute in Chevy Chase, Maryland, would not be renewed after it ran out in 2003. His lab began to struggle financially, and went \"deep into the red\" to support his expensive crystallization crusade. \"I don't think I ever considered giving up,\" says Kobilka. \"I admit that it was frustrating at times, but I enjoyed the challenge and I wanted to know the answer.\" Kobilka says that one of his friends \"best described my persistence as 'irrational optimism'\". Finally, in late 2004, his group managed to grow tiny crystals \u2014 too small to be analysed at Stanford's synchrotron facility. Gebhard Schertler, a crystallographer then at the Medical Research Council Laboratory of Molecular Biology in Cambridge, UK, suggested that Kobilka take his samples to the European Synchrotron Radiation Facility (ESRF) in Grenoble, France, which had the tightly focused beamline needed to analyse such small crystals. \"We were running out of money all the time,\" says Schertler, now at the Paul Scherrer Institute in Villigen, Switzerland. \"The measurements were all on my grants.\" The crystals diffracted to a resolution of around 20 \u00e5ngstr\u00f6ms \u2014 so low that there was no discernible image. A resolution of about 4 \u00c5 is needed to see the organization of individual atoms. Still, says Kobilka, \"it was very exciting. In part it was because I was very naive. I thought we'd be able to get to 3 \u00c5 in no time.\" He finally had the confidence to hire postdocs for the crystallography project. In 2005, he also received a financial lifeline, winning two funding streams from the US National Institutes of Health in Bethesda, Maryland. But frustration followed \u2014 the team couldn't get the crystals to grow any bigger or diffract any better. The receptor's changeable activation states and floppy segments \u2014 particularly one restless loop on the intracellular side of the receptor \u2014 made it very difficult to trap all the proteins in an identical conformation. The team realized that they'd have to do something radical: chop off the loose ends, and either anchor the loop in place with an antibody or replace it altogether with a protein known to crystallize well. The antibody project, led by postdoc S\u00f8ren Rasmussen, came together first. As before, they first took the crystals to Schertler, at the ESRF. \"It was the greatest thing,\" says Schertler. \"Brian and I and our team were at the synchrotron. We were sitting in front of the machine when the measurements came in. When we first saw the pattern for 3.5 \u00c5, everyone in the room jumped up. It was a very happy moment. That's what makes scientists go. That moment, when we know we've found a new continent.\" The structure, published in  Nature 1 , was the second crystal structure of a GPCR, after rhodopsin. Scientists, however, reserve their accolades for the fusion-protein project, which wasn't far behind. Postdoc Daniel Rosenbaum found that one protein \u2014 T4 lysozyme (T4L) \u2014 looked promising for fusing to the receptor in place of the loop. And Kobilka had got in touch with Ray Stevens at the Scripps Research Institute in La Jolla, California, and his new postdoc Vadim Cherezov, who had been optimizing a fatty scaffold to lock the membrane proteins in place for crystallization. T4L and the fatty scaffold turned out to be the winning combination. Just days after the  Nature   paper, Kobilka and Stevens published back-to-back papers in  Science 2 , 3  that solved the structure of the engineered \u03b2 2 AR to 2.8 \u00c5. The trio of papers marked a milestone in structural biology, and sharply intensified the competition in what was by now a fast and aggressive field. Stevens became one such competitor, and his lab is now powering through further GPCR structures. But Kobilka's heart was set on a different goal. The GPCR structures had been snapshots of receptors in an inactive state. To really understand the receptor's workings, researchers needed to see it as it was being activated by a ligand and turning on the G protein. This project was even more technically daunting than the last. The protein complex was too big to hold in the fatty scaffold; the G protein kept falling off; and this time, the extracellular part of the receptor wouldn't sit still for crystallization. \"This was difficult enough that I wasn't sure we were ever going to get it,\" says Kobilka. \"I thought it might be my retirement project.\" He also knew that several other labs, particularly those studying rhodopsin, were breathing down his neck. Kobilka reached out to all manner of experts for help, including Roger Sunahara, an expert on G proteins at the University of Michigan in Ann Arbor. The various groups developed a detergent for stabilizing the receptor with its G protein; a lipid scaffold that could support the complex; and an antibody that could hold it together. And Rasmussen was relentless, testing thousands upon thousands of crystallization conditions and ways to engineer the protein. One morning last May, Kobilka took a quick peek down the microscope at Rasmussen's latest effort to produce crystals. \"They were already bigger than any other crystals we'd grown of the complex,\" says Kobilka. \"It was extremely exciting. I didn't know if they would diffract well or not, but I had a good feeling.\" But a planned trip to China meant that he couldn't be there for the first X-ray images. \"As soon as we got to the hotel in Beijing, he got online,\" says Tong Sun. When he found out it was solved, \"he was on cloud nine. You could tell he just wanted to go back.\" The new picture, solved to a resolution of 3.2 \u00c5, reveals a tangled molecular threesome: \u03b2 2 AR with a ligand clasped at one end and the G protein nested up on the other. \"There's definitely been a race, and in my opinion, Kobilka has triumphed,\" says Chuck Sanders, a structural biologist at Vanderbilt University in Nashville, Tennessee. \"Hopefully the field will spread out a little. This complex was the prize, and that's been done now.\" Kobilka, of course, diverts credit to his collaborators and the \"unsung heroes\" of his lab, anxious that everyone should see their names in this story. His allegiance to the receptors is as resolute as ever. \"The more we learn\" about these proteins, he says, \"the more complicated and fascinating they are\". He is already working to understand more of the complications: what the various active states of the receptor look like, why different receptors couple to different G proteins and what happens when different ligands bind to the same receptor. He is also using other techniques, such as electron microscopy and nuclear magnetic resonance, to understand how GPCRs flex and move. \"The job isn't done yet,\" he says. Maybe not \u2014 but few would doubt now that Kobilka will finish it. \"Brian ultimately reaches his goals,\" says Lefkowitz. \"Sometimes it takes 15 years, but he gets there.\" Lizzie Buchen is a freelance writer based in San Francisco, California. \n                     Brian Kobilka's lab \n                   Reprints and Permissions"},
{"file_id": "476391a", "url": "https://www.nature.com/articles/476391a", "year": 2011, "authors": [{"name": "Naomi Lubick"}], "parsed_as_year": "2006_or_before", "body": "Some of the most powerful earthquakes emanate from remote ocean-floor faults. Geophysicists are now laying networks of sensors to keep tabs on these hidden killers. Japanese seismologists have been worried about a sea-floor fault for years. The Nankai trough off the nation's southeast coast has produced some of the most devastating earthquakes in Japanese history and is considered ripe for another. So, earlier this year, researchers went out in the ship  KAIYO   to install underwater seismic observatories that should reveal more about the fault and provide seconds of warning when the next big quake hits. In March, the ship was just depositing a batch of the sensors when the massive Tohoku-Oki earthquake broke a completely different fault, 800 kilometres to the northeast, triggering a tsunami that crippled coastal communities. Seismologists had not thought that the Japan trench off the Tohoku coast was capable of generating such giant earthquakes, in part because they did not have enough devices on the sea floor to catch signs of the building stress. The same dearth of data exists elsewhere, wherever one tectonic plate rams into another and slides beneath it. Such subduction zones generate the planet's most powerful earthquakes, including the biggest ever recorded, a magnitude-9.5 monster off the coast of Chile in 1960. In 2004, a subduction-zone quake off the Indonesian island of Sumatra triggered a tsunami that killed more than 230,000 people. And researchers forecast that a subduction zone bordering the northwestern coast of the United States is building towards a magnitude-9 shock that could happen within the next century. The trouble for seismologists is that these major faults sit hundreds of kilometres from land and buried by thousands of metres of water, where it is difficult to place and maintain observatories equipped with the seismometers, global-positioning-system units and other instruments that can reveal the structure of the faults and detect changes such as warping of the crust. \n               Click here for larger image \n               Japan has just 50 observatories offshore to watch its sea-floor faults, compared with more than 8,700 on land. Other nations are even less prepared, with few or no sensors on the sea floor, where the most dangerous parts of subduction zones lie. Instead, they rely on measurements taken by stations on land \u2014 which provide a distant and muffled reading, much like that obtained by a cardiologist who places a stethoscope over a patient's shoe in a fruitless attempt to monitor the heart. Researchers are now trying to get much closer to the action. The Japanese effort to wire up the Nankai trough is the most ambitious project so far, but the United States and Canada have programmes to monitor the Cascadia subduction zone, which runs from northern California to British Columbia (see  'Sensors on the sea floor' ). With better data, geophysicists hope to improve their understanding of how subduction zones work and possibly identify signs of impending disasters. \"Without ocean-bottom measurements, we're always guessing,\" says Emma Hill, a geodesist at the Earth Observatory of Singapore who is studying quake risks off Indonesia.  \n                Faulty understanding \n              Subduction zones are the recycling centres in the theory of plate tectonics, which describes the movement of the great oceanic and continental plates that make up Earth's brittle outer shell. When two plates collide, the cold, dense ocean crust sinks, and the plate with more-buoyant, crustal rock rides up over it. But the cartoon-like model \u2014 a conveyer belt with slabs of ocean rock diving beneath sheet-like continents above \u2014 is an overly simplified picture. \"It's going to be complex,\" says Hill. \"And we're still modelling it as a nice smooth plane.\" Geophysicists want to know the details of what goes on where the two plates grind against each other. They suspect that the plates somehow become locked together, perhaps when seamounts or other rough features on the subducting plate catch on the underside of the upper plate. After many decades or centuries, the plates then spring free in a mammoth megathrust earthquake. In the case of the March earthquake in Japan, researchers had suspected that the boundary between the two plates was locked, but did not appreciate the risk because they lacked knowledge about the structure of the subduction zone and how stress was accumulating there. They were more worried about the Nankai trough area, which has a 70% chance of producing a magnitude-8 quake in the next 30 years, according to Japan's official hazard forecast. As part of the Dense Oceanfloor Network System for Earthquakes and Tsunamis (DONET), geophysicists will create a network of 20 underwater observatories in the region of the trough where earthquakes are thought to originate. The DONET project, started in 2006 and to be completed this year, ran to some \u00a56.3 billion (US$82 million), excluding ship time. The observatories contain seismometers that record vibrations emanating from quakes within the subduction zone as well as seismic waves from tremors across the globe \u2014 all of which should help to illuminate the geometry of the interface between the upper and lower plates. Pressure sensors track warping of the crust by measuring changes in the weight of the water column above. Telecommunications-grade cables connect the observatories with land stations, so researchers have access to the sensor data in real time. Yoshiyuki Kaneda, who is leading the DONET project for the Japan Agency for Marine-Earth Science and Technology, hopes that the observatories will capture a whole earthquake cycle \u2014 from the build up of stress to the release during a great earthquake, followed by the gradual reaccumulation of stress. Researchers hope to learn, for example, how major earthquakes start and what kind of activity precedes them. When a big quake does come, the DONET observatories should be close enough to provide early warning to Osaka, Tokyo and other cities that could soon be hit by devastating seismic waves. The pressure sensors can also give advance notice of a tsunami racing toward the coast.  \n                Western worries \n              US researchers have seismic worries of their own, focused on the Cascadia region. Great tremors and tsunamis have hit there before, and more than 300 years have passed since the last one. \"The seismic hazard from the Cascadian subduction zone is profound,\" says Maya Tolstoy of Columbia University's Lamont-Doherty Earth Observatory in New York. Tolstoy is one of the principal investigators in the Cascadia Initiative, a four-year-long programme that is installing temporary observation posts to learn about the behaviour of the massive offshore fault. Last month, the team began deploying the first of 60 such observatories on the sea floor that will enhance an existing network onshore. The project is funded in part by US$5 million from the American Recovery and Reinvestment Act of 2009, with support from the US National Science Foundation. Each ocean-floor device, which costs $60,000\u201380,000, contains a pressure gauge and a seismometer the size of a soup can, housed in a pressure case with levelling systems and protected from currents and fishing gear by a steel hood. These observatories will not be connected by cable to the mainland. Instead, researchers will retrieve each sensor and download its data once a year before moving it to a new spot. By pinpointing where earthquakes occur in the subduction zone, data from the Cascadia Initiative will help to resolve the location and structure of the interface between the plates \u2014 such as how rough it is and which areas remain locked, says Richard Allen of the University of California, Berkeley. The sea-floor observations should also help to decipher unusual seismic signals that have been recorded by land-based seismometers in the Cascadia area. The land sensors picked up swarms of tiny quakes occurring roughly every 12\u201314 months ( N. I. Gershenzon  et al. Geophys. Res. Lett.    38,   L01309; 2011 ) as well as seismic events that unfold so slowly that nobody can feel the shaking, even though they release substantial amounts of energy. Researchers suspect that the signals reflect activity within the subduction zone, perhaps layers within the plates moving separately or fluids migrating deep under the surface \u2014 and hope that recordings from the offshore sensors will help them to come up with an explanation. The sensors might also be able to detect changes in the locked parts of the subduction zone. The United States is also teaming up with Canada to install long-term sea-floor observation posts in the Cascadia area as part of the Neptune project, which will collect a wide variety of biological, oceanographic and seismic data both for basic research and for potential early warnings of hazards such as algal blooms or earthquakes. Canada is spending Can$143 million (US$145 million) on its part of the effort, and last year finished installing a system that includes three seismic stations, five sea-floor pressure recorders and other instruments, connected by an 800-kilometre loop of cable that transmits information to a land station. Budget problems delayed the US portion of Neptune, which began laying cable this summer for observatories to be established over the next few years. Because installing hundreds of kilometres of fibre-optic cable is expensive, researchers are working to develop autonomous seafaring robots that are able to collect the data. One proposed project would pair wave-powered robots developed by Liquid Robotics in Sunnyvale, California, with stand-alone sea-floor sensors. As readings pour in from the various networks of offshore sensors, researchers hope they can finally gain a more sophisticated understanding of subduction zones and the hazards they hold. Such ocean-bottom observations are \"going to be the story of the next few years\", says Hill. Although some projects were in the works before the March earthquake, that catastrophe has added urgency to the research. \"Because of Japan,\" says Hill, \"people's minds are on this right now.\" Naomi Lubick is a freelance writer in Stockholm, Sweden. \n                     Nature Geoscience \n                   \n                     Earth Sciences @ Nature \n                   \n                     DONET project \n                   \n                     Cascadia Initiative \n                   \n                     Canadian Neptune project \n                   \n                     US Neptune project \n                   Reprints and Permissions"},
{"file_id": "477023a", "url": "https://www.nature.com/articles/477023a", "year": 2011, "authors": [{"name": "Kerri Smith"}], "parsed_as_year": "2006_or_before", "body": "Scientists think they can prove that free will is an illusion. Philosophers are urging them to think again. The experiment helped to change John-Dylan Haynes's outlook on life. In 2007, Haynes, a neuroscientist at the Bernstein Center for Computational Neuroscience in Berlin, put people into a brain scanner in which a display screen flashed a succession of random letters 1 . He told them to press a button with either their right or left index fingers whenever they felt the urge, and to remember the letter that was showing on the screen when they made the decision. The experiment used functional magnetic resonance imaging (fMRI) to reveal brain activity in real time as the volunteers chose to use their right or left hands. The results were quite a surprise. \"The first thought we had was 'we have to check if this is real',\" says Haynes. \"We came up with more sanity checks than I've ever seen in any other study before.\" The conscious decision to push the button was made about a second before the actual act, but the team discovered that a pattern of brain activity seemed to predict that decision by as many as seven seconds. Long before the subjects were even aware of making a choice, it seems, their brains had already decided. As humans, we like to think that our decisions are under our conscious control \u2014 that we have free will. Philosophers have debated that concept for centuries, and now Haynes and other experimental neuroscientists are raising a new challenge. They argue that consciousness of a decision may be a mere biochemical afterthought, with no influence whatsoever on a person's actions. According to this logic, they say, free will is an illusion. \"We feel we choose, but we don't,\" says Patrick Haggard, a neuroscientist at University College London. You may have thought you decided whether to have tea or coffee this morning, for example, but the decision may have been made long before you were aware of it. For Haynes, this is unsettling. \"I'll be very honest, I find it very difficult to deal with this,\" he says. \"How can I call a will 'mine' if I don't even know when it occurred and what it has decided to do?\"  \n                Thought experiments \n              Philosophers aren't convinced that brain scans can demolish free will so easily. Some have questioned the neuroscientists' results and interpretations, arguing that the researchers have not quite grasped the concept that they say they are debunking. Many more don't engage with scientists at all. \"Neuroscientists and philosophers talk past each other,\" says Walter Glannon, a philosopher at the University of Calgary in Canada, who has interests in neuroscience, ethics and free will. There are some signs that this is beginning to change. This month, a raft of projects will get under way as part of Big Questions in Free Will, a four-year, US$4.4-million programme funded by the John Templeton Foundation in West Conshohocken, Pennsylvania, which supports research bridging theology, philosophy and natural science. Some say that, with refined experiments, neuroscience could help researchers to identify the physical processes underlying conscious intention and to better understand the brain activity that precedes it. And if unconscious brain activity could be found to predict decisions perfectly, the work really could rattle the notion of free will. \"It's possible that what are now correlations could at some point become causal connections between brain mechanisms and behaviours,\" says Glannon. \"If that were the case, then it would threaten free will, on any definition by any philosopher.\" Haynes wasn't the first neuroscientist to explore unconscious decision-making. In the 1980s, Benjamin Libet, a neuropsychologist at the University of California, San Francisco, rigged up study participants to an electroencephalogram (EEG) and asked them to watch a clock face with a dot sweeping around it 2 . When the participants felt the urge to move a finger, they had to note the dot's position. Libet recorded brain activity several hundred milliseconds before people expressed their conscious intention to move. Libet's result was controversial. Critics said that the clock was distracting, and the report of a conscious decision was too subjective. Neuroscience experiments usually have controllable inputs \u2014 show someone a picture at a precise moment, and then look for reactions in the brain. When the input is the participant's conscious intention to move, however, they subjectively decide on its timing. Moreover, critics weren't convinced that the activity seen by Libet before a conscious decision was sufficient to cause the decision \u2014 it could just have been the brain gearing up to decide and then move. Haynes's 2008 study 1  modernized the earlier experiment: where Libet's EEG technique could look at only a limited area of brain activity, Haynes's fMRI set-up could survey the whole brain; and where Libet's participants decided simply on when to move, Haynes's test forced them to decide between two alternatives. But critics still picked holes, pointing out that Haynes and his team could predict a left or right button press with only 60% accuracy at best. Although better than chance, this isn't enough to claim that you can see the brain making its mind up before conscious awareness, argues Adina Roskies, a neuroscientist and philosopher who works on free will at Dartmouth College in Hanover, New Hampshire. Besides, \"all it suggests is that there are some physical factors that influence decision-making\", which shouldn't be surprising. Philosophers who know about the science, she adds, don't think this sort of study is good evidence for the absence of free will, because the experiments are caricatures of decision-making. Even the seemingly simple decision of whether to have tea or coffee is more complex than deciding whether to push a button with one hand or the other. Haynes stands by his interpretation, and has replicated and refined his results in two studies. One uses more accurate scanning techniques 3  to confirm the roles of the brain regions implicated in his previous work. In the other, which is yet to be published, Haynes and his team asked subjects to add or subtract two numbers from a series being presented on a screen. Deciding whether to add or subtract reflects a more complex intention than that of whether to push a button, and Haynes argues that it is a more realistic model for everyday decisions. Even in this more abstract task, the researchers detected activity up to four seconds before the subjects were conscious of deciding, Haynes says. Some researchers have literally gone deeper into the brain. One of those is Itzhak Fried, a neuroscientist and surgeon at the University of California, Los Angeles, and the Tel Aviv Medical Center in Israel. He studied individuals with electrodes implanted in their brains as part of a surgical procedure to treat epilepsy 4 . Recording from single neurons in this way gives scientists a much more precise picture of brain activity than fMRI or EEG. Fried's experiments showed that there was activity in individual neurons of particular brain areas about a second and a half before the subject made a conscious decision to press a button. With about 700 milliseconds to go, the researchers could predict the timing of that decision with more than 80% accuracy. \"At some point, things that are predetermined are admitted into consciousness,\" says Fried. The conscious will might be added on to a decision at a later stage, he suggests.  \n                Material gains \n              Philosophers question the assumptions underlying such interpretations. \"Part of what's driving some of these conclusions is the thought that free will has to be spiritual or involve souls or something,\" says Al Mele, a philosopher at Florida State University in Tallahassee. If neuroscientists find unconscious neural activity that drives decision-making, the troublesome concept of mind as separate from body disappears, as does free will. This 'dualist' conception of free will is an easy target for neuroscientists to knock down, says Glannon. \"Neatly dividing mind and brain makes it easier for neuroscientists to drive a wedge between them,\" he adds. The trouble is, most current philosophers don't think about free will like that, says Mele. Many are materialists \u2014 believing that everything has a physical basis, and decisions and actions come from brain activity. So scientists are weighing in on a notion that philosophers consider irrelevant. Nowadays, says Mele, the majority of philosophers are comfortable with the idea that people can make rational decisions in a deterministic universe. They debate the interplay between freedom and determinism \u2014 the theory that everything is predestined, either by fate or by physical laws \u2014 but Roskies says that results from neuroscience can't yet settle that debate. They may speak to the predictability of actions, but not to the issue of determinism. Neuroscientists also sometimes have misconceptions about their own field, says Michael Gazzaniga, a neuroscientist at the University of California, Santa Barbara. In particular, scientists tend to see preparatory brain activity as proceeding stepwise, one bit at a time, to a final decision. He suggests that researchers should instead think of processes working in parallel, in a complex network with interactions happening continually. The time at which one becomes aware of a decision is thus not as important as some have thought.  \n                Battle of wills \n              There are conceptual issues \u2014 and then there is semantics. \"What would really help is if scientists and philosophers could come to an agreement on what free will means,\" says Glannon. Even within philosophy, definitions of free will don't always match up. Some philosophers define it as the ability to make rational decisions in the absence of coercion. Some definitions place it in cosmic context: at the moment of decision, given everything that's happened in the past, it is possible to reach a different decision. Others stick to the idea that a non-physical 'soul' is directing decisions. Neuroscience could contribute directly to tidying up definitions, or adding an empirical dimension to them. It might lead to a deeper, better understanding of what freely willing something involves, or refine views of what conscious intention is, says Roskies. Mele is directing the Templeton Foundation project that is beginning to bring philosophers and neuroscientists together. \"I think if we do a new generation of studies with better design, we'll get better evidence about what goes on in the brain when people make decisions,\" he says. Some informal meetings have already begun. Roskies, who is funded through the programme, plans to spend time this year in the lab of Michael Shadlen, a neurophysiologist at the University of Washington in Seattle who works on decision-making in the primate brain. \"We're going to hammer on each other until we really understand the other person's point of view, and convince one or other of us that we're wrong,\" she says. Haggard has Templeton funding for a project in which he aims to provide a way to objectively determine the timing of conscious decisions and actions, rather than rely on subjective reports. His team plans to devise an experimental set-up in which people play a competitive game against a computer while their brain activity is decoded. Another project, run by Christof Koch, a bioengineer at the California Institute of Technology in Pasadena, will use techniques similar to Fried's to examine the responses of individual neurons when people use reason to make decisions. His team hopes to measure how much weight people give to different bits of information when they decide. Philosophers are willing to admit that neuroscience could one day trouble the concept of free will. Imagine a situation (philosophers like to do this) in which researchers could always predict what someone would decide from their brain activity, before the subject became aware of their decision. \"If that turned out to be true, that would be a threat to free will,\" says Mele. Still, even those who have perhaps prematurely proclaimed the death of free will agree that such results would have to be replicated on many different levels of decision-making. Pressing a button or playing a game is far removed from making a cup of tea, running for president or committing a crime. The practical effects of demolishing free will are hard to predict. Biological determinism doesn't hold up as a defence in law. Legal scholars aren't ready to ditch the principle of personal responsibility. \"The law has to be based on the idea that people are responsible for their actions, except in exceptional circumstances,\" says Nicholas Mackintosh, director of a project on neuroscience and the law run by the Royal Society in London. Owen Jones, a law professor at Vanderbilt University in Nashville, Tennessee, who directs a similar project funded by the MacArthur Foundation in Chicago, Illinois, suggests that the research could help to identify an individual's level of responsibility. \"What we are interested in is how neuroscience can give us a more granulated view of how people vary in their ability to control their behaviour,\" says Jones. That could affect the severity of a sentence, for example. The answers could also end up influencing people's behaviour. In 2008, Kathleen Vohs, a social psychologist at the University of Minnesota in Minneapolis, and her colleague Jonathan Schooler, a psychologist now at the University of California, Santa Barbara, published a study 5  on how people behave when they are prompted to think that determinism is true. They asked their subjects to read one of two passages: one suggesting that behaviour boils down to environmental or genetic factors not under personal control; the other neutral about what influences behaviour. The participants then did a few maths problems on a computer. But just before the test started, they were informed that because of a glitch in the computer it occasionally displayed the answer by accident; if this happened, they were to click it away without looking. Those who had read the deterministic message were more likely to cheat on the test. \"Perhaps, denying free will simply provides the ultimate excuse to behave as one likes,\" Vohs and Schooler suggested. Haynes's research and its possible implications have certainly had an effect on how he thinks. He remembers being on a plane on his way to a conference and having an epiphany. \"Suddenly I had this big vision about the whole deterministic universe, myself, my place in it and all these different points where we believe we're making decisions just reflecting some causal flow.\" But he couldn't maintain this image of a world without free will for long. \"As soon as you start interpreting people's behaviours in your day-to-day life, it's virtually impossible to keep hold of,\" he says. Fried, too, finds it impossible to keep determinism at the top of his mind. \"I don't think about it every day. I certainly don't think about it when I operate on the human brain.\" Mele is hopeful that other philosophers will become better acquainted with the science of conscious intention. And where philosophy is concerned, he says, scientists would do well to soften their stance. \"It's not as though the task of neuroscientists who work on free will has to be to show there isn't any.\" Kerri Smith is editor of the Nature Podcast, and is based in London. \n                     Big questions in free will \n                   Reprints and Permissions"},
{"file_id": "477148a", "url": "https://www.nature.com/articles/477148a", "year": 2011, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "Can violent hurricanes, floods and droughts be pinned on climate change? Scientists are beginning to say yes. When the weather gets weird, as happens a lot these days, one question inevitably arises from reporters, politicians and the general public alike: is this global warming? The question was asked after last year's catastrophic floods in Pakistan and record-breaking heat wave in Russia. It was asked again this year about the freakish tornado clusters in the southeastern United States and the devastating drought in Africa. And it was asked yet again this August as Hurricane Irene roared up the US East Coast. For the most part, climate researchers have shied away from answering. Their mantra has been that science cannot attribute any particular drought or hurricane to climate change; the best it can do is project how the frequency of extreme weather events might change as the globe warms, through shifts in factors such as evaporation rates over the open ocean, water vapour and cloud formation, and atmospheric circulation. Lately, however, that reluctance has started to fade. \"My thinking has evolved,\" says Gavin Schmidt, a climate modeller at the NASA Goddard Institute for Space Studies in New York. Thanks to advances in statistical tools, climate models and computer power, \"attribution of extremes is hard \u2014 but it is not impossible\", he says. Two studies published last February in  Nature   showed links between extreme weather and climate change \u2014 one looking at the catastrophic flooding in the UK in 2000 1 , the other at the late-twentieth-century increase in intense rainfall across the Northern Hemisphere 2 . Also in the past year, climate researchers in the United States and Britain have formed a loose coalition under the banner 'ACE' \u2014 Attribution of Climate-related Events \u2014 and have begun a series of coordinated studies designed to lay the foundations for a systematic weather-attribution programme. Ultimately, the group hopes to create an international system that could assess the changing climate's influence on weather events almost as soon as they happen or even before they hit, with results being announced on the nightly weather reports. \"The idea is to look every month or so into the changing odds\" associated with that influence, says Peter Stott, a climate scientist with the UK Met Office's Hadley Centre in Exeter and a leader of the ACE group. Stott is writing a white paper laying out plans and requirements for a near-real-time attribution system, which he will present in October at the World Climate Research Programme conference in Denver, Colorado.  \n                Terrible toll \n              Extreme weather events are among the most destructive disasters known, whether their toll is measured in lives \u2014 some 40,000 people died as a result of Europe's record-breaking heat wave in 2003 \u2014 or in money \u2014 the US Gulf Coast suffered more than US$80 billion in damages in September 2005 from Hurricane Katrina. Worse, that toll is escalating: figures from the US National Climatic Data Center in Asheville, North Carolina, show that the frequency of multibillion-dollar weather disasters has at least doubled since 1980. Knowing what causes those disasters is a matter of core interest to insurance companies who have to set rates; to civil engineers who have to decide how (or whether) to strengthen protections such as levees; and to communities, regions and nations struggling to adapt to long-term changes in climate. If the surge in frequency is a result only of natural cycles, it will probably subside someday soon. But if the increase is a result of global warming, losses and damages could continue rising indefinitely. Reliable attribution of extreme weather events is also important for the public's understanding of climate change, and to their willingness to support measures to reduce greenhouse-gas emissions. Unlike more distant impacts of global warming such as the slowly rising sea level, the effects of local weather extremes tend to be instantly tangible and vividly remembered. Surveys suggest that people who feel they have personally experienced the effects of climate change are more likely to believe it is a real problem \u2014 and one that needs solving \u2014 than those who have not.  \n                Charting a course \n              With those imperatives in mind, the ACE group has set out to explore the climate\u2013weather connection systematically, by feeding observational data from the UK Met Office and the US National Center for Atmospheric Research (NCAR) in Boulder, Colorado, into seasonal forecasts and long-term climate models. Attribution, however, is no simple task: multiple factors influence a given weather event. Global climate change must have some effect: basic physics suggests that a warmer atmosphere can hold more water vapour, for example, and should therefore develop more storms, which feed on moisture and heat. But natural cycles such as El Ni\u00f1o have an equally obvious effect: freakish weather was a problem for humans long before anyone started pumping industrial quantities of carbon dioxide into the atmosphere. \n               Click here for larger image \n               So the goal of the ACE group is to carry out 'fractional attribution' of extreme events, estimating how much each one was influenced by anthropogenic greenhouse warming and how much by natural cycles (see  'Climate shift' ). The studies that appeared in  Nature   last February 1 , 2  offer pioneering examples of how to do this. In one, Pardeep Pall, an atmosphere researcher at the University of Oxford, UK, and his team generated several thousand simulations of the weather in England and Wales during the autumn of 2000. Some of the simulations included observed levels of human-generated greenhouse gases, whereas others did not. The researchers then fed the results of each simulation into a model of precipitation and river run-off to see what kind of flooding would result. In 10% of the cases, twentieth-century greenhouse gases did not affect the local flood risk. But in two-thirds of the cases, emissions increased the risk of a catastrophic flood \u2014 like the one that occurred in 2000 \u2014 by more than 90%. Another group, led by climate scientist Seung-Ki Min of the Climate Research Division of Environment Canada in Toronto, used a similar approach. Inspired by the observation that intense rainfall in the Northern Hemisphere has worsened over the second half of the twentieth century, the group compared actual precipitation data with simulations from six different climate models, both with and without greenhouse warming. They found that the extreme precipitation patterns observed did not match anything expected from natural climate cycles, but closely matched those expected from greenhouse warming. Such attribution studies can sometimes exonerate climate change. In one published in March 3 , Randall Dole and his colleagues at the National Oceanic and Atmospheric Administration in Boulder, Colorado, concluded that the intense 2010 Russian heat wave was probably a result of natural cycles. Although the basic approach seems straightforward, says Stott, fractional attribution is only as good as the climate models that drive it. \"We still need to understand which types of weather events we can confidently attribute,\" he says, \"and those for which the models are not yet good enough.\" In general, he says, attribution is easiest with heat waves and other temperature-related events. It is much harder with precipitation-related events such as floods and droughts, as the models have to take into account not just rainfall, but soils, natural terrain and human management of rivers and wetlands. And some weather events can't yet be linked to climate change at all. The frequency of tornadoes, for example, depends on a balance between moist air convection, which encourages their formation, and wind shear, which tends to disrupt them \u2014 but scientists cannot say for sure how climate change affects that balance. Another issue is the limited spatial resolution of climate models. At present, for example, they are far too coarse to represent small-scale 'convective' rainfall, a common phenomenon in which warm, moist air near the ground wells up to form an isolated thundercloud. Such convection is especially pronounced \u2014 and even harder to model \u2014 in mountainous regions such as the Andes or the Himalayas. Such deficiencies in the models explain why many climate scientists remain sceptical of attribution efforts. \"Scientifically unsound\" is the assessment of Judith Curry, a climatologist at the Georgia Institute of Technology in Atlanta. Even converts such as Schmidt are cautious. \"There is a lot of scope for doing a much better job,\" he says.  \n                Beyond the horizon \n              The ACE group plans to address these shortcomings in next month's white paper. As a first step, the group suggests that leading centres, such as NCAR and the Met Office, carry out fractional attribution assessments of notable weather extremes over the past 50 years, using large ensembles of coupled climate models and all available weather data. The lessons learned from these retrospective studies could then allow scientists to progress into routine attribution of recent weather, as well as climate-based forecasts of extreme weather. It is not yet clear what such a plan would cost, or who would pay for it. Kevin Trenberth, a climate scientist with the NCAR, estimates that a few million dollars would be enough to coordinate an international service using facilities already in place at his institution, the Met Office and elsewhere. But going beyond this bare-bones effort \u2014 creating, for example, a free-standing attribution centre with monthly, seasonal and decadal forecasting capacities \u2014 would cost much more. Given that governments on both sides of the Atlantic are slashing their budgets wherever possible, Trenberth admits that the prospects for launching such a programme anytime soon seem remote. But neither weather nor climate pays the slightest attention to what policy-makers are doing. And with events such as Hurricane Irene making themselves felt in politicians' backyards, an attribution service might someday be seen as a good investment.\n \n                 See Editorial  \n                 page 131 \n               Quirin Schiermeier is a reporter for Nature based in Munich. \n                     Nature Climate Change \n                   \n                     Pacific Institute \n                   \n                     Met Office \n                   \n                     National Center for Atmospheric Research \n                   \n                     IPCC \n                   Reprints and Permissions"},
{"file_id": "477020a", "url": "https://www.nature.com/articles/477020a", "year": 2011, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "Working weekends. Leaving at midnight. Friday evening meetings. Does science come out the winner? It's just about midnight on a hot Friday night in July, Enrique Iglesias' '  Dirty Dancer  ' is on the radio, and 26-year-old graduate student Sagar Shah is starting to look winded. The problem, he says, is not how late it is, or even that he has spent the past three hours working in a cramped sterile cell-culture hood. The problem is that the routine cell-culture maintenance he is doing, bathing his collection of rare human tumour cells with fresh medium, produces no data. And a lack of data, says Sagar, makes him \"hungry\" for it. Next to Sagar, Lyonell Kone, a 22-year-old student, rises from another sterile hood and heads for the microscope, jostling his lab-mate Nathaniel Tippens out of the way. He squints at his cultures, checking to make sure the cells are growing at the right density. Satisfied, he backs away, gingerly places his flasks in an incubator, rubs his eyes and stretches. He's finished for the night. The weary waltz within this cramped cell-culture room is the only flicker of activity at this hour in the Koch Cancer Research Building at Johns Hopkins University in Baltimore, Maryland. It's the Friday before the 4 July holiday, and even the night cleaners quit hours ago, leaving behind the faint smell of disinfectant and the occasional haunting beep of an autoclave echoing down silent hallways. But these members of neurosurgeon Alfredo Qui\u00f1ones-Hinojosa's laboratory are accustomed to being the last out of the building. In a lab where the boss calls you at 6 a.m., schedules Friday evening lab meetings that can stretch past 10 p.m., and routinely expects you to work over Christmas, sticking it out until midnight on a holiday weekend is nothing unusual. Many labs are renowned for their intense work ethic and long hours. When I set out to profile such a laboratory, I wanted to find out who is drawn to these environments, what it is really like to work there and whether long hours lead to more or better science. I approached eleven laboratories with reputations for being extremely hard-working. Ten principal investigators turned me down, some expressing a fear of being seen as 'slave-drivers'. Number eleven \u2014 Qui\u00f1ones-Hinojosa \u2014 had no such qualms. His work ethic is no secret: a 2007 essay in the  New England Journal of Medicine 1  and several television and newspaper reports have traced his path from 19-year-old illegal immigrant from Mexico, labouring in the fields of California, to neurosurgeon at one of the United States' leading research hospitals. He did not get there by working 9 to 5. Qui\u00f1ones-Hinojosa fondly recalls the long nights he worked alone in the laboratory as an undergraduate at the University of California, Berkeley, and again as a medical student at Harvard University in Cambridge, Massachusetts. When he was a resident at the University of California, San Francisco, his three young children thought he lived in the hospital. In effect he did, putting in 140 hours a week and grabbing 10-minute naps when he could. Qui\u00f1ones-Hinojosa credits his professional rise to his resilience and a seemingly limitless capacity for hard work. \"When you go that extra step, you are training your brain like an athlete,\" he says. And the fact that his group has published 113 articles in the past six years and holds 13 funding grants is not, he says, because he is brighter or better connected than colleagues. \"It's just a matter of volume,\" he says. \"The key is we submit a couple of dozen grant applications a year, and we learn from our mistakes.\" And so, at ease with hard work and the media and steeped in the long-hours culture of medicine, Qui\u00f1ones-Hinojosa eagerly welcomed me into his research laboratory. \"I would be delighted,\" he said. The morning I arrived \u2014 at 8 a.m. sharp \u2014 Qui\u00f1ones-Hinojosa insisted that I observe his first surgery of the day. He and his resident, Shaan Raza, were removing a pituitary tumour from a 54-year-old woman. The operating room is an extension of his laboratory, Qui\u00f1ones-Hinojosa explained: it is there that he collects the tissue samples that his staff \u2014 with the patients' consent \u2014 will immortalize in cell culture. They are the grist for the lab's studies of how cancer stem cells fuel brain-tumour development and how tumour cells spread through the brain. Qui\u00f1ones-Hinojosa had gone to bed at 1 a.m. the night before, and was up again at 5. Walking to surgery, he passes by Kone, who is climbing the stairs to the lab. \"You ready to rock 'n' roll?\" Qui\u00f1ones-Hinojosa asks reflexively as he walks by. Then he glances at the time and a mischievous smile darts across his face. \"Hey, it's 10 a.m.,\" he calls over his shoulder, never breaking stride. \"What are you doing coming in at 10 a.m.?\" As we walk out of the building Qui\u00f1ones-Hinojosa nudges me with his elbow and laughs: \"See, now he's going to go back to the lab and tell everyone, 'Dr. Q caught me coming in at 10 a.m.!'\" (Lab members, who in fact mostly arrive after 9 a.m., confirmed that Kone did exactly that.) Qui\u00f1ones-Hinojosa is gregarious and charming, with an infectious energy and a habit of advertising his humility. But he also knows how intimidating he can be to the people who work for him, and he's not afraid to capitalize on that. In 2007, just two years after he started at Hopkins, he rounded a corner in the cafeteria and saw his lab members sitting at a table, talking and laughing. When they caught sight of him, he says, they stopped, stood up, and went straight back to the lab. Qui\u00f1ones-Hinojosa has another way to keep his lab motivated. Every so often, he asks a cancer patient or his or her family to join the lab meeting. It is a chance for the patients to learn about the research being done with their tumours. And for the lab, it is a reminder of the urgency of their work. Qui\u00f1ones-Hinojosa draws out each patient's personal story: how they found out they had cancer, how they felt when they got the news and how it has impacted on their family. Being confronted with all this can be a shock for researchers without medical training. \"You can see it in their faces,\" says Hugo Guerrero-C\u00e1zares, a research associate in the lab. \"When someone says 'I'm going to die in six months', it really hits them.\" Back in the operating room, nurses and surgeons buzz about setting up equipment around the unconscious patient. Pituitary tumours can nestle between the two carotid arteries that supply the brain with blood, making the growths exquisitely difficult to remove. (Qui\u00f1ones-Hinojosa says he woke up last night worrying about the operation and spent two hours practising every move of the surgery in his mind before nodding off again.) Normally the tumours are about the size of a pea; this one is closer to a golf ball. Qui\u00f1ones-Hinojosa and Raza meticulously scoop out the tumour piece by piece. The surgery seems to be a success. Qui\u00f1ones-Hinojosa steps back from the patient and makes sure the sample is labelled and stored appropriately on ice. He checks with the pathologists down the hall to make sure it includes the tumour tissue he wants, then sends it on to his lab: sample 872 in his collection.  \n                Fast food \n              In the laboratory, near lunchtime, endocrinology research fellow Nestoras Mathioudakis prepares the tissue in a sterile tissue-culture hood. While the cells are incubating with an enzyme that destroys contaminating red blood cells, he dashes out to eat a frozen meal. He practically lives on them, he says, but worries that the high salt content may be giving him searing headaches. One day, after eating about five frozen dinners, he sat down at a microscope and found it difficult to focus his right eye. Still, the meals are cheap, fast and a way to grab food without leaving the lab, and Mathioudakis predicts that today will be a busy, multi-frozen-dinner day. He doesn't really mind. \"Only people with a certain type of personality would stay in a lab like this,\" says Guerrero-C\u00e1zares, who has worked there for four years. The night before I arrived, Qui\u00f1ones-Hinojosa was checking his e-mail on his way home when he noticed a message from a medical student at Rosalind Franklin University of Medicine and Science in Chicago, Illinois, who wanted to work in the laboratory. Qui\u00f1ones-Hinojosa receives several such enquiries a day, but something about this student \u2014 Joshua Bakhsheshian \u2014 caught his eye. He fired back a message: give me a number at which I can reach you at 6 a.m.. It was midnight. A minute later he had his reply. At 6:02 a.m. Qui\u00f1ones-Hinojosa called Bakhsheshian. \"I laid it on so thick for this guy,\" Qui\u00f1ones-Hinojosa crooned later that morning. \"I said, 'You've seen me on TV, you think I'm so nice. But you come into my lab, you're going to work. The people in my lab, they work 24 hours a day. They're here over Christmas and New Year writing grants, and you will be, too.'\" \"That's music to my ears,\" replied Bakhsheshian, who later told me he had never expected such a speedy reply from the busy surgeon, and had stayed up much of the night frantically studying the lab's publications. (Qui\u00f1ones-Hinojosa later offered him a spot in the lab if Bakhsheshian could get a fellowship.) Not everyone whom Qui\u00f1ones-Hinojosa selects adapts well to the rigours of his laboratory. Research fellow David Chesler, a neurosurgery resident at the University of Maryland in College Park with a PhD in neuroimmunology and circles under his eyes, recalls a technician who \"wasn't keeping up\" \u2014 and Guerrero-C\u00e1zares recounts the tale of a colleague who simply stopped coming to the Friday night lab meetings. Both left the lab. Qui\u00f1ones-Hinojosa says that he asked them to leave \"very nicely\", and helped them to find positions elsewhere. Still, Qui\u00f1ones-Hinojosa's technique of screening for work habits and personality traits may be one reason why the lab runs so smoothly, despite its intensity. Pierre Azoulay, associate professor of strategy at the Massachusetts Institute of Technology's Sloan School of Management in Cambridge, says that asking an employee to work long hours can backfire if that person is used to operating differently. \"Unless you select your trainees very carefully on those criteria \u2014 which I wager most principal investigators don't \u2014 there would presumably be deleterious effects.\" Another key is autonomy. Many members of the Qui\u00f1ones-Hinojosa lab develop their own projects, and write the grant applications to fund them. They express a proud sense of ownership when it comes to their work. And despite the 6 a.m. phone calls from the boss \u2014 made during his commute to the hospital \u2014 they say they feel reasonably free to set their own schedules. Shah says that 20-hour days are not uncommon for him. But \"I don't believe in clocking in and clocking out,\" he says. \"I could do that at Walmart and get overtime.\" That freedom is essential to keeping researchers happy and productive, says Azoulay. \"Science is a harsh mistress,\" he says. \"I think relatively few scientists are expecting 9-to-5 jobs. But they are expecting autonomy, and a principal investigator that violates that expectation could potentially run into problems.\" So far, Qui\u00f1ones-Hinojosa's lab seems relatively problem-free. But are the long hours and personal sacrifices worth it, for the lab members and for science? In 2004, Steven Stack, a sociologist at Wayne State University in Detroit, Michigan, published an analysis of survey data collected by the US National Research Council on 11,231 PhD scientists and engineers working in academia 2 . He found that the average scientist worked about 50 hours a week, and in general the more hours an individual put in, the more publications he or she cranked out. Qui\u00f1ones-Hinojosa's lab seems to fit that mould. Of the 113 articles he has published since he launched the lab in 2005, most are from a small 'dry' laboratory working on clinical outcomes in cancer. His 27-person 'wet' lab has published 29. Overall, his  h   index \u2014 a measure of productivity that factors in the number of articles published and how often they are cited \u2014 is 27, compared with 10.7 for US neurosurgeons at the same associate professor level 3 . Qui\u00f1ones-Hinojosa also notes that it takes researchers in his department an average of 15 years to be promoted to full professor. He was recommended for a full professorship this year, after just six. Biochemist Philip Cohen of the University of Dundee, UK, says that of the 70 postdocs and nearly 50 students he has supervised during his career, the most successful were those who put in long hours and worked efficiently. Cohen frets that the lab culture is changing. \"Everyone's told not to stress themselves or overdo things, and I could not disagree more,\" he says. \"I'm afraid they're losing all the fun in life if they don't really push themselves to the limit.\" But not everyone agrees that more hours yield more results. Dean Simonton, a psychology researcher at the University of California, Davis, who has studied scientific creativity, says that the pressure for publications, grants and tenure may have created a single-minded, \"monastic\" culture in science. But some research suggests that highly creative scientists tend to have broader interests and more hobbies than their less creative colleagues, he says. Chemist Stephen Buchwald of the Massachusetts Institute of Technology urges the members of his lab to take a month's holiday every year, and not to think about work when they're gone. \"The fact is, I want people to be able to think,\" he says. \"If they're completely beaten down, they're not going to be very creative.\" His approach does not seem to have hurt productivity: Thomson Reuters declared Buchwald one of the most highly cited chemists from 1999 to 2009, with an average of more than 86 citations for his 171 papers. An intense work schedule also comes with personal costs that can be hard to measure. \"The area in which I have failed the most is as a father,\" Qui\u00f1ones-Hinojosa readily admits. It is something he is trying to correct, by spending more time with his kids and shuttling them to swimming lessons (although phoning lab members on the way). And postdoc Pragathi Achanta looks wistful when she talks about her niece in India, who was six months old the last time Achanta saw her \u2014 now she's nearly five. Achanta has been working on grant applications over the holidays, and hasn't had time to visit her family. Now, at 8 p.m. on Friday 1 July, Achanta is taking advantage of the unusually short lab meeting to prepare surgical tools for a mouse experiment to model the effects of radiotherapy on stem cells. She wants to be ready so that she can complete the surgeries quickly on Saturday morning before she leaves to help teach a course at Cold Spring Harbor Laboratory in New York. Later this year, grant schedule allowing, she hopes to travel to India to see her niece at last. But she admits to being nervous about broaching the subject with the boss. Qui\u00f1ones-Hinojosa, though, says that he has nothing against holidays. \"Vacations are great,\" he says. \"Take a weekend off.\" \n                 See Editorial  \n                 p. 5 \n                 and Comment  \n                 p.27 \n               Heidi Ledford is a reporter for Nature in Cambridge, Massachusetts. \n                     NatureJobs \n                   \n                     Alfredo Qui\u00f1ones-Hinojosa \n                   Reprints and Permissions"},
{"file_id": "477264a", "url": "https://www.nature.com/articles/477264a", "year": 2011, "authors": [{"name": "Stephen S. Hall"}], "parsed_as_year": "2006_or_before", "body": "In 2009, an earthquake devastated the Italian city of L'Aquila and killed more than 300 people. Now, scientists are on trial for manslaughter. From when he was a young boy growing up in a house on Via Antinori in the medieval heart of this earthquake-prone Italian city, Vincenzo Vittorini remembers the ritual whenever the family felt a seismic tremor overnight. \"My father was afraid of earthquakes, so whenever the ground shook, even a little, he would gather us and take us out of the house,\" he says. \"We would walk to a little piazza nearby, and the children \u2014 we were four brothers \u2014 and my mother would sleep in the car. My father would stand outside, smoking cigarettes with the other fathers, until morning.\" That, he says, represented the age-old, cautionary \"culture\" of living in an earthquake zone.  Vittorini, a 48-year-old surgeon who has lived in L'Aquila all his life, will never forgive himself for breaking with that tradition on the night of 5 April 2009. After hundreds of low-level tremors over several months, L'Aquila shook with a strong, magnitude-3.9 tremor shortly before 11\u00a0p.m. on that Palm Sunday evening. Vittorini debated with his wife Claudia and his terrified nine-year-old daughter Fabrizia whether to spend the rest of the night outside. Swayed by what he describes as \"anaesthetizing\" public assurances by government officials that there was no imminent danger, and recalling scientific statements claiming that each shock diminished the potential for a major earthquake, he persuaded his family to remain in their apartment on Via Luigi Sturzo. All three of them were huddled together in the master bed when, at 3:32\u00a0a.m. on 6 April, a devastating magnitude-6.3 earthquake struck the city.  \"It was like being in a blender,\" Vittorini recalls. \"It wasn't a roar, it was a gigantic noise. And then darkness.\" The apartment building, a structure of reinforced concrete constructed in 1962, instantly collapsed, and their third-floor apartment ended up in a jumble of wreckage several feet off the ground. Seven people were killed in the collapse of the building, including Vittorini's wife and daughter; he was pulled from the rubble, injured but alive, six hours later. The earthquake claimed 309\u00a0lives in L'Aquila and several towns nearby, injured more than 1,500 people, destroyed some 20,000 buildings and left 65,000 people temporarily displaced. The apartment building on Via Luigi Sturzo is \"just a hole now\", Vittorini says, and his childhood home and the piazza where families spent the night are, like almost all of L'Aquila's historic centre, now in a barricaded and inaccessible 'red zone'. More than two years after the earthquake, block after block of elegant, centuries-old buildings is corseted by bands of structural reinforcement; wooden braces prop up numerous Gothic windows and arches in uninhabitable buildings. The basilica of San Bernardino, the city hall, the Cinema Massimo \u2014 all closed. On a cracked ochre wall along the main corso, one of the few streets that remain open in the centre, someone has scribbled in black paint: \"  L'Aquila \u00e9 morta .\" (L'Aquila is dead.) In a trial set to begin next week, an Italian judge will decide whether the symbolic death of L'Aquila \u2014 and, more specifically, the earthquake-related deaths of dozens of citizens included in the lawsuit, including Vittorini's wife and daughter \u2014 constituted a crime due to the negligence of six leading Italian scientists and one government official, who have been charged with manslaughter in connection with the case. When the charges were first aired in June 2010 by public prosecutor Fabio Picuti, the case was likened to a frivolous attempt by overzealous local prosecutors to make scapegoats out of some of Italy's most respected geophysicists: Enzo Boschi, then-president of Italy's National Institute of Geophysics and Volcanology (INGV) in Rome; Franco Barberi, at the University of 'Rome Tre'; Mauro Dolce, head of the seismic-risk office at the national Department of Civil Protection in Rome; Claudio Eva, from the University of Genova; Giulio Selvaggi, director of the INGV's National Earthquake Centre in Rome; and Gian Michele Calvi, president of the European Centre for Training and Research in Earthquake Engineering in Pavia; as well as government official Bernardo De Bernardinis, then vice-director of the Department of Civil Protection. According to an open letter to the president of Italy, Giorgio Napolitano, signed by more than 5,000 members of the scientific community, the seven Italians essentially face criminal charges for failing to predict the earthquake \u2014 even though pinpointing the time, location and strength of a future earthquake in the short term remains, by scientific consensus, technically impossible.  The indictments have drawn global condemnation. The American Geophysical Union and the American Association for the Advancement of Science (AAAS), both in Washington DC, issued statements in support of the Italian defendants. In an open letter to Napolitano, for example, the AAAS said it was \"unfair and naive\" of local prosecutors to charge the men for failing \"to alert the population of L'Aquila of an impending earthquake\". And last May, when Italian magistrate Giuseppe Gargarella ruled at a preliminary hearing that the scientists would have to stand trial this September, the Italian blogosphere lit up with lamentation and defence lawyers greeted the decision with disbelief. \"On the one hand, he's stunned,\" Francesco Petrelli said of his client, Barberi. \"On the other, he's very pained and sad.\" The view from L'Aquila, however, is quite different. Prosecutors and the families of victims alike say that the trial has nothing to do with the ability to predict earthquakes, and everything to do with the failure of government-appointed scientists serving on an advisory panel to adequately evaluate, and then communicate, the potential risk to the local population. The charges, detailed in a 224-page document filed by Picuti, allege that members of the National Commission for Forecasting and Predicting Great Risks, who held a special meeting in L'Aquila the week before the earthquake, provided \"incomplete, imprecise, and contradictory information\" to a public that had been unnerved by months of persistent, low-level tremors. Picuti says that the commission was more interested in pacifying the local population than in giving clear advice about earthquake preparedness. \"I'm not crazy,\" Picuti says. \"I know they can't predict earthquakes. The basis of the charges is not that they didn't predict the earthquake. As functionaries of the state, they had certain duties imposed by law: to evaluate and characterize the risks that were present in L'Aquila.\" Part of that risk assessment, he says, should have included the density of the urban population and the known fragility of many ancient buildings in the city centre. \"They were obligated to evaluate the degree of risk given all these factors,\" he says, \"and they did not.\" \"This isn't a trial against science,\" insists Vittorini, who is a civil party to the suit. But he says that a persistent message from authorities of \"Be calm, don't worry\", and a lack of specific advice, deprived him and others of an opportunity to make an informed decision about what to do on the night of the earthquake. \"That's why I feel betrayed by science,\" he says. \"Either they didn't know certain things, which is a problem, or they didn't know how to communicate what they did know, which is also a problem.\" Although the outcome of the trial may not be known for months, if not years, the events leading up to the earthquake are already being viewed as a sobering case study in risk assessment and public communication \u2014 a scenario that might easily be replayed in a future that includes not just 'conventional' natural disasters (such as volcanic eruptions, earthquakes, and tsunamis), but also extreme weather events (such as tornadoes, hurricanes, floods and droughts) perhaps cooked up by climate change.\u2029The trial has already had a chilling effect on scientists' willingness to share their expertise with the public. \"When people, when journalists, asked my opinion about things, I used to tell them, but no more. Scientists have to shut up,\" says Boschi, whose successor at the INGV was appointed last month. Others see the case as an indictment of the obfuscating, probabilistic language with which scientists characterize the uncertain potential of natural disasters. Selvaggi, one of the indicted scientists, says that the charges serve as a \"dangerous\" warning to researchers, who may find themselves in legal trouble because of the way that non-scientists such as public officials or journalists translate their risk analyses for public consumption. Given the novelty of the issues, says defence lawyer Filippo Dinacci, \"not only the press, but the academic legal community will be watching this case with great interest\".  Thomas Jordan, director of the Southern California Earthquake Center at the University of Southern California in Los Angeles, and chair of the International Commission on Earthquake Forecasting (ICEF), which reviewed the L'Aquila events in a report released in May, says that in his view the prosecution charges have \"no merit\". But he adds that the trial is nonetheless a \"watershed case\" that will force seismologists worldwide to rethink the way they describe low probability, high-risk events, as well as an opportunity for the scientific community at large to assess \"rising public expectations\" about how information on natural disasters should be handled. \"The public expects authoritative, transparently available information,\" he says, \"and we need to say what we know in an explicit way.\"  In Jordan's view, \"It has to be done right, and it was not in L'Aquila.\"  \n                Seismic reputation \n              L'Aquila is \u2014 or was \u2014 a jewel of medieval beauty set in the middle of one of the most seismically dangerous zones in Italy. Surrounded by the massive peaks of the restless Apennine mountain range, the city, capital of the Abruzzo region, was largely destroyed by earthquakes in 1461 and in 1703. Its seismic reputation was such that the nineteenth-century British travel writer Augustus Hare noted that, \"nature suddenly often sets all the bells ringing and the clocks striking, and makes fresh chasms in the old yellow walls\".  \n               Click here for larger image \n               Its most recent seismic tragedy began in October 2008, when dozens of low-magnitude tremors began to hit the city and surrounding areas along the Aterno River valley (see  'A shaken city' ). Known as seismic swarms, these tremors continued intermittently over the first three months of 2009; according to Picuti, they numbered 69 in January, 78 in February and 100 in March, with an additional 57 shocks during the first five days of April. \"It was like this almost every day,\" says Pier Paolo Visione, a local accountant, shaking a table in a restaurant with a slow but vigorous motion that nearly topples a bottle of the local red Montepulciano wine. \"I had never been afraid of earthquakes before, but my skin began to crawl.\" (Visione's sister died in the quake, and he is a civil party to the suit.) Unnerving though these clusters may be, experts agree that seismic swarms rarely precede major earthquakes. In 1988, seismic engineer Giuseppe Grandori, now professor emeritus at the Polytechnic of Milan, and his colleagues published a retrospective analysis of seismic swarms in three other earthquake-prone Italian localities ( G. Grandori  et al. Bull. Seismol. Soc. Am.    78,   1538\u20131549; 1988 ). They concluded that a medium-sized shock in a swarm forecasts a major event within several days about 2% of the time, and Grandori says that the same was probably true for the region around L'Aquila. Translating these risks is extremely challenging for civil defence officials. In Grandori's view, there is a 98% probability of a false alarm if officials issue an alert, yet a terrible price to pay in loss of life and property if they fail to issue a warning and a major quake occurs. After a medium-sized shock in a seismic swarm, the risk of a major quake can increase anywhere from 100-fold to nearly 1,000-fold in the short term, according to Jordan, although the overall probability remains extremely low. \"What do you tell people in that situation?\" he says. \"You're sort of between Scylla and Charybdis on this thing.\"  To this difficult exercise in risk probability was added a wild card in the case of L'Aquila: a resident named Giampaolo Giuliani began to make unofficial earthquake predictions on the basis of measurements of radon gas levels. Giuliani, who had worked for 40 years as a laboratory technician, including 20 years at the nearby Gran Sasso National Laboratory until his retirement in 2010, had deployed four home-made radon detectors throughout the region.  The idea behind radon measurement, Giuliani says, is that emissions of the gas fluctuate significantly in the 24\u00a0hours before an earthquake. But their use as a reliable short-term predictor of earthquakes has never been scientifically proved or accepted. The recent ICEF report deemed Giuliani's findings \"unsatisfactory\", and he has yet to publish a single peer-reviewed paper on his radon work. Nonetheless, he maintained an open website that posted real-time radon measurements from his detectors, and in interviews with journalists and in an informal mobile-phone network, Giuliani made predictions about low-level seismic activity. Although the ICEF report notes that he made two false forecasts,  The Guardian   newspaper dubbed him \"The Man Who Predicted An Earthquake\", after the April 2009 quake hit. As word spread about Giuliani's unofficial predictions, even more unease percolated through the population. Marcello Melandri, the lawyer for Boschi, says that Giuliani had been terrifying local residents, and that Guido Bertolaso, head of Italy's Department of Civil Protection agency, \"was very worried about the population of L'Aquila\". On 30 March, Giuliani says, national civil-protection officials cited him for  procurato allarme   \u2014 essentially instigating public alarm or panic \u2014 and forbade him from making any public pronouncements. That same day, L'Aquila was hit by an intense, magnitude 4.1 shock in the afternoon that deeply rattled local residents. Vittorini, who performs his surgeries in the nearby town of Popoli, received an anguished call from his wife and son. (His daughter was not at home at the time.) He urged them to leave the house immediately and get outside, he says. L'Aquila's mayor, Massimo Cialente, ordered the evacuation of several public buildings and closed the De Amicis primary school to inspect for structural damage. Italian seismologists had been monitoring the swarm in the Abruzzo region for months, and notifying civil-protection officials in real time of every tremor with a magnitude of greater than 2.5. Now, given the growing unease in L'Aquila, Bertolaso decided to convene an unusual meeting of the risks commission. The commission normally meets in Rome to assess the probability of earthquakes, volcanoes and other natural disasters, but this meeting was to take place the next day in L'Aquila. The goal, according to a press release from the Department of Civil Protection, was to furnish citizens in the Abruzzo region \"with all the information available to the scientific community about the seismic activity of recent weeks\".  \n                Meeting of minds \n              The now-famous commission meeting convened on the evening of 31 March in a local government office in L'Aquila. Boschi, who had travelled by car to the city with two other scientists, later called the circumstances \"completely out of the ordinary\". Commission sessions are usually closed, so Boschi was surprised to see nearly a dozen local government officials and other non-scientists attending the brief, one-hour meeting, in which the six scientists assessed the swarms of tremors that had rattled the local population. When asked during the meeting if the current seismic swarm could be a precursor to a major quake like the one that levelled L'Aquila in 1703, Boschi said, according to the meeting minutes: \"It is unlikely that an earthquake like the one in 1703 could occur in the short term, but the possibility cannot be totally excluded.\" The scientific message conveyed at the meeting was anything but reassuring, according to Selvaggi. \"If you live in L'Aquila, even if there's no swarm,\" he says, \"you can never say, 'No problem.' You can never say that in a high-risk region.\" But there was minimal discussion of the vulnerability of local buildings, say prosecutors, or of what specific advice should be given to residents about what to do in the event of a major quake. Boschi himself, in a 2009 letter to civil-protection officials published in the Italian weekly news magazine  L'Espresso , said: \"actions to be undertaken were not even minimally discussed\". Many people in L'Aquila now view the meeting as essentially a public-relations event held to discredit the idea of reliable earthquake prediction (and, by implication, Giuliani) and thereby reassure local residents. Christian Del Pinto, a seismologist with the civil-protection department for the neighbouring region of Molise, sat in on part of the meeting and later told prosecutors in L'Aquila that the commission proceedings struck him as a \"grotesque pantomine\". Even Boschi now says that \"the point of the meeting was to calm the population. We [scientists] didn't understand that until later on.\" What happened outside the meeting room may haunt the scientists, and perhaps the world of risk assessment, for many years. Two members of the commission, Barberi and De Bernardinis, along with mayor Cialente and an official from Abruzzo's civil-protection department, held a press conference to discuss the findings of the meeting. In press interviews before and after the meeting that were broadcast on Italian television, immortalized on YouTube and form detailed parts of the prosecution case, De Bernardinis said that the seismic situation in L'Aquila was \"certainly normal\" and posed \"no danger\", adding that \"the scientific community continues to assure me that, to the contrary, it's a favourable situation because of the continuous discharge of energy\". When prompted by a journalist who said, \"So we should have a nice glass of wine,\" De Bernardinis replied \"Absolutely\", and urged locals to have a glass of Montepulciano.  The suggestion that repeated tremors were favourable because they 'unload', or discharge, seismic stress and reduce the probability of a major quake seems to be scientifically incorrect. Two of the committee members \u2014 Selvaggi and Eva \u2014 later told prosecutors that they \"strongly dissented\" from such an assertion, and Jordan later characterized it as \"not a correct view of things\". (De Bernardinis declined a request for an interview through his lawyer, Dinacci, who insisted that De Bernardinis's public comments reflected only what the commission scientists had told him. There is no mention of the discharge idea in the official minutes, Picuti says, and several of the indicted scientists point out that De Bernardinis made these remarks before the actual meeting.) That message, whatever its source, seems to have resonated deeply with the local population. \"You could almost hear a sigh of relief go through the town,\" says Simona Giannangeli, a lawyer who represents some of the families of the eight University of L'Aquila students who died when a dormitory collapsed. \"It was repeated almost like a mantra: the more tremors, the less danger.\" \"That phrase,\" in the opinion of one L'Aquila resident, \"was deadly for a lot of people here.\" The press conference and interviews, prosecutors argue, carried special weight because they were the only public comments to emerge immediately after the meeting. The commission did not issue its usual formal statement, and the minutes of the meeting were not even prepared, says Boschi, until after the earthquake had occurred. Moreover, it did not issue any specific recommendations for community preparedness, according to Picuti, thereby failing in its legal obligation \"to avoid death, injury and damage, or at least to minimize them\". Picuti argues that the fragility of local housing should have been a central component in the commission's risk assessment. \"This isn't Tokyo, where the buildings are anti-seismic,\" he says. \"This is a medieval city, and that raises the risk.\" In 1999, Barberi himself had compiled a massive census of every seismically vulnerable public building in southern Italy; the survey, according to the prosecution brief, indicated that more than 550 masonry buildings in L'Aquila were at medium\u2013high risk of collapsing in the event of a major earthquake.  The failure to remind residents of earthquake preparedness procedures in the face of such risks is one of the reasons that John Mutter, a seismologist at Columbia University's Lamont-Doherty Earth Observatory, declined to sign the open letter circulated to support the Italian scientists. Mutter says that in his opinion, \"these guys shouldn't go to jail, but they should be fined or censured because they should have said something other than what they said. To say 'don't worry' \u2014 that sort of thing just isn't helpful. You need to remind people of their earthquake drills: if they feel the house moving, get out of the building if you can, or get under a table or a door frame if you can't. Do all the things that we know save lives.\"  As part of the prosecution's case, Picuti argues in his brief that local residents made fateful decisions on the night of the earthquake on the basis of statements made by public officials outside the meeting. Maurizio Cora, a lawyer who lived not far from Vittorini, told prosecutors that after the 30 March shock, he and his family retreated to the grounds of L'Aquila's sixteenth-century castle; after the 11 p.m. foreshock on 5 April, he said his family \"rationally\" discussed the situation and, recalling the reassurances of government officials that the tremors would not exceed those already experienced, decided to remain at home, \"changing our usual habit of leaving the house when we felt a shock\". Cora's wife and two daughters died when their house collapsed.  \"That night, all the old people in L'Aquila, after the first shock, went outside and stayed outside for the rest of the night,\" Vittorini says. \"Those of us who are used to using the Internet, television, science \u2014 we stayed inside.\"  \n                Disputed advice \n              In an interview in the Rome offices of his lawyer, Boschi derided as \"absurd\" the idea that he in any way played down the risk to L'Aquila. Brandishing a copy of the INGV's seismic hazard map of Italy, which shows a broad swath of the Apennines in bright hues indicating high risk, the tall, silver-haired geophysicist insisted: \"No one can find a single piece of paper where I say, 'Be calm, don't worry'. I have said for years that the Abruzzo is the most seismologically dangerous zone in all of Italy. It's as if I suddenly became an imbecile. I'm accused of being negligent!\" He was not invited to participate in the press conference after the meeting, he says, and didn't even know about it until after his return to Rome. Attorneys for the other scientists all insist that the charges are without foundation, while raising additional arguments. Barberi's lawyer, Petrelli, acknowledges that the meeting was intended \"in part\" to defuse the panic over Giuliani's predictions, but insists that everything his client said was scientifically sound and correct. To convey the difficulty of communicating risk assessments, he offers the analogy of being asked the safest way to travel, and recommending flying because it is statistically much safer than car or train. \"If the person takes the plane, and the plane is involved in an accident, this doesn't mean that my advice was wrong,\" he said. \"I gave the right advice, since scientific advice is based on statistics, and the statistics don't exclude the possibility of an event that we would like to avoid.\"  Alessandra Stefano, the lawyer for Calvi, says that the mass media has played a part in the case by disseminating incorrect information about \"especially delicate\" scientific matters. Eva's lawyer, Alfredo Biondi, has pointed out that in 1985, the then-head of civil protection, Giuseppe Zamberletti, was investigated for instigating a public panic when he ordered the evacuation of several villages in northwest Tuscany after a seismic swarm; on that occasion, no major quake occurred. Antonio Pallotta has argued that his client, Selvaggi, was not an official member of the commission. As for the statement that seems to have resonated most with the residents of L'Aquila \u2014 De Bernardinis's claim that during seismic swarms, repeated tremors were \"favourable\" \u2014 Dinacci says of his client: \"He's not a seismologist, he's a hydraulic engineer,\" and that he had only relayed what the scientists had told him. As to De Bernardinis's suggestion to have a glass of Montepulciano, Dinacci says, \"This was a joke! To have made a joke about a glass of wine and then face a conviction is absurd. It's something out of the Middle Ages.\" The outcome of the trial that begins next week in L'Aquila can no more be predicted than can earthquakes themselves. It will ultimately be up to a single magistrate to decide whether the actions of the commission, and the alleged \"erroneous information\" released by officials outside the meeting, rise to the level of criminal culpability. Although defence lawyers say that the prosecution's case is logically flawed, the stakes are high. If convicted, the scientists could face up to 15 years in jail, according to prosecutors. In addition, plaintiffs in a separate civil case are seeking damages in the order of \u20ac22.5 million (US$31.6 million).  \n                After shock \n              Irrespective of the verdict, the episode has been a painful tutorial about the importance of clear public communication when potential disasters loom. The commission and the civil-protection department \"got trapped in the wrong conversation because of the hullaballoo that was happening\" around the unofficial predictions of earthquakes, says Jordan. \"The issue became, is there going to be an earthquake or not, and that choice is the wrong way to talk about this.\" Mutter adds that in his opinion, the commission's focus on whether earthquakes could be predicted or not ultimately didn't tell people what they wanted to know. \"People aren't stupid,\" he says. \"They know we can't predict earthquakes. They just want clear advice on what they should do.\" The recent ICEF report argues that frequently updated hazard probabilities are the best way to communicate risk information to the public. \"Seismic weather reports, if you will, should be put out on a daily basis,\" Jordan says. \"Nobody has set up a good system for doing this, and our understanding of the 'weather' in this case is very poor, so we can only see through the glass darkly.\" But in an age of social media and instantaneous communication, he says, misinformation travels fast, and the public needs clear, real-time risk assessment. As Selvaggi warns, the number of situations in which scientists are asked to assess hazard is certain to rise. \"We have an increasing number of extreme events,\" he said, \"and we have increasing numbers of people living in high-risk regions. It's time to address this problem.\" Jordan says that the L'Aquila incident raises one other fundamentally important issue about risk assessment. \"The role of science is to present information about hazards,\" he says. \"But it's the role of the decision-makers to take that information, and a lot of other information, in order to make decisions about public welfare.\" In fact the legal fight in L'Aquila is viewed by some as a philosophical dispute between scientists, who believe that their role is pure hazard assessment, and the local prosecutors, who argue that Italian law obliges scientific advisers to evaluate the fragility of buildings and other factors in their assessment of risk.  Scientists will also have to work hard to convince the public, at least in L'Aquila, that frequent, probabilistic risk assessment is a better way to protect them than age-old traditions. As Vittorini told Picuti after the earthquake, the messages from the commission meeting \"may have in some way deprived us of the fear of earthquakes. The science, on this occasion, was dramatically superficial, and it betrayed the culture of prudence and good sense that our parents taught us on the basis of experience and of the wisdom of the previous generations.\" Glancing at an image of his deceased wife and daughter on his mobile phone, Vittorini says: \"We're not crazy people. We just want accountability. We hope this trial can be a symbol of change.\" \n                 See World View  \n                 page 251 \n               \n                 Stephen S. Hall is a science writer based in New York who also teaches public communication to graduate students in science at New York University.  \n               \n                     Italy\u2019s National Institute of Geophysics and Vulcanology (INGV) on L\u2019Aquila \n                   Reprints and Permissions"},
{"file_id": "477150a", "url": "https://www.nature.com/articles/477150a", "year": 2011, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "Since the anthrax attacks in 2001, some $60 billion has been spent on biodefence in the United States. But the money has not bought quite what was hoped. It took one routine smallpox vaccination to expose the holes in the United States' defences against bioterrorism. In January 2009, the jab was given to 20-year-old Lance Corporal Cory Belken of the US Marine Corps, as it is to many members of the military who are about to be deployed abroad, to protect him against a potential attack with the lethal virus. But in this case, the timing was unfortunate. Two weeks after the vaccination Belken was diagnosed with leukaemia; he then underwent chemotherapy that wiped out his immune system. Suddenly, the live vaccinia virus, the milder relative of smallpox used in the vaccination, was able to multiply into a dangerous infection. Doctors turned to their only means of counter-attack: three smallpox drugs, two of them experimental and developed as part of US efforts to build up an arsenal against potential bioterror agents. The marine received 30 times the standard dose of the first drug, an approved antibody, to no avail. The second, called STS-246, had been used in only one person infected with vaccinia before. By the time doctors administered the third drug, CMX001, Belken had developed a bacterial infection that spread to his feet, brought him near death and required surgeons to amputate both his legs below the knees. Only after he received all three medicines did he start to recover \u2014 and it is still not known which of the drugs, if any, eventually helped. The marine's case is just one of many events that have raised questions about the biodefence research and development enterprise that sprang from bioterror attacks in the United States ten years ago. Shortly after the terrorist attacks of 11 September 2001, anthrax spores sent to media outlets and politicians killed five people and compounded already widespread fear and horror. The incidents spurred the US government to launch a major scientific effort to develop 'countermeasures': diagnostics, vaccines and drugs against potential biological threats such as smallpox and anthrax. In a three-part strategy, the federal government poured money into basic research at the National Institutes of Health (NIH); created the Biomedical Advanced Research and Development Authority (BARDA) to carry new concepts forward into further development and testing; and established BioShield, a US$5.6-billion programme to purchase the finished drugs and vaccines. But none of the links in this chain has worked exactly as it was supposed to. Between 2001 and the end of this year, the federal government will have spent $60 billion on such biodefence efforts (see  'A decade of biodefence' ), according to analyses from the Center for Biosecurity of the University of Pittsburgh Medical Center in Baltimore, Maryland. The money has helped to modernize the nation's crumbling public-health system, and BioShield has invested in a stockpile of 20 million doses of smallpox vaccine, 28.75 million doses of anthrax vaccine and 1.98 million doses of four medicines to treat complications of smallpox, anthrax and botulism. But few researchers or policy-makers seem happy with an arsenal of six drugs that address only three of the potential threats \u2014 even if they are among the most serious. \"The pipeline we rely on to provide those critical countermeasures \u2014 diagnostics, vaccines, antivirals, antibiotics \u2014 is full of leaks, choke points and dead ends,\" said Kathleen Sebelius, US Secretary of Health and Human Services, in a statement last year. Critics say that the effort has been hobbled by a lack of strategic thinking, focus and coordination between the federal agencies involved, and by unrealistic expectations of what the money could buy. \"There was no evidence that they looked at what our top priorities are and asked, 'What's needed on the basic-science side?', 'What's needed on the development side?', and 'What's needed in the stockpile?',\" says Andrew Pavia, an infectious-diseases doctor at the University of Utah in Salt Lake City. Until earlier this year, Pavia served on the National Biodefense Science Board, which advises the US Department of Health and Human Services (DHHS) and in March last year released a report,  Where Are The Countermeasures? , that was critical of the federal biodefence effort. What is more, developing therapies for diseases that are mercifully rare among humans is a unique challenge. Drug development is difficult at the best of times \u2014 and experts say that there is simply not enough money in biodefence to entice big companies into the field. \"It is a bit discouraging considering we've spent more than $60 billion on this in the past decade,\" says Randall Larsen, a member of the Commission on the Prevention of Weapons of Mass Destruction Proliferation and Terrorism, which in a January 2010 'report card' gave the nation a failing grade for its ability to prevent a bioterror attack from causing huge casualties. \"The question is whether it has been spent properly,\" he says.  \n                Priority pathogens \n              That point is especially pressing now. The United States is in dire financial straits and may be forced to slash the research budgets of the NIH and other agencies if Congress does not agree on other spending cuts by 23 December. Still, some researchers think that ten years is simply too soon to expect pay-offs from a research programme that essentially started from scratch. \"There have been some really important lessons received from what has admittedly been a very large investment,\" says David Relman, a microbiologist at Stanford University in California who has been heavily involved in biodefence research and policy. \"Perhaps with a more refined idea of the goal, [the money] might have been used in a more productive or effective way. But at the time, we didn't really know what we needed and we didn't know how hard it would be to make any of these things that we needed.\" A pillar of the biodefence enterprise is the US National Institute of Allergy and Infectious Diseases (NIAID) in Bethesda, Maryland, which got a $1.5-billion budget boost in 2003 and has so far received $14 billion for biodefence. It is there, say critics, that some early and crucial mistakes were made. In a series of reports issued in 2002 and 2003, the NIAID outlined its plans to fund basic research aimed at the development of treatments and vaccines for more than 50 'priority pathogens' and toxins classified into three categories. Category A covers agents considered to be the most dangerous and likely to be used in an attack, such as smallpox and anthrax. Categories B and C include threats such as food- and waterborne illnesses. (The list was similar to a catalogue of 'select agents' kept by the US Centers for Disease Control and Prevention (CDC) in Atlanta, Georgia.) The agency also built 15 labs across the country \u2014 part of a building boom that has led, so far, to the planning, construction or renovation of nearly 20 labs for the study of dangerous pathogens at a cost of more than $2 billion. But some experts say that attempting to tailor vaccines and treatments to individual pathogens is misguided: some of the pathogens are difficult to turn into bioweapons or just aren't very dangerous, and the costs of developing a large defensive arsenal are astronomical. It makes more sense, these experts say, to stockpile antibiotics and other medicines that could be used against many pathogens. \"You can look at some of the vaccine investments, like for plague and [the bacterial disease] tularaemia, and wonder who decided that was the highest priority, as opposed to developing new antibiotics,\" says Pavia. The NIAID reorganized its biodefence research efforts in 2007, increasing its focus on 'broad-spectrum' priorities that would work against multiple pathogens. And in June this year, a federal panel recommended trimming and reorganizing the CDC's select-agent list. But the NIAID is still funding research on plague and tularaemia vaccines, and defends the work, saying that research on tularaemia, for example, has yielded insights about immunity that are relevant to other pathogens. Michael Kurilla, director of the Office of Biodefense Research Affairs in the NIAID's Division of Microbiology and Infectious Diseases, points to recent work on a broad-spectrum antiviral drug that, he says, sprang from studies of the Nipah virus, a category C bioterror threat. He says that this demonstrates the value of continued work on such pathogens as well as model microbes such as the bacterium  Escherichia coli . \"If you say everyone should study  E. coli   because it's like everything else, you won't get those concepts that come out of some unusual bug and have other applications,\" he says. Despite the problems with basic biodefence research, critics see far more to complain about in the later stages of the process. The therapies can't be rigorously tested in humans (it would be unethical to infect people with pathogens such as smallpox for testing). And government agencies had little idea how to go about developing such therapies. The smallpox drugs CMX001 and STS-246 are cases in point. CMX001 is a version of an established antiviral drug called cidofovir that must be given as an injection. In 2000, Karl Hostetler, a chemist at the University of California, San Diego, formed Chimerix, a pharmaceutical company based in Research Triangle Park, North Carolina, to develop a cidofovir pill that could be taken by mouth. In September 2003, the NIAID awarded Chimerix a $36-million, five-year grant to develop CMX001 as a treatment for smallpox. The drug looked promising in tests on mice and rabbits, and Chimerix teamed up with the US Army Medical Research Institute of Infectious Diseases in Fort Detrick, Maryland, to test it in monkeys infected with the related virus monkeypox. But the drug didn't cure the disease \u2014 owing, Chimerix said, to a quirk of metabolism not relevant to humans. Meanwhile, SIGA, a pharmaceutical company based in New York, was racing ahead with STS-246, a small molecule that blocks viral maturation. In 2006, the company reported that its drug protected monkeys from monkeypox. While Chimerix struggled for funding, the federal government continued to award money to SIGA and, in October 2010, SIGA won a BARDA contract for STS-246 worth up to $2.8 billion. Chimerix protested \u2014 and SIGA's award was later whittled down to $433 million. Yet the case of the sick marine in 2009 showed that fighting biothreats can take a whole armamentarium of drugs. Belken's condition didn't improve until CMX001 was added to STS-246. \"The reality is that we need two smallpox antiviral drugs,\" says Robert Kadlec, a former Senate staff member who helped to write the 2006 legislation that created BARDA. The episode shows how challenging it is to develop therapies, especially when there are no good animal models or data showing whether the drug fights disease in humans. The smallpox virus infects only humans, for example, and monkeypox is an imperfect mimic. Yet authorities often need to rely on animal tests when they make expensive decisions about which drug to buy, and small biodefence companies can be dependent on the funding that results from these decisions. \"The regulatory process is still evolving, and the federal government doesn't have a clear sense of what it needs,\" says Jim Davis, executive vice-president of Human Genome Sciences in Rockville, Maryland. \"It's frustrating for everyone involved.\" In October 2009, the US Food and Drug Administration (FDA) decided not to approve an antibody against anthrax developed by Human Genome Sciences \u2014 even though BARDA had already agreed to spend $326 million on the drug. The company had thought that it had met the FDA's criteria but, according to Davis, the agency decided that it wanted a drug that is more effective than the existing anthrax treatment, ciprofloxacin. Robin Robinson, director of BARDA, says that the agency is funding the creation of better animal models. The DHHS reviewed medical countermeasures in 2010, and said that it will do more to try to help companies to bridge the gap between basic research and the clinic. The DHHS has also proposed reallocating $170 million in existing FDA funds to help update regulatory review in biodefence. \n               Click here for larger image \n               But revamping biodefence is going to take more money \u2014 and critics say that some of the $60 billion spent so far has simply been wasted. They point to a $533.8-million surveillance project called BioWatch, created by the Department of Homeland Security, which has deployed detectors for airborne bioterror agents in 30 cities. The system has been criticized in part because technicians must manually collect the air filters and take them to a lab for analysis, creating a delay of 10\u201334 hours before results are in and hampering the system's ability to provide an early warning. In a report entitled  BioWatch and Public Health Surveillance , released last year, a committee convened by the US National Academies said that BioWatch faces \"serious technical and operational challenges\". The next-generation Biowatch is designed to improve the programme. Much of the biodefence money didn't even go into research, as a breakdown of spending shows (see  'Biodefence in billions' ). The CDC has received the most so far \u2014 $17.4 billion \u2014 and put the vast majority into bolstering an underfunded public-health infrastructure. The rationale is that the nation has little chance of fighting a bioterror attack without a strong system for detecting, reporting and treating any emerging infectious disease. Most of the biodefence spending, in fact, has spin-offs into other fields; even BARDA is involved in developing medicines against threats such as pandemic flu. In all, only $11.99 billion of the $60 billion has been spent on programmes solely concerned with biodefence. That's just over $1 billion per year from 2001 to 2011. Drug-makers often say that it takes at least $800 million and ten years to develop a single drug, so a much greater investment is required before the biodefence effort can yield many new countermeasures. Kadlec recommends that the United States spend $10 billion a year on biodefence in future.  \n                Funding crunch \n              Such sums seem unlikely to materialize. BioShield's funding is set to expire in 2013, and Congress has proposed refunding it at $2.8 billion for 2014\u201318 \u2014 about the same as before. Cutbacks are eroding some of the gains in public-health infrastructure: local health departments have lost 29,000 jobs, some 19% of the workforce, over the past three years. Now, say observers, the federal government must take a hard look at its biodefence programme and devise a more coordinated strategy that strikes a balance between developing pathogen-specific countermeasures and working on a more generalized resilience to infectious disease. \"If the expectations were that we were going to come up with a whole armamentarium of new products by 2011, that was probably unrealistic,\" says Relman. \"It might make sense to pick a few [threats] that are at the top of all possible lists, but then to say, a lot of the rest of the work needs to be in creating a fertile ground for innovation and product development.\" \"We're at a point after ten years,\" says Michael Osterholm, director of the University of Minnesota's Center for Infectious Disease Research and Policy in Minneapolis, \"where we have got to start producing the kinds of plans and cost estimates about what it will take for a country like ours to be prepared in a moderate way.\" Preparing for only the worst eventualities might now be the best the nation can do. \n                 See Comment  \n                 page 153 \n               Erika Check Hayden is a senior reporter for Nature in San Francisco, California. \n                     NIAID biodefence resources \n                   \n                     BARDA \n                   \n                     Biodefence funding, 2001\u201312 \n                   \n                     WMD Center's 2010 report \n                   \n                     HHS Public Health Emergency Medical Countermeasures Review \n                   \n                     National Biodefense Science Board \u201cWhere are the Countermeasures\u201d report \n                   Reprints and Permissions"},
{"file_id": "477390a", "url": "https://www.nature.com/articles/477390a", "year": 2011, "authors": [{"name": "Sharon Weinberger"}], "parsed_as_year": "2006_or_before", "body": "Wartime explosions may be creating an epidemic of brain damage \u2014 and a major challenge for scientists. To Burt, the blasts he experienced in Afghanistan eventually became a kind of music. The detonation of C4 and other such military-grade explosives felt like extremely high notes \u2014 painful, yet over quickly. But blasts from bombs made out of fertilizer \u2014 a favourite of Afghan insurgents \u2014 were like standing next to a speaker at a rock concert: the dull bass thuds didn't necessarily hurt, but they would reverberate through his body like a wave, and stay with him for a long time afterwards. They're with him still. Burt, who asks that his real name not be used, spent four months as a tactical adviser to a US military bomb-disposal unit in Afghanistan, during which he was within 50 metres of a detonating improvised explosive device (IED) more than 18 times. His sleeping problems began even before he left. So did the headaches, the ringing in his ears and the nausea. He started to forget things \u2014 a problem that got even worse after he returned home. Burt would find himself in a room in his house and wonder why he was there. One time, he told his wife they should try a new restaurant in town. She replied that they had eaten there with friends just a few days before. As recently as two years ago, this constellation of symptoms might have been diagnosed as a classic case of post-traumatic stress disorder (PTSD), a psychological condition that can be caused by the constant stress of being in combat. But Burt, now on medical leave, blames those low notes. He is convinced that the body-shaking blasts did something to his brain. And many doctors, medical researchers and military officials have come to believe he is right. The visible toll of insurgent-made IEDs has been awful enough. In the ten years since military operations began in Afghanistan and then Iraq, IEDs have killed more than 3,000 US and allied troops, and wounded roughly ten times that number. But many more troops have been exposed to multiple blasts and not suffered any visible physical injuries. Like Burt, they often report an array of symptoms, ranging from sleep disturbance to problems concentrating. And an increasing body of evidence suggests that the repeated concussions have left them with an invisible, subcellular-level form of traumatic brain injury (TBI) that not only impairs their day-to-day functioning, but also increases their long-term risk of developing neurodegenerative diseases. \"We've got a lot of guys out there that might be 30 years old that have been blown up a dozen times,\" says Kevin Kit Parker, a biomedical engineer at Harvard University in Cambridge, Massachusetts, who is conducting research on TBI. \"And the risk that these guys are going to get a disease like Alzheimer's or Parkinson's is soaring.\" The number of troops affected by this kind of silent TBI has already topped 200,000, according to the Defense and Veterans Brain Injury Center in Washington DC. A survey done by the Rand Corporation, a not-for-profit research firm in Santa Monica, California, suggests it could be as high as 320,000. The Pentagon and the US Department of Veterans Affairs, which are responsible for the health care of current and former troops, respectively, are getting worried about a potential epidemic of disability and dementia. The disorder also presents a major challenge for researchers. No one fully understands what the blast waves are doing to the brain, explains Walter Koroshetz, deputy director of the US National Institute of Neurological Disorders and Stroke in Bethesda, Maryland. Thanks to mounting evidence from professional sports, he says, \"it's been known for a long time that repetitive head injuries lead to chronic degenerative disease. But no one has really got a hold on how that happens.\" Worse, he says, coming up with an effective treatment, and not just alleviating symptoms, could take years: some 20 compounds and interventions have been tested in more than 50 trials in the past 30 years. \"People just look at this field and turn around and run,\" Koroshetz says.  \n                Playing catch-up \n              The good news is that the Pentagon has finally begun to put a high priority on understanding, diagnosing and treating these injuries. But, as officials there now admit, it is playing catch-up after too many years of ignoring the problem. \"The system of care was really in denial for the longest time,\" says Colonel Christian Macedonia, a physician with the US Army who serves as medical-sciences adviser to Admiral Michael Mullen, chairman of the Joint Chiefs of Staff. Partly this was just the culture of the military, says Macedonia: because most soldiers dazed by a blast wave seemed to recover very quickly \u2014 on the surface \u2014 the attitude was, \"Hey, shake it off\". When the symptoms did begin to show, he says, troops with TBI were often misdiagnosed as having PTSD, which has similar symptoms. And veterans of Iraq and Afghanistan have all too often been exposed to physical and psychological traumas that could easily cause both. But most of all, Macedonia thinks that the reluctance to recognize silent TBI was \"the ghost of the Gulf War\" \u2014 the ongoing scientific controversy around the diffuse symptoms described by many troops who served in the 1991 conflict. Study after study has failed to identify a root cause for Gulf War syndrome, he says, so when people started coming forward with TBI \u2014 yet another constellation of complaints that could not be linked to a single cause \u2014 the frustrated military-medicine hierarchy just didn't want to hear about it. That attitude didn't begin to shift until senior military leaders began to sense a dissonance between the official reports they were being given and what they saw when visiting injured troops. One crucial moment came in 2009 when Marine Corps commandant General James Amos toured Walter Reed Hospital in Bethesda, Maryland, and was introduced to a patient who said, with considerable effort, \"General, I know who you are. I have a picture of you and I together in Iraq.\" It turned out that Amos had a copy of the picture, too. It had been taken just two years earlier, when he had posed with a group of marines who had just survived an IED that had detonated directly under their vehicle. Thanks to the vehicle's advanced armour, all of them seemed unscathed. But this young man, a bomb-disposal expert, went straight back to work and was quickly exposed to several more blasts. His physical condition deteriorated rapidly, his life began to unravel and \u2014 after some difficulty getting the military medical establishment to recognize his TBI \u2014 he had been admitted to Walter Reed with severe neurological problems. Amos describes the meeting as a seminal moment for him. \"This TBI business is real, and we've got to get past the point of ignoring it,\" he recalls of his reaction. \"We need to do something about it.\" Mullen was coming to much the same conclusion. Concerned that he wasn't getting a full picture of the brain-injury problem, he asked Macedonia to help organize a 'Gray Team' of researchers and medical professionals with combat experience to look at the realities of TBI on the battlefield. The Gray Team (named after the brain's grey matter) made its first visit to Afghanistan in 2009, says Macedonia, and quickly concluded that Mullen's suspicions were well founded. Official reports had claimed that more than 90% of troops with concussion were being assessed with the 13-point Military Acute Concussion Evaluation (MACE). But when the Gray Team travelled to Afghanistan, the group found that the vast majority of medical professionals \u2014 in both large military hospitals and remote outposts \u2014 didn't even know what a MACE was. \"Doctors couldn't tell you the first thing about it, even though they had all the training materials,\" says Macedonia. No one was enforcing the screening. In parallel with the efforts of the Gray Team, the Defense Advanced Research Projects Agency (DARPA) and the Office of Naval Research were sponsoring a study that for the first time sought to understand how the brain is affected by blast waves, which may cause different injuries from the blunt-force trauma seen in sports injuries. The study focused on breachers: marines who specialize in using explosives to enter buildings. The first paper is only now going through review, but researchers say that they have found evidence of neurological impairment in the instructors, who have had long-term, repeated exposure to low-level blasts. On 21 June 2010, guided in part by the breacher study, the Pentagon announced its first policies for identifying and treating people who may have TBI. Included were the first military-wide mandatory triggers for screening troops, including a rule that anyone within 50 metres of a blast had to be evaluated for signs of brain injury.  \n                The research scramble \n              The Pentagon has also started to make up for its long neglect of brain-injury research. The Department of Defense's Congressionally Directed Medical Research Programs, one of the major conduits for medical-research funding, provided no money specifically for TBI or PTSD between 1999 and 2005. In fiscal year 2006, a small amount, US$3.7 million, went to PTSD, but TBI was not even listed as a research topic. In 2007, however, mounting reports of battlefield brain injuries persuaded Congress to allocate $150 million for TBI research, with another $150 million for PTSD research. That influx of money was enough to open the door to people such as Parker, one of the few medical researchers working on TBI who has combat experience. His research focus had been on cardiac cell mechanics. But in 2002, he served the first of his two tours of duty as an infantry officer in Afghanistan and began to see the effects of TBI on his fellow soldiers. The bombs then were still relatively small and unsophisticated \u2014 artillery shells hooked up to garage-door openers, for instance. But by the time of his second tour in 2009, troops were encountering 200-kilogram fertilizer bombs that could blow unarmoured vehicles to smithereens. As he puts it, only half jokingly, once people started trying to kill him with IEDs, \"I figured I had better turn into some kind of neuroscientist\". In fact, Parker's first formal involvement with brain-injury research began when he attended a DARPA workshop on the subject in 2005. There he learned that one of the challenges was to understand the effects of an explosive blast on the brain. With his background in cell mechanics, Parker immediately began to wonder about integrins, receptors that mediate the cell's attachment to surrounding tissue. Could a blast wave damage them enough to disrupt the proteins' functioning? The idea got a cool reception at first, says Parker, who is now a member of the Gray Team. \"The community that does neuroscience and understands cell mechanics is non-existent,\" he says. \"It's like if you're used to reading English and I hand you a paper in Mandarin Chinese: it's going to be kind of difficult.\" But a grant from DARPA allowed Parker and his group to develop an  in vitro   model to test his idea. And in July, his team published a paper showing that the idea is essentially correct: blast-induced brain injury sets off a cellular chain reaction that disrupts integrin signalling, impairing connections among the brain's neurons ( M. A. Hemphill  et al. PLoS ONE    6,   e22899; 2011 ). The increased funding has also led to progress towards a blood test for diagnosing silent TBI. Currently, clinicians can only infer the presence of such brain damage by cognitive-impairment tests. This means that, because the symptoms overlap with those of other disorders, brain-injury researchers can't always be sure about what they're measuring \u2014 and patients might not be receiving the most appropriate care. Now, after looking at a variety of proteins that seem to become elevated in the bloodstream after a brain injury, army-funded researchers tested two that seemed especially promising in small-scale, phase II clinical trials. Known as ubiquitin C-terminal hydrolase (UCH-L1) and glial fibrillary acidic protein (GFAP), they will soon be tested in large-scale, phase III trials. Working independently of the Pentagon, Bennet Omalu, a forensic pathologist at the University of California, Davis, and the chief medical examiner for San Joaquin County in California, has started to look at veterans' brains for chronic traumatic encephalopathy. First identified in professional athletes involved in contact sports, this neurodegenerative disorder is believed to be caused by multiple concussions. In November, Omalu expects to publish what may be the first case study demonstrating chronic traumatic encephalopathy in a military veteran with silent TBI. \n               Click here for larger image \n               The young man had been exposed to multiple blasts during two deployments to Iraq, explains Omalu, who in 2005 published the first evidence of chronic traumatic encephalopathy, which he had identified from autopsy samples from an American football player ( B. I. Omalu  et al. Neurosurgery    57,   128\u2013134; 2005 ). After returning home, the man began to experience memory problems, mood disorders and self-control problems. Then, aged 27, he committed suicide. With the permission of his relatives, says Omalu, \"I got his brain, examined it, and lo and behold, he had CTE changes\" \u2014 abnormal accumulations of the tau protein associated with Alzheimer's disease and other dementias (see  'Trauma in the brain' ).  \n                Limited access \n              Few medical researchers working on brain injuries have an easy way to collaborate with the Pentagon. Its unique combination of bureaucracy and national-security considerations prevents access to many data and brain-tissue samples that could be useful for medical researchers. For example, access to the Pentagon's Joint Theater Trauma Registry \u2014 a compilation of all military trauma-related data \u2014 is highly restricted, lest enemies use the information to improve their ability to injure US soldiers. \"Giving the NIH access is not impossible, but it is very, very difficult,\" says Major General James Gilman, who heads the US Army Medical Research and Materiel Command at Fort Detrick, Maryland. There have been some signs of change. A joint programme by the US National Institutes of Health (NIH) and the Uniformed Services University of the Health Sciences, both in Bethesda, recently hired a neuropathologist specifically to look at brain tissue of deceased troops, although access to the tissue is not yet guaranteed. Also, the Pentagon and the NIH agreed in August to develop a database for TBI that is similar to the ones created for Alzheimer's disease, autism and cancer research. The idea is to standardize data collection across studies so that researchers can compare results more easily. Among other things, such comparisons should help investigators to get a clearer picture of how well TBI therapies work. They need as much help as they can get, says Koroshetz: for all the progress in understanding the causes and progression of silent TBI, treatments remain elusive. Dozens of clinical trials have been done over the past two decades, looking at everything from antioxidants to hyperbaric oxygen. \"No one has been able to figure out how to make a difference,\" says Koroshetz. \"In terms of outcomes in patients, there is very little, if any, evidence that any single thing works.\" The Pentagon has come a long way from just three years ago, when TBI was mostly ignored. In January, it became mandatory for the military to track all concussive injuries, and troops now receive pre-deployment cognitive testing that can be used as a baseline in case they are later affected by concussion. Experiments with brain-wave measurements are also under way. And with the new reporting requirements, the military is creating what is likely to be the single largest repository of data on TBI. The question is how to keep the momentum going. That may prove difficult, given the United States' mounting budget woes. After the initial boost in 2007, funding levels for TBI research dropped dramatically. In fiscal year 2011, the congressional appropriation specifically for the Pentagon's brain-injury research is expected to be just $45 million. \"Where's the interest, where's the support, where's the national effort?\" asks Colonel Dallas Hack, director of the army's Combat Casualty Care Research Program at Fort Detrick. Brigadier General Robert Thomas, the army's assistant surgeon-general, hopes that the military's involvement is now doing for research and treatment of brain injuries what it has done in the past for yellow fever, trauma care and medical evacuation. For better or worse, he says, \"combat is the greatest catalyst to medical innovation\". But in the meantime, Burt and the hundreds of thousands of other people with brain injuries can only hope that progress comes in time to help them. Once an ambitious multi-tasker, Burt says he now has problems with basic tasks. These days, he can get around the house, and even manage trips to the store \u2014 as long as he makes lists or uses some other form of reminder. \"But I will never be what I was,\" he says. \n                 See Editorial  \n                 page 369 \n               Sharon Weinberger is a Carnegie fellow at Northwestern University's Medill School of Journalism. \n                     Beyond the bomb special \n                   \n                     Defense and Veterans Brain Injury Center \n                   \n                     Brain Injury Research Institute \n                   \n                     NINDS Traumatic Brain Injury Information Page \n                   Reprints and Permissions"},
{"file_id": "477388a", "url": "https://www.nature.com/articles/477388a", "year": 2011, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "The military has a vast array of scientifically valuable data \u2014 some more accessible than you think. No one monitors our planet more closely than the military. Thirty-six thousand kilometres above Earth, US Air Force satellites watch for the heat plume of a ballistic missile. An array of other surveillance satellites patrol lower altitudes. Some can see a rifle from space; others penetrate cloud cover with radar, seeking military hardware or installations. Still closer in, aircraft and drones fly over conflict zones collecting intelligence, and seismometers listen for shudders from an underground nuclear test. Even the deepest oceans are prowled by military submarines, watching their foreign adversaries. Through most of their history, the data collected by this vast blanket of military sensors have been highly classified. But on occasions when scientists are lucky enough to see the data, their view is considerably different from that of the generals. Satellites designed to track missiles can also spot the flaming trails of meteors; aerial photographs of Iraq have allowed archaeologists to trace ancient canals. Even the military's most banal weather satellites collect data on ocean precipitation that are valuable for understanding Earth's energy cycles. After the cold war, some of these data did start trickling out to scientists, mainly in the United States, which has vast military resources and a vibrant scientific community. The flow ebbed after 2000 \u2014 but there are hints that it is resuming, and that more fruitful data collaborations are to come. A group of security-cleared scientists called MEDEA has recently rekindled ties with the US intelligence community to discuss the use of military environmental data for the study of climate change. And an agreement set to be finalized in October between NASA and the US Air Force will give astronomers unprecedented access to data on meteors entering the atmosphere. Some details of those data must be obfuscated to preserve state secrets, but researchers say that the trove nonetheless has enormous scientific potential. \"I think it's become more useful now than it ever has been before,\" says John Orcutt, an oceanographer at the University of California, San Diego, and a member of MEDEA. In the United States, the start of the Manhattan Project in 1942 set the tone for collaboration between the modern military and civilian scientists. The greatest physicists of the era, conscripted to build the atomic bomb, spent years working closely with the US Army. The Pentagon has used outside scientists to help shape its capabilities ever since. It maintains a handful of quasi-academic labs near university campuses, and a truculent panel of independent scientists \u2014 known as the JASONs \u2014 advises it on technical topics such as submarine detection and nuclear weapons (see  page 397 ). At the same time, opportunistic collaborations have sprung up between civilian scientists and the defence establishment. With the advent of nuclear submarine warfare in the 1950s, the US Navy devoted enormous resources to mapping and understanding the sea floor \u2014 including mid-ocean ridges, where Navy mapping yielded clues to the theory of plate tectonics, according to Raymond Jeanloz, an Earth scientist at the University of California, Berkeley, and a long-time member of the JASONs. Seismic networks used to monitor nuclear tests have also mapped earthquakes. Jeffrey Richelson, a historian at the National Security Archive in Washington DC, says that since the 1970s, the US defence department has occasionally shared satellite imagery with civilian agencies in response to natural disasters such as flooding and forest fires. But the military's most sensitive data remained off-limits to academics. In 1967, for example, early-warning radar in Alaska spotted pulsars \u2014 rotating stars that emit a pulsing radio signal \u2014 months before any civilian astronomers did. The staff sergeant who made the observations kept quiet about his discovery for 40 years, until the sightings were declassified in 2007 1 . After the end of the cold war, restrictions began to loosen. In the mid-1990s, astronomers struck up an ad hoc arrangement with Air Force Space Command in which they could ask for data on specific meteors that had been collected by missile-warning satellites. At around the same time, Al Gore, then a Democratic senator from Tennessee, began to ask what the intelligence community could offer climate scientists. Gore was interested in environmental issues and had also served on intelligence and military committees in Congress. He wrote to Robert Gates, then the director of the Central Intelligence Agency, prompting Gates to invite a group of scientists to gain security clearance and take a look at what the military had to offer. After Gore took office as Bill Clinton's vice-president in 1993, the group solidified under the name MEDEA \u2014 Measurements of Earth Data for Environmental Analysis. \"With the proper justification, I could ask for almost anything,\" says William Schlesinger, a MEDEA member and president of the Cary Institute of Ecosystem Studies in Millbrook, New York. Schlesinger used reconnaissance imagery going back to the Second World War to search for climate change's influence on desertification of the Sahara (he didn't find any) 2 .  \n                Trade secrets \n              MEDEA did succeed in getting intelligence satellites to systematically photograph locations of environmental interest in the Arctic, Antarctic and the continental United States. In 1995, the group also successfully lobbied for the release of images from early photo-reconnaissance satellites Corona, Argon and Lanyard, which took more than 860,000 photographs of Earth between 1960 and 1972, recorded on rolls of film. Since then, an entire cottage industry has sprung up involving archaeologists who search for roads and other ancient features in the photos, many of which show tracts of land that have since been consumed by urban sprawl. Jason Ur, an archaeologist at Harvard University in Cambridge, Massachusetts, for example, has used them to map massive canals dug by ancient Assyrian kings 3  (see  'Spying on an ancient city' ). \n               Click here for larger image \n               In the late 1990s, work by Gore and MEDEA led the United States and Russia to declassify Arctic-sea-ice data recorded between the 1970s and 1990s by satellites, submarines and other sources. Scientists have since been able to use those data to reconstruct the gradual thinning of Arctic ice in the decades before civilian monitoring began. \"Without the early classified data, people wouldn't have a clue,\" says Ralph Cicerone, the president of the US National Academy of Sciences. Then, around 2000, MEDEA abruptly halted its work and, in 2009, the informal meteor data from the Air Force stopped flowing too. No one really knows why. But such twists and turns are the price of working with the intelligence community. As Schlesinger puts it, researchers aren't privy to the \"darkened world where a bunch of people make a decision\". Sharing will never be a priority for those charged with defending the United States, says Steven Aftergood, who heads the Project on Government Secrecy at the Federation of American Scientists in Washington DC and has spent decades tracking the US intelligence agencies. Even if information is unclassified, agencies may not want to dole it out freely \u2014 or devote resources to converting it into formats that scientists can use. \"No organization spontaneously discloses and shares its information; that's just a bureaucratic law of physics,\" Aftergood says. Political pressure, such as that applied by Gore, is key to persuading intelligence agencies to share data, he says. These days, new collaborations are emerging. In 2008, congressional committees concerned about climate change quietly reconvened MEDEA to examine whether military- and intelligence-community assets could supply environmental data. The answer was yes, according to Cicerone, who has served as informal chair of MEDEA since 2008. Although intelligence satellites aren't as useful as custom-built instruments, the panel concluded that they could fill some gaps in climate data gathered by civilian satellites, particularly given recent budget shortfalls and launch failures such as the loss of the NASA Orbiting Carbon Observatory in February 2009 4 . Also in 2009, MEDEA persuaded intelligence officials to publicly share images of areas of environmental interest that had, by that time, been photographed regularly for more than a decade. The images are now archived as the Global Fiducials Library, available through the US Geological Survey (USGS). Orcutt says they are \"relatively priceless at this point\" because they are gathered roughly once every few weeks \u2014 more frequently and continuously than those from civilian research satellites. Lindley Johnson, who oversees NASA's Near-Earth Object Observation programme, believes that the space policy unveiled in 2010 by US President Barack Obama, which explicitly endorses data sharing, may have smoothed his efforts to secure data from the US Air Force. Johnson says the new arrangement, which will give astronomers access to data from missile-warning satellites on all meteors \u2014 not just the ones researchers knew about already \u2014 will allow scientists to gain a better understanding of the range of near-Earth objects in orbit. How much science will emerge from these burgeoning relationships remains to be seen. So far, the newly available image libraries of the Arctic and Antarctic have seen only modest use from scientists. \"One of our biggest challenges is to educate the science community about the existence of our programme,\" says Bruce Molnia, executive director of the Civil Applications Committee at the USGS in Reston, Virginia, which oversees civilian use of classified image data. And the members of MEDEA, who have access to the full array of classified data, are, for now at least, using it to address policy questions raised by government agencies \u2014 such as what national security risks are posed by climate change \u2014 rather than conducting fundamental research of their own choosing. Yet Cicerone is hopeful that even more of the intelligence data being collected can eventually be shared. It is now feasible to save almost everything that the military's eyes and ears are recording about Earth. \"As scientists, we don't want observations to be thrown away,\" he says. \"With the Earth, as time passes, you just get one shot at it.\" \n                 See Editorial  \n                 page 369 \n               Geoff Brumfiel is a senior reporter for Nature in London. \n                     Beyond the bomb special \n                   \n                     Global Fiducials library \n                   \n                     USGS EROS data center \n                   Reprints and Permissions"},
{"file_id": "477526a", "url": "https://www.nature.com/articles/477526a", "year": 2011, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "Clinical trials are crumbling under modern economic and scientific pressures.  Nature   looks at ways they might be saved. Developing a drug is a costly gamble. Getting one to market takes, on average, more than ten years and a billion dollars. About 85% of therapies fail in early clinical trials. And of those that survive through to phase III, generally the last step before regulatory approval, only half will actually be approved. Although a promising compound can fail for many reasons, from safety concerns to corporate decisions, many say that a significant number of good drugs are being lost to outdated and impractical clinical-trial designs (see  'The clinical-trial cliff' ). The drugs may work, says Lillian Siu, an oncologist at Princess Margaret Hospital in Toronto, Canada, \"we just don't know how to test them appropriately.\" \n               boxed-text \n             Solving the problem may require fundamental changes to the clinical-trial system to make it faster, cheaper, more adaptable and more in tune with modern molecular medicine. The old paradigm, established in the 1960s, was based on single trials, carried out at single sites, and designed to answer a single question, says Rachel Sherman, associate director for medical policy at the Center for Drug Evaluation and Research at the US Food and Drug Administration (FDA) in Bethesda, Maryland. \"But that's not the world we're living in now.\" Today, the world is more risk-averse, and demands larger trials to pinpoint safety concerns. Compounds that confer only small benefits when compared with existing drugs require large sample sizes for the results to be statistically significant. As a result, trials have become bigger, and often occur at multiple sites, even in multiple countries, and can involve thousands of personnel. The long-heralded era of personalized medicine \u2014 tailoring treatments or combinations of treatments to a specific patient \u2014 adds its own complications. Such an approach has the potential to lower the failure rates of investigational drugs by testing them only in the individuals most likely to benefit. But research teams struggle to identify biological markers that can be used to stratify patients by the characteristics of their disease, and when they do, diseases can get splintered into rare subtypes that each affect just a few individuals. This means that researchers must screen a much larger pool of potential participants. Some researchers are working to improve the fortunes of potential therapeutics, however. Here are four ways that they have come up with to give clinical trials a better success rate. \n                Recruit early \n              Patient recruitment has been a major stumbling block. At least 90% of trials are extended by at least 6 weeks because investigators fail to enrol patients on schedule. Only about one-third of the sites engaged in any multicentre study ever manage to enrol the requisite number, says Kenneth Getz, an expert on clinical research at the Tufts Center for the Study of Drug Development in Boston, Massachusetts. The result: longer, more expensive trials \u2014 some of which may never be completed. Personalized medicine, which has moved apace in cancer research and development, exacerbates the recruitment problem. George Sledge, an oncologist at Indiana University in Indianapolis, offers the example of a class of drug that inhibits enzymes called kinases. Imagine a trial that targets two kinases, one of which is mutated in 25% of patients, and the other of which is mutated in 8%. \"You'd have to screen about 50 patients to find one eligible for the trial,\" he says. For more complex combinations and more diverse patient pools, the problem \"gets to be virtually insoluble\", he says. Researchers and patient advocates are trying to make it easier to find eligible volunteers. They are taking a page from organizations such as the Alpha-1 Foundation in Miami, Florida, which has created a registry of patients with alpha-1 antitrypsin deficiency, a disorder that makes them particularly susceptible to lung and liver diseases, who are willing to be contacted about clinical trials. The Moffitt Cancer Center in Tampa, Florida, for example, runs a Total Cancer Care programme that unites 18 hospitals, compiling medical history, tissue samples and genetic information about each patient's tumour. Samples are all stored for future analysis, and patients can consent to doctors contacting them about trials. A similar union of four institutions is in the works in the Netherlands, says Jan Schellens, an oncologist and pharmacologist at the Netherlands Cancer Institute in Amsterdam. Negotiating agreements across institutions can be tricky. \"Each brings its own values, preferences and interpretation of the privacy laws to the table,\" says Walter Kernan, a neurologist at Yale University in New Haven, Connecticut. \"To set up a system in which a hospital provides investigators at another institution with names of patients and diagnoses requires enormous trust.\" Several groups that have been talking about forming networks are likely to face technical barriers. They need to develop appropriate patient-consent forms, unified databases and ultra-secure networks to connect hospitals. \"Many of these groups don't have anywhere near the information-technology networks you'd need to carry this off,\" says Sledge. But that doesn't mean it won't happen, he adds. \"It's the wave of the future, but it's not going to be simple.\" \n                Skip animals \n              In 2009, researchers at the National Cancer Institute (NCI) in Bethesda faced a dilemma. They had evidence that blocking a protein called AKT, involved in cell death and proliferation, could stave off cancer, but initial animal studies on a compound that blocked the protein suggested that the drug was poorly absorbed by the human body. So researchers developed five formulations of the drug. The question was, should they test each one in animals, or could they skip ahead and evaluate them in humans? In 2006, the FDA and the European Medicines Agency (EMA) introduced guidelines for testing very small 'microdoses' of drugs in humans. These are concentrations less than a one-hundredth of the therapeutic dose. Because the concentrations are so low, the drugs can be tested in a small number of patients without the level of safety data normally required before a phase I study. These early tests, dubbed 'phase 0' studies, would show, for instance, how the drug is distributed and broken down in the body, and whether it hits the right molecular target. Paul Limburg, a gastroenterologist at the Mayo Clinic in Rochester, Minnesota, decided to launch a phase 0 study of the new AKT drug formulations. The results, published in March, allowed the team to pick the one absorbed best by the body for use in future trials ( J. M. Reid  et al. Cancer Prev. Res.    4,   347\u2013353; 2011 ). Proponents of phase 0 testing argue that it makes sense to get human data quickly. About one-quarter of the molecules entering clinical trials fail because of \"poor pharmacology\", says Schellens. The drug may not be readily absorbed, for example, or may not reach its target organ. With a simple test in humans, he says, \"you could kill that drug much earlier, before you have invested so much time and money\". But Razelle Kurzrock at the MD Anderson Cancer Center in Houston, Texas, is concerned that developers will kill promising drugs in response to negative microdose data, even though the drug may work at higher, therapeutic concentrations. To test this hypothesis, she and her colleagues are conducting phase 0 trials with FDA-approved drugs to find out whether any would have failed at that stage. And phase 0 trials may be tiny, but they are not easy. They require a test sensitive enough to detect the minute quantities of the drug in the body \u2014 and possibly also ways to track its mechanism of action. For veliparib, a potential anti-cancer drug, Abbott Laboratories, headquartered in Abbott Park, Illinois, started developing assays that were robust and sensitive enough to detect tiny concentrations of the drug more than a year before its researchers embarked on a phase 0 trial, says James Doroshow, the NCI oncologist who led the trial. Few, says Schellens, plan so far ahead. \"Often when people start thinking about the phase 0 trial in the drug development process, they are too late.\" \n                Use models \n              Although phase 0 trials could help to wean researchers off pharmacology studies in animals, moves are also afoot to bring mouse experiments closer to the clinic. The problem with most of today's animal studies, says Eric Holland, a neurosurgeon at the Memorial Sloan-Kettering Cancer Center in New York, is that they typically focus on safety rather than efficacy. And they rarely predict exactly what will happen in the clinic, because the doses, formulations and schedules of medication differ from those given to the animals. A person with prostate cancer, for example, does not usually take an experimental medication until the standard hormone therapy has failed. But the mice that experimental drug was tested on are unlikely to have received the same treatment. Pier Paolo Pandolfi, a cancer researcher at Harvard Medical School in Boston, has therefore pioneered a technique called the 'co-clinical trial', in which mice with similar disease characteristics are treated in a similar way to the humans. Holland recently used the approach to test combinations of drugs that target AKT and another protein, called mTOR ( K. L. Pitter  et al. PLoS ONE    6,   e14545; 2011 ). The mouse trial allowed the researchers to try more concentrations than the human trial, and suggested new ways of screening patients for inclusion in future clinical trials. \"The more complicated the trial, the more likely it is that the co-clinical strategy would be beneficial,\" Holland says. In 2009, the NCI invested US$4.2 million in Pandolfi's co-clinical trials in prostate and lung cancer. In them, mice receive the same therapies as people do before the start of experimental treatments. At The Jackson Laboratory, based in Bar Harbor, Maine, which has a large personalized medicine programme, researchers graft tumour cells taken from patients into mice. These mice can then be given the same treatment as the patient, so that researchers can track how the tumour cells respond, and use this information to tailor future treatment for the patient. \n                Alter course \n              Perhaps nowhere in science is ignorance prized as highly as in clinical trials. To keep expectations or biases from inadvertently influencing the results, patients and investigators are often kept in the dark about who is receiving what treatment until the end of the study. At best, an external committee may take a secret peek at the results mid-trial to make sure it is safe for the experiment to continue. But looking just once at the data is \"like driving home from work and only opening your eyes once to see where you're going\", says Donald Berry, a statistician at the MD Anderson Cancer Center. Berry specializes in designing 'adaptive' trials, which can change course as the data roll in. If one treatment regimen seems to be more successful, for example, researchers might increase the proportion of participants that should receive that treatment. These trials can also be used to identify biological markers, such as mutations or altered metabolite levels, associated with the success or failure of a given regimen. For example, before women enrol in an ongoing adaptive trial called I-SPY2, run by Berry and Laura Esserman, an oncologist at the University of California, San Francisco, their breast-cancer tumours are biopsied and tested for genetic markers. The genetic make-up of their tumours determines whether they are eligible to enter the trial, and which group of treatments they should be randomized to. But because the results are reanalysed each time a woman completes her treatment, the participants are more likely to receive a treatment that has worked in people with genetically similar tumours. If a treatment does not perform well in any patients, it is cut from the programme; other treatments can then be added to the mix. The FDA and the EMA have encouraged drug developers to embrace these designs. In addition to high-profile trials such as I-SPY2, Berry says that he has designed trials running the whole gamut of disease, from diabetes to pandemic flu. \"Virtually every pharma company is doing this,\" he says. But Getz disagrees and says that he's seen a more muted response to adaptive trials. \"Our clinical-trial protocols have become too complex across the board,\" says Getz. \"And adaptive designs add to the logistical complexity.\" One issue is statistical: the more tests on the data a researcher conducts, the more likely it is that they will introduce false positive results. Scott Evans, a biostatistician at the Harvard School of Public Health in Boston, is more concerned about bias, however. Even if it is an independent panel that views the data and adapts the trial, it is often impossible to hide a change in protocol from everyone. \"Any in-flight adjustment you make in a clinical trial is potentially observable,\" he says. \"Now you've created the potential for an operational bias.\" \"There has been a lot of controversy about adaptive trials,\" Siu says. \"But as the molecular era arrives, they will become more and more relevant.\"  \n                Tear down or tweak? \n              While researchers dream up new, better ways to design clinical trials, many involved acknowledge that the changes with the biggest impact will probably be more bureaucratic than conceptual. Simply standardizing the forms used to record clinical-trial data would reduce costs and cut down on record-keeping errors and omissions. The NCI is now rolling out a data-management system that will standardize data entry across all 2,000 sites that conduct NCI-sponsored trials, says Doroshow. The FDA is also looking at ways to cut down on reporting requirements and paperwork, so that investigators can submit summaries of case reports rather than each individual document. Trials that involve multiple sites also have to get approval from each institutions' review committee, referred to as an institutional review board. This can take months or even years. To adapt to the multicentre climate, the US Office for Human Research Protections in Rockville, Maryland, which oversees human studies funded by the US National Institutes of Health, has proposed changes to its guidelines that would require designation of a single review board for each project. However, individual research centres may be reluctant to loosen their hold on the reins. The clinical-trial system may be outdated, but at least it is not inflexible. Most radical designs have probably been tried in some capacity. Getting rid of control groups? It happens in rare-disease trials. Blending phases? Some trials are already starting to do this. Still, even small tweaks require a leap of faith when they are being made to a complex system with such high stakes \u2014 the protection of human health. The way that clinical trials are designed and run may need to be revolutionized, but \"there's not going to be a single solution because it's a multifactorial process,\" says Richard Schilsky, an oncologist at the University of Chicago, Illinois. \"Everybody who is a stakeholder in the clinical-trial process has to contribute to the solutions.\" Heidi Ledford writes for Nature from Boston. \n                     Nature Medicine \n                   \n                     Nature Reviews Drug Discovery \n                   \n                     Critical Path Initiative \n                   \n                     Total Cancer Care \n                   \n                     I-SPY2 trial \n                   Reprints and Permissions"},
{"file_id": "477524a", "url": "https://www.nature.com/articles/477524a", "year": 2011, "authors": [{"name": "Colin Macilwain"}], "parsed_as_year": "2006_or_before", "body": "The United States' 2009 financial stimulus bill has provided research with breathing space, rather than the sharp shot in the arm that many anticipated. When Cathy Lord arrived in New York last month, her US$1.2-million grant was about to run out. The autism researcher had won the funding from the National Institutes of Health (NIH) under the American Recovery and Reinvestment Act (ARRA), better known as the stimulus bill, and the money had helped to support her 15-strong team at the University of Michigan in Ann Arbor for two years. But the expiry of such a major grant didn't leave Lord's team in the lurch \u2014 far from it. The grant was not like a standard NIH award. Instead of paying for experiments to test a hypothesis for publication \u2014 something that usually takes at least three years \u2014 the money allowed Lord's team to compile its existing knowledge, preparing a kit that other researchers can use to screen people for social and behavioural indicators of autism. In that sense, the grant was more of a bonus than a lifeline. Now, Lord is about to open an autism clinic and research centre at NewYork-Presbyterian Hospital in White Plains, and about half of her team have followed her there. For them, she says, the end of the ARRA money is no different from the usual rough and tumble that results from the coming and going of research grants. \"If you find good people you just try and keep the ball rolling,\" she says. \n               boxed-text \n             Such a smooth transition is not what many expected for scientists facing the end of a stimulus grant. When the bill \u2014 now estimated to be worth $840 billion \u2014 was signed into law by President Barack Obama in February 2009, it contained more than $52 billion for research and development (see  'Spending a windfall' ), of which $15 billion was expressly for scientific research at the NIH, the National Science Foundation and the Department of Energy's Office of Science (see   Nature  461, 856\u2013857; 2009 ). The stimulus package as a whole was designed to create jobs and ease the pain of the recession, and at first the administration pledged to get this money distributed and spent as quickly as possible \u2014 the NIH, for example, devoted much of its allocation to two-year grants that were meant to run from October 2009 to this month. So even as scientists scrambled to get a share of the windfall, some universities and science lobby groups were warning of a 'funding cliff' when the money \u2014 which would have boosted total NIH funding by about one-sixth if it was spent over two years \u2014 came to an abrupt end.  \n                The cliff that wasn't \n              From the beginning, however, the funding pulse didn't have quite the anticipated effects. Duke University in Durham, North Carolina, for example, expected a hiring rush after it attracted $210 million in ARRA funds, making it one of the ten most successful universities in the country in this regard, says Jim Siedow, Duke's vice-provost for research. But staffing barely budged. \"We gave a party that nobody came to,\" he says. \"A lot of people used the money to keep the people they already had.\" And despite the early political pressure to get the money out of the door quickly, agencies have allowed funds to be released gradually, to avoid waste. The NIH quietly relented earlier this year and is permitting 'no-cost extensions' that allow team leaders, including Lord, to defer spending and wind down their projects neatly. The agency has now spent $7 billion of its $10.4-billion allocation, and Sally Rockey, the NIH's deputy director of extramural research, says that \"a big chunk of it will still be ongoing\" until 2013. The result, at Duke as elsewhere, has been that the ARRA funding will help to sustain research departments for at least another two years. Siedow says that a detailed internal analysis suggests that Duke will lose only \"a very small number\" of academic positions as the funding winds down. \"We'd been concerned that funding would fall off a cliff; but so far, it hasn't happened,\" he says.  \n                Measured recovery \n              Other research funding agencies report a similarly gradual tail-off. The Department of Energy's Office of Science says that it expects substantial spending to continue for some time \u2014 it expects to disburse about $260 million in fiscal year 2012 and $170 million in fiscal year 2013. The NSF, which was under less pressure than the NIH to rush the money out, has so far distributed most of it to previously unsuccessful applicants for standard 3\u20135-year NSF grants. Only $1.4 billion of the agency's ARRA allocation has been spent so far. Despite this stretching out and morphing, the ARRA is set to leave a distinct legacy in a few areas. One of these is metrics. Kei Koizumi, assistant director for federal research and development at the White House Office of Science and Technology Policy, says that ARRA funding is \"the most extensively documented set of research investments we've ever seen\". Because of the initial expectation that stimulus funding would generate jobs, its recipients are required to report regularly on the employment impact of their funding. Their returns are compiled and posted immediately on a public website ( http://www.recovery.gov ); these responses suggest that the NIH ARRA funding, for example, is currently supporting the equivalent of about 20,000 full-time postions. However, such self-reported data are not always accurate. The passage of the stimulus bill also gave birth to Star Metrics, an ambitious cross-government programme set up to monitor research inputs and outputs including, in its first phase, jobs (see   Nature  465, 682\u2013684; 2010 ). Star Metrics started off tracking the employment impact of ARRA and non-ARRA research spending at about 70 volunteer research universities, representing an estimated 40% of the US university research system. The results of this analysis, which takes its data directly from university payroll systems, are set to be published shortly. But further detail about the impact of ARRA research spending may be slow to emerge. Congressional committees have shown little interest so far in raking over the data, according to research lobbyists and university leaders. \"Priorities change very rapidly now,\" says Chris King, a staff member with the House of Representatives science committee. Formal programme evaluation, as envisaged by Star Metrics, is just \"not very critical for people on Capital Hill\", he says. In the meantime, Star Metrics has shifted its focus away from the ARRA, towards the impacts of all research. \"What has happened is that the notion of accountability has spread out much more broadly,\" says Julia Lane, an economist and the NSF official who leads Star Metrics. The programme is planning a second stage, in which every researcher in the United States who receives federal funds is assigned an 'identifier', opening the road to a national database that keeps tabs on everything from total federal funding and employment to publications, citations and patent applications. Although much of the ARRA research money has gone on grants, assumed to be the fastest way to spend the money, some of it is building a concrete legacy in the shape of investment in bricks, mortar and instrumentation. These include a handful of stand-alone projects \u2014 such as a $16-million lab-animal facility at Duke \u2014 and many injections into projects already under way. One of the largest such investments is the $150 million being spent by the Department of Energy to speed up construction of the $1-billion National Synchrotron Light Source II: a high-energy photon source at Brookhaven National Laboratory on Long Island, New York, that will enable materials scientists and structural biologists to look at molecules and crystals at the nanoscale. The project was approved in 2005, but the energy department lacked the funds to build it quickly; the facility is now expected to be finished in 2014. \"ARRA enabled us to pull forward the construction work, and we'll have a more fully outfitted facility, with better beamlines and laboratories,\" says Steve Dierker, head of photon science at Brookhaven. The facility will be 10,000 times brighter than the light source that it replaces, says Dierker. The stimulus has also funded experiments in approaches to research funding. Some of these set out specifically to support 'high-risk' research projects that might struggle to attract support through the normal grant peer-review process \u2014 most notably at the Department of Energy's new branch, the Advanced Research Projects Agency\u2014Energy (ARPA-E), which was conceived in 2004 but got funded, to the tune of $400 million, only under ARRA. ARPA-E has been backing research in untested areas, such as the development of microbes engineered to produce biofuels. More commonly, as at the NIH, ARRA has forced a compression of the peer-review process, and resulted in grants that last for less time than normal. Some of these, such as Lord's, allow research groups to expand or wrap up projects, but not necessarily test a new hypothesis. \"We did an experiment with two-year grants, and we'll follow up to see how that worked,\" says Rockey. One of the biggest legacies of the science stimulus, however, will be that thousands of students have stayed in science longer than they might otherwise have done. \"ARRA has provided postdocs and graduate students with great training that'll help them with the rest of their careers,\" says Rockey. \"These people will move on to do productive and wonderful things.\" Another legacy will be the ideas, large and small, that have been germinated by ARRA funding \u2014 even if their creators are uncertain that they can bring them to fruition. Dan Gauthier, a physicist at Duke, used his three-year, $480,000 NSF grant to study the interaction of laser light with very cold rubidium atoms. Gauthier is delighted to have had the chance to pursue a project that would otherwise have been out of reach; he thinks that his findings could eventually help in the development of quantum communication networks. At the same time, he worries about next summer, when the grant supporting him and his two research assistants will end. To keep his team intact, he will have to win normal funding from the NSF. \"If we don't get funded, it'll just be devastating,\" he says. \"This was all predicated on the idea that the economy would get better after two or three years: now we don't know that it will.\" Colin Macilwain is a freelance writer based in Edinburgh, UK. \n                     Recovery.gov: Track the stimulus money \n                   Reprints and Permissions"},
{"file_id": "477386a", "url": "https://www.nature.com/articles/477386a", "year": 2011, "authors": [{"name": "Sharon Weinberger"}], "parsed_as_year": "2006_or_before", "body": "Basic research funded by the Pentagon is facing an uncertain future. In 2005, as roadside bomb attacks were claiming ever more lives in Iraq, senior Pentagon officials called on the academic community to join a 'Manhattan Project' to counter these improvised explosive devices. By invoking the Second World War race to build the atomic bomb, military leaders seemed to be pushing for a massive investment in science that could, like the first nuclear weapon, turn the tide of war. Academics responded with a collective shrug. The Pentagon's grand rhetoric wasn't matched with any great influx of funding for science, and it wasn't clear how any one technology could help fight a loosely organized, deliberately low-tech enemy. Besides, says Julia Erdley, deputy science adviser to the Pentagon's Joint Improvised Explosive Device Defeat Organization, \"we are looking for near-term solutions\". Six years later, the Department of Defense (DOD) has spent more than US$17 billion on countering improvised explosive devices, but, as Erdley suggests, the vast majority of that money has gone on implementing known solutions such as stronger armour for vehicles and personnel, not advanced research. Roadside bombs remain the single biggest killer of US and allied troops in Iraq and Afghanistan. Pentagon officials now admit that there is no technological 'silver bullet' for preventing, detecting and disarming roadside bombs, and their Manhattan project rhetoric has long since been replaced with more sober talk of disrupting highly distributed terrorist networks (see    Nature    471,   566\u2013568; 2011 ). The failure to mobilize the scientific community for the war on terror stands in stark contrast to what happened in the cold war, when Pentagon-supported science boomed, and was viewed as a crucial asset to counter Soviet technological prowess. Today's military has to operate in much more ambiguous and complex environments, in which 'soft' skills such as trust-building, intelligence-gathering and cultural insight may prove as decisive as any technological advantage. Given this new military reality, it is becoming less clear what science and technology research has to offer.  \n                Broken programmes \n              That uncertainty may help to explain what some now see as a lack of sustained Pentagon support for blue-sky basic science and a preference for applied research with a short-term pay-off. \"We believe that important aspects of the DoD basic research programs are 'broken' to an extent that neither throwing more money at these problems nor simple changes in procedures and definitions will fix them,\" wrote the JASONs, a defence advisory group made up of independent scientists, in the most recent publicly available assessment of Pentagon science and technology. (Completed in 2009, the JASON report was released to the public in May 2010.) On the surface, the Pentagon's science base looks healthy enough, and it supports a vast array of research (see  page 369 ). The science and technology budget, which consists of basic research, applied research and advanced technology development \u2014 budget categories 6.1, 6.2 and 6.3 in Pentagon parlance \u2014 has fallen from its post-11 September 2001 peak in 2005, when it reached some $14.7 billion per year (see  'Rise and fall' ). But most of that decline came in the advanced-technology category, not basic research. And the total still stands at about $12 billion a year, nearly twice the $6.8 billion budget of the US National Science Foundation, and much higher than defence science expenditures in Europe, where countries have traditionally spent only a fraction of what the United States spends on the military. In 2009, the most recent year for which figures are available, the members of European Defence Agency \u2014 every country in the European Union except Denmark \u2014 spent an aggregate of only \u20ac2.26 billion (US$3.1 billion) in the 'research and technology' category, the vast majority of which goes to the development of advanced aircraft and other weaponry, not science. \n               Click here for larger image \n               Pentagon research also had a champion in former US defence secretary Robert Gates, a one-time CIA director who had been president of Texas A&M University in College Station before he came to the DOD in 2006. For example, Gates was well aware that in many academic fields, notably the social sciences, relations with the military have been fraught and often hostile since the Vietnam War (1955\u201375). In 2008, hoping to rebuild those ties, Gates proposed Minerva: a basic-science programme that would specifically focus on the social sciences. Gates saw Minerva as emblematic of military science's changing mission. \"The challenges facing the world require a much broader conception and application of national power than just military prowess,\" he said in announcing the programme. \"The government and the Department of Defense need to engage additional intellectual disciplines \u2014 such as history, anthropology, sociology and evolutionary psychology.\"  \n                Magnificent seven \n              Beginning with the president's fiscal year 2012 budget request this past February, Gates set a target of 2% annual growth in the basic-science budget over the coming years, and pledged to hold the applied- and advanced-technology accounts steady. That was particularly heartening news for those disciplines to which defence funding is crucial. About one-third of all the funding for oceanography research and computer science in the United States comes from the Pentagon, for example, as does a majority of the funding for mechanical engineering (see    Nature    466,   656\u2013657; 2010 ). \"Physics research is no longer tied so exclusively to military funding,\" says David Kaiser, a historian of science at the Massachusetts Institute of Technology in Cambridge, \"although it still has a large role.\" And the defence department is also now the largest single source of funding for research into traumatic brain injury (see  page 390 ). Shortly before stepping down on 30 June this year, Gates signed off a new science and technology plan for the Pentagon. The policy includes a list of priorities \u2014 which Pentagon insiders immediately dubbed the 'magnificent seven' \u2014 to be used for budget planning over the next five years. And yet, Gates's efforts also illustrate some of the many strains in the Pentagon's science and technology programme. Minerva, in particular, has met with decidedly mixed reactions, as academics question whether the Pentagon has any business setting the course of social-science research (see    Nature    455,   583\u2013585; 2008 ). And the magnificent-seven list, which includes topics such as research to counter weapons of mass destruction, and engineering resilient systems, arguably hasn't done much to inspire the scientific community. \"It would be hard to categorize it as bold or prescient,\" says Mark Lewis, an aerospace engineer at the University of Maryland in College Park and a former chief scientist of the US Air Force. It is essentially a compendium of the individual military services' wish lists. A Pentagon spokesperson also says that there are no funding goals tied to the magnificent seven. A more fundamental issue is what many observers see as a lack of high-level vision and coordination for Pentagon research. In earlier decades, that coordination was carried out by the director for defence research and engineering (DDR&E), a position established in 1958 after the Soviet launch of the Sputnik satellite. Located in the Pentagon's power centre \u2014 the Office of the Secretary of Defense \u2014 this director oversaw all of the department's science and technology programmes. But in the late 1970s, the position ceded much of its authority over budget and policy to the under secretary for acquisition \u2014 the chief weapons buyer. The DDR&E, recently renamed the assistant secretary of defence for research and engineering, was left with a limited staff, overseeing a vast portfolio of science accounts at the individual services and the Pentagon-wide Defense Advanced Research Projects Agency. In recent years, the office has become marginalized, with its staff fending off spending cuts in the science budget, rather than being a driving force in military science policy. And even when the office succeeds in defending basic research, according to the 2009 JASON report, the research inexorably gets pushed towards immediate applications. In a sample of 258 basic-research projects funded by the Air Force Office of Scientific Research in 2007, and a similar sample funded by the Army Research Office, the group found that as many as 81% \"are not, even by a generous stretch, 6.1 research\". The JASONs urged the defence department to elevate and strengthen the DDR&E office, and make it independent of weapons acquisition. But the defence-department bureaucracy has given no sign that any such change is in the offing. In the meantime, the Pentagon faces a more urgent threat. \"We're starting to see a downward trend in R&D funding,\" says Todd Harrison, a fellow at the Center for Strategic and Budgetary Assessments in Washington DC. The Obama administration has already asked the Pentagon to cut $400 billion from its budgets over the next 12 years \u2014 the current budget is about $700 billion per year \u2014 and there's no guarantee that those cuts won't be expanded as Congress struggles to trim the US federal deficit, or that the money won't come from the science and technology budget. Lewis sees Gates's commitment to increasing basic-science spending as one of the most important changes in the Pentagon's science policy over the past few years. The question now, however, is whether Gates's successor, former CIA director Leon Panetta, will uphold that commitment. Lewis points to the new defence secretary's confirmation hearings on Capitol Hill, when he was specifically asked that question. Panetta replied that he valued basic research \u2014 but that \"all defence appropriations must be considered during this time of budget constraints\". In other words, everything is on the table for cuts, including science. \"That would be a profound change,\" says Lewis. \n                 See Editorial  \n                 page 369 \n               Sharon Weinberger is a Carnegie fellow at Northwestern University's Medill School of Journalism. \n                     Beyond the bomb special \n                   \n                     Department of Defense Research & Engineering Enterprise \n                   \n                     DARPA \n                   \n                     DOD Armed With Science \n                   Reprints and Permissions"},
{"file_id": "478022a", "url": "https://www.nature.com/articles/478022a", "year": 2011, "authors": [{"name": "Brendan Maher"}], "parsed_as_year": "2006_or_before", "body": "The first clinical uses of whole-genome sequencing show just how challenging it can be. The first thing Debbie Jorde noticed about her newborn daughter was that her arms were bent at unnatural angles. She had other problems, too: a cleft palate, eight fingers, eight toes and no lower eyelids. She would eventually be diagnosed with Miller syndrome, a disease so rare that doctors have long assumed that each case arises through spontaneous mutation, rather than being passed down through families. Doctors told Jorde that her chances of having a second child with the syndrome were less than one in a million. They were wrong. Jorde's son, born three years after his sister, had the same features. Lynn Jorde, Debbie's current husband and a geneticist at the University of Utah in Salt Lake City, still cringes when Debbie recounts what the doctors had told her. \"The right answer for that situation is that there have been so few cases that we really can't predict the risk,\" he says. Thanks to next-generation genome sequencing, Debbie and her children now know the family's genetic risks. Lynn and his collaborators had been talking about sequencing the genomes of an entire 'nuclear' family affected by a genetic disease, both to identify the mutation responsible and to investigate how genes are inherited in unprecedented detail. Debbie, her former husband and her now-adult children, Heather and Logan Madsen, were happy to be take part, and in 2009 became the first family in the world to have their genomes fully sequenced 1 . Over the course of six months, the research team cross-compared the whopping amount of DNA data from the four genomes. With the help of a parallel sequencing effort that included others with Miller syndrome 2 , the researchers identified the gene involved, called  DHODH , which encodes a protein involved in the synthesis of nucleotides. The disease, it turns out, is recessive. In this case, both parents carried a single mutated copy of the gene, so their chance of having a child with the syndrome was actually one in four. The analyses also revealed that the children had a second recessive genetic disorder, primary ciliary dyskinesia, which affects lung development. Before that discovery, says Debbie, \"We never knew why they kept getting pneumonia.\" \n               boxed-text \n             Families like Debbie Jorde's are part of a small but growing vanguard of people, mostly with rare diseases and cancers, whose genomes have been sequenced to help diagnose or understand their condition. Although knowing the sequence didn't alter treatments for Heather and Logan, some individuals are being sequenced with that intent. A boy in Wisconsin was given a risky but life-saving bone-marrow transplant last year on the basis of a partial genome sequence 3 ; a woman with leukaemia was spared a similar procedure after her genome was sequenced 4 ; and genome sequencing was used to refine the therapy given to twins with a rare disorder (see  '6 billion to one' ) 5 . Most of those involved so far have been lucky enough to know the right people \u2014 researchers with an interest in clinical genetics \u2014 or determined enough to seek them out, and many, such as Debbie Jorde's family, were taking part in research projects. But now, with genome sequencing becoming much cheaper and faster, clinical programmes are starting up around the world that will routinely analyse genomes for those who might benefit from the information. Illumina, which is based in San Diego, California, and provided the sequencing machines for many of the programmes, offers whole-genome sequencing for as little as US$7,500 for people with life-threatening disease, and for $10,000 for people with cancers that require the sequencing of both tumour and non-cancer cells. As prices fall further, some say that prescribing a genome sequence or analysis will become akin to requesting a magnetic resonance imaging (MRI) scan. \"It's just like any other test in medicine. There's nothing remotely special about it,\" says David Bick, a clinical geneticist at the Medical College of Wisconsin in Milwaukee. But, he adds, \"people will cry and scream and yell about that statement\". That's true: unlike the results of most medical tests, a genome sequence provides a vast amount of difficult-to-interpret data, not all of which will be necessary for diagnosing or treating the patient's condition and which could provide unwanted clues to future health risks. The few success stories published so far also suggest that wringing information from the human genome and counselling patients and their families adequately may be too big a burden for medical systems that are already stretched to their limits. \"You can't immediately jump from those few profound but limited stories and think that you can reduce this to practice for clinical care,\" says Eric Green, director of the National Human Genome Research Institute (NHGRI) in Bethesda, Maryland. Still, from the pioneering cases, much can be learned.  \n                Rare births \n              Take Nicholas Volker. From the time he was born, an undiagnosed condition ravaged his intestines, sometimes causing fistulae: holes that ran from his gut through to the outside of his body, leaking faeces and requiring surgery. By the time he turned three, Volker had been in an operating room more than 100 times. Doctors hypothesized that he had an immune deficiency and that a bone-marrow transplant might correct the problem. But a number of tests, including the sequencing of several genes, were inconclusive. After intense deliberation, a team at the Medical College of Wisconsin was cleared to sequence Volker's exome, the 1\u20132% of the genome that codes for proteins and key regulatory RNA molecules. Using computational tools, the team combed Volker's DNA for sequences that vary from person to person. They compared these with known variants in the general population, with variants associated with diseases and with related sequences in other species, looking for a mutation that might have caused the problem, says David Dimmock, a clinical geneticist at the college. It took, \"basically one person staring at a computer for three and a half months\", he says, but eventually they identified a mutation on the X chromosome in a gene called  X-linked inhibitor of Apoptosis , or  XIAP   (ref.  3 ). A deficiency of the protein encoded by this gene is known to put patients at high risk for a deadly immune-cell disorder, and a bone-marrow transplant suddenly became imperative. More than a year later, Dimmock says, Volker is doing well. What started as an experiment has become a programme at Wisconsin, where Dimmock, Bick and their colleagues now aim to provide comprehensive whole-genome sequencing for patients. The team is focusing on people with rare disorders that are thought to involve a genetic defect, and in whom identifying that defect is likely to inform the course of treatment. Bick says that of 48 patients evaluated for the programme, 17 have been accepted, and their families have gone through six hours or more of genetic counselling before sequencing. Insurance companies have agreed to foot the bill for at least two of the cases. Their rationale is straightforward, says Tina Hambuch, a senior scientist at Illumina's clinical services laboratory, which has been doing the sequencing for this programme. A full genome sequence can be less expensive than a series of single genetic tests, and might clarify whether a costly treatment is required. \"There are cases where it's cost effective,\" Hambuch says.  \n                Genome factories \n              Other institutions are following suit. In the United Kingdom, the Wellcome Trust Centre for Human Genetics at the University of Oxford has made plans with Illumina to sequence 500 genomes from people \u2014 some from the same family \u2014 with a wide range of conditions. The Undiagnosed Diseases Program at the National Institutes of Health in Bethesda has been running a sequencing programme since 2008. It has sequenced more than 140 exomes and 5 genomes in its attempts to find the molecular underpinnings of diseases that have eluded diagnosis. The programme was so overwhelmed by interest that it temporarily stopped accepting applications a few months ago. Green says that \"now is the time to push the accelerator\". Clinical geneticists often talk about tackling Mendelian disorders: diseases thought to involve a single gene and that roughly obey the rules of inheritance drawn up by Gregor Mendel in the nineteenth century. These conditions may account for as many as 20% of paediatric hospitalizations worldwide and a large share of health-care costs. Yet their genetic basis is often unknown. The compendium of such conditions, called Online Mendelian Inheritance in Man (OMIM), currently contains just under 7,000 disorders, about half of which have been assigned a molecular cause. This autumn, Green says, the NHGRI will announce the winners of its Mendelian Disorders Genome Centers grants, which will fund sequencing centres looking for causes of the rest. Still, many researchers worry that it will be difficult to make clinical use of most genomes. At the Undiagnosed Diseases Program, the misses have certainly outnumbered the hits so far. \"I think we've learned a lot about how hard evaluating an exome is,\" says Thomas Markello, from the medical-genetics branch of the NHGRI. \"I'm most concerned that people don't recognize that what's been published to date are the success stories.\" Many researchers say that genome sequencing could be used in diagnosis and therapy of cancer more easily than in rare diseases. Clinicians are already doing sophisticated analyses of some tumours in order to tailor therapies to the patient's genetic characteristics; a genome sequence provides even more molecular detail. For example, an individual's cancer genome sometimes reveals defects in a pathway that might point to use of a known drug, but were not apparent from standard tests. In 2007, a 78-year-old man in Canada with a rare tongue cancer that had spread throughout his body was being treated at the British Columbia Cancer Agency in Vancouver. There was no approved treatment for his type of cancer and \u2014 being what doctors described as a \"savvy sort\" \u2014 he and his clinician convinced scientists at the agency to sequence the cancer's genome. The scientists also analysed its transcriptome, revealing both the sequence and the amount of RNA that the tumour was producing. The team then compared these data with those for other cancers and for the patient's normal cells. The researchers homed in on  RET , a gene known to promote cancer, which was duplicated in the tumour genome and churning out RNA. Several drugs are known to inhibit the protein encoded by this gene. Marco Marra, director of the cancer agency's genome-sciences centre, says that \"after much agonizing and hand-wringing\", the clinical team prioritized these drugs and tried the top one, sunitinib. The cancer stabilized for several months on this and a second treatment, but eventually started to spread again. An analysis of the recurring tumours showed that different cancer-promoting pathways had been activated 6 , making the tumours resistant to the first drug, but possibly responsive to others. Unfortunately, by then it was too late to do more: the man died.  \n                Unstoppable train \n              Marra's group is now setting up a project to better diagnose subtypes of another cancer, acute myelogenous leukaemia, using transcriptome and other sequencing methods. Partly inspired by Marra's efforts, Elaine Mardis, a geneticist at Washington University in St Louis, Missouri, and her collaborators have used genome sequencing to try to help a handful of people with cancer, including the woman with leukaemia 4 . The woman had been treated and had gone into remission, but standard tests were unable show conclusively whether she had acute promyelocytic leukaemia (APL) \u2014 which generally has a good outcome with standard therapy \u2014 or a type of leukaemia that would require aggressive follow-up treatment, such as a bone-marrow transplantation. Over about seven weeks, the team sequenced the cancer's genome and found a gene fusion that was consistent with APL. Mardis is enthusiastic about the approach, but notes its limitations. \"It's another piece of evidence,\" she says. \"It's not going to be the only thing that you're looking at when going to a patient diagnosis.\" Moving whole-genome sequencing from research to clinic is beset with challenges. Unlike in research, DNA sequencing that is intended to inform a diagnosis must be done in accredited laboratories, such as those used by Illumina. The institutional review boards that oversee research in humans have not reached a consensus on whether approval is needed for clinical genome sequencing; and the US Food and Drug Administration is yet to work out how to regulate the coming wave of clinical sequencing. Many researchers and clinicians worry that health systems don't have enough people well versed in genomics or bioinformatics to interpret the flood of data. What's more, say experts, function and disease information for the human genome is scattered across scientific articles and databases that are hard to troll through and aren't always correct. Sequence analysis is where most costs now lie. Hambuch says that for the few research projects on which Illumina has collaborated, just identifying all the variants in a genome has taken two to three weeks. \"That's a lot of effort from high-skill people,\" she says. The information could also overwhelm patients. Medical geneticists and ethicists have long worried about finding genetic pointers to disease risks that are unrelated to the illness being treated. With a full genome sequence, the likelihood of such incidental findings shoots up. The situation is particularly tricky for young patients. Do parents have the right to decide for them what information is revealed? This is where many of those hours of genetic counselling are spent, says Bick. For these reasons, Stephen Kingsmore at Children's Mercy Hospital in Kansas City, Missouri, argues that clinical sequencing should be limited in scope. He advocates sequencing just what he calls the Mendelianome, the genetic regions known to be involved in inherited diseases. \"Ethically, legally, socially that's going to be more acceptable,\" he says. His group is developing methods that use a panel of mutations associated with just over 600 recessive diseases for such screening. Doing much more than this, he says, puts research goals ahead of the patient. But some geneticists think that the train is unstoppable. \"Once you demonstrate how informative this technology is, I think this is going to be widely adopted,\" says Hakon Hakonarson, who is starting a programme for clinical assessment of genomes at the Children's Hospital of Philadelphia in Pennsylvania. The members of Debbie Jorde's family still ponder what their genome sequences have meant for them. Although the sequences didn't alter treatment, if they had known about the lung problem earlier it might have prevented a dangerous procedure that both Heather and Logan underwent to reduce the recurrence of pneumonia. Still, Lynn Jorde thinks that more successes are on the way for genomes in clinical care. \"I'd predict some spectacular applications.\" But, he adds, \"I'm a congenital optimist\".   See News    p.17  &  p.19 Brendan Maher is a features editor for Nature. \n                     The Human Genome at Ten \n                   \n                     Debbie Jorde's blog \n                   \n                     Medical College of Wisconsin Human and Molecular Genetics Center \n                   \n                     ClinVar \n                   Reprints and Permissions"},
{"file_id": "478026a", "url": "https://www.nature.com/articles/478026a", "year": 2011, "authors": [{"name": "Richard Van Noorden"}], "parsed_as_year": "2006_or_before", "body": "A surge in withdrawn papers is highlighting weaknesses in the system for handling them. This week, some 27,000 freshly published research articles will pour into the Web of Science, Thomson Reuters' vast online database of scientific publications. Almost all of these papers will stay there forever, a fixed contribution to the research literature. But 200 or so will eventually be flagged with a note of alteration such as a correction. And a handful \u2014 maybe five or six \u2014 will one day receive science's ultimate post-publication punishment: retraction, the official declaration that a paper is so flawed that it must be withdrawn from the literature. It is reassuring that retractions are so rare, for behind at least half of them lies some shocking tale of scientific misconduct \u2014 plagiarism, altered images or faked data \u2014 and the other half are admissions of embarrassing mistakes. But retraction notices are increasing rapidly. In the early 2000s, only about 30 retraction notices appeared annually. This year, the Web of Science is on track to index more than 400 (see  'Rise of the retractions' ) \u2014 even though the total number of papers published has risen by only 44% over the past decade. \n               boxed-text \n             Perhaps surprisingly, scientists and editors broadly welcome the trend. \"I don't think there's any doubt that we're detecting more fraud, and that systems are more responsive to misconduct. It's become more acceptable for journals to step in,\" says Nicholas Steneck, a research ethicist at the University of Michigan in Ann Arbor. But as retractions become more commonplace, stresses that have always existed in the system are starting to show more vividly. When the UK-based Committee on Publication Ethics (COPE) surveyed editors' attitudes to retraction two years ago, it found huge inconsistencies in policies and practices between journals, says Elizabeth Wager, a medical writer in Princes Risborough, UK, who is chair of COPE. That survey led to retraction guidelines that COPE published in 2009. But it's still the case, says Wager, that \"editors often have to be pushed to retract\". Other frustrations include opaque retraction notices that don't explain why a paper has been withdrawn, a tendency for authors to keep citing retracted papers long after they've been red-flagged (see  'Withdrawn papers live on' ) and the fact that many scientists hear 'retraction' and immediately think 'misconduct' \u2014 a stigma that may keep researchers from coming forward to admit honest errors. Perfection may be too much to expect from any system that has to deal with human error in all its messiness. As one journal editor told Wager, each retraction is \"painfully unique\". But as more retractions hit the headlines, some researchers are calling for ways to improve their handling. Suggested reforms include better systems for linking papers to their retraction notices or revisions, more responsibility on the part of journal editors and, most of all, greater transparency and clarity about mistakes in research. The reasons behind the rise in retractions are still unclear. \"I don't think that there is suddenly a boom in the production of fraudulent or erroneous work,\" says John Ioannidis, a professor of health policy at Stanford University School of Medicine in California, who has spent much of his career tracking how medical science produces flawed results. In surveys, around 1\u20132% of scientists admit to having fabricated, falsified or modified data or results at least once ( D. Fanelli  PLoS ONE    4,   e5738; 2009 ). But over the past decade, retraction notices for published papers have increased from 0.001% of the total to only about 0.02%. And, Ioannidis says, that subset of papers is \"the tip of the iceberg\" \u2014 too small and fragmentary for any useful conclusions to be drawn about the overall rates of sloppiness or misconduct. Instead, it is more probable that the growth in retractions has come from an increased awareness of research misconduct, says Steneck. That's thanks in part to the setting up of regulatory bodies such as the US Office of Research Integrity in the Department of Health and Human Services. These ensure greater accountability for the research institutions, which, along with researchers, are responsible for detecting mistakes. The growth also owes a lot to the emergence of software for easily detecting plagiarism and image manipulation, combined with the greater number of readers that the Internet brings to research papers. In the future, wider use of such software could cause the rate of retraction notices to dip as fast as it spiked, simply because more of the problematic papers will be screened out before they reach publication. On the other hand, editors' newfound comfort with talking about retraction may lead to notices coming at an even greater rate. \"Norms are changing all the time,\" says Steven Shafer, editor-in-chief of the journal  Anesthesia & Analgesia , who has participated in two major misconduct investigations \u2014 one of which involved 11 journals and led to the retraction of some 90 papers.  \n                It's none of your damn business! \n              But willingness to talk about retractions is hardly universal. \"There are a lot of publishers and a lot of journal editors who really don't want people to know about what's going on at their publications,\" says New York City-based writer Ivan Oransky, executive editor at Reuters Health. In August 2010, Oransky co-founded the blog Retraction Watch with Adam Marcus, managing editor at  Anesthesiology News . Since its launch, Oransky says, the site has logged 1.1 million page views and has covered more than 200 retractions. In one memorable post, the reporters describe ringing up one editor, L. Henry Edmunds at the  Annals of Thoracic Surgery , to ask about a paper withdrawn from his journal (see  go.nature.com/ubv261 ). \"It's none of your damn business!\" he told them. Edmunds did not respond to  Nature  's request to talk for this article. The posts on Retraction Watch show how wildly inconsistent retractions practices are from one journal to the next. Notices range from informative and transparent to deeply obscure. A typically unhelpful example of the genre would be: \"This article has been withdrawn at the request of the authors in order to eliminate incorrect information.\" Oransky argues that such obscurity leads readers to assume misconduct, as scientists making an honest retraction would, presumably, try to explain what was at fault. To Drummond Rennie, deputy editor of the  Journal of the American Medical Association , there are two obvious reasons for obscure retraction notices: \"fear and work.\" The fear factor, says Wager, is because publishers are very frightened of being sued. \"They are incredibly twitchy about publishing anything that could be defamatory,\" she says. 'Work' refers to the phenomenal effort required to sort through authorship disputes, concerns about human or animal subjects, accusations of data fabrication and all the other ways a paper can go wrong. \"It takes dozens or hundreds of hours of work to get to the bottom of what's going on and really understand it,\" says Shafer. Because most journal editors are scientists or physicians working on a voluntary basis, he says, that effort comes out of their research and clinical time. But the effort has to be made, says Steneck. \"If you don't have enough time to do a reasonable job of ensuring the integrity of your journal, do you deserve to be in business as a journal publisher?\" he asks. Oransky and Marcus have taken a similar stance. This summer, for example, Retraction Watch criticized the  Journal of Neuroscience   for a pair of identical retraction notices it published on 8 June: \"At the request of the authors, the following manuscript has been retracted.\" But the journal's editor-in-chief, neuroscientist John Maunsell of Harvard Medical School in Boston, Massachusetts, argues that such obscurity is often the most responsible course to take. \"My feeling is that there are far fewer retractions than there should be,\" says Maunsell, who adds that he has conducted 79 ethics investigations in more than 3 years at the journal \u2014 1 every 2\u20133 weeks. But \"authors are reluctant to retract papers\", he says, \"and anything we put up in the way of a barrier or disincentive is a bad thing. If authors are happier posting retractions without extra information, I'd rather see that retraction go through than provide any discouragement.\" At the heart of these arguments, says Steneck, lie shifting norms of how responsible journal editors should be for the integrity of the research process. In the past, he says, \"they felt that institutions and scientists ought to do it\". More and more journal editors today are starting to embrace the gatekeeper role. But even now, Shafer points out, they have only limited authority to challenge institutions that are refusing to cooperate. \"I have had institutions, where I felt there was very clear misconduct, come back and tell me there was none,\" Shafer says. \"And I have had a US institution tell me that they would look into allegations of misconduct only if I agreed to keep the results confidential.\"  \n                The blame game \n              Discussions on Retraction Watch make it clear that many scientists would like to separate two aspects of retraction that seem to have become tangled together: cleaning up the literature, and signalling misconduct. After all, many retractions are straightforward and honourable. In July, for example, Derek Stein, a physicist at Brown University in Providence, Rhode Island, retracted a paper in  Physical Review Letters   on DNA in nanofluidic channels when he found that a key part of the analysis had been performed incorrectly. His thoroughness and speed \u2014 the retraction came just four months after publication \u2014 were singled out for praise on Retraction Watch. But because almost all of the retractions that hit the headlines are dramatic examples of misconduct, many researchers assume that any retraction indicates that something shady has occurred. And that stigma may dissuade honest scientists from doing the right thing. One American researcher who talked to  Nature   about his own early-career retraction said he hoped that his decision would be seen as a badge of honour. But, even years later and with his career established, he still did not want  Nature   to use his name or give any details of the case. There is no general agreement about how to reduce this stigma. Rennie suggests reserving the retraction mechanism exclusively for misconduct, but that would require the creation of a new term for withdrawals owing to honest mistakes. At the other extreme, Thomas DeCoursey, a biologist at Rush University Medical Center in Chicago, argues for retraction of any paper that publishes results that are not reproducible. \"It does not matter whether the error was due to outright fraud, honest mistakes or reasons that simply cannot be determined,\" he says. A better vocabulary for talking about retractions is needed, says Steneck \u2014 one acknowledging that retractions are just as often due to mistakes as to misconduct. Also useful would be a database for classifying retractions. \"The risk for the research community is that if it doesn't take these problems more seriously, then the public \u2014 journalists, outsiders \u2014 will come in and start to poke at them,\" he points out. The only near-term solution comes back to transparency. \"If journals told readers why a paper was retracted, it wouldn't matter if one journal retracted papers for misconduct while another retracted for almost anything,\" says Zen Faulkes, a biologist at the University of Texas\u2013Pan American in Edinburg, Texas. Oransky agrees. \"I think that what we're advocating is part of a much larger phenomenon in public life and on the Web right now,\" he says. \"What scientists should be doing is saying, 'In the course of what we do are errors, and among us are also people that commit misconduct or fraud. Look how small that number is! And here's what we're doing to root that out.'\"\n Richard Van Noorden is an assistant news editor for Nature in London. For more analysis of retraction statistics,  click here  . \n                     Retraction Watch \n                   \n                     COPE (Committee on Publication Ethics) guidelines on retraction (PDF) \n                   \n                     An online tool that tracks PubMed retractions, by bioinformatician Neil Saunders \n                   Reprints and Permissions"},
{"file_id": "478172a", "url": "https://www.nature.com/articles/478172a", "year": 2011, "authors": [], "parsed_as_year": "2006_or_before", "body": "Getting to grips with a changing polar landscape. The Arctic covers around 5% of the planet's surface, but it is capturing a disproportionate amount of attention. With temperatures rising at twice the global rate, the region's summer sea ice is shrinking rapidly, making access easier than ever before. At the same time, countries are racing to claim parts of the Arctic's sea floor and the vast deposits of hydrocarbons that lie beneath it. \n               Disappearing sea ice  \n             \n               Click here for larger image \n               Since satellite observations started in 1979, the September sea-ice extent has declined by 12% per decade, and the past 5 years have marked the lowest on record. The ice cover is thinning ( see graph ), making it more vulnerable to warmer temperatures. Forecasts by climate models ( see graph ) suggest that summer sea ice will largely disappear in the second half of the century, but the current rate of ice loss exceeds the models' forecasts, suggesting that ice-free conditions could arrive sooner. \n               Shifting borders  \n             \n               Click here for larger image \n               Under the United Nations Convention on the Law of the Sea, countries can claim rights to seabed resources in the Arctic Ocean, depending on their coastline and the sea-floor geology. Dark shading on this map represents each nation's existing exclusive economic zone, which extends up to 370 kilometres from its coastline. Lighter shading depicts extended regions to which countries may be eligible. Russia and Norway are the only Arctic nations to have submitted their bids. \n               Untapped resources  \n             \n               Click here for larger image \n               With its thick piles of sedimentary rock, the Arctic may hold some of Earth's biggest hydrocarbon stores. The high price of oil is driving companies northwards, with drilling taking place or planned off the coast of Greenland and in the Kara, Barents and Chukchi seas.   See    After the ice ,  Nature's special on the changing face of the Arctic, for more.  \n                     Special: After the ice \n                   \n                     Nature Climate Change \n                   \n                     Nature Geoscience \n                   \n                     US National Snow and Ice Data Center \n                   \n                     International Boundary Research Unit \n                   \n                     USGS Arctic Resource Assessment \n                   Reprints and Permissions"},
{"file_id": "478174a", "url": "https://www.nature.com/articles/478174a", "year": 2011, "authors": [{"name": "Daniel Cressey"}], "parsed_as_year": "2006_or_before", "body": "As the ice melts, fresh obstacles confront Arctic researchers. Last month, US researchers took a 4,000-tonne gamble when they steered the  Marcus G. Langseth   through the Bering Strait and into the Arctic Ocean. The 72-metre research vessel was not built to plow through ice, so it had never ventured that far poleward before. But the rules are changing quickly in the new north. Managers at the US National Science Foundation (NSF), which owns the ship, decided to send the  Langseth   into the Arctic after reviewing satellite images that showed that the intended survey area in the Chukchi Sea had been largely clear of ice for four of the past five summers. In an e-mail to  Nature   during the cruise, its principal investigator, Bernard Coakley, said: \"We are rolling the dice a bit to take her up north.\" But the bet paid off for Coakley, a marine geologist at the University of Alaska Fairbanks. Sea-ice coverage was at near-record lows this summer, and the  Langseth   \u2014 due back in dock this week \u2014 has not encountered any troubling ice. With the Arctic warming roughly twice as fast as the rest of the globe, there is more need than ever to monitor the changing conditions there. And the retreating summer sea ice is opening up new options for scientists who want to explore the once difficult-to-reach Arctic waters, allowing them, for example, to use vessels other than icebreakers. But the scientists are not alone. Businesses, too, are racing to exploit the Arctic \u2014 for tourism, fishing, transportation and, especially, resources such as hydrocarbons. According to the US Geological Survey, the Arctic could hold up to 30% of the world's undiscovered gas and as much as 13% of its undiscovered oil 1 . Governments keen to access this wealth are stepping up their activities in the area as a prelude to claiming rights to resources in vast swathes of territory under the United Nations Convention on the Law of the Sea (UNCLOS). When Russia planted a flag on the sea bed under the North Pole in 2007, many people saw the action as a symbolic statement about the country's territorial ambitions \u2014 a view bolstered this July when Russia pledged to station two brigades permanently in the Arctic. The next month, Canada launched its annual sovereignty operation in the Arctic and claimed that it now had more military capability in the region than ever before. By some accounts, all this bluster points to a new cold war that could hamper scientists working in the Arctic. But geologists, oceanographers and others who have been conducting research in the region generally see more cooperation than competition. \"What you read in the media is geopolitical conflict,\" says Hajo Eicken, a sea-ice researcher at the University of Alaska Fairbanks. \"What we see is quite the contrary. In many cases, you can do Arctic research only if you have good international collaboration in place.\" For researchers, a bigger problem is securing ship time. With the flurry of interest in the Arctic, scientists must compete with drilling companies and others for time on ships designed to operate in the region, which are in short supply. That is how Coakley and his colleagues ended up testing the  Langseth   in the Chukchi Sea.  \n                Territorial ambitions \n              When Coakley submitted his proposal to the NSF to investigate the geology of the region, he originally requested the  Healy , the main US icebreaker, which had been used in past expeditions in the Chukchi. But several factors conspired against that plan, and the  Healy   became overbooked. The icebreaker spent late August and September cruising through the Arctic alongside the Canadian coastguard's icebreaker  Louis S. St-Laurent . As part of a multi-year project between the two governments, the ships mapped the extended continental shelf off North America to gather information that may help both countries to stake claims to parts of the Arctic sea floor. Under the UNCLOS, a nation can claim rights to the seabed beyond the usual 200-nautical-mile zone of control \u2014 known as an exclusive economic zone \u2014 if it can prove that the claimed region is a natural extension of the country's continental shelf. That requires extensive mapping expeditions, which have generated a large amount of work for marine geologists in recent years, but have also sucked up precious icebreaker time. The United States has not yet ratified the UNCLOS, and signatories have 10 years after they ratify to make formal claims. So it remains unclear exactly what areas might be snapped up in this process. Some projections suggest that the nations bordering the Arctic Ocean will submit claims that cover the vast majority of that region. Interest in the Arctic extends far beyond its neighbours. China and South Korea, for example, have built large icebreakers to maintain their presence in the north. Commercial concerns are driving much of the activity in the Arctic Ocean, with oil companies, in particular, jockeying for position. Exxon, for example, beat out BP this summer by signing a multi-billion-dollar agreement with Russian petroleum giant Rosneft to drill jointly in the Kara Sea. Cairn Energy in Edinburgh, UK, is sinking exploratory wells off the west coast of Greenland, and Shell is moving forward with plans to drill in the Chukchi Sea off Alaska next year. At the same time, tourist ships are increasingly plying the waters off the west coast of Greenland. Between 2000 and 2010, the number of cruise ships visiting the island more than trebled, as did the number of tourists on those ships, according to the Greenland tourism and business council. The north has become a popular destination.  \n                Ice-free future \n              Opportunities for travel in the Arctic will expand as its sea-ice cover continues to wither. Experts debate how quickly that will happen, because natural variability in the past 10\u201320 years may have accelerated the disappearance of ice caused by human-induced global warming. So ice loss could slow and ice cover might even rebound for brief periods. \"At the end of the day, there's a lot of uncertainty,\" says Marika Holland, a sea-ice researcher at the National Center for Atmospheric Research in Boulder, Colorado. But the long-term trend seems clear. \"The consensus seems to be that out in the 2030s to 2050s or 2060s is when we might see the loss of all the ice for a short period during the summer,\" says John Walsh, an atmospheric scientist at the University of Illinois at Urbana\u2013Champaign. That is likely to lead to a significant rise in traffic in the Arctic Ocean. With the summer sea ice both shrinking in extent and thinning, the region is becoming much more accessible to ice-strengthened ships, which have less formidable protection than true icebreakers. A study conducted by researchers at the University of California, Los Angeles, estimated that by mid-century 23% more of the Arctic's waters will be accessible to vessels capable of just limited icebreaking 2 . Eventually, shipping companies might start to use the Arctic as a short-cut for transporting goods between cities on the Pacific Rim and those bordering the Atlantic. Experimental voyages have been made along the north coast of Russia, and a smattering of ships has crossed the Northwest Passage north of Canada. But don't expect a significant rise in trans-Arctic traffic any time soon. In a 2009 assessment 3 , the Arctic Council, an intergovernmental forum for issues affecting the region, projected that most of the shipping in the region will involve bringing supplies to northern communities and exporting resources such as oil and minerals, for at least the next decade and possibly much longer. \"The notion that the Arctic Ocean will become a Panama Canal or a Suez Canal is a figment of the media,\" says Lawson Brigham, a geographer at the University of Alaska Fairbanks and chairman of the assessment. But, he adds, \"there may be a short, summer 'window of opportunity' for trans-Arctic navigation\". These changes are creating a sense of urgency among scientists trying to answer a string of questions about the region (see  'Top questions in Arctic research' ). Researchers seeking access to the Arctic Ocean have traditionally relied on icebreakers to get through the ice. But these ships are in short supply in the United States and, to a lesser extent, in Europe, because of lack of investment. Even ice-strengthened vessels can be difficult for researchers to secure. \"Because of the retreat of the sea ice and the oil development we have pending in the Chukchi and Beaufort Seas, a lot of the ice-strengthened vessels are being taken up by industry,\" says Jacqueline Grebmeier, an Arctic researcher at the University of Maryland in Solomons who has made several trips through the region on the Canadian Coast Guard icebreaker  Sir Wilfrid Laurier . Ice-strengthened vessels, meanwhile, are not sturdy enough to provide the kind of access that scientists most desire. \"A lot of the processes that are really fundamental to the understanding of how Arctic climate, oceanography and biology work are not happening in summer time,\" says Lester Lembke-Jene, a marine geologist at the Alfred Wegener Institute for Polar and Marine Research in Bremerhaven, Germany. \"If you don't have the full annual observation,\" he says, \"you have a very, very imbalanced and narrow glimpse of what's going on there.\" Given the shortage of vessels, researchers are trying to convince others to do some of the work on their behalf. Some commercial ship operators are collecting data for climate scientists, and last year the US government relaunched the Science Ice Exercise, in which military submarines collect water samples and measure temperature, salinity, nutrients, chlorophyll and other data for scientists. Researchers are also starting to use autonomous underwater and aerial vehicles to collect data (see  'New eyes for science' ).  \n                Frozen out \n              The territorial ambitions of different nations may also end up restricting scientific access. In theory, the areas claimed under the UNCLOS apply only to the sea floor and do not give a country rights over the water above. In practice, however, such claims could hinder scientific work. \"If a coastal state wanted to, it could, by declaring regions to be of special interest for exploration, require that other states request permission to conduct research in the area of the extended continental shelf,\" says Larry Mayer, a marine geologist at the University of New Hampshire in Durham. This is more than just idle speculation, he says, because Russia has a history of impeding access to scientists from other nations seeking to work in its waters. Some researchers say that their attempts to put out or collect equipment from areas under Russian control have been thwarted when applications for permits were either denied or went unanswered. The Integrated Ocean Drilling Program, for example, could not obtain permission to drill in Russian parts of the Bering Sea in 2009. Some scientists familiar with Russia say that the permission problems stem more from the nation's massive bureaucracy than a deliberately obstructionist policy. \"Inertia here coming from the Soviet era is really huge,\" says Igor Polyakov, a Russian Arctic researcher who now works at the University of Alaska Fairbanks. He says that gaining permission for research in Russian waters is much easier now than in the past. Others report just the opposite. Cheryl Rosa, deputy director of the US Arctic Research Commission in Anchorage, Alaska, says that researchers are still experiencing problems with permits, visas, taxations on funding, getting data out of Russia and other issues. Despite these concerns with Russia, the view from scientists is that collaborations in the Arctic have never been stronger. The recent International Polar Year, which ran from March 2007 to March 2009, helped by bringing together scientists from different nations to work on dozens of projects in the Arctic, ranging from permafrost studies to research on indigenous peoples. \"That's really made a big difference and probably pushed science collaborations further and faster,\" says Julie Brigham-Grette, a glacial geologist at the University of Massachusetts in Amherst. Mayer hopes that the Arctic Council or some other body will forge an agreement that will allow researchers continued access across the Arctic. A positive step came in May when the council's eight member states agreed to work together in search and rescue in the region, creating the first legally binding agreement made via the Arctic Council. Arctic scientists say that the spirit of cooperation is changing their research in profound ways. They are probably ahead of their colleagues elsewhere in terms of releasing information quickly and widely, says Eicken. Data from the NSF's Arctic Observing Network \u2014 a system of atmospheric and land- and ocean-based monitoring tools \u2014 for example, are made immediately available through an online database so that anyone can access near-real-time readings from the buoys and other observatories. The advances are motivated in part by the changes in the region. Scientists often take two to three years to process and publish data, but in the Arctic, Eicken says, \"that cycle isn't able to keep up with the rapid development\". And if projections are right, scientists will need to work faster than ever to keep up with the unstable conditions. Grebmeier saw just how changeable the region could be in July in her latest trip on the  Sir Wilfrid Laurier . Several years ago, the ship had run into thick, multi-year ice when cruising near Barrow, Alaska. But this year, the crew found just one bit of ice and even that wasn't terribly impressive. \"I was working on the deck \u2014 someone yelled 'sea ice',\" she says. \"I looked up and actually thought taking a picture wasn't worth my time.\" Daniel Cressey is a reporter with Nature in London. Additional reporting by Richard Monastersky. \n                     Special: After the ice \n                   \n                     Bernard Coakley \n                   \n                     Hajo Eicken \n                   \n                     Marika Holland \n                   \n                     John Walsh \n                   \n                     Jacqueline Grebmeier \n                   \n                     Lester Lembke-Jene \n                   \n                     Larry Mayer \n                   \n                     Igor Polyakov \n                   \n                     Julie Brigham-Grette \n                   Reprints and Permissions"},
{"file_id": "476266a", "url": "https://www.nature.com/articles/476266a", "year": 2011, "authors": [{"name": "Gayathri Vaidyanathan"}], "parsed_as_year": "2006_or_before", "body": "Do chimpanzees have traditions? As wild populations dwindle, researchers are racing to find out. Thump! Thump! Thump! As the hollow sound echoes through the Liberian rainforest, Vera Leinert and her fellow researchers freeze. Silently, Leinert directs the guide to investigate. Jefferson 'Bola' Skinnah, a ranger with the Liberian Forestry Development Authority, stalks ahead, using the thumping to mask the sound of his movement. In a sunlit opening in the forest, Skinnah spots a large adult chimpanzee hammering something with a big stone. The chimpanzee puts a broken nut into its mouth then continues pounding. When Skinnah tries to move closer, the chimp disappears into the trees. By the time Leinert and her crew get to the clearing, the animal is long gone. For the past year, Leinert has been trekking through Sapo National Park, Liberia's first and only protected reserve, to study its chimpanzee population. A student volunteer at the Max Planck Institute for Evolutionary Anthropology (EVA) in Leipzig, Germany, Leinert has never seen her elusive subjects in the flesh but she knows some of them well. There's an energetic young male with a big belly who hammers nuts so vigorously he has to grab a sapling for support. There are the stronger adults who can split a nut with three blows. And there are the mothers who parade through the site with their babies. They've all been caught by video cameras placed strategically throughout Sapo. Chimpanzees in the wild are notoriously difficult to study because they flee from humans \u2014 with good reason. Bushmeat hunting and human respiratory diseases have decimated chimpanzee populations 1 , while logging and mining have wiped out their habitat. Population numbers have plunged \u2014 although no one knows by exactly how much because in most countries with great apes, the animals have never been properly surveyed. The Pan Africa Great Ape Program, the first Africa-wide great-ape census to be mounted, could change that. In addition to surveying chimpanzee numbers (see  'How many chimpanzees are left?' ), project scientists plan to set up automated video and audio recording devices at 40 research sites in 15 countries with chimp populations. Led by Christophe Boesch, director of the primatology department at the EVA, and Hjalmar K\u00fchl, also at the EVA, the programme aims to get a picture of how chimpanzee behaviour \u2014 from nut cracking to vocal calls \u2014 varies across Africa. Ultimately, the hope is to learn about the origins and extent of what, in humans, would be called culture. Until recently, scientists regarded culture \u2014 defined as socially transmitted behaviours \u2014 as exclusive to humans, but there is growing recognition that many animals exhibit some sort of culture. Chimpanzees, which share 98% of their genes with humans, have the most varied set of behaviours documented in the animal world. The difference between humans and animals is growing less distinct, say some researchers. \"It is not black and white,\" says K\u00fchl, who is Leinert's supervisor at the EVA. In the old scenario, \"only humans have culture\", says Jason Kamilar, a biogeographer in the department of anthropology at Yale University in New Haven, Connecticut. \"Then, culture would be the defining feature of humanity, which evolved some time after the split between the human and chimp lineages,\" he says. But \"if chimps have culture, then presumably the last common ancestor of chimps and humans had culture\".  \n                Mapping behaviour \n              Some chimps dance slowly at the beginning of rain showers, others don't; some use long sticks to dig up army ants; others use short sticks. In West Africa, some chimp groups hammer nuts with a stone or a piece of wood to open them. But east of the river Nzo-Sassandra, which cuts across C\u00f4te d'Ivoire, only one group has been seen cracking nuts. So far, researchers have observed these variations over years spent studying groups of chimpanzee that have been carefully habituated to the presence of humans. There are just 12 such colonies in Africa (see  'Chimpanzee census' ), the most famous of which is in Gombe Stream National Park in Tanzania, where primatologist Jane Goodall worked. \n               Click here for larger image \n               In 1999, evolutionary psychologist Andrew Whiten of the University of St Andrews, UK, and his colleagues compiled a list of behaviours seen in seven of those groups and showed that chimpanzees have unique traditions depending on where they live 2 . They identified at least 39 behaviours from a list of 65 that varied between groups for no obvious reason. In humans, culture is passed on from one person to another, and in laboratory studies chimpanzees have shown the capacity to pass on learned customs. In one experiment, Whiten and his colleagues taught two chimps a complex series of steps for getting food from a box. Soon after the chimps were reunited with their groups, all the animals were using this method to get their food 3 . But whether such social learning happens in the wild is less clear. Gorillas and bonobos can also learn to use tools in the lab, but rarely use them in their natural habitat 4 . Deciphering culture in the wild is difficult because researchers must ensure that behavioural differences between groups do not have other causes, such as variation in genetics or environmental conditions. \"Why is it all chimps don't do everything? One solution is that there are hidden ecological differences between populations,\" says primatologist Richard Wrangham at Harvard University in Cambridge, Massachusetts. A behaviour could be linked to any number of variables such as amount of rainfall, the types of tree available, or the kinds of predator in the area, he says. These influences can be subtle, as researchers found while studying how chimps use sticks to harvest army ants. Chimpanzees in Guinea sometimes use short sticks and sometimes use sticks up to twice as long. No reason for this was obvious until Tatyana Humle, an anthropologist at the University of Kent, UK, found that some ants are more aggressive, with longer legs and larger mandibles; they run up sticks quicker and bite harder 5 . This might explain why chimps elsewhere in Africa also choose tools of varying lengths to get at ants. But researchers have not been able to find obvious explanations for other variations related to ant harvesting. Chimpanzees in Cote d'Ivoire sweep the ants off their sticks and into their palms before eating; in Guinea, only about 320 kilometres away, the animals stick the ant-laden sticks directly into their mouths. The same type of ant is present in both places. Ruling out genetic influences is equally complicated. This year, molecular ecologist Kevin Langergraber at the EVA and his colleagues compared genetic and behavioural data for nine groups of chimpanzee. They found that communities with greater overlap in their mitochondrial DNA showed more similarities in their behaviour 6 . \"What we are saying is, you haven't really ruled out the genetic explanation,\" says Langergraber. There may be a few hundred thousand chimpanzees in Africa, but researchers have studied just 700\u20131,000 chimpanzees at the dozen sites with well habituated colonies, says Whiten. The available information from those groups is too little to determine how genes and the environment influence behavioural variations. K\u00fchl compares the situation to using a handful of villages scattered around the world to draw basic conclusions about all the rituals that define human culture. Whiten and his colleagues are now carrying out more detailed comparisons of the behaviour and ecology of chimps at all the habituated sites. But it has taken 50 years to capture the data they are using, most of which were recorded by painstaking observational studies. The way forward may be the use of cameras hidden in strategic sites, like those Leinert and her team are setting up in Liberia. Such techniques have already proved their worth. Two years ago in Gabon, Boesch and his team were puzzled by random pits they observed in the ground. They set up camera traps and obtained video recordings of chimps digging to extract honey from underground bees' nests \u2014 something that had never been seen before 7 . \"Camera traps are proving to be an exciting way to reveal new and often complex behavioural techniques in wild chimpanzee communities,\" says Whiten.  \n                Caught in the act \n              At the site in Sapo, Leinert pulls on gloves to measure the rock used by the chimp to crack open nuts of the Guinea plum,  Parinari excelsa . The rock is sizeable, weighing in at 880 grams. She collects nuts for later analysis, as well as hair and dung samples for genetic studies. Leinert may later put up a video camera at the location to collect more data on the nut-cracking behaviour. The cameras are mounted in boxes on tree trunks at the height of a chimp's shoulder, and powered by rechargeable batteries. An infrared motion detector activates the camera for one minute when anything moves in its range. Near the nut-cracking site, a solar-powered audio device is already continuously recording the forest sounds. Chimpanzees emit a range of calls, including short, high-pitched 'pant hoots' that are unique to each individual, and researchers can use them to identify individuals and to tally the size of a community. These calls may be a form of vocal culture, somewhat like human dialects 8 . Over the next five years, the Pan Africa Great Ape Program will establish similar recording stations at locations across Africa. \"So potentially we might have, in a few years, behavioural differences from 40 different populations, which is, as you know, four times more than what we have now,\" says Boesch. K\u00fchl proposes that these data could help in designing computer models to test how genes, ecology and social transmission influence the distribution and spread of behaviours such as nut cracking. One idea is that when female chimpanzees reach sexual maturity and move to new communities, they pass along their learned behaviours. Another possibility is that each group invents its own behaviours, some of which catch on and become a culture. Individual practices can die out in particular groups but thrive in others. Or, it might be that some chimp groups refuse to take up new ways of doing things from incoming individuals. This could explain why some populations show similar behaviours and others do not. Before K\u00fchl and his colleagues can conduct the modelling work, they need to devise a faster way to go through the recordings made by the camera and audio traps, which are accumulating at a rate of hundreds of hours each month. Students are currently carrying out the analysis but it can take 10 hours to go through an hour of video, according to K\u00fchl. So engineers at the Fraunhofer Institute for Digital Media Technology, based in Ilmenau, Germany, have developed a computer algorithm to recognize individual chimpanzees from their facial patterns and distinctive features, such as the wrinkles under their eyes. In tests of zoo animals, the software can correctly identify individual chimpanzees 83% of the time, and it processes recordings ten times faster than a person can. Nevertheless, the cameras cannot reveal how an adult chimp patrols its range, or other actions that play out over a wide area. The full portfolio of traditions in the community will remain a mystery. And automated recordings will never capture the subtle ecological information \u2014 such as the mandible size and leg length of army ants \u2014 that may eventually explain particular behaviours. These require boots on the ground, and long-term behavioural studies are needed to see how chimpanzees pass traditions on to each other as a driver of culture. But already, the 30 cameras that Leinert has set up in Sapo Park have delivered some tantalizing clues. She is most interested in the lively young male she calls 'Janosch', whom she likes for \"his big belly and the way he strikes out to crack the nuts\". Besides being entertaining, he sometimes carries his pounding rock away with him, something Leinert hasn't seen with most other chimpanzees in Sapo. The practice may yet catch on with others there. If so, Leinert could be seeing the beginnings of a cultural variation, captured by the cameras she set up in the forest. Gayathri Vaidyanathan is an International Development Research Center fellow at Nature. \n                     Webfocus on chimp genome \n                   \n                     News special on chimp genome \n                   \n                     Jane of the jungle \n                   \n                     Max Planck Institute for Evolutionary Anthropology \n                   \n                     A.P.E.S. Database \n                   Reprints and Permissions"},
{"file_id": "474146a", "url": "https://www.nature.com/articles/474146a", "year": 2011, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "David Sloan Wilson is using the lens of evolution to understand life in the struggling city of Binghamton, New York. Next, he wants to improve it. David Sloan Wilson is holding a white ceramic dog dish and making the rounds at the Lost Dog Caf\u00e9 in Binghamton, New York. \"Just like in church!\" the biologist jokes, as he collects crumpled dollar bills on this snowy March afternoon. It is Yappy Hour, a fund-raiser for a project to turn an abandoned dirt-bike track into a dog park. The plan is a contender in Wilson's 'Design Your Own Park Competition', a venture that he says is \"richly informed\" by evolutionary theory \u2014 and one of the many community projects that he is running, co-running or up to his neck in. As with most of Wilson's endeavours these days, the motivation is twofold: to improve the quality of life in Binghamton and to study the city as an evolutionary landscape. Wilson, who works at the State University of New York in Binghamton, has been a prominent figure in evolutionary biology since the 1970s. Much of his research has focused on the long-standing puzzle of altruism \u2014 why organisms sometimes do things for others at a cost to themselves. Altruism lowers an individual's chances of passing its own genetic material on to the next generation, yet persists in organisms from slime moulds to humans. Wilson has championed a controversial idea that natural selection occurs at multiple levels: acting not only on genes and individuals, but also on entire groups. Groups with high prosociality \u2014 a suite of cooperative behaviours that includes altruism \u2014 often outcompete those that have little social cohesion, so natural selection applies to group behaviours just as it does on individual adaptations 1 . Many contend that group-level selection is not needed to explain altruism, but Wilson believes that it is this process that has made humans a profoundly social species, the bees of the primate order. Wilson originally built the case for multi-level selection on animal studies and hypothetical models. But eight years ago, he decided to come down from the ivory tower and take a closer look at the struggle for existence all around him. A city \u2014 with dozens to hundreds of distinct social groups interacting and competing for resources \u2014 seemed to Wilson the ultimate expression of humanity's social nature. If prosociality is important in the biological and cultural evolution of human groups, he reasoned, he should be able to observe it at work in Binghamton, a city of about 47,000 people.  \n                The urban laboratory \n              Differences in prosociality, Wilson thought, should produce measurable outcomes \u2014 if not in reproductive success, perhaps in happiness, crime rates, neighbourhood tidiness or even the degree of community feeling expressed in the density of holiday decorations. \"I really wanted to see a map of altruism,\" he says. \"I saw it in my mind.\" And with a frisson of excitement, he realized that his models and experiments offered clues about how to intervene, how to structure real-world groups to favour prosociality. \"Now is the implementation phase.\" Evolutionary theory, Wilson decided, will improve life in Binghamton. He now spends his days in church basements, government meeting rooms, street corners and scrubby city parks. He is involved in projects to build playgrounds, install urban gardens, reinvent schools, create neighbourhood associations and document the religious life of the city, among others. Wilson thrives on his hectic schedule, but it is hard to measure his success. Publications are sparse, in part because dealing with communities and local government is time-consuming. And the nitty-gritty practical details often swamp the theory; the people with whom he collaborates sometimes have trouble working out what his projects have to do with evolution. At the Lost Dog, I ask city planner and frequent collaborator Tarik Abdelazim whether he understands why an academic scientist is taking such a proactive interest in the city. He leans against the bar, glass of wine in hand, and addresses Wilson. \"I know you talk about 'prosociality', but how that connects to our good friend Darwin, I don't know.\" Fellow biologists are also bemused. According to Wilson's former graduate student Dan O'Brien, now a biologist at the Radcliffe Institute for Advanced Study at Harvard University in Cambridge, Massachusetts, many have reacted to Wilson's work with \"a mixture of intrigue and distance\". That, says O'Brien, \"is because he's not doing biology anymore. He's entered into a sort of evolutionary social sciences.\" Wilson has acquired the language of community organizing and joined, supported and partially funded a slew of improvement schemes, raising the question of whether he is too close to his research. Has David Sloan Wilson fallen in love with his field site? Binghamton is hard to love. Established in the early nineteenth century, it has long relied on big industry for its identity and prosperity \u2014 early on through the Endicott-Johnson Shoe Company and then through IBM, which was founded in the area. But the manufacturers mostly decamped in the 1990s, and since then the city has taken on an aimless, shabby air. Dollar stores and coin-operated laundries fill the gaps between dilapidated Victorian houses and massive brick-and-stone churches. A Gallup poll in March 2011 listed Binghamton as one of the five US cities least liked by its residents (see  go.nature.com/tfdkqi ).\"It is a town that knows it is badly in need of a revival,\" says Wilson. Even its motto, 'Restoring the pride', speaks of a city clinging to its past and ashamed of its present. But as in any city, there is variation: some neighbourhoods are friendlier than others, some are more private and some feel downright dangerous. Wilson started his social experiment by measuring levels of prosociality in each neighbourhood. He and his research group gave surveys to teenagers from the local schools asking how often they helped or were helped by others. The researchers also dropped 200 stamped letters all over the city to see where people would help out a stranger by popping them into the postbox. They even recorded the density of Halloween and Christmas decorations. Wilson's group used some of these data to create maps of the city's prosociality 2 . Neighbourhoods in which people take care of each other appear as peaks, whereas selfishness creates valleys. There isn't a straightforward correlation with any socioeconomic factor, Wilson found. Some of the most cooperative people were from \"good, poor neighbourhoods\". \n               boxed-text \n             So Wilson decided to see whether he could raise up the prosocial valleys by creating conditions in which cooperation becomes a winning strategy \u2014 in effect, hacking the process of cultural evolution (see  'A map of prosociality' ). He set about this largely by instituting friendly competitions between groups. His first idea was a park-design project, in which neighbourhoods were invited to compete for park-improvement funds by creating the best plan. But Wilson soon found out that field experiments in real cities can take on lives of their own: different neighbourhoods couldn't get their acts together on the same schedule, so the competition aspect largely disappeared. Instead, he is now working on turning multiple park ideas into reality. The dog park is one. Another is Sunflower Park, the most advanced project to date, but still a sad, mainly empty lot surrounded by a chain-link fence. Children don't spend a lot of time playing here. Undaunted, Wilson is raising funds and laying plans for a relaxing community space flush with trees and amenities. \"In a year,\" he says, \"we will serve you a hot dog from the pavilion.\"  \n                Schooled for success \n              Wilson starts more projects than one man can usually handle. If there is an idea to paint a mural in a blighted area, Wilson wants in. The design can be a competition. And sure, he'll have time to help. \"No project should remain unborn,\" he says. He then lets time constraints weed his overstuffed garden of initiatives. Right now, education is winning a sizeable share of Wilson's attention. He and his graduate students are working with school administrators in several locations to see whether they can improve student performance with programmes that reward good behaviour and encourage group cooperation. Treats, mostly donated by teachers, range from snacks and school supplies to odd but coveted items such as toiletries from hotels. Wilson insists that this approach is fundamentally evolutionary, pointing out that in 1981, psychologist B. F. Skinner described how reinforcement and punishment 'select' for desired behaviour 3 . Wilson is tweaking the school environment so that it selects for prosociality, and is hoping that the student culture will evolve accordingly. The connection is less clear for Miriam Purdy, the principal of Regents Academy, a school for at-risk teenagers at which Wilson has been advising. When I ask how she sees evolution playing into the incentive programmes, Purdy surprises me and Wilson by saying that she doesn't believe in evolution. But that doesn't bother Wilson. It's no matter, he says, if the principles work. So far, it is hard to tell whether they do. Rick Kauffman, a graduate student who has spent so much time with students and teachers at Regents that he is a de facto staff member, says that no data have been collected on the incentive programmes because the rules have been adjusted weekly. Instead, he and Wilson are comparing the standardized test scores of Regents students with those of a control group of at-risk kids at the more traditional Binghamton High. A higher percentage of Regents students have taken and passed the state tests administered in January, and scientists and administrators alike are looking forward to the results of tests given later in the year. Wilson has also been studying Binghamton's religious institutions through an evolutionary lens. He is an atheist, but sees religion as a potentially positive source of community cohesion and a centre of meaning in people's lives. He has written extensively about religion as an adaptation of groups 4 , and has been funded by the Templeton Foundation in West Conshohocken, Pennsylvania, to study its effects. At the moment, he is trying to find out which of Binghamton's 100 or so congregations are the most and least likely to flourish in the current cultural environment. Wilson's trait of interest is the 'openness' of churches. Traditional protestant denominations, of which Wilson is fond, tend towards openness: details of belief and moral codes are individual, arrived at after prayer and discussion. Newer, conservative churches that adhere strictly to the Bible as a literal text would be considered less open. Wilson would like to understand from an evolutionary perspective why the membership of open churches in Binghamton is currently declining, but 'closed' churches are booming. Perhaps uncertain times create a fearful and socially isolated populace, interested in firm and clear guidance. Or perhaps closed churches uplift their members or focus on group solidarity and recruitment. When people's economic and educational situations are better they may become attracted to more open churches. And Wilson says it is possible that the open churches, by allowing congregants to draw their own conclusions in matters of faith, predispose them to losing faith altogether. Wilson hopes to test these ideas. The first congregation that he is studying is open, liberal and protestant: the Tabernacle United Methodist Church on Main Street. It has 100 regular members, who barely fill the hulking Victorian building. Last year, Wilson and two graduate students met extensively with the church's Strategic Planning Task Force to craft a survey that was given to everyone on the church's rolls in September. The survey included questions about prosocial behaviour and about religious faith (such as \"How often do you experience the following during worship services at this congregation? A sense of God's presence, Inspiration, Boredom, Awe or mystery, Frustration, Spontaneity, A sense of fulfilling my obligation\"). The researchers hope to expand the survey to the roughly 100 congregations across the city, and to create maps describing the ecology of religion in Binghamton. The work has been met with curiosity and befuddlement. Richard Sosis, an evolutionary anthropologist at the University of Connecticut in Storrs, says that the work is wholly appropriate. \"Religion is something that is human, generated from the human mind, which has been designed by natural selection.\" He adds, \"People are looking forward to seeing how this all unfolds, and the kind of success he has with it.\" However, Ron Numbers, a historian of science and religion at the University of Wisconsin\u2013Madison, is \"a little ambivalent and confused\". Religious groups develop naturally owing to many historical factors, he says. \"Going into some evolutionary account about this doesn't add anything to our knowledge.\" There is also the possibility that Wilson has become too close to the church. He has held so many community meetings in Tabernacle's basement that he may even have increased the size of the congregation. I asked Wilson whether the effects he has had on the church will contaminate his study. He pauses for a long time before answering, \"Not if you are monitoring what you are doing.\" Wilson is upfront about his affection for the group. \"I'd love this church to grow,\" he says. One of Wilson's students on the religion project, Ian MacDonald, says that Wilson has \"temporarily\" allayed his fears about helping religious organizations. But MacDonald is uneasy about what will happen when they try working with closed, dogmatic churches that condemn homosexuality or teach women to obey their husbands' every command. Wilson says that he is, \"sympathetic to the 'niche' occupied by 'closed' churches\"; he is not there to judge.  \n                Long-term project \n              The playgrounds, schools and churches are but a few of Wilson's ongoing schemes. Another is the West Side Neighborhood Project, which is surveying residents' attitudes towards groups such as black people and students, before and after the creation of a landlord\u2013tenant association and a student association. And he is even doing some work on the genetic level. In Wilson's office in the city, 15-millilitre conical tubes hold DNA samples from elderly Binghamtonites who have been interviewed for their oral histories. Each of the 47,000 people in Binghamton generates an infinite number of data points, from genomes to attitudes and habits of prayer. Wilson could spend the rest of his life \u2014 and those of several graduate students \u2014 studying and influencing lives here. But can he study the town and save it at the same time? Ben Kerr, a biologist at the University of Washington in Seattle, says, \"In the sense of providing a deeper understanding, evolutionary biology has something to offer.\" And he likes that Wilson is doing good in the community, but warns, \"One has to exercise caution when moving from descriptive to prescriptive stances.\" Among social scientists \u2014 and that is arguably what Wilson has become \u2014 it is not uncommon to become involved with the communities being studied. Such involvement \"might be a social good in itself, but it also helps you understand that community better\", says Kathleen Blee, head of sociology at the University of Pittsburgh in Pennsylvania. \"The desire to promote a social good may be stronger than leaving your field site untouched,\" she adds. Still, does one man have time to do both? Mary Webster, a resident who has been working on a park-design project in her neighbourhood, says that she initially saw Wilson as a professor with all the answers. Now, she says she realizes that he is \"flying by the seat of his pants\". That \"sounds about right\", says Wilson and, paraphrasing Einstein, he offers, \"If we knew what we were doing, it wouldn't be called research.\" It is past eight in the evening at Tranquil, one of the few upscale restaurants in town, where Wilson has invited members of the West Side Neighborhood Project. I quiz Wilson's graduate students about how he could possibly pay enough attention to them with all the projects on his plate. But they are loath to say anything negative about the man who is buying them dinner. \"He's been better recently,\" says one. Meanwhile, Wilson disappears to take a call. A letter had been published in  Nature   that day 5  defending inclusive fitness theory, a framework that seeks to explain the evolution of altruism, and that some evolutionary biologists have called unnecessary. The paper had made ripples in the press, and because Wilson made his name in such theoretical discussions, I assume that he is talking to someone about it. But when he returns, he says that he was on the phone with the religion reporter for the  Binghamton Press & Sun-Bulletin , giving the scoop on an agreement to survey more congregations. What seems like big news in the academic world has faded into the background for Wilson. There is so much to do right where he lives. \"When you compare what I am doing here with furiously pounding away on my typewriter about that arcane debate, I think I made the right choice,\" he says.\n Emma Marris writes for Nature from Columbia, Missouri. \n                     Science and the city \n                   \n                     Darwin at 200 \n                   \n                     David Sloan Wilson \n                   \n                     The Evolution Institute \n                   \n                     Evolutionary Religious Studies \n                   Reprints and Permissions"},
{"file_id": "474268a", "url": "https://www.nature.com/articles/474268a", "year": 2011, "authors": [{"name": "Meredith Wadman"}], "parsed_as_year": "2006_or_before", "body": "As pressure from activists builds, the United States is considering whether it should end invasive experiments in chimpanzees. The unusual meeting was held in a conference room, but it might have been called a war room. Gathered inside a little-known research centre in southern Louisiana, the people who oversee chimpanzee research in the United States were preparing to battle for the survival of their enterprise. Although no other country besides Gabon carries out invasive experiments with chimpanzees, the United States continues such work at three major research facilities. Louisiana's New Iberia Research Center (NIRC) is the largest, with a population of 360 chimps, used by investigators from pharmaceutical companies and federal agencies to test new drugs and study diseases such as hepatitis. During the meeting, Thomas Rowell, director of the NIRC, stood up, surveyed the audience, and launched into a presentation about possible strategies to build public support for their work. \"How do we get industry to be forthcoming about their use of chimpanzees?\" a slide read. \"Could we get at least a few solid examples of how the use of chimpanzees has truncated the time to discovery?\" And \"When we talk about time and lives saved by using chimpanzees, can we provide actual time span data or numbers?\" Another slide went on to note that the National Institutes of Health (NIH) spends about US$12 million a year caring for the chimpanzees it supports (currently totalling 734), versus the billions in health-care costs for the human diseases that can be studied through experiments on chimpanzees. One of them, hepatitis C, currently affects at least 170 million people globally. If researchers don't have access to the chimp model, said Rowell, people afflicted with hepatitis C will suffer. \"Their lifespans are going to be shortened. They will not have a proper quality of life.\" He called them a \"silent voice\". Rowell's pep talk in April was partly for the benefit of some visitors at the meeting: representatives from the Food and Drug Administration, the National Institute of Allergy and Infectious Diseases, the drug industry and, most importantly, the Institute of Medicine (IOM). The IOM, the medical branch of the independent National Academy of Sciences, was asked by the NIH in January to examine whether the government should keep supporting biomedical research on chimpanzees \u2014 the closest living relatives of  Homo sapiens . The NIH called for the study after the agency sparked a storm of opposition last year, when it announced plans to move 186 semi-retired chimps back into active research 1 . After protests by the Humane Society of the United States (HSUS) in Washington DC, famed primatologist Jane Goodall and others, the NIH changed course and said that it would make no decision on moving the chimps until the IOM study is complete. The study, it announced, would be \"an in-depth analysis to reassess the scientific need for the continued use of chimpanzees to accelerate biomedical discoveries\". Proponents say that the research is necessary for continued progress towards a hepatitis-C vaccine; for developing more effective drugs against hepatitis B and C; for testing monoclonal antibody treatments for a variety of conditions; and for research to develop a vaccine against respiratory syncytial virus, a seasonal virus that kills more than more than 66,000 children under the age of 5 each year across the globe 2 . For many of these conditions, backers argue, the chimpanzee is either the only available model, or by far the best one. But chimpanzee research in the United States is facing growing public and political opposition. Animal-welfare activists have stepped up their efforts to end the work, arguing that it is inhumane, ineffective and a waste of taxpayer money. The day after the meeting, activists held a press conference on Capitol Hill to mark the introduction of the Great Ape Protection and Cost Savings Act. The act would make all invasive chimpanzee research illegal, including private-sector work conducted at the centres and paid for by drug companies. The bill's lead sponsor in the House of Representatives is Roscoe Bartlett (Republican, Maryland), who trained as a physiologist and conducted primate research with NASA and with the military in the 1960s. \"There's just no valid argument to continue to keep these great apes as they're now being kept,\" Bartlett told the news conference. \"Very few of them are used in research and I'm not sure that any of them need to be used.\" The scrutiny this year adds to the tension felt by researchers who work with chimpanzees. That stress is particularly intense at the NIRC, which has been on the defensive ever since a television documentary two years ago showed footage of employees there mistreating and neglecting chimpanzees and macaques. The NIRC, which is part of the University of Louisiana at Lafayette, later paid a fine and has since passed numerous inspections, but the expos\u00e9 helped to propel the activism. In the contest for public support, says Rowell, \"our backs are up against the wall\".  \n                A study up close \n              On the same day that the chimp-protection measure was introduced in Congress, staff at the NIRC prepared to start a drug-company trial that used two chimpanzees to test the absorption, metabolism and excretion of an experimental medication. One of the animals was Simba, an 88-kilogram male around 40 years old. That morning he was coaxed from his outdoor enclosure, where he lives in a large social group, into an individual cage. A technician used a needle and syringe to sedate him. He was then strapped to a stretcher and transported by ambulance to Building 52 to receive a pre-study physical examination. At 9:27 a.m., Simba was slid off the stretcher \u2014 where it became clear that he had defecated \u2014 and onto a stainless-steel gurney. His fleshy pink gums were relaxed and prominent. He was drooling. \"I need to do a dental on him,\" said Dana Hasselschwert, head of the veterinary-sciences division and one of nine veterinarians on the NIRC staff. The veterinarians care for the centre's chimps, along with its 6,500 macaques and other monkeys. Today, three technicians are assisting Hasselschwert with the physical. Speed is important, because the sedative is short-lived. Fully alert chimpanzees are strong and sometimes violent. One technician quickly shaved Simba's forearms, armpits and groin. On the skin of his right groin, a tattoo identified him as chimpanzee number xo19. The other technicians placed electrodes on his body; his electrocardiogram revealed a regular heart rhythm. Simba's blood pressure was 143/87 millimetres of mercury \u2014 normal for him, Hasselschwert said. Blood was drawn from Simba's left femoral vein; his rectal temperature was taken and was normal, at 37.3 \u00b0C. His pulse was 104 beats per minute; his respirations 32. Hasselschwert palpated his liver and kidneys and found nothing abnormal. But one of the technicians was having trouble catheterizing Simba to collect a urine sample. Hasselschwert placed an ultrasound paddle on Simba's lower abdomen and located his bladder on a nearby screen. An assistant quickly shaved the overlying area. \"It's undignified, a male having bikini marks,\" Hasselschwert declared. She inserted a needle through Simba's abdominal wall and withdrew three millilitres of pale yellow urine. Simba's breathing was speeding up, a sign of growing wakefulness. \"Y'all, we need to move,\" Hasselschwert said. She wiped Simba's drooling gums with paper towels, and patted his open palm. His hand was half again as big as hers. \"He looks good,\" she declared, and, at 9:40 a.m., Simba was wheeled away on the gurney and placed in a wire cage that measured 2 metres long by 1.5 metres wide by 2.2 metres high. The cage is one of many in the room, and it can be compressed if an animal refuses to present an appendage for injections or blood withdrawal \u2014 a procedure that staff call \"squeezing up\". Three days later, Simba would be injected with the experimental drug. After that, for 72 hours, at regular intervals, his blood would be drawn and his urine collected from a pan beneath the cage. He would then be returned to his outdoor enclosure. Last year, the NIRC conducted 23 chimpanzee studies, which typically involve between two and six animals. On the day of Simba's physical, ten chimps were in experiments. The remaining chimps are kept in the outdoor cages. To keep the chimps prepared for being research subjects, trainers reward them with fruit in exchange for presenting their legs for mock injections, or for urinating in a cup. The chimps are wary of strangers, at whom they are wont to hurl gravel or faeces. Chimpanzee studies are expensive, costing anywhere from $20,000 to $250,000. And roughly 85% of the revenue for the NIRC comes from a score of pharmaceutical companies that are regular customers. (Other centres tilt more towards academic and governmental clients.) As well as conducting drug and vaccine trials, the NIRC breeds macaques for several companies, and is a registered importer of the monkeys. The other 15% of the centre's revenue comes from government agencies, mainly the NIH. The biomedical agency owns 124, or roughly one-third, of the NIRC chimpanzees, and pays the centre to maintain two breeding colonies of macaques. The centre also conducts chimpanzee research under contract for the Centers for Disease Control and Prevention. It owns 11 more chimps, which are kept at Bioqual, a company in Rockville, Maryland, where young animals are used in hepatitis-C studies run by the NIH's infectious-diseases institute. \n               boxed-text \n             If the IOM were to recommend that the NIH stop supporting chimpanzee research, and if the NIH were to comply, this would, theoretically, not affect the drug-company funded research at the NIRC and the other centres. But in practice, the directors say, it would hobble their enterprise, not least because some two-thirds of the chimpanzees available for research in the United States are owned or supported by the NIH (see  'Chimpanzee Research in the United States' ). What is more, they say, the per diem fees and user fees paid by companies for individual experiments do not begin to cover the long-term care of the animals, which is supported by NIH infrastructure grants. \"The lifetime maintenance of chimpanzees requires a long-term commitment of financial support that individually sponsored studies do not provide,\" the directors wrote in a jointly authored statement to  Nature .  \n                Inspired work \n              Rowell, who is 52, has been working with chimps most of his life, ever since he took a job cleaning cages at the NIRC when he was 17. He quickly got a taste of the value of chimpanzee research, when Carleton Gajdusek shared the 1976 Nobel Prize in Physiology or Medicine for discovering that neurodegenerative disorders such as kuru and scrapie are transmitted by infectious agents 3 . As part of his research, Gajdusek injected infected human brain tissue into chimps from the centre. The thrill of Gajdusek's work rubbed off on Rowell. \"This is what was so exciting \u2014 a teenager off the street working at the level that I was \u2014 and being involved with something so huge.\" Rowell chose his career at that point, and earned a degree in veterinary medicine at the Louisiana State University School of Veterinary Medicine. Hired by the NIRC in 1990, he became its director in 1998. Rowell has expanded the centre significantly, from about 170 employees in 1998 to 249 today, and from 4,560 primates in 1998 to 6,860 today. He also strengthened the NIRC's experimental credentials and abilities, which has made the centre highly attractive to the pharmaceutical industry. In the years since Rowell first started working at the facility, support for chimp research has slowly eroded around the world. The United States stopped importing chimpanzees after signing a 1973 treaty banning trade in endangered species. When the AIDS epidemic hit, the NIH launched a breeding programme for chimpanzees, but the agency declared a moratorium on breeding in 1995, after it became clear that chimps were a poor model for the disease. Soon, countries started to outlaw chimp research completely. In 1997, the United Kingdom took that step. Another eight countries followed suit in the next decade, and last year, the European Union outlawed great-ape experimentation. Only one pharmaceutical company, GlaxoSmithKline, has dropped chimp research, at least publicly. It announced in 2008 that \"the case for using great apes in the future is less clear than it may have been previously\". Opponents of chimp research have painted the United States as an outlier for continuing to allow such experiments. That charge irks the directors of the chimpanzee centres. Responding to a request from  Nature , the directors catalogued 27 chimpanzee studies carried out at their centres by foreign companies or scientists since 2005. \"The Europeans did not ban their companies from coming to the United States,\" says John VandeBerg, director of the Southwest National Primate Research Centre in San Antonio, Texas, another of the centres that conducts chimp research. \"And I can assure you they are not going to ban the importation of drugs into their countries that are developed using chimpanzees.\" In the United States, public pressure to shut down the research intensified after the television expos\u00e9. The show contained video footage obtained surreptitiously by an HSUS investigator who infiltrated the NIRC and worked there for nine months. In one scene from the resulting documentary, aired by the ABC news show  Nightline , a sedated chimp fell several feet from a bench to the cement floor of a cage. In another scene, a sedated chimp is carried by its arms and legs, not on a stretcher. Footage of some of the centre's monkeys was equally damaging. A technician hit a baby on the head after it bit her, and another employee rapped a monkey's teeth with a metal pole. In a different scene, an anaesthetized monkey was allowed to fall from a chest-level counter to the floor of a lab room. The show drew strong reactions. Jane Goodall issued a statement saying: \"In no lab I have visited have I seen so many chimpanzees exhibit such intense fear.\" And agriculture secretary Tom Vilsack ordered an investigation of the NIRC's animal-welfare practices. In the following 14 months, the centre underwent inspections by two units of the US Department of Agriculture (USDA), the NIH's Office of Laboratory Animal Welfare in Bethesda, Maryland, the Association for Assessment and Accreditation of Laboratory Animal Care International in Frederick, Maryland, and auditors from every one of the NIRC's pharmaceutical clients. The government agencies also paid surprise visits to the other facilities conducting chimpanzee research.  \n                The case for research \n              In May 2010, the NIRC paid $18,000 to the USDA to settle six alleged violations of the Animal Welfare Act, such as leaving sedated adult chimps unattended with nursing infants. As part of the agreement, the centre neither admitted nor denied that the violations took place, or that they were, in fact, violations. But the NIRC has since retrained employees in, for instance, keeping animals safe when they are sedated. Rowell says that he watched all ten hours of undercover video footage that the HSUS turned over to the NIH. He concedes that there were moments of carelessness and one case of inappropriate behaviour, when the technician hit the infant monkey. But he says that the undercover operative \u2014 who was working as an aide \u2014 bears responsibility for the fall involving the sedated monkey. It was her job to protect the animal, but she had stepped away to film the room from a distance. Overall, he says, \"I was proud of what I saw\". Now, focused by the IOM study, he is going on the offensive. Others are also speaking up, such as Christopher Walker, a hepatitis-C researcher based at Nationwide Children's Hospital in Columbus, Ohio, who is the main academic customer at the NIRC. Walker is part of a team funded by a five-year, $12.5-million grant from the Bill & Melinda Gates Foundation in Seattle, Washington, to try to develop a drug for hepatitis C that reinvigorates exhausted immune-system cells called T cells; he also relies on NIH grants. Walker has not spoken to the press before about his work with chimpanzees; he has been afraid of being targeted by animal-rights activists. But he is talking now, he says, \"because we are reaching a critical decision point\". Walker's work focuses on unravelling the role of cellular immunity in hepatitis-C infection, which often leads to liver cancer, a disease that is almost always fatal without a liver transplant. While working at a firm called Chiron in Emeryville, California, in the 1990s, Walker did the scientific groundwork 4 , 5  in chimpanzees that led Merck to develop a hepatitis-C vaccine programme. Several Merck employees then spun off Okairos, a Rome-based biotech, which has since moved a vaccine into human trials, after publishing proof-of-concept work in chimpanzees 6 . \"The chimpanzees were absolutely critical,\" Walker says, in establishing that immune-system cells called T-cells have an important role in controlling the hepatitis-C virus, and that any successful vaccine would need to generate a T-cell response. Walker is a strong believer that chimpanzee studies continue to be needed not only for developing a hepatitis-C vaccine, but also for testing the safety of new, and potentially risky, medicines to treat both hepatitis C and B. He points, for instance, to research published in 2009 that showed RNA silencing to be effective in controlling hepatitis-C infection in chimps 7 . Some others who use chimpanzees see few remaining justifications for experiments on the animals. Michael Houghton, a virologist at the University of Alberta in Edmonton, and a co-discoverer of the hepatitis-C virus, says that in research related to that virus, \"we do not need the chimp any more for diagnostic development or for antiviral-drug development as we have the infected human available\". The risk-to-benefit ratio for infected people in such studies is low enough to justify testing in humans, he says. Still, Houghton supports chimpanzee use for hepatitis-C vaccine development, because vaccines must be tested in uninfected individuals. He also supports studies in chimpanzees for the development of riskier immunotherapies against the disease. \"As inconveniencing tens of chimpanzees impacts the health of millions of humans, it is unethical not to use the chimp model for certain indications,\" says Houghton. He also believes that it would be unwise not to keep humanely treated chimps available in sanctuaries in case of bioterrorist attacks; the animals could be used to study the transmission of infectious bioweapons as well as vaccines and therapies, he says. Activists, though, see no rationale for continuing tests on chimps, partly, they say, because ever-more sophisticated  in vitro   methods make it unnecessary 8 . They also argue that, despite the genetic similarities between chimps and humans, they have relevant differences in, for instance, immune-response genes 9 , and that differences in gene expression make chimps weak as a biological model. \"Stop using the excuse that chimps are essential to this research,\" says John Pippin, a physician who is senior medical and research adviser for the Physicians Committee for Responsible Medicine in Washington DC. By the end of the year, the IOM committee will offer its own analysis of whether chimp research is scientifically warranted. The committee's report will be the most weighty pronouncement on the issue so far in the United States, but it may not settle the debate. The ongoing controversy has taken a toll on some of those who work with chimpanzees. Rowell says he does not take the same amount of pleasure in his work that he did five years ago. \"I'm exhausted,\" he says. Still, he vows to stay in the job. \"It's not something that I do. It's who I am.\" \n                 See Editorial  \n                 page 251 \n               Meredith Wadman is a reporter for Nature based in Washington DC. \n                     Outlook on hepatitis C \n                   \n                     NIH reversal on Alamogordo chimps \n                   \n                     Webfocus on chimp genome \n                   \n                     News special on chimp genome \n                   \n                     New Iberia Research Center \n                   \n                     Institute of Medicine charge to evaluate US chimpanzee research \n                   \n                     Institute of Medicine Committee on the Use of Chimpanzees in Biomedical and Behavioral Research \n                   \n                     NIH statement on Alamogordo Primate Facility \n                   \n                     Letter asking for an \u201cin-depth analysis of the current and future need for chimpanzee use in biomedical research.\u201d \n                   \n                     National Center for Research Resources chimpanzee and other primate resources \n                   \n                     Cost of NIH support for chimpanzee maintenance \n                   Reprints and Permissions"},
{"file_id": "474272a", "url": "https://www.nature.com/articles/474272a", "year": 2011, "authors": [{"name": "Philip Ball"}], "parsed_as_year": "2006_or_before", "body": "The key to practical quantum computing and high-efficiency solar cells may lie in the messy green world outside the physics lab. On the face of it, quantum effects and living organisms seem to occupy utterly different realms. The former are usually observed only on the nanometre scale, surrounded by hard vacuum, ultra-low temperatures and a tightly controlled laboratory environment. The latter inhabit a macroscopic world that is warm, messy and anything but controlled. A quantum phenomenon such as 'coherence', in which the wave patterns of every part of a system stay in step, wouldn't last a microsecond in the tumultuous realm of the cell. Or so everyone thought. But discoveries in recent years suggest that nature knows a few tricks that physicists don't: coherent quantum processes may well be ubiquitous in the natural world. Known or suspected examples range from the ability of birds to navigate using Earth's magnetic field to the inner workings of photosynthesis \u2014 the process by which plants and bacteria turn sunlight, carbon dioxide and water into organic matter, and arguably the most important biochemical reaction on Earth. Biology has a knack for using what works, says Seth Lloyd, a physicist at the Massachusetts Institute of Technology in Cambridge. And if that means \"quantum hanky-panky\", he says, \"then quantum hanky-panky it is\". Some researchers have even begun to talk of an emerging discipline called quantum biology, arguing that quantum effects are a vital, if rare, ingredient of the way nature works. And laboratory physicists interested in practical technology are paying close attention. \"We hope to be able to learn from the quantum proficiency of these biological systems,\" says Lloyd. A better understanding of how quantum effects are maintained in living organisms could help researchers to achieve the elusive goal of quantum computation, he says. \"Or perhaps we can make better energy-storage devices or better organic solar cells.\"  \n                Energy routefinder \n              Researchers have long suspected that something unusual is afoot in photosynthesis. Particles of light called photons, streaming down from the Sun, arrive randomly at the chlorophyll molecules and other light-absorbing 'antenna' pigments that cluster inside the cells of every leaf, and within every photosynthetic bacterium. But once the photons' energy is deposited, it doesn't stay random. Somehow, it gets channelled into a steady flow towards the cell's photosynthetic reaction centre, which can then use it at maximum efficiency to convert carbon dioxide into sugars. Since the 1930s, scientists have recognized that this journey must be described by quantum mechanics, which holds that particles such as electrons will often act like waves. Photons hitting an antenna molecule will kick up ripples of energized electrons \u2014 excitons \u2014 like a rock splashing water from a puddle. These excitons then pass from one molecule to the next until they reach the reaction centre. But is their path made up of random, undirected hops, as researchers initially assumed? Or could their motion be more organized? Some modern researchers have pointed out that the excitons could be coherent, with their waves extending to more than one molecule while staying in step and reinforcing one another. If so, there is a striking corollary. Coherent quantum waves can exist in two or more states at the same time, so coherent excitons would be able to move through the forest of antenna molecules by two or more routes at once. In fact, they could simultaneously explore a multitude of possible options, and automatically select the most efficient path to the reaction centre. Four years ago, two teams working under Graham Fleming, a chemist at the University of California, Berkeley, were able to obtain experimental proof to back up this hypothesis (See  'Quantum fact meets fiction' ). One team used a string of very short laser pulses to probe the photosynthetic apparatus of the green sulphur bacterium  Chlorobium tepidium 1 . The researchers had to chill their samples to 77 K with liquid nitrogen, but the data from their laser probes showed clear evidence of coherent exciton states. The second team carried out a similar study of the purple bacterium  Rhodobacter sphaeroides 2 , and found much the same electronic coherence operating at temperatures up to 180 K. In 2010, researchers from the first group published evidence of quantum coherence in their bacterial complex at ambient temperatures 3  \u2014 showing that coherence is not just an artefact of cryogenic laboratory conditions, but might actually be important to photosynthesis in the real world. Around the same time, a team led by Gregory Scholes, a chemist at the University of Toronto in Canada, also reported coherence effects at ambient temperatures 4  \u2014 this time not in bacteria, but in photosynthetic cryptophyte algae, evolutionarily distinct organisms that are more closely related to plants and animals, and that use completely different light-absorbing chemical groups. But how can quantum coherence last long enough to be useful in photosynthesis? Most physicists would have assumed that, at ambient temperatures, the surrounding molecular chaos in the cell destroys the coherence almost instantly. Computer simulations carried out by Lloyd and some of his colleagues suggest an answer: random noise in the environment might actually enhance the efficiency of the energy transfer in photosynthesis rather than degrade it 5 . It turns out that an exciton can sometimes get trapped on particular sites in the photosynthetic chain, but simulations suggest that environmental noise can shake it loose gently enough to avoid destroying its coherence. In effect, says Lloyd, \"the environment frees up the exciton and allows it to get to where it's going\". Photosynthesis is not the only example of quantum effects in nature. For instance, researchers have known for several years that in some enzyme-catalysed reactions 6 , protons move from one molecule to another by the quantum-mechanical phenomenon of tunnelling, in which a particle passes through an energy barrier rather than having to muster the energy to climb over it. And a controversial theory of olfaction claims that smell comes from the biochemical sensing of molecular vibrations \u2014 a process that involves electron tunnelling between the molecule responsible for the odour and the receptor where it binds in the nose 7 . Are such examples widespread enough to justify a whole new discipline, though? Robert Blankenship, a photosynthesis researcher at Washington University in St Louis, Missouri, and a co-author with Fleming on the  C. tepidium   paper, admits to some scepticism. \"My sense is that there may well be a few cases, like the ones we know about already, where these effects are important,\" he says, \"but that many, if not most, biological systems will not utilize quantum effects like these.\" But Scholes believes that there are grounds for optimism, given a suitably broad definition of quantum biology. \"I do think there are other examples in biology where an understanding at the quantum-mechanical level will help us to appreciate more deeply how the process works,\" he says.  \n                The bird's-eye compass \n              One long-standing biological puzzle that might be explained by exotic quantum effects is how some birds are able to navigate by sensing Earth's magnetic field. The avian magnetic sensor is known to be activated by light striking the bird's retina. Researchers' current best guess at a mechanism is that the energy deposited by each incoming photon creates a pair of free radicals 8  \u2014 highly reactive molecules, each with an unpaired electron. Each of these unpaired electrons has an intrinsic angular momentum, or spin, that can be reoriented by a magnetic field. As the radicals separate, the unpaired electron on one is primarily influenced by the magnetism of a nearby atomic nucleus, whereas the unpaired electron on the other is further away from the nucleus, and feels only Earth's magnetic field. The difference in the fields shifts the radical pair between two quantum states with differing chemical reactivity. \"One version of the idea would be that some chemical is synthesized\" in the bird's retinal cells when the system is in one state, but not when it's in the other, says Simon Benjamin, a physicist at the University of Oxford, UK. \"Its concentration reflects Earth's field orientation.\" The feasibility of this idea was demonstrated in 2008 in an artificial photochemical reaction, in which magnetic fields affected the lifetime of a radical pair 9 . Benjamin and his co-workers have proposed that the two unpaired electrons, being created by the absorption of a single photon, exist in a state of quantum entanglement: a form of coherence in which the orientation of one spin remains correlated with that of the other, no matter how far apart the radicals move. Entanglement is usually quite delicate at ambient temperatures, but the researchers calculate that it is maintained in the avian compass for at least tens of microseconds \u2014 much longer than is currently possible in any artificial molecular system 10 . This quantum-assisted magnetic sensing could be widespread. Not only birds, but also some insects and even plants show physiological responses to magnetic fields \u2014 for example, the growth-inhibiting influence of blue light on the flowering plant  Arabidopsis thaliana   is moderated by magnetic fields in a way that may also use the radical-pair mechanism 11 . But for clinching proof that it works this way, says Benjamin, \"we need to understand the basic molecules involved, and then study them in the lab\".  \n                Selected benefits \n              Quantum coherence in photosynthesis seems to be beneficial to the organisms using it. But did their ability to exploit quantum effects evolve through natural selection? Or is quantum coherence just an accidental side effect of the way certain molecules are structured? \"There is a lot of speculation about the evolutionary question, and a lot of misunderstanding,\" says Scholes, who is far from sure about the answer. \"We cannot tell if this effect in photosynthesis is selected for, nor if there is the option not to use coherence to move the electronic energy. There are no data available at all even to address the question.\" He points out that it isn't obvious why selection would favour coherence. \"Almost all photosynthetic organisms spend most of the day trying to moderate light-harvesting. It is rare to be light-limited. So why would there be evolutionary pressure to tweak light-harvesting efficiency?\" Fleming agrees: he suspects that quantum coherence is not adaptive, but is simply \"a by-product of the dense packing of chromophores required to optimize solar absorption\". Scholes hopes to investigate the issue by comparing antenna proteins isolated from species of cryptophyte algae that evolved at different times. But even if quantum coherence in biological systems is a chance effect, adds Fleming, its consequences are extraordinary, making systems insensitive to disorder in the distribution of energy. What is more, he says, it \"enables 'rectifier-like' one-way energy transfer, produces the fastest [energy-transfer] rate, is temperature-insensitive and probably a few other things I haven't thought of\". These effects, in turn, suggest practical uses. Perhaps most obviously, says Scholes, a better understanding of how biological systems achieve quantum coherence in ambient conditions will \"change the way we think about design of light-harvesting structures\". This could allow scientists to build technology such as solar cells with improved energy-conversion efficiencies. Seth Lloyd considers this \"a reasonable expectation\", and is particularly hopeful that his discovery of the positive role of environmental noise will be useful for engineering photonic systems using materials such as quantum dots (nanoscale crystals) or highly branched polymers studded with light-absorbing chemical groups, which can serve as artificial antenna arrays. Another area of potential application is in quantum computing. The long-standing goal of the physicists and engineers working in this area is to manipulate data encoded in quantum bits (qubits) of information, such as the spin-up and spin-down states of an electron or of an atomic nucleus. Qubits can exist in both states at once, thus permitting the simultaneous exploration of all possible answers to the computation that they encode. In principle, this would give quantum computers the power to find the best solution far more quickly than today's computers can \u2014 but only if the qubits can maintain their coherence, without the noise of the surrounding environment, such as the jostling of neighbouring atoms, destroying the synchrony of the waves. But biology has somehow solved that challenge: in effect, quantum coherence allows a photosystem to perform a 'best-path' quantum computation. Benjamin, whose main interest is in designing materials systems for quantum computation and information technology, sees the ambient-temperature avian compass as a potential guide. \"If we can figure out how the bird's compass protects itself from decoherence, this might just give us a few clues in the quest to create quantum technologies,\" he says. Learning from nature is an idea as old as mythology \u2014 but until now, no one has imagined that the natural world has anything to teach us about the quantum world. Philip Ball is a writer based in London. \n                     Graham Fleming \n                   \n                     Greg Scholes \n                   \n                     Greg Engel \n                   \n                     Robert Blankenship \n                   \n                     Seth Lloyd \n                   \n                     Simon Benjamin \n                   Reprints and Permissions"},
{"file_id": "474436a", "url": "https://www.nature.com/articles/474436a", "year": 2011, "authors": [{"name": "Eric Hand"}], "parsed_as_year": "2006_or_before", "body": "By mining a database of the world's books, Erez Lieberman Aiden is attempting to automate much of humanities research. But is the field ready to be digitized? Erez Lieberman Aiden is standing on the sun deck of his town house, rocking back and forth on the balls of his bare feet as he belts out a blessing. The Hebrew words echo across the quiet courtyards of Harvard University in Cambridge, Massachusetts. The sky has turned indigo as the light and warmth leak away from this day in late April.  Shalom aleichem , he sings. Peace be upon you. Lieberman Aiden \u2014 molecular biologist, applied mathematician and, at 31 years old, the precocious doyen of the emerging field known as the digital humanities \u2014 could do with a little peace. The cries of his 10-month-old son have abated \u2014 for the moment \u2014 and he has had just enough time to throw on a pair of frayed black trousers and a shiny synthetic pullover before his guests arrive. A five o'clock shadow darkens the terrain between his thick goatee and unkempt hair. The night before, he caught a late train back from Princeton University in New Jersey, where he, the geeky scientist, had the delicate task of informing a room of erudite historians that his efforts at mining a database of 5\u00a0million books, about 4% of all those ever published, had made much of what they do trivially easy. The scrupulous tracking of ideas across history, for instance \u2014 work that has consumed entire careers \u2014 can be done in seconds with tools that Lieberman Aiden and his colleagues have invented.  Yet his role as evangelist for change in the humanities \u2014 or doomsday prophet, depending on your point of view \u2014 is just one of the many parts played by Lieberman Aiden. He is also: the inventor of a groundbreaking protocol that reveals how DNA can be tightly wound and yet untangled enough to orchestrate life; the chief executive of iShoe, a company that is testing sensor-stuffed shoe inserts to help the elderly with their balance; and the co-founder, with his wife, of Bears Without Borders, which sends thousands of stuffed animals to children in the developing world. (Barely concealed in the couple's basement are mounds of donated animals awaiting delivery.) In pouring his energies into all the projects that excite him, Lieberman Aiden doesn't transcend disciplinary boundaries so much as ignore them. And although he is still technically a postdoctoral researcher at Harvard, Lieberman Aiden seems to publish the results of those projects almost exclusively on the covers of  Science   and  Nature ; hung in the stairwell below the sun deck, he has framed blow-ups of the magazine covers to prove it. But that is work, and this is Shabbat dinner, the start of the Jewish Sabbath: a time for rest. The light switches will remain untouched, leaving the house illuminated through the night; the hot plate in the kitchen, on which the meal is being warmed, is on a timer. Three candles have been lit, one for each member of the household. Lieberman Aiden sings unabashedly in a hearty baritone that is not at all like his reedy, excitable speaking voice. He gazes at his wife, Aviva Presser Aiden, who grins back at him, holding her sweater tight to herself in the chilly night air. She too has reason to rest contentedly. The week before, she learned that she had won a US$100,000 grant from the Bill & Melinda Gates Foundation in Seattle, Washington, to build a microbial fuel cell that could charge mobile phones in Africa. The project means a year-long break from her studies at Harvard Medical School in Boston, where she is adding an MD to her PhD in genetics.  It is only by comparison with this academic power-couple that the other dinner guests \u2014 two young, self-assured Harvard physics graduates \u2014 look a bit lost, but that probably has more to do with their unfamiliarity with the Shabbat rituals. They flip through the Hebrew prayer books and try to follow along. But Lieberman Aiden, who in his 20s toyed with becoming a rabbi, has no need for the book. These are the texts that he has studied for years. These are the words he knows best.  \n                Reading very not-carefully \n              As a reader with a finite amount of time, Lieberman Aiden likes to say, you pretty much have two choices. You can read a small number of books very carefully. Or you can read lots of books \"very, very not-carefully\". Most humanities scholars abide by the former approach. In a process known as close-reading, they seek out original sources in archives, where they underline, annotate and cross-reference the text in efforts to identify and interpret authors' intentions, historical trends and linguistic evolution. It's the approach Lieberman Aiden followed for a 2007 paper in  Nature 1 . Sifting through old grammar books, he and his colleagues identified 177\u00a0verbs that were irregular in the era of Old English (around AD 800) and studied their conjugation in Middle English (around AD 1200), then in the English used today. They found that less-commonly used verbs regularized much more quickly than commonly used ones: 'wrought' became 'worked', but 'went' has not become 'goed'. The study gave Lieberman Aiden a first-hand lesson in how painstaking a traditional humanities approach could be. But what if, Lieberman Aiden wondered, you could read every book ever written 'not-carefully'? You could then show how verbs are conjugated not just at isolated moments in history, but continuously through time, as the culture evolves. Studies could take in more data, faster. As he began thinking about this question, Lieberman Aiden realized that 'reading' books in this way was precisely the ambition of the Google Books project, a digitization of some 18\u00a0million books, most of them published since 1800. In 2007, he 'cold e-mailed' members of the Google Books team, and was surprised to get a face-to-face meeting with Peter Norvig, Google's director of research, just over a week later. \"It went well,\" Lieberman Aiden says, in an understatement. \n               Click here for larger image \n               Working with Google and his chief collaborator, 29-year-old Harvard psychology postdoc Jean-Baptiste Michel, Lieberman Aiden built a software tool called the  n -grams viewer to chart the frequency of phrases across a corpus of 500 billion words. A 'one-gram' plots the frequency of a single word such as 'feminism' over time; a 'two-gram' shows the frequency of a contiguous phrase, such as 'touch base' (see  'Think outside the box' ).  \n               Click here for larger image \n               Google unveiled the tool on 16\u00a0December 2010, the same day that Lieberman Aiden and his colleagues published a paper in  Science 2  describing how the tool could be used, for example, to identify the verb that has regularized the fastest: 'chid' and 'chode' to 'chided' in some 200 years (see  'The fastest verb on the planet' ). \"We found 'found' 200,000\u00a0times more often than we finded 'finded',\" they wrote, with characteristic playfulness. \"In contrast, 'dwelt' dwelt in our data only 60 times as often as 'dwelled' dwelled.\" Interspersed between the jokes were real discoveries \u2014 many of which had nothing to do with verbs. By comparing German and English texts from the first half of the twentieth century, the team showed that the Nazi regime suppressed mention of the Jewish artist Marc Chagall, and that the  n -grams tool could be used to identify artists, writers or activists whose suppression had hitherto been unknown. Lieberman Aiden and Michel called their approach culturomics, a reference to the genomics-like scale of the book database, and a nod to the future, when they hope that more of the media that underpin culture \u2014 newspapers, blogs, art, music \u2014 will be folded in. In the first 24 hours after its launch, the  n -grams viewer ( ngrams.googlelabs.com ) received more than one million hits. Dan Cohen, director of the Roy Rosenzweig Center for History and New Media at George Mason University in Fairfax, Virginia, calls the tool a \"gateway drug\" for the digital humanities, a field that has been gaining pace and funding in the past few years (see  'A discipline goes digital' ). The name is an umbrella term for approaches that include not just the assembly of large-scale databases of media and other cultural data, but also the willingness of humanities scholars to develop the algorithms to engage with them. \"These tools are revolutionizing the way we work and the kinds of arguments we can make,\" says Dan Edelstein, a historian at Stanford University in California, who has used mapping software to show unexpected patterns in the way that Voltaire's letters spread through Europe during the Enlightenment.  Yet some humanities researchers in the traditional camp complain that their field can never be encapsulated by the frequency charts of words and phrases produced by the  n -grams tool. \"I think saying all books equal the DNA of human experience \u2014 I think that's a very dangerous parallel,\" says Cohen. How do you factor in the cultural contributions of furniture, or dance, or ticket stubs at a movie hall, he asks. What about all the books that were never published? Or the culture as experienced by the world's vast illiterate populations?  Other scholars have deep reservations about the digital humanities movement as a whole \u2014 especially if it will come at the expense of traditional approaches. \"You can't help but worry that this is going to sweep the deck of all money for humanities everywhere else,\" says Anthony Grafton, a historian at Princeton and president of the American Historical Association, who uses a giant, geared wooden reading wheel to help him manage his oversized, Renaissance texts. He wants researchers to hold onto the power that comes with intimately knowing their primary sources, right down to the scribbled notes in the margin that would elude the book scanners. \"You don't want to give up what is your own core activity,\" he says.  \n                Following tradition \n              Back at the Aiden house, the Shabbat dinner guests have all laved their hands with a glass of water and returned to the sun deck for matzo-ball soup. Lieberman Aiden explains some of the trepidation he felt when he and Michel talked to the historians at Princeton about their work. \"I was a little bit nervous going in,\" he says. \"I really thought that we were going to get denounced at one point.\"  Although Lieberman Aiden and Michel are sensitive to the feelings of traditional humanities scholars, they are also too young, restless and deeply ambitious to slow their own pursuits. Lieberman Aiden says that the influence of technology on the humanities is already past a tipping point. The tools and methods that it provides, he says, will be impossible for researchers to ignore. And yet he doesn't think that the old approaches will ever disappear. \"I think you should use the best methods available \u2014 and all of them,\" he says. \"And I think that includes carefully reading texts and trying to get behind what authors think.\" Daniel Koll, one of the dinner guests, shyly interrupts. \"Erez? Do you think you're maybe partly influenced in that kind of thinking by your religious upbringing? From my limited outsider's perspective, Judaism has a very strong interpretive component. There is no single authority on a text, and so on.\" He wonders whether Lieberman Aiden, like any good humanities scholar, enjoys wrestling with the ambiguities of religious texts as much as he enjoys cool, hard data.  Clearly, the answer is yes \u2014 why else would his host have spent a year of his life at Yeshiva University in New York, studying the Talmud and Jewish case law? But Lieberman Aiden, who prefers to talk about other people and their ideas rather than himself, provides an indirect answer by way of history. He tells the story of Isaac Casaubon, a sixteenth-century Protestant scholar, who undermined the presumed Egyptian provenance of a set of religious texts by identifying a reference to a Greek play on words \u2014 something that could only have been written hundreds of years later. \"That point is as objective an interpretive remark as any remark a scientist might make,\" says Lieberman Aiden. \"So the methods of humanists are very, very formidable. And I think the degree of insecurity they have over whether these methods are here to stay is not really befitting.\"  \n                Two cultures \n              From the day he was born in a New York City hospital, Lieberman Aiden was steeped in the cultures of both language and technology. The son of a Hungarian mother and Romanian father, both \u00e9migr\u00e9s by way of Israel, Lieberman Aiden grew up in a community of Satmar Jews, a branch of Hasidic Orthodox Judaism. English was his third language, after Hungarian and Hebrew, and by the age of nine, he was helping his father, a self-taught inventor, with the English contracts for the family saw-manu\u00adfacturing business. Lieberman Aiden studied at a religious high school in Brooklyn, but soon found that video games held more allure. In his second year there, he found himself flunking classes, and his addiction to  X-COM: UFO Defense   was consuming so much time that he eventually had to quit, cold turkey. \"It was an amazing game, actually,\" he says, ruefully. Lieberman Aiden soon found more edifying outlets for his energies: he was allowed to skip school one day a week to study in a molecular-biology lab at Brooklyn College, and he began his own computer-repair business. The family was quite secular by Hasidic standards, going to synagogue only for the High Holidays of Rosh Hashana and Yom Kippur. One day in high school, he went to Burger King for his usual bacon cheeseburger, and decided to respect kosher rules by forgoing the bacon \u2014 not realizing that mixing dairy and beef in the cheeseburger itself was not kosher at all. Lieberman Aiden went to Princeton as an undergraduate, where he wasn't content to study just maths and physics, but also fulfilled all the requirements for a philosophy degree. And even as he took five, six, seven classes a term, he managed to squeeze in creative-writing courses, specializing in haiku poetry. While at Yeshiva University after Princeton, he taught maths on the side to pay for a master's degree in history, and completed the first year of rabbinical studies. \"He's a non-conformist by design and he revels in it,\" says his Talmudic study partner, Avi Bossewitch. And yet, he says, \"he's the least arrogant person I've ever met\".  The allure of science eventually proved too strong. Lieberman Aiden left Yeshiva to begin a PhD at the Broad Institute of the Massachusetts Institute of Technology (MIT) and Harvard in Cambridge, under the supervision of famed geneticist Eric Lander. But even while mastering molecular biology, he put his maths skills to use. He realized that a knot-free shape described in a 120-year-old maths paper \u2014 a fractal globule \u2014 could describe the way that the 2-metre-long human genome folds up into the cell nucleus, a space one million times smaller. He then developed a protocol to prove that this was true. The results, published in the first of his papers to make it onto the cover of  Science 3 , showed that the fractal globule allowed widely separated sections of DNA to unfold and interact. \"There are no bounds to what he can be interested in,\" says Lander, who, like others, suspects that culturomics might eventually be merely a sideline for Lieberman Aiden in his ascent in mathematical biology.  It was during his time in the Lander lab that he met Aviva Presser, a shy young woman from Los Angeles who was also working towards her PhD. They married in 2005, with Lander giving one of the blessings. Rather than one of them taking the other's name, they decided to share a common new name: Aiden, which means Eden in Hebrew and, in Gaelic, little fire. Those fires stoked, they were faced with another naming issue last June: what to call their son. The working title,  in utero , was Snedley Balagan (Balagan means 'fiasco' in Hebrew). But they soon settled on Gabriel Galileo Aiden. Presser Aiden says that no one believes them when they say that they had no idea that their son's initials formed the DNA code for a sweet amino acid.  \n                Work and play \n              Towards the end of the Shabbat dinner, Gabriel decides to wake up, adding to Presser Aiden's obvious exhaustion. But her husband doesn't want to miss out on the highlight of the night, an Aiden-family staple: Dessert Face-off, in which each guest competitively designs a slice of brownie within a theme. Given that guests Koll and his girlfriend, Larissa Zhou, are interested in molecular gastronomy, Lieberman Aiden decides on the theme of food science. A box of edible Betty Crocker decorations is laid out. Koll turns his brownie into the cross-section of a wok. Zhou transforms hers into a pig \u2014 definitely not kosher. But Lieberman Aiden's construct is perplexing: he has created a starry night-time skyscape, with multicoloured sprinkles as stars and the Milky Way. What does this have to do with food science? \"Well, you know,\" he says, a little proud of himself, \"gastronomy is just one letter away from astronomy.\"  Gabriel has returned to bed, and Presser Aiden looks ready to follow, but her husband isn't quite finished yet. As midnight approaches, Lieberman Aiden is holding forth on the mathematical beauty of ramen noodles \u2014 as depicted in the 2008 film  Kung Fu Panda   \u2014 and remarking on a piece of maths software, KnotPlot, that could help his wife to braid some really radical challah bread. The guests finally do Presser Aiden a favour and excuse themselves to go home.  But Lieberman Aiden just might keep revving and riffing through the night: he can work for 70\u201380 hours straight, fuelled by Diet Coke and junk food. He has big plans for cultur\u00adomics, as he and Michel add more languages, books and other media to the  n -grams database. He also has new projects to think over, such as one currently under way with Ed Boyden, a young, acclaimed neurobiologist at MIT, in which the pair are developing a way to detect the genes expressed in thousands of individual cells at a time. But tonight and tomorrow he'll keep his computer off, although it is not religious conviction that makes him abide by the Sabbath rules. Doing so forces him to detach, clear his mind and go for walks in the park with his wife and son. And yet the boundary between work and play \u2014 just like that between the sciences and the humanities \u2014 is not one that Lieberman Aiden respects. That might just be what makes him successful, says Lander. For centuries, the best science has come from the most playful scientists, he says. Think of Watson and Crick shirking the lab in favour of tennis; think of Einstein and his wild-haired bike rides. \"What do children do?\" says Lander. \"They learn, they're curious, they're stimulated. The problem is, at some point, many people get in a rut. They're not really interested in learning more. They're not able to be fascinated and delighted by everything around them. Erez \u2014 he hasn't lost the playfulness.\"\n \n                     Erez Lieberman Aiden \n                   \n                     n-grams viewer \n                   \n                     Culturomics \n                   Reprints and Permissions"},
{"file_id": "474556a", "url": "https://www.nature.com/articles/474556a", "year": 2011, "authors": [{"name": "Vivienne Irikefe"}, {"name": "Gayathri Vaidyanathan"}, {"name": "Linda Nordling"}, {"name": "Aimable Twahirwa"}, {"name": "Esther Nakkazi"}, {"name": "Richard Monastersky"}], "parsed_as_year": "2006_or_before", "body": "Africa's nations are achieving some success in building their science capacity, but the foundations remain unsteady. The forecast for science in Africa has brightened over the past decade. After enduring civil wars and economic crises, many countries have entered a period of rapid growth and leaders are starting to see science and technology as the keys to progress. In 2006, members of the African Union endorsed a target for each nation to spend 1% of its gross domestic product (GDP) on research and development (R&D). And at a summit the following year, heads of state in Africa declared 2007 the year for scientific innovation. The available data show much progress, but many nations have big gaps to overcome. In May, the African Union released  African Innovation Outlook 2010 , a survey of some of the scientifically most productive sub-Saharan nations. It showed that only three \u2014 Malawi, Uganda and South Africa \u2014 topped the 1% spending threshold in 2007; most remained far from that mark, even when the support from foreign donors was included. More recent spending totals are not available for most nations, but interviews with scientists and governmental officials across sub-Saharan Africa suggest that funding levels remain low. Money is just one of many problems, as  Nature   reports in the following profiles of six nations that highlight some of the issues confronting the region. Many labs are poorly equipped, and science students get little practical research training because research centres are often separate from universities. Financial and logistical support for science is typically divided between many ministries with little coordination, and some states rely too much on intermittent foreign funding. Even when research is successful, it is hard to push developments to the marketplace. And poor governance \u2014 from corruption to ineffective bureaucracy \u2014 stymies progress in many nations. Despite these hurdles, some African nations can point to notable achievements, in individual institutes and in areas of research. They will need to build on these advances if they are to have any hope of tackling the problems facing Africa today, such as poverty, rampant infectious diseases, the impacts of climate change and the lack of clean water and energy. The progress described here too easily grinds to a halt when conflicts erupt or governments lose interest in supporting R&D. But science and technology leaders say that they are trying to develop and sustain capacity in the research that can most help their nations to develop. That hope has lured Wole Soboyejo, a professor of mechanical and aerospace engineering at Princeton University in New Jersey, temporarily back to Nigeria, where he grew up. As vice-president in charge of academic research and innovation at the African Institute of Science and Technology (AIST) in Abuja, Soboyejo wants to reverse the brain drain that is robbing Africa of many leading scientists and engineers. His goal is to train top students from across the continent, then to help establish them back in their home countries, where they can serve as seeds of excellence. \"It has the potential to raise the level of the continent as a whole,\" he says.  \n                Nigeria: The heavyweight \n              Africa's most populous nation wants to catapult itself into the world's top 20 economies by 2020. And science and technology is a central part of the strategy. Nigeria's president, Goodluck Jonathan, has a PhD in zoology, and in the first few weeks after being elected he approved a US$5-million grant to the Nigerian Academy of Science to support its work. \n               Click here for larger image \n               In terms of scientific publications, Nigeria is already a powerhouse of the region, coming in second to South Africa among sub-Saharan nations, according to an analysis of Elsevier's Scopus database. It outpaces South Africa in a few fields, such as food science and agronomy, and it boasts an AIST, which opened in 2008 and is drawing students from around the continent with its emphasis on innovation. But Nigeria publishes far less, relative to its GDP, than many other African nations (see 'Publishing landscape'). And it has fewer researchers, relative to its population size, than countries such as Cameroon, Gabon, Senegal and South Africa. Scientific leaders say that the problems are systemic: poor science education, lack of equipment in labs, a dearth of funding and little appreciation for the role that science can have in helping the country. \"Even though people see all the problems that confront us as a nation,\" says Oye Ibidapo-Obe, president of the Nigerian Academy of Science, \"somehow scientists have not been able to convince the people that the solution lies in their libraries, in their workshops and in their laboratories.\" Paul Orhii, director-general of the National Agency for Food and Drug Administration and Control, says that \"this is not just the fault of the federal government\". He places some of the blame on the private sector, which is not investing in research as much as it does in other nations. Umar Bindir, director-general of the National Office for Technology Acquisition and Promotion, has tried to change that with a programme that gets multinational companies doing business in Nigeria to give back by sharing technology or training. On the horizon, Nigerians look forward to the imminent launch of a second national Earth-observing satellite, NigeriaSat-2, as well as the experimental NigeriaSat-X \u2014 the first satellite built by Nigerian engineers.  \n                Uganda: Growing pains \n              In economic terms, Uganda is one of the success stories in sub-Saharan Africa. Its GDP has grown at an average rate of more than 7% between 2000 and 2010 and it weathered the recent economic crisis better than many of its neighbours. Likewise, the country's science sector has grown. Under long-time president Yoweri Museveni, the country has been boosting its spending on R&D, from $73 million in 2005\u201306 to $155 million in 2008\u201309, which is close to 1% of the country's GDP. And in 2006, the country won $30 million in low-interest loans as part of the World Bank's Millennium Science Initiative (MSI). That five-year project, implemented by the Uganda National Council for Science and Technology (UNCST), paid for the training of 102 scientists and engineers at the master's and doctorate levels. Among the research projects funded by the MSI programme is one at Med Biotech Laboratories in Kampala to develop a malaria vaccine that has undergone successful testing in baboons, says Thomas Egwang, the director of the non-profit lab. Uganda also invests substantially in agricultural science; it is one of the few sub-Saharan nations to spend more than 1% of its agricultural GDP on R&D in this sector. That funding helped the National Agricultural Research Laboratories at Kawanda to develop bananas enriched with vitamin A and iron that are now being tested, as well as genetically modified bananas resistant to the banana wilt disease. But behind these highlights lie major concerns. Uganda has one of the lowest densities of researchers among the most scientifically advanced nations in sub-Saharan Africa: just 25 researchers per million inhabitants. The country is trying to reduce its dependence on foreign donors, so it decided not to request additional loans through the World Bank when the MSI money runs out this year. The government promised to continue funding the MSI grants itself, but it did not allocate money for them in its recent budget. Although research activities are growing in the country, observers say that the educational system is lagging. Only 6 of the 27 universities in Uganda offer science-related programmes; and even at those few universities, only one in five students pursues a degree in science. Another problem that the country faces is that it has no science ministry, despite the government's promises last year to establish one. \"They should create a ministry for us,\" says Peter Ndemere, executive secretary of the UNCST. \"You need somebody in cabinet to defend your budget.\"  \n                Kenya: In search of talent \n              Kenyan science is a study in contrasts. Among sub-Saharan nations, it ranks third \u2014 behind South Africa and Nigeria \u2014 in its output of scientific papers published in international journals, and its publishing outranks that of economic heavyweight Nigeria in fields such as environment, ecology and immunology. It is also a hub of collaborations on the continent (see 'Country connections'). But Kenya's research output has grown more slowly than most other sub-Saharan nations. In the recent African Union survey, Kenya scored last in terms of the increase in the numbers of published research papers, normalized for population size. \n               Click here for larger image \n               Most of the scientific work in Kenya is centred in government-owned research institutes that have extensive international collaborations. Among the most renowned is the Kenya Medical Research Institute (KEMRI), which has centres around the country and does basic research as well as developing drugs, vaccines and products such as diagnostic kits for HIV \u2014 an important service because Kenya lacks a thriving private sector for commercialization of research. KEMRI has a budget of $37.5 million, with 45% coming from its international collaborators, including the Wellcome Trust, a London-based medical research charity. Other centres also stand out, such as the Kenya Agricultural Research Institute, headquartered in Nairobi, which has an international reputation for its work on crops and agricultural diseases. And the Kenya Marine and Fisheries Research Institute, headquartered in Mombasa, has a programme focused on mangrove research that is considered the best in sub-Saharan Africa. By contrast, the universities suffer from lack of infrastructure and money. The government and donors have focused on boosting primary and secondary education, but have neglected universities, say observers. The government invested only $3.6 million in 2010 on university-based research, according to Shaukat Abdulrazak, secretary of the National Council for Science and Technology. And there is a shortage of professors to serve a student population that grew from 90,000 in 2004 to more than 120,000 in 2008.  \n                Tanzania: Eyeing independence \n              Tanzania is overshadowed by neighbouring Kenya in most things \u2014 trade, political influence and traditionally also in science and technology. But in recent years, its government has taken steps towards strengthening home-grown research and cutting dependence on foreign funding. At $234 million, or 0.48% of its GDP, Tanzania's research spending in 2007 was on a par with Kenya's as a proportion of GDP, according to the African Union's survey. But 38% of its R&D funding comes from abroad \u2014 as opposed to 18% in Kenya. This means that research funding is precarious for the country's academics. Despite this, the country has pockets of long-established research excellence. The Ifakara Health Institute (IHI), a jewel of Tanzanian science, grew out of a partnership with the Swiss Tropical Institute and remains almost entirely funded by foreign partners. Researchers there have studied malaria in central Tanzania since the 1950s, and in 2009 it became the first institution in Africa to start clinical trials of RTS,S, a promising malaria vaccine. The IHI's prominent malaria research is one reason why Tanzania scores as one of Africa's top five countries in terms of immunology publications, according to  Global Research Report \u2014 Africa   published by Thomson Reuters in April 2010. The country also ranks highly in social sciences and environmental science. However, agricultural research \u2014 the field in which more than half of the country's researchers work \u2014 does not excel. In 2010, the country's president Jakaya Kikwete announced a financial boost for science. Funding for the Commission for Science and Technology (COSTECH), which gives out research grants, increased thirty-fold last year to $20 million. This funding will support 200 new PhD and master's students, agricultural research and technology transfer projects. Hassan Mshinda, COSTECH's director, expects another increase this year. \"The money has started to flow,\" he says. But the funding increase is throwing up a new problem for him: making sure the money ends up in the right hands. That means putting funding applications up for rigorous peer review, which has not happened in the past to Tanzania's public R&D funding. Some of the spending is going towards training reviewers and setting up quality-control mechanisms. \"We want to build a culture of competitive and peer-reviewed research,\" Mshinda says.  \n                Senegal: Ageing expertise \n             At first glance, Senegal seems to be doing well in science. With 661 researchers per million inhabitants (see graphs), the country is second to South Africa in researcher density, according to the African Union report. The people who work in R&D are also highly qualified compared with the African average: more than one-quarter have a PhD. However, Senegal's scientists are getting old. More than half of its agricultural researchers were over 50 years old in 2007, in part because there was a slump in spending for higher education and research in the late 1980s and early 1990s. As agriculture is one of the areas in which the Senegalese government wants to invest, the country's scientific development depends on training and retaining enough early and mid-career scientists. Historical ties with France have left a strong legacy in mathematics and physical sciences. In September, Senegal will officially open an African Institute for Mathematical Sciences in Mbour on the coast south of the capital Dakar. The institute is based on a model pioneered in Cape Town, South Africa (see  page 567 ), and is the second in a network of such institutions that is planned across the continent. Like researchers in Tanzania, Senegal's receive more than 38% of their funding from abroad. And when foreign grants end, there is scarce national funding to sustain long-term research projects. But the government's interest in science is growing. In October 2010, it vowed to boost funding in three areas of science: renewable energy, reducing soil salinity and seed development. The priority areas are matched with the country's biggest challenges: frequent power cuts are a bane of Senegal's citizens and businesses. Decades of drought have resulted in expanding areas with salty soils that don't support crops. And the World Food Programme estimates that 46% of households in Senegal are vulnerable to food shortages, with 20% considered highly vulnerable. The country is also creating a National Centre for Scientific and Technological Research to coordinate government policy across the ministries that deal with science.  \n                Rwanda: Wired for science \n              President Paul Kagame has frequently declared science to be a key part of Rwanda's development. In 2006, he established a ministry of scientific research and the following year he vowed to vastly increase the country's spending on the promotion of science, with a goal of reaching 3% of its GDP by 2012. Since the 1994 genocide and civil war, Rwanda has rebuilt many of its institutions and has chalked up some notable successes in areas linked to science, such as reducing the prevalence of malaria and HIV/AIDS. And the country just finished laying 2,300 kilometres of fibre-optic cable to expand Internet access. In its 2011\u201312 budget, released this month, the Rwandan government again announced plans to increase R&D spending, with a focus on constructing and equipping science laboratories and on health and agriculture research. Rwanda's officials are seeking to improve its capacity in part by partnering with other nations to promote skills, training and knowledge exchanges between scientists. But researchers in Rwanda say the spending levels remain too low. According to information provided by various government agencies, Rwanda will spend just over $12 million on scientific research activities in the new budget. The lack of support for research is causing science graduates to choose other professions. \"Poverty is still an issue for the majority of graduates in science, and looking for jobs in other careers is the only option for them when money is short,\" says Hermogene Nsengimana, vice-dean of the faculty of science at the National University of Rwanda. Rwanda's science output falls well short of its ambitions. It ranks 27th out of 43 sub-Saharan nations in publications, two places behind Namibia, which has just one-fifth the population. But Rwandan officials hope that the investments that they have made in rebuilding the country's infrastructure will eventually pay off. \n                Africa on the rise \n              Despite the many problems confronting scientists in sub-Saharan Africa, there are signs that they are starting to build momentum. After a period of relatively slow growth during the 1990s and early 2000s, the output of publications is now rising rapidly. In 1996, sub-Saharan researchers produced roughly 0.8% of the total papers in the Scopus database. By 2009, that fraction had reached about 1%. Part of this trend can be explained by increasing collaborations among researchers in Africa and the developed world. KEMRI in Kenya, for example, has seen its output grow by 45% in the last 5 years, with an increasing number of papers coauthored by researchers at institutions such as the London School of Hygiene and Tropical Medicine and the Centers for Disease Control and Prevention in Atlanta, Georgia. Other African institutes are seeing growth in collaborations with rapidly developing countries such as China (see  page 560 ) and Brazil. Expanding access to the Internet across sub-Saharan Africa is one factor behind these intercontinental ties. But they also reflect the growing ambitions of Africa's own researchers. Vivienne Irikefe is a journalist in Lagos, Nigeria. Gayathri Vaidyanathan is an International Development Research Center fellow at Nature and covered Kenya. Linda Nordling, a freelance writer in Cape Town, South Africa, wrote about Senegal and Tanzania. Aimable Twahirwa is a journalist in Kigali, Rwanda. Esther Nakkazi is a journalist in Kampala, Uganda. Richard Monastersky is a features editor with Nature in Washington DC. \n                     Science in Africa special \n                   \n                     Nature Climate Change \n                   \n                     Nature Middle East \n                   \n                     Africa Innovation Outlook 2010 \n                   \n                     UNESCO Science Report 2010 \n                   \n                     Thompson Reuters Global Research Report: Africa \n                   Reprints and Permissions"},
{"file_id": "474563a", "url": "https://www.nature.com/articles/474563a", "year": 2011, "authors": [{"name": "Gayathri Vaidyanathan"}], "parsed_as_year": "2006_or_before", "body": "Scientists are fighting damaging wheat fungi from East Africa, but breeding new crops won't help unless farmers plant them. David Cheruiyot noticed that his wheat fields were turning the wrong colour. The stems of the plants took on a sickly brown hue, and when he peeled open the heads there was no grain inside. \"If you go to inspect it, there is nothing but dust,\" he recalls. Ug99, a virulent fungus that causes a disease called stem rust, arrived on Cheruiyot's farm in Kenya in 2007. It devastated wheat fields in the country that season, slashing yields by as much as 80% in some regions. Since that epidemic, Cheruiyot has sprayed his wheat three times a season with fungicide, something that few farmers in Africa can afford. Stem rust has plagued farmers for millennia, but Ug99 is a new superstrain that overcomes defensive genes in 90% of the wheat crops planted around the globe. Since it was first detected in 1998, spores of the fungus have spread from East Africa into Yemen and Iran. If the disease continues its march eastwards, hitting the breadbaskets of south Asia and China, it will threaten the food supply of hundreds of millions of people. Yet there are potential defences against the scourge, growing in a field just a few kilometres from Cheruiyot's farm. At the Kenya Agricultural Research Institute in Njoro, plant scientists have screened more than 200,000 lines of wheat for variants that are resistant to fungal attack. By combining genes using traditional breeding methods, the researchers have already developed 20 types of wheat that can withstand an attack from Ug99. Some countries are testing and starting to distribute these varieties. But that is only half the battle. There is a large gap between the controlled world of agricultural research and the farming communities of developing nations. In the case of wheat, governments and farmers have been slow to adopt agricultural advances, especially in Africa. There are problems on both sides: farmers tend to be inherently conservative, but researchers do not always know which crops will appeal most. Farmers look not only for high yields, but also for by-products such as straw that can double as animal feed, or stalks for thatching roofs, so researchers must take that into account when designing wheat varieties. It can take years of work \u2014 and millions of dollars \u2014 to get new seeds to catch on. The divide between research and farming was thrown into sharp relief this year by a different epidemic. A related fungal disease \u2014 yellow rust \u2014 swept through parts of Africa and the Middle East, cutting yields in half in some places. In hard-hit countries such as Ethiopia, farmers were unprepared, and are only now seeking out seeds that can resist the disease. But the varieties that are widely available are not the newest and best that science has to offer. Getting the right seeds to farmers \u2014 and convincing them to try them \u2014 is a major challenge in Africa and the rest of the developing world. \"It's not just putting seed in a storeroom where you sell it,\" says Ravi Singh, head of irrigated bread-wheat improvement at the International Maize and Wheat Improvement Center (CIMMYT), based in Mexico City, which developed the new wheat varieties. \"It has to do with proper demonstration and publicity so farmers come to see it. Seeing is what counts in the end.\"  \n                Black rust \n              In the developing world, wheat is second only to rice as a staple food. More than 4.5 billion people worldwide depend on wheat, and forecasts suggest that demand for the grain will soar by as much as 60% in the developing world by the middle of this century. At the same time, climate change is expected to reduce wheat production in the developing world. A shock to the system in the form of a devastating new race of wheat rust \u2014 whether stem or yellow rust \u2014 could push millions into food insecurity. When spores of the stem-rust fungus  Puccinia graminis   land on wheat plants, they form brown pustules and suck up nutrients meant for the developing grain. Soon, the plant is enveloped in reddish-brown or black pustules, leading to the disease's other name, black rust. In the 1960s, agronomist Norman Borlaug and his colleagues at CIMMYT bred varieties of wheat to contain genes that conferred resistance to rust, including one gene called  Sr31 , or stem rust 31. His team tested many of their plants in nurseries in Njoro, because wheat fungi thrive in the East African climate all year round. The new varieties also had other desirable traits, such as high yields and a compact size to resist winds. Those were some of the key advances that sparked the Green Revolution in agricultural production, and in 1970 Borlaug won a Nobel Peace Prize for his efforts. Then people forgot about stem rust. With the disease under control, wheat supplies and prices stabilized, and money drained away from rust-resistance research. The Njoro nursery was shut down. CIMMYT continued breeding wheat, but switched its focus to properties such as increased yield. In 1990, scientists created a high-yielding wheat called Attila, which soon dominated fields in North and East Africa, the Middle East and parts of south Asia. About 20 million hectares of the developing world is now planted with this monoculture. Meanwhile,  P. graminis   continued to evolve in the warm, humid climes of East Africa. In 1998, CIMMYT scientists in Uganda noticed a highly virulent type of stem rust that ruined wheat varieties containing the  Sr31   gene, including Attila. The new strain was dubbed Ug99, after the place in which it was found and the year it was formally identified. Since then, seven variants of Ug99 have been found, each of which can overcome different sets of resistance genes. The spores travel by wind, hopping across nations and waiting for humid and hot weather to spark an epidemic. In 2005, after hearing about the stem-rust resurgence, Borlaug mobilized scientists in a project that came to be known as Durable Rust Resistance in Wheat (DRRW), spearheaded by plant researchers at Cornell University in Ithaca, New York, and CIMMYT. Screening programmes resumed in Njoro, and scientists from research institutes and universities around the world started sending their wheat lines to Kenya for testing. In February this year, the DRRW received a US$40-million donation from the UK Department of International Development and the Bill & Melinda Gates Foundation in Seattle, Washington.  \n                Breeding immunity \n              Twice each year, plant researchers from nations including Mexico, Canada, Australia, the United States and India converge on Njoro to inspect the crops. Ignoring their hunger and jet lag, they don white protective bodysuits and rush to a 3-hectare wheat field surrounded by acacia trees and barbed wire. To the untrained eye, all the plants in the field look the same. But this season, 17,000 distinct lines of wheat have been planted there. The traits of each need to be examined, scored, selected and \u2014 where desirable \u2014 bred into successive generations. Julio Huerta-Espino, a senior geneticist and pathologist with CIMMYT's irrigated wheat-breeding programme, screens plants by touch. He runs his thumb and forefinger down the stem of a plant that looks clean, but tiny bumps alert him to the early stage of a rust infection. On a disease scale up to 100, Huerta-Espino scores these particular plants as 70 \u2014 or almost completely susceptible \u2014 and snaps the stems in half. He tags plants that have fewer pustules, about a ten on the scale, with blue tape. He will take these plants to CIMMYT's centre in Mexico and select them for other qualities that make the perfect wheat variety \u2014 such as a large head that yields a lot of grain, many heads per plant and resistance to yellow rust and another related disease, leaf rust. Singh will then choose which of the plants to cross, and send their offspring back to Njoro to make sure that the new generation has inherited sufficient resistance to stem rust. Scientists use this old-fashioned 'shuttle' breeding in large part because it is difficult to genetically modify wheat. The plant is hexaploid, having three copies of its chromosome pairs. Its genome is five times larger than the human genome, making regions of interest hard to pinpoint and manipulate. The wheat genome has been sequenced, but scientists have yet to annotate it. And relatively few markers have been identified, so it is difficult to map genes and link them to traits. In the past, scientists have sought to protect wheat by breeding for 'major' genes \u2014 such as  Sr31   \u2014 that confer significant protection against a particular strain of fungus. The genes are thought to code for effector proteins that recognize the invading fungus and trigger plant defences. Over time, though, the fungus can mutate and evade the plant's recognition or defence systems. That was how Ug99 was born. Scientists at CIMMYT are now harnessing 'minor' genes, which offer broader, but less powerful, defences. Each reduces the severity of disease by only 10% or 20%. But a combination of minor genes, stacked up in one plant, can form an effective shield \u2014 and is harder for the fungus to evade, because it would need to evolve resistance to several genes at once. Scientists in Canada and Australia, affiliated with the DRRW, are taking a different approach, stacking major resistance genes and identifying new ones. Others are searching for genetic markers linked to minor and major genes, to ease future gene-based selection. The researchers want to keep developing more types of resistant varieties in case the fungus, which is always present in East Africa, evolves a way to defeat the minor-gene protection. \"This is what challenged us with Ug99,\" says Ronnie Coffman, an agricultural biotechnologist at Cornell and vice-chairman of the Borlaug Global Rust Initiative, an advocacy and research-coordination programme. The protection afforded by  Sr31   lasted for 30 years. \"People began to assume it would endure,\" he says. Coffman believes that once the wheat genome has been annotated, it will be easier for scientists to use genetic tools to screen for the ultimate multiple-disease-resistant wheat variety. \n               Click here for larger image \n               Meanwhile, scientists with CIMMYT have spent years screening varieties bred with minor genes, and have produced 20 resistant types, which have been released to national breeding programmes in eight nations in Africa and Asia, including Kenya and Ethiopia (see 'Wheat's worst enemies').  \n                Evolving threat \n              Stem rust is not the top threat to farmers in the Munesa district of Ethiopia, 1,100 kilometres north of Njoro. Although Ug99 is present here, these wheat-growing regions are usually too cool for stem rust, there has been no major outbreak since 1993. But last year, yellow rust significantly cut wheat yields in Munesa and other parts of Ethiopia. Researchers suspect that last year's outbreak was caused by Yr27, a new race of the yellow-rust fungus, which also devastated fields in North Africa and the Middle East. First seen in south Asia in 2002, Yr27 has the potential to defeat popular types of wheat, including Attila. So plant scientists quickly bred varieties resistant to it. But farmers and seed multipliers in Ethiopia had no interest in the new varieties, because they liked the high yields from Attila. So, without much demand, the seeds never became widely available. This is a familiar story. Since 2006, the Ethiopian Institute for Agricultural Research (EIAR) in Addis Ababa has been promoting a type of wheat called Digalu, which is resistant to Ug99 and has some resistance to yellow rust. Researchers with the EIAR convinced a few farmers to plant the seed, but it did not gain much traction against the high-yielding Attila. All that changed last year. The yellow-rust epidemic hit fields of Attila hard, but areas planted with Digalu were untouched. Seeing healthy plants amid sickened fields, farmers finally started seeking out the new seeds. But Digalu is not the best variety out there. Its resistance to Ug99 is based on a single major gene rather than a set of minor genes, so at some point it will fall prey to the constantly evolving stem-rust fungus. If farmers take to Digalu, they may reject better varieties that have minor-gene resistance when they become available. In 2010, the Ethiopian government approved two new Ug99-resistant varieties from the 20 developed by the DRRW programme, and one of them is also resistant to yellow rust. Although both varieties yield more grain than Attila and should appeal to farmers, neither is yet available for purchase, because they are in the preliminary stages of seed multiplication. The situation should improve if the multipliers can produce a large crop of the seeds, says Singh. If all goes well, nearly 8% of Ethiopia's bread-wheat area will be planted with the two new varieties by 2013, he says. \"To me, this is a major achievement in a short time period.\" That is a best-case scenario. It takes time to build up support for a variety, and success depends on many factors, such as whether governments have enough funding and agents to demonstrate the new seeds to farmers. Ethiopia has long been lacking in these areas, but this year it received $3 million from the US Agency for International Development to multiply seeds, including the two DRRW varieties. Scientists recognize the difficulty of getting plants with desirable traits into wide circulation, so they are now bringing farmers into the experimental nurseries and asking them which traits would be most valuable in the real world. Ethiopian agricultural officials used such advice from farmers to select which Ug99-resistant varieties to release in their country. Bedada Girma, coordinator for the DRRW programme in Ethiopia and a scientist at the EIAR, then recruited influential farmers to try out the new seeds. Last year, Girma asked Manza Hamda, a farmer in the Oromia region, to grow one of the wheat varieties, called Kakaba. Hamda planted the seed on 0.25 hectares of his 3-hectare farm. When yellow rust came, Kakaba fared much better than Attila did. Other farmers noticed, and asked Hamda for some of the resistant seed. He had enough to sell to one of his neighbours, who plans to plant Kakaba in his fields in the next few weeks. Slowly, one farmer at a time, the fruits of science are starting to take root in East Africa. Gayathri Vaidyanathan is an International Development Research Center fellow at Nature. \n                     Science in Africa special \n                   \n                     Food and the future \n                   \n                     CIMMYT \n                   \n                     The Durable Rust Resistance in Wheat Project \n                   \n                     Borlaug Global Rust Initiative \n                   \n                     Rust Mapper \n                   Reprints and Permissions"},
{"file_id": "474560a", "url": "https://www.nature.com/articles/474560a", "year": 2011, "authors": [{"name": "Linda Nordling"}], "parsed_as_year": "2006_or_before", "body": "China is pumping money into African science. But what do both sides stand to gain \u2014 and lose? Before Emeka Oguzie went to China, he had only read about potentiostats in journals. The Nigerian materials scientist knew that he needed the electrochemical analysis tool to advance his search for indigenous plant extracts that can slow the corrosion that rots industrial machinery. But his cash-strapped department at Nigeria's Federal University of Technology in Owerri could not afford a US$25,000 piece of equipment. And his lack of skill with the device meant that scientists in the United States or Europe would not offer him a postdoc position abroad. \"I had no experience with the facilities they work with,\" he says. Oguzie's luck turned in 2005, when he won a fellowship from the Chinese Academy of Sciences and TWAS, the academy of sciences for the developing world, to spend a postdoctoral year in China. At the Institute of Metal Research in Shenyang, northeast China, he was trained on the potentiostat he had lacked for so long. Chopsticks, however, he had to master on his own. \"In the early days I always moved around with a fork in my pocket just to ensure I could eat happily,\" he says. The year in China was a turning point for Oguzie. Now he is back in Nigeria, but he presents his research at international conferences and publishes in high-impact journals. His department has not one, but three potentiostats. One of them, he bought with a grant from TWAS; the other two were hand-me-downs from researchers in the United States and Britain. He uses them to analyse potential environmentally friendly corrosion inhibitors from plants such as the native bitter kola ( Garcinia kola ) or the tropical roselle ( Hibiscus sabdaffia ) \u2014 something Nigeria's own oil and gas industry might use. He also returns to see his Chinese colleagues, and admires their strong sense of community. \"I really think it is a great nation. They seem to believe in collective action: 'we' achieving greatness together as a people rather than 'I' as is common in Nigeria,\" he says. Oguzie's story is not unique. Over the past few years a patchwork of initiatives has increased the number of African researchers and students spending time in China and has boosted collaborations. The number of African students in China rose by 40% to nearly 4,000 between 2005 and 2006, and the trend has continued. China is also emerging as a science collaborator with Africa, a role traditionally occupied by the United States and Europe. In Nigeria, one of China's biggest African trade partners, Chinese researchers co-authored 1.5% of the country's total research publications in international journals between 2004 and 2008 \u2014 up from 0.3% in 2000 (see 'Nigeria\u2013China collaboration'). Cheap Chinese loans are funding a new teaching research hospital in Nairobi, Kenya, and a new science and technology university in Thyolo, Malawi. In November 2009, the Chinese government launched its most ambitious plan to date for boosting African science. The Forum on China\u2013Africa Cooperation (FOCAC), involving 49 African countries, includes a Chinese pledge to fund 100 joint demonstration projects on scientific and technological research and to host 100 African postdoctoral fellows and 5,500 African scholarship students in China, all by 2012. The plan also promises clean-energy projects and agricultural research centres as well as the dispatch of Chinese agricultural experts to Africa \u2014 and by this year, Chinese authorities say they have made progress towards many of these goals (see 'Building bridges with Africa'). \n               Click here for larger image \n               \n                Caring for a continent \n              The science investment is still outpaced by US and European funding. But it is a part of a bigger game that China is playing in Africa. The fast-growing economy is thirsty for Africa's resources: its oil, minerals and agricultural land. Investing in science and technology helps China to show that it also cares about the continent's development. \"China is responding to criticism that it is not building enough capacity in Africa,\" says Sven Grimm, director of the Centre for Chinese Studies at Stellenbosch University in South Africa. African countries, meanwhile, have welcomed the Chinese investment, which is seen as less tied up with cumbersome conditions than is financial support from the West. But hard questions are still being asked. The quality of the Chinese assistance is one issue: some Africans are disappointed by their stay in China \u2014 perhaps the hosting institution wasn't as advanced as hoped. A lack of follow-through from African countries is another. Although the Chinese authorities have pledged to support African researchers when they return home, for example, the scarcity of resources in many countries raises concerns that Chinese-trained researchers will languish at home and move elsewhere. And if Africa cannot capitalize on the Chinese investments, then will they really benefit the continent? One thing is clear: the Chinese are everywhere in Africa. By 2008, trade between the world's second largest economy and the poorest continent had increased ten-fold from 2000 (see 'China's trade with Africa'). The Chinese are selling telecommunications equipment in Uganda, chickens in Zambia, building roads in Ethiopia and pumping oil out of Angola. China's foray has received much criticism, especially from rich countries in the West. Many experts on Sino\u2013African relations cry hypocrisy, pointing out that Western countries have their own sometimes-exploitative history with the continent. Still, China has come under fire for dealing with regimes with bad human-rights records, such as Sudan, and Chinese companies have been accused of cutting corners. Stories abound of newly built roads that collapse after one rainy season, buildings proclaimed structurally unsound months after they are opened, and deplorable working conditions in Chinese-owned mines. Some investments come in return for diplomatic favours \u2014 China's loan to Malawi for the science university only became possible after Malawi severed its diplomatic ties with Taiwan in 2007. The science partnerships with Africa are signs that the relationship is becoming more complex. FOCAC and other collaborative initiatives have emphasised 'win\u2013win' scenarios. The Africans get loans without too many questions asked. And Chinese funding for science infrastructure \u2014 research hospitals, universities, labs \u2014 is typically channelled directly to the contractors, usually Chinese. Many initiatives focus on industries in which China has a clear interest in exploiting Africa's resources \u2014 petroleum science in Nigeria, or agriculture in Mozambique. So is China training Africans because it helps the nation to extract the resources it needs, or is it a genuine attempt to help Africa develop? It's both, says Simon Zadek, a visiting fellow at Harvard University's John F. Kennedy School of Government in Cambridge, Massachusetts, who has consulted with the UK government on China. \"Don't assume that there is one China in Africa, or that the plan is sophisticated or even internally consistent,\" he says. \"It is a maturing process with many moving parts, diverse and often uncoordinated Chinese actors and many hit and miss experiences.\" Outside Maputo in Mozambique, one of the agricultural technology centres promised in the 2009 FOCAC agreement is taking shape. The $5.5-million Agricultural Technology Resource and Transfer Centre will aim to improve the productivity of Mozambican farmers by testing the suitability of new breeds of crops \u2014 including maize (corn), rice, vegetables and fruit \u2014 in the country's pest- and flood-prone climate. Among other things, China has pledged to increase rice production in the country fivefold. There will be a lab for soil analysis and facilities for demonstrating different types of irrigation systems. Initially it will be staffed with Chinese researchers, who will extend the knowledge to farmers through interpreters. The laboratories may also be open to local university researchers to conduct studies on locally important issues such as pest resistance and fish farming. One of the main differences between working with China and working with Western donors is the speed with which things get done, says Vasco Lino, research and innovation director of Mozambique's science and technology ministry. \"They bring everything, they set up everything in place; infrastructure, expert assistance. We never see the money, everything is handled by them,\" he says. \"It's very easy and fast. In one year they finished everything.\" But speed can also cause problems. Adams Bodomo, a Ghanaian social scientist at the University of Hong Kong who has lived and worked in China for 14 years, believes that the Chinese are trying to fulfil their pledges to African research by meeting quantitative targets. In surveys that he has conducted with African students and researchers in China he has formed the impression that the student recruitment could be happening too fast, at the expense of quality. Many Africans were dissatisfied with the selection process for their scholarships, he says. The surveyed scientists told Bodomo that the criteria for selection were often based not on excellence, but on connections with the officials in their home country. \"The aim should not be to add more students to meet the target, but to improve the quality of the education,\" he says. \"In China, not all universities are good,\" says Hassan Hussein Musa, an animal-health scientist at Nyala University in Darfur, Sudan. \"Some universities are still growing; their staff are still training and some of them don't speak English well.\" If Africans end up in poor-quality institutions, they could waste their time. But Africans can minimize their chances of getting a raw deal on their fellowship in China by thoroughly researching the institutes they have an opportunity to be based at, says Musa, so they can make an informed choice. He spent five years at the Yangzhou University in Jiangsu province for his PhD and postdoc, returning to Nyala in October 2009. He made many friends and still works with colleagues in China, but knows others who did not fare as well. What happened when Musa returned home illustrates another major challenge that Africans can face: the inability of their home countries to capitalize on the gains. In 2009, he became one of five African scientists to receive 150,000 renminbi (US$23,000) from the Chinese government to spend on equipment for his lab. The funding was announced alongside the China\u2013Africa science collaboration plan. But Musa needed more money to fulfil his dream \u2014 establishing an institute of molecular biology at his poorly resourced university. He managed to get some funding from the Sudanese government for research, and has sent a number of students to get their PhDs in China. But now he is at a loss for how to fund them when they return. There won't be enough funding from the Chinese to sustain their work, and he is not sure that his country's government will step into the breach. The whole dream seems a distant prospect. \"I am struggling at the moment, and if a chance comes to move to a good institute abroad I will leave,\" he says.  \n                National challenges \n              A similar fate could meet other Chinese-funded science projects in Africa if the host countries don't invest their own money. The agricultural resource centre in Mozambique will receive funding from China for three years, after which the Mozambican government is expected to take over the running and financing, something it may not be able to afford. Malawi's new science university \u2014 supposed to be staffed by government-funded scientists once it is built \u2014 faces similar uncertainty. Experts agree that an increase in national research funding is essential to make sure the continent does not get saddled with expensive institutes that it can't staff or sustain, and that its freshly trained academics have something to return to. \"The key problem is on the African side,\" says Grimm. \"If governments don't develop their science, for instance by tapping into returnees from other parts of the globe, it's really wasting human resources,\" he says. Ludger Kuehnhardt, a political scientist at Bonn University in Germany, adds that African countries also need to push their own scientific agenda to ensure that they get the research institutes and expertise that they need to tackle national challenges, and that China's interest doesn't skew African research priorities. Decades of US and European investment have already left African science propped up by foreign funding and sensitive to foreign priorities. If Africa doesn't leverage this new investment, these woes could be exacerbated. In the end, then, the outcome will depend on the African partners, and how wisely they manage their special relationship with China, says Oguzie. \"Most of us believe, wrongly though, that the Chinese investment programme in Africa is a sort of grant-in-aid, that they are doing it as a favour.\" Africans need to assert themselves, he says. \"Partnerships and collaborations should be for mutual interests and benefits, and the earlier we in Africa realize this, the better for everyone.\" Linda Nordling is a freelance writer based in Cape Town, South Africa. \n                     Science in the Developing world \n                   \n                     Insight: China: Views from the West \n                   \n                     Science in Africa special \n                   \n                     China-Africa science and technology cooperation \n                   Reprints and Permissions"},
{"file_id": "474555a", "url": "https://www.nature.com/articles/474555a", "year": 2011, "authors": [], "parsed_as_year": "2006_or_before", "body": "Addressing a continent's challenges. Africa is a continent that is young at heart. It has the most youthful population, and the number of people is growing faster than anywhere else: from 1 billion people now, the population is predicted to swell to 3.5 billion by the turn of the century. Its nations are also young, with most having achieved independence in the past 50\u201360 years, and a new country, South Sudan, will to be born in July. Africa also struggles with a disproportionate share of strife, disease, poverty and hunger. Scientific and technical advances \u2014 particularly those that draw on research on the continent \u2014 will be central to tackling those problems.   Nature   this week examines the world of scientific research and development in this rapidly evolving continent. Because news coverage tends to focus on South Africa, the News Features and Comment articles in this issue look at science output and education across sub-Saharan Africa. These countries are tremendously diverse, yet there are several themes that run throughout the region, from the importance of agricultural research to the increasing collaboration with China. The package highlights the huge potential of these nations, as well as some of the major barriers they need to overcome. \n                     Science in Africa special \n                   Reprints and Permissions"},
{"file_id": "475156a", "url": "https://www.nature.com/articles/475156a", "year": 2011, "authors": [{"name": "Mary Carmichael"}], "parsed_as_year": "2006_or_before", "body": "By raising hell about newborn blood-spot screening, Twila Brase could jeopardize public-health programmes and derail research. The problem is, she has a point. Twila Brase was not always the kind of person who hands out politically charged propaganda in airports. On a first meeting at her modest office in a shopping plaza in St Paul, Minnesota, she seems more like the unassuming nurse she was back in 1995 \u2014 before she began her second life as a bioethical gadfly, and before she had started making YouTube videos that accuse her state of commandeering the DNA of children as \"government property\" through widespread newborn screening programmes. Her voice is quiet and level. It is difficult to write her off as a conspiracy theorist: she simply doesn't sound like one, even when, 4.5 minutes into making the case against screening, she suggests that \"some researchers\" might be trying to convince the state to test day-old infants for genes linked to \"a tendency towards violence\". But Brase may have brought newborn screening and associated research in Minnesota to the point of crisis with her allegations. By tapping into ideological veins that run deep in the United States \u2014 wariness of government intrusion and fears about threats to privacy \u2014 she could influence the fates of many studies that seek to use human samples from state biobanks, not to mention the fates of thousands of children with rare diseases. Brase has a bent for the hyperbolic: her website features, among other touches, a photo of an infant in a shirt that reads, \"Help! The Gov't Has My DNA\". She also bends the truth at times to make her points. The claim that the state might test for \"propensity to violence\", for example, is based on a self-published proposal by a student at the University of Connecticut School of Law in Hartford ( http://go.nature.com/uodhkh ) \u2014 hardly an imminent programme. But one argument gives her detractors pause. Parental consent for research on infant blood spots is handled poorly \u2014 if it is broached at all \u2014 in the laws of many US states. What little parents know about newborn screening often comes from a short brochure given to them just after labour and delivery, when they're too distracted to process the contents. Supporters of the screening tend to emphasize the medical benefits. \"This is not about privacy or invading anyone's life,\" says Nancy Mendelsohn, a medical geneticist at Children's Hospitals and Clinics of Minnesota in Minneapolis. \"These aren't things that we're doing to children. These are things that we're doing for children.\" But some state health departments are already rethinking their approach to informing parents and asking for consent.  \n                Heel pricks for health \n              Among public-health professionals worldwide, newborn screening is generally viewed as one of the most successful innovations of the modern era. Within a few days of birth, infants are pricked at the heel, and their blood is tested for rare genetic and endocrine conditions that can be harmful or fatal if they are not caught early. Screening programmes began in the 1960s with a test for phenylketonuria, a disorder with effects on mental development that can be avoided through dietary restrictions. In the past decade, screening has expanded to encompass roughly 40 diseases. In most cases, DNA is not tested; rather, protein analysis screens for enzymes that might be affected by disease. Most developed countries have some form of newborn screening in place. The US programme alone identifies at least 3,400 children in need of treatment every year. Beyond the public-health initiative, however, the United States and many other countries save 'blood-spot' samples on cards, and some give them to scientists for use in population-based studies, after stripping away identifying details. Advocates of biobanking view blood-spot repositories as a valuable scientific resource. The samples have been used to develop tests for debilitating and fatal disorders, such as severe combined immunodeficiency, and to ensure the accuracy of existing tests. They have also been used in epidemiological research \u2014 for instance, blood spots have helped scientists in Minnesota to examine blood mercury levels and prenatal exposures to tobacco. Because it is performed on tissue samples rather than on live human beings, such research generally does not require explicit informed consent. And parents are often uninformed. (Although some countries \u2014 such as the United Kingdom, Germany and the Netherlands \u2014 do have informed-consent policies for screening.) A 2009 survey conducted in part by Genetic Alliance, a research and health-care advocacy group in Washington DC, found that 62% of new mothers in the United States were not given any information about newborn screening, were not given enough information, or did not remember whether they had been given any. Informed consent is central to Brase's campaign. This year, she worked with a Minnesota legislator to introduce a bill amendment that would change the state's entire screening programme \u2014 not just the research portion \u2014 from an opt-out model to an opt-in one. It also required the destruction of blood spots and test results within hours of testing. Opponents said that the policy would result in the deaths of children and the shutdown of labs statewide \u2014 federal law requires that test results from the samples be kept for two years for quality control \u2014 and the amendment was shelved. But Brase had another weapon. The state's Supreme Court is now considering a case on the same issues, filed by nine families brought together by the Citizens' Council for Health Freedom (CCHF) in St Paul, an advocacy group that Brase leads. If the group loses, it could appeal. If at any point it wins, it could set a precedent for public-health officials and researchers across the country and around the world. Brase became a privacy activist in the 1990s, during attempts at health-care reform by the Bill Clinton administration. She believed that government-imposed decisions on health care could affect people at their most vulnerable times. \"I was a nurse, so I understood that patients often cannot protect themselves,\" she says. \"I just said, 'I have to do something.'\" Brase took a break from nursing in 1995 to found the CCHF, and never went back. Her attention turned to newborn screening when she was poring through an annual state appropriations bill one day in 2003. \"I remember on page 80, I got to this language that said essentially that the health department would have the discretion to test every child for whatever conditions it wanted without having to come back to the legislature. And that was when I realized that newborn screening was not just newborn screening \u2014 it was genetic testing.\" Brase became the bane of the state health department. She lobbied for better education of parents and got it: in the mid-2000s, Minnesota added a note to its newborn-screening brochure saying that parents could opt out of screening or storage, provided that they did so in writing. But that wasn't good enough for Brase. \"They didn't say that there was an official form or tell people where they could find it,\" she says. So she began a second campaign, this time arguing that the newborn screening programme violated a genetic-privacy bill passed by the state in 2006. She won that too, although perhaps not in the way she wanted: in 2008, the legislature voted to exempt newborn screening from the bill. Brase's efforts had tied up the capitol for weeks. She does not shoot for the subtle. Brase frequently name-checks the 1997 dystopian science-fiction film  Gattaca , in which genetically 'inferior' people form a social underclass, and when she testified to the state House of Representatives in 2009, she placed two books in front of her: one about the US eugenics movement, the other about the Holocaust. She podcasts. She tweets. And every time she flies, she takes a stack of wallet-sized cards to hand out at the gate. \"Protect your baby:\" they read, \"Reclaim their DNA!\" Brase's rhetoric may be overblown, but in Texas, many share her concerns. Two years ago, an investigative journalist discovered that Texas had been shipping blood-spot cards to the US military, which was trying to build a national mitochondrial-DNA database for forensic identification. The state had also been bartering with private companies, trading blood spots for lab equipment. Worse, it had been trying to keep the initiatives under wraps: in an e-mail obtained by the  Texas Tribune , one researcher argued against informing the public, saying that a press release would \"only generate negative publicity\". After a series of expos\u00e9s and a lawsuit, Texas had to incinerate 5.3 million cards. The fallout continues: in May, the state legislature voted to change the research portion of its newborn screening programme from opt-out to opt-in. The Minnesota Department of Health has never been accused of a Texas-style cover-up. And although many people within the department consider Brase something of an enemy of the state, they rarely speak publicly about her. But Edward Ehlinger, Minnesota's commissioner of health, says that she has, in one sense, been helpful. \"We think data privacy is incredibly important, and we also think individuals should know how information is going to be used. What Twila has done is to make sure we have those conversations,\" he says. Still, he adds, \"as with any conversation, you do need to come to a place where you can move on\". More than a few of Brase's critics say that she could have an effect similar to that of Andrew Wakefield, the disgraced British doctor whose fraudulent research led millions of parents to believe in a link between vaccines and autism. Brase could cause parents to shun disease screening. She says that according to her figures, more Minnesota parents have declined newborn screening each year since 2003 \u2014 the parents of 156 children refused it last year. The more children go unscreened, the more likely it is that some with debilitating or fatal diseases will go untreated, says Mendelsohn. \"With some of these disorders, if they're not caught quickly, the kids lose IQ points by the week.\"  \n                Research paralysis \n              Selling parents on the long-term benefits of research can be difficult. Many of the projects listed on the Minnesota Department of Health's website have not been written up and published, despite years of apparent work. The controversy has stymied others. Piero Rinaldo, a researcher into biochemical genetics at the Mayo Clinic in Rochester, Minnesota, had hoped to conduct pilot studies on several rare disorders to add to the state panel. A validated test could help doctors to treat the diseases earlier. Rinaldo says that he has taken all the required ethical precautions, but with the state \"in paralysis\", he has not been able to begin. \"We're doing this because we want to save more lives,\" he says. \"But [Brase] acts as if it's all an excuse for the government to build some inventory of imperfect children. It's like she has the idea that all science is bad.\" Brase herself says almost as much. \"I have a less glorified sense of research than some people do,\" she says. \"It seems like there's no final answer to some of the questions.\" She points to studies overturning previous recommendations on hormone-replacement therapy. For years, apparently well-founded advice urged women to take hormones, only to be overturned when later, better studies showed that they could be harmful. To ensure that screening programmes aren't affected by attacks on the research, some experts propose separating the consent processes \u2014 making screening opt-out and research opt-in, with more comprehensive information given to parents. \"It seems to me you have to,\" says James Evans, a medical geneticist at the University of North Carolina in Chapel Hill, who has advised the government on bioethical issues. \"If as many people don't participate in research, that is unfortunate. But when people don't participate in newborn screening, babies die.\" Michigan's public-health department, which has made a point of openly discussing newborn screening with the public, has settled on this approach. Another proposal that has gained traction among bioethicists is to educate parents earlier. \"It's very clear that this shouldn't be done in the peripartum period,\" says Ellen Clayton, a bioethicist at Vanderbilt University in Nashville, Tennessee. The United Kingdom already has such a policy; its prenatal educational programmes begin with a leaflet given to parents in the third trimester of pregnancy. For that matter, Minnesota already provides information on newborn screening to obstetricians. But neither change would fully appease Brase, because the government would still hold children's blood samples, and thus potentially their genetic information. \"Yes, the government has to follow the law, but nobody's in there watching. So the best way to make sure it never happens is to simply not get screened,\" she says. Confronted with the point that children with the targeted conditions could die if their parents opt out, she says that parents could have their infants tested privately instead. The right to make that choice should be theirs, she adds. The debate is likely to become more ferocious \u2014 and more complicated \u2014 as genome sequencing begins to enter medical practice. Population-level sequence data would be a gold mine for researchers. But many parents who have no problem with limited newborn screening might well feel uncomfortable having their children's entire genome sequences on file, no matter how strict the privacy protections. Research that makes use of blood spots is likely to increase. The National Institutes of Health has established the Newborn Screening Translational Research Network, a group intended to facilitate data sharing and encourage more scientific work. One of the many goals of the incipient network is to make it easier for scientists to access the millions of blood spots nationwide. The network's proponents recognize that it could engender controversy. In February, in the  American Journal of Public Health , several members of the advisory committee wrote that parents are too poorly informed, and that \"addressing concerns from stakeholders will be necessary for state-level adoption of national recommendations\". (E. W. Rothwell  et al. Am. J. Public Health    doi:10.2105/AJPH.2010.200485  2011). This is something that Brase can agree with. \"I really believe if you do not respect the rights of people, research is not going to be trusted in the future,\" she says. \"Researchers will be looked at as people who want to stamp on your rights to get their grants and their fellowships and their chairs to prop themselves up.\" There are more nuanced ways of putting it, but many of Brase's opponents concede the essence of her point. \"If scientists want to be able to do science, they have to convince the public that it's a good thing to do, that there are protections in place, and that the practices are transparent,\" says Clayton. \"Newborn screening cannot fail to do that.\" That will mean listening to objections, even if they come from the likes of Brase \u2014 otherwise, there might one day be many more like her. \n                 See Editorial  \n                 p.265 \n               Mary Carmichael is a freelance writer in Boston, Massachusetts. \n                     The Genome at Ten \n                   \n                     Vaccine Special \n                   \n                     Citizens' Council for Health Freedom \n                   \n                     Minnesota Department of Public Health information on newborn screening \n                   \n                     US Department of Health and Human Services statement on newborn screening (PDF) \n                   Reprints and Permissions"},
{"file_id": "475023a", "url": "https://www.nature.com/articles/475023a", "year": 2011, "authors": [{"name": "Amy Maxmen"}], "parsed_as_year": "2006_or_before", "body": "A vociferous debate about vitamin-D supplementation reveals the difficulty of distilling strong advice from weak evidence. With his skull-and-crossbones bow tie tied tight, Clifford Rosen strides to the podium at the Metropolitan Bone Club, a meeting of researchers and clinicians in New York City concerned with all things skeletal. He begins by bracing himself: \"If you want to ask a question or just yell at me, go ahead,\" he says. \"I'm used to a lot of antagonism, anger, and frustration.\" Rosen is director of clinical and translational research at Maine Medical Center Research Institute in Scarborough and is a respected member of the bone-research community. But his role last year on an expert panel to determine how much calcium and vitamin D people need put him at odds with many of his colleagues. In the past few years, vitamin D has earned a reputation in Western countries for preventing or fighting prostate cancer, cardiovascular disease, multiple sclerosis and about 30 other maladies, leading to advice that most people should be supplementing what the body produces naturally when exposed to sunlight. But in November, the panel, put together by the Institute of Medicine (IOM) \u2014 a non-profit group affiliated with the US National Academy of Sciences \u2014 issued a report 1  that challenged that view. Blood levels of vitamin D need not be as high as many physicians and testing companies had been advocating, it said, and high doses of the vitamin could actually cause harm. Since the report was released, Rosen says he's received about 150 e-mails critical of the panel's decisions. About one-third were downright hateful. \"A rehabilitation doctor in Texas threatened to bring me to the board of malpractice to have my licence revoked. People tell me I don't know what I'm doing,\" he says. \"It has become personal.\" \n               Click here for larger image \n               Much is at stake. By 2009, the amount spent on vitamin-D supplements in the United States had risen tenfold in ten years (see  'Raising the stakes' ). Medical practitioners and public-health officials worldwide look to the IOM for guidance on how to interpret the conflicting claims about vitamin D. Yet several vitamin-D proponents say that the IOM's methods, which involved a systematic review of the literature, were flawed. They have accused the panel of misinterpreting data and over-emphasizing the danger of heavy supplementation. Just last month, the Endocrine Society, a professional association of 14,000 researchers and clinicians based in Chevy Chase, Maryland, released guidelines that recommend higher doses than the IOM did 2 . Why, instead of clearing confusion as was the IOM's goal, has the report sown division and unrest? \"The IOM was too definitive in its recommendations,\" says Michael Holick, an endocrinologist at Boston University School of Medicine in Massachusetts, and an outspoken critic of the IOM panel's conclusions. \"Basically, the vitamin-D recommendations are based on low-quality evidence,\" says Gordon Guyatt, a clinician researcher at McMaster University in Hamilton, Ontario, who has been a consultant on various guidelines. \"I think admitting that would have made some of the angst disappear.\" Poor data is one reason that the panel did not recommend higher doses, say interested observers. \"We are not free to just accept enthusiastic reports, unless they are based on comprehensive, well-characterized data sets,\" says Paul Coates, director of the Office of Dietary Supplements at the National Institutes of Health in Bethesda, Maryland, one of the agencies that had requested the IOM's evaluation. \"I think everyone wants to do the right thing, but I would say that the government is inherently more conservative.\" The episode demonstrates the difficulty of producing public-health advice from disparate and sometimes feeble evidence. The former panel members have been touring the United States and Europe to defend and explain their methods. That's what brought Rosen to the bone club. \"In the past 50 years of IOM reports, this one has received the most visibility \u2014 fortunately or unfortunately,\" he tells the audience.  \n                Systematic review \n              Vitamin D's role in promoting bone health through the regulation of calcium is fairly unassailable, but in the past several years, the medical and scientific communities have become preoccupied with how it might prevent chronic disease. Some physicians recommend supplementation of up to 6,000 international units (IU) a day to make up for the time that people spend indoors. This is less than the amount a fair-skinned person without sunblock might make in half an hour of exposure to the midday summer Sun. In August 2008, the US and Canadian governments asked the IOM for unbiased advice about how much vitamin D and calcium people need. Rosen and 13 colleagues who were selected to serve on the panel amassed about 1,000 studies on metabolism, vitamin intake and impact on human health. They then ranked the studies by the quality of design and execution. Randomized placebo-controlled studies earned the highest rating. Of roughly 70 such trials, most assessed the effect of vitamin D on falls, fractures and bone quality. About a dozen looked at cancer, cardiovascular disease and diabetes, but because of the way those outcomes were assessed, the panel didn't place much trust in the results. Most of the other research has been observational. A 2008 study, for example, reported that men with low levels of vitamin D were more likely to have heart attacks than were those with higher levels 3 , but it couldn't rule out other explanations for the link. The panel met 8 times over 20 months. Its efforts culminated in a 1,132-page report 1  concluding that people should aim for blood levels of 50 nanomoles per litre (nmol/L). This level, the IOM said, can be achieved with 600 IU of vitamin D per day (800 for those older than 70) \u2014 an amount that doesn't necessarily require supplementation, because many people would get this naturally from Sun exposure, fatty fish and fortified foods. The IOM also stated that reports of widespread deficiency have been exaggerated; the majority of North Americans already have enough vitamin D; and too much of it could be harmful. Passions ignited immediately. Physicians and alternative-medicine advocates posted websites and Facebook pages declaring the IOM guidelines flawed. Some claimed that its recommendations were an industry-motivated scheme to keep people in need of prescription drugs and other costly treatments, a theory that Rosen dismisses. And although conspiracy theories tend to be thin on logic or factual substance, scientists and clinicians have raised some legitimate concerns.  \n                Sticky stats \n              Michael Amling, a bone expert at the University Medical Center Hamburg-Eppendorf in Germany is one such critic. He says he was thrilled when Rosen e-mailed him in 2009 to enquire about some of his data. \"I wanted to support the work of the IOM,\" says Amling. He assumed its analysis would conclude that most people were vitamin-D deficient, and that this might encourage the German government, which does not fortify food, to reconsider the issue. The IOM was interested in a study Amling had published, in which he had measured bone quality and blood levels of vitamin D in the bodies of 675 people who had died in good health (for example, in car accidents and suicides) 4 . Amling concluded that an ideal level for the general population would be 75 nmol/L because everyone above that level had strong bones, and they therefore weren't at a high risk of fractures. \n               Click here for larger image \n               The IOM's mandate was to set the levels that protect most people, but not all. It found that Amling's data supported a 50 nmol/L threshold (which had been suggested elsewhere in the literature) because at that level, only 1% of people in the study had weak bones. But Amling says that the IOM made a mathematical mistake: it should have looked at the risk of weak bones in people at or above a certain level, not in the whole population (see  'Denominator dispute' ). Instead of dividing the 7 people with weak bones and levels above 50 nmol/L by all 675 people in the study, he says it should have divided 7 by the 82 individuals with levels above 50 nmol/L. Charles McCulloch, a biostatistician at the University of California, San Francisco, who has no vested interest in vitamin-D thresholds, agrees: the panel should have found that 8.5% of the population above 50 nmol/L had weak bones, and therefore according to its goal of allowing no more than 2.5% of the population to be at risk, Amling's data would support a higher level. \"I'm very shocked they made such a basic mathematical mistake,\" Amling says. Another researcher whose work received a fair share of the IOM's attention is Heike Bischoff-Ferrari, director of the centre for ageing and mobility at the University of Zurich in Switzerland. She published a meta-analysis in 2009 that pooled eight clinical trials testing the ability of vitamin-D supplements to reduce falling in elderly people 5 . In her analysis, participants who took daily doses of 700\u20131,000 IU fell less often than those taking a placebo. Doses below 700 IU made no difference. When the IOM panel came to analyse Bischoff-Ferrari's data, it decided to include different studies. It removed a study 6  showing a benefit from doses higher than 800 IU because the study had focused on groups of about 20 people, which the panel considered too small. And it added a trial 7  that Bischoff-Ferrari had excluded because it hadn't been double-blinded. Once the IOM swapped trials in Bischoff-Ferrari's meta-analysis, the evidence showed no benefit from supplementation. Needless to say, Bischoff-Ferrari and others disagree with the IOM's decision. With no universal criteria to identify which studies ought to be included in meta-analyses, it's hard to say which team selected the most appropriate ones. What is clear, however, is that a lack of high-quality primary research makes these decisions difficult and prone to bias. Another criticism levelled at the report has to do with the IOM's warning that too much vitamin D could cause harm. In the only clinical trial claiming risk, elderly women treated with a single 500,000-IU dose of vitamin D annually fell and fractured their bones more often than those in the placebo group 8 . Many researchers find the study ridiculous. \"No one absorbs 500,000 IU a day from the Sun, so why would you give that as a supplemental dose?\" says Edward Giovannucci, a nutritional epidemiologist at the Harvard School of Public Health in Boston, Massachusetts.  \n                The defence \n              These are just some of the criticisms that Rosen and other members of the former IOM committee have heard. In response to Amling's charge that the IOM made a mathematical mistake, Rosen maintains that the method the IOM used to calculate 1% risk is standard procedure for dietary recommendations. The group was asking about a natural population with wide variation in vitamin-D levels. He adds that other skeletal studies showed no benefit in increasing the threshold above 50 nmol/L. With regard to Bischoff-Ferrari's meta-analysis, Rosen stands by the IOM's decision to remove studies with few participants because they are sensitive to random errors; including them, he adds, can exaggerate an erroneous finding. And JoAnn Manson, an epidemiologist at Harvard Medical School in Boston, dismisses the notion that the mega-dose trial ought to be ignored: \"Within the first three weeks of this trial, when serum levels were at or above 100 nmol/L, there was an increased risk of falls and fractures.\" This trial contributed to the IOM's anxiety about doses that might raise blood levels to this amount. Specifically, the IOM set an upper dosage limit of 4,000 IU \u2014 a number it arrived at by taking calculations from various studies. Such precautions are not unexpected, says Guyatt. Health officials put a high value on avoiding recommendations that could prove dangerous over time, he says. The Endocrine Society's guidelines, which were based on four years of periodic review of the literature, call people with levels under 50 nmol/L \"vitamin-D deficient\", and those with levels between 50 nmol/L and 72.5 nmol/L \"insufficient\". Insufficiency versus deficiency is not a common distinction in guidelines, says Holick, but it reflects the opinion that people benefit from the higher threshold. The society's guidelines also offer an 'ideal' level of 100\u2013150 nmol/L for non-skeletal health benefits, which would require 1,500\u20132,000 IU daily, and it advises physicians to monitor vitamin-D levels in healthy people. Quest Diagnostics, a medical-testing corporation headquartered in Madison, New Jersey, that Holick advises, has already begun to implement these deficiency and insufficiency standards over the IOM's, and many physicians are expected to follow suit. The now-disbanded IOM panel has been formulating a response to clarify how physicians and the public should make sense of the discrepancies in recommendations. The panel's members acknowledge that the case for vitamin D benefiting general health is not closed, and say that the best way to clarify it is with large clinical trials. Manson recently began enrolment for a 5-year, 20,000-person trial to test the effect of supplements on cancer and cardiovascular disease. Reinhold Vieth, a vitamin-D researcher at the University of Toronto in Canada, calls this demand for huge trials \"a cop-out\". He says that there is good evidence that higher levels of vitamin D would reduce rates of multiple sclerosis, but a clinical trial to test this would require thousands of people and 30 years. \"Saying we need perfect, placebo-controlled trials is denying the plausible evidence we have,\" Veith says. \"At what point do you offer advice?\" Manson urges caution. \"We've seen promising correlations before that turned out to be wrong when tested in clinical trials.\" She offers the example of \u03b2-carotene, which showed promise as a cancer preventative in observational studies but proved dangerous in high-dose trials. \"Why are all these lessons of history no longer relevant when it comes to vitamin D?\" she asks. Perhaps IOM panel members underestimated the passion present in the vitamin-D field. Physicians who recommend high doses of vitamin D might not want to believe that the evidence they have trusted isn't quite up to par. \"One thing I wasn't aware of before, is the tremendous pressure from industry and investigators who are tied to their religious belief in vitamin D,\" says Rosen. Guyatt says that much of the current fracas could have been avoided if the IOM panel had been a bit more equivocal in its reporting. But Rosen doesn't regret having been dogmatic in the recommendations. Still, he says that he would have liked to take more time to explain the IOM's methods up front rather than just presenting the bottom line. He suspects that unbiased, systematic reviews such as this one will increasingly come under fire when they lead to hard recommendations. \"This is the beginning of a whole new phase,\" he says. \"In the old days of medicine we believed experts, and now we say, show us the data.\" Amy Maxmen is a freelance writer in New York City. \n                     Vitamin D gets a dosing down \n                   \n                     IOM report on Vitamin-D intake \n                   \n                     The Endocrine Society \n                   Reprints and Permissions"},
{"file_id": "475020a", "url": "https://www.nature.com/articles/475020a", "year": 2011, "authors": [{"name": "Eugenie  Samuel Reich"}], "parsed_as_year": "2006_or_before", "body": "A legal case about public access to documents is raising questions about the US Department of Energy's scrutiny of alleged scientific misconduct. On 6 April, a federal district judge in Boston, Massachusetts, dismissed a lawsuit that I had filed in 2009 under the US Freedom of Information Act. He concluded that the US government does not have to release a report on an investigation into a case of alleged scientific misconduct at a national laboratory. The ruling was disappointing but liberating: I finally had occasion to write about a case that has shown how the US Department of Energy (DOE) takes a strikingly hands-off approach to the oversight of such investigations. The lawsuit concerns a report on an investigation into vehemently denied allegations of data fabrication by scientists who receive millions of dollars per year from the DOE's Office of Science. I have asked the judge to reconsider his decision, so the case remains pending. But the larger issue of lax oversight is highlighted in documents filed in court by the government, including nine sworn declarations from DOE and lab officials. These documents show that the DOE officials overseeing the investigation merely skim-read an incomplete version of the resulting report, did not read the complete final report, did not keep a copy of that final report anywhere in their files, and voluntarily gave away a version that was sent to them. I have also learned, through interviews, that scientists who tried to contact the DOE to express concerns about the investigation did not get a hearing. The court filings suggest that agency officials did not think that they were acting improperly; indeed, they justify their actions by citing the US federal policy on research misconduct, which since 2000 has governed how allegations should be handled. But the DOE's approach contrasts with much stricter interpretations of the same policy at some other US government science-funding agencies, which typically handle such allegations in an independent office staffed with professional investigators. The DOE is unusual in allowing the allegations to be handled by the officials who awarded the grants in the first place, who will potentially be reluctant to see a problem in that research. Independent observers worry that the DOE's loose oversight could undermine the credibility of research funded through its Office of Science, which supports 10 national laboratories and around 27,000 scientists.  \n                Corrections and refutations \n              The allegations that were under investigation first emerged in 2006, when an anonymous peer reviewer accused a research group at Oak Ridge National Laboratory (ORNL) in Tennessee of fabricating data in two manuscripts: a then-current submission to  Nature Physics , which has not been published, and a paper that had been published in 1993 ( Nature   366 , 143-146; 1993). The ORNL group is headed by Stephen Pennycook, an electron microscopist, and the earlier work had been a landmark paper in atomic-scale imaging. Pennycook, a pioneer of such techniques, has been pushing the limits of spatial resolution in electron microscopy to solve problems in a variety of research areas, including materials sciences, nanotechnology and condensed-matter physics. Pennycook told me in an e-mail that he preferred not to comment for this article, \"as I think there is nothing to discuss\". He does address the allegations on the group's website ( http://go.nature.com/gtyqpm ), where he denies that his group fabricated or falsified data \u2014 although he admits that the researchers made errors of judgement in their data presentation. A panel of investigators appointed by ORNL managers upheld the group's conduct. In a summary statement posted on the ORNL website in 2008 ( http://go.nature.com/ilywxc ), the three scientists on the panel said that there was evidence that Pennycook's team had made careless factual errors and errors of judgement in data presentation, but there was no evidence for misconduct or fraud. In an interview with me, David Williams, one of the investigators and now dean of engineering at Ohio State University in Columbus, went further, slamming whoever made the anonymous allegations. \"To go around claiming fraud is a witch hunt of zealots,\" he said. \"It doesn't serve the cause of science.\" Following the investigation, the Pennycook group published a correction to its 1993 paper. Nonetheless, some scientists outside ORNL were uncomfortable with the investigation, and particularly with the managers' failure to release the resulting report. \"It's taken place in the dark,\" says John Silcox, an electron microscopist at Cornell University in Ithaca, New York, who was a reviewer of the 1993 paper. It was also unclear how officials at the DOE, which funds Pennycook's group to the tune of around US$2 million a year, had overseen the investigation. This lack of transparency at both ORNL and the DOE was a leading motivation for my freedom-of-information request to see the report. Although the report has not been released, declarations filed in court in response to my lawsuit have shed light on the oversight of the investigation. In 2006, the declarations say, James Roberto, then ORNL deputy director for science and technology, sent a report on the lab's investigation to Patricia Dehmer, then the associate director of basic energy sciences in the DOE's Office of Science, and now the office's deputy director. In her own declaration, Dehmer says that she understood that this report was only a draft, because it said as much on the first page, and because it didn't contain referenced appendices \u2014 although she does not specify what she would have expected such appendices to consist of. Dehmer says that she looked through the document only briefly, to check that it reflected what Roberto had told her about the case during conversations and telephone discussions. In an e-mail, Dehmer told me that by November 2006, ORNL and the Office of Science considered that the investigation was complete. But some time after that, ORNL reopened the case, because managers there had received extra material, the nature of which they do not describe in the court documents. As a result, ORNL produced an extended final report; the lab sent this revised document to the DOE by overnight post. This time, however, Dehmer didn't read the report at all. Her declaration to the court says, \"a copy of briefing materials in a binder was sent to me. I do not recall the contents of this binder, nor do I recall reading or studying these materials \u2026 due to the other pressing responsibilities during this time frame.\" Later, she and several members of DOE staff went to a meeting with Roberto, where they questioned him about the case. Dehmer approved the investigation, she says, on the basis of the \"thorough\" responses that she received from Roberto. In declarations to the court, five DOE employees who attended the meeting say that they did not read the final report. At the end of the meeting, Dehmer gave the document to Roberto to take back to ORNL; nothing was left in the government's possession except the incomplete e-mailed copy that Dehmer had skim-read. This procedure raises substantial concerns among external experts. Christine Boesz, a former inspector-general overseeing research integrity at the National Science Foundation (NSF), says that it is highly problematic for the government not to keep possession of a final report about a case of alleged misconduct. She points out that government agencies need to keep records, in case of a change of personnel. A failure to fully document cases could make it impossible for the agency to police its research effectively, because it wouldn't be aware of patterns or alleged patterns involving contractors. \"Even a pattern of allegations can tell you a lot,\" says Boesz. \"The written document is an integral part of the process. If this is the way they typically handle it, then it raises big questions in my mind. Is this looking after the government interest?\" Sybil Francis helped to draft the federal policy on research misconduct while working at the White House Office of Science and Technology Policy in 2000. She says that, according to the spirit, intent and letter of that policy, the government should have access to the documents it needs to conduct oversight. \"I don't see how a federal agency can carry out its oversight responsibilities in a case of alleged or proven research misconduct without the proper documentation at its disposal,\" she says. Francis adds that at the very least, she would expect an adjudicating official to get a member of her staff to read the investigation report carefully, and not simply rely on oral presentations. C. K. Gunsalus, a law professor who studies research misconduct at the University of Illinois at Urbana\u2013Champaign, is perplexed by DOE officials' failure to read the report. \"I think it's breathtaking, I'm truly astonished,\" she says. She contrasts the DOE's practices with those of two other government agencies: the NSF and the Office of Research Integrity at the Department of Health and Human Services, which funds research at the National Institutes of Health. \"The submission of an investigation report to the Office of Research Integrity or the inspector-general of the NSF means without question that the report will be examined with care,\" she says. She notes that it is not uncommon for an investigation committee's first draft to flinch from difficult conclusions, to make statements without support or to overlook something. But at these other federal agencies, oversight is iterative, with officials encouraging a thorough investigation by examining the evidence for themselves and asking investigators at the institution for further documentation. If they don't read the report, they can't do that. Peter Stockton, a senior investigator at the watchdog group Project on Government Oversight in Washington DC, has studied management of weapons labs by the DOE. He says that a general problem at the labs and at DOE headquarters is that officials do not want to probe difficult situations that might result in political fallout and budget cuts. \"The most important thing is to limit damage to the organization,\" he says. He comments that the Office of Science has a large enough budget, at $4.9 billion a year, for the DOE to consider launching an office of research integrity to oversee it. Dehmer would not comment on these criticisms. But in her declaration filed in court, she says that federal policy allows government agencies to delegate the job of investigating to research institutions, because to do otherwise \"would have involved a substantial new federal bureaucracy, which is not thought desirable\" \u2014 a quote from the federal misconduct policy. The current director of the Office of Science, William Brinkman, also did not respond to my request for comment, and a DOE spokeswoman notes that \"the department does not comment on specific questions relating to litigation matters\". Raymond Orbach, director of the DOE Office of Science at the time of the investigation, says that he does not recall any details of the case. In general, he says, he reads investigation reports when acting in oversight of a case. Orbach notes that ethics are usually handled by the general counsel's office at the DOE, and are a very serious issue for any government agency. But he disputes the value of an office of research integrity at the DOE, saying that he is concerned about the possibility of oversight boards probing into mistakes that scientists have made. \"The community has a well-worn method for policing itself,\" he says: checking whether results are reproducible.  \n                External interest \n              While the DOE was preparing to close the case, two electron microscopists at Cornell, David Muller and Silcox, were undertaking an independent analysis of the Pennycook group's 2006 correction. In 2006, the two researchers submitted a technical comment for publication in  Nature , claiming to show that Pennycook's correction is inaccurate, and that the central claim of the 1993 work \u2014 that the team had successfully used an electron microscope to image the interface between two materials with atomic resolution \u2014 is not supported. Neither Muller nor Silcox made the original allegations against Pennycook's group, although Muller says that he gave advice to the scientist who did.  Nature   accepted the technical comment for publication, pending a reply by Pennycook. A draft of that reply says that the technical comment contains \"a mix of criticism with little scientific basis, false statements, irrelevancies, and suggestions of scientific misconduct\", and that the group has been cleared. Although Muller's comment (J. Silcox and D. A. Muller Preprint at  http://arxiv.org/abs/1106.4534  ; 2011) was accepted, it was never published in  Nature . Karl Ziemelis,  Nature  's chief physical sciences editor, told me that he cannot comment on unpublished submissions, but notes that, in general, technical comments are a forum for scientific discussion, not for airing allegations of misconduct. The DOE was aware of Muller and Silcox's analysis; in 2007, Dehmer referred to it in an e-mail to me. But the agency has never asked to see it, and when Muller and Silcox tried tentatively to raise their concerns with the relevant officials, they had a frosty reception. Muller says that his initial approach was to contact ORNL to express his concerns and offer full details to any independent committee, but none was appointed. Barbara Penland, a spokeswoman for ORNL, says that any extra concerns that the lab received about the case were referred to an internal inquiry that did not find any issues that warranted further investigation, and that the entire ORNL response was assessed by the science and technology committee of the board of governors of UT-Battelle, the contractor that runs the lab. \"It concluded that the investigation was handled properly, free of conflict of interest and consistent with applicable federal guidelines,\" she says. Muller and Silcox did consider contacting the DOE directly. Cornell's vice-provost for research, physicist Robert Richardson, called Orbach to try to open a line of communication, but was unsuccessful. Orbach, who is now director of the Energy Institute at the University of Texas, Austin, says that he does not recall specifics of any phone calls. Silcox says that he tried to contact Harriet Kung, current associate director of basic energy sciences at the Office of Science, who was involved in approving the investigation according to the government filings, but that she would not discuss the case with him. Kung did not respond to a request for comment. For Muller, the lesson of this episode is clear. \"The DOE needs an office of research integrity,\" he says. Not only would that allow for independent oversight, but it would also protect complainants who risk losing their own funding if their concerns are not welcomed by grant officers. Since he submitted his comment to  Nature , Muller has himself received DOE funding, and he admits that the fear of losing that money makes it harder to bring his concerns forward. As for my lawsuit, the DOE's handling of the investigation report seems to have protected the document from disclosure, at least for now. The judge found that the report is not a government record releasable under the Freedom Of Information Act \u2014 primarily because government officials have never read it. I have brought a motion to reconsider, so the case remains open. But an unexpected question has now been raised: should taxpayers be allowed to read about how alleged misconduct at the US national labs has been investigated, even if government officials don't? \n                 \n                     See Editorial  \n                   \n               Eugenie Samuel Reich is a contributing correspondent for Nature. In Reich v. the US Department of Energy 1:09-cv-10883-NMG in the US District Court in Boston, Massachusetts, she is represented by David B. Smallman and Michael A. Pezza Jr. \n                     Department of Energy policy on research misconduct (PDF) \n                   \n                     ONRL's discussion of the allegations \n                   \n                     National Science Foundation's Office of Inspector-General \n                   \n                     Department of Health & Human Services Office of Research Integrity \n                   \n                     John Silcox and David Muller's Technical Comment \n                   Reprints and Permissions"},
{"file_id": "475283a", "url": "https://www.nature.com/articles/475283a", "year": 2011, "authors": [{"name": "Ewen Callaway"}], "parsed_as_year": "2006_or_before", "body": "Anti-doping researchers are looking for new ways to catch cheaters. Can a biological passport help to save the sport? Cyclist Borut Bo\u017ei\u010d drew his hands to his chest with a look of joy, disbelief and exhaustion after defeating some of the world's best sprinters in the Swiss village of Tobel. His stage victory at the week-long Tour de Suisse last month netted the 30-year-old Slovenian a \u20ac4,000 (US$5,600) bonus and probably helped to secure his spot in this month's Tour de France, cycling's most prestigious race.  His stage win also automatically earned Bo\u017ei\u010d a trip to a cramped medical trailer. Inside, he and three other riders each filled two small jars with urine. The containers were sealed, anonymized and sent to the Swiss Laboratory for Doping Analyses in Lausanne, where technicians would test them for traces of steroids, stimulants and a potent blood-boosting drug called erythropoietin (EPO).  Such tests have become as much a part of professional cycle racing as carbon-fibre bicycles, but decades of doping scandals show that they are no guarantee of a drug-free race. It is tough to name a Tour de France win in recent years that has gone unmarred by doping accusations. Last year's winner, Alberto Contador, tested positive for the banned drug clenbuterol. He has successfully argued that it came from contaminated meat, but an arbitration hearing could still erase his victory. And last year, it was revealed that seven-time Tour de France winner Lance Armstrong has been the focus of a US Justice Department investigation into doping \u2014 although he has never been disciplined and maintains that he never doped. Confronted with increasingly sophisticated dopers, anti-doping scientists face a daunting game of catch-up. \"This is an endless whirl,\" says Martial Saugy, the director of the Lausanne laboratory. In hopes of slowing the whirl, Saugy's team has pioneered a new kind of anti-doping test: the biological passport. Instead of scouring an athlete's urine for traces of drugs or their breakdown products \u2014 as the Lausanne lab would do for Bo\u017ei\u010d's sample \u2014 the passport builds up a profile of an individual over time and tries to detect biochemical changes that might indicate doping. Since 2008, Saugy's laboratory and the International Cycling Union (UCI), cycling's international federation based in Aigle, Switzerland, have created biological passports for hundreds of professional cyclists, some containing data from dozens of blood draws. Other sports are looking to follow suit. Some researchers say that the passport offers the best line of defence against EPO use, which has bedevilled inspectors for the past two decades; and biological passports to detect steroid and growth-factor doping are in the works. The technology may see its Olympic debut at the games in London next year. Still, critics \u2014 and some athletes \u2014 say that it is no match for determined dopers.  \"The biological passport is a joke,\" said Floyd Landis, a former US pro cyclist, to sport-news website ESPN.com in May 2010. After losing a costly four-year battle to overturn his conviction for using steroids during the 2006 Tour de France, Landis admitted to doping for much of his career and said that pro cyclists knew how to defeat the biological passport before it was introduced. But the passport has already led to convictions, and \u2014 perhaps briefly \u2014 shifted the advantage back to the testers. \"I think we are forcing people to decrease their doping habits,\" says Saugy.  \n                An endless cycle \n              Anti-doping efforts started in earnest after the 1960 Olympic Games in Rome. During a team time trial, 23-year-old Danish cyclist Knud Enemark Jensen collapsed, fractured his skull and died. An autopsy reportedly found traces of amphetamine and a blood-vessel dilator in his system. Although the drugs might not have caused his death, the episode forced cycling officials to take a closer look at doping. The UCI banned some performance enhancers, and in 1967 the International Olympic Committee established a commission to ferret out doping in sport. The task is thankless: anti-doping agencies thwart one cheating strategy, only for another to emerge. The 1972 Olympic Games in Munich, Germany, ushered in testing for stimulants, but athletes had started to take anabolic steroids. A test for steroids arrived at the next summer Olympics, in Montreal, Canada. But four years later, at the Moscow Olympiad, athletes had moved on to undetectable, naturally occurring hormones, such as testosterone. Anti-doping authorities now measure the ratio of testosterone in the blood to a related molecule called epitestosterone. In response, some athletes have reportedly found ways of regulating epitestosterone to keep the ratio in check.  For cycling and other endurance sports, human recombinant EPO fuelled a doping revolution. EPO is a natural hormone that promotes production of oxygen-carrying red blood cells. The first synthetic, or recombinant, version was developed by the biotechnology company Amgen in Thousand Oaks, California, and in 1989 it was approved by the US Food and Drug Administration to treat anaemia. It also offered cyclists an easy endurance boost that helped them to excel in gruelling stage races. The drug is nearly identical to the hormone naturally churned out by the kidneys, so was impossible to detect. It is also easier to administer than blood transfusions, which had been used to the same effect. \"In the 1990s and 2000s, it was quite easy for the cheaters to use huge amounts of EPO,\" says Saugy. Don Catlin, a pharmacologist who used to run an anti-doping laboratory at the University of California, Los Angeles, has a grimmer view. \"Every\u00adone in cycling was doping,\" he says. Without a test for EPO, cycling regulators turned to an indirect measure\u00adment called the haematocrit \u2014 the percentage of blood volume made up of red blood cells. Typically, red blood cells account for 40\u201345% of the blood, but in the heyday of EPO doping, some riders were showing up at starting lines with haematocrits of more than 60%. Their blood was so viscous that they would collapse before races, says Neil Robinson, who led the development of the passport at the Lausanne laboratory. The UCI instituted a 'no-start' rule, disqualifying riders if their haematocrits on the morning of a race were above 50% for men and 47% for women. So cyclists began diluting their EPO-boosted blood with saline solution to keep their haematocrits below the threshold, says Robinson.  The drug companies that produce EPO have helped anti-doping laboratories to develop direct tests based on subtle biochemical differences between the recombinant molecules and the natural form. The first of these was approved for use in 2000. But athletes increasingly obtain knock-off forms produced in China and India, and researchers have struggled to keep up, says Robinson: \"The solution is the passport.\"  The passport started taking shape in 1999, when Robinson and Saugy began clinical studies of EPO doping in volunteers. \"We immediately realized that there were major differences between subjects,\" says Robinson. For example, in one volunteer, levels of immature red blood cells called reticulocytes might rocket up in response to the hormone, whereas in another, they might barely rise. The researchers realized that instead of comparing such blood metrics against a wide range of values based on the general population, it made more sense to use an athlete as his or her own control and look for unusual fluctuations. \n               boxed-text \n             Today, the passport is an electronic record of several different characteristics of red blood cells \u2014 haematocrit, the concentration of the blood protein haemoglobin, the percentage of reticulocytes and other metrics \u2014 collected periodically in and out of competition for an individual athlete (see 'Good blood, bad blood'). A statistical model that accounts for factors such as an athlete's sex or the altitude at which a sample was collected (thinner air boosts red-blood-cell production) estimates the probability that a rider's profile is anomalous. \"The model will not tell you whether they've doped or not \u2014 it tells you the degree of abnormality,\" says Robinson.  \n                Model of honesty \n              A panel of anti-doping experts reviews profiles identified as suspicious and determines which cases merit a full-blown investigation. Although it is generally used to target riders for direct testing, cyclists have been successfully prosecuted on the basis of their biological passports alone. And in March, the Court of Arbitration for Sport, an international supreme court of sport based in Lausanne, upheld two of these prosecutions, further legitimizing the approach. More cases may be on the way. A report leaked to the French sports newspaper  L'\u00c9quipe   revealed a list compiled by the UCI, rating last year's Tour de France riders on a scale of 0 to 10 on the basis of their biological passports. From a total of 198\u00a0riders, 42 were rated at 6 or higher, meaning that they showed \"overwhelming\" evidence of doping, according to  L'\u00c9quipe . Although it isn't proof of doping, the list may be used in deciding which riders to scrutinize in the future.  \"There are still chinks in the armour,\" says Catlin. A team led by Michael Ashenden, an anti-doping researcher who heads the Science and Industry Against Blood Doping consortium in Gold Coast, Australia, simulated EPO 'microdosing' in ten volunteers 1 . They received small intravenous injections twice weekly for 12 weeks. The treatment boosted the subjects' haemoglobin mass by 10%, equal to two bags of transfused blood, but the biological passport didn't flag a single profile as suspect.  In another study 2 , Carsten Lundby, a cardiac physiologist at the University of Zurich in Switzerland, and his team subjected three groups of volunteers to different EPO regimens for ten weeks. A testing approach similar to the biological passport caught only 58% of the doped volunteers. \"I'm happy I'm not working in anti-doping, because it must be frustrating,\" says Lundby. Some researchers say that the statistical model underpinning the passport might produce an unacceptably high number of false positives \u2014 clean riders who look dirty on a test. Clifford Spiegelman, a statistician at Texas A&M University in College Station, complains that the model wrongly assumes that biological variations follow what statisticians call a normal distribution. Normal distributions resemble bell-shaped curves, with few outliers. The problem, says Speigelman, is that biological measurements are chock full of outliers \u2014 far more than would be predicted by a normal distribution. Proponents of the passports are \"presenting themselves as more accurate than they really are\", he says, and he estimates that the false-positive rate of the passport could be off by a factor of 10 or even 100. Pierre-Edouard Sottas, a Lausanne-based scientist with the World Anti-Doping Agency who developed the statistical model underpinning the passports, says that tests on thousands of clean athletes show that the blood characteristics used do follow a normal distribution. Moreover, he notes that a panel of experts, not his statistical model, makes the final decisions about an abnormal profile.  \n                No sign of the finish line \n              Robinson acknowledges that the passport cannot catch everyone, but it could deter dopers. The UCI points to a study 3  from its scientists indicating that the incidence of blood metrics that suggest doping has declined since the introduction of the passport. Anti-doping scientists think that they can improve the tests using tactics such as monitoring sudden spikes in performance, which could indicate something other than intensive training. Robinson and his team want to incorporate information garnered through police investigations of telephone and customs records into the biological passport's predictive model, so that suspicious behaviour and blood chemistry could both be used to flag a rider for closer follow-up. \"We have to use the same approach as a crime scene,\" says Robinson. His team is also developing versions of the passport to detect steroid and growth-hormone abuse by charting changes in the urine or blood levels of compounds such as testosterone and insulin-like growth factor-1. These researchers and others are also looking to improve the biological passport by searching for new molecular indicators of blood doping. For example, according to unpublished research by the Lausanne lab, circulating levels of a microRNA called miR-144, which is involved in regulating red-blood-cell production, spike after volunteers take EPO. Yorck Olaf Schumacher, an anti-doping scientist at the University of Freiburg in Germany, says that his lab has found changes in gene expression in response to transfusions of a patient's own blood, which can't be detected using conventional markers. Robinson says that it will be several years before these new markers make their way into the biological passports. \"We need to validate all these approaches, and that gets tricky.\"  As the three-week, 3,400-kilometre trek of the Tour de France nears its finish on 24 July on the Champs-\u00c9lys\u00e9es in Paris, Bo\u017ei\u010d has yet to duplicate his Tour de Suisse stage victory. But his biological passport has gained another data point. Before setting off on this year's Tour de France, Bo\u017ei\u010d and the other 197 riders gave blood samples for their passports, says Robinson, whose team plans to use these data anonymously to estimate the prevalence of blood doping in this year's race.  The team hopes that the passport will keep more riders honest. But after running an anti-doping laboratory for a quarter of a century, Catlin is convinced that tests, no matter how sophisticated, will never keep up with the most determined dopers. \"For every move to the right, the other guys are moving to the left and it balances out again.\" Ewen Callaway writes for Nature from London. \n                     Drug testing: One size doesn\u2019t fit all \n                   \n                     Union Cycliste Internationale \n                   \n                     World Anti-doping Agency \n                   \n                     Tour de France \n                   \n                     Swiss Laboratory for Doping Analyses \n                   Reprints and Permissions"},
{"file_id": "475159a", "url": "https://www.nature.com/articles/475159a", "year": 2011, "authors": [{"name": "Fredric Heeren"}], "parsed_as_year": "2006_or_before", "body": "The sauropods were the biggest creatures ever to walk the planet. But the keys to their success emerged in their tiny ancestors. From tail to snout, they stretched as long as four London double-decker buses parked end-to-end. The largest grew from 10-kilogram hatchlings to 100,000-kilogram adults. Their legs alone weighed several tonnes. No land creatures before or since have ever attained the size of the sauropod dinosaurs.  Those four-legged titans of the Jurassic and Cretaceous periods, 200 million\u201365 million years ago, had a suite of specializations that enabled them to reach such immense proportions. With long necks, wide-opening jaws and rake-like teeth,  Diplodocus ,  Brachiosaurus   and their ilk swept their heads through the treetops, consuming vast amounts of foliage without expending a lot of energy moving their massive legs. Adaptations of the pelvis and limbs created a frame sturdy enough to support their heft, and hollowed-out vertebrae and relatively small heads lightened the load. Their specialized bone development made it possible for juvenile sauropods to grow quickly, putting on several tonnes per year.  Palaeontologists have long thought that these anatomical novelties arose with the large sauropods \u2014 that a burst of evolutionary specializations coincided with the explosion in size. But a slew of discoveries in recent years reveals that many important changes first showed up long before, among the relatively puny forerunners of sauropods known as the early sauropodomorphs. Paul Barrett, a palaeontologist at the Natural History Museum in London, calls this group \"the unsung members of the dino community\". Walking upright on two legs, the early sauropodomorphs looked nothing like the lumbering beasts that came to dominate later. But these small creatures and their descendants gradually acquired adaptations that changed how they ate, moved and breathed \u2014 in ways that would later enable sauropods to achieve their size (see  'How to build a giant' ).  \"It is not that sauropods have these characters because they were gigantic,\" says Diego Pol, a palaeontologist at the Egidio Feruglio Palaeontological Museum in Trelew, Argentina. \"Instead, they achieved their gigantic size because they evolved from small-bodied ancestors that already had these features.\"  \n                Stage 1: starting small \n              The discoveries have not come easily. Many of the relevant fossils were found in remote sites in the Southern Hemisphere, including Argentina and South Africa. In 2006, palaeontologist Ricardo Martinez discovered a promising set of bones in the desert of northwestern Argentina. They emerged from rock that dated to the late Triassic Period, about 230 million years ago \u2014 a time when the first dinosaurs were starting to appear. He hauled the prize back to the Natural Sciences Museum in San Juan, then spent months freeing a lower jaw from the surrounding rock. Martinez found that the teeth had coarse serrations along their edges, an adaptation for cutting through fibrous plant material. Other early dinosaurs had fine serrations, more suitable for slicing through flesh. This told Martinez that he had found a tiny predecessor of the great sauropods, one with a relatively big skull like its carnivorous ancestors, but teeth more like those of an omnivore.  In 2009, Martinez and Oscar Alcober, also at the San Juan museum, described 1  the partial skeleton as the earliest and most primitive sauropodomorph yet found. Moving on two legs, the 1.6-metre-long animal had a body the size of a turkey, and a long tail. It weighed only 7\u20138 kilograms. Martinez called it  Panphagia protos , meaning 'first eater of everything', to celebrate its step on the road from carnivory to herbivory.  Barrett lists \"the greater reliance on vegetation rather than animal food\" as one of the factors that \"kick-started the increase in body size\". The advantage of herbivory is in the logistics of gathering food. Giant sauropods would never have been able to find and catch enough prey to fill their daily nutritional quota \u2014 which might have neared a tonne's worth for the largest.  Traditional grazing could not have done the job either, says Martin Sander, a palaeontologist at the University of Bonn in Germany. Instead of continuously shifting locations, burning through energy as they hoisted their colossal legs, sauropods swung their heads back and forth, mowing efficiently through the foliage.  That kind of feeding required long necks, which would have been impossibly heavy if they were built with solid vertebrae. But large sauropods had vertebrae riddled with holes. These air-filled, or pneumatic, bones weighed only about 35% as much as solid ones, which helped the sauropods to carry necks up to 15\u00a0metres long, says Mathew Wedel, a palaeontologist at the Western University of Health Sciences in Pomona, California. Hollow areas within the pneumatic bones may have connected to air sacs in the body cavity that helped to blow air through the lungs and improved the breathing efficiency of the giants \u2014 features seen in modern birds. Without the extra volume provided by such air sacs, it would have been impossible for the sauropods to clear the stale air that filled their necks after each breath; their lungs were simply too small to do the job alone. Pneumatic vertebrae would seem to be an adaptation related to giant size. But Wedel has found potential precursors in a small, early sauropodomorph named  Pantydraco . Its neck vertebrae have pits that match the positions of the holes in the sauropod vertebrae 2 .  So how could the precursors of air sacs and pneumatic bones aid tiny dinosaurs? Researchers suspect that they increased the efficiency of oxygen exchange, possibly helping the ancestors of dinosaurs to out-compete their contemporaries during the late Permian and early Triassic periods (260\u00a0million\u2013240 million years ago), when atmospheric oxygen concentrations were much lower than they are today 3 .  \n                Stage 2: Adding tonnes per year \n              The earliest sauropodomorphs were small, fast, and mostly moved on two legs. They could rely on speed to evade predators. But the next stage in evolution took the creatures a step up in size, to between 2 and 10 metres long.  The oldest known fossils of these 'core prosauropods' date from the start of the Jurassic period, about 200 million years ago. These creatures had longer necks and torsos, with larger bodies and relatively shorter legs than their predecessors. That made prosauropods less nimble, but their size helped to keep them safe.  That defence took its most extreme form with the later sauropods. \"Adult sauropods presumably were almost immune from predation because of their body mass being an order of magnitude greater than that of the largest predators,\" says Sander. \"Their sheer volume made it difficult for an attacker to place an effective bite.\"  If sauropods grew slowly like most reptiles, each one could have taken more than one hundred years to reach full size. But that would have left the smaller juveniles vulnerable for decades. Instead, evidence is emerging that these dinosaurs grew much faster than modern reptiles.  The key innovation was fibrolamellar bone, which develops in two stages, says Sander. \"A scaffold of bone is thrown up very quickly, making the bone grow in thickness by about one tenth of a millimetre per day, which then is filled in more gradually.\" Over the past decade, Sander and other researchers have analysed the structure of fossilized bone and documented the presence of fibrolamellar bone in sauropods. He estimates that the animals could grown by a few tonnes per year. But the origins of this trait appeared long before the giant sauropods. In 2005, Sander and one of his graduate students, Nicole Klein, reported 4  signs of fibrolamellar bone in  Plateosaurus , a core prosauropod that lived during the late Triassic and reached only about 10 metres long. By studying bones from more than 40\u00a0 Plateosaurus   individuals in Germany, Klein and Sander found that some reached full size in as little as 12\u00a0years.  Growth that fast is more characteristic of a warm-blooded animal than a cold-blooded one, and some dinosaurs might have had elevated body temperatures. Robert Eagle, a geochemist at the California Institute of Technology in Pasadena, and his colleagues reported 5  last month that two giant sauropods,  Brachiosaurus   and  Camarasaurus , had body temperatures 5\u201312\u00a0\u00b0C higher than those of modern alligators.    Plateosaurus   and other prosauropods showed further anatomical developments that later helped their descendants to achieve massive size. For example, they had a beefed-up sacrum \u2014 the structural link between the backbone and rear legs. Early sauropodomorphs had two sacral vertebrae, but prosauropods had three, which would have given more support 6 .  That and other developments helped to fuel an evolutionary jump during the late Triassic, from 7-kilogram early sauropodomorphs such as  Panphagia   to the 4,000-kilogram  Plateosaurus . \"The dramatic size increase observed along the first 25\u00a0million years of sauropodomorph history was the fastest one in the history of life,\" says Martin Ezcurra, a palaeontologist at the Bernardino Rivadavia Argentinian Museum of Natural Sciences in Buenos Aires.  \n                Stage 3: edge of greatness \n              Some of the most recent fossil discoveries fall into a third chapter of sauropodomorph evolution: creatures that could be called near-sauropods. Adam Yates, a palaeontologist at the University of Witwatersrand in Johannesburg, came to South Africa hoping to find fossils from this stage that could reveal how sauropodomorphs became quadrupedal.  He and a student hit the jackpot on a hill called Spion Kop. \"Bone was piled upon bone,\" says Yates. \"Given that we were finding so much of the skeleton, including parts of the small, fragile skull, we knew this was a major find.\" Last year, Yates and his colleagues named the new species  Aardonyx celestae 7 . From the lower jaw of  Aardonyx , Yates could tell that it did not have fleshy cheeks that limited how far the jaw could gape open. Instead of taking small bites and chewing as its older relatives did,  Aardonyx   could have opened its jaws wider and grabbed big mouthfuls, gulping them down whole.  That adaptation enabled the development of extremely long necks among sauropods, because it did away with the need for big jaw muscles and massive heads. \"The long neck was only possible because they did not chew,\" says Sander.   Aardonyx   was bipedal, but it had acquired some leg features that show up in quadrupedal sauropods, with their lumbering gait. Matthew Bonnan, a palaeobiologist at Western Illinois University in Macomb and a co-author on the  Aardonyx   paper, says that the creature's thigh bones were longer than the bones of its lower legs, unlike earlier sauropodomorphs, in which these bones were about the same size. \"This alone suggests animals that were built not for speed but for support,\" says Bonnan.  The forelimbs of  Aardonyx   also showed quadruped-like adaptations. In true sauropods, the two long bones of the forearm interlocked in a way that made the front limbs sturdier.  Aardonyx   showed an earlier stage of this interlocking forearm, connected to a hand that could grasp. Before the discovery of  Aardonyx   and a related species called  Melanorosaurus , Bonnan had hypothesized that such locking would produce an evolutionary chain reaction that also altered the hands in ways more suited to walking. He suggested that these adaptations would come in an \"integrated functional suite\" of shifting bones, which he expected to see first in a full-blown, quadrupedal sauropod. This hypothesis, he says, was \"smashed\" by the features of the bipedal  Aardonyx .  This year, Pol described 8  another near-sauropod that demolished expectations. The early Jurassic dinosaur,  Leonerasaurus taquetrensis , was just 2.5\u00a0metres long and walked on two legs. But it had four sacral vertebrae. Just last year, Yates had written 7  that four sacrals were diagnostic of the four-footed posture. Pol also found that  Leonerasaurus   had spoon-shaped, forward-leaning front teeth for raking in vegetation \u2014 much like later sauropods. This 2.5-metre animal with many sauropod traits is helping to build a new picture of sauropod evolution that, Pol says, \"has turned upside down the previous ideas\". Researchers note that  Leonerasaurus   and other known sauropodomorphs were not the ancestors of sauropods. Because the fossil record is so spotty, it is usually impossible to identify direct ancestors. But the prosauropods and near-sauropods of the Jurassic preserve information about adaptations that appeared among the unknown ancestors of sauropods.  \n                Stage 4: on all fours \n              Many late Triassic and early Jurassic sauropodomorphs could walk on two legs or four, as needed. But in 2008, Ronan Allain, a palaeontologist at the National Museum of Natural History in Paris, and Najat Aquesbi, a palaeontologist at Mohammed V University in Rabat, described an animal from the later part of the early Jurassic that seemed committed to four 9 .  \"  Tazoudasaurus   could be considered the oldest known 'true sauropod',\" says Allain. Unlike its forebears, which had long fingers that could grasp, this 9-metre-long animal had a stubby hand suited to bearing weight. Allain placed  Tazoudasaurus   into a new group of sauropods that he named Gravisauria, or 'heavy lizards'.  Heaviness is relative, and the most massive sauropods did not arise for another 90 million years. By the Cretaceous, fossils hint that some sauropods reached lengths of 40\u00a0metres and approached body masses of 100 tonnes. Yet in comparison with the changes that occurred during the early stages of sauropodomorph evolution, these later developments were relatively minor tweaks to a body plan that had emerged earlier. The sauropod story shows the importance of pre-adaptations \u2014 traits that are neutral or serve some purpose but later become co-opted to fill a new function. Such traits constrain the future evolutionary pathways of a lineage, but with hindsight they can seem fortuitous for something that researchers consider an important attribute, such as gigantism. \"The evolution of sauropods does look like kind of a crapshoot in which everything fell into place,\" says Wedel. \"Sauropods seem to have somehow gotten the evolutionary Wonka ticket of all the features that they needed to grow big.\" Fredric Heeren is a freelance writer in Olathe, Kansas. \n                     Sauropod Vertebra Picture of the Week \n                   \n                     Adam Yates \n                   \n                     Biology of the Sauropod Dinosaurs project \n                   Reprints and Permissions"},
{"file_id": "475440a", "url": "https://www.nature.com/articles/475440a", "year": 2011, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "Joe Bast and his libertarian think tank are a major force among climate sceptics \u2014 but they just can't win the battle over science. Joe Bast considers himself an environmentalist. He and his wife Diane used to volunteer for the Sierra Club, one of the United States' largest environmental groups. The two built a geodesic dome in the Wisconsin countryside and once entertained the idea of abandoning the city to live off the land. But today Bast is on the other side of a cultural divide, fighting former colleagues, politicians and scientists on the battlefield of global warming. As head of the Heartland Institute, a policy think tank with a free-market focus based in Chicago, Illinois, Bast has raised millions of dollars to mount a systematic attack on mainstream climate science. The organization provides fodder for politicians and conservative commentators bent on preventing government regulation of carbon emissions, and Heartland's climate conferences have become rallies for sceptics. The Heartland Institute, and Bast's personal story, offer a window on the political upheaval taking place in the United States and how it is helping global-warming sceptics to win over some sectors of the public. The shift to the right that enabled conservative Republicans to take over the House of Representatives has killed prospects for comprehensive climate policy in the United States any time soon. Now President Barack Obama's administration is under pressure to scale back a host of regulations on climate, air quality and public health. All of this contributed to a buoyant mood at Heartland's sixth International Climate Change Conference in Washington DC, which ran from 30 June to 1 July. Speakers presented a litany of accusations against mainstream climate scientists and the Intergovernmental Panel on Climate Change (IPCC). They questioned the validity of many elements of climate science, including the modern temperature record, palaeoclimate reconstructions and simulations of future conditions. Since they began in 2008, the conferences have become a prime networking opportunity for those who oppose political action relating to global warming. \"For the first time you had this rag-tag army of global-warming sceptics come together to meet each other,\" says Marc Morano, a former spokesman for Capitol Hill's most renowned sceptic, Senator James Inhofe (Republican, Oklahoma). At the conference, Bast was upbeat and claimed confidently that publications such as  Nature   and institutions such as the IPCC represent an increasingly entrenched minority. But he nonetheless hesitates when asked about the future. Sitting down for a quiet moment during the conference \u2014 because of stress-related health problems, he no longer manages the entire affair \u2014 Bast temporarily lets his guard down and acknowledges his movement's failure to win over mainstream climate scientists. More than ever, they continue to warn about the dangers of human-driven global warming and they retain credibility in the public eye. And Bast points out that the government is still writing cheques that will keep the current research system running. \"We've won the public opinion debate, and we've won the political debate as well,\" Bast says. \"But the scientific debate is a source of enormous frustration.\"  \n                Party hearty \n              The 2010 election showed the surging appeal of Tea Party activism and its platform of limiting government spending and power, but Bast found his way to a libertarian philosophy long before that. After attending the University of Chicago, he co-founded the Heartland Institute in 1984 at the age of 26. Heartland's annual report says that corporations provided 34% of its US$6.1-million budget in 2010, with the rest coming from individuals and conservative foundations \u2014 some of which have industry ties of their own. In the past, Heartland has often been criticized for collecting money from tobacco and energy companies, but Bast says Heartland is advocating its own ideology, which generally opposes regulation. He is among the last public defenders of smoking and has argued that concerns about second-hand smoke are as bogus as those surrounding greenhouse gases. Bast's assault on climate research takes two forms: challenging the credibility of the science, and disputing the claim that there is a scientific consensus on climate change. He does not necessarily deny that humans are having an influence on the climate, but he does question the forecasts of catastrophic impacts and the rationale for curbing carbon emissions. Heartland plans to spend $1.8 million on its climate programme this year. Of that, $413,000 will go to supporting the Nongovernmental International Panel on Climate Change (NIPCC), a small group of sceptics who have set themselves up as a counterweight to the IPCC. Made up of Bast and a few dozen colleagues, the NIPCC mines the scientific literature for nuggets of contrary evidence and doubt \u2014 often the kind of uncertainties that scientists readily acknowledge in their publications. The NIPCC also ignores mountains of evidence about the adverse effects of global warming and instead strings together a confident story that makes rising carbon dioxide concentrations seem entirely beneficial. The group published its first report,  Climate Change Reconsidered , in 2009, following the release of an executive summary the previous year. Running to more than 800 pages, the report spans the full range of climate science, with a narrative summary of the peer-reviewed literature followed by a detailed list of references. The group plans to release a preliminary draft of its second report next month, and then a final version in 2012 as a preemptive strike against the IPCC, which will begin rolling out its fifth assessment the following year. Jay Gulledge, senior scientist at the Pew Center on Global Climate Change in Washington DC, says that Bast, Heartland and the NIPCC all approach scientific data as attorneys, simply trying to sow doubt and justify political inaction. Other climate scientists agree. Gavin Schmidt of NASA's Goddard Institute for Space Studies in New York says the NIPCC homes in on scientific findings it likes and then blows them out of proportion. The last NIPCC report, for example, highlighted a 1999 NASA study 1  that proposed how a negative feedback mechanism over the tropics keeps sea-surface temperatures from exceeding 30\u00b0 C. The NIPCC extrapolated to say: \"If confirmed, this could totally compensate for the warming influence of all anthropogenic CO 2  emissions experienced to date as well as all those that are anticipated to occur in the future.\" The author of the study, Yogesh Sud of NASA's Goddard Space Flight Center in Greenbelt, Maryland, says that the NIPCC \"totally misintepreted my paper\". Those tactics are not limited to the NIPCC report. Heartland is still circulating a 2007 pamphlet claiming strong disagreement among climate scientists regarding the strength of the scientific case for global warming. The pamphlet cites data from a 2003 online survey 2  by climate scientist Hans von Storch and sociologist Dennis Bray, both affiliated with the Helmholtz Centre for Materials and Coastal Research in Geesthacht, Germany. In the survey, nearly 56% of climate scientists agreed that human activity is causing climate change, 14% were unsure and 30% disagreed. \"The survey clearly shows that the debate over why the climate is changing is still underway, with nearly half of the climate scientists disagreeing with what is often claimed to be the 'consensus view',\" the pamphlet states. \"We printed 500,000 copies and sent one to every important person in the United States,\" Bast says, including policy-makers, teachers and business leaders. But Bast dismisses the findings of a follow-up survey by Bray and von Storch 3 , which found that more than 85% of the responding scientists agreed that human activity is behind climate change. When the NIPCC released its last report, most scientists paid no attention or rejected it entirely. Michael Mann, a palaeoclimate researcher at Pennsylvania State University in University Park, says the report \"is nothing but a mix of myths, half-truths, cherry-picked distortions, and regurgitated climate-change-denial talking points.\" Environmental-policy specialist Roger Pielke Jr at the University of Colorado in Boulder calls it \"a big fat bowl of cherries\" selectively picked to support the idea that global warming is not a problem. And although Pielke sees the NIPCC as largely irrelevant, he argues that the IPCC has opened the door to such counter-efforts because its most recent assessment did not reflect the extent of the ongoing debates and uncertainties. By failing to be as comprehensive as it could have been, the IPCC \"ceded that territory to its critics\", he says.  \n                Waiting for the fall \n              Bast happily acknowledges hand-picking data to support his position, but argues that scientists on the other side do the same thing when they are building a case for global warming. He also says it is only natural that a libertarian like him would decide to question the scientific foundation for climate change. Getting serious about global warming means implementing government regulation, going after industry, raising taxes, interfering in markets \u2014 all anathema to a conservative agenda. \"The left has no reason to look under the hood of global warming,\" he says. \"The right does, and that's what happened.\" William O'Keefe, chief executive of the conservative George C. Marshall Institute in Arlington, Virginia, says that more than any other organization, Heartland has kept the focus on the weaknesses in science. But O'Keefe is careful when he talks about what is driving climate politics currently. \"I don't think anyone should run to the head of the line in saying they brought the climate legislation to a halt,\" he says. \"I think it was the economy, and when the economy does finally get on a sustainable growth path there will be a willingness to go back and revisit that discussion.\" Bast seems to believe that the foundation supporting climate science is collapsing, but with a little prodding, he will talk about his fears, too. Harking back, Bast says he racked up frequent-flier miles throughout the 1990s fending off large government subsidies for sports stadiums all around the country. By the time he made the rounds once, the whole debate would start again. In the end, the stadium projects went forward with taxpayers' money. \"They wore me down,\" says Bast, \"and the same thing may happen with climate change\". \n                 See Editorial  \n                 p.423 \n               Jeff Tollefson is a correspondent for Nature based in Washington DC. \n                     The Heartland Institute \n                   \n                     Heartland's climate conference \n                   Reprints and Permissions"},
{"file_id": "475280a", "url": "https://www.nature.com/articles/475280a", "year": 2011, "authors": [{"name": "Adam Mann"}], "parsed_as_year": "2006_or_before", "body": "A quarter of a century after the discovery of high-temperature superconductivity, there is still heated debate about how it works. \"Even bouncers in New York City nightclubs were aware of our notoriety,\" says Paul Grant, thinking back to the 1987 March meeting of the American Physical Society (APS). The hype had been building for months, as newspapers, magazines and morning television talk shows heralded jaw-dropping announcements from physics labs. A technological revolution seemed at hand, promising an era of levitating trains, coin-sized computers and power lines that could span continents without losing energy. When the meeting finally convened, says Grant, a physicist at the energy consulting firm W2AGZ Technologies in San Jose, California, anyone with an APS badge who arrived at a trendy club aptly named 'The Limelight' was ushered straight to the front of the queue. Yet the public's excitement was nothing compared with the eager frenzy of the physicists. On the evening of Wednesday 18 March, more than 1,800 APS attendees squeezed into a ballroom at the New York City Hilton (while another 2,000 milled outside) to watch a marathon set of presentations that lasted more than 7 hours. At the sometimes-raucous symposium \u2014 dubbed the 'Woodstock of physics' \u2014 researchers devoured the latest findings on what was easily the most astonishing discovery their field had seen in a generation: materials that became superconductors at high temperatures. 'High-temperature' was a relative term: even the best of the materials would not transition to become superconducting \u2014 having no resistance to an electric current \u2014 until it was chilled below 93 K (roughly 200 \u00b0C below room temperature). But that was nearly four times higher than the transition temperature of any previously observed superconducting material, and shattered what had once seemed to be a solid theoretical upper limit of 30 K. Everyone in the ballroom knew that, whatever was going on, it was something profoundly new. Better still, they knew that 93 K could be achieved easily with cheap, plentiful liquid nitrogen as a coolant, instead of the expensive, tricky-to-handle liquid helium required by the earlier superconductors. Suddenly, applications of superconductivity such as lossless power lines seemed economically feasible. And the room was alive with an even more electrifying idea: could there be materials that superconduct without any refrigeration at all? But 25 years after the publication of the first paper on high-temperature superconductivity 1 , such materials remain a dream. So do most of the miraculous-sounding applications. And so does a deep understanding of what is going on. Despite increasingly refined experimental techniques and nearly 200,000 published papers, physicists still do not have a complete theoretical explanation for high-temperature superconductivity. \"It's not that there's no theory; there are lots of theories \u2014 just none that most people agree on,\" says John Tranquada, a physicist at the Brookhaven National Laboratory in Upton, New York.  \n                Slow progress \n              Still, history offers some reassurance. Physicists took 50 years to understand conventional superconductivity \u2014 which was discovered 100 years ago in the laboratory of Heike Kamerlingh Onnes, at Leiden University in the Netherlands (see  'A century of superconductivity' ). On 8 April 1911, after testing for electrical resistance in a sample of mercury at 3 K, Onnes wrote down \"  Kwik nagenoeg nul   (Mercury practically zero)\", marking the first observation of a superconductor. A step towards an explanation of superconductivity came in the 1920s with the development of quantum mechanics, which provided an underlying model for the structure of ordinary metals. Metal atoms form a regular crystalline lattice and hang on to a tightly bound inner core of electrons. But their loosely attached outer electrons become unbound, collecting into a mobile 'electron sea'. Under the influence of an electric field, this ocean of free electrons will drift throughout the lattice, forming the basis of conductivity. In a normal metal, this motion isn't always predictable: no matter how cold it gets, random thermal fluctuations scatter the electrons, interrupting their forward motion and dissipating energy \u2014 thereby producing electrical resistance. But as some metals are cooled to temperatures close to absolute zero, the electrons suddenly shift into a highly ordered state and travel collectively without deviating from their path. Below a critical temperature that is unique to each of these metals, the electrical resistance falls to zero and any current flows practically forever. They become superconductors. But why does this ordered state form? In February 1957, three physicists \u2014 John Bardeen, Leon Cooper and Robert Schrieffer, all then at the University of Illinois in Urbana-Champaign \u2014 published the first complete answer 2 . According to their proposal, now known as BCS theory, an electron moving through a positively charged lattice of atomic nuclei leaves behind a small wake, like the deformation caused by a bowling ball rolling across a mattress. The distortion pulls in another electron, and the two become what is known as a Cooper pair. If many such pairs form, as happens at very low temperatures, their quantum-mechanical wavefunctions align, drawing the pairs into a collective state known as a condensate. Once there, they keep one another in check because breaking up one pair would raise the energies of all the others. The net result is that they all flow together without interruption, creating superconductivity. The theory was very successful, making many predictions that were quickly confirmed by experiment. But it also implied that the forces binding the Cooper pairs were very feeble, so they would be ripped apart by thermal vibrations at anything other than extremely low temperatures. \"Armies of researchers in the 1950s and '60s worked on improving the temperature range,\" says Jan Zaanen, a theoretical physicist at Leiden University. \"But they soon realized that they could not give rise to superconductivity above 25 K or 30 K\" \u2014 temperatures that generally require elaborate cooling systems for liquid helium, which boils at 4.2 K. This did not stop the use of superconducting wires and films in certain high-value applications such as medical magnetic resonance imaging (MRI) machines and particle colliders. But the expense seemed to rule out any wider application. Then, in June 1986, physicists Georg Bednorz and Alex M\u00fcller at the IBM Laboratory in Zurich, Switzerland, reported 1  that they had created a material that became superconducting at 35 K. The finding was dramatically confirmed in January 1987, when physicists in the United States found a material in the same class that became superconducting at 93 K (ref.  3 ). The Woodstock of physics followed barely two months later. One of the many astonishing aspects of Bednorz and M\u00fcller's work was that they were looking not at metals, but at insulating materials called copper oxides, which physicists would soon dub cuprates. In particular, they were investigating what happens when a cuprate is 'doped', or has foreign elements such as lanthanum or barium introduced into the parallel planes of copper and oxygen that comprise its structure. What they found was that the foreign atoms freed up the outer electron of some of the copper atoms, which then flowed through the lattice. If the cuprate was then cooled enough \u2014 to a temperature that depended on how it was doped \u2014 the electrons would flow freely, and the material would become superconducting. This strange state of affairs \u2014 superconductivity in an insulator \u2014 quickly led physicists to re-examine their basic ideas about condensed matter. But because some of the experiments were unknowingly done on impure samples, people were having trouble reproducing the results. \"The first years of the field were very confusing,\" says Patrick Lee, a physicist at the Massachusetts Institute of Technology in Cambridge. Hypotheses invoking bizarre and exotic physics cropped up, often without much evidence to back them up. The field soon broke up into competing camps, each advocating a different theory. Researchers would often ignore data that did not jibe with their pet theory, clinging almost religiously to their ideas, and attacking those who believed otherwise. Kathryn Moler, a physicist at Stanford University in California, recalls a colloquium in which a scientist in the audience stood up, pointed a finger at the speaker and shouted, \"Liar! Liar! Ladies and gentlemen, that man is a liar \u2014 don't listen to a word he's saying!\" Igor Mazin, a physicist at the Naval Research Laboratory in Washington DC, remembers a conference in 1989 when physicists promoting the different theories stood on stage \"yelling like schoolchildren\". Eventually, the cacophony sorted itself into the two theories with which most physicists now work. The first, resonating-valence-bond theory 4 , is largely the creation of Philip Anderson, a condensed-matter physicist at Princeton University in New Jersey. The theory states that the electron-pairing mechanism is imprinted in the cuprates' structure. Neighbouring copper atoms can become linked through chemical valence bonds, in which they share electrons with opposite spins. Typically, the bonding locks these spin pairs in place, preventing any current from being carried. But when the material is doped, the pairs become mobile and the valence bonds become Cooper pairs that condense into a superconductor. The second theory, called spin fluctuation 5 , has the most support in the community. Devised by Philippe Monthoux of the University of Edinburgh, UK, Alexander Balatsky from Los Alamos National Laboratory in New Mexico and David Pines from the University of Illinois\u2013Urbana Champaign, it posits that without doping, cuprates are locked into an ordered state called an antiferromagnet. That means that the outer electron on each copper atom lines up such that its spin is opposite to that of its neighbour: one electron will have its spin up, the next down, the next up, and so on. The magnetic fields produced by the spins lock the electrons in place. But in doped cuprates, the foreign atoms break up this rigid chequerboard pattern, giving the spins room to wobble. A passing electron can then set up a pulsating pattern of spins analogous to the lattice distortions of conventional superconductivity. This disturbance then draws moving electrons together, allowing them to associate into Cooper pairs and achieve a superconducting state. In the early days, says Tranquada, advocates of these two mechanisms were at loggerheads as much as anyone else in this field. But after a while, he says, \"it becomes easier to relax a little bit and try to start discussing where the points of agreement are and where the points of disagreement are. We can get beyond opinions and try to make some progress by agreeing on some experiments or calculations that may help.\" Most researchers now broadly agree on many aspects, such as the importance of magnetic interaction. Things have also calmed down a bit in the laboratory, as improved techniques have helped researchers to weed out the more exotic theories and refine those that remain. A good example is angle-resolved photoemission spectroscopy (ARPES), a method that uses high-energy photons to probe what electrons are doing. \"In 1993, the best we could do was four spectra in 12 hours,\" says Zhi-Xun Shen, a physicist at Stanford University who works with ARPES. \"One of vastly superior quality now takes 3 seconds.\" And in 2008, Hideo Hosono and his colleagues at the Tokyo Institute of Technology in Japan discovered a second class of high-temperature superconducting material \u2014 this time based on iron and arsenic \u2014 called pnictides 6 . These materials superconduct at lower temperatures than most cuprates \u2014 often only below 40 K\u2014 but they have given theorists a new arena for testing their ideas. \"It's almost like a do-over,\" says Thomas Maier, a physicist at Oak Ridge National Laboratory in Tennessee. Pnictides have a more complex structure than cuprates, but they might help to uncover which phenomena are central to high-temperature superconductivity, and which are simply due to the copper oxide structure. Moreover, finding the pnictides has reassured researchers that they might be able to find other high-temperature superconductors, providing more information or perhaps even a path to the elusive room-temperature superconductor. \"Once there's two, there's a high probability of there being more,\" says Andrew Millis, a physicist at Columbia University in New York. Researchers have made progress in practical applications. In the past five years, for example, they have managed to string cuprate materials into superconducting tape that can be used in power transmission cables or MRI machines cooled with liquid nitrogen.  \n                The root of the matter \n              No one is predicting a full understanding of high-temperature superconductivity any time soon \u2014 not least because such an account would have to make sense of the huge number of papers. \"A rich enough theory should explain everything and not just cherry pick,\" says David Pines, a physicist from the University of Illinois at Urbana-Champaign. But it's not always clear exactly what needs to be explained. Roughly 15 years ago, for example, researchers discovered that some high-temperature superconductors allow electron pairs to form above the transition temperature. In this 'pseudogap' regime, the material spontaneously organizes itself into stripes: linear regions that act like rivers and carry electron pairs through the insulating landscape where electrons remain stuck in place. \"It's a precursor state to the superconducting state and is therefore fundamental to understanding this problem,\" says Ali Yazdani, a physicist at Princeton University. Not so, says Pines, who thinks the pseudogap state \"interferes with superconductivity but is not responsible for it\". Much as physicists had to wait for highly developed quantum-mechanical tools to unlock the secret behind traditional superconductivity, researchers today may require future ideas to complete their task. If nothing else, the field's early quarrels have ensured that only the most determined researchers have stayed. Those remaining are perhaps humbled by their experiences. \"I think our biggest problem has been human fallibility,\" says Anderson. And perhaps these initial difficulties have helped to forge theories that can stand the test of time. \"In the end, it's your competitor that makes you strong,\" says Shen. Adam Mann is a freelance writer based in Oakland, California. \n                     Institute for Complex Adaptive Matter \n                   \n                     Shen laboratory \n                   \n                     Columbia University Nanoscale Science and Engineering Center \n                   \n                     Harvard Condensed Matter Theory Group \n                   \n                     Ali Yazdani's lab \n                   \n                     Phil Anderson \n                   \n                     Naval Research Laboratory's Materials Science and Technology Division \n                   \n                     Kathryn Moler's lab \n                   \n                     Brookhaven National Laboratory's neutron scattering group \n                   Reprints and Permissions"},
{"file_id": "475442a", "url": "https://www.nature.com/articles/475442a", "year": 2011, "authors": [{"name": "M. Mitchell Waldrop"}], "parsed_as_year": "2006_or_before", "body": "The closure of the Allen Telescope Array shifts the search for extraterrestrial intelligence away from big science. Out where the Hat Creek Valley twists among the ancient lava fields north of California's Lassen Peak, the only sounds are the wind and the lowing of distant cattle. The soft growl of antenna motors has long since fallen silent; all 42 radio dishes of the Allen Telescope Array stand motionless in the hot summer sun, staring blindly at the mountains on the southern horizon. The grass growing around their mounts \u2014 its neatness once a point of pride for observatory staff \u2014 is getting shaggy. The two caretakers still on site at the Hat Creek Radio Observatory don't have the resources to keep it trimmed. For nearly four years, these dishes listened for radio signals from an alien civilization. But since April, when the state's budget crisis forced the University of California, Berkeley, to suspend operations at the observatory, the world's largest instrument dedicated to the search for extraterrestrial intelligence (SETI) has been left in limbo. If the money cannot be found to reopen the array, the 6-metre antennas will have to be dismantled and removed. The melancholy vista at Hat Creek makes it easy to entertain equally melancholy thoughts about the SETI enterprise itself. It's the ultimate in high-risk, high-payoff science, pursued by only a handful of passionate researchers. In 50 years of searching, they have turned up nothing \u2014 and they can't quite shake an association in the public mind with flying-saucer sightings and Hollywood science fiction, all of which is so easy for cost-cutting politicians to ridicule that any substantial federal funding for SETI is impossible. Private support for the search is getting tighter because of the global recession. And many of the pioneers who have championed the search are now well into their 60s, 70s or 80s. Given all that, what kind of future can SETI have? Quite a vigorous one, insist the SETI researchers. By nature they are an optimistic lot, given to taking the long view. \"I'm a Pollyanna\" about raising the money to restart the Allen array, declares Jill Tarter, head of the search programme at the SETI Institute in Mountain View, California \u2014 the nonprofit research organization that conceived the array, operated it in partnership with astronomers at Berkeley and is now seeking the funds to resurrect it. Besides, the array is only one SETI initiative among many. \"If it closes I'll be sad for my colleagues at the SETI Institute,\" says Daniel Werthimer, a radio astronomer at Berkeley who runs several SETI surveys using data from the 305-metre radio dish at the Arecibo Observatory in Puerto Rico, \"but that's not going to affect us here.\" Or elsewhere: smaller, cheaper SETI searches are currently being conducted on telescopes around the world ([slideshow 1 center]). Some are seeking alien radio beacons, whereas others are looking for the flicker of interstellar communication lasers. Some projects are looking at specific stars that seem likely to host Earth-like planets; others are doing a less sensitive but broader scan of the entire sky in the hope of catching signals of a type not yet conceived. The reason to keep going is in that plethora of projects, says Frank Drake, an early pioneer of SETI studies and now an 81-year-old emeritus professor at the University of California, Santa Cruz. Our Galaxy is vast. The fraction searched so far is tiny. And the conceivable modes of alien communication are myriad. So there are always new possibilities to explore. Or, as Tarter likes to put it, giving up now would be like dipping a cup into the Pacific Ocean, finding nothing but clear water and declaring, 'the oceans have no fish'.  \n                Intelligence test \n              From this wider perspective, the closing of the Allen array would mark the end of a 'big science' approach to SETI, but not of SETI itself. And, as the array's creators ruefully admit, even that approach might have succeeded \u2014 if it were not for their own early miscalculations. Like every modern SETI effort, the Allen array follows the path blazed by Drake in 1960, when he was a staff astronomer at the National Radio Astronomy Observatory in Green Bank, West Virginia, and managed to wangle 200 hours of telescope time to mount the first search for alien signals. Focusing on just two of the closest Sun-like stars, Tau Ceti and Epsilon Eridani, his Project Ozma yielded nothing but static. But Drake's effort got some prestigious support from physicists Giuseppe Cocconi and Philip Morrison of Cornell University in Ithaca, New York, who had independently come to the same conclusion he had: that massive new radio telescopes like the ones at Green Bank could detect potential alien signals across interstellar distances (G. Cocconi and P. Morrison  Nature   184 , 844-846; 1959). Between Drake, Cocconi and Morrison, SETI gained instant scientific credibility, which began to draw other scientists into the field. \"I realized I was part of the first generation that didn't have to ask a priest the 'Are we alone?' question,\" says Tarter, who committed her career to SETI in 1971 at the age of 27, after reading NASA's first major report on the subject. For advocates, the obvious next step was a large-scale, federally funded effort to listen for aliens in as much of the Galaxy as technology allowed. This was a tough sell in Congress, says Drake. \"Their first question is, 'How long will it take?', and you can't tell them.\" Then they ask how much it will cost \u2014 and that depends on how long it takes. \"So you can't guarantee success,\" says Drake, \"and you're asking them for a blank cheque.\" But the SETI advocates persevered, and in October 1992 the first phase of NASA's US$12-million-per-year SETI search, dubbed the High Resolution Microwave Survey (HRMS), got under way at Arecibo, targeted at the 1,000 nearest Sun-like stars. A year later it was dead: Senator Richard Bryan (Democrat, Nevada) engineered a bill cancelling the HRMS as an utter waste of taxpayers' money. \"The Great Martian Chase may finally come to an end,\" he proclaimed. From then on, says Tarter, \"we became the 4-letter 'S-word' at NASA headquarters\". The SETI Institute moved quickly to pick up the pieces. It had been founded in 1984 by scientists from NASA's Ames Research Center in Moffett Field, California, and had been managing the first phase of the HRMS under contract to Ames. So it had no problem hiring many of the NASA employees who had been working on the programme, and arranging to use the custom-built signal analysers that NASA had already paid for. (These devices were designed to scan radio-telescope data for extremely narrow-band emissions, which were presumed to be the technological signature of an alien civilization; all known natural radio sources have a much broader bandwidth.) \"We sponsored a big series of workshops between 1997 and 1999 on what the future of SETI ought to be,\" says Drake, who served as the first chairman of the SETI Institute's board of trustees, and is still a board member. One of the prime recommendations was the construction of a large array of small, high-quality radio telescopes that could be dedicated full-time to SETI surveys. As in any radio array, the signals from all the antennas could be combined to look as though they came from a single antenna covering the same area \u2014 in this case, spanning 10,000 square metres, or one hectare. The One Hectare Telescope, as it was then known, became the SETI Institute's top priority. To help create it, the institute enlisted the telescope-building expertise of the Radio Astronomy Laboratory at Berkeley, where researchers were interested in using the array for conventional surveys of galactic and extragalactic radio sources in parallel with SETI. The partners agreed that the institute would raise the money to build the array, and that Berkeley would design it and pay for 20 years of operation at Hat Creek. And that, looking back, was when the miscalculations started. \"The Allen array was born in a time of irrational exuberance, and ended in the great recession,\" says Geoffrey Bower, a radio astronomer at Berkeley who was deeply involved in designing the array. \"Those two play a big role in how it was imagined, and now how it's coming to an end.\" A prime example of over-optimism was the antenna design. The original concept, proposed by Drake, was to use commercial satellite-television dishes as a way to keep the initial antenna cost to an absolute minimum. He had been inspired by a 3-metre dish that he had bought and assembled himself for $600, and that still stands behind his house in the hills near Santa Cruz. \"It has receivers on it as good as any radio telescope,\" he says. For the array, which was intended to have 350 antennas, Drake found a commercial manufacturer in Wisconsin that would sell him 4.2-metre dishes \u2014 good enough, he felt \u2014 for about $1,100 apiece. But that plan was quickly set aside as the Berkeley team came up with multiple innovations for optimizing performance. Less noise in the receivers. A wider frequency range. More-sensitive measurements of the radio waves' polarization. More sophisticated electronics. Tarter and her colleagues at the SETI Institute endorsed these design decisions. But the attempt to do so many new things inevitably led to overruns and delays. The result, despite other innovations to minimize the price tag, was a design for 6-metre antennas that would cost at least $200,000 apiece. \"If we had just reduced the technical complexity of the telescopes early in the project,\" says Bower, \"we could have built hundreds of them.\" They might have been able to do so anyway \u2014 except that the SETI Institute was beginning to realize that it might not have been such a good idea to start the project without getting all the money up front. \"We're scientists,\" sighs Tarter. \"What did we know?\" The first $25 million was easy enough to raise through the institute's contacts in the computer industry. Paul Allen, co-founder of the software powerhouse Microsoft, promised to contribute that much towards design and construction, and the facility was renamed the Allen Telescope Array. But by the time Allen's first instalment of funds arrived in 2001, the industry was reeling from the collapse of the dot-com boom, making further donations harder to come by. And those computer-industry philanthropists who were still giving, preferred to fund projects with a nearer-term, more tangible payoff. \"We don't have patients we can cure,\" says Thomas Pierson, the SETI Institute's chief executive, summing up the problem. In the end, the institute was able to raise only about $50 million in total, roughly $60 million shy of what it would need to pay for all 350 dishes. The array began operations in 2007 with just 42 antennas. This lack of collecting area greatly slowed down the alien-hunting. And worse, it left the array without the sensitivity required for cutting-edge radio astronomy \u2014 a fact cited by the National Science Foundation in 2008 when it declined to renew the grant that the university had been using to fund Hat Creek operations. Suddenly, Berkeley had no way to pay the array's $2.5-million annual operating costs. The partnership managed to limp along for a few more years. But its increasingly urgent efforts to find a permanent source of funding ran headlong into the worldwide economic downturn and California's resulting budget crisis. On 22 April 2011, Berkeley and the SETI Institute were forced to pull the plug on Hat Creek.  \n                The search continues \n              Berkeley doesn't hold out much hope of reopening the array, and has started to look at other radio-astronomy projects. \"We've poured everything we had into the array,\" says Bower, \"and really hit a wall with it.\" The SETI Institute, though, is still trying. In June, it launched an experiment in 'crowd-sourced' funding on a website called SETIstars.org, through which individual supporters can make donations of $10\u20131,000. The hope is to raise a few hundred thousand to a million dollars per year that way (the take so far is just over $100,000). The institute is also in ongoing talks with the US Air Force, which is interested in providing some operations money in return for part-time use of the array to track debris in orbit around Earth. But if the closure proves to be permanent, say institute officials, their plan B is to fall back on smaller-scale efforts \u2014 in effect, turning to the methods that the rest of the SETI community has followed all along. This approach tends to be very ad hoc and informal, says Werthimer. It's done with limited funding, borrowed telescope time and investigators who, like Werthimer himself, do SETI only part-time. \"And that's the way it should be,\" says Werthimer. \"It's naive to think that we know what ET, a billion years ahead of us, is going to be doing. So we want to be a small-scale science, trying lots of things.\" Data from the Arecibo telescope, for example, are sent to five receivers at once: three doing conventional radio surveys, and two, operated by Werthimer's group, doing SETI. At the Oak Ridge Observatory in Harvard, Massachusetts, Paul Horowitz \u2014 a Harvard University astronomer \u2014 and his students are searching for extraterrestrial laser pulses with a small optical telescope, built for the search with roughly $300,000 from nonprofit organizations. \"I see SETI as a terrific thing for a graduate student,\" says Horowitz. \"It's an unploughed field.\" Students can think up a plausible mode of extraterrestrial communication; design, build and test a detector; analyse the data; and get tangible, hands-on experience. Most students in conventional astronomy just get observation time on a big, institutional telescope. \"When I first heard what Dan was doing \u2014 wow! I couldn't imagine doing anything else,\" says Andrew Siemion, who is writing his PhD thesis on SETI instrumentation under Werthimer. Perhaps this is the lesson from the mothballed array at Hat Creek: the odds against success with SETI are so long that it is best done as a small-science, part-time pursuit. Or perhaps not: the SETI Institute may yet find a way around all the funding woes and political headwinds, and bring the Allen array back to life. Either way, SETI is not going to disappear. \"I went into SETI because it was just too cool not to,\" says Horowitz. And others will too, he says. \"All you need is a university environment, with a few tenured faculty willing to host something wacko. The question is always interesting, and there will always be people willing to take a long shot.\" M. Mitchell Waldrop is a features editor for Nature. \n                     SETI Institute \n                   \n                     SETI at the Planetary Society \n                   \n                     SETI at the University of California, Berkeley \n                   \n                     Optical SETI at Harvard University \n                   \n                     The cancellation of NASA's SETI programme \n                   Reprints and Permissions"},
{"file_id": "476022a", "url": "https://www.nature.com/articles/476022a", "year": 2011, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "Mu-ming Poo is nurturing a Shanghai neuroscience institute that offers a glimpse of his country's future as a bioscience superpower. Mu-ming Poo leads a double life. For three weeks every month, he works in a cramped, cluttered office at the University of California, Berkeley. Looking drab in his dark-green pullover, olive trousers and black Adidas sports shoes, the 62-year-old neuroscientist slumps slightly in his chair. In the adjoining laboratory, half a dozen postdoctoral researchers, expected to work independently, go quietly about their business. Cut to Shanghai, China, where Poo spends the remaining quarter of his time. In the director's office at the Institute of Neurosciences (ION), he sports a pressed, light-blue shirt neatly tucked into belted trousers (same trainers). With few books and papers about, the room seems more spacious than its Californian counterpart; mangoes and other fruit in a bowl provide a tasteful flourish. Here, Poo supervises only one postdoctoral researcher, but a dozen chattering graduate students are stuffed into an office, waiting for the hour that he sets aside for each one during his whirlwind visits. Poo sits straighter, talks faster and seems more alert, alive \u2014 younger, even. As stimulating as he finds his research in the United States, where he is a member of the National Academy of Sciences, Poo finds a sense of mission in China. \"It's more exciting, exhilarating here,\" he says. \"They need me. I feel it's the best use of my life.\" \n               Click here for larger image \n               China is alive with possibilities in science, but realizing them is a complicated affair. The country's fondness for speed \u2014 for short-term achievements and, increasingly, short-term profits \u2014 has worked relatively well in the chemical and physical sciences and in large-scale genomics, where researchers can systematically tick off the chemical compounds or genetic sequences that they have produced (see  'Eastern promise' ). But neuroscience and other fields of biology, which involve so many more unknowns, require more patience and a free-thinking academic culture that doesn't mesh well with some entrenched features of Chinese academia. Graduate students in China typically become cogs in their supervisor's machine, emulating his or her work. With quantity used as the yardstick for achievement, students churn out articles in minor journals. And principal-investigator positions are assigned by a bureaucratic tallying of young researchers' publications. The best scientists, therefore, are almost guaranteed to go to the United States or elsewhere to progress. But that is changing, thanks in no small part to Poo and the ION, a decade-old institute with 26 research groups, 350 members of staff and an outsize reputation. Poo is sometimes criticized as a micromanager and dictator (\"Maybe a benevolent dictator,\" he smiles), but ION students and group leaders revere him for his tireless mentoring, and for creating a US-style research culture geared towards top-level science. From his students, he demands critical thought and one first-author publication in a high-quality journal, rather than several in lesser ones. He evaluates faculty members and recruits on the basis of peer-reviewed publications and other achievements. The results are clear. ION researchers authored China's first neuroscience papers in  Cell ,  Science ,  Neuron ,  Nature Neuroscience   and  Nature Cell Biology   and, until 2005, the ION accounted for more than half of all top-level neuroscience publications from Chinese institutions, by Poo's count. The ION's \"primary accomplishment has been to put neuroscience on the map in China in a way that no other basic neuroscience-research institute in the country has done\", says Richard Morris, a neuroscientist at the University of Edinburgh, UK, and a member of the institute's international advisory board. Over the past two years, the government and the Chinese Academy of Sciences (CAS) in Beijing awarded the institute three major funding programmes that will allow it to double in size by 2020. And the institute is pumping out some of the world's sharpest, and most aggressive, young neuroscientists, who are seeding its culture elsewhere. \"It is exciting to see that high-quality neuroscience research has spread beyond the ION to other institutions throughout China,\" says Bai Lu, who helped Poo to draft the proposal for the ION and is now vice-president of biology at the pharmaceutical company GlaxoSmithKline (GSK) China in Shanghai. Even Westerners can see the tide turning. \"It is only a matter of time before students will remain in China for their postdoctoral training,\" says Thomas Insel, director of the US National Institute of Mental Health in Bethesda, Maryland. \"And perhaps in a few years, we will be seeing US students and postdocs applying for positions in China.\"  \n                The dilettante \n              Born on the mainland and raised in Taiwan, Poo earned a degree in physics at Taiwan's elite Tsinghua University before moving to the United States, and into biophysics. His early studies of the movement of cell-membrane proteins led to an interest in events at the synapse, the junction between two neurons \u2014 and then onto synapse formation and plasticity, the process by which the junctions change with neuronal activity. His conversion from physicist to biologist defined a dilettante approach that he has maintained to this day. \"I have a rather unconventional style,\" says Poo. \"I don't work on one problem until it's fully solved. I tend to skip around.\" Poo says what he thinks, an approach that he acknowledges has made him some enemies. And he is as direct when poking holes in the Chinese research system as when pinning down the next key experiment. He took that attitude to the ION, which opened in 1999. Although the institute was part of the crusty CAS, Poo's mandate was to transform how science is practised in China by operating with autonomy and a Western style. To replace the bureaucratic review of scientific work in China, for example, Poo brought in an independent review system that applied even to esteemed CAS members. \"That was totally unheard of,\" says Poo. One refused to be assessed, and left. In a review last year, two out of eight research groups at the ION got a 'conditional' pass, meaning that they need to publish within two years or risk dismissal. The institute spans the range of basic neuroscience, from sensory mechanisms to neurodegeneration, stem cells and genetics. Following the US system, ION students \u2014 20 or so graduate with PhDs each year \u2014 rotate through three different laboratories, evaluate their experiences and then choose a principal investigator as a thesis supervisor. Poo encourages students to critique articles in  Nature ,  Science   and  Cell , which are often considered unassailable in China. He arranges for renowned scientists to give lectures, and asks his students to grill them with challenging questions. The ION was also the first CAS institute to allow a departing investigator to take, with some compensation, equipment and reagents to their next employer. \"That offered the mobility that this country needs,\" says Poo. Meanwhile, Poo's research continues full steam at Berkeley, where he has worked since 2000. He studies how neural activity shapes neural circuits, and increasingly how these mechanisms correlate with wider behavioural patterns such as learning and addiction. In one project, Andrei Popescu, a postdoc in Poo's Berkeley lab, is using mice to improve understanding of a phenomenon seen in children in the 1990s. Researchers at Rutgers University in Newark, New Jersey, foundthat when children were trained to overcome a hearing problem using rewards for success such as entertaining computer animations, they improved not just in the specific hearing skills, but also in broader language abilities 1 , 2 . Popescu and Poo hope to understand that process of 'generalized' learning by mimicking it in mice. Poo's hypothesis is that learning a reward-associated task modifies the brain's dopamine-based reward circuit, which is also thought to have a role in drug addiction 3 , in a way that makes an individual a better learner for other tasks. As Poo suggests, the subjects become \"addicted to learning\". Poo's teaching philosophy mirrors his research hypothesis. He wants students to get hooked on research with the high of an early influential publication. \"Those who get frustrated and give up \u2014 it's because they didn't have enough self-discipline to make the first success,\" says Poo, who became hooked with his own paper in  Nature   in 1974 (ref.  4 ). Poo works hard to shore up that discipline. Besides the monthly supervisory sessions with each of his ION graduate students, he holds weekly lab meetings over Skype. He reads all manuscripts from the ION to ensure that they are up to scratch. With two labs on different continents, not to mention an entire institute to run, how does he fit it all in? \"I put in more time, more effort,\" he says. \"I take no weekends. I work like a dog. I'm here every night.\"  \n                The Tao of Poo \n              Poo's reputation took a public beating in 2002, when one of his Berkeley lab members posted online an e-mail in which Poo lamented the group's poor progress. \"If there is no drastic change in the lab, Poo lab will soon cease to be a productive, first-rate lab that you chose to join in the first place,\" he wrote, before imposing strict rules, including a minimum 50-hour, 6-day working week spent mostly at the bench (reading papers was for after-hours). Anyone who chose not to follow the rules should \"start making plans immediately and leave the lab\". One student did. Poo stands by his words. \"Young people, if they want to make it in sciences, must work hard,\" he says. Did the letter have an effect on his ability to get graduate students? Poo pauses. \"It had a screening effect.\" His last Berkeley PhD student graduated last year, and he hasn't actively recruited since. \"I don't have the time to mentor them,\" he says. The lab has shrunk from some 20 people in 2007 to just 6. Popescu says that images of Poo as a dictator are off-base. Two conversations with former lab members quickly reassured him before he joined. \"I thought it better to trust people who had been there than something posted on the Internet,\" he says. Poo's dozen Shanghai students, who come from nearly as many provinces around China, describe an intense but supportive and tolerant atmosphere. One boasts about starting work days at noon (although admits working six or seven days per week). Hailan Hu, a principal investigator at the ION, says that Poo's meticulous attention \u2014 he \"doesn't just fix the paper but also teaches grammar and presentation skills\" \u2014 is \"difficult for him but great for the students. You need to book him in advance\". Insel recalls a visit to the ION a couple of years ago that \"left quite a lasting impression\". Challenging questions from graduate students and the intense focus of the faculty members, he says, \"all left me with a very profound sense of respect for what Mu-ming Poo and his colleagues were building in Shanghai\".  \n                Fertile ground \n              In 2009, the CAS approved a ten-year plan to expand the ION to 50 laboratories. In the same year, it selected the centre as a 'trial institute' for stable funding and doubled the support per investigator from 600,000 renminbi (US$93,000) to 1.2 million renminbi. Stable funding now covers 40\u201350% of the ION budget, in comparison with 30% before. Research grants make up most of the rest. Poo scored an even bigger coup this year, when his 'neural basis of intelligence' project was selected to receive one of China's largest and most sought-after basic-science grants, as part of the science ministry's National Basic research Program. Only 11 projects were selected this year, and just 3 in biological sciences. With 80 million renminbi for the next five years and a promise that the project will continue for another ten, Poo is now building a national programme and recruiting a dozen research groups from ten institutions, in addition to the existing ION groups. Signs of China's impending boom in neuroscience are evident elsewhere. In 2008, East China Normal University in Shanghai launched an Institute of Cognitive Neuroscience, full of returnees from the United States, and started planning a large primate centre. This April, Patrick McGovern, a technology and media tycoon who bankrolled the McGovern Institute for Brain Research at the Massachusetts Institute of Technology in Cambridge, agreed to donate $10 million to form a sibling institute at Tsinghua University in Beijing. He is negotiating similar agreements with Beijing Normal University and Peking University. But the rapid progress in Chinese neuroscience is creating problems for the institute that is driving it. GSK's international neurodegenerative-disease research base \u2014 attracted to Shanghai partly because of the ION \u2014 has recruited at least seven graduate students from the institute, many of whom, Poo laments, left without finishing a doctorate. The ION has also lost senior staff to the National Institute of Biological Sciences in Beijing, and other universities. Poo blames the departures on better salaries elsewhere, although a critic \u2014 who does not want to be named \u2014 points to his 'overly controlling' managerial style. Either way, more than half of the researchers at the ION have been there for fewer than five years; and publications in leading journals have dropped. Poo created a fluid system, and the people flowed. \"But I don't worry,\" he says. \"It's a place to start your career. It's a nursery.\" Poo is still sceptical about the future of Chinese science. He worries that misconduct is still tolerated and that the country's work ethic is being eroded, with students demanding comfortable living arrangements, better food and vacations. And Poo pooh-poohs the government's readiness to give huge packages to scientists solely because they have worked overseas. \"That's no way to cultivate native talent,\" he says. Listen to Poo talk about China, and it sounds as if the country might not soon become a global force in biological sciences. But watch what he does, and the signs of the country's emergence in this field are clear. \"I have 40 years of experience in first-rate institutions,\" he says. \"If you want this to be a first-rate institution, follow me.\" And China is following. See  Editorial David Cyranoski is  Nature 's Asia-Pacific correspondent. \n                     China special \n                   \n                     Institute of Neuroscience \n                   \n                     Mu-ming Poo's lab at the University of California, Berkeley \n                   Reprints and Permissions"},
{"file_id": "476020a", "url": "https://www.nature.com/articles/476020a", "year": 2011, "authors": [{"name": "Gwyneth Dickey Zakaib"}], "parsed_as_year": "2006_or_before", "body": "From giant viruses to unexplained marine DNA, there may be more to the tree of life than the three domains currently defined. The e-mail arrived two weeks after Jonathan Eisen's paper was published. Tongue-in-cheek, it read: \"Welcome to the 'Fourth Domain' club.\" Eisen, an evolutionary biologist at the University of California, Davis, chuckled. His paper 1 , which came out in March, hinted at bizarre new forms of microscopic life in the ocean. The e-mail was from Didier Raoult, a biologist at the University of the Mediterranean in Marseilles, France, who for years has argued that the colossal viruses he has identified belong to a unique, hitherto unknown, branch of life: the fourth domain. Raoult believes that this domain branched off from the very base of life's evolutionary tree \u2014 a declaration that has been met with strong scepticism. Researchers disagree with the way he interprets sequence data. The debate highlights how hard it is to decipher the original relationships between microorganisms that have swapped, tinkered with and generally scrambled their DNA over billions of years. Whether or not the fourth domain exists, the hunt for evidence has turned up several surprises. Raoult's microbes blur the boundaries between viruses \u2014 generally considered non-living things because they cannot function on their own \u2014 and the cellular organisms that host them, reigniting a debate about what constitutes life. Raoult says that some viruses are sophisticated enough to warrant special consideration. Eisen's approach, which sifted DNA from thousands of seawater samples, picked out sequences from \u2014 well, nobody quite knows what. \"I would call it the dark matter of the biological universe,\" says Eisen of the countless unidentified DNA sequences lurking in the environment. \"There is potentially enormous diversity out there, and this is the way to look for it.\"  \n                Cellular division \n              Before the advent of DNA sequencing, biologists divided life into two domains based on cellular structures. Eukaryotes, which encompass everything from amoebas to trees to humans, have large cells with internal structures such as nuclei and mitochondria. Prokaryotes are smaller and for the most part lack these structures. But in 1977, Carl Woese, a microbiologist at the University of Illinois at Urbana-Champaign, turned taxonomy upside down when he used RNA sequences to show that prokaryotes comprise two very different groups: bacteria, which are distantly related to the eukaryotes, and archaea, which sit much closer to them on the evolutionary tree 2 . Woese faced considerable backlash from the scientific community. It wasn't until the mid-1980s that his theory was accepted, bolstered by mounting genetic and molecular evidence. The textbooks were duly rewritten to feature three domains: eukaryotes, bacteria and archaea. Raoult's claim for a fourth domain stems from his study of giant viruses. He and his colleagues identified the first of these 3 , Mimivirus, in 2003. The microbe had been found years before, infecting amoebas in the water of a cooling tower in Bradford, UK, but it was so big that it was mistaken for a bacterium. When Raoult and his colleagues couldn't identify it from its DNA, they looked at it under a microscope \u2014 and saw a virus. The discovery stunned microbiologists. At 750 nanometres, Mimivirus was the first viral particle to be visible under a light microscope. \"It was an exciting moment,\" says Jean-Michel Claverie, an evolutionary microbiologist at the University of the Mediterranean, who collaborated with Raoult on the discovery and analysis of Mimivirus. \"People realized we didn't know anything about microbial biodiversity.\" In 2004, Raoult and his group published the genome of Mimivirus 4 . At 1.2 million base pairs, its genome was at least twice as large as the largest known viral genome. And some of its more than 1,000 genes were involved in translating genetic information into proteins, something never before seen in viruses because they usually must rely on host translation machinery. The group drew up phylogenetic trees by comparing sequence information for seven proteins from Mimivirus with counterparts from organisms in the three domains. The giant virus, they wrote \"appears to define a new branch\", suggesting that it arose from an ancestor that could have predated the eukaryotes. According to Raoult, some viruses might deserve this special status on the tree because of their complexity and genetic make-up. Objections ensued. David Moreira and Purificaci\u00f3n L\u00f3pez-Garc\u00eda of the University of Paris-Sud in Orsay, France, included theirs in a list of ten reasons why viruses should be excluded from the tree of life 5 . For one thing, viruses can steal genetic material from their hosts in a process called horizontal gene transfer, which could be the source of the protein-translation genes. \n               Click here for larger image \n               Raoult, meanwhile, had found more giant viruses, including one in a Parisian cooling tower. Last year, he published a paper 6  that incorporated genetic data from the newly discovered giant viruses and from some other known large DNA viruses into phylogenetic trees. Because the sequences from these viruses seem to cluster together on their own branch of the tree, he argued, their genes must have been inherited together through evolution, and the group, called nucleocytoplasmic large DNA viruses (NCLDVs), shared a common ancestor (see  'Family trees' ). But resolving lineage using genetic sequences is not straightforward, particularly over great evolutionary distances. Over time, genes can mutate one way and later revert back to their former sequences, erasing the evolutionary trail of breadcrumbs that scientists try to follow. Two species can also independently evolve highly similar gene sequences, even though they aren't related, in a process called convergence. So organisms such as the NCLDVs could appear to belong together on close branches when really they are farther apart. In June, Eva Heinz, an evolutionary biologist at Newcastle University, UK, and her colleagues published a response to Raoult's redrawn trees 7 . They reanalysed his data and built fresh trees based on models that account for artefacts such as convergence. The new trees showed the NCLDVs sprouting out as 'twigs' from several different branches, rather than being constrained to one branch. Heinz and her co-authors interpreted this to mean that the genes in the NCLDVs could have been stolen from various hosts on those branches through horizontal gene transfer. \"With ancient phylogenetics, the model you use really matters,\" says Tom Williams, one of the co-authors, who is also at Newcastle. \"When we used more biologically realistic models, we got a different tree that fitted the data better.\" And that tree had no fourth domain. Raoult isn't convinced. It is true that large viruses do not branch together on the new trees, he says, but in his opinion, neither do the eukaryotes. \"Maybe their analysis is not able to cluster organisms that have the same origin,\" he says. Heinz disagrees, saying that the eukaryotes do cluster together in their analysis. So Raoult is currently standing alone. \"I'm ready to believe many exotic scenarios,\" says Claverie, \"but my feeling is that at this point we don't have enough data, especially on those large viruses.\"  \n                Giant hunting \n              Raoult is still trying to work out how his huge viruses evolved. One idea is that they could have come from a more complex ancestor that was later reduced to a parasitic shell. Raoult continues to search for new giant viruses and wants to use their genome sequences to work out when this group of viruses emerged, based on a molecular clock deduced from the viruses' mutation rates. And he avoids conversing with members of the virology community who are unwilling to challenge the 'dogma' of their field. \"I'm more excited by discussions with the people trying to put a little bit of disorder in the way we think about biology,\" he says. Woese says he can relate to this. \"You've got to hang those ideas out there and take what comes,\" he says. But although Woese's iconoclastic ideas stood up to the criticism back in his day, he says he is less hopeful for Raoult's fourth domain. So where does this leave Eisen? In his March study, he and his team probed seawater samples from the J. Craig Venter Institute's Global Ocean Sampling Expedition, an around-the-world venture that aims to find novel organisms by sequencing the mix of marine DNA. Most researchers doing environmental genomics search for organisms closely related to those already known, but Eisen cast a wider net. And when he drew up phylogenetic trees based on his haul, several unknowns stood out. A few looked like they might belong in Raoult's fourth branch, whereas others poked out from other branches. But with neither the time nor the funds to identify the organisms containing the sequences, Eisen's team published the paper 1  detailing the methods and sequences in order to seek help from others. Eugene Koonin, a computational evolutionary biologist at the National Center for Biotechnology Information in Bethesda, Maryland, has done a preliminary analysis of the data that he says turned up nothing too unusual. \"Quite a few of these sequences may represent very interesting new branches of bacteria and archaea,\" he says. But he ultimately sees it as a strong negative result: \"They were hunting for the fourth domain but their net came back empty.\" Eisen, who has yet to see Koonin's data, is not ready to sign up for Raoult's club. Rather, he says he published his data as a challenge to other researchers to be alert to what is truly unique in environmental samples, rather than discarding it as an artefact, as they often do. And there is plenty left to be discovered within the archaea and bacteria, says Francisco Rodr\u00edguez-Valera, a microbiologist at the Miguel Hern\u00e1ndez University in Alicante, Spain. \"There is a huge amount of microbial diversity that is unknown,\" he says. \"I don't think we need to discover a new domain every ten years to convey to the general public the fact that microbes are important.\" Gwyneth Dickey Zakaib writes for  Nature  from Washington DC. \n                     Focus on Marine Microbiology \n                   \n                     Jonathan Eisen \n                   \n                     Didier Raoult \n                   \n                     NCBI Viral Genomes \n                   Reprints and Permissions"},
{"file_id": "474142a", "url": "https://www.nature.com/articles/474142a", "year": 2011, "authors": [{"name": "Sharon Weinberger"}], "parsed_as_year": "2006_or_before", "body": "Last year's Stuxnet virus attack represented a new kind of threat to critical infrastructure. Just over a year ago, a computer in Iran started repeatedly rebooting itself, seemingly without reason. Suspecting some kind of malicious software (malware), analysts at VirusBlokAda, an antivirus-software company in Minsk, examined the misbehaving machine over the Internet, and soon found that they were right. Disturbingly so: the code they extracted from the Iranian machine proved to be a previously unknown computer virus of unprecedented size and complexity. On 17 June 2010, VirusBlokAda issued a worldwide alert that set off an international race to track down what came to be known as Stuxnet: the most sophisticated computer malware yet found and the harbinger of a new generation of cyberthreats. Unlike conventional malware, which does its damage only in the virtual world of computers and networks, Stuxnet would turn out to target the software that controls pumps, valves, generators and other industrial machines. \"It was the first time we'd analysed a threat that could cause real-world damage, that could actually cause some machine to break, that might be able to cause an explosion,\" says Liam O Murchu, chief of security response for the world's largest computer-security firm, Symantec in Mountain View, California. Stuxnet provided chilling proof that groups or nations could launch a cyberattack against a society's vital infrastructures for water and energy. \"We are probably just now entering the era of the cyber arms race,\" says Mikko Hypponen, chief research officer for F-Secure, an antivirus company based in Helsinki. Worse yet, the Stuxnet episode has highlighted just how inadequate are society's current defences \u2014 and how glaring is the gap in cybersecurity science. Computer-security firms are competitive in the marketplace, but they generally respond to a threat such as Stuxnet with close collaboration behind the scenes. Soon after VirusBlokAda's alert, for example, Kaspersky Lab in Moscow was working with Microsoft in Redmond, Washington, to hunt down the vulnerabilities that the virus was exploiting in the Windows operating system. (It was Microsoft that coined the name Stuxnet, after one of the files hidden in its code. Technically, Stuxnet was a 'worm', a type of malware that can operate on its own without needing another program to infect. But even experts often call it a 'virus', which has become the generic term for self-replicating malware.) One of the most ambitious and comprehensive responses was led by Symantec, which kept O Murchu and his worldwide team of experts working on Stuxnet around the clock for three months. One major centre of operations was Symantec's malware lab in Culver City, California, which operates like the digital equivalent of a top-level biological containment facility. A sign on the door warns visitors to leave computers, USB flash drives and smart phones outside: any electronic device that passes through that door, even by mistake, will stay there. Inside the lab, the team began by dropping Stuxnet into a simulated networking environment so that they could safely watch what it did. The sheer size of the virus was staggering: some 15,000 lines of code, representing an estimated 10,000 person hours in software development. Compared with any other virus ever seen, says O Murchu, \"it's a huge amount of code\". Equally striking was the sophistication of that code. Stuxnet took advantage of two digital certificates of authenticity stolen from respected companies, and exploited four different 'zero day vulnerabilities' \u2014 previously unidentified security holes in Windows that were wide open for hackers to use. Then there was the virus's behaviour. \"Very quickly we realized that it was doing something very unusual,\" recalls O Murchu. Most notably, Stuxnet was trying to talk to the programmable logic controllers (PLCs) that are used to direct industrial machinery. Stuxnet was very selective, however: although the virus could spread to almost any machine running Windows, the crucial parts of its executable code would become active only if that machine was also running Siemens Step7, one of the many supervisory control and data acquisition (SCADA) systems used to manage industrial processes. \n               boxed-text \n             Many industrial control systems are never connected to the Internet, precisely to protect them from malware and hostile takeover. That led to another aspect of Stuxnet's sophistication. Like most other malware, it could spread over a network. But it could also covertly install itself on a USB drive. So all it would take was one operator unknowingly plugging an infected memory stick into a control-system computer, and the virus could explode into action (see  'How a virus can cripple a nation' ).  \n                Murky motives \n              It still wasn't clear what Stuxnet was supposed to do to the Siemens software. The Symantec team got a clue when it realized that the virus was gathering information about the host computers it had infected, and sending the data back to servers in Malaysia and Denmark \u2014 presumably to give the unknown perpetrators a way to update the Stuxnet virus covertly. Identifying the command and control servers didn't allow Symantec to identify the perpetrators, but they were able to convince the Internet service providers to cut off the perpetrators' access, rerouting the traffic from the infected computers back to Symantec so that they could eavesdrop. By watching where the traffic to the servers was coming from, O Murchu says, \"we were able to see that the majority of infections were in Iran\" \u2014 at least 60% of them. In fact, the infections seemed to have been appearing there in waves since 2009. The obvious inference was that the virus had deliberately been directed against Iran, for reasons as yet unknown. But the Symantec investigators couldn't go much further by themselves. They were extremely knowledgeable about computers and networking, but like most malware-protection teams, they had little or no expertise in PLCs or SCADA systems. \"At some point in their analysis they just couldn't make any more sense out of what the purpose of this thing was, because they were not able to experiment with the virus in such a lab environment,\" says Ralph Langner, a control-system security consultant in Hamburg, Germany. Langner independently took it upon himself to fill that gap. Over the summer, he and his team began running Stuxnet in a lab environment equipped with Siemens software and industrial control systems, and watching how the virus interacted with PLCs. \"We began to see very strange and funny results immediately, and I mean by that within the first day of our lab experiment,\" he says. Those PLC results allowed Langner to infer that Stuxnet was a directed attack, seeking out specific software and hardware. In mid-September 2010, he announced on his blog that the evidence supported the suspicion that Stuxnet had been deliberately directed against Iran. The most likely target, he then believed, was the Bushehr nuclear power plant.  \n                Industrial sabotage \n              Speculative though Langner's statements were, the news media quickly picked up on them and spread the word of a targeted cyberweapon. Over the next few months, however, as Langner and others continued to work with the code, the evidence began to point away from Bushehr and towards a uranium-enrichment facility in Natanz, where thousands of centrifuges were separating the rare but fissionable isotope uranium-235 from the heavier uranium-238. Many Western nations believe that this enrichment effort, which ostensibly provides fuel for nuclear power stations, is actually aimed at producing a nuclear weapon. The malware code, according to Langner and others, was designed to alter the speed of the delicate centrifuges, essentially causing the machines to spin out of control and break. That interpretation is given credence by reports from the International Atomic Energy Agency (IAEA) in Vienna, which document a precipitous drop in the number of operating centrifuges in 2009, the year that many observers think Stuxnet first infected computers in Iran. True, the evidence is circumstantial at best. \"We don't know what those machines were doing\" when they weren't in operation, cautions Ivanka Barszashka, a Bulgarian physicist who studied Iranian centrifuge performance while she was working with the Federation of American Scientists in Washington DC. \"We don't know if they were actually broken or if they were just sitting there.\" Moreover, the Iranian government has officially denied that Stuxnet destroyed large numbers of centrifuges at Natanz, although it does acknowledge that the infection is widespread in the country. And IAEA inspection reports from late 2010 make it clear that any damage was at most a temporary setback: Iran's enrichment capacity is higher than ever. However, if Natanz was the target, that does suggest an answer to the mystery of who created Stuxnet, and why. Given the knowledge required \u2014 including expertise in malware, industrial security and the specific types and configurations of the industrial equipment being targeted \u2014 most Stuxnet investigators concluded early on that the perpetrators were backed by a government. Governments have tried to sabotage foreign nuclear programmes before, says Olli Heinonen, a senior fellow at the Belfer Center for Science and International Affairs at Harvard University in Cambridge, Massachusetts, and former deputy director-general of the IAEA. In the 1980s and 1990s, for example, Western governments orchestrated a campaign to inject faulty parts into the network that Pakistan used to supply nuclear technology to countries such as Iran and North Korea. Intelligence agencies, including the US Central Intelligence Agency, have also made other attempts to sell flawed nuclear designs to would-be proliferators. \"Stuxnet,\" says Heinonen, \"is another way to do the same thing.\" Langner argues that the government behind Stuxnet is that of the United States, which has both the required expertise in cyberwarfare and a long-standing goal of thwarting Iran's nuclear ambitions. Throughout the summer of 2010, while Langner, Symantec and all the other investigators were vigorously trading ideas and information about Stuxnet, the US Department of Homeland Security maintained a puzzling silence, even though it operates Computer Emergency Readiness Teams (CERTs) created specifically to address cyberthreats. True, the CERT at the Idaho National Laboratory outside Idaho Falls, which operates one of the world's most sophisticated testbeds for industrial control systems, did issue a series of alerts. But the first, on 20 July 2010, came more than a month after the initial warning from Belarus and contained nothing new. Later alerts followed the same pattern: too little, too late. \"A delayed clipping service,\" said Dale Peterson, founder of Digital Bond, a SCADA security firm in Sunrise, Florida, on his blog. \"There is no way that they could have missed this problem, or that this is all a misunderstanding. That's just not possible,\" says Langner, who believes that the Idaho lab's anaemic response was deliberate, intended to cover up the fact that Stuxnet had been created there. But even Langner has to admit that the evidence against the United States is purely circumstantial. (The US government itself will neither confirm nor deny the allegation, as is its practice for any discussion of covert activity.) And the evidence against the other frequently mentioned suspect, Israel, is even more so. Symantec, for example, points out that a name embedded in Stuxnet's code, Myrtus, could be a reference to a biblical story about a planned massacre of Jews in Persia. But other investigators say that such claims are beyond tenuous. \"There are no facts\" about Israel, declares Jeffrey Carr, founder and chief executive of Taia Global, a cybersecurity consulting company in Tysons Corner, Virginia.  \n                The Aftermath \n              The 'who?' may never be discovered. Active investigation of Stuxnet effectively came to an end in February 2011, when Symantec posted a final update to its definitive report on the virus, including key details about its execution, lines of attack and spread over time. Microsoft had long since patched the security holes that Stuxnet exploited, and all the antivirus companies had updated their customers' digital immune systems with the ability to recognize and shut down Stuxnet on sight. New infections are now rare \u2014 although they do still occur, and it will take years before all the computers with access to Siemens controllers are patched. If Stuxnet itself has ceased to be a serious threat, however, cybersecurity experts continue to worry about the larger vulnerabilities that it exposed. Stuxnet essentially laid out a blueprint for future attackers to learn from and perhaps improve, say many of the investigators who have studied it. \"In a way, you did open the Pandora's box by launching this attack,\" says Langner of his suspicions about the United States. \"And it might turn back to you guys eventually.\" Cybersecurity experts are ill-prepared for the threat, in part because they lack ties to the people who understand industrial control systems. \"We've got actually two very different worlds that traditionally have not communicated all that much,\" says Eric Byres, co-founder and chief technology officer of Tofino Industrial Security in Lantzville, Canada. He applauds Symantec, Langner and others for reaching across that divide. But the effort required to make those connections substantially delayed the investigation. The divide extends into university computer-science departments, say Byres, himself an ex-academic. Researchers tend to look at industrial-control security as a technical problem, rather than an issue requiring serious scientific attention, he says. So when graduate students express interest in looking at, say, cryptography and industrial controls, they are told that the subject is not mathematically challenging enough for a dissertation project. \"I'm not aware of any academic researchers who have invested significantly in the study of Stuxnet,\" agrees Andrew Ginter, director of industrial security for the North American group of Waterfall Security Solutions, based in Tel Aviv, Israel. Almost the only researchers doing that kind of work are in industrial or government settings \u2014 among them a team at the Idaho National Laboratory working on a next-generation system called Sophia, which tries to protect industrial control systems against Stuxnet-like threats by detecting anomalies in the network. One barrier for academics working on cybersecurity is access to the malware that they must protect against. That was not such a problem for Stuxnet itself, because its code was posted on the web shortly after it was first identified. But in general, the careful safeguards that Symantec and other companies put in place in secure labs to protect the escape of malware may also inadvertently be a barrier for researchers who need to study them. \"If you're doing research into biological agents, it's limited groups that have them and they are largely unwilling to share; the same holds true for malware,\" says Anup Ghosh, chief scientist at the Center for Secure Information Systems at George Mason University in Fairfax, Virginia. \"To advance the field, researchers need access to good data sets,\" says Ghosh, who was once a programme manager at the US Defense Advanced Research Projects Agency, and is now working on a malware detector designed to identify viruses on the basis of how they behave, rather than on specific patterns in their code, known as signatures. Academic researchers are also inhibited by a certain squeamishness about digital weaponry, according to Herb Lin, chief scientist at the Computer Science and Telecommunications Board of the US National Research Council in Washington DC. He points out that to understand how to guard against cyberattacks, it may help to know how to commit them. Yet teaching graduate students to write malware is \"very controversial\", he says. \"People say, 'What do you mean: you're training hackers?'\"  \n                Preparing for the next attack \n              A study last year by the JASON group, which advises the US government on science and technology matters, including defence, found broad challenges for cybersecurity (JASON  Science of Cyber-Security ; MITRE Corporation, 2010). Perhaps most important was its conclusion that the field was \"underdeveloped in reporting experimental results, and consequently in the ability to use them\". Roy Maxion, a computer scientist at Carnegie Mellon University in Pittsburgh, Pennsylvania, who briefed JASON, goes further, saying that cybersecurity suffers from a lack of scientific rigour. Medical professionals over the past 200 years transformed themselves from purveyors of leeches to modern scientists with the advent of evidence-based medicine, he notes. \"In computer science and in computer security in particular, that train is nowhere in sight.\" Computer science has developed largely as a collection of what Maxion calls \"clever parlour tricks\". For example, at one conference, the leading paper showed how researchers could read computer screens by looking at the reflections off windows and other objects. \"From a practical point of view, anyone in a classified meeting would go, 'pooh',\" he says. \"In places where they don't want you to know [what's on the computer screen], there are no windows. Yet, that was the buzz that year.\" Maxion sees an urgent need for computer-science and security curricula to include courses in traditional research methods, such as experimental design and statistics \u2014 none of which is currently required. \"Why does it matter?\" he asks. \"Because we don't have a scientific basis for investigating phenomena like Stuxnet, or the kind of defences that would be effective against it.\" Also troubling for many of the Stuxnet investigators was the US government's lacklustre response to the virus (assuming that it was not the perpetrator). Stuxnet represents a new generation of cyberweapon that could be turned against US targets, but there is no evidence that the government is making the obvious preparations for such an attack \u2014 for example, plans for a coordinated response that pools resources from academia, research institutes and private business. Other countries seem to be taking the threat more seriously. Some of China's universities and vocational colleges have reportedly forged strong connections with the military to work on cybersecurity, for example. And Israel also seems to be exploiting its computing expertise for national security. A few months before the discovery of Stuxnet, Yuval Elovici, a computer scientist and director of Deutsche Telekom Laboratories at Ben-Gurion University of the Negev in Beersheba, Israel, told  Nature   that he was working closely with the country's Ministry of Defense on cybersecurity. He presciently warned that the next wave of cyberattacks would be aimed at physical infrastructures. \"What would happen if there were a code injection into SCADA? What if someone would activate it suddenly?\" Elovici asked. He and other experts have been warning for several years now that such an attack on SCADA systems controlling the electricity grid could spark nationwide blackouts, or that the safety systems of power plants could be overridden, causing a shutdown or a serious accident. Similar disruptions could hit water and sewage systems, or even food processing plants. Such attacks, Elovici warned, are both realistic and underestimated. Asked how bad one would be, Elovici was unequivocal. \"I think,\" he said, \"it would be much stronger than the impact of setting several atomic bombs on major cities.\"\n See Editorial  page 127 Sharon Weinberger is an Alicia Patterson Foundation fellow based in Washington DC. \n                     Symantec's Stuxnet Dossier \n                   \n                     Ralph Langer \n                   \n                     Report: How Stuxnet Spreads \u2014 A Study of Infection Paths in Best Practice Systems \n                   \n                     Department of Homeland Security's Computer Emergency Readiness Team \n                   Reprints and Permissions"},
{"file_id": "476142a", "url": "https://www.nature.com/articles/476142a", "year": 2011, "authors": [{"name": "Alla Katsnelson"}], "parsed_as_year": "2006_or_before", "body": "How 'sham' brain surgery could be killing off valuable therapies for Parkinson's disease. Peggy Willocks was 44 when she was diagnosed with Parkinson's disease. It progressed quickly, forcing her to retire four years later from her job as a primary-school principal in Elizabethton, Tennessee. Soon, her condition had deteriorated so much that she was often unable to dress and feed herself, take care of basic hygiene or walk unaided across a room. Willocks enrolled in a trial for an experimental therapy called Spheramine, developed by Titan Pharmaceuticals, a biotechnology company in South San Francisco, California. Spheramine consists of cultured human retinal epithelial cells bound to specialized man-made carrier molecules. The cells are implanted into the brain, where it is hoped that they will produce the dopamine precursor levodopa, which can reduce the symptoms of Parkinson's disease. In August 2000, Willocks became the second person ever to receive the treatment. After having a steel halo \u2014 a stereotactic frame \u2014 bolted to her skull, she was put under general anaesthesia. Surgeons then used the frame and coordinates obtained from numerous magnetic resonance imaging (MRI) scans to pinpoint the location at which to drill. They then snaked a catheter through her brain's white matter to deliver the cells into the striatum. At first there was no effect, but Willocks says that after 6\u20138 months she began to feel better. The changes were always moderate and gradual, except for once, about nine months after her surgery, when she showed what her doctor called a \"radical\" improvement in balance. By a year after the treatment, she and the five other patients in the phase I trial showed an improvement in motor ability of 48%, and those gains largely held 4 years later 1 . Ten years on, she says she notices her condition worsening, but is still doing much better than she was before her operation. She has no doubt that the treatment works. Investigators disagree: Spheramine was shelved in 2008 after a follow-up phase II, double-blind study found that it was no more effective than placebo 2 . This time, the researchers compared the treatment with a 'sham' brain surgery that copied almost every aspect of the procedure Willocks received, short of injecting cells into the brain. For many investigators aiming to treat Parkinson's and other neurological diseases invasively, using sham brain surgery as a control is, well, a no-brainer. And the practice is likely to expand in coming years, as researchers continue to develop experimental tissue transplants, gene therapies and stem-cell treatments. Small safety trials such as the one in which Willocks was enrolled may hint at the efficacy of a treatment, but they are not designed to prove it. And because they are 'open label' \u2014 both the investigators and the participants know that the drug is being administered \u2014 they are riddled with biases that can skew results. \"It is so clear that open-label studies provide information that is not reliable,\" says Warren Olanow, a neurologist at New York's Mount Sinai Medical Center who has worked on cell-based neurosurgical therapies in Parkinson's for more than two decades. \"It's almost impossible for me to imagine how a serious scientist can not desire their data or hypothesis to be tested in double-blind studies.\" Other scientists, however, say that sham brain surgery is an expensive, potentially dangerous and possibly unethical bit of biomedical theatrics. It may also be unnecessary. Clinical neuroscientist Roger Barker at the University of Cambridge, UK, contends that because there is huge variation in how these therapies are administered and in how patients respond, the protocols need to be refined in an open-label setting before going on to the next stage of development. And because cost, complexity and the small number of people eligible for such invasive therapies limit the size of the studies, a sham control provides results of limited statistical utility. Barker and his colleagues across Europe are currently enrolling patients in a \u20ac12-million (US$17-million) multicentre trial of a fetal dopaminergic nerve-cell treatment for Parkinson's disease. The treatment may never be tested against a sham-surgery control. \"There's a sort of historical precedent\" for using placebo controls, but it may not apply to neurosurgical trials, he says. Willocks and other patients go further. Placebo-controlled studies aren't just unnecessary, they say, they are actually causing the downfall of potentially valuable treatments.  \n                A complicating control \n             \n               boxed-text \n             During the past 25 years, surgical therapies for Parkinson's disease have travelled a rocky road. In 1987, a report by Mexican surgeons 3  described seemingly miraculous effects in two patients with severe Parkinson's who received transplants of tissue from the adrenal gland, which produces dopamine. In the next several years, hundreds of patients received the treatment, but some autopsies later showed that the cells didn't in fact survive 4 . Around the same time, researchers started to test fetal nerve-cell transplants (similar to those in Barker's trial) in small-scale studies, finding mixed but promising results. Two studies 5 , 6  comparing the treatment to sham surgery concluded, however, that the transplants were not only ineffective, but also often caused dyskinesia \u2014 the movement disorder that plagues people with Parkinson's disease. In the past seven years, three experimental treatments (including Spheramine) that showed promise in small, open-label studies 1 , 7 , 8  failed in phase II trials 2 , 4 , 9  comparing them with a sham control (see  'The sham wall' ). Sham brain surgery is no sugar pill. After the stereotactic frame is affixed to the skull, the patient is usually anaesthetized and surgeons drill into the skull. In most cases, the burr holes stop at the dura mater, a protective membrane covering the brain, but they sometimes go deeper: in a phase II trial testing the nerve growth factor GDNF, investigators catheterized the brain in all participants but infused saline, rather than GDNF, into the controls 9 . \"We have to stage the whole thing such that from the outside it's completely indistinguishable\" from the real thing, says Joao Siffert, chief medical officer of Ceregene, a company in San Diego, California, that is working on a therapy that delivers a gene for another nerve growth factor, called neurturin, using a viral vector. For many sham treatments, everyone in the operating room, from surgeons to nurses' assistants, must pretend that they are busily performing the complete operation \u2014 in some cases, turning on machines to elicit appropriate noises. An extremely complex protocol ensures that no one outside the surgical team knows who got what treatment. \"It's very complicated, there are a lot of moving parts,\" Siffert says. All that ratchets up the cost of a trial; Siffert estimates that between operating-theatre costs, follow-up and the unwieldy infrastructure required for data management, a 50-patient study would cost more than $10 million. Still, at least in North America, Parkinson's disease investigators overwhelmingly support the use of sham surgery \u2014 at a rate of 94%, according to a 2004 survey 10 . Around 20% said that penetrating the brain is justifiable. And proponents say the procedure is relatively safe. Although sham brain surgery has definite risks, most notably those associated with general anaesthesia, supporters note that adverse events are almost unheard of, unlike the risks of the actual treatments. And participants in the sham groups are generally promised the treatment if it is ultimately approved; in that event, they will already have the burr holes in their skulls through which it would be administered. Sham treatments help to tease out the placebo effect and biases. In Parkinson's disease, the placebo effect is especially strong. One reason is that patients' expectations that they will benefit from a treatment induce the release of dopamine 11 , the neurotransmitter that is lacking in the disease. \"The placebo effect is real, it's huge and it's got a physiological basis,\" says Jon Stoessl, a neurologist at the University of British Columbia in Vancouver, Canada, who studies Parkinson's and the placebo effect. In one double-blind study of fetal nerve-cell transplants, patient improvement correlated with whether they believed they had received the treatment, irrespective of whether they actually had 12 . And the effect can last as long as two years, Stoessl says, citing an unpublished study by his colleagues. Many regard bias as a more significant confounder. \"Investigators have a tremendous vested interest in seeing that their treatment is effective,\" says Anthony Lang, a neurologist at the University of Toronto in Canada who has participated in several neurosurgical trials for experimental Parkinson's therapies. In any trial, bias can affect how researchers assess patient responses and may inflate the patients' expectations, further enhancing the placebo effect. Compounding the problem for Parkinsons' research is the fact that there are no objective measures for how well a patient is doing. \"It's just a sort of perfect storm conspiring against our ability to see definitive changes in the underlying disease,\" says Steven Piantadosi, a clinical-trials methodologist at Cedars-Sinai Medical Center in Los Angeles, California. \"Sham surgery, properly done, can control for that.\" Barker counters that it is possible to control for investigator bias in an open-label trial by taking steps such as having blinded raters assess patients. His position is in some ways unsurprising; in Europe, sham surgery is deemed much less acceptable than it is in the United States. It has never been used in the United Kingdom. Barker is categorical about his belief that transplantation of fetal tissue works, at least for some people. \"I don't need sham surgery to show that,\" he says, pointing instead to a paper 13  published last year describing two patients treated 13 and 16 years previously who were still benefiting from the treatment, and whose brains showed functional dopamine-producing neurons at the transplant site. He attributes the mixed results in past studies to variation in the patients selected for treatment, the characteristics of the tissue being implanted and the methods used to implant it. His trial will have to demonstrate efficacy without eliciting some of the side effects found in the two sham-controlled studies. That will require some type of control study, he says, but it might take the form of comparison to an approved therapy that is known to work, such as deep brain stimulation. But time, says Barker, will best establish efficacy. In most trials the end point is no more than a year after the treatment. That may not be long enough: implanted cells or injected growth factors might take longer than this to become fully functional, and the placebo effect may not have had time to dissipate. \"We want a 3\u20135-year endpoint,\" says Barker. There are hints from some of the failed phase II trials that patients followed up beyond study endpoints might tell a more positive story 4 . Some say, therefore, that sham controls are sinking the prospects of valuable drugs. Anders Bj\u00f6rklund, a neuroscientist at Lund University in Sweden who is collaborating with Barker, says that sham surgery can lead researchers to throw out a strategy prematurely if the trial fails because of technical or methodological glitches rather than a true lack of efficacy.  \n                Advocacy and frustration \n              According to Perry Cohen, who leads a network of patient activists called the Parkinson Pipeline Project, that's exactly what is happening. He had always questioned the need for sham surgery, he says, but after the string of phase II failures, \"We started saying, 'Hey, this is a problem. These trials failed, but we know they are working for some people.'\" For researchers, it is easy to dismiss patients' concerns as being driven by emotion. \"Patients want cures,\" says Lang, \"and they will often be convinced that the more aggressive, surgical therapies are more likely to be curative.\" But Cohen counters that patients have different priorities and that researchers must take these into account. Researchers use placebo controls to weed out false positives. But for patients, the real ogre is the false negatives \u2014 which can sink a therapy before it has been optimized. The better a trial is at stamping out the former, the higher the rate of the latter \u2014 which means at best delays, and at worst dead ends. Spheramine, for example, \"is still on a shelf somewhere\", Cohen says. Then there's Amgen's phase II trial of GDNF. The trial was halted in 2004 amid lacklustre results and potential safety concerns, which some have attributed to Amgen's procedure, rather than to the therapy itself. Now researchers are taking a renewed interest in the molecule, but although Cohen is glad it is getting a second chance, \"we lost 6 years on it\", he says. Patients also have different perspectives on risk from researchers, Cohen says. He offers the story of Tom Intili, who had had Parkinson's for 10 years when, at the age of 50, he signed on to the double-blind, placebo-controlled trial of neurturin. At first, Intili improved dramatically. But when the results were unblinded, he learned that he had received the sham. His condition plummeted, leaving him more debilitated than he had been before the trial. \"We just don't know what the psychological effects of unblinding are,\" Cohen says. Moreover, trying to exclude the placebo effect is simply misguided, Cohen argues. \"I don't want to subtract out the placebo effect \u2014 I want to keep it, because in real life it's part of the treatment,\" he insists. Because psychological factors are so salient in Parkinson's, a placebo response might actually potentiate a therapy, he explains. \"I want to be convinced that sham surgery is necessary. I'm looking for arguments that might change my mind, but I haven't found any yet,\" he says. Willocks says that she is living proof that many of the recently shelved therapies are in fact salvageable. Of course from a scientific perspective, her story is an anecdote, not data. In May, the failed phase II study of Spheramine \u2014 the therapy she received a decade ago \u2014 was finally published 2 . The paper closes with a warning about the dangers of the placebo effect and stresses the importance of controlling for it with a double-blind design. \"That last paragraph bothered me,\" Willocks says. \"I just don't see how they can call it a placebo effect after ten years.\" Alla Katsnelson is a freelance science writer in New York City. \n                     Outlook: Parkinson's Disease \n                   \n                     Roger Barker \n                   \n                     Parkinson Pipeline Project \n                   Reprints and Permissions"},
{"file_id": "478171a", "url": "https://www.nature.com/articles/478171a", "year": 2011, "authors": [], "parsed_as_year": "2006_or_before", "body": "As the Arctic thaws, can science help to chart a sustainable path for the north? Last winter, parts of the Canadian Arctic basked in record-breaking warmth. In the town of Coral Harbour, at the mouth of Hudson Bay, temperatures rose above freezing for a few days in January for the first time ever. Across the Arctic, extreme climate conditions are becoming the norm, even as the region faces other profound changes, such as the growing political power of indigenous peoples and the race to extract mineral resources (see  page 172 ). This week,  Nature   examines how these changes are affecting scientific access to the north (see  page 174 ), and what scientists should do to keep Arctic development green (see  page 179 ) and peaceful (see  page 180 ). Some are calling for international regulations to safeguard the environment as ship traffic increases (see  page 157 ). Both research and development need to consider the views of local peoples, and scientists are learning how to do so (see  page 182 ). Locals can provide insight into environmental changes; scientists might help them to be heard. There is a long tradition of international scientific cooperation in the Arctic. That tradition must be preserved and expanded, even as nations in the region push forward with territorial claims under the UN Convention on the Law of the Sea. Countries should work together, perhaps through the Arctic Council, to ensure that researchers from all nations are allowed access to all parts of the Arctic Ocean. At the same time, scientists should make their data available in public databases as soon as possible after collection. The far north is changing faster than anywhere else on Earth, with potentially vast impacts on climate as carbon-rich permafrost melts, and dark ground and water exposed by the retreating ice soak up more heat from the Sun. It is crucial that science keeps up. \n                     Special: After the ice \n                   Reprints and Permissions"},
{"file_id": "472156a", "url": "https://www.nature.com/articles/472156a", "year": 2011, "authors": [{"name": "Douglas Fox"}], "parsed_as_year": "2006_or_before", "body": "Scientists reviving a decades-old technique for brain stimulation have found that it can boost learning. So what else can be done with some wires and a nine-volt battery? Last year a succession of volunteers sat down in a research lab in Albuquerque, New Mexico to play  DARWARS Ambush! , a video game designed to train US soldiers bound for Iraq. Each person surveyed virtual landscapes strewn with dilapidated buildings and abandoned cars for signs of trouble \u2014 a shadow cast by a rooftop sniper, or an improvised explosive device behind a rubbish bin. With just seconds to react before a blast or shots rang out, most forgot about the wet sponge affixed to their right temple that was delivering a faint electric tickle. The volunteers received a few milliamps of current at most, and the simple gadget used to deliver it was powered by a 9-volt battery. It might sound like some wacky garage experiment, but Vincent Clark, a neuroscientist at the University of New Mexico, says that the technique, called transcranial direct-current stimulation (tDCS), could improve learning. The US Defense Advanced Research Projects Agency funded the research in the hope that it could be used to sharpen soldiers' minds on the battlefield. Yet for all its simplicity, it seems to work. Volunteers receiving 2 milliamps to the scalp (about one-five-hundredth the amount drawn by a 100-watt light bulb) showed twice as much improvement in the game after a short amount of training as those receiving one-twentieth the amount of current 1 . \"They learn more quickly but they don't have a good intuitive or introspective sense about why,\" says Clark. The technique, which has roots in research done more than two centuries ago, is experiencing something of a revival. Clark and others see tDCS as a way to tease apart the mechanisms of learning and cognition. As the technique is refined, researchers could, with the flick of a switch, amplify or mute activity in many areas of the brain and watch what happens behaviourally. The field is \"going to explode very soon and give us all sorts of new information and new questions\", says Clark. And as with some other interventions for stimulating brain activity, such as high-powered magnets or surgically implanted electrodes, researchers are attempting to use tDCS to treat neurological conditions, including depression and stroke. But given the simplicity of building tDCS devices, one of the most important questions will be whether it is ethical to tinker with healthy minds \u2014 to improve learning and cognition, for example. The effects seen in experimental settings \"are big enough that they would definitely have real-world consequences\", says Martha Farah, a neuroethicist at the University of Pennsylvania in Philadelphia. Getting to this point, however, was hardly straightforward. Direct-current brain stimulation has emerged from a long, touch-and-go history that ranges from the simply bizarre to the simply irreproducible. And for some, it still has much to prove. The Italian scientist Jean Aldini first tried direct-current stimulation around 1800 \u2014 initially to induce movement in the corpses of recently executed felons. Later, he claimed in a paper to cure two acquaintances of the mood disorder then known as 'melancholy'. By the 1940s, many patients with depression were being given electric shocks to the temples that were strong enough to induce seizures \u2014 so-called electroconvulsive therapy. But for decades people toyed with the idea of treating mental illness with electric shocks that were much milder \u2014 1,000 times less intense than electroconvulsive therapy.  \n                Weak origins \n              In 1964, Joe Redfearn, a psychiatrist at Graylingwell Hospital in Chichester, UK, applied some promising results in rats directly to humans, delivering weak currents \u2014 of 50\u2013250 microamps \u2014 to the scalps of volunteers. He reports that the volunteers became talkative, even giggly, when current was run in one direction, but withdrawn when it ran the other way 2 . He gave the 'giggly' treatment to 29 patients with depression and claims that half of them improved 3 . But no one could replicate his results, and the technique was abandoned. In retrospect, several factors seem to have undermined his work. Among them, Redfearn used currents ten times lower than in modern tDCS \u2014 perhaps because he had no way to measure how much electricity was actually reaching his patients' brains. Within a few decades, however, the necessary methods would become available, notably as researchers began to study brain activity induced by transcranial magnetic stimulation (TMS). In TMS, a magnetic coil running at thousands of volts is positioned just outside the head, leading to electrical surges inside the brain that can be precisely measured with external electrodes. Alberto Priori, a neuroscientist now at the University of Milan in Italy, showed in the 1990s that tDCS increased the effectiveness of TMS. He stimulated the motor cortex of volunteers for seven seconds with a direct current of 0.5 milliamps, then started hitting the area with short bursts of TMS. The assumption was that if tDCS made neurons more responsive, then more of those neurons would respond when TMS was subsequently applied. It turned out to be true \u2014 volunteers who got a short pulse of direct current had a larger response to TMS. But when Priori presented his results in 1993, colleagues doubted that the electricity was penetrating the skull. It took him until 1998 to convince reviewers that his results were bona fide 4 . Michael Nitsche, a clinical neurologist at the University of G\u00f6ttingen in Germany, was intrigued by the published findings. He had been experimenting with TMS to treat epilepsy at the time \u2014 but the equipment is unwieldy and expensive, and its effects on brain activity were too brief to help patients. Nitsche, a recent graduate at the time, and his supervisor, Walter Paulus, spent a year fiddling with tDCS. Their interest alarmed their colleagues. \"It's fucking dangerous,\" Nitsche recalls being told. \"You should stop this immediately.\" Nitsche managed to get his studies approved by university ethics boards, but a shortage of volunteers willing to have their brains zapped often forced him to experiment on his father, his sister and himself. In 2000, Nitsche and Paulus published a paper 5  showing that up to five minutes of weak current \u2014 around 1 milliamp \u2014 on the human scalp renders the motor cortex more responsive to signals for several minutes after the electricity is shut off. Like Priori, he used TMS to measure the effects. Nitsche and others have begun to clarify how tDCS works. Physiological studies indicate that direct current creates an electric field in brain tissue that changes the voltage across the neuronal membranes. 'Anodal' stimulation, in which electrons flow into the electrode on the head, pulls neurons a few millivolts towards 'depolarization', making them more likely to fire when signals arrive from other cells. 'Cathodal' stimulation, in which electrons flow out of the electrode on the head, has the opposite effect, 'hyperpolarizing' neurons and making them less responsive to signals from other cells. Effects seen after the electricity is shut off can last for an hour or so and seem to arise from a second mechanism. Pharmacological evidence suggests that the current increases the expression of proteins called NMDA receptors at the synapses, the connections between neurons. This heightens the plasticity of brain tissue \u2014 leaving it in a temporary state somewhat like wet clay, in which it is more apt to reshape its synaptic connections in response to stimuli, such as when learning a video game. \n               boxed-text \n             Researchers are exploring the ways in which this wet-clay state can be exploited. In a 2009 study 6 , Leonardo Cohen at the National Institute of Neurological Disorders and Stroke in Bethesda, Maryland, showed that tDCS improved people's ability to learn a simple coordination exercise \u2014 and that the improvement was still apparent three months after the experiment ended. Such results have led to an interest in stroke rehabilitation strategies. Small trials by Cohen, Nitsche, and others have shown improved recovery of hand function when tDCS is used this way (see 'Wired up'). Another group of researchers, led by Felipe Fregni of the Berenson-Allen Center for Noninvasive Brain Stimulation in Boston, Massachusetts, and Paulo Boggio of Mackenzie Presbyterian University in S\u00e3o Paulo, Brazil, is experimenting with tDCS as a way to treat depression. Several small trials done by this group and others suggest that a few sessions of tDCS to a part of the brain called the dorsolateral prefrontal cortex can improve mood for several weeks.  \n                Risky approach \n              In 2007, Boggio and Fregni reported that applying tDCS to the same region can make people less likely to take risks 7 . They asked healthy university students to play a game in which they press a computer key to pump air into a cartoon balloon. The more they pump, the more virtual money they earn \u2014 but if the balloon bursts, they lose all their winnings. People treated with tDCS were less willing to push their luck. The results may be generalizable to addictions, in which people lack \"inhibitory control\", says Boggio. In 2008, he and Fregni published three studies 8 \u2013 10  showing that stimulation of the dorsolateral prefrontal cortex blunted cravings for alcohol, cigarettes and sweets when people later watched videos in which these were being consumed. They hope, eventually, to test the same technique in a clinical trial for smoking cessation. For a method that has seen its ups and downs, these results are encouraging. \"There has been a lot of hokey stuff, frankly, and it affects the credibility of the entire field,\" says Marom Bikson, a biomedical engineer at the City College of New York. But in contemporary mechanistic studies to optimize tDCS, he says, \"people have been much more careful\". Not everyone is convinced that the disappointments are over, though. Helen Mayberg, a clinical psychiatrist at Emory University in Atlanta, Georgia, has been experimenting with the use of deep-brain stimulation (DBS), in which electrodes are placed deep in the brain, to treat depression. She is excited about the non-invasiveness of tDCS but points out that the trials conducted to date have been short-term. The real questions, she says, will be: \"How do you use it chronically, and what kinds of rebounds and relapses are there?\" Boggio and others have begun a clinical trial to answer this question, in which patients being treated with tDCS for depression will be observed for up to six months.  \n                Target practice \n              Still, the stimulation from tDCS is less focused than that from TMS or DBS. Its effects on neurons also drop off rapidly a few centimetres below the scalp, putting some important medical targets out of reach. Bikson has designed a more refined version of tDCS that he hopes will address these shortcomings. Instead of one electrode, he places five on the head in an X configuration. The one in the centre pushes current in the desired direction and the four around it siphon off excess current that would otherwise spread and activate wider brain areas. The configuration could, he says, allow for slightly higher currents that would penetrate deeper into the brain in more focused areas. Such innovations might even help to persuade companies to invest in clinical trials. Currently, Cohen says, no one stands to gain enough return from therapies that can be administered using just US$1,000 worth of off-the-shelf equipment. Aside from treatment, tDCS is also receiving attention for its potential to enhance the minds of healthy people. In addition to Clark's work showing enhanced ability to see concealed threats, other studies with tDCS have shown improvements in working memory 11 , word association 12  and complex problem-solving 13 . Most of these studies address scientific questions \u2014 but one neuroscientist unabashedly aims to boost the brains of healthy people. Allan Snyder, director of the Centre for the Mind at the University of Sydney in Australia, hopes to develop \"a thinking cap\", a tDCS device that corporate executives or advertising copywriters might use to bump up their creativity before walking into a brainstorming meeting. Snyder is cagey about how far he is in product development \u2014 but his latest demonstration, published this February 14 , garnered plenty of attention. Snyder claims to have boosted people's flair for sudden insight by stimulating their anterior temporal lobes. People who received tDCS were two to three times more likely than those receiving sham stimulation to solve a creativity problem in which they raced against the clock to spell out maths equations with matchsticks. The jury is still out on whether these results will translate into real-world benefits. Nitsche says that it will be harder to improve cognition in young, healthy people \u2014 whose minds are theoretically already optimized \u2014 than in elderly people or those with addictions, for instance. \"I wouldn't say it wouldn't be possible,\" says Nitsche. \"But things might be a little more complicated.\" That's not stopping some people from trying it at home. Discussions are already appearing on the Internet: buy a 9-volt battery, some wire and a resistor, and you're theoretically there. One person, hoping to improve his concentration, was alarmed by the flashing lights he experienced \u2014 a commonly reported side effect, along with burning or itching at the site of the electrode. \"I probably won't be doing this again,\" he said in a message posted online. Another wrote in an online patients' forum that the tDCS treatments he was giving to his wife were alleviating her chronic pain. Safety is an important issue. \"With wires and batteries and home hobbyists trying to run electricity through their heads, somebody could get hurt,\" says Farah. And wider adoption raises ethical concerns similar to those that surround mind-enhancing drugs such as Adderall and Modafinil, which some students take as study aids. Students might secretly 'electrodope' with tDCS before a university entrance exam to inflate their scores. Ethicists worry that this will give some an unfair advantage or create a culture in which people feel pressured to use such devices. None of the studies published so far have shown a type of mind-sharpening that would help in such exams, says Farah, but that might simply be a matter of targeting the right brain areas. \"It would not surprise me\" if such effects were possible, she says. Overall, though, the optimism among tDCS's believers remains high. Although it has generated some disappointments, many are convinced that the present buzz is warranted. \"Sometimes in the history of medicine you have to try again after one century or so,\" says Priori. \"You use a novel technical device, and you succeed where somebody else failed.\" Douglas Fox is a freelance writer in Northern California. \n                     Enhancing cognitive performance with drugs \n                   \n                     Vincent Clark \n                   \n                     Michael Nitsche \n                   \n                     Felipe Fregni \n                   \n                     Allan Snyder's Centre for the Mind \n                   Reprints and Permissions"},
{"file_id": "473268a", "url": "https://www.nature.com/articles/473268a", "year": 2011, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "The United States has abandoned comprehensive greenhouse-gas curbs, but California is pressing ahead. Mary Nichols is leading the fight against emissions. Mary Nichols can take some pride in the view as she travels out of Los Angeles. The San Gabriel Mountains rise up to the north, framed by blue sky with just a touch of midday haze. The clear vista comes in large part because of the California Air Resources Board (CARB), the agency that Nichols leads, which has spent decades cleaning up the city's air. Now she and her team are setting their sights even higher \u2014 with an ambitious plan to cut California's greenhouse-gas emissions. With an economy that outranks all but eight countries, California is a political and economic heavyweight that has never been afraid to flex its muscles. It is big enough make an impact, and now that politicians in Washington DC have abandoned attempts to enact a national climate law, California is forging ahead on its own. Nichols feels the burden of that strategy acutely, and she is well aware of the challenges ahead. In the run-up to the state elections last November, many feared that Californian voters would follow Washington DC's lead and cast aside the state's landmark climate legislation, AB 32. The 2006 law requires a 10% reduction in greenhouse-gas emissions by 2020, and critics \u2014 fuelled in part by donations from the fossil-fuel industry \u2014 argued that the state's economy was too fragile to withstand aggressive new regulations. But voters turned out en masse to preserve the initiative, which is the first comprehensive climate programme in the United States. California has committed to reducing emissions by the same percentage as the European Union, and the state's unique plan could chart new ground internationally. Since the 1970s, California has pushed the boundaries of environmental regulation, acting out of both pride and self-preservation. The state has pioneered environmental laws targeting air pollution, water contamination and toxic chemicals. It has advanced the sciences of atmospheric physics and chemistry, developed pollution-control technologies and bullied powerful industries into submission in an epic battle against choking smog in the Los Angeles basin. Other states, and eventually the nation, have followed California's path in developing regulations to control pollution. But Nichols and her staff at CARB need to go even further to rein in greenhouse-gas emissions. The agency plans to clean up vehicle fuels, promote renewable energy and squeeze more reductions by improving energy efficiency. It is also designing the world's most comprehensive carbon market, set to launch at the start of 2012. Nichols believes that California will one day be able to demonstrate to the rest of the country how environmental protection and economic growth can coexist. \"People in this state are bullish on the ability of California to survive and change, and they fundamentally care about air pollution and environmental issues,\" says Nichols. \"What we do here matters.\" From Washington DC to Brussels and Beijing, government leaders will monitor the state's progress closely. Henry Derwent, president of the International Emissions Trading Association based in Geneva, Switzerland, says that California's plans are reassuring governments around the world that all is not lost in the United States. \"The overriding feeling in Europe at the government level is relief,\" says Derwent. \"Even though it's not the entire United States, it's a pretty big consolation prize.\"  \n                Charm offensive \n              On this day in February, Nichols is travelling from her office in Los Angeles to a conference on sustainable growth at the California State Polytechnic University in Pomona. But the route along Interstate 10 illustrates the scale of the problem. The greater Los Angeles urban area sprawls outwards through towns and cities, filled with millions of people who love their vehicles. Despite that, Los Angeles has managed to clean its air through a productive interplay between technology and environmental policy. Nichols says that modern vehicles produce 1% of the toxic pollutants emitted by their forerunners in 1975. The city's population has doubled since then and the use of vehicles has grown at an even faster rate, yet the air just keeps getting cleaner. \n               Click here for larger image \n                But CARB now faces a bigger and broader challenge. If no action is taken, California's emissions are projected to climb from 474 million metric tonnes of carbon dioxide equivalent in 2008 to 596 million metric tonnes in 2020. To reach the target set in AB 32, Nichols and CARB must get the total down to 427 million metric tonnes, the amount that the state was emitting in 1990 (see 'Cleaning up California'). To do that, they need to make emissions reductions everywhere they can, and that is what brings Nichols to Pomona. She is addressing the small conference regarding one of the latest tools in CARB's belt: SB 375, a 2008 law requiring the agency to set targets for greenhouse-gas emissions from vehicles in all metropolitan areas. Her team set those targets last September, and the local and regional planning organizations must now develop strategies to meet them by, for example, promoting public transport, bike lanes and mixed-use zoning that brings amenities to people instead of forcing them to drive. CARB set a 13%-reduction target for the area that includes Los Angeles, but many local officials complained that the state was imposing costly rules without providing any money to help them comply. Nichols knows that some of those officials are in the audience, and she has come in peace. As she steps up to the microphone, she gives a confident smile and disarms the sceptical leaders by acknowledging that the law's future is in their hands. \"You could probably ignore it,\" she says, scanning the quiet audience for a reaction. \"Nothing will happen, as far as I can tell.\" Nichols then launches into a pep talk. SB 375 is not a top-down state solution, she says, but a bottom-up tool to help local and regional governments make their communities into more livable places, where people walk and exercise and spend more time with their families and less time alone in cars. This kind of master planning, she says, could set the stage for more organized \u2014 and less contentious \u2014 development because all parties would have agreed on the basic framework for growth. Nichols then ties up her talk by offering a small cash sweetener, in the form of grants to help local governments get the process started. There isn't so much as a peep of protest, and by the time lunch rolls around conversations are focusing on how to implement the law. \"If we see there is rising opposition, then we need to act and explain or make adjustments,\" says Nichols on her way back to the office. That kind of flexibility makes it easier for states than the federal government to negotiate difficult new regulations, she adds. \"We are closer to the people that we regulate.\" If Nichols makes it look easy, she has had a lot of practice. An environmental lawyer by training, Nichols is a diehard Democrat who has burnished her credentials working for environmental groups. She has also honed her diplomatic skills in various government posts, including a previous stint as head of CARB, from 1979 to 1983. She eventually rose to assistant administrator of air and radiation at the US Environmental Protection Agency (EPA) in 1993, under President Bill Clinton. For Nichols, these political appointments always represented an opportunity to put ideas into practice and put her stamp on the world. By the time the California legislature enacted AB 32 in 2006, Nichols was ensconced in academia as director of the Institute of the Environment at the University of California, Los Angeles. She wasn't looking for a job when Republican governor Arnold Schwarzenegger asked her in 2007 to take over CARB and find a way to meet the target, nor was she particularly thrilled about going to work for a Republican film star. She jokes that when she met Schwarzenegger, she interviewed him for the job, and he passed the test. Convinced that he was genuinely interested in making the programme work, Nichols jumped back into government.  \n                In the driver's seat \n              CARB's plan bets heavily on innovation, some of which the agency is developing and testing at its own facilities. Nichols spends much of her time working from CARB's main science laboratory in El Monte, east of central Los Angeles. This is where agency engineers invented the check-engine light in the 1980s to alert drivers to problems with their vehicle's pollution-control systems. CARB is now developing automated sensors that will allow technicians to more accurately track emissions data in cars using a secure onboard computer. Engineers are busy analysing emissions from advanced vehicles, testing the performance of hybrid electric cars and studying how various technologies could help the state to meet its 2020 goal and a further, non-binding commitment to reduce greenhouse-gas emissions by some 80% by mid-century. Part of being a successful leader is having followers. ,  The most ambitious element of CARB's plan is an overarching cap-and-trade programme that will cover roughly 85% of the state's emissions by 2015. Under that system, the state will issue a set number of allowances \u2014 initially for free but later through an auction \u2014 that companies will need to cover their greenhouse-gas emissions. The total number of permits will decrease each year, and companies will need to either reduce their emissions or buy spare allowances from other companies that have made reductions more cheaply. The cap-and-trade programme is an insurance policy. On their own, individual regulations for vehicle efficiency, renewable energy and other items will lower emissions, but they do not guarantee that the state will meet its targeted reductions. The cap-and-trade programme should \u2014 if it ever gets off the ground. In March, a California judge determined in a preliminary finding that CARB had failed to do a proper environmental analysis of the programme. The agency is now awaiting a final ruling on how to proceed, but CARB officials hope that the programme will move forward on schedule to begin next year. Meanwhile, the agency is pressing ahead with other bold plans. CARB is working with partners in Brazil and Mexico to design what would be the world's first market-based programme to allow businesses to offset their emissions by protecting tropical forests. The agency is also establishing another type of offset, involving ozone-depleting compounds such as chlorofluorocarbons, which are powerful greenhouse gases not included in the United Nations' 1997 Kyoto Protocol for reducing greenhouse-gas concentrations. Companies in California could avoid reducing their emissions of carbon dioxide or other Kyoto gases by curbing \u2014 or paying someone else to curb \u2014 their emissions of the non-Kyoto greenhouse gases, which are not targeted by a more limited cap-and-trade scheme launched by the European Union in 2005. CARB is trying to avoid pitfalls revealed by the European programme. That scheme, for example, initially issued too many allocations, which led to a collapse in the price of carbon. CARB is taking care to keep an inventory of emissions, so that it can issue an accurate number of initial allowances. But the inventory is calculated in part from figures provided by polluters, so CARB is also carrying out an independent check, funding scientists to measure concentrations of greenhouse gases and other pollutants in the field and then calculate emissions from that data. Already, CARB knows that methane emissions around Los Angeles are higher than the inventory suggests. Nicholas Bianco, a senior associate at the World Resources Institute in Washington DC who advises agencies on emissions reduction, says that the California cap-and-trade scheme represents a major step forward. \"It will be the first of its kind in the world.\" James Sweeney, director of the Precourt Energy Efficiency Center at Stanford University in California, says that what is happening in the state is exciting, but he has two fears. The first is that funding for energy and climate research will dry up in the current budget crisis, making the challenge of meeting long-term greenhouse-gas reduction targets in California and elsewhere even more difficult. The second relates to scale. California is important, but it represents just 7% of US emissions. \"The bottom line,\" says Sweeney, \"is that if California is going to have a real impact it will be as the laboratory for the nation.\" The chances of that happening are unclear. Northeastern states have a limited cap-and-trade programme for power plants, but western states have backed away from joining California's scheme \u2014 although at least three Canadian provinces are expressing interest. California isn't big enough to run its own system forever, says Nichols, but the state will stay the course for now. She points out that the agency has history on its side. When CARB published its first greenhouse-gas regulations for cars in 2004, it quickly ran into legal battles with the automobile industry and the administration of President George W. Bush. But last year, the Obama administration brought the various players together in a deal that essentially established CARB's vehicle regulations as national ones. \"When we started the first round of greenhouse-gas standards, the automobile companies wouldn't even talk to us,\" says Paul Hughes, who headed the effort as manager of the Low Emission Vehicle programme. Today, Hughes says, car makers are engaged at every step in the process as CARB and the EPA prepare to release identical new standards for California and the nation for model years 2017\u201325. Due late this year, those regulations are expected to translate into an average fuel-efficiency rating of 20\u201326 kilometres per litre for cars and trucks \u2014 a big jump from the current standard of less than 12 kilometres per litre. For Hughes, it is just a matter of time before other CARB policies diffuse outward and upward into the national scene. On the drive back from Pomona, Nichols ponders the roller-coaster progress of the past few years. With Obama in the White House, it looked as if the United States was finally gearing up for a serious push on global warming. Then lawmakers rejected the idea, leaving California on its own. The optimist in Nichols thinks that the United States will eventually find its way on climate. But she is also a realist and has a simple message for the rest of the country. \"California set itself up to be at the head of what we thought was going to be a parade, but part of being a successful leader is having followers,\" she says. \"At the end of the day, Californians are not going to accept a lonely role as the sole state in the union that is doing anything in terms of carbon.\"\n Jeff Tollefson covers energy and environment for  Nature  in Washington DC. \n                     CARB climate programme \n                   \n                     Northeastern Regional Greenhouse Gas Initiative \n                   Reprints and Permissions"},
{"file_id": "472024a", "url": "https://www.nature.com/articles/472024a", "year": 2011, "authors": [{"name": "Anjali Nayar"}], "parsed_as_year": "2006_or_before", "body": "The country's vast, education-hungry population could supply the next generation of the world's scientists \u2014 but only if it can teach them. Subha Chakraborty has hardly left the lab in three months. His master's research in micro-scale systems is running into the early hours almost every morning, and \"that is not the right time to go back to your room and sleep\", he says. So he bunks on a makeshift bed under his computer and cooks on a toaster in the corner of the lab's common room. Chakraborty isn't alone: most of the lab's ten postgraduate students follow a similar schedule. \"There's some kind of charm here,\" says one of them, Anindya Roy, who has decided to officially surrender his dormitory room. These students at the banyan-tree-lined campus of the Indian Institute of Technology (IIT) in Kharagpur are among India's luckiest and best: once they have completed their degrees, they will end up working at top universities and private research hubs in India and around the world. But the optimism and drive are ubiquitous. \"When you go to the rural parts of the country you meet extraordinarily bright kids who just have to be given the opportunity,\" says Chintamani Rao, chief scientific adviser to India's prime minister. There are a lot of them \u2014 around 90 million between the college-going ages of 17 and 21, rising to an estimated 150 million by 2025. And they are hungry, starving even, for an education (see 'Technology levels the educational playing field'.  \n                Brain drain \n             \n               Click here for larger image \n               But can India feed that hunger? The government has pledged to make it a priority, but faces tremendous obstacles. Most of the elite science and engineering graduates opt for high-paying jobs in industry rather than independent research. Other students far too often end up in high-priced commercial diploma-mills that deliver little real education. Many, many more young Indians don't even get that far: the country's 500 universities and 26,000 colleges have space for only about 12% of its eligible youth. And the population is growing by 1.34% a year, more than twice the rate of growth in China (see  'A double explosion' ). But if India cannot meet this challenge, it could miss out on becoming one of the world's great innovation hubs, says Rao. \"There is a very large population out there that is extremely qualified and they end up in second or third-rate institutions,\" agrees Pradeep Khosla, dean of engineering at Carnegie Mellon University in Pittsburgh, Pennsylvania, and a graduate of IIT Kharagpur. \"A lot of talent gets wasted.\" On the surface, India seems to be in the middle of an educational renaissance, thanks largely to its booming economy. After decades of economic stagnation under the socialist policies that followed the country's independence in 1947, Indians enthusiastically embraced a series of business-friendly reforms that began in the early 1990s. The result has been economic growth that currently averages more than 8% a year, with only a slight and temporary slowdown during the global financial crisis that began in 2008. That growth, in turn, has created a flourishing market for qualified graduates in everything from construction to information technology and health care. \"There are a lot of stories of successes \u2014 from rags to riches \u2014 of Indians who made it just on the basis of good education,\" says Pawan Agarwal, author of  Indian Higher Education: Envisioning the Future   (Sage; 2009). \"This is creating high aspirations among Indians about higher education.\" Those ambitions, along with the population growth, have fuelled an eight-fold increase in science and engineering enrolment at India's colleges and universities over the past decade, with most of the growth occurring in engineering and technology \u2014 fields in which jobs are especially plentiful. The low cost of doing business in India and the large crop of English-speaking graduates has made it a global hot spot for investment in research and development (R&D). \"In 2003, 100 foreign companies had established R&D facilities in India,\" says Thirumalachari Ramasami, head of the government's Department of Science and Technology. \"By 2009, the number had grown to 750.\" Those companies include technology and communications firms such as IBM, General Electric, Cisco, Motorola, Oracle and Hewlett-Packard, all eager to get a foothold in the fast-growing information-technology hub around Bangalore. \n               Click here for larger image \n               Small wonder, then, that the 15 IIT campuses nationwide have roughly 300,000 applicants every year, or that the students who make it in are very, very good: IIT acceptance rates are about 2% (see  'Only the best' ), compared with around 7% at Harvard University in Cambridge, Massachusetts, an emblem of US elitism. \"Statistically, out of a billion people there must be a Michael Faraday,\" says Rao. \"There must be a number of talented people.\" Look closer, however, and it becomes apparent that there are serious cracks in the system. For example, the vast majority of India's science and technology graduates immediately head for high-paying jobs in industry. Only about 1% of them go on to get PhDs, compared with about 8% in the United States. \"Internally the brain drain is quite high,\" says Rao. \"All the talent goes into sectors that make money but produce very little in terms of creative things for the country.\" What makes this problematic, adds Rao, is that the country's rising economic tide is largely the result of its myriad outsourcing centres and the computer industry. If India cannot broaden its economy \u2014 and make better use of its brightest scientific minds \u2014 it will have little chance of solving its challenges in areas such as poverty, food, energy and water security. \"Everyone's just making computers faster, and our computers are pretty fast already,\" agrees Manu Prakash, who graduated from the IIT in Kanpur \u2014 and who, like many Indians with academic ambitions, elected to pursue his education elsewhere. He earned his PhD from the Massachusetts Institute of Technology in Cambridge, and now runs his own biophysics lab at Stanford University in California. Prakash says that although the IIT system does attract superb students, it is institutionally broken because it doesn't value creativity. \"You have a brilliant mathematician coming into an engineering course and then taking a nine-to-five job with a company,\" he says. \"There is something wrong there.\"  \n                Quantity versus quality \n              Whatever its flaws, the IITs remain out of reach for millions of eager, ambitious Indian students. The higher-education system is expanding pell-mell to accommodate them \u2014 with the burgeoning private sector filling around 90% of the demand. \"We will need another 800\u2013900 universities and 40,000\u201345,000 colleges within the next 10 years,\" says Kapil Sibal, India's minister of human resources and development. \"And that's not something the government can do on its own.\" For-profit colleges and universities are popping up around the country by the day \u2014 nearly 4,000 of them in 2010 alone. The road leading out of Chennai in southern India, like many around the country, is crammed with hundreds of private engineering colleges. The government has struggled to maintain any kind of standard. \"The big challenge is that when you move to grant more access [to education], that the access must come with quality,\" says Sibal. Many private institutions have only a few hundred students each and offer little in the way of laboratory or practical training, because labs are expensive. Curricula are outdated and there are crippling shortages of teaching staff, thanks to the allure of higher-paying industry jobs. \"The younger generation is completely disillusioned with pursuing higher education with the intention of going into teaching,\" says Agarwal. Sibal estimates that at least 25% of academic posts are vacant and more than half of professors lack a postgraduate education. Rahul, who prefers that his real name not be used, studies information technology at a private college an hour outside Delhi. \"We are spoon-fed,\" he says. \"The teachers dictate and the students literally write down what they say.\" Rahul's parents paid hundreds of thousands of rupees up front to get him into the institute after he scored poorly on entrance exams. He says that about 30% of his peers entered in the same way, and at other colleges the informal 'management quota' can be as high as 40\u201350%. This year, tuition at the institute cost 85,000 rupees (US$1,900): more than three times that charged by the IIT system. And the payments at many private colleges don't stop there, says Rahul. \"A few days before [exams] you can pay 1,000 rupees for a copy of the paper, and you can pay another couple of thousand rupees if you didn't get the right marks,\" he says. \"Then, if you don't attend classes or labs, you can pay 5,000 rupees to fulfil your attendance quota. Education here is based entirely on money. And to think, my institute is one of the best in the area.\" There are more than 600 colleges affiliated with one university in his province alone, and every college has 5\u20136 branches, with 60\u2013120 students each. \"That's lakhs [hundreds of thousands] of students passing out of these colleges per year,\" says Rahul. Moreover, many of the students are graduating with abysmal literacy and numeracy skills. Employers' surveys suggest that up to 75% are unemployable. \"You can pay to get in, you can pay to get good marks and you can pay for your attendance, but you can't pay to get into a good company,\" says Rahul. \"There are people at my college who don't even know how to say 'how are you?' in English\" \u2014 the working language of most companies. Rahul's experience is not unusual. Geeta Kingdon, who studies education, economics and international development at the University of London's Institute of Education, points to allegations of widespread corruption in how Indian institutes and universities are accredited. \"Even those who have got the relevant accreditation only got it because they paid the relevant bribe,\" she says. Many don't bother. A government crackdown on unaccredited institutions in 2010 left more than 40 universities and thousands of colleges in court. Corruption has even reached the august halls of IIT Kharagpur. Last October, a handful of the institute's top engineering professors were accused of running a fake college called the Institution of Electrical Engineers (India) from the campus. The scheme allegedly involved the use of forged documents bearing the IIT logo to lure in students, who were charged 27,000 rupees for admission, roughly what the IITs charge per year. The IIT Kharagpur has launched an inquiry into the incident. \"But there will always be another scandal down the road,\" says Srinivasan Ramanujam, a mechanical engineer at the institute. \"Students are desperate to get into a college and people exploit this mentality.\" With all these desperate but half-baked graduates, India's hopes of becoming a global centre of innovation are being compromised. Too often, the corporate R&D model sweeping through India treats science graduates more as grunt workers than true innovators, says Ramasami. \"Just availability of scientifically talented people does not provide scientific breakthroughs. For the discovery process you need ambience and creative people.\" India's government is working hard to change the trend. In January 2010, for example, it pledged to ramp its investment in R&D up from the current 1% of the gross domestic product to 2%, but this will happen very slowly, says Rao. The government's budget for 2011\u201312 included a one-third increase in its annual higher-education investment, to a total of 130 billion rupees. And it has approved a new funding agency, the National Science and Engineering Research Board, which is expected to become operational this year, and will have an initial budget of around US$120 million, says Rao. By 2014, says Ramasami, the hope is that such measures will raise the number of science and technology PhDs awarded each year from the current 8,900 \u2014 less than one-third that of the United States or China \u2014 to at least 10,000. By the end of the decade, he says, the target is 20,000 PhDs a year.  \n                Overseas input \n              The government is also counting on an injection of money and expertise from foreign academic institutions. With enrolment rates waning abroad, many universities are looking to India as a new academic market \u2014 including US institutions such as the University of California, Berkeley, and Carnegie Mellon University. US President Barack Obama's trip to India last November highlighted the growing interest: included in his delegation were three presidents of US universities and senior representatives of several more. During the trip, Obama and Indian Prime Minister Manmohan Singh announced that they would hold a US\u2013India summit on higher education this year to help encourage collaborations. So far, Indian law has restricted foreign universities to forming partnerships with Indian institutions, says Sibal. But a Foreign Educational Institutions Bill being considered in India's parliament would allow them to build full-blown campuses of their own. Sibal takes it as a sign of what India could become. \"Top-quality institutions of the United States and around the world are actually knocking at our door,\" he says. \"The India of tomorrow will be an India that provides solutions not just for itself, but also for the rest of the world.\" But that is only if India's rising youthful generation can break out of its current job-based mentality \u2014 not easy in a developing country. One evening late last year, Shirsesh Bhaduri, a fourth-year biotechnology student at IIT Kharagpur, visited Tikka \u2014 a makeshift caf\u00e9 in the shade of a banyan tree, where students and faculty members catch up over cups of 3-rupee tea and samosas. But just over the campus's whitewashed walls is the reality of West Bengal state and most of India: unruly fields, shanty villages, water buffalo and jungle. \"In other countries, people may choose their career according to their interests,\" says Bhaduri, who has just been to an interview with London-based bank Barclays. \"But here the industries that pay the maximum attract the maximum applications. Most people do a master's in business administration after the IIT \u2014 and that is the aim of most people out here. Everything is money-oriented.\"\n Anjali Nayar is a freelance writer based in Nairobi. \n                     Nature India Portal \n                   \n                     Nature Outlook on India \n                   \n                     Indian Ministry of Human Resource Development \n                   \n                     Indian Institute of Technology Kharagpur \n                   Reprints and Permissions"},
{"file_id": "472022a", "url": "https://www.nature.com/articles/472022a", "year": 2011, "authors": [{"name": "Nicola Jones"}], "parsed_as_year": "2006_or_before", "body": "Super-powerful magnets would boost the performance of electric cars and other green technology. Why is it so hard to make them? For Christmas, magnetics researcher William McCallum got one of the latest cool toys: 'Buckyballs: The Amazing Magnetic Desktoy You Can't Put Down!' The magnets are state-of-the-art \u2014 strong enough that, if they were cubes rather than spheres, you wouldn't be able to pry them apart. But if McCallum has his way, his team will make them look like weaklings. McCallum, a materials scientist at Iowa State University in Ames, is tackling two big problems at the same time: magnet strength and cost. For most of the twentieth century, the strength of available magnets doubled every decade or two, but it stalled in the 1990s. The limit has hampered efforts to make high-tech products such as electric cars more efficient. And in the past two years, the cost of the rare-earth elements that are essential to advanced magnets has shot up. The price of neodymium oxide jumped from US$17 a kilogram to $85 a kilogram in 2010 alone. Despite their name, rare-earth elements such as neodymium aren't truly rare geologically, but they are expensive to mine and process. China, which provides about 95% of the 96,000 tonnes currently produced worldwide every year, has put increasingly stringent caps on exports, even as the need for the elements is booming. Magnets made with them are at the heart of modern technology from mobile phones and laptops to high-efficiency washing machines. And many devices that are part of the green economy require substantial amounts: an electric car carries a few kilograms of rare-earth elements, and a 3-megawatt wind turbine uses about 1.5 tonnes. Demand leapt from 30,000 tonnes in the 1980s to 120,000 tonnes in 2010 (which was met in part by depletion of national stockpiles), and is predicted to hit 200,000 tonnes by 2015, says Gareth Hatch, founder of the Technology Metals Research consultancy in Carpentersville, Illinois (see 'Market forces'). \n               boxed-text \n             Fortunately, the leading idea for how to make 'next-generation' magnets could solve both problems at once. It involves combining nanoparticles of rare-earth magnets with nanoparticles of cheaper magnetic materials \u2014 creating super-strong end-products with far less of the expensive ingredients. Governments keen to invest in energy-efficient technology, and scared by a global crunch in the rare-earth market, have started to pay attention to magnetics research. In the United States, an infusion of funds has come from the Department of Energy, home of the Advanced Research Projects Agency \u2014 Energy (ARPA-E), which was established in 2009 to bring high-risk, potentially 'transformative' technologies to the market. ARPA-E has allocated $6.6 million to research on next-generation magnets \u2014 a shot in the arm for the field. \"We're long overdue\" for the next magnet revolution, says George Hadjipanayis, a physicist at the University of Delaware in Newark, who is head of a $4.4-million ARPA-E consortium of which McCallum is part. \"We need to do it.\" Permanent magnets get their pulling power from the orbits and spins of unpaired electrons, which tend to align with an external magnetic field and stay that way when that field is taken away. These magnets are ranked by their 'energy product' in kilojoules per cubic metre (kJ m \u22123 ) \u2014 a combination of how much they respond to an applied magnetic field (their magnetization) and how well they resist being demagnetized. These properties don't always go hand in hand. Iron\u2013cobalt alloy has the highest potential magnetization known, but its energy product is effectively zero because it is easily demagnetized: it has a symmetrical cubic crystal structure, with nothing to keep its electron spins pointing in any one direction, so they can be jolted out of alignment by a bump or a nearby magnetic field.  \n                Spins in sync \n              Newer magnetic materials have a complex crystalline structure that helps to keep the spins pointing one way. In the 1950s, the best of such magnets, made of an alloy of iron, aluminium, nickel and cobalt called Alnico, achieved an energy product of 40 kJm \u22123  (see 'Stalled progress'). The 1960s brought the first generation of rare-earth magnets, made of samarium and cobalt, which eventually enabled energy products to exceed 250 kJm \u22123 . In the 1980s, researchers devised neodymium\u2013iron\u2013boron (NIB) magnets, which hold the record at about 470 kJm \u22123 . If the magnets have to work at high temperatures \u2014 such as in a car engine \u2014 the rare-earth element dysprosium is added to the mix. \n               boxed-text \n             The dream is to unite the magnetic punch of something like iron\u2013cobalt with the stability of, for example, a NIB magnet. That should be possible by combining nanoparticles of the two, packed so closely that neighbouring electrons influence each other and keep their spins aligned. In theory, a nanocomposite could reach an energy product of a whopping 960 kJm \u22123 , with rare earths making up just 5% of its weight, compared with 27% in a normal NIB magnet ( R. Skomski and J. M. D. Coey  Phys. Rev. B    48,   15812\u201315816; 1993 ). But making such a composite is extremely difficult. The grains in a successful nanocomposite must be small (10 nanometres or less); have the right crystal structure; have aligned magnetic directions; and be tightly packed. Achieving all of these at once is an engineering nightmare. On top of that, rare-earth nanoparticles aren't stable \u2014 they love to react with oxygen, which ruins their magnetic properties. In 2006, a team led by Ping Liu, a physicist at the University of Texas at Arlington, pioneered a manufacturing method that used steel balls to grind up magnetic material with the desired crystalline structure in a solution containing detergents. \"I had postdocs working for years on this before we got a publication,\" says Liu. \"They hated me.\" The soap lets the team produce nano-sized grains that don't adhere to each other but do keep their magnetic properties. Hadjipanayis is using the same technique, and says that in the past year he has made grains as small as 2.7 nanometres. Even more difficult is making a bulk magnet out of these grains. One standard technique \u2014 pressing the grains together and heating them to 800\u20131,000\u00b0C \u2014 causes them to diffuse into each other, so they become too big to create the cooperative nanocomposite effect. Another method \u2014 using polymer glues to bind the grains \u2014 dilutes the magnetic material. There are alternatives. Hadjipanayis plans to charge one set of nanoparticles positively and the other negatively, so that electrostatic attraction binds them together. Liu's group squeezes about half a gram of the nano-grains in a press for 30 minutes instead of the standard half a minute. He also adds a bit of warmth (about 500\u00b0C) to help them deform, but not so much as to ruin them. Using this method, Liu has managed to make relatively strong, dense magnets, but the grains aren't magnetically aligned, so the magnets are still weaker than a standard NIB one. Alignment is the final hurdle. Liu's group is trying to clear it by putting material through a second slow- compaction process, but is having limited success. The researchers are fiddling with the details, trying to hit on a recipe that works. \"I hope it can be done before my retirement,\" says Liu.  \n                Corporate competition \n              Liu could be beaten by his competition before he reaches that deadline. The technology firm General Electric, headquartered in Fairfield, Connecticut, has been given a $2.2-million ARPA-E grant to pursue nanocomposites, and has beefed up its magnetics research team. The company, which started its experimental work in January, told  Nature   that it has a good way to make crystalline grains, but it wouldn't give details. Last December, the US Department of Energy released its  Critical Materials Strategy , which outlines a three-part mission to deal with shortages in rare-earth elements: secure new supplies, promote recycling and conduct research into alternatives, such as next-generation magnets. This push toward stronger magnets is a welcome change with potentially big pay-offs, says Liu. According to his calculations, doubling the strength of a magnet in an electric car should improve the motor's efficiency by about 70% \u2014 although that number could vary wildly depending on the design of the magnet and engine. Although the United States seems to be making the most concerted push towards creating the strongest magnets, other nations have invested more money in general magnetics research, says Liu. China's 5-year economic plan for 2011\u201315 includes a big boost \u2014 reportedly more than 4 trillion renminbi (US$610 billion) \u2014 for spending in seven 'strategic emerging industries', including energy systems, clean cars and new materials. Observers such as Hatch and Liu expect great things from the investment. Japan has invested heavily in magnet research for its high-tech industry, and has strong government\u2013industry collaborations \u2014 although one of its largest centres for magnetics research is Tohoku University in Sendai, which was hit hard by the earthquake and tsunami in March (see   Nature  471, 420; 2011 ). Last year, the European Union's research-funding framework put out a \u20ac4-million (US$6.3-million) call for proposals from groups working to develop novel materials, with the goal of totally replacing rare earths. But most researchers say that this is massively overreaching. \"This is a joke, scientifically,\" says Liu of the quest to remove rare earths from strong magnets. Several major labs have had proposals rejected because they aimed simply to reduce the quantities of rare earths used in magnets, says Dominique Givord, a magnetics researcher at the Louis N\u00e9el Laboratory in Grenoble, France. Researchers' target of building next-generation nanocomposite magnets is, most admit, a long shot. \"I know that this activity is becoming popular in the United States, but I feel that their goal is a bit too ambitious,\" says Kazuhiro Hono, a magnetics researcher at the National Institute for Materials Science in Tsukuba, Japan. Givord agrees. \"It is extraordinarily challenging,\" he says. More realistic, he says, are attempts to make existing magnets a bit stronger and cheaper by altering their microstructures. In Japan, such efforts have helped to reduce dysprosium demand. But Hatch, who has worked in the field for nearly two decades, says that next-generation magnets are worth the battle. \"Yes, it is ambitious, but that's exactly why we need to be doing it,\" he says. \"It's time to put money behind it.\"\n Nicola Jones is a freelance journalist based near Vancouver, Canada. \n                     Blog: An elementary energy problem \n                   \n                     Blog: new year, new rare earth fear \n                   \n                     Blog: DOE issues 'critical materials' strategy \n                   \n                     Blog: The rare earth blues \n                   \n                     ARPA-E University of Delaware grant \n                   \n                     ARPA-E General Electric grant \n                   \n                     Terra Magnetica: a permanent magnet blog \n                   \n                     DOE Critical Materials Strategy \n                   \n                     William McCallum \n                   Reprints and Permissions"},
{"file_id": "471562a", "url": "https://www.nature.com/articles/471562a", "year": 2011, "authors": [{"name": "Mark Peplow"}], "parsed_as_year": "2006_or_before", "body": "Twenty-five years after the nuclear disaster, the clean-up grinds on and health studies are faltering. Are there lessons for Japan? The morning train from Slavutych is packed with commuters playing cards, browsing e-readers, or watching the monotonous flood plains pass by. It looks like any other routine journey to work. But rather than facing a crush through subway turnstiles at the end of the 40-minute trip, the workers are met by a row of full-body radiation monitors. It is the start of another day at the Chernobyl power plant, the site of the world's worst civilian nuclear disaster. As the train trundles through the bleak Ukrainian countryside, another nuclear crisis is unfolding halfway around the world. Barely a week after the partial meltdown at the Fukushima Daiichi nuclear power station, it is no surprise that some of the chatter on the train turns to the incident there. \"It looks bad,\" says one commuter. \"But not as bad as Chernobyl,\" he adds, with a hint of grim pride.  \n               boxed-text \n             When Chernobyl's reactor number 4 exploded in the early hours of 26 April 1986, the ensuing blaze spewed 6.7 tonnes of material from the core high into the atmosphere, spreading radioactive isotopes over more than 200,000 square kilometres of Europe (see  'The hottest zone' ). Dozens of emergency workers died within months from radiation exposure and thousands of children in the region later developed thyroid cancer. The region around the plant became so contaminated that officials cordoned off a 30-kilometre exclusion zone that straddled Ukraine's border with Belarus. Today, a staff of about 3,500 enters the zone each day to monitor, clean and guard the site, where remediation work will continue for at least another 50\u00a0years (see  'Half-life of a disaster' ). So far, the Fukushima accident is less severe. Radiation levels measured near the Japanese power plant have been less than those at Chernobyl after the blast there (see ). And although radiation has spread from Fukushima, it does not match the amounts that rained down in the region around Chernobyl.  Despite those differences, the quarter-century of work following the Chernobyl disaster will offer some important lessons for Japan as the nation begins to assess the health and environmental consequences of Fukushima. The problems that followed Chernobyl also provide a grim reminder about the value of accurate information. Officials need to tell people immediately how to avoid the initial, most dangerous, exposure; yet in the longer term, scientists and the government must battle against unnecessary concern over low-level doses of radiation, which often causes more harm than the radiation itself.  In some ways, the connection between the two accidents may yield the biggest benefits for Chernobyl. For a brief window of time, the world has again focused attention on the largely overlooked work there. The renewed interest may spur nations to chip in the cash needed to complete the clean-up of the site, and to carry out health studies that have languished for want of proper coordination and funding. \"In recent years, Chernobyl has been neglected by funding agencies and, to an extent, the scientific community,\" says Jim Smith, a radioecologist at the University of Portsmouth, UK, who has studied the consequences of the accident for 20 years. \"But there is still more to learn from Chernobyl about decommissioning and the effects of the radiation,\" says Smith, who is touring the site with a group of other scientists. After clearing a security checkpoint, the visiting researchers board a bus that heads towards the heart of the ageing power plant. They pass abandoned buildings and bump along potholed roads running beneath archways made of piping; since the accident, pipes have been laid above ground to avoid disturbing contaminated soil. The visitors stop to look at the most visible reminder of the accident, the concrete sarcophagus that entombs the shattered reactor building. Completed hastily in November 1986, the sarcophagus was built to contain the escaping radiation, but it is now crumbling and streaked with rust. Smith whips a dosimeter out of his rucksack and poses for a photograph in front of the sarcophagus. The reading is 5\u2009\u00b5Sv\u2009h \u22121 : about 10 minutes of exposure at that level equals the same dose as an arm X-ray. The plant's bright main office is a stark contrast to the sarcophagus. Stained-glass windows depict \u2014 in glorious socialist\u2013realist style \u2014 the harnessing of atomic energy. But the plant has not produced power since 2000, when the last reactor was shut down. Valeriy Seyda, a deputy director of the Chernobyl Nuclear Power Plant, explains that the plant's top priority now is to construct a new confinement shelter for reactor 4 before the sarcophagus becomes too unstable. If it collapses before the new shell is in place, it could throw up a cloud of radioactive particles and expose the deadly remnants of the reactor.  \n                Replacing the rusting tomb \n              The plan is to build an enormous steel arch adjacent to the reactor and slide it along a runway to cover the building. The arch will reach 105 metres high, with a span of 257 metres \u2014 the world's largest mobile structure, according to its designers. It is expected to be in place by 2015 and should last for 100 years. It will enable robotic cranes inside to dismantle the sarcophagus and parts of the reactor. Long-term plans call for finishing the clean-up work at Chernobyl by 2065. Some of the concrete trenches for the project are in place. But the international Chernobyl Shelter Fund that supports the US$1.4-billion effort still lacks about half of that cash, and the completion date has slipped by almost ten years since the shelter plan was agreed in principle in 2001. One of the key goals of a forthcoming conference \u2014 Chernobyl, 25 Years On: Safety for the Future \u2014 to be held in Kiev on 20\u201322 April is to secure more cash commitments from international donors. Meanwhile, Chernobyl is developing long-term storage facilities for the debris that will be hacked out of reactor 4; and for more than 20,000 spent fuel canisters from the site's other reactors, a facility that will cost about \u20ac300 million (US$420\u00a0million). Although all those reactors have been shuttered, the plant continues to generate large amounts of radioactive waste \u2014 partly because of persistent flooding in some of the waste-storage buildings and reactor\u00a04's turbine hall. Every month, at least 300,000 litres of radioactive water must be pumped out of the structures and stored on site.  The main cause of this flooding is Chernobyl's brimming cooling pond, which artificially elevates groundwater levels in the area. Alexander Antropov, a Chernobyl veteran with ice-blue eyes and a cool manner to match, is in charge of a project to decommission the pond. The term 'cooling pond' usually refers to the containers where spent fuel rods are stored until their radiation dissipates enough that they can be put into long-term storage. But Chernobyl's pond is actually a vast reservoir covering 22\u2009square kilometres into which water from the reactor cooling systems was discharged. The pond also contains long-lasting radioactive material such as caesium-137 and strontium-90, which rained down after the explosion. Besides causing flooding at the plant, the high water levels in the cooling pond raise the risk that a weak dyke along its east side will burst, which would send water coursing into the Pripyat River. Radioactivity in the escaping water would be quickly diluted by the river, so although it would not significantly raise exposure levels for people downstream, it could cause panic among the local population.  Antropov says that his team cannot simply lower the water levels in the pond because they don't know what effect microscopic radio\u00adactive sediment particles would have if exposed. In the meantime, the team maintains the status quo by pumping water from the Pripyat River into the pond at a cost of a few hundred thousand euros per year. But the long-term plan is to lower the water level by 7 metres to form a patchwork of 10\u201320 smaller ponds that would keep the most dangerous sediments in place. The project would cost \u20ac3\u00a0million to \u20ac4\u00a0million, says Antropov. He is already in discussions with the relevant regulators and is optimistic that the necessary feasibility studies and environmental impact assessments can be completed. But the effort has been a long time coming. The decommissioning plan is more than a decade old, and was supported by a 2005 survey for the European Commission, led by Smith. Once again, money has been a key factor in the delay. The major parts of Chernobyl's decommissioning plan are paid for by international funds, but the cooling pond project is not. Nor is the research needed to satisfy the regulators. \"Most of our own activities come from the Ukrainian budget, and we are not a rich country,\" says Seyda.  After leaving the cooling pond, the visitors stop at Pripyat, an abandoned town just 3 kilometres from the reactor complex. Some 44,000 residents were evacuated the day after the accident, and many of their belongings still litter the decaying buildings. Antropov once lived here \u2014 his daughter was a few months old at the time of the accident \u2014 and as deputy chief of the town's Communist party office, he was responsible for evacuating part of the town. Because he worked as a senior engineer at the nuclear plant, he knew that the disaster would have repercussions for decades to come. \"I understood that I would never return to live in Pripyat,\" he says, in an uncharacteristically soft voice. \"I still feel some sense of loss.\" The evacuees from Pripyat also live with lingering fear about the radiation they were exposed to before fleeing their homes. Along with millions of others from the surrounding regions, they often attribute any sign of ill health to the accident. But pinning down Chernobyl's true public-health impact has proved remarkably difficult. There is little disagreement about the terrible fate of the workers who brought Chernobyl's stricken reactor under control. Of 134 emergency workers diagnosed with acute radiation sickness, 28 died from their exposure within four months. Another 19 have died since from various causes, and many of the surviving workers now have cataracts and skin injuries. More than 5,000 cases of thyroid cancer have so far been seen in people who were children at the time of the accident and lived in contaminated areas of the former Soviet Union \u2014 a more than ten-fold increase from normal levels (adults were mostly unaffected by the disease). Most of these cases were caused by drinking milk contaminated with radioiodine. Fewer than 20 of these people have died, but the sheer number of cancers, and their rapid onset within 5 years of the accident, surprised many epidemiologists.  This triggered a plethora of thyroid studies, most notably a long-term cohort study of 25,000 people in Ukraine and Belarus who were children in 1986 that is being coordinated by the US National Institutes of Health's National Cancer Institute (NCI) in Bethesda, Maryland. The latest results from the Ukrainian section of this cohort 1  confirm previous findings that the incidence of thyroid cancer is proportional to the size of the dose, with a particularly high risk seen in younger people and in those who were iodine-deficient due to poor diet. The research is having a direct impact in Japan, where those at risk of exposure are being given potassium iodide tablets to prevent the uptake of radio\u00adiodine in their thyroid.  The NCI oversees a second cohort made up of liquidators, a group of more than half a million people sent into the exclusion zone to help clean up and monitor the area after the initial emergency phase of the accident. Liquidators have a slightly raised risk of developing cataracts, and possibly a small increased risk of leukaemia 2 .  \n                Long-term effects \n              But what was the impact on the wider population? Various studies have tried to estimate how many deaths Chernobyl will eventually cause across the whole of Europe, but their answers range from a few thousand to hundreds of thousands 3 . Cancer causes about a quarter of all deaths in Europe, so teasing out Chernobyl's far-reaching influence would probably be impossible, say epidemiologists. Moreover, focusing on such intangible numbers can distract from the much broader social impact of the accident. In Ukraine and Belarus, hit hard by the break-up of the Soviet Union in 1991, lingering fears about radiation are thought to have contributed to a sense of hopelessness that is linked to high rates of alcoholism and smoking \u2014 factors that have a much bigger health impact. \"There's tremendous uncertainty for these people,\" says Elisabeth Cardis, a radiation epidemiologist at the Centre for Research in Environmental Epidemiology in Barcelona, Spain. \"Some think they are doomed because of their radiation exposure.\" Further research could provide convincing evidence that Chernobyl's radiation did not significantly harm the wider population, but \"we won't know unless we look\", says Dillwyn Williams, a cancer researcher at the Strangeways Research Laboratory in Cambridge, UK. A handful of Chernobyl studies have found small increases in rates of breast cancer and cardiovascular disease, but they did not properly account for confounding factors, such as nutrition, alcohol consumption and smoking habits. And although some researchers have claimed to see an increase in genetic mutations in the children of parents irradiated after Chernobyl 4 , there has been no similar evidence of hereditary effects even in the children of Japanese atomic bomb survivors, who on average received much larger radiation doses. This means that there is still a substantial gap in the overall understanding of Chernobyl's health effects, says Williams. The problem is exacerbated by the piecemeal nature of previous studies. \"There has been a failure of European-level coordination on this,\" he says.  Williams hopes that there is now a chance to establish a Chernobyl Health Effects Research Foundation, which would mirror the highly effective Radiation Effects Research Foundation that monitors the long-term health impacts of the atomic bombs in Japan. Together, the efforts could reveal the differences between the single short-term dose of external radiation delivered by the atomic bombs, and the low-level long-term exposure seen after Chernobyl. Long-term doses were once thought to carry much less risk than the immediate exposure, but evidence is accumulating that the risks may be much the same 5 . If confirmed, it would mean that people routinely exposed to low-level radiation have a greater chance of health problems than previously thought. The European Commission has funded Williams, Cardis and a core group of other scientists to develop a research plan, dubbed the Agenda for Research on Chernobyl Health (ARCH), that maps out how the existing cohorts could be used to study a wider range of diseases, such as breast cancer and cardiovascular disease, and to address the questions about the long-term effects of low doses. The liquidator cohort, for example, is six times larger than that of atomic bomb survivors, with a much wider range of exposure doses. It could show how risk varies over that large range of doses and uncover rarer effects at lower doses. It could also help to reassess the threshold dose to prevent nuclear workers from developing problems such as cataracts. ARCH also suggests testing the feasibility of setting up new cohorts including liquidators' offspring and highly exposed evacuees, along with a tissue bank. The bank may reveal whether people's genetic make-up influences their susceptibility to radiation \u2014 key information for determining how individuals are likely to respond to the radiation received during medical procedures such as X-ray scans and radiation treatment. There are several hurdles, however, to getting ARCH off the ground. The project needs support from the NCI, which stopped funding active clinical monitoring of the thyroid cohort in 2008 because of budgetary constraints. And ARCH's proposals would also require better access to medical records in Ukraine and more information about participants' lifestyle factors \u2014 both potentially tall orders.  The ARCH plan will be presented at the 25th anniversary conference in April, and Cardis hopes that a positive reception will prompt the European Commission to boost its support. It is likely to be difficult to secure a long-term commitment for the studies, which will cost about \u20ac3\u00a0million to set up, but that cost is minor compared with the billions that will be spent on remediation at Chernobyl, says Williams.  Beyond obtaining the necessary funds, researchers will also require cooperation from participants to expand the cohort studies. That could be difficult. Gennady Laptev, now a hydrologist based at the Ukrainian Hydrometeorological Institute in Kiev, was a liquidator for three years, and says that he stopped attending his medical check-ups about ten years ago because they were too time-consuming. \"They never found any major health problems,\" he says.  Laptev's work involved flying by helicopter from Kiev to Chernobyl twice a week to take radiation readings and collect soil and water samples for analysis. \"Nobody forced me to do the work \u2014 I did it because it was interesting, and I really enjoyed it,\" he says. But after three years, he became worried about the risk of working near the plant, so he took a job researching how radioisotopes dispersed in the local water system (see  'Life as a liquidator' ). Concerns about radiation exposure continue to plague residents in the region, and the planned studies could provide the answers they so desperately need about Chernobyl's real health legacy. \"I have a house in a village near Slavutych, on contaminated territory,\" says Antropov during the site visit. \"Two of my neighbours died of cancer, and this was probably the result of their radiation doses.\"  \n                Lessons for Japan \n              It's too early to say how the Chernobyl health studies will help those affected by the Fukushima accident. But Chernobyl has already given the world a lasting lesson on the importance of clear communication during a nuclear disaster, and in the years afterwards.  There was no systematic distribution of prophylactic potassium iodide to the people around Chernobyl, and Pripyat's children were allowed to play outside during the day after the accident, while the reactor continued to burn. \"The failure to rapidly communicate radiation risks at Chernobyl led to people receiving higher radiation exposures than was necessary,\" says Smith. The Japanese government has been lambasted for not keeping citizens well informed about the accident there. But it was swifter to act than Soviet officials were, ordering the evacuation of people who live near the plant within hours of recognizing the growing nuclear emergency, and expanding that evacuation zone to a radius of 20\u2009kilometres the following day. As well as distributing potassium iodide, the Japanese government banned the sale of food and milk produced in the provinces around the stricken plant. \"The Japanese have done exactly the right thing,\" says Andrew Sherry, director of the Dalton Nuclear Institute at the University of Manchester, UK. Ultimately, says Smith, Chernobyl's most important lesson for Fukushima is that a nuclear accident haunts a region long after the reactors have cooled. If areas of Japan are significantly contaminated with radioactive caesium-137, which loses half its radioactivity in 30 years, the government may have to maintain an exclusion zone for decades. Decommissioning the Fukushima reactors may also take decades, depending on the extent of damage to their cores. And the uncertainty surrounding the health risks may exact a psychological toll that could surpass the physical harm from the radiation, adds Smith. Many of the workers at Chernobyl understand those lessons all too well as they shuffle onto the train to Slavutych at the end of their day. The workers will return to tend to the plant tomorrow and the next day \u2014 and for many years to come. Mark Peplow is Nature's news editor. \n                     Japan's nuclear crisis \n                   \n                     Chernobyl slideshow \n                   \n                     Chernobyl Nuclear Power Plant \n                   \n                     ARCH proposal \n                   \n                     UNSCEAR report \n                   \n                     Health Risks from Exposure to Low Levels of Ionizing Radiation (BEIR VII) \n                   \n                     Chernobyl, 25 Years On: Safety for the Future conference \n                   \n                     NCI: Chernobyl Research \n                   \n                     Ukrainian Hydrometeorological Institute \n                   Reprints and Permissions"},
{"file_id": "471566a", "url": "https://www.nature.com/articles/471566a", "year": 2011, "authors": [{"name": "Sharon Weinberger"}], "parsed_as_year": "2006_or_before", "body": "Can computational social science help to prevent or win wars? The Pentagon is betting millions of dollars on the hope that it will. If George Clooney stars in a movie, will it be a hit? Or will it flop, like his 2008 comedy  Leatherheads ? That question, at least in broad outline, made its way to Ken Comer, deputy director for intelligence at the Joint Improvised Explosive Device Defeat Organization (JIEDDO) of the US defence department, and the man at the centre of the US military's war on roadside bombs. He recently made the time for a briefing by scientists from the US Department of Energy, who were honing their modelling skills by working with a film studio on the formula for a successful blockbuster. Comer listened to them describe how they had analysed and reanalysed the data that Hollywood vacuums up about its audiences, slicing the results in every way they could think of, only to come to the same conclusion every time: you can't tell. \"You can dress George Clooney up,\" recalls Comer, \"you can dress him down, you can put a beard on him, and yet there's no reliable method for predicting whether or not a George Clooney movie is going to be a blockbuster.\" And that, says Comer, is a perfect illustration of why the Department of Defense (the Pentagon) is rethinking its data-mining approach to the problem of roadside bombs \u2014 not to mention counter-insurgency and other aspects of warfare. \"I speak as one who has been swimming in data for three years,\" he says, referring to the reams of information that the department has compiled about roadside bombs after nearly a decade of war in Iraq and Afghanistan: \"Data is not our solution to this problem.\" Instead, Comer and other officials are placing their bets on a new generation of computer models that try to predict how groups behave, and how that behaviour can be changed. This work goes under a variety of names, including 'human dynamics' and 'computational social science'. It represents a melding of research fields from social-network analysis to political forecasting and complexity science. Figures on total funding for this work are difficult to come by. But one of the field's major supporters, the Office of the Secretary of Defense, is planning to spend US$28 million on it in 2011, almost all on unclassified academic and industrial research. And separate computational social-science programmes are being funded by bodies such as the Defense Advanced Research Projects Agency (DARPA), the Defense Threat Reduction Agency and the research arms of the Army, Navy and Air Force. The Pentagon's embrace of this work has been so enthusiastic that some scientists have urged a slow-down, for fear that such nascent models will be pushed into operation before they are ready. In their current state of development, says Robert Albro, an anthropologist at American University in Washington DC, the models are often a waste of time. \"I am not saying that computational social science is voodoo science,\" says Albro, a member of the American Anthropological Association's Commission on the Engagement of Anthropology with the Security and Intelligence Communities. \"I'm saying that voodoo science is all too frequently being generated from the work of computational social science.\"  \n                Cloudy, with an 80% chance of war \n              One often-cited inspiration for the current modelling work is an episode in 2003, when coalition forces in Iraq were searching in vain for deposed dictator Saddam Hussein. With conventional methods leading nowhere, a group of US Army intelligence analysts decided to aggregate the available information about Saddam's social network using a link diagram to depict relationships. As they factored in key variables such as trust, the analysts began to realize that the most-wanted government officials \u2014 those pictured on the 'personality identification playing cards' that had been widely distributed among US troops \u2014 were not necessarily the people whom Saddam trusted most, and were thus not likely to know where he was hiding. Instead, the diagram led the analysts to focus their attention on trusted lower-level associates \u2014 including one key bodyguard, whose information led the trackers to the dictator's underground hideaway on a farm near Tikrit. Today's simulations are similar in concept, but with one crucial difference: the Army analysts' diagram was static, constructed by hand and analysed manually. Now the goal is to do all that with algorithms, using computers to integrate vast amounts of data from different sources, and then to keep the results up to date as the data evolve. A prime example of a system that creates such models is the Organization Risk Analyzer (ORA): a 'dynamic network analysis' program devised by Kathleen Carley, a computer scientist at Carnegie Mellon University in Pittsburgh, Pennsylvania, who has emerged as a leading figure in Pentagon-funded computational social science. \"We build the psychohistory models,\" Carley jokes, referring to the 'science' of group behaviour invented in the 1940s by the science-fiction author Isaac Asimov for his classic Foundation novels. \"We are the Foundation!\" \n               Click here for larger image \n               To create an ORA model for a politically unstable region such as Sudan, explains Carley, she uses her own program, AutoMap, to trawl through publicly available news reports and automatically extract names and other key data (see  'The conflict forecast' ). The ORA might then use that information to identify people \u2014 or, in the lexicon of social-network analysis, nodes \u2014 with a high degree of 'betweenness', meaning lots of direct connections to other people within the network. These individuals \"are often those that are considered influential because \u2026 they broker information between groups and so on\", says Carley. The same types of model can be used to predict how a terrorist ideology might catch on in the local population and propagate from person to person like a spreading virus. Carley's system can factor in cultural variables, using records of the opinions and attitudes that tend to prevail among specific ethnic groups. The goal, she says, is to produce an effective strategy for stopping the epidemic of radicalization or for destabilizing the terrorist networks by identifying key individuals and groups to be targeted with diplomatic negotiation or military action. Another example is the Spatial Cultural Abductive Reasoning Engine (SCARE) developed by Venkatramanan Subrahmanian, a computer scientist and co-director of the Laboratory for Computational Cultural Dynamics at the University of Maryland in College Park. Subrahmanian says that SCARE was able to predict the locations of arms caches in Baghdad to within half a mile, using a combination of open-source data on past roadside bomb explosions and constraints based on distance (terrorists didn't want to carry their explosives very far for fear of getting caught) and culture (most of the attacks that they tracked came from Shiite groups with ties to Iran, so the caches were probably not in Sunni neighbourhoods). Subrahmanian says that he has given copies of the program to the military, and \"they're clearly trying it out\". Can a model predict a war? The Integrated Crisis Early Warning System (ICEWS) is being developed with DARPA funding by university researchers working with defence giant Lockheed Martin. A revival, at least in part, of a more primitive, 1970s-era DARPA forecasting project, the current incarnation of the ICEWS focuses on predicting political events such as insurgency, civil war, coups or invasions. The system draws its data mainly from the Reuters online news feed, and combines them with models incorporating previous behaviour of ethnic or political groups, economic factors such as a country's gross domestic product and geopolitical relationships with neighbouring countries. The result is an ICEWS forecast that might predict, for example, that 'Country X has a 60% probability of civil war'. The ICEWS has been producing monthly predictions since March 2010, says Sean O'Brien, programme manager for the effort at DARPA. He believes that such models, although imperfect, are already nearing the point at which they can be useful for military leaders. O'Brien has considerable company elsewhere in the Pentagon: the Office of Director of Defense Research and Engineering, for example, is sponsoring its own programme in Human, Social, Cultural and Behavior modelling. And although the office did not provide details, it says that some of its simulations are already being used by the US Special Operations Command and the US Africa Command.  \n                A generation away \n              Even among researchers working on models with Pentagon funding, there is concern that such enthusiasm may be premature. It seems, for example, that neither computer models nor human analysts were able to precisely predict this year's uprisings in the Middle East. When it comes to prediction, \"I would say the weather guys are far ahead of where we are\", says Subrahmanian, who notes that meteorologists are frequently accused of being wrong as much as they are right. \"And that might give you some relative understanding of where the science is.\" Carley points to the pitfalls of automated data collection. \"One of the issues,\" she says, \"is that you will get people who are \u2026 talked about as part of the networks who aren't technically alive.\" In the ORA model for Sudan, for example, the textual analysis resulted in a network in which one of the key individuals was Muhammad \u2014 the Islamic prophet who died in AD 632. \n               Click here for larger image \n               Albro, who has reviewed a number of computational social-science models as part of the US National Research Council's Committee on Unifying Social Frameworks, worries that much of the work is being done by computer scientists, with only token input from social scientists, and that minimal attention is being paid to where the data come from, and what they mean. He points to some models that look for signs of extremist violence by tracking phrases such as \"blow up\" in online social-media discussions. \"There's the constant implication that discursive violence adds up to real violence, and that's crazy,\" he says. Robert Axtell, a computational social scientist at the Krasnow Institute for Advanced Study at George Mason University in Fairfax, Virginia, and a pioneer of agent-based modelling, argues that there simply aren't enough accurate data to populate the models. \"My personal feeling is that there is a large research programme to be done over the next 20 years, or even 100 years, for building good high-fidelity models of human behaviour and interactions,\" he says. Similar notes of caution can be heard within the defence department. \"We're at the very beginnings of this,\" says John Lavery, who manages a programme of complex-systems modelling at the Army Research Office in Research Triangle Park, North Carolina, and who compares the current state of computational social science with physics in the early nineteenth century. \"It's a tool, and if you can leverage it, that's great,\" agrees Brian Reed, a behavioural scientist at the Network Science Center of the US Military Academy at West Point, New York, who was a key architect of the network analysis that led to Saddam's capture. \"But you can get too much information,\" he warns, \"and someone has to provide a focus.\" Reed cites an example from his own return to Iraq, where he was deployed from 2008 to 2009 in the province of Diyala. Wanting to stop roadside bomb attacks, he asked his intelligence organization for a network analysis of the insurgent network. They provided an overload of data. \"What they crunched, no one at our end could understand,\" says Reed. Critics such as Albro worry that too many researchers are unaware of the real limitations of their work. Many of the models that Albro has seen focus on verification \u2014 ensuring that the simulations are internally consistent \u2014 but give short shrift to validation, or making sure that they correlate to something in the real world. The models might provide an interpretative tool that allows policy-makers or military leaders to think critically about a problem, he suggests, but the technique's limitations are sometimes overlooked. \"It does not answer our questions for us,\" says Albro. \"It does not solve that dilemma of what decision I need to make.\" Indeed, it is often far from clear whether the current generation of models is telling people anything that an expert in the relevant subject wouldn't already know. Carley recalls a conference at which she presented her results about the key individuals whom her ORA model had identified in Sudan. \"Yeah,\" came the response from the regional specialists in the audience, \"we kind of knew most of this.\" For all the caveats, however, the need to help soldiers on the ground carries an acute sense of urgency back at JIEDDO headquarters. \"We have a few instances of models that have docked with the data successfully,\" says Comer, citing an agent-based simulation of the Iraqi city of Samarra, which was funded by JIEDDO. \"The big magic trick is to move those models to a point where they can be predictive.\" The model of Samarra was able to match specific changes in US military strategy to decreases or increases in the incidence of roadside bombs, but it was specific to that city. The researchers \"did a great model and it was really useful\", says Comer. \"Just as soon as they delivered it we said, 'Gee, thanks. Now you'll have to rewrite that for Afghanistan.'\" Comer acknowledges the irony that as the world's most technologically advanced military spends tens of millions of dollars on sophisticated computer tools to predict insurgent behaviour, the insurgents in question are busy building crude bombs with little more than fertilizer and basic electronics. \"The enemy is holding his own,\" says Comer, \"not only without the data, but without the computer power, without the Internet, without the databases \u2014 and without the science.\" Sharon Weinberger is an Alicia Patterson Foundation Fellow based in Washington DC. \n                     US defence department on human social cultural behavioural modelling \n                   \n                     Office of Naval Research human social cultural behavioural sciences \n                   \n                     George Mason University Krasnow Institute for Advanced Study \n                   \n                     Carnegie Mellon University, computational analysis of social and organizational systems \n                   \n                     University of Maryland Laboratory for Computational Cultural Dynamics \n                   \n                     Network Science Center at West Point \n                   Reprints and Permissions"},
{"file_id": "472280a", "url": "https://www.nature.com/articles/472280a", "year": 2011, "authors": [{"name": "Alison McCook"}], "parsed_as_year": "2006_or_before", "body": "Fix it, overhaul it or skip it completely \u2014 institutions and individuals are taking innovative approaches to postgraduate science training. \"Most of them are not going to make it.\" That was the thought that ran through Animesh Ray's mind 15 years ago, as he watched excellent PhD students \u2014 including some at his own institution, the University of Rochester in New York \u2014 struggle to find faculty positions in academia, the only jobs they had ever been trained for. Some were destined for perpetual postdoctoral fellowships; others would leave science altogether. Within a few years, the associate professor was in a position to do something about it. A stint in a start-up company in California had convinced him that many PhD graduates were poor at working in teams and managing shifting goals, the type of skills that industrial employers demand. So he started to develop a programme that would give students at Keck Graduate Institute (KGI) in Claremont, California, these skills. \"I was determined not to have to keep watching scientists struggle to find the jobs they were trained to do.\" Ray is one of a number of researchers and administrators who are attempting to reshape graduate training. They want to save young scientists from falling into the postdoc holding pattern or taking jobs below their station. Here,  Nature   presents five approaches to shaking up the hallowed foundations of academia. They range from throwing scientists deep into independent study, to going interdisciplinary, to forgoing the PhD altogether. 1 Jump in at the deep end | 2 Forget academia |3 Trample the boundaries | 4 Get it online | 5 Skip the PhD  \n                1 Jump in at the deep end \n              For Michael Lenardo, a molecular immunologist at the US National Institutes of Health (NIH) in Bethesda, Maryland, the thought process went like this: When too many scientists are looking for too few academic positions, PhD programmes need to admit the students most likely to succeed, and provide them with all the skills they'll need. And neither the United States nor the United Kingdom seemed to be getting the mix exactly right. In the United Kingdom, PhD students are given independence early, and degrees rarely last more than 4 years. But not all institutions require that students publish a first-author paper, which Lenardo sees as a drawback. US science degrees often do require first-author papers, but have ballooned to more than 7 years in duration. In 2001, Lenardo created a new degree programme, called the  NIH Oxford-Cambridge Scholars Program , that would combine the best elements of each system for a cadre of truly elite students. It admits just 12 of the 250\u2013300 applicants per year. Independence is stressed \u2014 students devise and write their own project plan, begin their thesis work immediately, and skip the uniform coursework \u2014 but they must meet requirements such as authoring papers. Students split their time between the United States and the United Kingdom, and have at least two mentors, one in each country (and often in different disciplines). Because no adviser has full control, students learn how to operate independently, says Lenardo. Travelling to another country reinforces that autonomy, and ensures that the students work with the best people in their field, he says. In the ten years since the programme's inception, more than 60 students have graduated, taking slightly more than 4 years apiece. They published an average of 2.4 first-author papers out of their PhD research. Eighty percent of graduates are still in academia, and half a dozen are already working as principal investigators. Ambika Bumb, now a postdoc at the National Cancer Institute in Bethesda, spent her PhD developing a nanoparticle with magnetic, optical and nuclear properties that might one day aid in imaging tumours and delivering targeted therapies. She finished in just three years, had four advisers in two countries and received training in engineering, immunology, radiochemistry and radiology. She published at least four scientific papers and one review article from her PhD research, and she is now applying for faculty positions. Developing independence is a crucial step to becoming an investigator, says Richard Hetherington, a postgraduate-skills development coordinator at Newcastle University, UK. \"Having that will make them stronger when they get to the end,\" he says. But a lack of structure and core coursework could leave some students unprepared, says Nathan Vanderford, who manages a grant and manuscript development office at the University of Kentucky in Lexington, and has written about career issues in science. \"I don't see that you'd get the depth of the history [of science], and the central core principles, strictly in a lab setting.\" Some students may struggle.  \n                2 Forget academia \n              Ray's experiences encouraged him to think more about non-academic training for PhDs. Many institutes, including KGI, had already embraced Professional Science Master's (PSM) programmes as a way to stock the ranks of industry and keep training scientists, but Ray found that these degrees could limit students' opportunities. He watched as graduates of KGI's Master's of Bioscience often started as an assistant to a consultant, or a mid-level manager, then advanced from there. They did well, but typically remained in the management side of a company, separate from the science. So Ray worked with David Galas, a KGI co-founder, and Sheldon Schuster, the institute's president, to extend the PSM's reach and develop a PhD programme that would provide students with both industry know-how and technical research training. To complete a  PhD in Applied Life Sciences at KGI , students must first complete the master's course there, then spend three to four more years doing original research, with at least one adviser from industry. Eric Tan, the first graduate of the programme, spent his PhD at KGI developing a DNA chip that might have applications in diagnostics or assessing biological threats. He learned not only the scientific method, but also how to write a business plan and present it to venture capitalists, how to carry out market research and the ins and outs of patent legislation. Courses in marketing and communication are useful for any scientist, even those who stay in academia, says Vanderford. \"Regardless of the career path a PhD would take, having those courses would be helpful.\" Time will tell if it is working. Ray is inspired by the success of KGI's PSM programme, which has seen nearly all of its 300 graduates find jobs since it started in 2000. Since the PhD programme began in 2006, three students have earned their degrees, and each has found a job earning more than the median starting salary for the PSM students (US$73,000). It is a result that Ray calls \"astounding\". Ray says he hopes that the rounded training will give his students the ability to manage scientists and interact with business people. \"They can see and appreciate the big picture; at the same time, they are well-versed in the technological depth for which they will be valued.\" But well-rounded students may have some dull edges, and Ray acknowledges that KGI cannot provide coursework in specific areas such as physical chemistry or cell biology. It will be an \"ongoing process to try to figure out the balance between how much detailed science courses you need versus how much professional development you need\", says Vanderford.  \n                3 Trample the boundaries \n              Marc Jacofsky was working on a PhD in physical anthropology at Arizona State University (ASU) in Tempe when his brother, an orthopaedic surgeon, told him about all the questions he wanted to investigate in movement and artificial joints. Jacofsky remembers interrupting his brother with a few suggestions: \"He looked at me and he said, 'I thought you studied monkeys.'\" Jacofsky did study monkeys \u2014 but also engineering, mathematics, computer science, kinesiology and neurophysiology. He was enrolled in a new programme developed by ASU faculty members from a wide range of departments, an attempt to go beyond interdisciplinary studies and instead create entirely new disciplines. Nearly every new PhD programme at ASU is designed to be \"transdisciplinary\", says Maria Allison, dean of the graduate college. Other examples include Human and Social Dimensions of Science and Technology, Biological Design and Urban Ecology. Some degrees involve more than 80 faculty members, because of the range of topics covered. The initial funding for Jacofsky's programme, called Neural and Musculoskeletal Adaptations in Form and Function, and some of the other ASU degrees came from a National Science Foundation project known as IGERT, or  Integrative Graduate Education and Research Traineeship . IGERT provides US$3-million 5-year grants to US institutions to develop programmes that help students to gain career skills and tackle real-world problems. Since 1998, the IGERT programme has funded nearly 5,000 graduate students. An independent survey found that IGERT students are better able than their non-IGERT peers to work in multidisciplinary teams and to communicate with non-experts, without sacrificing expertise in their chosen area. There is even some indication that IGERT graduates have an easier time finding a job. Similar interdisciplinary programmes are starting up elsewhere. The Canadian government has an initiative called the Collaborative Research and Training Experience Program, and a new PhD course in Bangalore, India, trains engineers, chemists, computer scientists and physicists in interdisciplinary life sciences, teaching them to use the tools of physical science to tackle biological problems. Started around five years ago by physicists at the National Centre for Biological Sciences, the Interdisciplinary Biology, or iBIO, programme has graduated eight students. Two are already tenure-track faculty members. It is good to expose trainees to different fields, but specialization is still important, says Hetherington. The purpose of a PhD is to provide a \"deep understanding of a specific area\". Even cross-disciplinary research consists of scientists who contribute specific skills from their particular fields, he says. Broadening the scope of a programme has advantages, however. It teaches students about their options. Jacofsky had entered his degree thinking he would one day teach university-level anthropology. Instead, he is vice-president of research and development at the at the Center for Orthopedic Research and Education, or CORE Institute, in Phoenix, Arizona, co-owned by his brother. Jacofsky studies biomechanics and gait before and after orthopaedic procedures. \"If I'd done a traditional anthropology degree, I think there's an incredibly small chance I'd be working in industry.\"  \n                4 Get it online \n              Some potential postgraduate students do not have the flexibility to commit to full-time studies, or to travel to a lab. Online training aims to fill this gap and provide more individuals with appropriate training, even at the PhD level. Rana Khan started teaching an online course initially out of curiosity \u2014 she didn't understand how it would work. \"I was fascinated by the whole idea,\" she says. \"How do you do it?\" At the time, she was a postdoc at the US Department of Agriculture, investigating how to make soya beans more resistant to pathogens. She wanted teaching experience, and saw a job listing at the University of Maryland University College in Adelphi. The job was to teach part of an online biotechnology Master's degree. The college had set up an online classroom, where Khan posts weekly lectures, and students are required to complete assignments and participate in discussions throughout the week. At least once a day, Khan checks in, answering students' questions. At the end of the programme, students do an online internship, in which they do group projects for real companies \u2014 investigating, for example, potential competitors with a new technology \u2014 and submit 100\u2013200 page reports. There is no lab component, but there could be, says Khan, who directs the programme, now a PSM: students could simply work at a nearby lab and submit their data online, she says. The college's programme has been around since 2001 and now graduates approximately 50 students a year. Roughly 10% live outside the United States. That's a big advantage of online degrees, Khan notes \u2014 some of her current students are members of the military, stationed in Afghanistan and Iraq. One graduate is Kyle Retterer, who started a PhD in physics. After realizing he didn't want to spend years focusing on a narrow area in semiconductors, he abandoned academia. When he began to miss research, he looked for programmes that tackled cutting-edge problems and let him do what he had always loved \u2014 analyse huge amounts of data. His mother had completed two online degrees in information technology and is now a vice-president at Nasdaq, so he saw the potential in distance learning. He graduated in two years, and two months later had a job at GeneDx, a clinical genetic-testing company in Gaithersburg, Maryland, analysing data from multi-gene tests. He now makes three to four times what he was making as a graduate student. \"I feel like I'm in pretty good shape.\" Even a PhD is possible from a distance.  The Open University , which is headquartered in Milton Keynes, UK, now has about 40 part-time science PhD students. They work locally, conducting research at a local astronomy lab, for example, then are expected to check in every two weeks via Skype \u2014 or sometimes in person \u2014 with supervisors, usually at the university's main campus. \"That can be just as rewarding\" as having a supervisor on-site, says James Bruce, who manages the university's science PhD students. Online PhDs are a rarity, but that could change, speculates Hetherington. Science isn't done in isolation, he says, so degrees in which students work alone and simply check in with a mentor won't teach them about managing relationships with mentors and peers. However, future tools could make it easier for students to interact with others remotely, better preparing them for being collaborative researchers, he says. \"It will become increasingly more possible to do it.\"  \n                5 Skip the PhD \n              Some are choosing to forgo the PhD altogether. Deanna Pickett had always expected to get a PhD, maybe in engineering or environmental chemistry. That changed last year, during her final year as an undergraduate in chemistry at the College of Wooster in Ohio. Paul Edmiston, a chemistry professor, asked her to help him investigate the properties of a new material that absorbed contamination from drinking water. It was real work that had an immediate impact; she loved it. So when she later visited a potential graduate school, she was unimpressed. The prospect of years of more theoretical work, when she was already doing field research, was unappealing. When Stephen Spoonamore, the chief executive and co-founder (along with Edmiston) of the company ABSMaterials in Wooster, asked her to continue her work after she graduated, she changed her plans. \"It is just a little more fulfilling next step of my life than going to do another five years of research on another topic.\" Pickett's opportunity is unusual, perhaps more so now than ever before. Academia and industry have such a rich choice of PhD graduates for jobs that those without PhDs need not apply. \"There is currently an ample supply of highly skilled people on the market,\" says David Harwell, assistant director of career management and development at the American Chemical Society in Washington DC. In some fields, such as bioinformatics, simple on-the-job training can sometimes suffice, but even then scientists generally need a PhD to advance. \"Anyone can cite examples of non-PhD bioinformaticists who have made really major contributions, but few of these people have taken on the full range of responsibilities typically reserved for PhD investigators,\" says Maynard Olson, a genomics researcher at the University of Washington in Seattle. ABSMaterials is one of the few exceptions \u2014 mostly because Spoonamore believes that PhDs \"have got the wrong training\". Spoonamore says that he often pays undergraduates \"about the same\" as PhDs, and promotes them just as easily. He himself founded 13 technology companies without finishing an undergraduate degree, the first at the age of 18 with funding from his lawn-mowing business. \"I will always have a preference for an incredibly smart, top-of-their-class undergraduate student in chemistry. Every time.\" In her second day on the job, Pickett gave a presentation to a group of entrepreneurs, and a week later, had to develop a pilot plan to clean up a site in Ohio that had been contaminated with trichloroethylene. She says she probably does many things a PhD graduate would do. \"I do feel like I've skipped a step,\" she says. But she knows she might not get as many responsibilities if she decided to change companies. For this reason her colleague, Laura Underwood, has decided to pursue a PhD after working with ABSMaterials for 3 years. Underwood, who has a similar background to Pickett, was the company's first employee, with huge responsibilities \u2014 running a manufacturing facility, overseeing conference planning and managing a lab. Without a PhD, she fears it might be hard to find the same kinds of opportunities elsewhere. But she's glad she worked for a while before going to grad school. \"If you go straight into a PhD, something that sounds great in a lab may be kind of underwhelming when you get into the field.\"\n Alison McCook is a freelance writer in Philadelphia, Pennsylvania. \n                 Join the discussion on the future of the PhD \n               \n                     Join the discussion on the future of the PhD \n                   \n                     NIH Oxford-Cambridge Scholars Program \n                   \n                     NIH/Wellcome Trust Program \n                   \n                     Keck Graduate Institute PhD in Applied Life Sciences \n                   \n                     NSF Integrative Graduate Education and Research Traineeship program \n                   \n                     Open University \n                   \n                     Scripps-Oxford Joint DPhil/PhD \n                   \n                     The Janelia Farm Graduate Program \n                   \n                     Georgia Tech/Emory University/Peking University joint PhD in biomedical engineering \n                   \n                     Newcastle University's medical campus in Malaysia \n                   Reprints and Permissions"},
{"file_id": "472404a", "url": "https://www.nature.com/articles/472404a", "year": 2011, "authors": [{"name": "Jo Marchant"}], "parsed_as_year": "2006_or_before", "body": "Some researchers claim to have analysed DNA from Egyptian mummies. Others say that's impossible. Could new sequencing methods bridge the divide? Cameras roll as ancient-DNA experts Carsten Pusch and Albert Zink scrutinize a row of coloured peaks on their computer screen. There is a dramatic pause. \"My god!\" whispers Pusch, the words muffled by his surgical mask. Then the two hug and shake hands, accompanied by the laughter and applause of their Egyptian colleagues. They have every right to be pleased with themselves. After months of painstaking work, they have finally completed their analysis of 3,300-year-old DNA from the mummy of King Tutankhamun. Featured in the Discovery Channel documentary  King Tut Unwrapped   last year and published in the  Journal of the American Medical Association   ( JAMA ) 1 , their analysis \u2014 of Tutankhamun and ten of his relatives \u2014 was the latest in a string of studies reporting the analysis of DNA from ancient Egyptian mummies. Apparently revealing the mummies' family relationships as well as their afflictions, such as tuberculosis and malaria, the work seems to be providing unprecedented insight into the lives and health of ancient Egyptians and is ushering in a new era of 'molecular Egyptology'. Except that half of the researchers in the field challenge every word of it. Enter the world of ancient Egyptian DNA and you are asked to choose between two alternate realities: one in which DNA analysis is routine, and the other in which it is impossible. \"The ancient-DNA field is split absolutely in half,\" says Tom Gilbert, who heads two research groups at the Center for GeoGenetics in Copenhagen, one of the world's foremost ancient-DNA labs. Unable to resolve their differences, the two sides publish in different journals, attend different conferences and refer to each other as 'believers' and 'sceptics' \u2014 when, that is, they're not simply ignoring each other. The Tutankhamun study reignited long-standing tensions between the two camps, with sceptics claiming that in this study, as in most others, the results can be explained by contamination. Next-generation sequencing techniques, however, may soon be able to resolve the split once and for all by making it easier to sequence ancient, degraded DNA. But for now, Zink says, \"It's like a religious thing. If our papers are reviewed by one of the other groups, you get revisions like 'I don't believe it's possible'. It's hard to argue with that.\"  \n                Rise and fall \n              The disagreement stems from the dawn of ancient-DNA research. In the 1980s, a young PhD student called Svante P\u00e4\u00e4bo worked behind his supervisor's back at the University of Uppsala in Sweden to claim he had done what no one else had thought was possible: clone nuclear DNA from a 2,400-year-old Egyptian mummy 2 . Soon researchers realized that they could use a new technique called polymerase chain reaction (PCR) to amplify tiny amounts of DNA from ancient samples. There was a burst of excitement as DNA was reported from a range of ancient sources, including insects preserved in amber and even an 80 million-year-old dinosaur 3 . Then came the fall. It turned out that PCR, susceptible to contamination at the best of times, is particularly risky when working with tiny amounts of old, broken-up DNA. Just a trace of modern DNA \u2014 say from an archaeologist who had handled a sample \u2014 could scupper a result. The 'dinosaur' DNA belonged to a modern human, as did P\u00e4\u00e4bo's pioneering clone. Once researchers began to adopt rigorous precautions 4 , including replicating results in independent labs, attempts to retrieve DNA from Egyptian mummies met with little success 5 . That's no surprise, say sceptics. DNA breaks up over time, at a rate that increases with temperature. After thousands of years in Egypt's hot climate, they say, mummies are extremely unlikely to contain DNA fragments large enough to be amplified by PCR. \"Preservation in most Egyptian mummies is clearly bad,\" says P\u00e4\u00e4bo, now at the Max Planck Institute for Evolutionary Anthroplogy in Leipzig and a leader in the field. Ancient-DNA researcher Franco Rollo of the University of Camerino in Italy went so far as to test how long mummy DNA might survive. He checked a series of papyrus fragments of various ages, preserved in the similar conditions to the mummies. He estimated that DNA fragments large enough to be identified by PCR \u2014 around 90 base pairs long \u2014 would have vanished after only around 600 years 6 . Yet all the while, rival researchers have published a steady stream of papers on DNA extracted from Egyptian mummies up to 5,000 years old. Zink and his colleagues have tested hundreds of mummies, and claim to have detected DNA from a range of bacteria, including  Mycobacterium tuberculosis ,  Corynebacterium diphtheriae   and  Escherichia coli , as well as the parasites responsible for malaria and leishmaniasis. In a high-profile study last year, a team led by microbiologist Helen Donoghue at University College London reported finding DNA from  M. tuberculosis   in Dr Granville's mummy 7  \u2014 named after physician Augustus Granville, the first person to autopsy a mummy, in 1825. In the case of tuberculosis (TB) at least, Donoghue vehemently disagrees with the idea that DNA can't survive in Egyptian mummies. Mycobacteria such as  M. tuberculosis   have cell walls that are rich in lipids, which degrade slowly and protect the DNA, she argues. Donoghue claims that in many cases she has confirmed the presence of the bacterium by detecting these lipids directly. She says the extreme anti-contamination measures demanded by the big ancient-DNA labs are not as vital for ancient microbial DNA as they are for human DNA. After all, she says, modern diagnostic labs routinely detect TB using PCR \u2014 which suggests that the test is not as susceptible to contamination as the sceptics fear. In Donoghue's view, \"some of the precautions they talk about are totally over the top compared to every diagnostic lab in the country\". The sceptics are unmoved. Without highly stringent controls in place, it's impossible to show that any microbial sequences are from ancient DNA and not from related modern microbes, says Gilbert. \"How do you know you've got TB and not some other bacterium with a similar DNA sequence?\" He and other critics believe that this entire body of research is based on wishful thinking. The two groups have now grown tired of arguing. \"It's largely dealt with by ignoring each other,\" says Ian Barnes, a molecular palaeontologist at Royal Holloway, University of London, who works on DNA from ancient animals, including mammoths. \"There's enough dead stuff around, you're not obliged to get into anyone else's area.\"  \n                A royal argument \n              After the  JAMA   study on Tutankhamun and his family, however, the arguments resumed in force. Studies of human DNA from Egyptian mummies are the most controversial of all. One reason is the high profile of the claims. Another is that contamination from modern human DNA is excruciatingly difficult to detect, because its genetic make-up is almost identical to that of a human mummy's. On top of that, restricted access to samples makes it hard to check any claims in an independent lab. After more than a century in which valuable artefacts flooded out of the country to museums and private collections all over the world, the Egyptian authorities imposed a ban on removing archaeological samples from Egypt. Most non-Egyptian researchers wanting to study mummies are limited to museum exhibits elsewhere. The Tutankhamun project was carried out by an Egyptian team recruited by archaeologist Zahi Hawass, Egypt's top official in charge of antiquities. It was the first ancient-DNA study on royal mummies, and the country lacked the necessary expertise. So Hawass asked Zink, a prominent researcher at the EURAC Institute for Mummies and the Iceman in Bolzano, Italy, and Pusch, of the University of T\u00fcbingen, Germany, to act as consultants. The pair designed and oversaw the study, including the building of two dedicated labs in Cairo. The labs were partly paid for by the Discovery Channel, which filmed the project. The researchers deny that the television involvement put them under excessive pressure to produce dramatic results. But working for the cameras did make a challenging project even tougher, says Pusch. \"Each time they came in to film, we had to close the lab for a week to clean.\" Eventually the TV crew was banished and the lab scenes reconstructed. In the end, the project seemed to be a wild success, and its findings drew wide press attention. The researchers claimed to have detected DNA from the malaria parasite  Plasmodium falciparum   in several of the mummies, including Tutankhamun, suggesting that the infection had contributed to their deaths. They also said they had retrieved fragments of human DNA from every mummy tested and used the data to construct a five-generation family tree, from Tutankhamun's great-grandparents to the two tiny bodies found in his tomb, identified as his stillborn children. The whole episode has only raised eyebrows in the other half of the community. \"I'm very sceptical,\" says Eske Willerslev, director of Copenhagen's Center for GeoGenetics, who co-authored a letter to  JAMA   disputing the results 8 . His major concern, shared by others, was the method of DNA analysis used. Rather than extracting and sequencing DNA, the team used a technique called genetic fingerprinting, which involves measuring the size of the DNA products that have been amplified by PCR. It is rarely used in ancient-DNA studies, say critics, because without sequence data it is especially difficult to rule out contamination. And on a well-handled mummy such as Tutankhamun, say sceptics, contamination could be rife.  \n                Bones of contention \n              The Tutankhamun team carried out many controls, including replication of the tests by different teams in the two labs and comparing the mummy DNA fingerprints with those of the research team to cross-check for contamination. Zink and Pusch add that the samples were taken from within the mummies' bones where, they say, contaminating DNA should not have reached. Zink and Pusch think that the mummification process protected the DNA from degrading in the hot tomb by removing water, which is required for the main mechanism of DNA decay, called depurination. Egyptian embalmers dried bodies with natron, a naturally occurring mixture of salts, immediately after death. \"The Egyptians really knew how to preserve a body,\" says Zink. \"They got rid of the water very fast.\" Tutankhamun was also smothered with embalming and anointing materials, thought to contain ingredients such as bitumen, plant oils and beeswax, and Pusch believes it gave the DNA additional protection from the damaging effects of water. Hawass was not directly involved in the DNA research, but he stands by the team's conclusions, saying that the DNA in Egyptian mummies seems to be well preserved.  \"There are a number of things right about the paper,\" says David Lambert, an ancient-DNA researcher and evolutionary biologist at Griffith University in Nathan, Queensland. Lambert points out that the Tutankhamun team was not able to amplify Y-chromosome markers from the female mummies, which argues against contamination from modern archaeologists, who are generally male. In unpublished work, he says he has amplified DNA from mummified ibises, a sacred bird in ancient Egypt. \"We're confident that traditional PCR methods work with some of the material that we've got,\" he says. Sceptics, however, doubt that there was sufficient DNA left in Tutankhamun for the result to be real. They say that a mummified body would soon soak up any moisture available in the atmosphere, especially into its porous bones. When British archaeologist Howard Carter first opened Tutankhamun's coffins in 1925, he reported that they had been damaged by humidity. But it is difficult for anyone else to replicate the DNA work without permission to access the samples. The Tutankhamun study has left the field more divided than ever, with clear frustration on both sides. \"I don't understand people's harshness,\" Pusch says. \"This is pioneering work.\" He and Zink say that they are sequencing DNA from the mitochondria and Y chromosomes of the mummies, and plan to publish these results this year. But now, after years of conflict, strides in sequencing technology are changing the game. The newest techniques can read much shorter fragments \u2014 easily down to the 30 base pairs that might be found in a 2,000-year-old Egyptian mummy. \"That pushes the [DNA] survival time a long way back,\" says Gilbert. \"Things that we wrote off in the past, we can now get genomes on.\" And, crucially, the speed of the techniques makes it much easier to sequence a sample multiple times and to rule out contamination by checking for patterns of damage characteristic of ancient DNA. Last year, these techniques enabled Willerslev, Gilbert and their colleagues to publish the full genome sequence of a palaeo-Eskimo from Greenland that is some 4,000 years old 9 . Within weeks, teams led by P\u00e4\u00e4bo published the genome of a 38,000-year-old Neanderthal 10  and a previously unknown hominin from southern Siberia 11 . Meanwhile Zink's team is on the brink of publishing the genome of \u00d6tzi the Iceman. All these specimens were preserved in the cold \u2014 but Willerslev is already using next-generation techniques to extract DNA from various South American mummies, some of which have been preserved in warmer conditions. \"Some are definitely working,\" he says. But, he adds, he is finding tremendous variability in whether samples yield DNA \u2014 a possible reason why Egyptian mummies have yielded such conflicting results. With the cost of sequencing falling sharply, researchers are lining up to try the techniques on Egyptian mummies. Zink and Pusch are now negotiating the complex political path towards using next-generation techniques on Tutankhamun and his kin. \"We would love to do this,\" says Zink. \"It would absolutely make sense. The problem is to do it in Egypt.\" With no samples allowed out of the country, they would have to take the sequencing machines to Cairo, an expensive proposition. And there is concern, says Zink, that such work might yield politically sensitive information about the genetic origin of the pharaohs, and whether any of their descendants are alive today. \"This goes right to their history.\" Still, Zink is optimistic that next-generation sequencing will help to bring the fractured field back together. \"I think it is really time to bring together the different sides and stop arguing about each other's work,\" he says. \"With next-generation sequencing, people can't just say 'I don't like it'. People have to discuss the work based on the data themselves.\" Willerslev agrees, offering a rare olive branch. \"I think we will find that the believers have been too uncritical,\" he says. \"But the sceptics have probably been too conservative.\"\n Jo Marchant  is author of   Decoding the Heavens: Solving the Mystery of the World's First Computer. \n                     Ancestry and pathology in King Tutankhamun's family \n                   \n                     Interactive on King Tutankhamun's family \n                   \n                     King Tut unwrapped documentary \n                   \n                     Zahi Hawass \n                   \n                     Albert Zink \n                   \n                     University of Copenhagen Center for GeoGenetics \n                   Reprints and Permissions"},
{"file_id": "472152a", "url": "https://www.nature.com/articles/472152a", "year": 2011, "authors": [{"name": "Mark Schrope"}], "parsed_as_year": "2006_or_before", "body": "The Gulf of Mexico oil spill set records for its size and depth. A year on, the biggest impacts seem to be where they are hardest to spot. Late last year, oceanographers prowling the sea floor of the Gulf of Mexico came upon what looked like a crime scene. Cameras on a remotely operated vehicle revealed corpses of deepwater coral covered in brown goo. As the researchers watching from above saw one grim scene after another, \"the whole place got silent, everything totally stopped\", says Tim Shank an oceanographer from the Woods Hole Oceanographic Institution in Massachusetts, and a member of the survey team. The field of coral was just 11 kilometres from the Deepwater Horizon well head, which earlier in the year had spewed out more than 4 million barrels of oil and a similar amount of methane \u2014 the largest ever accidental release in the ocean. The spill was unique in other ways, too. Located beyond the continental shelf and some 1,400 metres below the surface, it happened in deeper water than any other major spill in history. Those factors make it much harder for researchers to discern what happened to the oil and how it affected wildlife. Assessing the impacts of this spill in deep water is \"probably one of the most challenging things ever\", says Steve Murawski, chief scientist for the Fisheries Service at the National Oceanic and Atmospheric Administration (NOAA) during the spill and now a fishery biologist at the University of South Florida in St Petersburg. And that is \"not only because of the physical environment but also because of the breadth of the potential impacts\", he says. But tracking the oil and its impacts remains an essential task. It will help to determine how much needs to be paid in restoration costs by BP \u2014 the company in charge of the well \u2014 and possibly other companies deemed partially responsible. And as drilling by the United States and other countries expands into deeper waters, lessons from last year's spill could help in responding to any future ones. At first, scientists and the public were most concerned about how the disaster would harm coastlines and near-shore waters. Although those areas sustained some damage, they did not come to as much harm as many had feared. Instead, as the anniversary of the spill approaches, signs of significant damage are showing up farther from shore and in deeper water. It was a stroke of bad luck that the well happened to be located in the most species-rich part of the deep gulf. Thomas Shirley, a veteran of spill research at the Harte Research Institute at Texas A&M University in Corpus Christi, says that his view of the spill is evolving. \"I'm beginning to think the deep sea is where we'll see most of the effects,\" he says.  \n                Beyond the edge \n              True to its name, the Deepwater Horizon oil rig was stationed beyond the lip of the continental shelf, where the sea floor rapidly falls away towards the lower reaches of the Gulf of Mexico. On 20 April 2010, a catastrophic blowout caused an explosion on the oil rig that claimed 11 lives and sent the rig to the sea floor. Government and independent estimates calculate that the broken well sprayed 4.1 million barrels of oil \u2014 and perhaps as much as 363,000 tonnes of natural gas \u2014 into the deep waters of the Gulf of Mexico. \n               Click here for larger image \n               Operations to collect and burn the oil took care of only about one-quarter of the liquid that came out of the well, according to a controversial government report 1 . The rest dissolved into the sea, dispersed into small drops, evaporated into the atmosphere or initially formed visible surface slicks and tar balls (see 'What happened to the oil?'). Some of it apparently went through an unusual transformation. Scientists who were out on boats in the spill zone during the early weeks after the disaster saw unusual strings of viscous material, which they dubbed 'sea snot'. The material looked like oil mixed with phytoplankton and other organic matter \u2014 and resembled a very thick batch of egg-drop soup. Vernon Asper, a geochemist at the University of Southern Mississippi near Diamondhead, had never seen anything like it. \"What is becoming of all that,\" he asked in mid-May while looking into water just a few kilometres from the blowout site. \"Where is it going?\" Asper and others found that the substance extended well below the surface in places. \"This is very strange material,\" says Ed Overton, a marine chemist and spill expert at Louisiana State University in Baton Rouge. But although many researchers reported seeing it, few were equipped to collect it. \"We had one hell of a time getting good samples,\" says Overton. Although his team is still analysing samples obtained from other researchers, the group has already made some intriguing findings that could explain the bizarre appearance. In shallow spills, oil tends to rise quickly to the surface, where it weathers, dissolves and evaporates in chemically predictable patterns. However, the largest drops of oil from the Deepwater Horizon well head took at least four hours to reach the surface, and smaller droplets rose much more slowly. During that long voyage, smaller droplets could have lost some of the lighter hydrocarbons that help to keep the various oil compounds from separating, suggests Overton. \"That changes the properties of the oil so it goes from a nice little round droplet, I think, into these strange-looking filamentous globs floating up the water column,\" he says. \"Unfortunately we didn't get enough oil to really harden this theory.\" Still, the issue of how the oil transformed is a crucial one for researchers to address. The processes involved can affect the oil's toxicity and how long it is likely to stick around. If Overton is right, the stringy masses that researchers found were a reminder that Deepwater Horizon was a different kind of spill. Soon after the disaster, Asper and his colleagues started seeing indications that some oil never made it to the surface \u2014 instead, it formed diffuse plumes more than 1,000 metres below the surface 2 . Reports of those finds were initially greeted with scepticism, in part because BP and most government officials expected that all the oil would float. But Asper's group and other researchers eventually confirmed that an unknown fraction of the oil had drifted away from the well head in deep-water plumes 3 , the effects of which are still not clear. Since then, controversy has also erupted over how much oil settled on the ocean floor and in what form. Samantha Joye, a geochemist at the University of Georgia in Athens is part of a team that found a layer of brown lumpy material on the sea floor that she says looks like dirty cauliflower and can be seen at sites as far as 130 kilometres from the well head. Andreas Teske, a microbiologist at the University of North Carolina, Chapel Hill, collaborates with Joye and suggests that the lumpy layer came from oil that had once been at the surface, perhaps beginning as the stringy sea snot. As microbes consumed the oil, it would have lost buoyancy and sunk, he says. Kai Ziervogel, a biogeochemist also at the University of North Carolina, tested this hypothesis in the laboratory by incubating seawater samples with surface oil from the spill. He saw masses of bacteria and oil forming, some of which sank and looked much like the lumpy layer of material in the sediments. Teske's team analysed samples from the lumpy layer and found that they contained oil with an unsually high percentage of the heavier compounds that would have been hard for microbes to eat, which fits with the idea that bacteria had consumed the lighter compounds. The group has also dated the lumpy layer \u2014 using the radioactive decay of thorium-234 as a clock \u2014 and found that it formed during and shortly after the spill. Joye has called the cauliflower layer a graveyard because she and her colleagues found in it countless dead worms and other common sediment dwellers, as well as the remains of jellyfish and other animals from above. And near to the well head the layer shows little microbial activity, suggesting that it will not break down quickly. The group has not yet published these findings and has just started to describe them at scientific conferences. But a number of researchers have questioned the existence of a widespread lumpy layer containing oil. Arden Ahnell, gulf-coast restoration science manager for BP, says that researchers working with the company have confirmed low concentrations of oil in some of the areas where Joye and her colleagues collected sediment, but found no evidence of a pervasive layer of unusual material. Joye has grown used to people challenging her work, having been part of the group that first reported the existence of a deep oil plume. In her laboratory, she points out the difference between normal grey sediments collected from deeper layers near the spill site and the lumpy brown sediments that, she says, formed after the spill. \"These are not normal-looking sediments. They are the most putrified, ugly, nasty-looking things you could imagine.\" Her arguments get some support from Amanda Demopoulos, a benthic ecologist with the US Geological Survey in Gainesville, Florida. Demopoulos, too, found an unusual lumpy brown deposit at one of the sites later visited by Joye and her colleagues. In Demopoulos's samples, \"there were some snails still living but the remaining animals I found were not moving around, and that's strange\", she says.  \n                Building a case \n              Some of these data may end up in court, as part of a process called the Natural Resource Damage Assessment (NRDA). The US government will use studies conducted through the NRDA to document the effects of the spill and then determine what damages should be paid by the responsible companies. This will include reimbursing the government for the cost of the NRDA. Because of the legal nature of the work, many of the academic researchers who are participating in it find the process foreign. Some signed non-disclosure forms that prevent them from publicly discussing their results without approval. And the researchers must maintain strict procedures for handling samples. Erik Cordes, a deep-sea ecologist from Temple University in Philadelphia, Pennsylvania, was one of the leaders of the coral study and he expressed frustration early on with the NRDA process. Cordes had wanted to conduct a well-accepted analysis of stress on the seafloor animals but was told by NRDA legal advisers that the work might not be accepted because the techniques had not been proven in court. \"The idea that there was a higher standard than peer review and scientific consensus is very difficult to accept,\" says Cordes. The NRDA is a major source of funding for research into the effects of the oil spill. Scientists should eventually gain access to around $400 million in research money from BP, but for now that fund has released only limited amounts because of bureaucratic and political problems 4 . A significant amount of BP's money will go towards studies of how the spill damaged gulf ecosystems. Its location 66 kilometres offshore might have helped to limit damage to the estuaries and coastal waters, but it was still located in a region of relatively rich biodiversity, says Shirley. He and his colleagues have amassed a database of 15,419 species living within the gulf. According to the database, some 1,728 species inhabit the region surrounding Deepwater Horizon at depths of between 1,000 and 3,000 metres, where the well is located. \"Ironically, this is the most speciose area of the gulf for this depth range \u2014 BP could not have selected a worse area to have a spill, at least from the point of species richness,\" says Shirley. \n               Click here for larger image \n               Researchers are finding signs that the damage extends throughout the water column, from the sea floor to the surface (see 'The big stain'). During the expedition on which Joye and others made the sediment discoveries, other researchers were concerned by jellyfish and other creatures they pulled up in nets from lower depths as far as 150 kilometres from the well head. \"The gelatinous animals are usually pinkish or translucent, but an awful lot of them were much darker brown or even black,\" says Joseph Montoya, chief scientist on the cruise, and a biological oceanographer at the Georgia Institute of Technology in Atlanta. In the spill zone soon after the disaster began, Asper and his colleagues often pulled up their equipment and found it decorated with the dead bodies of invertebrates known as pyrosomes, or fire salps. Pyrosomes probably have an important role in the food web near the surface, says Shirley, because they consume small plankton. Aside from those more obvious casualties, researchers have been hard pressed to find mass deaths that are clearly linked to the spill. This year, the bodies of 151 bottlenose dolphins have washed ashore in the northern gulf, but mass dolphin strandings have happened there before. The number of animals with clear evidence of oil contamination is much smaller. Only 6 dolphins and 18 sea turtles have been found dead with visible signs of oil, according to NOAA. Researchers caution that the oil spill probably took a much larger toll on whales, dolphins and turtles than has been observed. Blair Witherington, a sea-turtle expert with the Florida Fish and Wildlife Conservation Commission in Melbourne Beach, Florida, is particularly concerned about juvenile Kemp's ridleys ( Lepidochelys kempii ), the rarest species of sea turtle. Young turtles are the most vulnerable, he says, and are generally found well offshore. His team found hundreds that had visible signs of oil. For dolphins and other cetaceans, the gap between known deaths and actual ones may be vast. Judging from past studies of death rates, a group of cetacean specialists concluded last month that \"the true death toll could be 50 times the number of carcasses recovered\" 5 .  \n                The most vulnerable \n              Researchers are especially concerned about the youngest creatures because the spill came at a time when many animals were spawning. Larvae, for example, are known to be particularly susceptible to the toxic chemicals in the oil. And the problems may have been compounded by the 3 million litres of dispersants that were released to break up the oil. The dispersants themselves can be toxic, and they make oil droplets smaller and therefore more likely to affect even the smallest creatures. Ed Stellwag, a developmental biologist at East Carolina University in Greenville, North Carolina, is exploring what mixtures of spill oil and dispersant do to the embryos of zebrafish, the aquatic equivalent of lab rats. He found that even at the modest concentrations that many animals would have encountered during the spill, the mixture caused fatal heart and other defects in all of the embryos tested. Stellwag says that assessing how embryonic damage plays out offshore is likely to be difficult if not impossible because larger animals eat most of the larvae in surface waters. Damaged larvae, he fears, are more likely to end up as a meal than in a researcher's survey net. Larva surveys are already under way as part of the NRDA process. But given the difficulty of the work and the controls on releasing information, it could be years before researchers can offer an assessment of how the oil spill harmed larval fish and the thousands of other denizens of the gulf. During the heat of the crisis last year, thousands of men and women flocked to the spill zone to stop the oil leak and contain its damage. A year later, the Deepwater Horizon site is strangely quiet. The last drillship pulled out in February and vanished over the horizon. On the water's surface, there are no lasting impressions of the crisis, but not so below. The wreckage of one of the world's most advanced drilling rigs lies hidden on the sea floor, as do the ecological damages that are proving so challenging to assess. Mark Schrope is as a freelancer writer in Melbourne, Florida. \n                     Deepwater Horizon special \n                   \n                     Nature Geoscience editorial \n                   \n                     US government's Gulf of Mexico spill site \n                   \n                     US government's Oil Budget Calculator \n                   \n                     National Oceanic and Atmospheric Administration's data on cetacean deaths \n                   \n                     Natural Resource Damage Assessment process \n                   Reprints and Permissions"},
{"file_id": "473021a", "url": "https://www.nature.com/articles/473021a", "year": 2011, "authors": [{"name": "Lee Billings"}], "parsed_as_year": "2006_or_before", "body": "Reusable commercial rockets will soon be able to take scientists \u2014 and tourists \u2014 on suborbital spaceflights. Are these vehicles vital research tools, or an expensive dead end? As NASA's space shuttle  Discovery   roared into the sky on 24 February 2011, the bass rumble of its main engines and the staccato crackle of its solid-rocket boosters rolled out across the central Florida countryside, growing fainter and fainter with distance. Viewed from a hotel patio some 65 kilometres away in Orlando, the pillar of flame seemed to rise soundlessly, a silent apparition from a bygone era.  Discovery   was on its final mission; only two shuttle flights were left before the programme ended for good. In the United States, the classical era of the nation's human spaceflight was drawing to a close, 50 years after it began with the 15-minute flight of astronaut Alan Shepard on 5 May 1961. Three days after  Discovery  's launch, in the bar of the Orlando hotel, two planetary scientists are talking with a group of fellow researchers about what should come next. Sipping his drink, Daniel Durda laments that after half a century, only about 500 people have flown in space. Access to humanity's final frontier is still restricted to people employed by a handful of powerful governments and corporations, plus the occasional joyriding mega-millionaire. \"I'd prefer for anyone to be able to go, for any reason they choose,\" says Durda, of the Boulder, Colorado, branch of the Southwest Research Institute (SwRI). His companion, Catherine Olkin, also of the SwRI, agrees. \"What we're doing is the next step,\" she says. \"There are huge opportunities up there, not just for science, but for everyone.\" That next step is the subject of the meeting that has brought them all here. The second annual Next-Generation Suborbital Researchers Conference (NSRC) is inspired by the growth in recent years of a plethora of commercial companies making rockets designed to carry instruments and paying passengers more than 100 kilometres above Earth \u2014 past the edge of the atmosphere and into space. 'Suborbital' denotes vehicles that will come down again without entering orbit, but will still offer researchers precious minutes to make astronomical observations unblurred by the atmosphere, or to study physical processes in the absence of gravity. Indeed, conference attendees are already buzzing with the news that the SwRI has budgeted US$1.3 million for a four-year suborbital science programme, a portion of which will be used to book passenger seats on spacecraft for Durda, Olkin and their colleague Alan Stern, the SwRI's associate vice-president of research and development. If all goes as planned, the three researchers will be flying into space as fully fledged astronauts by mid-2013. The SwRI is so far the only research institution to have made such a deal, and everyone here knows the arguments for caution. None of the leading suborbital companies has yet flown its vehicles into space and back. And seats on future flights are going for some $100,000\u2013200,000, yet will give researchers no more than five minutes of weightless 'hang time' above the atmosphere. For most data-collecting needs, it is just as effective to launch automated equipment on an unmanned rocket. Many space scientists, therefore, remain dubious about the usefulness of commercial suborbital spaceflights, particularly those on which researchers accompany their equipment. But few of those sceptics have made the trip to the Orlando conference, where the prevailing mood is enthusiasm. No one embodies that feeling better than Stern. He hasn't made it to the hotel bar this evening, but that is only because he is busy conferring with launch-industry executives \u2014 not to mention preparing to chair panel sessions, deliver a plenary talk and give press conferences. As the principal investigator for NASA's New Horizons mission to Pluto and former head of the agency's Science Mission Directorate, Stern has a reputation for making big things happen \u2014 including the NSRC itself, which he and Durda helped to organize. He frequently compares the state of the nascent suborbital industry to the early days of commercial aviation and personal computing. Eventually, he explains when  Nature   catches up with him the next day, \"we'll no longer have one centrally planned space programme where only NASA has the keys to the space shuttle, and everyone has to funnel through that system. Everyone who can afford a ticket will go, and that will generate a lot of innovation, a lot of variety. This is going to be like the Wild West.\" Ironically, this 'centrally controlled' programme has relied on private rockets for decades. Most communications satellites and even classified military payloads are sent into orbit atop commercial launchers. Similarly, unmanned private suborbital flights are routine, with various companies marketing 'sounding rockets' that take measurements and perform experiments in space. The problem is that these launchers are so expensive that only the government and large telecommunications corporations can afford them. One reason for their cost is that they are expendable, discarded after a single use. In the 1970s, NASA tried to eliminate much of that waste by developing the fleet of space shuttles, which were partially reusable. But in practice, the advantage of reusability was more than offset by the difficulty of engineering a vehicle that could withstand the stresses of launch and re-entry, and be ready to fly again; the shuttles proved to be hideously pricey. Now, with the shuttles nearing retirement, NASA has been trying again to get the launch costs down \u2014 this time by encouraging the private sector to develop cheaper rockets and crew capsules to reach low Earth orbit. Last month, for example, the agency awarded $269 million in development money to four companies, one of which \u2014 the SpaceX Corporation of Hawthorne, California \u2014 has already had a successful launch with the Falcon 9 rocket, which first reached orbit in June 2010.  \n                Commercial enterprise \n              For the time being, however, more entrepreneurial energy is focused on the suborbital regime, in which the costs are lower and potential customers are more plentiful. The suborbital race began in 2004, when test pilot Mike Melvill repeatedly flew a privately developed reusable spacecraft,  SpaceShipOne , to altitudes of more than 100 kilometres in the skies over Mojave, California. Shortly afterwards, Burt Rutan, the craft's designer, partnered with entrepreneur Richard Branson to form Virgin Galactic, a venture to fly tourists on $200,000-per-seat, 110-kilometre-high suborbital jaunts using a fleet of 6-passenger '  SpaceShipTwo  ' spaceplanes, which are currently in development. \n               Click here for larger image \n               A number of other start-up firms followed \u2014 many with roots in the computer and Internet industries, a testament to the symbiosis between space dreams and lucrative high-tech careers. The companies include Armadillo Aerospace, organized and funded in Heath, Texas, by John Carmack, the computer-graphics wizard behind the hit videogames  Doom   and  Quake . There is also Blue Origin of Kent, Washington, founded by Jeff Bezos using a fraction of the fortune he earned creating Amazon.com; Masten Space Systems of Mojave, established by David Masten, a former information-technology networking guru; and XCOR Aerospace, also of Mojave, headed by Jeff Greason, an engineer who helped to develop the technology used in Intel's Pentium line of computer chips (see  'Space for everyone' ). These entrepreneurs expect one of the most lucrative applications for suborbital spaceflight to be space tourism, but tourist flights won't begin for at least a year, and probably two. In the meantime, to flesh out launch manifests and help to subsidize unmanned test flights, companies have begun courting research institutions, government agencies and independently wealthy investigators who want to run scientific experiments in suborbital space. All five companies sent representatives to the NSRC this year, hoping to court more clients like the SwRI, which has bought a total of eight seats, and options for nine more, on suborbital flights, split between Virgin Galactic's  SpaceShipTwo   and XCOR's  Lynx . Few of the attendees in Orlando needed much convincing; just about everyone there seemed eager to climb on board. Astronomers talked about raising telescopes above the atmosphere to perform mid-infrared searches for water on the Moon and to observe planets, comets and asteroids. Planetary scientists detailed microgravity experiments to investigate the collisions of dust and sand that form the building blocks of planets. Atmospheric scientists discussed  in situ   sampling of the 'ignorosphere', the largely unexplored stratum of the upper atmosphere that lies above the altitudes attainable by weather balloons, but below those of satellites. Materials scientists were eager to study how microgravity affects processes such as combustion. But the enthusiasm of individual researchers is one thing. Getting the institutions they work for to pay for tickets is something else. NASA, for example, has signed contracts with Armadillo and Masten, paying nearly $500,000 for seven flights carrying engineering equipment. But the agency does not yet have approval to buy seats for suborbital passengers \u2014 both Armadillo and Masten are currently focusing more on unmanned flight \u2014 and none of the flights it has purchased will actually reach space. The highest planned altitude is about 40 kilometres. To sell those passenger seats, the launch companies will have to convince decision-makers that their reusable vehicles offer significant advantages over existing ways to access the weightless space environment. For only a few tens of thousands of dollars per trip, researchers can book custom-modified aeroplanes that fly in a series of parabolic trajectories, providing microgravity in 30-second bursts for a total of 5\u201310 minutes per flight. Or, for an admittedly steep $1 million to $2 million, researchers can put automated equipment on a sounding rocket that provides up to 20 minutes of microgravity far above Earth's atmosphere. The new commercial vehicles vary in their capabilities, but generally fall between the two existing options: they can reach between 30 and about 100 kilometres in altitude and offer 3\u20135 minutes of weightlessness. Stern and other proponents believe that the reusable space vehicles' short times in space are counterbalanced by their high frequency of flights. \"As a principal investigator, it took me nearly a decade to get seven flights on NASA's sounding rockets, but [the SwRI is] going to be flying these eight missions in the space of about a year,\" says Stern. \"Virgin Galactic alone will be flying daily with six vehicles; XCOR is going to fly four times per day; Blue Origin says it'll fly once a week. This will give us unprecedented access to space.\"  \n                The human element \n              But do humans really need to ride along on such experiments, with all the risks and complexity that entails? \"The advantage is twofold,\" says Stern, making much the same argument that human-spaceflight proponents have been making since NASA's Apollo programme of the 1960s. \"First, you don't have to pay to automate your experiment any more. And second, that means you can easily react to your data collection in real time and make changes in your experiment to get better results.\" To demonstrate those advantages, the SwRI researchers are planning three showcase experiments. One, 'Box of Rocks', is a transparent case of stone fragments and ceramic bricks meant to simulate how loose material settles on asteroids with low surface gravity. Another uses a refurbished ultraviolet telescope, which flew on a space shuttle in 1997, to observe astronomical objects and upper-atmosphere phenomena through a cabin window. The last uses a 'bioharness' to monitor and record how the blood pressure, heart rate and other physiological parameters of passengers vary under the flight profiles of different vehicles. Paul Hertz, the chief scientist in NASA's Science Mission Directorate, remains sceptical. He is broadly optimistic about the science potential of reusable suborbital vehicles, but less convinced that involving humans in experiments requiring astronomical observations will be useful. \"Because these flights are relatively short, it's really not that constraining to pre-program your observing plan,\" he says. The weight of seats and life-support exacts a huge performance penalty, agrees Stephan McCandliss, an astronomer at Johns Hopkins University in Baltimore, Maryland. McCandliss has flown ultraviolet astronomy experiments using sounding rockets for more than two decades, and what the new reusables can do \"pales in comparison\", he says. \"Sounding rockets are extremely valuable,\" Stern concedes. \"They go higher, have more sophisticated pointing systems, and can carry payloads commercial reusables just can't. But reusables can fly far more frequently, they are 10\u201320 times lower in cost, and they can bring scientists along with their instruments. This is a debate about having a fork or a spoon at the table \u2014 they are for different purposes.\" John Grunsfeld, a former shuttle astronaut and current deputy director of the Space Telescope Science Institute in Baltimore, is also doubtful that the new vehicles will be better than sounding rockets, but admits that reusables could offer fresh opportunities for science. \"The real potential to benefit science is not necessarily in this first wave of vehicles, but in their prospect for a future in which crewed commercial vehicles will routinely and frequently access longer periods of weightlessness, or better yet, reach Earth orbit,\" says Grunsfeld. That ready availability, in turn, could facilitate imaginative science that might not make it through the peer-review processes of government agencies and major academic institutions. Of course, cautions Grunsfeld, \"the value of that science remains to be seen\". For Stern, the value of routine suborbital spaceflight shouldn't be measured only in grants awarded and peer-reviewed papers published. Speaking at one NSRC session, he told the audience that they should feel no shame if their interest in suborbital science stemmed mainly from their yearning to fly in space. \"I don't think most scientists appreciate very well how motivational human spaceflight can be,\" Stern said. \"Going into science is hard \u2014 there are easier careers where you can make more money. But when everyday educators and working researchers can visit classrooms and speak to schoolchildren about personally going into space, that has real effects. We can contribute to the future by giving birth to this new industry and the opportunities it brings.\" That message seemed to resonate at the NSRC. Between and after the sessions, discussions about suborbital science frequently segued into conversations about inspiring the next generation of researchers to do great things \u2014 to beam energy to Earth from vast solar panels on satellites, to visit asteroids, to colonize the Moon or to travel to Mars. Indeed, for most of the scientists gathered in the conference rooms and hotel bars, the prospect of democratized suborbital flight seemed to be a blank screen on which they could project their current plans and future dreams of humanity's expansion into, at minimum, the rest of the Solar System. \"A lot of people are reluctant to talk about the big picture, and they may not be able to always clearly articulate why they want to take these trips, but what many of them want is to take part in making the future happen,\" says Greason, XCOR's chief executive. He is no exception. \"The reason why I'm in this business is because I think it has the potential to be the beginning of something that will last for a very, very long time.\"\n Lee Billings is a freelance writer in New York. \n                     Next-Generation Suborbital Researchers Conference \n                   \n                     Commercial Spaceflight Federation \n                   \n                     NASA Flight Opportunities programme \n                   Reprints and Permissions"},
{"file_id": "473138a", "url": "https://www.nature.com/articles/473138a", "year": 2011, "authors": [{"name": "Eugenie Samuel Reich"}], "parsed_as_year": "2006_or_before", "body": "A  Nature   poll reveals how researchers guard, and sometimes burnish, their online image. In March, a puzzling press release began circulating around the Internet. Titled \"DR. Anil Potti Likes Spending Quality Time With His Wife And Three Daughters\", it listed a series of qualifications, honours and prizes won by Potti, a cancer geneticist formerly at Duke University in Durham, North Carolina. He resigned in 2010 after it was revealed that he had falsely claimed to be a Rhodes scholar and Duke began to investigate errors in his work. Most scientists embroiled in scandal shrink from view, but Potti's online presence began booming in unexpected ways. After he resigned, he, or someone using his name, created more than half a dozen websites about him and his research, including:  www.pottianil.com  ,  www.anilpotti.com  ,  www.anilpotti.net  and  www.dranilpotti.com . Twitter and Facebook accounts appeared in his name in January, followed by a stream of press releases notable for their breathless banality: aside from enjoying time with family, Potti believes that most lung cancers are caused by smoking; he is an advocate for personalized cancer therapy; and he donates his time and money to the local school system and church. It is difficult to identify the source of the releases, websites and social-media accounts. Potti could not be reached for comment, and his lawyer, Jim Maxwell of Maxwell, Freeman & Bowman in Durham, declined to comment on them, explaining that there is a confidential research-misconduct investigation ongoing at Duke. \"Until that is concluded and Dr Potti is cleared of any wrongdoing, he is not in a position to be making public comments,\" Maxwell says. The only clue to their origin lies in the registration information for  http://www.anilpotti.com  and  http://www.anilpotticv.com , which list an e-mail address from  Online Reputation Manager  as an administrative contact. Online Reputation Manager, headquartered near Rochester, New York, is a company that uses search-engine optimization strategies to repair the online image of clients who have been besieged with unfavourable press. These include flooding the Internet with positive messages to drown out the negative. A company representative confirmed ownership of the e-mail address, but could not say whether Potti is a client. \n               Click here for larger image \n               Potti's reputation may have needed serious work, but a  Nature   poll reveals that a significant number of scientists are concerned with maintaining their online image. The poll was e-mailed to 30,000 working scientists and was promoted on Facebook and Twitter. Of 840 respondents, 77% say that their personal online reputation is important to them and 88% say that the online reputation of their work is important (see  'A name online' ). Thirteen per cent say that they have used search-optimization strategies to improve the visibility of their research, and as many as 10% say they have considered using external services to manage their online reputations ( click here  for the full poll results). Several researchers have set up biographies on the online site Wikipedia \u2014 the online encyclopaedia that practically anyone can edit \u2014 or edited entries to include references to their own papers. And many simply use social-networking sites or blog regularly about science, which can help to shape a digital persona. The poll and subsequent interviews suggest a growing recognition in the scientific community: maintaining a prominent online presence can help researchers to network with colleagues, share resources, raise money and communicate their work. \"It is incredibly valuable,\" says Gia Milinovich, a web producer based in London who has studied scientists' use of Twitter. At a minimum, says Alex Bateman, a bioinformatician at the Wellcome Trust Sanger Institute near Cambridge, UK, scientists should ensure that they have an online profile that includes contact details. For his part, he routinely checks that his publications come up together in a list when his name is searched in databases such as Web of Science and Scopus. If he finds an error, he contacts the database company to complain. \"They're very quick to respond,\" he says. Others are looking at the face they present to the wider world, through sites such as Wikipedia. For many people looking for information on a scientific topic, Wikipedia is a first port of call \u2014 and our poll shows that scientists use it regularly. As many as 72% admit to checking Wikipedia at least once a week and about one-fifth do so for references to themselves or their group's work. Nine per cent of our sample say that they have inserted references to their or their group's work in the past 12 months, and nearly 3% have edited their biographies, something that is frowned on by Wikipedia editors, according to Bateman. \"You shouldn't be editing articles you are too close to because you have a conflict of interest,\" he says. That said, roughly one-tenth of the respondents to our poll say that their work has been misrepresented on the web, and some scientists in this situation feel the need to set the record straight. Walt de Heer, a physicist at the Georgia Institute of Technology in Atlanta, works on graphene \u2014 two-dimensional carbon sheets that may have applications in electronics. In 2009, de Heer caught wind of rumours that his research had been inspired by work done at the University of Manchester, UK, by Andre Geim and Konstantin Novoselov (the pair won the Nobel prize for this research in 2010). De Heer saw that the Wikipedia article on graphene emphasized the Manchester work and suspected that it was fuelling the rumours. So de Heer created his own biography on Wikipedia. And although it was nominated for deletion by at least one Wikipedia editor, enough users of the site have agreed that it should remain. Darren Logan, a geneticist at the Wellcome Trust Sanger Institute, is an administrator on Wikipedia \u2014 a position that gives him additional editing powers. He agrees that editing Wikipedia can be a very influential way of getting a point across, even within the scientific community. One article he has written, on major urinary proteins, included references to his scientific work and introduced terminology that others later used to describe his work. \"The purpose of writing wasn't to promote my own work but a consequence was that a lot more people read my research articles. It's influencing them,\" he speculates.  \n                Manipulating visibility \n              A handful of researchers are using more sophisticated tools to increase the visibility of a website. Software engineer Brian Turner has been trying to promote software developed at a lab at the Hospital for Sick Children in Toronto, Canada, where he works. He uses Google's webmaster and analytics tools to figure out how Google 'sees' the lab's website and how much traffic the website gets through Google. The analysis prompted him to change the titles of several pages from obscure identifiers to ones that include the names of proteins that people might search for. \"That made a big difference to our search rankings,\" he says. Social-networking tools can also boost a person's visibility on the web. Among the 549 people who responded to the e-mail invitation to take part in our survey, 59% had used Facebook and 23% had used Twitter. About 17% of them had written at least one blog. Although blogging is usually deemed extra-curricular, some say it has definite career benefits. Paul 'P. Z.' Myers, a biologist at the University of Minnesota Morris, runs the popular blog Pharyngula, which he says gets about a million visits a month. He says that he has never mentioned the blog on his CV or applications for tenure, but his tenure referees raised the blog as an example of something positive he was doing. Unsurprisingly, younger researchers tend to be more preoccupied with online reputations than older ones. Although more than half of researchers under 35 say that they strongly agreed that the online reputation of their work was important, that number dropped to 42% for those aged 35\u201354 and to 32% for those aged 55 and over. Peter Ruben, a biophysicist at Simon Fraser University in Burnaby, Canada, falls in that latter demographic and says he doesn't care about his online reputation. In 2005, Ruben published a paper (S. L. Geffeney  et al .  Nature   434 , 759-763; 2005) that reported on evolution of resistance to a toxin in garter snakes. Although his work was misrepresented on creationist websites, Ruben didn't try to set the record straight, and doesn't really think that it has tarnished his reputation. The positive messages being posted on Potti's behalf have had some effect. In a Google search for 'Anil Potti' on 9 May, five of the top ten links were to positive material placed in the past several months. But a detractor has surfaced, setting up a satirical Twitter account, @anil_potti, which posts links to articles about the ongoing investigation at Duke. An article in the independent student newspaper,  The Chronicle , also questioned the ethics of the online management activity, pointing out that it discusses Potti's research without saying that it has been questioned. Ronald Smith, manager of business development at Online Reputation Manager, stresses that the kind of work it does is ethical, legal and accepted in the search-optimization industry. But Bateman says that whoever is doing the work on Potti's reputation has their work cut out for them. \"In the Internet world it is impossible to remove the evidence. In this case Anil Potti is in danger of bringing more attention to his alleged scientific misconduct.\" Potti's is an extreme case, but it holds a lesson for anyone who sets out to court attention online: the web can be unpredictable.\n \n                 See Editorial  \n                 p.124 \n               The full results of the Nature poll are available  here . Eugenie Samuel Reich is a reporter for Nature in Cambridge, Massachusetts. Survey work was aided by Laura Harper. \n                     Nature poll on online reputations \n                   \n                     Online Reputation Manager \n                   \n                     Gia Milinovich's blog \n                   \n                     Google Analytics \n                   \n                     Pharyngula \n                   Reprints and Permissions"},
{"file_id": "472276a", "url": "https://www.nature.com/articles/472276a", "year": 2011, "authors": [{"name": "David Cyranoski"}, {"name": "Natasha Gilbert"}, {"name": "Heidi Ledford"}, {"name": "Anjali Nayar"}, {"name": "Mohammed Yahia"}], "parsed_as_year": "2006_or_before", "body": "The world is producing more PhDs than ever before. Is it time to stop? Scientists who attain a PhD are rightly proud \u2014 they have gained entry to an academic elite. But it is not as elite as it once was. The number of science doctorates earned each year grew by nearly 40% between 1998 and 2008, to some 34,000, in countries that are members of the Organisation for Economic Co-operation and Development (OECD). The growth shows no sign of slowing: most countries are building up their higher-education systems because they see educated workers as a key to economic growth (see  'The rise of doctorates' ). But in much of the world, science PhD graduates may never get a chance to take full advantage of their qualifications. \n               boxed-text \n             In some countries, including the United States and Japan, people who have trained at great length and expense to be researchers confront a dwindling number of academic jobs, and an industrial sector unable to take up the slack. Supply has outstripped demand and, although few PhD holders end up unemployed, it is not clear that spending years securing this high-level qualification is worth it for a job as, for example, a high-school teacher. In other countries, such as China and India, the economies are developing fast enough to use all the PhDs they can crank out, and more \u2014 but the quality of the graduates is not consistent. Only a few nations, including Germany, are successfully tackling the problem by redefining the PhD as training for high-level positions in careers outside academia. Here,  Nature   examines graduate-education systems in various states of health.  \n                Japan: A system in crisis \n              Of all the countries in which to graduate with a science PhD, Japan is arguably one of the worst. In the 1990s, the government set a policy to triple the number of postdocs to 10,000, and stepped up PhD recruitment to meet that goal. The policy was meant to bring Japan's science capacity up to match that of the West \u2014 but is now much criticized because, although it quickly succeeded, it gave little thought to where all those postdocs were going to end up. Academia doesn't want them: the number of 18-year-olds entering higher education has been dropping, so universities don't need the staff. Neither does Japanese industry, which has traditionally preferred young, fresh bachelor's graduates who can be trained on the job. The science and education ministry couldn't even sell them off when, in 2009, it started offering companies around \u00a54 million (US$47,000) each to take on some of the country's 18,000 unemployed postdoctoral students (one of several initiatives that have been introduced to improve the situation). \"It's just hard to find a match\" between postdoc and company, says Koichi Kitazawa, the head of the Japan Science and Technology Agency. This means there are few jobs for the current crop of PhDs. Of the 1,350 people awarded doctorates in natural sciences in 2010, just over half (746) had full-time posts lined up by the time they graduated. But only 162 were in the academic sciences or technological services,; of the rest, 250 took industry positions, 256 went into education and 38 got government jobs. \n               boxed-text \n             With such dismal prospects, the number entering PhD programmes has dropped off (see  'Patterns of PhD production' ). \"Everyone tends to look at the future of the PhD labour market very pessimistically,\" says Kobayashi Shinichi, a specialist in science and technology workforce issues at the Research Center for University Studies at Tsukuba University.  \n                China: Quantity outweighs quality? \n              The number of PhD holders in China is going through the roof, with some 50,000 people graduating with doctorates across all disciplines in 2009 \u2014 and by some counts it now surpasses all other countries. The main problem is the low quality of many graduates. Yongdi Zhou, a cognitive neuroscientist at the East China Normal University in Shanghai, identifies four contributing factors. The length of PhD training, at three years, is too short, many PhD supervisors are not well qualified, the system lacks quality control and there is no clear mechanism for weeding out poor students. Even so, most Chinese PhD holders can find a job at home: China's booming economy and capacity building has absorbed them into the workforce. \"Relatively speaking, it is a lot easier to find a position in academia in China compared with the United States,\" says Yigong Shi, a structural biologist at Tsinghua University in Beijing, and the same is true in industry. But PhD graduates can run into problems if they want to enter internationally competitive academia. To get a coveted post at a top university or research institution requires training, such as a postdoctoral position, in another country. Many researchers do not return to China, draining away the cream of the country's crop. The quality issue should be helped by China's efforts to recruit more scholars from abroad. Shi says that more institutions are now starting to introduce thesis committees and rotations, which will make students less dependent on a single supervisor in a hierarchical system. \"Major initiatives are being implemented in various graduate programmes throughout China,\" he says. \"China is constantly going through transformations.\"  \n                Singapore: Growth in all directions \n              The picture is much rosier in Singapore. Here, the past few years have seen major investment and expansion in the university system and in science and technology infrastructure, including the foundation of two new publicly funded universities. This has attracted students from at home and abroad. Enrolment of Singaporean nationals in PhD programmes has grown by 60% over the past five years, to 789 in all disciplines \u2014 and the country has actively recruited foreign graduate students from China, India, Iran, Turkey, eastern Europe and farther afield. Because the university system in Singapore has been underdeveloped until now, most PhD holders go to work outside academia, but continued expansion of the universities could create more opportunities. \"Not all end up earning a living from what they have been trained in,\" says Peter Ng, who studies biodiversity at the National University of Singapore. \"Some have very different jobs \u2014 from teachers to bankers. But they all get a good job.\" A PhD can be lucrative, says Ng, with a graduate earning at least S$4,000 (US$3,174) a month, compared with the S$3,000 a month earned by a student with a good undergraduate degree. \"I see a PhD not just as the mastery of a discipline, but also training of the mind,\" says Ng. \"If they later practise what they have mastered \u2014 excellent \u2014 otherwise, they can take their skill sets into a new domain and add value to it.\"  \n                United States: Supply versus demand \n              To Paula Stephan, an economist at Georgia State University in Atlanta who studies PhD trends, it is \"scandalous\" that US politicians continue to speak of a PhD shortage. The United States is second only to China in awarding science doctorates \u2014 it produced an estimated 19,733 in the life sciences and physical sciences in 2009 \u2014 and production is going up. But Stephan says that no one should applaud this trend, \"unless Congress wants to put money into creating jobs for these people rather than just creating supply\". \n               boxed-text \n             The proportion of people with science PhDs who get tenured academic positions in the sciences has been dropping steadily and industry has not fully absorbed the slack. The problem is most acute in the life sciences, in which the pace of PhD growth is biggest, yet pharmaceutical and biotechnology industries have been drastically downsizing in recent years. In 1973, 55% of US doctorates in the biological sciences secured tenure-track positions within six years of completing their PhDs, and only 2% were in a postdoc or other untenured academic position. By 2006, only 15% were in tenured positions six years after graduating, with 18% untenured (see  'What shall we do about all the PhDs?' ). Figures suggest that more doctorates are taking jobs that do not require a PhD. \"It's a waste of resources,\" says Stephan. \"We're spending a lot of money training these students and then they go out and get jobs that they're not well matched for.\" The poor job market has discouraged some potential students from embarking on science PhDs, says Hal Salzman, a professor of public policy at Rutgers University in New Brunswick, New Jersey. Nevertheless, production of US doctorates continues apace, fuelled by an influx of foreign students. Academic research was still the top career choice in a 2010 survey of 30,000 science and engineering PhD students and postdocs, says Henry Sauermann, who studies strategic management at the Georgia Institute of Technology in Atlanta. Many PhD courses train students specifically for that goal. Half of all science and engineering PhD recipients graduating in 2007 had spent over seven years working on their degrees, and more than one-third of candidates never finish at all. Some universities are now experimenting with PhD programmes that better prepare graduate students for careers outside academia (see  page 280 ). Anne Carpenter, a cellular biologist at the Broad Institute of the Massachusetts Institute of Technology (MIT) and Harvard University in Cambridge, Massachusetts, is trying to create jobs for existing PhD holders, while discouraging new ones. When she set up her lab four years ago, Carpenter hired experienced staff scientists on permanent contracts instead of the usual mix of temporary postdocs and graduate students. \"The whole pyramid scheme of science made little sense to me,\" says Carpenter. \"I couldn't in good conscience churn out a hundred graduate students and postdocs in my career.\" But Carpenter has struggled to justify the cost of her staff to grant-review panels. \"How do I compete with laboratories that hire postdocs for $40,000 instead of a scientist for $80,000?\" she asks. Although she remains committed to her ideals, she says that she will be more open to hiring postdocs in the future.  \n                Germany: The progressive PhD \n              Germany is Europe's biggest producer of doctoral graduates, turning out some 7,000 science PhDs in 2005. After a major redesign of its doctoral education programmes over the past 20 years, the country is also well on its way to solving the oversupply problem. Traditionally, supervisors recruited PhD students informally and trained them to follow in their academic footsteps, with little oversight from the university or research institution. But as in the rest of Europe, the number of academic positions available to graduates in Germany has remained stable or fallen. So these days, a PhD in Germany is often marketed as advanced training not only for academia \u2014 a career path pursued by the best of the best \u2014 but also for the wider workforce. Universities now play a more formal role in student recruitment and development, and many students follow structured courses outside the lab, including classes in presenting, report writing and other transferable skills. Just under 6% of PhD graduates in science eventually go into full-time academic positions, and most will find research jobs in industry, says Thorsten Wilhelmy, who studies doctoral education for the German Council of Science and Humanities in Cologne. \"The long way to professorship in Germany and the relatively low income of German academic staff makes leaving the university after the PhD a good option,\" he says. Thomas J\u00f8rgensen, who heads a programme to support and develop doctoral education for the European University Association, based in Brussels, is concerned that German institutions could push reforms too far, leaving students spending so long in classes that they lack time to do research for their thesis and develop critical-thinking skills. The number of German doctorates has stagnated over the past two decades, and J\u00f8rgensen worries about this at a time when PhD production is growing in China, India and other increasingly powerful economies.  \n                Poland: Expansion at a cost \n              Growth in PhD numbers among Europe's old guard might be waning, but some of the former Eastern bloc countries, such as Poland, have seen dramatic increases. In 1990\u201391, Polish institutions enrolled 2,695 PhD students. This figure rose to more than 32,000 in 2008\u201309 as the Polish government, trying to expand the higher-education system after the fall of Communism, introduced policies to reward institutions for enrolling doctoral candidates. Despite the growth, there are problems. A dearth of funding for doctoral studies causes high drop-out rates, says Andrzej Kra\u015bniewski, a researcher at Warsaw University of Technology and secretary-general of the Polish Rectors Conference, an association representing Polish universities. In engineering, more than half of students will not complete their PhDs, he says. The country's economic growth has not kept pace with that of its PhD numbers, so people with doctorates can end up taking jobs below their level of expertise. And Poland needs to collect data showing that PhDs from its institutions across the country are of consistent quality, and are comparable with the rest of Europe, says Kra\u015bniewski. \n               boxed-text \n             Still, in Poland as in most countries, unemployment for PhD holders is below 3%. \"Employment prospects for holders of doctorates remain better than for other higher-education graduates,\" says Laudeline Auriol, author of an OECD report on doctorate holders between 1990 and 2006, who is now analysing doctoral-student data up to 2010. Still, a survey of scientists by  Nature   last year showed that PhD holders were not always more satisfied with their jobs than those without the degree, nor were they earning substantially more (see  'What's a PhD worth?' ).  \n                Egypt: Struggle to survive \n              Egypt is the Middle East's powerhouse for doctoral studies. In 2009, the country had about 35,000 students enrolled in doctoral programmes, up from 17,663 in 1998. But funding has not kept up with demand. The majority comes through university budgets, which are already strained by the large enrolment of students in undergraduate programmes and postgraduate studies other than PhDs. Universities have started turning to international funding and collaborations with the private sector, but this source of funding remains very limited. The deficit translates into shortages in equipment and materials, a lack of qualified teaching staff and poor compensation for researchers. It also means that more of the funding burden is falling on the students. The squeeze takes a toll on the quality of research, and creates tension between students and supervisors. \"The PhD student here in Egypt faces numerous problems,\" says Mounir Hana, a food scientist and PhD supervisor at Minia University, who says that he tries to help solve them. \"Unfortunately, many supervisors do not bother, and end up adding one more hurdle in the student's way.\" Graduates face a tough slog. As elsewhere, there are many more PhD holders in Egypt than the universities can employ as researchers and academics. The doctorate is frequently a means of climbing the civil-service hierarchy, but those in the private sector often complain that graduates are untrained in the practical skills they need, such as proposal writing and project management. Egyptian PhD holders also struggle to secure international research positions. Hana calls the overall quality of their research papers \"mediocre\" and says that pursuing a PhD is \"worthless\" except for those already working in a university. But the political upheaval in the region this year could bring about change: many academics who had left Egypt are returning, hoping to help rebuild and overhaul education and research. Few PhDs are trained elsewhere in the Middle East \u2014 less than 50 a year in Lebanon, for example. But several world-class universities established in the oil-rich Gulf States in recent years have increased demand for PhD holders. So far, most of the researchers have been 'imported' after receiving their degrees from Western universities, but Saudi Arabia and Qatar in particular have been building up their infrastructure to start offering more PhD programmes themselves. The effect will be felt throughout the region, says Fatma Hammad, an endocrinologist and PhD supervisor at Al-Azhar University in Cairo. \"Many graduates are now turning to doctoral studies because there is a large demand in the Gulf States. For them, it is a way to land jobs there and increase their income,\" she says.  \n                India: PhDs wanted \n              In 2004, India produced around 5,900 science, technology and engineering PhDs, a figure that has now grown to some 8,900 a year. This is still a fraction of the number from China and the United States, and the country wants many more, to match the explosive growth of its economy and population. The government is making major investments in research and higher education \u2014 including a one-third increase in the higher-education budget in 2011\u201312 \u2014 and is trying to attract investment from foreign universities. The hope is that up to 20,000 PhDs will graduate each year by 2020, says Thirumalachari Ramasami, the Indian government's head of science and technology. Those targets ought to be easy to reach: India's population is young, and undergraduate education is booming (see   Nature  472, 24\u201326; 2011 ). But there is little incentive to continue into a lengthy PhD programme, and only around 1% of undergraduates currently do so. Most are intent on securing jobs in industry, which require only an undergraduate degree and are much more lucrative than the public-sector academic and research jobs that need postgraduate education. Students \"don't think of PhDs now, not even master's \u2014 a bachelor's is good enough to get a job\", says Amit Patra, an engineer at the Indian Institute of Technology in Kharagpur. Even after a PhD, there are few academic opportunities in India, and better-paid industry jobs are the major draw. \"There is a shortage of PhDs and we have to compete with industry for that resource \u2014 the universities have very little chance of winning that game,\" says Patra. For many young people intent on postgraduate education, the goal is frequently to go to the United States or Europe. That was the course chosen by Manu Prakash, who went to MIT for his PhD and now runs his own experimental biophysics lab at Stanford University in California. \"When I went through the system in India, the platform for doing long-term research I didn't feel was well-supported,\" he says.\n \n                 Join the discussion on the future of the PhD \n               \n                     Careers of doctorate holders: employment and mobility patterns \n                   \n                     Science and Engineering Indicators 2010 \n                   Reprints and Permissions"},
{"file_id": "473140a", "url": "https://www.nature.com/articles/473140a", "year": 2011, "authors": [{"name": "Katherine Barnes"}], "parsed_as_year": "2006_or_before", "body": "Vesuvius is one of the most dangerous volcanoes in the world \u2014 but scientists and the civil authorities can't agree on how to prepare for a future eruption. It starts with a blast so strong that a column of ash and stone rockets 40 kilometres up into the stratosphere. The debris then drops to Earth, pelting the surface with boiling hot fragments of pumice and covering the ground with a thick layer of ash. Roofs crumble and vehicles grind to a halt. Yet the worst is still to come. Soon, avalanches of molten ash, pumice and gas roar down the slopes of the volcano, pulverizing buildings and burying everything in their path. Almost overnight, a packed metropolis becomes a volcanic wasteland. This is Naples, Italy, in the throes of a cataclysmic eruption of Vesuvius \u2014 the volcano that destroyed the city of Pompeii in AD 79. The scenario may sound far-fetched, but in the wake of Japan's recent earthquake and tsunami, many areas are reassessing the risks from their own 'black swans', a term used to describe unlikely but potentially devastating disasters. And Naples stands out as particularly vulnerable, with a population of 3 million living in the shadow of Vesuvius. The volcano has been eerily dormant since a small eruption in 1944, but recent studies suggest that Vesuvius could be more dangerous than previously assumed, which has prompted a vigorous debate about the risk and scale of future disasters. Local authorities face the difficult task of deciding how to protect a large population in the event of earthquakes and other signs heralding the volcano's reawakening. \"There would be no modern precedent for an evacuation of this magnitude,\" says Giuseppe Mastrolorenzo at the Vesuvius Volcano Observatory in Naples. \"This is why Vesuvius is the most dangerous volcano in the world.\"  \n                Rumblings of dissent \n              The slumbering giant won't stay quiet forever. Seismic imaging studies have detected an unusual layer about 8\u201310 kilometres deep under the mountain's surface. Mastrolorenzo and his colleague Lucia Pappalardo interpret this layer as an active magma reservoir 1 , which could produce large-scale 'plinian'-style explosions \u2014 named after Pliny the Younger, who described the AD 79 eruption. The first rumblings of activity at Vesuvius could come weeks to years before an eruption, but there might be little, if any, warning of the eruption itself. Pappalardo and Mastrolorenzo analysed the geochemistry of rock fragments from past eruptions, and found evidence that magma ascended rapidly \u2014 in just a few hours \u2014 from its deep chamber to the surface. For many years the largest known eruption of Vesuvius was that of AD 79. But in 2006, Mastrolorenzo and Michael Sheridan at the University at Buffalo in New York described geological evidence for a much larger blast, about 3,800 years ago in the Bronze Age 2 . Fiery avalanches of ash and debris called pyroclastic flows travelled 20 kilometres and covered the whole of the area of present-day Naples. \"The deposits right in the centre of Naples are 4 metres thick,\" says Sheridan. \"Even a few inches would be enough to kill everyone.\" Given these concerns, the Vesuvius observatory team has urged the Neapolitan authorities to base their emergency plan on a worst-case 'maximum possible' eruption similar to the Bronze Age blast. \"A crisis could start today,\" says Mastrolorenzo. \"The trouble is that nobody would be able to tell how long it would last, what type of eruption it would be, or how the event would evolve.\" The researchers recommend the complete evacuation of an area 20 kilometres around Vesuvius if earthquakes and other signs of unrest hint that it is coming back to life. Not all scientists share this doom-laden outlook. Some groups have even proposed that Vesuvius is becoming less explosive. Bruno Scaillet and his colleagues at the University of Orleans in France argue that the eruptive style of Vesuvius has changed as the magma chambers feeding the eruptions have migrated upwards, with the small 1944 eruption coming from a relatively shallow level 3 kilometres below the surface 3 . Evidence suggests the magma stored there is less viscous, so it is less prone to causing large explosions. If the past trend holds, says Scaillet, the next eruption could be similar to the most recent ones. Scaillet adds that the seismically unusual layer 10 kilometres below the surface could be magma, but it could also be some other fluid such as water or brine. \"These various issues are far from being settled,\" he says.  \n                Emergency planning \n              With the size of any future eruption in doubt, and a public more concerned about day-to-day problems such as traffic and crime, mitigating the hazard of Vesuvius is an enormous task shared by researchers and the civil authorities. Scientists keep constant tabs on Vesuvius through a network of sensors that monitor for earthquakes, ground deformation and changes in the chemistry of escaping gases. And Italy's Department of Civil Protection (DPC) maintains a National Emergency Plan for Vesuvius. The plan, first developed in 1995, is based on a scenario for an intermediate-sized eruption, similar to one that occurred in 1631. That sub-plinian blast killed 6,000 people but affected an area much smaller than the earlier plinian eruptions. \n               Click here for larger image \n               The plan divides the area around the volcano into three regions according to the type of hazard expected. The red zone, closest to Vesuvius, is deemed most at risk from pyroclastic flows, so the plan calls for the evacuation of all 600,000 residents in this area before an eruption starts (see  'In the line of fire' ). The main danger in the yellow zone comes from falling ash and small rocks. Officials would wait until the eruption starts, and the wind direction is known, before ordering an evacuation of regions in yellow zones downwind of the volcano. The blue zone is at risk from floods and mud flows triggered by the eruption, and would be evacuated according to the same plan. The city of Naples was excluded from any of the hazard zones because the prevailing wind typically blows ash to the east, away from the city. In 2003, the DPC announced that it would constantly update the emergency plan to take account of new scientific information. The red zone is being expanded to include the eastern districts of Naples and officials reduced the evacuation time from two weeks to 72 hours, recognizing that there may be less of a warning before the eruption. Nevertheless, some researchers argue that the plan has ignored important scientific evidence. Last year, Mastrolorenzo and Pappalardo 4  and Giuseppe Rolandi 5  of the University of Naples found that even with an intermediate-sized eruption, pyroclastic flows would threaten several municipalities not currently included in the red zone. Mastrolorenzo says that officials should also not wait to evacuate the yellow zone, because fine ash would rapidly fill the air and plunge the area into total darkness. \"You have to get people out before it starts,\" he says. And the wind does sometimes blow towards Naples, so the authorities cannot rule out heavy ashfall in the city, say both Mastrolorenzo and Rolandi. Putting all the evidence together, they and other researchers insist that the emergency plan should correspond to the 'worst-case scenario', which means including metropolitan Naples and its 3 million inhabitants. That makes sense for planning, says Jonathan Fink, a volcanologist at Portland State University in Oregon. Once the volcano shows signs of unrest, authorities and scientists can re-evaluate. \"If there is an error on the high side, there is less lost than would be the case in the opposite situation,\" he says. In a written response to  Nature , the DPC advocates evaluating the eruption risk \"on the basis of the present state of the volcano and not simply assuming the largest eruption event that ever occurred in the volcanic history\". Some scientists agree. \"You can't spend [everything] on the absolute worst case. You need to reduce the risk in a rational way,\" says Warner Marzocchi at the National Institute for Geophysics and Volcanology (INGV) in Rome. A complete evacuation of Naples' 3 million residents, he says, \"would be impossible to manage\". Marzocchi and other researchers are developing modelling tools \u2014 based on the probabilities of different scenarios \u2014 that could help civil authorities evaluate the evidence during a crisis and choose a course of action. Peter Baxter, an expert in emergency planning at the University of Cambridge, UK, specializes in the impacts of volcanic eruptions and used this type of method successfully during the 1997 eruption in Montserrat in the Caribbean to predict which regions would be affected. A complete evacuation of the island was avoided. For Vesuvius, Baxter and his colleagues have used geological data and models of eruptive processes to develop an 'event-tree' to display the full range of possible eruptions 6 . If sensors on the volcano pick up signs of magmatic unrest, the analysis suggests a 70% probability of an explosive eruption but only a 4% chance of a catastrophic plinian one. The most likely event is a violent but smaller blast, like the one in 1944, with lava flows and moderate ash emissions. For now, this kind of probabilistic approach seems the only way forward for volcanologists and disaster planners, as there is no recipe for accurate eruption forecasting on the horizon. \"It's an extremely complex problem to solve,\" says Augusto Neri of the INGV's laboratories in Pisa. \"We simply do not know how the volcano works.\"\n Katherine Barnes is a freelance writer in London. \n                     Nature Geoscience \n                   \n                     Italy's Department of Civil Protection \n                   \n                     The Vesuvius Observatory \n                   Reprints and Permissions"},
{"file_id": "473272a", "url": "https://www.nature.com/articles/473272a", "year": 2011, "authors": [{"name": "Erika  Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "The field of induced pluripotent stem cells has grown up fast. Now it is entering the difficult stage. Five years is an eye-blink in science, but that's all the time it has taken for the concept of adult-cell reprogramming to revolutionize the field of regenerative medicine. In August 2006, Shinya Yamanaka of Kyoto University in Japan told the world that he had turned mouse skin cells into induced pluripotent stem (iPS) cells, capable of becoming many types of cell 1 . The following year, he repeated the feat for human cells 2 . Like human embryonic stem cells, iPS cells could potentially be used as therapies, disease models or in drug screening. And iPS cells have clear advantages: they can be made from adult cells, avoiding the contentious need for a human embryo, and they can be derived from people with diseases to create models or even therapies based on a person's genetic make-up. Scientists predicted that iPS cells would change the face of biology and medicine \u2014 and some would say they already have. In the past year or so, researchers have published cellular models derived from iPS cells for a staggering array of conditions, from heart defects 3  to schizophrenia 4 . And treatments based on iPS cells are moving toward the clinic: in California, for example, a team hopes to gain approval within the next three years to start treating people with the devastating skin disease epidermolysis bullosa using skin tissue grown from their iPS cells. Yet work in the past few months has highlighted several potential roadblocks. Reprogramming can be inefficient and induce mutations; the reprogrammed cells cannot develop into some cell types; and those they can generate are not always a good model for disease. New issues are emerging apace: work published last week 5  shows that, in a particular strain of mice, iPS cells cause immune reactions when they are transplanted into other mice with the same genetic make-up \u2014 raising questions about whether it will be possible to transplant iPS-cell-derived tissue back into the person from which it is made. No one doubts that iPS cells still have enormous potential, but the field's initial optimism has cooled. \"Right now, we are a long way from being sophisticated enough to take advantage of these cells' potential,\" says neuroscientist Arnold Kriegstein of the University of California, San Francisco (UCSF). \"Things are still at a very early stage.\" Here,  Nature   looks at some of the field's biggest challenges, and how they are being tackled. \n                1. Finding a recipe \n              From the start, biologists have tried to devise safer and more efficient recipes for making iPS cells than Yamanaka's method, which used a retrovirus to deliver a powerful shot of four genetic reprogramming factors into cells. Retroviruses integrate into a host cell's DNA and can therefore potentially disrupt gene expression and lead to cancer; and one of the reprogramming factors,  Myc , is itself an oncogene that could cause cancer. To outsiders at least, a new, 'improved' reprogramming method seems to be published every month. But Yamanaka's retroviral method is still the most efficient, and the one used most widely. The retroviral technique can transform about 0.01% of human skin stem cells into pluripotent cell lines; by comparison, adenoviruses, which do not integrate into the genome, transform just 0.0001\u20130.0018% of cells 6 , and delivering the reprogramming factors directly into a human cell transforms 0.001% (ref.  7 ). Inefficiency increases the cost and difficulty of deriving iPS cells for cell banks, and poses a particular problem when working with rare cell sources. Researchers have also tried omitting  Myc , as well as silencing it or stripping it from the cell once reprogramming is complete. But these workarounds also lower the efficiency of reprogramming, and a silenced  Myc   might be reactivated. Addressing these concerns is already a top priority for the field. Researchers continue to tinker with their reprogramming recipes, trying to find the factors, and means of delivery, that are the most efficient and don't increase the risk of cancer. In April, a group led by Edward Morrisey at the University of Pennsylvania in Philadelphia reported that it could boost the efficiency of reprogramming by two orders of magnitude over standard techniques by using a retrovirus to shuttle in a cluster of microRNAs 8 . \"It is very important for us to get these reprogramming methods to work well enough so that we can compare them and see whether they make any difference in the stability of the cells and in tumorigenicity,\" says developmental neurobiologist Jeanne Loring of The Scripps Research Institute in La Jolla, California. \"No one has done that yet, and it is going to be a long haul before we figure this out.\" \n                2. Patching the scars \n              A whole new set of questions has arisen over the past year, concerning the genetic impact of the reprogramming process. In July 2010, groups led by George Daley 9  at the Children's Hospital Boston and by Konrad Hochedlinger 10  at the Massachusetts General Hospital in Cambridge published studies showing that iPS cells carried an 'epigenetic memory' \u2014 chemical modifications in their DNA that had come from the original adult cells and had not been erased by the reprogramming. This, they say, explains why iPS cells cannot generate as many adult cell types as embryonic stem cells can. Researchers were soon reporting that iPS cells were more likely to contain mutations than cultured human embryonic stem cells. Four groups scoured the genomes of iPS cells for changes in single DNA bases 11 , DNA rearrangements called copy-number variations 12 , 13  and differences in chromosome number 14 . The studies found higher levels of all three. Worse, the mutations in iPS cells were not just inherited from the parent cells \u2014 some seemed to result from the reprogramming and culture process. Loring's group reported 12 , for instance, that a protocol for differentiating iPS cells into cardiac cells selected for cells with genetic rearrangements. This picture is still coming together. One of the studies 13  on copy-number variations found that many of the rearrangements disappeared after the iPS cells were cultured over long periods of time, probably because the most severely mutated cells were outcompeted by the genetically healthier ones. But this February, a team led by Joseph Ecker at the Salk Institute for Biological Studies in La Jolla, California, reported that it had detected epigenetic signatures of the parent cells in human iPS cell lines even after they had been cultured many times and differentiated into specific cell types 15 . A third study 16  suggested that iPS cells are no worse than embryonic stem cells in this regard. Developmental biologist Alexander Meissner of Harvard University in Cambridge and his team reported that epigenetic and genetic variation was similar across 20 human embryonic stem cell lines and 12 iPS cell lines. \"What we see is not so much a lot of variation across iPS cells, but a lot of variation across pluripotent cells,\" Meissner says. Researchers expressed their concerns about such effects at a meeting convened by the US National Institutes of Health and the Food and Drug Administration (FDA) in Bethesda, Maryland, on 21\u201322 March that focused on hurdles to translating research on pluripotent cells into the clinic. The concern is that the mutations could have unpredictable and undesirable effects on the cells, and on the patients they end up in. \"The genomic changes are going to be a big deal to the FDA,\" Loring says. Meissner's group has devised a 'scorecard' of gene expression and methylation \u2014 a type of epigenetic mark \u2014 that correlates with an iPS cell line's level of pluripotency. It should help researchers to identify and avoid the practices that generate the worst genetic aberrations, and to screen for the lines that are least affected. And researchers are beginning to examine whether and how these genetic and epigenetic effects affect the capabilities and characteristics of iPS cells. \"Right now there are two schools of thought on this,\" Loring says. \"One is that the sky is falling, and the other is that it's a good thing that we're finding out about this now, so that we can discover whether these are biologically relevant changes.\" \n                3. Hitting the limits \n              iPS cells are immensely flexible, but they can't do everything. Liver cells derived from iPS cells could in theory replace animals in drug toxicology screening, for example. But researchers have struggled to get any human stem cells to differentiate into tissues, such as liver, that are normally derived from the endoderm \u2014 the most interior of the three germ layers that make up the embryo. This might be because the series of signals necessary for these cells' development and complex function are difficult to recapitulate. Liver specialist Holger Willenbring at the UCSF points out that hepatocytes have many roles, from detoxifying the blood to making circulating proteins. \"There are a lot of different jobs that the cell has to accomplish, and it is hard to get that right in cell culture,\" he says. One of the hottest areas of research on human stem cells is in cell-replacement therapy for type-1 diabetes, a disease that develops when insulin-producing cells in the pancreas are destroyed. But no one has been able to make a fully functional and mature insulin-producing pancreatic beta cell \u2014 also derived from the endoderm \u2014 because researchers don't know the exact series of growth signals necessary and, perhaps, because beta cells usually develop in a three-dimensional environment that is difficult to replicate in a culture dish. Maybe this won't matter, says developmental biologist Matthias Hebrok at the UCSF. Beta-cell 'progenitors' have been made, from embryonic stem cells and from iPS cells, that can secrete insulin. They are less efficient than a normal beta cell, but perhaps efficient enough to help a person with diabetes. Researchers are making a concerted effort to devise the correct recipe for making mature beta cells and hepatocytes, and Willenbring and others say that those in the field are working as a team on this for the first time. A paper published last week 17  circumvented iPS cells altogether, describing the generation of hepatocyte-like cells directly from mouse skin cells, using a cocktail of regulatory proteins important for liver development. But Willenbring says he still has questions about whether the cells were able to perform all the functions of hepatocyes.  \n                4. Maintaining standards \n              The relative ease of reprogramming has thrown the iPS cell field open to almost anyone. But from the start, researchers have worried that the low barrier to entry and the extremely competitive pace have meant that standards are not as rigorous as they should be. \"There's a tremendous amount of pressure to get these papers out there, and in the rush investigators are not characterizing their cells very well,\" Kriegstein says. Kriegstein points to a paper published last November 18  by a team led by Alysson Muotri of the University of California, San Diego. The team studied people with a mutated gene that causes the neurological condition Rett syndrome as a model for an autism-spectrum disorder, which causes behavioural difficulties. The researchers derived iPS cells from these patients, and showed that as the iPS cells differentiated into neurons, they initially expressed genes typically found in neural 'precursor' cells, then later expressed genes involved in neuronal signalling. The neurons from the patients with Rett syndrome were smaller than those derived from people without the disease, and they also had signalling defects and other differences. But Kriegstein says that this molecular characterization is not enough to show exactly what type of neurons had formed, what part of the central nervous system they represented and how they therefore relate to processes that go awry in the brain. \"Possibly the reported abnormalities are relevant to the intended diseases,\" Kriegstein says, referring to this and other iPS papers, but this would be very surprising given that most neurodevelopmental and neurodegenerative diseases affect specific populations of neurons at specific times during development, he says. Muotri says that his team's analysis of the neurons was thorough, and included electrophysiological tests showing that the cells could fire action potentials and were therefore functional. He also points out that almost all neurons in the brain express the gene, so he did not want to limit the studies just to regions that had been associated with Rett syndrome, as insights from the cultured neurons could illuminate the molecular mechanisms that disrupt the circuits controlling behaviour. \"When we find such a cellular phenotype in culture, we know we can now start from there to understand other layers of complexity,\" Muotri says. \n                5. Modelling the mind is hard \n              Investigators are creating patient-specific iPS cells to model almost every disease known \u2014 but in some cases, researchers question how much can be learned from the models. The biggest debate is over models of complex neuropsychiatric and behavioural disorders: can reprogrammed cells really mimic conditions such as schizophrenia or autism, which affect the brain and behaviour in complicated ways? \"I've had clinicians ask if we can make iPS cells from a patient who was mentally retarded,\" says developmental biologist Christine Mummery of the Leiden University Medical Center in the Netherlands. But she questions how useful that would be. \"I said, 'I don't know, how you would measure the IQ of a neuron in a dish?'\" Researchers working with these models argue that they are still valuable. In April, Fred Gage at the Salk Institute and his team reported 4  that they had derived neurons from the skin cells of a person with schizophrenia, and that some differences between those neurons and normal neurons could be corrected by administration of the antipsychotic drug loxapine. Like Muotri, Gage says that the model is designed to uncover how the genetic factors underlying schizophrenia affect the function of neurons. \"Although we cannot measure the behaviour of the patients, we propose that we can measure the activity of the neurons, and the goal is to search for cellular and molecular processes that underlie the behavioural phenotypes,\" he says. A related problem arises with diseases of ageing, a hot field in iPS-cell research: many of the conditions strike mature cells, so iPS cells \u2014 which are essentially starting their developmental lives afresh \u2014 might not be relevant. With a disease such as Parkinson's, says Mummery, \"this is a real issue \u2014 will you be able to get neurons mature enough to see anything? People are working very hard not only to make their cell type of interest but also to make them mature, so that's still a major technical obstacle.\" Some researchers counter that even 'young' cells show traits related to diseases of ageing. Renee Reijo Pera at Stanford University in California made iPS cells from Genia Brin \u2014 the mother of Google co-founder Sergey Brin. She has Parkinson's, a condition that is marked by the destruction of dopamine-producing neurons. Once differentiated into neurons, the cells secreted dopamine and were more sensitive to chemicals that can induce cell death than were dopamine-secreting neurons derived from healthy people 19 . \"This seems to be to be the best model of Parkinson's disease,\" Reijo Pera says. Many iPS-cell researchers see the field's growing pains as signs that it is reaching a state of maturity; they say that the problems are no different from those that many biomedical research fields face as they inch towards clinical application. \"There was this huge euphoria in the beginning, with everyone thinking iPS will do everything, cure all diseases, and be super-easy,\" Meissner says. \"But not everyone can become a stem-cell biologist overnight,\" he says. \"It's a bit of a reality check that things are not as simple as we thought.\"\n Erika Check Hayden is a senior reporter for  Nature  based in San Francisco. \n                     Kriegstein lab \n                   \n                     Willenbring lab \n                   \n                     Loring lab \n                   \n                     Muotri lab \n                   \n                     Mummery lab \n                   \n                     Meissner lab \n                   Reprints and Permissions"},
{"file_id": "473439a", "url": "https://www.nature.com/articles/473439a", "year": 2011, "authors": [{"name": "Corie Lok"}], "parsed_as_year": "2006_or_before", "body": "Can Bruce Walker transform HIV vaccine research? Bruce Walker didn't want to sit next to Terry Ragon on the 24-hour plane ride from Boston to South Africa. He had only recently met the wealthy, Cambridge, Massachusetts-based software executive and was about to spend two full days touring AIDS-ravaged Durban with him in hope of obtaining a donation. Walker, an immunologist and physician at Massachusetts General Hospital (MGH), wanted to give Ragon some space and get some work done, but Ragon insisted they sit together. During the flight, he peppered Walker with questions about his research in South Africa. He also warned him not to get his hopes up. \"I go on a lot of these kinds of trips, and I don't give people very much money,\" Ragon said. Walker was disappointed, but he stuck to the plan. He took Ragon to the crumbling, 100-year-old McCord Hospital, where he followed doctors and visited impoverished, young people with HIV. \"All three of the patients I sat in with were going to die, and one of them was dying right there in front of me,\" says Ragon. He had been to Africa before but never had he so intimately seen the pain and suffering caused by AIDS. As the trip neared its end, Walker knew that it was time to broach the subject of money again. He had been trained by MGH fundraisers to give potential donors a range of options. For a modest sum, US$5,000\u201320,000, Ragon could fund lab equipment or nurses \u2014 $1 million might fund a small clinical trial. But on a whim, Walker decided to float a more ambitious idea: creating an institute in which researchers from different fields could focus solely on HIV vaccines under one roof, with the kind of funding that would enable high-risk projects. \"I thought the idea was half-baked,\" says Ragon, \"but it intrigued me.\" That was in March 2007. Talks continued, and about a year later Ragon and his wife, Susan, agreed to give $100 million over 10 years to create the Ragon Institute of MGH, MIT and Harvard. Established in early 2009, with Walker as its director, the institute was a positive note at a challenging time for HIV vaccine development. In late 2007, the pharmaceutical company Merck announced that a high-profile vaccine candidate in a large phase II clinical trial failed to protect people from infection with HIV, and even increased the risk of infection for some 1 . Then, later in 2009, another surprise came with the results from a huge phase III trial in Thailand, dubbed the RV144 trial, showing that a combination of two previously unsuccessful vaccines had defied expectations and provided modest protection from infection 2 . Although researchers were cautious about the work, the results offered a glimmer of hope that protection was possible. \n               Click here for larger image \n               Researchers say that the trials \u2014 two of only three major vaccine efficacy studies undertaken in 25 years of research \u2014 have reminded them of just how little they know about harnessing the immune system to block this deadly disease. Researchers have called for a return to basic HIV research (see  'Where the money goes' ). But they need stronger interdisciplinary collaboration, innovative trial designs and, perhaps most of all, freedom \u2014 through funding \u2014 to take chances. Now, many say that the Ragon Institute has the opportunity to put these factors to work. Walker can clearly draw the money, and Anthony Fauci, director of the National Institute of Allergy and Infectious Diseases in Bethesda, Maryland, says that Walker can also gather the right team. \"He has a special talent for getting groups of people together from diverse backgrounds in a collaborative, synergistic way,\" he says. \"The challenge,\" says Herbert Virgin, an immunologist at Washington University in St Louis, Missouri, and head of the institute's external scientific advisory board, \"is now putting it all together to generate a vaccine.\"  \n                Hooked on basics \n              Two months ago in his MGH office, Walker was hanging on the words of Krista Dong, a Ragon Institute physician who lives in South Africa. Dong was rifling through detailed, handwritten notes about a clinical study she has been helping to design. The project aims to look at the immunological events during the first few days of HIV infection \u2014 most studies haven't been designed to capture this information. The researchers planned to collect blood samples from 200 young, uninfected women twice a week for a year. The research could provide crucial information about how the virus takes hold in the body \u2014 information that cannot be gleaned easily from animal studies \u2014 but it requires extraordinary cooperation and trust from the participants. As Dong explained how she and her team would do this through HIV-prevention and job-training programmes, the smile on Walker's face grew. Walker is a listener and, unlike many of his Boston peers, he speaks softly and slowly. Walker asked a few well chosen questions on logistics but was clearly sold. \"Let's start!\" he said, then began to list possible sources of funding, mostly from foundations and philanthropists. Although he hadn't actually secured the money, he urged Dong and her team to plough ahead. \"Bruce is endlessly optimistic in an infectious way,\" says Dong. \"He provides an environment that inspires innovation and personal drive.\" Walker's education as a fundraiser began in the mid 1990s, when he was head of the Partners AIDS Research Center at the MGH. He learned from a development officer there how to ask for a million dollars, something Walker found difficult at first. \"How could I ask that?\", he recalls thinking. \"But then I realized \u2014 how could I, in good conscience, not ask for help?\" He soon learned the power of showing, rather than telling, people what their money can do. In 2000, a postdoctoral fellow working out of a closet-sized lab in Durban encouraged Walker to visit what was becoming the epicentre of the African AIDS epidemic. South Africa had come to lead sub-Saharan Africa in both the number of people living with HIV (5.6 million in 2009) and the number dying from AIDS (310,000 in 2009). Durban is the largest city in the most highly affected province, KwaZulu-Natal. Here, six in ten women are HIV-positive by the age of 23. Walker was moved by what he saw. And when he met with other local HIV researchers looking for a bigger, better lab space in which they could work together, they hatched an idea to build a new research institute. Walker sent a proposal to the Doris Duke Charitable Foundation in New York, which was already funding some of his work. As he would do for Ragon several years later, he invited the head of the science programme to Durban. Shortly afterwards, the foundation committed $1.8 million to the construction of a new building, and another $2.25 million to support research and training for four years. The Doris Duke Medical Research Institute opened in 2003 on the campus of the Nelson R. Mandela School of Medicine at the University of KwaZulu-Natal. It was the foundation's first major international grant for HIV. Walker maintains close ties with the researchers at the institute through frequent phone calls and videoconferences. He also makes the long flight over there every other month, and co-supervises two PhD students and a postdoc. Walker's laid-back, affable style helps him connect with people, and his connections have greatly helped his work. With his collaborators, he developed a cohort of around 1,200 people with HIV in South Africa. Studies on these individuals have provided many insights over the past decade, demonstrating, for example, how the virus evolves as the disease progresses. \"It's not easy establishing all those links and collaborations and trust,\" says Andrew McMichael, an HIV immunologist at the University of Oxford, UK. Walker has also taken an interest in the roughly 1 in 300 people with HIV who are able to keep the virus in check without any drugs and don't progress to AIDS. Walker and his colleagues painstakingly tracked down some of these rare patients, known in some circles as 'elite controllers', by connecting with HIV patient groups and physicians. They built up a cohort of about 1,500 controller patients, along with a bank of their blood samples, which they have shared with other research groups. They want to discover how these people suppress the virus, in the hope that the mechanism can be mimicked using a therapeutic vaccine. The clues are mounting. A type of immune-system cell called a CD8 +  or 'killer' T cell may exert selective pressure that allows only weaker versions of the virus to survive in controllers 3 . \"Our direction right now is to try to understand how exactly these cells are driving these viruses to be less fit,\" says Walker. But some researchers think that the controllers' killer T cells are able to reproduce more and produce larger amounts of perforin \u2014 a protein that can poke holes in infected cells to help kill them 4 . Whatever mechanisms are at work, exploiting them will be a challenge, says Larry Corey, the principal investigator for the HIV Vaccine Trials Network (HVTN) and the head of the Fred Hutchinson Cancer Research Centre in Seattle, Washington. Studying the controllers \"is important conceptual work\", he says. \"How to translate that into making an effective vaccine is easier said than done.\"  \n                Branching out \n              Meanwhile, Walker has been trying to pull researchers from other fields into the fold. From the start, he wanted to build an interdisciplinary team to oversee the Ragon Institute, one that could bring fresh perspectives to bear on issues that have dogged HIV researchers for 25 years. Walker recruited a steering committee to help him oversee the institute, including a materials scientist and a computational biologist, both from the Massachusetts Institute of Technology (MIT) in Cambridge. A group of 14 labs at the MGH form the core of institute, which funds collaborative research projects headed up by at least two principal investigators. One key area Walker has built up in his institute is basic HIV research, which has been held back by inadequate animal models. The HIV field, he says, has grown insular, with little interaction with immunologists doing basic research. He brought in a leading immunologist, Laurie Glimcher at the Harvard School of Public Health in Boston, Massachusetts. Glimcher had a history of branching out into different disciplines but had never worked on HIV. She now heads the institute's basic immunology programme and is overseeing the development of a humanized mouse core \u2014 a bank of mouse models that have key components of a human immune system. Walker hopes that these mice will allow researchers to test preliminary vaccines  in vivo   sooner than they can now. Walker also pushed to have physical scientists join the team, something that resonated with Ragon, who has a physics degree from MIT. Three years ago, Walker approached Arup Chakraborty, a computational immunologist at MIT who had not studied HIV. Chakraborty was sceptical that he could contribute much to the field. But, as for others, an emotional trip to South Africa changed his mind. Now he heads the computational biology programme at the institute and more than one-third of his lab's work is devoted to HIV. Chakraborty's modelling expertise has helped to reveal important information in the wealth of immunological data already available. His preliminary work has shown, for example, that some variations in the HIV genome are linked. This means that if some mutations happen without others, the virus might become vulnerable to immune attack, says Walker. Walker's overarching goal for the institute is to contribute to the development of an HIV vaccine and, while doing so, create a team culture rather than one that emphasizes individual credit. \"It is going to take all of us making a contribution,\" says Walker. Walker says that better modelling of data and broader collaborations could feed into a new breed of clinical trial. Typical phase II and III vaccine trials are geared towards showing efficacy. That means that they need to recruit enough patients to be able to statistically show an effect. For a single trial, that can take years and cost many millions of dollars. (The RV144 trial enrolled more than 16,000 participants at a cost of $103 million, which is typical of vaccine phase III trials.) Walker aims to do a different type of vaccine trial: one that enrols 10\u201320 people, half of whom would receive a placebo. Researchers would then do a detailed analysis of the patients' immune responses, from T-cell activities to antibody generation. He hopes that the analyses, when done alongside or even before larger efficacy trials, would provide more clues about whether and how a vaccine candidate is stimulating the immune system \u2014 a crucial missing piece of the HIV vaccine puzzle. Such smaller, faster trials would allow candidates to be tested in parallel and hopefully give quicker indications of success or failure for less cost. This approach has been tried before, but until recently few HIV vaccine candidates elicited a strong enough response to study in this way. Two vaccine candidates inherited by the institute are now in phase I trials; one of them is being tested in collaboration with HVTN and the International AIDS Vaccine Initiative (IAVI), headquartered in New York. The institute is also developing other candidates and is collaborating with the IAVI to build clinical laboratory infrastructure in Durban, where testing can be done more cheaply than in Western countries. The facility should be operating by early 2013. Getting vaccine candidates into clinical trials, and testing them in innovative ways at a lower cost, will be key to the success of the Ragon Institute. And to encourage more interactions between the groups, Walker's lab, along with several other Ragon labs at the MGH and MIT plan to move in about a year into a new building near MIT. Ragon researchers based at other institutions will have space there as well. But future trials will depend on whether Walker can continue to raise funds. Even $100 million, the largest donation in the MGH's history, is not enough to tackle the sheer complexity of the virus and the large number of unknowns about how to stimulate the immune system to fight off the disease, Walker says. He is still searching for funding from donors and chasing grants. He will undoubtedly be bringing more people to Durban. And he remains optimistic. \"This is a solvable problem,\" he says. \"There's no time to waste.\" Corie Lok is Nature's Research Highlights editor in Cambridge, Massachusetts. \n                     Vaccines Special \n                   \n                     Outlook: HIV/AIDS \n                   \n                     The Ragon Institute \n                   \n                     RV144 Clinical Trial \n                   \n                     International AIDS Vaccine Initiative \n                   \n                     HIV Vaccine Trials Network \n                   \n                     The International HIV Controllers Study \n                   \n                     Bill & Melinda Gates Foundation \n                   Reprints and Permissions"},
{"file_id": "473436a", "url": "https://www.nature.com/articles/473436a", "year": 2011, "authors": [{"name": "Roberta Kwok"}], "parsed_as_year": "2006_or_before", "body": "Hysteria about false vaccine risks often overshadows the challenges of detecting the real ones. John Salamone is not a vaccine sceptic. He has never been persuaded by spurious claims that vaccines are toxic to children and responsible for autism or a host of other ailments. But tragically, Salamone found out first-hand that vaccines do have real, rare side effects when he saw his infant son, David, become weak and unable to crawl shortly after receiving the oral polio vaccine in 1990. After about two years of physical therapy and doctors' visits, Salamone learned that owing to a weakened immune system, David had contracted polio from the vaccine. \"We basically gave him polio that day,\" says Salamone, who has retired from a position as a non-profit executive, and lives in Mount Holly, Virginia. That was a known risk of the vaccination, which causes roughly one case of the disease per 2.4 million doses, often in people with an immune deficiency. A safer, inactivated, polio vaccine was available at the time, but the oral vaccine was cheaper, easier to administer and thought to be more effective at controlling outbreaks. But by the 1980s, polio had been all but eliminated in the United States; all cases originating in the country came from the vaccine. Salamone and other parents successfully campaigned for the United States to shift to the safer version in the late 1990s. Vaccines face a tougher safety standard than most pharmaceutical products because they are given to healthy people, often children. What they stave off is unseen, and many of the diseases are now rare, with their effects forgotten. So only the risks of vaccines, low as they may be, loom in the public imagination. A backlash against vaccination, spurred by the likes of Andrew Wakefield \u2014 a UK surgeon who was struck off the medical register after making unfounded claims about the safety of the measles, mumps and rubella (MMR) vaccine \u2014 and a litany of celebrities and activists, has sometimes overshadowed scientific work to uncover real vaccine side effects. Many false links have been dispelled, including theories that the MMR vaccine and the vaccine preservative thimerosal cause autism 1 . But vaccines do carry risks, ranging from rashes or tenderness at the site of injection to fever-associated seizures called febrile convulsions and dangerous infections in those with compromised immune systems. Serious problems are rare, so it is hard to prove that a vaccine causes them. Studies to confirm or debunk vaccine-associated risks can take a long time and, in the meantime, public-health officials must make difficult decisions on what to do and how to communicate with the public. Still, such work is necessary to maintain public trust, says Neal Halsey, a paediatrician at the Johns Hopkins Bloomberg School of Public Health in Baltimore, Maryland. \"If we don't do the research, there will be more people who don't believe in vaccines,\" he says.  \n                Victims of their own success \n              Technological advances have made modern vaccines purer and safer than their historical counterparts. Most developed countries have switched to the inactivated polio vaccine and stopped using whole-cell pertussis (whooping cough) vaccines, which are made from killed bacteria and cause relatively high rates of arm swelling, febrile convulsions and periods of limpness or unresponsiveness. \n               Click here for larger image \n               Improved safety means that researchers are sometimes searching for vanishingly small risks. Although vaccines must undergo stringent safety tests before distribution, the trials typically don't enrol enough people to catch risks on the order of one case per 10,000\u2013100,000 people (see  'Calculating risks' ). The only way to find such side effects is to deploy the vaccine in the population and watch. Officials have become increasingly vigilant. As worries about pandemic H1N1 influenza spread in 2009\u201310, several companies worked to prepare as many vaccine doses as possible. Meanwhile, health officials launched an unprecedented surveillance effort to monitor the vaccines' safety. US scientists and officials studied data from voluntary adverse-event reports, managed-care organizations, health-insurance companies, immunization registries, a network of neurologists and various health-care systems. European scientists linked data from 15 countries. And Chinese officials instructed health-care workers to report potential side effects within 24 hours; for the most serious events, they had two hours. Scientists were specifically looking for Guillain-Barr\u00e9 syndrome, a paralytic disorder that is often treatable but can cause long-term disability or death. A 1976 swine-flu vaccine distributed in the United States was associated with between five and nine cases per one million vaccine recipients. Studies of subsequent flu vaccines have not shown a consistent link, but officials have been on the lookout for it. During the 2009\u201310 pandemic, something stranger turned up: some 60 cases of narcolepsy emerged among 4- to 19-year-olds in Finland. Most had received the H1N1 vaccine Pandemrix, made by GlaxoSmithKline in Brentford, UK. Another narcolepsy cluster showed up in Sweden. Scientists have yet to confirm whether the vaccine caused the rise in incidence. Surveillance efforts have paid off for a variety of vaccines. A rotavirus vaccine was suspended in the United States in 1999 after public-health officials received 15 reports of intussusception, an infolding of the bowel, in vaccinated infants. The mechanism is uncertain, but the live-virus vaccine might cause swelling of bowel lymph nodes and increase contraction, leading to infolding. The vaccine is estimated to have caused about one case of intussusception per 10,000 recipients. In 2007, Nicola Klein, co-director of the Kaiser Permanente Vaccine Study Center in Oakland, California, and her colleagues found that children aged between 12 and 23 months who had been immunized with a combination vaccine for measles, mumps, rubella and varicella (MMRV) had more febrile convulsions 7\u201310 days after vaccination than those receiving separate MMR and varicella vaccines. The finding prompted a US immunization advisory committee to withdraw its preference for the MMRV vaccine. A subsequent study 2  suggested that the combined vaccine resulted in one more febrile convulsion per 2,300 doses than the MMR and varicella vaccines given separately. Efforts are under way to improve surveillance in low- and middle-income countries, some of which are gaining increased access to vaccines through an international programme called the GAVI Alliance (formerly the Global Alliance for Vaccines and Immunisation), based in Geneva, Switzerland. These areas could soon see new vaccines for diseases such as dengue and cholera. In 2006, the Pan American Health Organization, based in Washington DC, started a surveillance network among five Latin American countries. The World Health Organization (WHO) in Geneva is working with 12 countries, including Iran, Tunisia, Vietnam and India, to develop methods and tools for vaccine-safety monitoring, and half are already reporting to a global database, says Patrick Zuber, the WHO's group leader of global vaccine safety. Researchers have also started conducting larger clinical trials. Pre-licensure trials for two new rotavirus vaccines, RotaTeq by Merck, based in Whitehouse Station, New Jersey, and Rotarix by GlaxoSmithKline, each enrolled more than 60,000 infants to evaluate safety 3 , 4 . But even these large trials cannot rule out rare events, so efforts would be better spent on well planned surveillance after licensing, argues Rino Rappuoli, global head of vaccines research at Novartis Vaccines and Diagnostics in Siena, Italy. With big pre-licensure trials, \"you may feel better as a regulator, but you're not answering the scientific question\", he says. Preliminary post-licensure studies in Mexico have detected a possible slight increase in intussusception risk after the first dose of Rotarix, and a similar pattern has emerged in Australia for both vaccines 5 . However, some researchers speculate that rotavirus vaccination may also protect against intussusception later.  \n                Delayed results, lost trust \n              Even if a possible side effect is found, long periods of uncertainty can follow. To amass convincing evidence, scientists sometimes need to do controlled studies in multiple countries, covering hundreds of thousands or even millions of people. Scientists have not yet conclusively determined whether Pandemrix contributed to the European cluster of narcolepsy cases. Scientists in the Vaccine Adverse Event Surveillance & Communication Consortium, a European research network, are examining narcolepsy diagnosis rates and comparing cases with matched controls across several European Union countries, some of which used different H1N1 vaccines. Data suggest that diagnosis rates rose slightly in several countries starting in 2008, before H1N1 vaccines were being distributed, but not enough to explain the episode in Finland, says principal investigator Miriam Sturkenboom, a pharmacoepidemiologist at Erasmus University Medical Center Rotterdam in the Netherlands. GlaxoSmithKline is also funding a study in Canada, where an H1N1 vaccine nearly identical to Pandemrix was used, but no rise in narcolepsy has been reported. The increase in narcolepsy diagnoses might be explained by heightened disease awareness or infections with the H1N1 virus itself, says Jan Bonhoeffer, a paediatric-infectious-disease specialist at the University Children's Hospital Basel in Switzerland, and chief executive of the Brighton Collaboration, an international vaccine-safety research network. He says that the narcolepsy story fits a familiar pattern, similar to that seen with MMR and autism: people are eager to find an underlying cause for a serious, chronic, poorly understood disease. Researchers need to investigate possible safety issues quickly, Bonhoeffer adds. Otherwise, by the time scientists conclude that a concern is unfounded, \"no one cares, and it takes years to build up the trust again\", he says. \"So often, the widely communicated concern has caused more harm than it intended to prevent.\" A global vaccine-safety network would give scientists a faster way to test hypotheses with sufficient sample sizes, he says. In that spirit, the WHO is coordinating a global study on pandemic H1N1 flu vaccines and Guillain-Barr\u00e9 syndrome. But strictly controlled randomized trials \u2014 the highest standard of evidence for determining causality \u2014 are often not possible because of the large number of participants needed. And randomized trials in one location will not prevent some researchers questioning whether the results apply in others, says Alfred Berg, a clinical epidemiologist at the University of Washington in Seattle. Even if surveillance efforts became faster and more thorough, public-health officials still need to make quick decisions with incomplete data. Authorities often err on the side of caution, but warnings can make the public wary. In March, for example, Japanese officials suspended a vaccine for pneumococcal illnesses and one for  Haemophilus influenzae   type b when four children died shortly after immunization. Officials later concluded that there was no direct evidence of a link, but the episode still caused a scare, says Pier Luigi Lopalco, head of the vaccine-preventable-diseases programme at the European Centre for Disease Prevention and Control in Solna, Sweden. Suspending a vaccine tends to get more media attention than resuming one, he says, so people remember only the threat. US government officials have drawn criticism for pushing for removal of thimerosal from vaccines, despite a lack of evidence that it poses a risk. \"People said, why are you removing this if it's not a problem?\" says Ken Bromberg, a paediatrician at the Brooklyn Hospital Center in New York. \"It must really be a problem even though you say it's not.\" But inaction would have caused a loss of credibility, says Halsey. \"That is not something I think the public would have accepted.\"  \n                Finding those in danger \n              Researchers have long known that some individuals are more susceptible to vaccine risks than others. Immunocompromised individuals have generally been discouraged from receiving live-virus vaccines. But other possible vulnerabilities are less clear. Some speculate that children with metabolic disorders might be prone to vaccine side effects, but two studies published in April suggest otherwise. Klein and her colleagues reported 6  that children with inherited metabolic disorders do not show an increase in emergency-department visits or hospitalizations in the 30 days after being immunized. The other study found that children with one type of metabolic disorder \u2014 urea cycle disorders \u2014 did not have more serious metabolic problems than usual within 21 days of vaccination 7 . Some researchers hope that doctors will eventually be able to screen people for genetic predispositions to vaccine side effects. Gregory Poland, a vaccinologist at the Mayo Clinic in Rochester, Minnesota, says that once predispositions have been identified, genetic screening would at least make the risks and benefits explicit. Scientists have begun studying predispositions to side effects from smallpox vaccination: Kathryn Edwards, a vaccinologist at Vanderbilt University in Nashville, Tennessee, and her colleagues have reported 8  two genes that might be associated with reactions such as rashes, and Poland's team is searching for genetic risk factors for myopericarditis \u2014 inflammation of the heart muscle and surrounding tissue. Even if immunization does prove risky for certain children, withholding the vaccine could pose a greater threat. Vaccine-preventable diseases can be particularly severe or even fatal for patients with metabolic disorders, says Marshall Summar, chief of the division of genetics and metabolism at the Children's National Medical Center in Washington DC. Edwards and her colleagues have been studying how children with mitochondrial disorders, a group of metabolic disorders, respond to vaccines and natural infections. If vaccines present a risk, doctors could take steps to counteract possible effects, for example by ensuring that the child is well nourished after immunization, says Edwards. Safer vaccines and manufacturing processes are also in the works. A Novartis plant in Holly Springs, North Carolina, will produce influenza vaccine doses in cell culture, rather than the industry-standard chicken eggs. This process will improve reliability and reduce allergic reactions to egg proteins, says Rappuoli. The plant will be ready to make pandemic-flu vaccine this year if needed, he says. Researchers are also developing replacements for vaccines that can be risky for vulnerable groups. These include current smallpox vaccines that cannot safely be given to immunocompromised people; the tuberculosis vaccine, which is not recommended for HIV-positive infants; and the yellow-fever vaccine, which puts elderly people at particular risk of a yellow-fever-like illness. The challenge will be to make safer vaccines just as effective: James Cherry, a paediatric-infectious-disease specialist at the University of California, Los Angeles, speculates that an outbreak of whooping cough in California in 2010 might have occurred partly because the safer acellular pertussis vaccines now in common use in developed countries tend to be less effective than the best whole-cell vaccines. Researchers are quick to emphasize that the benefits of vaccines still greatly outweigh the risks. But as diseases recede from the public's memory, the population's tolerance for side effects will drop even further. \"If you don't know the diseases and you haven't seen them, then you really aren't willing to accept any risk,\" says Edwards. Despite scientists' best efforts, eliminating risk is impossible. Vaccines are biological products with biological effects, says Juhani Eskola, deputy director general of Finland's National Institute for Health and Welfare in Helsinki. \"We can never make them 100% safe.\" \n                 See Editorial  \n                 page 420 \n               Roberta Kwok is a freelance writer in Burlingame, California. \n                     Vaccines Special \n                   \n                     Biotechnology@nature.com \n                   \n                     US Vaccine Adverse Event Reporting System \n                   \n                     US Vaccine Safety Datalink \n                   \n                     Finland's report on Pandemrix and narcolepsy \n                   \n                     Sweden's report on Pandemrix and narcolepsy \n                   \n                     Brighton Collaboration \n                   \n                     Vaccine Adverse Event Surveillance & Communication \n                   \n                     US plan to monitor 2009 H1N1 flu vaccine safety \n                   \n                     GAVI Alliance \n                   \n                     WHO site on immunization safety \n                   \n                     US National Vaccine Injury Compensation Program \n                   Reprints and Permissions"},
{"file_id": "471433a", "url": "https://www.nature.com/articles/471433a", "year": 2011, "authors": [{"name": "Adam Mann"}], "parsed_as_year": "2006_or_before", "body": "The race to detect dark matter has yielded mostly confusion. But the larger, more sensitive detectors being built could change that picture soon. For a substance that is utterly invisible, dark matter does a remarkably good job of making its presence felt. Astronomers have been compiling evidence for it since the 1930s, tracing how it shapes galaxies, galaxy clusters and even bigger cosmic structures by the inexorable force of its gravity. Although its real nature is unknown, dark matter seems to outweigh the ordinary matter visible in stars and galaxies by roughly 5.5 to 1. Down here on Earth, however, physicists struggling to answer the 'what is it?' question often feel like they're chasing a ghost. Certainly, their detectors have been giving them a lot of strange and contradictory results. Two experiments are independently seeing what seems to be a flux of dark matter streaming through their apparatus. Another detector may have seen a handful of dark-matter particles last year \u2014 although the experimenters dismiss them as background noise. And yet another experiment has found no evidence for dark matter at all. Fortunately, this confusion is likely to be temporary. Dark-matter detectors are roughly 1,000 times more sensitive to ultra-rare events than they were 20 years ago, and that should increase by another factor of 100 over the next decade, as physicists build bigger detectors and become more skilled at suppressing the background noise than can be confused with genuine signals (See  'Dark-matter detectors' ). \"It would not be surprising if a year from now someone stood up and said we have done it, we've detected dark matter,\" says Sean Carroll, a theoretical physicist at the California Institute of Technology in Pasadena. Other physicists give a more cautious estimate of five to ten years. Nonetheless, there is a palpable sense that the field is on the verge of something big. \n               Click here for larger image \n               Most of the attempts to detect dark matter directly have started from the assumption that the stuff is a haze of weakly interacting, massive particles (WIMPs) left over from the Big Bang. The 'massive' part would explain the gravity. And the 'weakly interacting' part would explain the invisibility: the WIMPs would flow through stars, planets and people in untold numbers, almost never hitting anything. That assumption dictates the basic detection strategy: bring together a large target mass of material; put it deep underground to shield it from cosmic rays and other radiation that could produce misleading signals; then measure the recoil energy when a dark-matter particle finally hits an ordinary nucleus. The larger the mass of material, the more likely it is that a dark-matter particle will hit something. Beyond those basics, setting up such an experiment requires a certain amount of guesswork. To have a significant recoil effect, for example, researchers need a target nucleus of roughly the same mass as the dark-matter particle they are seeking. It's like watching for an invisible pool ball, says Jonathan Feng, a particle physicist at the University of California, Irvine. If the target nucleus is the equivalent of a bowling ball, the impact will barely move it. If, on the other hand, the target is the equivalent of a ping-pong ball, it will hardly be capable of deflecting the dark-matter particle, and so again there will be little energy transferred. What you want is another pool ball, Feng says.  \n                Supersymmetrical WIMPs \n              Several dark-matter experiments have placed their bets on supersymmetry: a theory in which each particle in the standard model of physics would have a heavier, and so far unobserved, partner 1 . Supersymmetry predicts the existence of a WIMP called a neutralino, which would have exactly the right properties to account for the dark-matter distribution seen in the Universe. Its interactions would be feeble enough, yet its mass would be substantial \u2014 fifty to a few thousand times the mass of a proton. One of the most highly regarded of the neutralino detection efforts is the XENON Dark Matter Search Experiment, located in the underground portion of the Gran Sasso National Laboratory near L'Aquila, Italy, and operated by a consortium of US and European universities. As its name suggests, the experiment's detection medium is a tank of liquid xenon, which has a mass of just over 131 atomic mass units \u2014 close to ideal for detecting WIMPs at the lighter end of the supersymmetry range, which is by far the easiest place to start the search. Photomultiplier tubes lining the inside of the XENON tank look for the characteristic flash of light called scintillation that would be generated if a xenon atom had recoiled from the impact of a WIMP. The XENON collaboration's first detector, built in 2006, used around 15 kilograms of xenon and found nothing that could not be attributed to background radiation. The team then upgraded to a bigger, more sensitive, 161-kilogram version in 2009, dubbed XENON100. Although an initial 11-day data run on this detector still failed to find any particles 2 , that result was significant in itself: WIMPs with a mass of less than 100 gigaelectronvolts (GeV) should have shown up, says Laura Baudis, the physicist who leads the XENON group at the University of Zurich, Switzerland. Because they didn't, those lower masses could be ruled out. Unfortunately, the results from a subsequent, 100-day run remain obscure: the researchers are still struggling to deal with unexpectedly high levels of background radiation caused by trace contaminants in the xenon 3 .  \n                Pure and simple \n              An experiment searching a similar mass range is the Cryogenic Dark Matter Search (CDMS) in the disused Soudan mine in northern Minnesota. As a detection medium, the CDMS team uses a collection of germanium and silicon crystals, which are among the only solid elements that can be made with high enough purity to be usable for detecting dark matter. When the detector is operating, these crystals \u2014 which are about 10 centimetres across \u2014 are cooled to a temperature of just 40 millikelvin, so any heat associated with a WIMP impact can be detected. Now running its second-generation experiment, called CDMSII, the collaboration generated some excitement early last year when it reported two detections that could be interpreted as dark-matter signals 4 . Despite the hubbub, the team is reserved. \"We don't claim this is significant; we see a lot of events at this low threshold and most of them are plausibly background,\" says Jeffrey Filippini, who works on the CDMS team from the California Institute of Technology. If those two events are discounted, the CDMS team gets much the same result as the XENON collaboration: null findings that effectively rule out low-mass WIMPs. Yet the XENON and CDMS results contradict those from other experiments, the operators of which claim to have detected the very low-mass WIMPs ruled out by the first two. Perhaps the most intriguing, and most controversial, of these experiments is the Dark Matter Large Sodium Iodide Bulk for Rare Processes (DAMA/LIBRA), which shares space with XENON at Gran Sasso. DAMA works on the principle that the Sun's orbit around the centre of the Galaxy carries the Solar System through the invisible cosmic background of dark matter at some 220 kilometres per second. So detectors on Earth should have dark matter flowing through them at that velocity, modulated by an annual variation of 30 kilometres per second as the planet orbits the Sun. The DAMA team, which looks for the scintillation of recoil events inside sodium iodide crystals, claims to have followed just such a periodic dark-matter signal for thirteen years 5 . However, the crystals cannot distinguish between WIMPs and background events from ordinary radiation in the detector's surroundings, so this result depends on the assumption that background events occur at a constant rate that does not vary with the season. If that result is valid, it flies in the face of the XENON and CDMS findings. \"If the main signal was as big as they claim, we and other teams would have seen it,\" says Leo Stodolsky of the Max Planck Institute for Physics in Munich, Germany, who works on a collaboration called the Cryogenic Rare Event Search with Superconducting Thermometers (CRESST), also at Gran Sasso. Voicing a scepticism shared by many non-DAMA physicists, Stodolsky says that any number of seasonal processes could release subatomic particles that would mimic DAMA's results for dark matter, including something as simple as snow melting and refreezing in the mountains above the lab. Further eroding DAMA's position is that no other dark-matter experiment is looking for a periodic signal, so its results cannot be directly replicated. Yet, despite the criticisms, the DAMA signal gets stronger each year. \"DAMA has been very courageous,\" says Juan Collar, a physicist at the University of Chicago in Illinois. \"They went out and made a claim\" when most other physicists were still inclined to dismiss their results as background noise. Collar leads an effort called Coherent Germanium Neutrino Technology (CoGeNT), whose detector sits near the CDMSII in the Soudan mine. CoGeNT uses germanium crystals tuned to detect incoming particles with much lower masses than those sought by its neighbour. It was originally intended to explore this range to rule out the existence of low-mass WIMPs, but its results only ended up making things murkier. Around the time that the CDMSII reported its 'nearly nothing' findings, CoGeNT released data from its first 56 days of operations 6 . The results showed hundreds of particle events that could be interpreted as dark matter with a mass between 7 and 11 GeV. These could also be the same particles that DAMA is detecting, but physicists have been quick to offer a more sober reading. \"For CoGeNT, the signal and the background could easily be mistaken for the same thing,\" says David Kaplan, a physicist at Johns Hopkins University in Baltimore, Maryland. The team has decided to wait a full year after its initial publication, to see if its findings show the same seasonal fluctuation as DAMA, before announcing any new results.  \n                Total annihilation \n              Meanwhile, a debate has broken out over another way to detect dark matter. One of the many oddities of dark-matter particles is that they can be their own antiparticles: put enough of them in one place, and they should start annihilating one another, producing \u03b3-rays in the process. In particular, the centre of the Milky Way should be producing excess \u03b3-radiation because dark matter is expected to concentrate there, says Dan Hooper, an astronomer at the Fermi National Accelerator Laboratory located near Batavia, Illinois. And Hooper claims to have found evidence for these \u03b3-ray excesses in data from NASA's Fermi Gamma-ray Space Telescope 7 . \"If you were to ask what kind of a signal you would want to see with dark matter in the galactic centre, this would be what you expect,\" says Neal Weiner, a theoretical physicist at New York University. The results are consistent with a dark-matter particle of 7.3\u20139.3 GeV, a range that fits well with the findings from both CoGeNT and DAMA. Other researchers remain sceptical. \"The galactic centre is so complicated that before you believe that you have dark-matter annihilation, you have to rule out all the options,\" says Doug Finkbeiner, an astronomer at the Harvard-Smithsonian Center for Astrophysics in Cambridge, Massachusetts. Finkbeiner points out that the signal could come from undetected pulsars \u2014 rapidly rotating neutron stars that produce copious amounts of high-energy radiation. Still, Hooper's results have given researchers food for thought. \"It's a case of too many coincidences,\" says Collar. When findings from three detectors all start to point towards dark-matter particles of similar mass, he says, \"you start to wonder if they're not coincidences any more\". Such thinking has led theorists such as Feng to take a fresh look at all the results to see whether they can come up with a coherent idea of what dark matter might be. If CoGeNT and DAMA are right, says Feng, then they are not detecting the expected dark-matter particle, the neutralino, as it should not be as light and interact as strongly as the results indicate. So perhaps dark matter is some very different particle \u2014 or perhaps the model of a single WIMP is not correct. \"If you look at the few per cent of the Universe that comprises us, it's quite complex,\" says Philip Schuster, a physicist at the Perimeter Institute for Theoretical Physics in Waterloo, Canada, referring to the known 'particle zoo' predicted by the standard model that includes such oddities as muons, neutrinos and quarks. \"It's a little insane to believe that the other 85% of the Universe would be so simple,\" he says. Along with his collaborators, Schuster is working to find evidence for a more complex theory of dark matter, called the 'dark sector'. This sector could include multiple types of dark matter and a number of dark forces, which, like ordinary matter, could combine to form dark atoms. It is being tested in an experiment called the A Prime Experiment (APEX), at the Thomas Jefferson National Accelerator Facility in Newport News, Virginia, which will accelerate a high-energy beam of electrons and search for relatively heavy force-carrying particles radiating from them. \"It might tell us that the Universe is a lot broader than we suspect,\" says Natalia Toro, a physicist who works at the Perimeter Institute and with Schuster on APEX. The good news is that both the XENON100 and CoGeNT collaborations are expected to release their first full year's worth of data this year. And larger, more sensitive detectors, such as the Large Underground Xenon (LUX) and Xenon neutrino Mass (XMASS) detectors, are scheduled to start operations not long afterwards. \"While we are in a 'he said, she said' situation now, it won't be like that indefinitely,\" says Weiner. \"We will have enough information to settle this in the next couple of years.\"\n Adam Mann is a freelance writer in Washington DC. \n                     Nature Physics \n                   \n                     DAMA \n                   \n                     CDMS \n                   \n                     XENON \n                   \n                     Gran Sasso National Lab \n                   \n                     Soudan Underground Laboratory \n                   \n                     Thomas Jefferson National Accelerator Facility \n                   \n                     ASA Fermi Gamma-Ray Telescope \n                   Reprints and Permissions"},
{"file_id": "473434a", "url": "https://www.nature.com/articles/473434a", "year": 2011, "authors": [], "parsed_as_year": "2006_or_before", "body": "Vaccination campaigns against measles have had dramatic results \u2014 but eradicating the disease is still a distant prospect. Great advances in the development and distribution of vaccines mean that some diseases can be eradicated. Measles is an important case study: efforts to stem the disease have been successful, but uneven political commitment, lack of funds and public fear threaten to undermine the progress. \n               Click here for larger image \n               \n               PAST: A KILLER CRUSHED  \n             In 1980, before vaccination was widespread, there were around 4 million cases of measles and an estimated 2.6 million deaths from the disease worldwide 1 . Childhood mortality targets set by the United Nations, along with accelerated control programmes, have cut the proportion of childhood deaths caused by measles from 7% in 1990 to 1% in 2008 2 .  \n               PRESENT: TROUBLE SPOTS  \n             \n               Click here for larger image \n               Ideally, 95% of children need to receive two doses of a measles-containing vaccine to interrupt disease transmission. By 2009, almost 60% of countries had achieved 90% coverage with at least one dose \u2014 but some are still far below this, and some are slipping backwards.   United States   Measles was officially eliminated in 2000, but cases imported from elsewhere threaten to reestablish the virus. More cases have been registered in 2011 than in any year since 1996, leading to fears of outbreaks among unvaccinated children. \n               Click here for larger image \n               India   India is struggling to reduce deaths from measles 3 , mainly because of a lack of money and political will to provide two doses of vaccine to all children. There are some indications that this is changing.   Europe   More than 30,000 measles cases were reported in 2010, five times more than the annual average for the past five years 4 . Many have been traced back to a major Bulgarian outbreak in 2009\u201310. Unfounded fears over the measles, mumps and rubella (MMR) vaccine have contributed to the resurgence. \n               Click here for larger image \n               Africa   Outbreaks have been seen in 28 countries in the past two years 5 , mainly because of a lack of funding and political commitment to follow-up vaccination campaigns, and problems with vaccine delivery. There has also been resistance among some religious groups in Zimbabwe, Botswana, Malawi and South Africa.  \n               FUTURE: FUNDING FEARS AND UNCERTAINTY  \n             \n               Click here for larger image \n               Because measles deaths have fallen, vaccination efforts now compete for funding with other diseases, so investment has dropped. Some countries are struggling to introduce the recommended second dose of measles-containing vaccine, let alone new vaccines \u2014 for invasive pneumococcal disease and rotavirus, for example \u2014 that could save many more lives.  Assuming no catch-up immunizations in troubled countries, public-health officials predict a worst-case scenario in which the death\u00a0toll could exceed 500,000 by 2013 1 . In a 'status-quo' scenario, modest increases in first-dose vaccine coverage are complemented by catch-up immunizations at about 2008 levels \u2014 but this still falls short of global eradication. \n                     Vaccines Special \n                   \n                     The Measles Initiative \n                   \n                     WHO Measles data and statistics \n                   Reprints and Permissions"},
{"file_id": "474020a", "url": "https://www.nature.com/articles/474020a", "year": 2011, "authors": [{"name": "M. Mitchell Waldrop"}], "parsed_as_year": "2006_or_before", "body": "Paul Davies likes to ask big questions. But how did the freethinking cosmologist suddenly find himself probing the physics of cancer? As best he can remember, says Paul Davies, the telephone call that changed his professional life came some time in November 2007, as he was sitting in the small suite of offices that comprise his Beyond Center at Arizona State University (ASU) in Tempe. Until then, the questions that animated Davies' research and 19 popular-science books had grown out of his training in physics and cosmology: how did the Universe come to exist? Why are the laws of physics suited for life? What is time? And how did life begin? But this particular call was nothing to do with that. The caller \u2014 Anna Barker, then the deputy director of the US National Cancer Institute (NCI) in Bethesda, Maryland \u2014 explained that she needed his help in the 'War on cancer'. Forty years into the government's multibillion-dollar fight, said Barker, cancer survival rates had barely budged. The hope now was that physicists could bring some radical new ideas to the table, and she wanted Davies to give a keynote address at an NCI workshop explaining how. Ummm, sure, said Davies, who until that minute had been only vaguely aware that the NCI existed. \"But I don't know anything about cancer.\" \"That's okay,\" Barker replied. \"We're after fresh insights.\" And with that, says Davies, he was hooked. \"If it had been just, 'Give us another beam', I wouldn't have been interested,\" he says, referring to X-rays, particle beams, magnetic resonance imaging and the many other tools that physicists had provided to medicine. But an opportunity to contribute entirely new concepts and ways of thinking \u2014 \"now that\", says Davies, \"was exciting\". That excitement explains how the 65-year-old Davies, at an age when most academics are planning their retirement, finds himself embarking on practically a new career. Barker's original workshop metamorphosed into a network of 12 Physical Sciences\u2013Oncology Centers, which launched in late 2009. Davies now finds himself the principal investigator of one such centre, and a major player in the physics-meets-cancer effort as a whole. Cancer gives Davies a new realm in which to exercise what many colleagues regard as his greatest talent: asking 'dumb' questions that provoke fresh ways of thinking about a problem. \"Paul is wrong sometimes. But he is not afraid to ask a very naive question that gets at the heart of the matter,\" says Robert Austin, a biophysicist who heads another of the 12 centres, at Princeton University in New Jersey. Davies' questions have addressed topics ranging from metastasis (when tumour cells come apart and migrate, is it because of some physical change in their stickiness?) to subatomic physics (is cancer influenced by quantum effects inside biomolecules?). \"I often joke that my main qualification for cancer research is that I am unencumbered by any prior knowledge of the subject,\" Davies says. True, his naivety sometimes makes biologists grit their teeth. (\"Aaargh! Physicists!\" wrote Paul 'PZ' Myers, a biologist at the University of Minnesota, Morris, in a blog response to Davies' proposal earlier this year that tumours are a reversion to primitive genetic mechanisms that pre-date the dawn of multicellular life.) \"But his critics don't appreciate the value of a disruptive agent,\" says biophysicist Stuart Lindsay, who works closely with Davies at the ASU physics\u2013cancer centre. \"It takes someone like Paul, constantly nagging, asking disruptive questions, to get people to take a fresh look at their assumptions.\" Davies says that he has been asking questions as long as he can remember. The suburbs of London were a dull place to grow up, he explains, thinking back to the post-Second-World-War austerity that prevailed in the years after he was born there in 1946. \"No toys. No money. We made our own entertainment \u2014 so we had to use our imagination a lot.\" Maybe that's why he became so fascinated with shooting stars and astronomy, he says. \"I liked the fact that, just by looking up, you could escape into this wonderland out there.\" And maybe that's why by the age of ten he had become enthralled with atoms, which seemed to embody a hidden order behind the surface complexity of the Universe. A few years later, he says, \"I remember being struck by the fact that the brain is made of atoms, and atoms follow the laws of physics \u2014 so how can we have free will?\" By age 16, says Davies, his course was set: he would become a theoretical physicist and spend his life trying to answer the \"deep questions\". He began to explore one such question \u2014 how does quantum theory operate when space and time are curved by gravity? \u2014 first as a PhD student at University College London in the late 1960s and later as a lecturer at Kings College London. \"It was a connection between the very small and the very large, between quantum mechanics and the whole Universe,\" Davies explains. He eventually summarized the field's accomplishments as co-author of a classic monograph,  Quantum Fields in Curved Space   (1982). But even then his interests were not easily confined \u2014 and he was beginning to develop a parallel outlet for them. In the early 1970s, the British magazine  Physics Bulletin   invited Davies to write a popular article dealing with a long-standing conundrum he had touched on in his PhD dissertation: why does time seem to flow only in one direction \u2014 towards the future \u2014 even though most physical laws make no distinction? After the article appeared, a publisher asked him for a book on the subject. And yet \"I barely scraped through English class!\" Davies says.  \n                Questions in writing \n              But Davies discovered that he enjoyed popular writing, and had a knack for it. Whereas early titles mirrored Davies' research in pure physics and cosmology, dealing with the physics of black holes, the unification of forces, and quantum theory, later ones have reflected his widening interests. In 1980, he joined the physics department at the University of Newcastle, UK, where half the department was focused on geophysics. So just by osmosis, he began to learn about the long history of life on our planet, and he found himself inexorably drawn to another big question: how is life even possible? How could such complexity arise in a lifeless Universe, purely by the action of natural law? Davies explored the complexity question in popular books such as  The Cosmic Blueprint   (1987), and then \u2014 after moving to the University of Adelaide, Australia, in 1990 \u2014 went on to tackle the philosophical implications of extraterrestrial life in books such as  Are We Alone?   (1995). At the same time, titles such as  God and the New Physics   (1983) and  The Mind of God   (1992) revealed his willingness to engage in the dialogue about religion and science \u2014 efforts that he assumes contributed to his winning the 1995 John Templeton Prize for Progress in Religion and, Davies admits, earned him plenty of criticism from anti-religious scientists. But Davies has never let critics stop him from asking provocative questions. In the 1990s, for example, he began to wonder whether Earth and Mars might share a biosphere. Might chunks of rock blasted loose from one planet by ancient asteroid impacts have carried viable microbes to the other when the rocks fell as meteorites? \"Most people dismissed the idea as total nonsense, in rather blunt terms,\" says Davies. By 2004, he was wondering whether life could have originated on our planet more than once, with each lineage based on utterly different biochemistries, perhaps even without DNA or RNA. Might some of those alien lineages still be alive today? That question has led a number of astrobiologists to search for alternative life forms in harsh environments such as Mono Lake in California. It is also how Davies came to act as an adviser and co-author on last year's highly controversial paper claiming to have found bacteria that break life's rules by using arsenic instead of phosphorus in their DNA (F. Wolfe-Simon  et al .  Science    doi:10.1126/science.1197258  ; 2010). In 2004, Davies was contacted by Michael Crow, president of ASU, who was looking to overhaul the university's department-based hierarchy in favour of a more interdisciplinary approach (see  Nature   446 , 968-970; 2007). Davies seemed to be one of those rare thinkers who could shake up the status-quo thinking in academia, says Crow. \"Individuals who are polymaths, able to think across different subjects, and who personify 'disruptive thinking'.\" He contacted Davies at Macquarie University in Sydney, where Davies had helped to found the Australian Centre for Astrobiology in 2001, and offered to set him up in a centre where he could freely pursue all his interests. It was an offer too good to refuse, says Davies. He moved to ASU to head the Beyond Center for Fundamental Concepts in Science in September 2006. Just over a year later, Anna Barker was on the other end of the phone.  \n                Making a list \n              Once Davies had signed up to talk at the physics-cancer workshop, he had to figure out what to say. He knew he couldn't tell experienced cancer researchers how to do their jobs. So instead, he did what he does best: \"I made a list of dumb questions.\" First was whether physics could contribute anything at all to cancer research. A little reading and talking to colleagues convinced him it could. \"To my astonishment, I learned that physical forces can affect gene expression,\" he says. Stretching, squashing \u2014 lots of things would do the trick. Davies also learned that biologists rarely think about the cell as a physical object. \"Look at something as straightforward as 'where does metastasis occur?'\" says Davies. Does a tumour tend to seed itself in a second organ simply because blood flows there from the primary site? And what makes tumour cells suddenly break apart and become mobile in the first place, despite all the biophysical forces that tend to stick them together? He added those to the list. Next, Davies was struck by the fact that biologists can now explore the cell in enormous detail \u2014 practically molecule by molecule. But that very power, he says, has often beguiled cancer researchers into focusing on individual genes and all the other pieces that go wrong, instead of how the pieces come together into a complex whole. \"It's like trying to run the economy of the United States by measuring every transaction in every commodity and every city,\" says Davies. Granted, the comparatively new discipline of systems biology has been trying to take a more global view. But even so, says Davies, few cancer biologists are familiar with nonlinear systems analysis, network theory or any of the other tools that have been developed by mathematicians and physicists over the past few decades to deal with complex systems. The questions, he says, were \"all very, very basic. My level of ignorance was embarrassing.\" But when the workshop convened in Arlington, Virginia, on 26 February 2008, Davies' talk was a hit. Barker remembers being delighted. \"He gave a fascinating perspective, at a level biologists often just haven't thought about.\" And the cancer researchers in the audience were very receptive, recalls Austin, who also presented at the workshop. Even though many biologists object to the involvement of physicists in their field, \"people in the oncology community have been very welcoming\", he says. \"They know they have a problem.\" In December 2008, the NCI outlined its plan to fund the 12 physics-oncology centres at roughly US$2 million apiece over five years, and invited applications to host them. Each one would look at cancer from one of four points of view: physics, evolution, biological-information processing or complex systems. Researchers at ASU's four-year-old Biodesign Institute spearheaded an application. William Grady, a gastroenterologist with a joint appointment at the University of Washington Medical Center and at the Fred Hutchinson Cancer Center in Seattle, agreed to serve as the senior scientific investigator on the project. But by the NCI's rules, the proposal needed a principal investigator in the physical sciences. Davies describes himself as a reluctant draftee to that role. \"I'm not a natural administrator,\" he says. But every time he protested \"I have no credibility\", his colleagues would point to his reception at the workshops and insist \"You've got lots.\" When the NCI announced its selection of the 12 centres in October 2009, ASU was among them. Since then, Davies has tailored his contribution to his strengths: he runs about three workshops a year for participants throughout the physics\u2013oncology community. The goal is to trigger new collaborations, new experiments, new thinking \u2014 and the topics have ranged \"all the way from downright crazy to productive\", says Lindsay. Quantum effects in cancer might fall in the former category; the latter might include the physics of chromatin, the mass of DNA and protein in the cell nucleus. \"He won't allow people to get into a shoot-out of your theory versus mine. Just lots of critical thinking: 'what does this mean?',\" says a frequent participant, oncologist Donald Coffey of the Johns Hopkins University in Baltimore, Maryland. \"These are dynamite meetings. I come out incredibly enlightened, with lots of things to think about. And it works because of his personality.\"  \n                Dynamite meetings \n              What remains to be seen, of course, is whether the multimillion-dollar physics\u2013cancer effort will pay off. The centres are only now starting to produce their first papers, Lindsay says, \"and there's nothing earth-shattering yet\". Several of the papers, including various studies on the mechanical properties of cancer cells and chromosomes along with a formal write-up from Davies on the idea that irritated Myers, appear in the February 2011 issue of the journal  Physical Biology . It's early days, agrees Austin \u2014 but not too early to fret. \"I worry that there aren't enough Paul Davies around to ask disruptive things,\" he says. \"I worry that we'll become conventional \u2014 another failed assault on cancer. I really hate the thought that in ten years we'll find we haven't accomplished anything.\" Davies isn't worried \u2014 and he is happy to keep asking those disruptive questions. \"My mother didn't understand science at all,\" he says, by way of an explanation. In fact, he was the first person in his family to go to university. \"But she was fond of saying that she hoped I could do two things with it.\" One was to build a robot to help with housework \u2014 something he'll have to leave to others. But the other was to find a cure for cancer. \"She would be quite thrilled that I'm finally doing something useful.\" M. Mitchell Waldrop is a features editor for Nature. \n                     Nature Physics special issue on physics and the cell \n                   \n                     Paul Davies' home page \n                   \n                     The Beyond Center \n                   \n                     The ASU Biodesign Institute \n                   \n                     The ASU Center for the Convergence of Physical Sciences and Cancer Biology \n                   \n                     The NCI Physical Sciences-Oncology programme \n                   Reprints and Permissions"},
{"file_id": "474024a", "url": "https://www.nature.com/articles/474024a", "year": 2011, "authors": [{"name": "Zeeya Merali"}], "parsed_as_year": "2006_or_before", "body": "Physicists have always thought quantum computing is hard because quantum states are incredibly fragile. But could noise and messiness actually help things along? In 2008, quantum physicist Andrew White found himself building a \"ridiculous machine\" in his lab at the University of Queensland in Brisbane, Australia. White had spent years working on quantum computation, attempting to exploit subatomic physics to create a device with the potential to outperform its best macroscopic counterparts. And he had learned that it was a tough job: the required quantum systems are fragile, and demand immaculate laboratory conditions to survive long enough to be of any use. Now White was setting out to test an unorthodox quantum algorithm that seemed to turn that lesson on its head. In this scheme, messiness and disorder would be virtues, not vices \u2014 and perturbations in the quantum system would drive computation, not disrupt it. \"I honestly thought, there's no way this will work,\" says White. But when he turned his ridiculous machine on, it ran 1 . White's experiment is just one of several in recent years that have suggested a fresh approach to quantum computers. The conventional view is that such devices should get their computational power from quantum entanglement \u2014 a phenomenon through which particles can share information even when they are separated by arbitrarily large distances. But the latest experiments suggest that entanglement might not be needed after all. Algorithms could instead tap into a quantum resource called discord, which would be far cheaper and easier to maintain in the lab. More experiments will be required to convince the many sceptics that the approach will work. But if it pans out, the era of widespread quantum computation could arrive sooner than anyone expected.  \n                Unprecedented speed \n              The idea of quantum computing dates from the 1980s, when Nobel-prizewinning physicist Richard Feynman realized that a machine using quantum rules could whizz through calculations that would take a standard computer billions of years. Classical computers have to encode their data in an either/or fashion: each bit of information takes a value of 0 or 1, and nothing else. But the quantum world is the realm of both/and. Particles can exist in 'superpositions' \u2014 occupying many locations at the same time, say, or simultaneously spinning clockwise and anticlockwise. \n               Click here for larger image \n               So, Feynman argued, computing in that realm could use quantum bits of information \u2014 qubits \u2014 that exist as superpositions of 0 and 1 simultaneously. A string of 10 such qubits could represent all 1,024 10-bit numbers simultaneously. And if all the qubits shared information through entanglement, they could race through myriad calculations in parallel \u2014 calculations that their classical counterparts would have to plod through sequentially (see 'Quantum computing'). The notion that quantum computing can be done only through entanglement was cemented in 1994, when Peter Shor, a mathematician at the Massachusetts Institute of Technology in Cambridge, devised an entanglement-based algorithm 2  that could factorize large numbers at lightning speed \u2014 potentially requiring only seconds to break the encryption currently used to send secure online communications, instead of the years required by ordinary computers. In 1996, Lov Grover at Bell Labs in Murray Hill, New Jersey, proposed an entanglement-based algorithm 3  that could search rapidly through an unsorted database; a classical algorithm, by contrast, would have to laboriously search the items one by one. But entanglement has been the bane of many a quantum experimenter's life, because the slightest interaction of the entangled particles with the outside world \u2014 even with a stray low-energy photon emitted by the warm walls of the laboratory \u2014 can destroy it. Experiments with entanglement demand ultra-low temperatures and careful handling. \"Entanglement is hard to prepare, hard to maintain and hard to manipulate,\" says Xiaosong Ma, a physicist at the Institute for Quantum Optics and Quantum Information in Vienna. \"It has been thoroughly investigated for years, with people expending much time and effort, but achieving little efficiency.\" The current entanglement record-holder intertwines just 14 qubits (ref.  4 ), yet a large-scale quantum computer would need several thousand. Any scheme that bypasses entanglement would be warmly welcomed, says Ma. Clues that entanglement isn't essential after all began to trickle in about a decade ago, with the first examples of rudimentary quantum computation. In 2001, for instance, physicists at IBM's Almaden Research Center in San Jose and Stanford University, both in California, used a 7-qubit system to implement Shor's algorithm 5 , factorizing the number 15 into 5 and 3. But controversy erupted over whether the experiments deserved to be called quantum computing, says Carlton Caves, a quantum physicist at the University of New Mexico (UNM) in Albuquerque. The trouble was that the computations were done at room temperature, using liquid-based nuclear magnetic resonance (NMR) systems, in which information is encoded in atomic nuclei using an internal quantum property known as spin. Caves and his colleagues had already shown 6  that entanglement could not be sustained in these conditions. \"The nuclear spins would just be jostled about too much for them to stay lined up neatly,\" says Caves. According to the orthodoxy, no entanglement meant no quantum computation. The NMR community gradually accepted that they had no entanglement, says Jiangfeng Du, an NMR-computing specialist at the University of Science and Technology of China, in Hefei. Yet the computations were producing real results. In 2001, Du and his colleagues published the first experiment to explicitly perform a quantum search without exploiting entanglement 7 . \"These experiments really called into question what gives quantum computing its power,\" says Animesh Datta, a physicist at the University of Oxford, UK. If researchers hope to build a large-scale quantum computer, they need to understand how the computation works.  \n                Order out of disorder \n              Datta, at the time a graduate student supervised by Caves at UNM, began to search for an alternative explanation. He came across discord, an obscure measure of quantum correlations first proposed 8  in 2000 by Wojciech Zurek, a quantum physicist at the Los Alamos National Laboratory in New Mexico. Discord quantifies how much a system can be disrupted when people observe it to gather information. Macroscopic systems are not affected by observation, and so have zero discord. But quantum systems are unavoidably affected because measurement forces them to settle on one of their many superposition values, so any possible quantum correlations, including entanglement, give a positive value for discord. The concept was largely ignored for years because it seemed so abstract, says Vlatko Vedral, a quantum physicist at the University of Oxford, who in 2002 independently derived a mathematical expression for discord 9  in collaboration with Leah Henderson at the University of Bristol, UK. \"But that changed when Datta connected discord to quantum computing.\" Datta had seized on an algorithm 10  proposed a few years earlier by NMR researchers Emanuel Knill, now at the US National Institute of Standards and Technology in Boulder, Colorado, and Raymond Laflamme, now at the University of Waterloo in Canada. Knill and Laflamme challenged the idea that quantum computing requires physicists to painstakingly prepare a set of pristine qubits in the lab. In a typical optical experiment, the pure qubits might consist of horizontally polarized photons representing 1 and vertically polarized photons representing 0. Physicists can entangle a stream of such pure qubits by passing them through a processing gate such as a crystal that alters the polarization of the light, then read off the state of the qubits as they exit. In the real world, unfortunately, qubits rarely stay pure. They are far more likely to become messy, or 'mixed' \u2014 the equivalent of unpolarized photons. The conventional wisdom is that mixed qubits are useless for computation because they cannot be entangled, and any measurement of a mixed qubit will yield a random result, providing little or no useful information. But Knill and Laflamme pondered what would happen if a mixed qubit was sent through an entangling gate with a pure qubit. The two could not become entangled but, the physicists argued, their interaction might be enough to carry out a quantum computation, with the result read from the pure qubit. If it worked, experimenters could get away with using just one tightly controlled qubit, and letting the others be battered by environmental noise and disorder. \"It was not at all clear why that should work,\" says White. \"It sounded as strange as saying they wanted to measure someone's speed by measuring the distance run with a perfectly metered ruler and measuring the time with a stopwatch that spits out a random answer.\" Datta supplied an explanation 11 . With Caves and Anil Shaji, a physicist then at UNM, he calculated that the computation could be driven by the quantum correlation between the pure and mixed qubits \u2014 a correlation given mathematical expression by the discord. It was a bold claim, says Kavan Modi, an expert on discord at the Centre for Quantum Technologies at the National University of Singapore. \"Before that, if you announced that discord was as important for computation as entanglement \u2014 if not more so \u2014 at a conference, people would laugh out loud at you.\" But it seemed shocking only because, at the time, physicists had never really analysed computation in real-world scenarios that included mixed states. \"It's true that you must have entanglement to compute with idealized pure qubits,\" says Modi. \"But when you include mixed states, the calculations look very different.\" Datta and his colleagues presented experimenters with a testable discord-based scheme. White doubted it would work, but jumped at the prospect of trying it out. \"I'm a lazy experimenter, so I loved the thought of quantum computation without the hassle of entanglement,\" he laughs. White was already practised at using polarized photons. He ran the computation as prescribed by Datta and, by averaging the values of the pure qubit over 2,000 runs, successfully summed the diagonal elements of a 2 \u00d7 2 matrix of numbers 1 . \"It's a small matrix, but this was a proof-of-principle to show that you get the right answer in a reasonable number of runs, as predicted,\" says White. The team confirmed that the qubits were not entangled at any point. Intriguingly, when the researchers tuned down the polarization quality of the one pure qubit, making it almost mixed, the computation still worked. \"Even when you have a system with just a tiny fraction of purity, that is vanishingly close to classical, it still has power,\" says White. \"That just blew our minds.\" The computational power only disappeared when the amount of discord in the system reached zero. \"It's counter-intuitive, but it seems that putting noise and disorder in your system gives you power,\" says White. \"Plus, it's easier to achieve.\" For Ma, White's results provided the \"wow! moment\" that made him take discord seriously. He was keen to test discord-based algorithms that used more than the two qubits used by White, and that could perform more glamorous tasks, but he had none to test. \"Before I can carry out any experiments, I need the recipe of what to prepare from theoreticians,\" he explains, and those instructions were not forthcoming. Although it is easier for experimenters to handle noisy real-world systems than pristine ones, it is a lot harder for theoretical physicists to analyse them mathematically. \"We're talking about messy physical systems, and the equations are even messier,\" says Modi. For the past few years, theoretical physicists interested in discord have been trying to formulate prescriptions for new tests. Such experiments are essential if advocates of discord are to win over the wider physics community, says Antonio Ac\u00edn, a quantum physicist at the Institute of Photonic Sciences in Barcelona, Spain. He notes that no one has yet proved that discord is essential to computation \u2014 just that it is there. Rather than being the engine behind computational power, it could just be along for the ride, he argues. Last year, Ac\u00edn and his colleagues calculated that almost every quantum system contains discord 12 . \"It's basically everywhere,\" he says. \"That makes it difficult to explain why it causes power in specific situations and not others.\" Modi shares the concern. \"Discord could be like sunlight, which is plentiful but has to be harnessed in a certain way to be useful. We need to identify what that way is,\" he says. Du and Ma are independently conducting experiments to address these points. Both are attempting to measure the amount of discord at each stage of a computation \u2014 Du using liquid NMR and electron-spin resonance systems, and Ma using photons. They hope to have results by the end of the year. A finding that quantifies how and where discord acts would strengthen the case for its importance, says Ac\u00edn. But if these tests find discord wanting, the mystery of how entanglement-free computation works will be reopened. \"The search would have to begin for yet another quantum property,\" he adds. Vedral notes that even if Du and Ma's latest experiments are a success, the real game-changer will be discord-based algorithms for factorization and search tasks, similar to the functions devised by Shor and Grover that originally ignited the field of quantum computing. \"My gut feeling is that tasks such as these will ultimately need entanglement,\" says Vedral. \"Though as yet there is no proof that they can't be done with discord alone.\" Zurek says that discord can be thought of as a complement to entanglement, rather than as a usurper. \"There is no longer a question that discord works,\" he declares. \"The important thing now is to find out when discord without entanglement can be exploited most usefully, and when entanglement is essential.\" \n                 See News  \n                 page 18 \n               Zeeya Merali is a freelance writer based in London. \n                     Nature News and Views Q&A on Quantum Computing \n                   \n                     Nature Futures: Quantum erat demonstrandum \n                   \n                     Nature Futures: Entanglement \n                   \n                     Animesh Datta \n                   \n                     Andrew White \n                   \n                     Xiaosong Ma \n                   Reprints and Permissions"},
{"file_id": "478305a", "url": "https://www.nature.com/articles/478305a", "year": 2011, "authors": [{"name": "Gayathri Vaidyanathan"}], "parsed_as_year": "2006_or_before", "body": "Scientists are hoping to stall plans to erect a string of dams along the Mekong River. This summer, a crew of strangers arrived in the tiny village of Pak Lan along the Mekong River in northern Laos. They sat around in shorts, examining technical drawings, and then surveyed the area, measuring the height of the riverbank, the size of the rice paddies and even the number of pigs. The tally is necessary because Pak Lan may soon disappear. The government will need to move it and 18 nearby villages, because they will be partially or fully submerged if a highly controversial dam, called the Xayaburi, is built. The US$3.5-billion project will create a 60-kilometre-long reservoir and generate 1260 megawatts of power, which will earn between $3 billion and $4 billion a year for the developer, CH Karnchang Public Company of Thailand. Somchit Tivalak, village chief and representative of the ruling communist Lao People's Representative Party, is not quite sure what a hydroelectric dam is or how it will work, but he is convinced that good things are on the horizon. He says that his village will move to a place where it will have roads and electricity, as well as a reservoir teeming with fish. Many others, however, are deeply worried. The lower Mekong, which winds through Laos, Thailand, Cambodia and Vietnam, is one of the last big untamed rivers in the world. Nearly 60 million people depend on its rich fisheries for their survival. If the Xayaburi dam is built, it will set a precedent for 10 other hydropower dams proposed for the main stem of the river. If all those proceed, nearly 55% of the river will be converted to slow-flowing reservoirs. Predicting the effects of such massive changes is impossible because the Mekong is one of the most poorly studied major rivers in the world. Taxonomists know so little about the fish there that they are discovering new species at an unparalleled pace. And governments do not consistently monitor water and sediment flows along the river. In the case of the proposed Xayaburi dam, some scientists say the environmental impact assessment (EIA) conducted for the builder is seriously flawed because it does not consider the wider effects of the dam. \"The EIA of the Xayaburi dam is the worst EIA that I've ever seen,\" says Ian Baird, a professor of geography at University of Wisconsin\u2013Madison who has studied the region for decades. Cambodia and Vietnam, which researchers say will receive a disproportionate share of the harm from the dam, have both objected to it. And a scientific panel hired by the Mekong River Commission \u2014 a regulatory body made up of government representatives from Thailand, Laos, Cambodia and Vietnam \u2014 last year recommended a 10-year delay on damming the river so that researchers could gather the needed data. But the Laotian government, which will receive up to 30% of the revenue, says that it will push ahead. So scientists are rushing to assess how the dams will affect the Mekong's fisheries and the flow of sediment that helps to sustain its vast delta. \"The problem is, dams are coming very fast and are going to deeply modify the environment in a very short time frame,\" says Eric Baran, fisheries researcher at the World Fish Center in Phnom Penh. \"And the countries are not equipped to deal with that yet.\"  \n                Calming the waters \n              From its origins in the Tibetan plateau, the Mekong winds 4,800 kilometres down to the South China Sea, making it the longest river in Southeast Asia. At least 781 species of freshwater fish ply its waters, including four of the largest freshwater fish species in the world. The biggest of them, the endangered Mekong giant catfish ( Pangasianodon gigas ), can grow to be as long as a car. \n               Click here for larger image \n               Years of war, lack of investment, and drastic variations in flow between the wet and dry seasons have held back hydropower development and helped to keep the lower reaches of the river wild. But in the 1990s, Chinese engineers began a project to build eight dams and reservoirs on the Upper Mekong, which have evened out the flow (see  'Taming a river' ). With the Mekong suitably subdued and with the demand for electricity rising in the region, the Laotian government and private developers are now racing to put up dams, and Xayaburi is the first one to near construction. If it is completed, eight more are likely to spring up in Laos and along its border with Thailand, according to International Rivers, an environmental non-governmental organization based in Berkeley, California. The EIA found that Xayaburi's effect on fish, water flow and erosion would be minimal. Instead of creating a large standing reservoir behind a massive concrete wall, Xayaburi will have a smaller wall that will allow water to pass beneath it in what is called a run-of-the-river design. According to the EIA, the dam will \"improve the overall natural fish production capacity on the Mekong River in the project area, especially in the dry season\". But researchers have challenged that conclusion, noting that Xayaburi and most of the dams proposed for the river's main stem will have concrete walls tall enough to raise upstream water levels by between 30 and 65 metres. Although smaller than conventional reservoir dams, the walls would still block sediment and migrating fish, says Tarek Ketelsen, a hydrologist at the International Centre for Environmental Management in Hanoi, Vietnam, which evaluated the Xayaburi EIA. Critics also object to the fact that the EIA considers the potential effects only for a \"downstream area about 10 kilometres from the barrage site\", according to the document. That is a remarkably small stretch of the river, say researchers. The EIA was conducted for Karnchang by TEAM Group of Companies, a conglomerate of consulting firms based in Bangkok. When contacted by  Nature , TEAM said it could not discuss the EIA because of the terms of its contract with Karnchang. Karnchang did not respond to calls or e-mails requesting comment.  \n                Fishing for trouble \n              Toun Neang, 52, gets up at 4 a.m. every day to go fishing on the Tonl\u00e9 Sap Lake, which connects to the Mekong River in Cambodia. When he arrives, he offers incense, rice and beer to the spirit in the river. \"If we forgot to ask permission or make an offer, that day we will not be able to catch even a single fish,\" he says. A fisherman since childhood, Neang has a keen eye for the migration cycles that bring fish into and out of the lake from the Mekong. Adult fish lay eggs far upstream, and then flooding during the rainy season brings those eggs and juveniles to the Tonl\u00e9 Sap, he says. He worries about dams. \"If the water is blocked, how can fish migrate downstream? And how can fishermen like us live if there are no more fish?\" The future of the fishery matters because the Tonl\u00e9 Sap \u2014 one of the world's most productive inland fisheries for its size \u2014 provides half of the protein consumed in Cambodia. \"It is hard for people in Europe or North America to imagine the role that freshwater capture plays in terms of food security, economically and even culturally,\" says Kirk Winemiller, a fisheries researcher at Texas A&M University in College Station. Modelling the effect of Xayaburi and other dams on this fishery is difficult because researchers lack baseline data about most fish in the Mekong. Around 229 species live upstream of the proposed Xayaburi site, and 70 of them are migratory. In terms of biomass, about 60% of the total catch in the Tonl\u00e9 Sap is made up of species that migrate long distances, some from as far up as the Xayaburi area, more than 1,500 kilometres upstream. Many dams have built-in fish ladders that allow some migrating fish to pass. But researchers say the two ladders in Xayaburi's design are not enough for the number of fish and the diversity of migratory species there. Among them is the Mekong giant catfish, the river's best-studied species and longest-distance swimmer. Zeb Hogan, fisheries researcher at the University of Nevada, Reno, spent years collecting fish and extracting calcified ear bones, called otoliths, from their heads. The otoliths grow a new layer each day, incorporating elements from the water, which creates a chemical record of a fish's travels. Otolith studies have shown, for example, that the tropical Asian catfish  Pangasius krempfi   makes an epic migration ( Z. Hogan  et al. J. Fish Biol.    71,   818\u2013832; 2007 ). It starts life in the higher reaches of the Mekong, then drifts down to the coastal flood plains during the monsoon season. Adult fish live in the brackish waters of the delta and the South China Sea, but they fight their way back upstream to spawn at the beginning of the rainy season every year. Michio Fukushima, a fisheries scientist at the National Institute for Environmental Studies in Tsukuba, Japan, and his colleagues at Ubon Ratchathani University in Thailand are trying to adapt otolith analysis to other species that migrate within the Mekong. Many of these species are commercially important, particularly the Siamese mud carp (genus  Henicorhynchus ), known as  trey riel   in Cambodia. This 15-centimetre-long fish is a major food source for larger carnivores. It is an important ingredient in fish paste and in feed used in aquaculture, and it is the most-harvested species in the Mekong. Fukushima's work has so far traced some of the  riel's   migration routes. The fish he captured from the Songkhram, a Mekong tributary in Thailand, seem to mature in the main stem of the Mekong before returning to the tributary. Baird says that the  riel   may become threatened in the Mekong as dams are built. \"You start putting dams along the river there, it will stop migration for the fish,\" he says. \"It is hard to say exactly \u2014 will it wipe it out all together or reduce it in number? We haven't faced this situation with such a highly abundant species.\" Even less is known about other fish in the Mekong. Fukushima and Baran are now creating an atlas of fish distribution, and Baran and others are modelling the effects of dams on fisheries. Preliminary runs suggest that if all the proposed main-stem dams are built, the region's annual catch of 2.1 million tonnes will drop by somewhere between 600,000 and 1.4 million tonnes. \"Six hundred thousand tonnes represents the whole annual freshwater fish production in West Africa\", says Baran. \"That's huge.\"  \n                Sedimental journey \n              The proposed dams will also exacerbate the Mekong Delta's ongoing battles with the sea. The delta, home to 17 million people in Vietnam and 2.4 million in Cambodia, seems to be losing coastal land, says James Syvitski, a geologist at the University of Colorado at Boulder. Sea levels there are rising by 6 millimetres a year because of a combination of global ocean swelling and local changes. And the destruction of mangrove forests has left the delta prone to devastating floods and typhoons. In a study of deltas around the world, Syvitski and his colleagues declared the Mekong Delta \"in peril\", noting that an area of nearly 21,000 square kilometres is already less than two metres above sea level ( J. P. M. Syvitski  et al. Nature Geosci.    2,   681\u2013686; 2009 ). The proposed dams are projected to accelerate the sinking by blocking the flow of sediment that would otherwise nourish the flood plains and build up the delta. Mathias Kondolf, a fluvial geomorphologist at the University of California, Berkeley, estimates that the dams in China and on the lower Mekong will block about half of the river's sediment, which could be disastrous for the delta. Some dam designs reduce the problem by incorporating wide, low-lying outlets that allow sediment to pass through. But these can compromise power generation, and might not let through the heavy sediment loads that would accumulate far upstream near the start of a 60-kilometre-long reservoir, says Ketelsen. All these unknowns explain why the team of consultants assembled by the Mekong River Commission (MRC) last year called for a 10-year delay in building the Xayaburi dam, recommending that Laos start with smaller dams on tributaries. The MRC did not take a stand on the proposed moratorium and would not have the power to enforce it. But scientists say that the MRC does have the clout to influence the design of dams. \"Hydropower is important for the development of a country like Laos, and it does have a right to develop,\" says Ketelsen. \"However, when it comes to a river like this, which has a global significance in terms of biodiversity, you don't have to start with the most high-impact projects\". The idea of waiting has gained some international support. The Asian Development Bank in Manila, for example, says that building dams on the main stem of the Mekong is premature because too little is known about the environmental and social costs. Last April, Laotian authorities agreed to delay construction of Xayaburi until after conducting another project review, the results of which are due to be submitted to the four nations of the MRC in a final meeting in the next few weeks. But officials have said recently in media reports that they have completed the review and plan to go ahead with construction. The MRC is keeping silent, waiting for the Mekong nations to meet. Near the Xayaburi site in northern Laos, it does not look as though construction crews are waiting for the final meeting. Trucks are paving mud roads with asphalt \u2014 a necessary first step towards dam construction. Downriver in Ubon Ratchathani, Thailand, Fukushima gets on a speedboat to collect fish, water and sediment samples from a dam on a Mekong tributary. For the past two years, he has travelled through Cambodia, Laos and Thailand by boat and by motorcycle twice a year to collect data. When he captures a fish, he performs a rough surgery, slicing open its head to extract otoliths for later analysis in his lab. Fukushima says that before the dams become a reality, he wants to establish a baseline of environmental and ecological conditions and to try to work with developers so that future dams will cause the least amount of harm. He remains cautiously hopeful that science can make a difference. Looking out over the water, he says, \"there must be some way we can move towards a better future\". Gayathri Vaidyanathan is a reporter with Greenwire in Washington DC. She was previously an International Development Research Center fellow at Nature. \n                     Nature Geoscience \n                   \n                     Mekong River Commission (MRC) \n                   \n                     MRC Environmental Impact Assessment (50 MB) \n                   \n                     MRC Strategic Environmental Assessment \n                   \n                     MRC Social Impact Assessment (50 MB) \n                   \n                     MRC Feasibility Study, Xayaburi Hydroelectric Plant \n                   Reprints and Permissions"},
{"file_id": "470156a", "url": "https://www.nature.com/articles/470156a", "year": 2011, "authors": [{"name": "Meredith Wadman"}], "parsed_as_year": "2006_or_before", "body": "Theresa Deisher once shunned religion for science. Now, with renewed faith, she is fighting human-embryonic-stem-cell research in court. Theresa Deisher was 17 years old the first time she saw a human fetus. Having graduated from the Holy Names Academy in Seattle, Washington, in 1980, she had taken a summer job in the pathology lab at the city's Swedish Hospital when a friend and co-worker miscarried in her fifth month of pregnancy. The fetus arrived fixed in formalin, and Deisher helped to section it to determine the cause of the miscarriage. The body hardly seemed to be the remains of a sentient, soul-bearing human, as the faith of her upbringing had taught, recalls Deisher. Instead, \"It looked like a space alien,\" she says. \"I called it 'the thing' for so many years.\" Thirty years later, Deisher sees the unborn in a different light. She has reversed her views on embryos and become one of two plaintiffs in a lawsuit filed in 2009, seeking to stop the US government from funding human-embryonic-stem-cell research. The courts hearing the case could issue a decision at any time; many, including Deisher, expect that the matter will end up before the US Supreme Court. Deisher's co-plaintiff, James Sherley, an adult-stem-cell scientist at the Boston Biomedical Research Institute in Watertown, Massachusetts, is well known as a provocateur. In 2007, he went on a hunger strike to protest against a decision by the Massachusetts Institute of Technology (MIT) in Cambridge to deny him tenure, which he attributed to racism. Deisher is less well known. A cellular physiologist educated at Stanford University in Palo Alto, California, she spent 17 years in the biotech industry at companies including Genentech, Immunex and Amgen. Three years ago, she founded a tiny, privately held Seattle firm called AVM Biotechnology \u2014 the name is a loose abbreviation for 'Ave Maria' \u2014 which is dedicated to hastening adult-stem-cell therapies to the market, and to developing alternatives to vaccines and therapeutics made using cell lines from aborted fetuses. She has also launched a non-profit group, the Sound Choice Pharmaceutical Institute, which among other things is investigating, as she puts it, \"the potential link between human DNA in childhood vaccines and autism\". Deisher, who is 48 and goes by the name Tracy, is smart, driven and committed. A devout Catholic and a divorced mother of two boys aged 9 and 12, she rises as early as 3:45 a.m. to ride an exercise bike while praying the rosary. She is casual and unpretentious, with a dry humour and a can-do attitude: she spent New Year's Eve laying carpet in the 180-square-metre office space that her company recently moved into. She is also a bundle of contradictions: an adamant right-to-lifer, whose closest, long-standing friends are pro-choice liberals. She made a healthy six-figure salary at the cream of US biotech companies, but thought nothing of mortgaging it all to launch a no-name firm as the economy slid into a recession. She is a no-frills dresser who has worn a simple gold cross virtually every day for the past 18 years. But she flaunts her intellect. In the past, she alienated friends with a formidable vocabulary fed by a dictionary-reading habit. And she says that those at her church who disagree with her stem-cell views \"oftentimes need some education\". Above all, Deisher is supremely confident in her positions, including her attempt to prevent hundreds of millions of dollars from going to human-embryonic-stem-cell research. \"It's very difficult to get passionately, morally protective of what physically truly is a clump of cells,\" she says. \"But that is a human being. Scientifically, you can't debate that.\" Her arguments, now part of a national discussion, can be hyperbolic. And she does not shy away from assigning motivations to her ideological foes. She says, for example, that embryonic-stem-cell scientists are mostly attracted to the cells' convenience \u2014 their rapid growth and what she calls the ease of working with them in the lab. Their science, she says, \"is not about helping patients and it's not about advancing the common good\". \"I wish that Tracy weren't so polarizing,\" says Chuck Murry, co-director of the Institute for Stem Cell and Regenerative Medicine at the University of Washington in Seattle, who has known Deisher since they were postdocs together at the university in the early 1990s. \"She's kind of the Sarah Palin of stem cells. It would be so much easier to have more rational discourse rather than somebody who heats up the vitriol like this.\" Deisher counters that she sticks to scientific arguments: \"My approach to the stem-cell issue is to remove the polarizing moral debates and speak and educate only about the science.\"  \n                Regaining the faith \n              Deisher showed a bent for science early, teaching herself calculus to win a state competition in which high-school students had to plot the orbit of Mars and design a spaceship and flight path to get there. \"Tracy was always very much a leader, an independent thinker,\" says Liz Swift, who taught Deisher physics at Holy Names and is now the school's principal. In those days, a fun Friday night for Deisher meant several hours at the University of Washington's astrophysics laboratory, followed at 10 p.m. by an outing with girlfriends \u2014 only after her mother had checked her for make-up and low necklines. As a girl, Deisher was torn between her mother's conviction that life began at conception and the views of her two outspoken aunts, both staunch supporters of Planned Parenthood, who reminded her regularly: \"It's not a baby. It's a clump of cells.\" Deisher's experience as a teenager in the Swedish Hospital pathology lab left her without any doubts as to who was right. \"I walked out of that lab that weekend and I threw my faith in the garbage can,\" she recalls. Weeks after her experience with the fetus, Deisher began undergraduate studies at Stanford, where she went on to earn her PhD in molecular and cellular physiology. On the side, she worked at Genentech in South San Francisco, California, developing assays to support the company's anti-platelet agents. \"I was very left-wing,\" she says. \"I was in science, and science was much more interesting than religion. I encouraged a couple of friends to have abortions,\" urging them to trust her first-hand experience with a fetus in formalin. Several years later, during an anatomy lab, she encountered the cadaver of a woman also embedded in formalin \u2014 looking, she says, not so very different from \"the thing\". It suddenly struck her that the fetus's 'alien' looks may have simply been attributable to the preservation process. That opened up what she calls \"a long, slow process\" of coming back to the faith of her childhood. It was one of three pivotal experiences that she talks about as having influenced her decision to actively fight against embryonic-stem-cell research. After completing a postdoctoral fellowship at the University of Washington in 1993, Deisher went to work for the biotech company Repligen in Waltham, Massachusetts, working on monoclonal-antibody therapeutics. After watching three rounds of lay-offs, Deisher decamped to a Seattle company called Zymogenetics, where she became involved in a cardiovascular-biology group. Soon after she arrived at the firm in 1995, Deisher isolated what seemed to be pluripotent stem cells from adult cardiac muscle. They differentiated, she says, into cell types including heart muscle, skeletal and smooth muscle, connective tissue, skin, bone and cartilage. \"People would come into the lab and they would practically start to drool,\" Deisher recalls. \"It was mind-boggling what these cells became.\" In March 1998 \u2014 8 months before the first report that human embryonic stem cells had been isolated \u2014 the company filed a patent application on the cells, with Deisher listed as first inventor. It was, and still is, a controversial claim. Kenneth Chien, an expert in studies of heart progenitor cells at the Department of Stem Cell and Regenerative Biology at Harvard University in Cambridge, Massachusetts, says that \"nobody has been able to identify a truly pluripotent stem cell from any adult mammalian heart\". Many of her colleagues at Zymogenetics reacted with \"ferocious hostility\", Deisher says. She recalls one scientist who cornered her, spittle flying from her mouth, shouting: \"Adult stem cells do not exist outside the haematopoietic system! Who the blank do you think you are, God?\" Deisher was ordered, she says, to stop working on the cells. The company abandoned the patent application in 2004, but Deisher remains unapologetic about her claims. The website for AVM proclaims: \"Dr. Deisher was the first person world-wide to identify and patent stem cells from the adult heart. Her discovery remains one of the most significant discoveries in the area of stem cell research.\" And the vehemence with which colleagues resisted \"made me open my eyes\", Deisher says, to the very real \u2014 and, she says, unscientific \u2014 passions that can infect defenders of scientific orthodoxy. Science, she reasoned, was not so objective after all. It was a second formative experience for her. Deisher had returned to religion, tentatively, in the early 1990s. Now, her disillusionment with colleagues at Zymogenetics \"led me back deeply and profoundly\", she says. She left the company for Immunex \u2014 which was acquired by Amgen in 2002. Human embryonic stem cells were back in the news, as president George W. Bush defined a policy that allowed federal funding for research on a score of existing cell lines. For Deisher, it was a score too many. \"I was extremely disappointed,\" she says. She felt the policy encouraged an unmerited hype around embryonic cells that deprived adult-stem-cell therapies of support. Through a friend of her parents, Deisher came into contact with Sharon Quick, a local doctor and conservative activist, who invited her in 2006 to speak on a televised panel about stem-cell research. Murry had also been invited to speak. He recalls Deisher reading prepared remarks about human-embryonic-stem-cell research. \"There was a lot of misinformation in there.\" Her talk, he says, \"didn't educate and focus. It obfuscated and frightened.\" In response to Murry's criticism, Deisher sent  Nature   a copy of the talk. It argues that human embryonic stem cells could provoke an immune response and form teratomas (tumours containing various types of cell); claims that safe, \"clinically proven\" alternatives exist; and categorically dismisses any potential promise embryonic cells may offer: \"There is no commercial, clinical or research utility in working with human embryonic stem cells.\" The event put Deisher on the map for anti-embryonic-stem-cell activists. It also led her to a third transformative moment in her advocacy. In early 2007, Deisher was invited to speak to a group of Republican state lawmakers in Olympia, Washington. One of the other speakers was a mother who had adopted a frozen embryo from a fertility clinic. The resulting child, a girl then four years old, stood beside her. Deisher was transfixed. It was, she says, \"the turning point to become less scientific about it, and actually feel emotion, and a stronger sense of commitment\". It was this commitment that led Deisher to found AVM in February 2008. The company's mission, in part, is to eliminate the need for embryonic-stem-cell therapies and enable adult-stem-cell companies to succeed by developing, for instance, drugs that promote stem-cell retention in target organs. It is also working on alternatives to vaccines currently produced using cell lines derived from fetuses that had been aborted decades ago. AVM has five members of staff, all of whom are unpaid, and occupies three rooms in a former nurses' dormitory. She financed AVM with her retirement savings, and with proceeds from the sale of her house. In 2009, an equity offering raised an additional $225,000 from 'angel' investors. Deisher's non-profit group, the Sound Choice Pharmaceutical Institute, is housed in the same premises and is staffed by four people. Last year, the institute won a $500,500 two-year grant from the MJ Murdock Charitable Trust, based in Vancouver, Washington, to study whether residual human DNA in the measles, mumps and rubella (MMR) vaccine might trigger autism. Stanley Plotkin, emeritus professor at the Wistar Institute in Philadelphia, Pennsylvania, and inventor of the rubella vaccine, calls the idea \"off the wall\". \"The whole idea, in my view, is just pernicious and just raises a spectre which has been redundantly disproven.\" John Van Zytveld, a senior fellow at the Murdock trust, who oversees its science grants, says that Deisher's proposal \"came back with a strong [peer] review and so we opted to support it\".  \n                A call to arms \n              In the spring of 2009, Deisher got a call from Sam Casey, a lawyer then based in Fairfax, Virginia, who was representing Do No Harm, a coalition opposed to human-embryonic-stem-cell research. The US National Institutes of Health (NIH) in Bethesda, Maryland, had just issued draft guidelines proposing to open up funding for the research, complying with an executive order from President Barack Obama. Casey enlisted Deisher to help write the group's response. He also told her that he was laying plans for a lawsuit if the final guidelines remained substantially unchanged from the draft. The suit would assert that the guidelines contravened an existing law, the Dickey\u2013Wicker amendment, which prohibits federal funding of research in which human embryos are destroyed. The NIH published its final guidelines on 6 July 2009, allowing financial support for work on human embryonic stem cells derived ethically from leftover embryos at fertility clinics, but not for work that went into their derivation. \"I was very disappointed,\" Deisher says. \"I had hoped and thought that they would listen.\" Soon afterwards, Casey, now with the Jubilee Campaign in Washington DC, called Deisher. He told her that the lawsuit was going ahead, and asked her to be one of the plaintiffs. She spent several weeks pondering her decision. \"There are huge ramifications to being involved in a lawsuit,\" she says. \"It is frightening to speak out. I don't care for the notoriety.\" Deisher was also keenly aware that James Sherley had signed on as a plaintiff. She had never met him, but she had followed his widely publicized tenure dispute with MIT. She worried about how a public association with him would affect her reputation. She made it clear to Casey that if he wanted her as a plaintiff, a high-profile, Sherleyesque approach was out of bounds. \"No theatrics, no histrionics, no hunger strikes,\" she says. It was agreed, and Deisher joined the suit. Her co-plaintiffs included 'embryos'; an embryo adoption agency called Nightlife Christian Adoptions; the Christian Medical and Dental Association based in Bristol, Tennessee; and individuals wishing to adopt embryos. The lawsuit, filed in August 2009, was barely noted by the press. And when, in October that year, District of Columbia District Court Judge Royce Lamberth ruled that none of the plaintiffs had standing to sue, Deisher received the news with a measure of relief. She could return to her preferred focus: her children and her work. But in June 2010, the US Court of Appeals for the District of Columbia Circuit ruled that Deisher and Sherley alone should be granted standing to sue, because, as adult-stem-cell researchers, they were in danger of 'imminent' injury. The court reasoned that by allowing federal funding of embryonic-stem-cell research the NIH increased competition for its limited funds, making it harder for adult-stem-cell researchers to win grants. The appeals court then sent the case back to Lamberth. Deisher was concerned. \"It's a little unnerving to know that you are the only two with standing.\" Unlike Sherley, Deisher has never applied for an NIH grant \u2014 as some opponents are quick to point out. She contends that she is still hurt by the guidelines, just as, by her reasoning, all adult-stem-cell researchers are hurt by the NIH's deliberate focus on embryonic stem cells. Moreover, she says, \"I would like to, I intend to and I plan to\" apply for NIH grants. It is hard to argue that adult-stem-cell researchers are at a disadvantage, however. Numbers provided by the NIH show that since 2002, when it first funded a human-embryonic-stem-cell grant, the agency has spent more than four times as much \u2014 $2.3 billion \u2014 on research with non-embryonic human stem cells. Nor has the money for non-embryonic work dwindled as embryonic funding has grown; in 2003, the NIH spent $191 million on adult-stem-cell research in humans; last year, it spent $388 million. Deisher responds that the United States lags in clinically testing new therapeutic uses for adult-stem-cells, instead focusing on well established indications such as leukaemia and lymphoma. Thirty-nine percent of adult-stem-cell trials for 'unconventional' indications registered with clinicaltrials.gov take place in the United States, compared with 71% of trials for 'conventional' uses. Defenders of the NIH say that lax regulatory and safety hurdles in some countries may explain the discrepancy. Sean Morrison, director of the Center for Stem Cell Biology at the University of Michigan in Ann Arbor, works on adult and embryonic stem cells, and says that \"the idea that the NIH is biased against adult-stem-cell research is ridiculous\".  \n                The hammer drops \n              On 23 August 2010, Lamberth issued a preliminary injunction siding with the plaintiffs. That immediately shut down federally funded human embryonic experiments, leaving the research community reeling and angry. Deisher's phone began ringing off the hook, with queries from reporters around the world. The next day, walking into her office in a building that shares space with other research groups, she was prepared for dirty looks. But \"if I got them, I didn't notice. The response was overwhelmingly positive.\" Deisher made a hastily arranged trip to Washington DC the next week. There, she met Sherley for the first time, during an hours-long strategy session at the offices of Gibson, Dunn and Crutcher, the DC law firm arguing the case. \"I asked lots of questions,\" Deisher says. (Sherley \"is a very nice man\", she adds. \"He's a good scientist.\") It would be 17 days from the preliminary injunction before a stay from the appeals court allowed embryonic-stem-cell research to resume. Since then, the lawsuit has been proceeding on two tracks. At the lower, district court, Judge Lamberth is considering both sides' requests for a speedy, 'summary' judgement on whether the NIH's guidelines are legal. The higher court, the Court of Appeals for the District of Columbia Circuit, which resides one level below the Supreme Court, is considering whether Lamberth met the legal standard for granting the preliminary injunction. Either court could rule at any time, and no matter what the decisions, appeals are expected (see  'Stem cells in court' ). The case has taken \"emotional energy\", Deisher says, but not a great deal of her time. She has not hung on every one of its twists and turns. In many ways, her life goes on unchanged. Old friends, for example, remain old friends. Two former high-school classmates who recently visited Deisher at her office both adamantly oppose her position on the research, but greet her with evident warmth. \"I can say wholeheartedly that I am envious of her passion,\" says one. But later, she e-mailed to ask that her name be withheld from this article. \"I cannot afford to have a search engine associate me with an individual whose actions are in such opposition to the beliefs of my personal and professional community,\" she wrote. The biggest lesson Deisher has learned from the lawsuit, she says, is \"how many scientists are against [human-embryonic-stem-cell research]. I did not know that. I did not expect the level of support and encouragement that I have received.\" The extent of that support may be tested if the Court of Appeals for the District of Columbia Circuit, when it rules on the issue, agrees with Deisher. If it does, it will shut down hundreds of human-embryonic-stem-cell experiments once more \u2014 possibly for good. Meredith Wadman is a reporter for Nature based in Washington DC. \n                     Stem Cell Injunction News Special \n                   \n                     AVM Biotechnology \n                   Reprints and Permissions"},
{"file_id": "470320a", "url": "https://www.nature.com/articles/470320a", "year": 2011, "authors": [{"name": "Brendan Borrell"}], "parsed_as_year": "2006_or_before", "body": "If a camera snaps everything you eat, you can't lie about it later. That's why scientists are building high-tech gadgets to measure the human 'exposome'. A decade ago, as part of a study on diet, psychologist Tom Baranowski was asked to recall everything he had eaten the previous day. A chicken dinner, he said confidently, remembering that he had prepared it for himself and his wife Janice. The thing was, he hadn't made chicken that night. It was only later that he realized he'd treated himself to a hamburger. If Baranowski, who studies children's diets at Baylor College of Medicine in Houston, Texas, was an unlikely candidate for making such a mistake, consider how abysmal the dietary memories of everyone else must be. By observing his study subjects one day and following up the next, Baranowski has found that children routinely forget about 15% of the foods they have eaten, and more than 30% of the foods they do recall turn out to be figments of their imagination. Adults show similar patterns. \"The errors of dietary assessment are overwhelming,\" says Baranowski. These mistakes are more than a reminder of the human memory's fallibility: they threaten to undermine the foundations of modern medical epidemiology. In this field, researchers make associations between past events and experiences, and later ones such as the emergence of cancer or other diseases. But if the initial records are inaccurate, these associations can be weak, misleading or plain wrong. Although the problem is most jarring in studies of diet, it also infects investigations of exercise, stress, pollution or smoking \u2014 basically, anything that relies on people reporting their own exposures through interviews or questionnaires. \"This is the weak part of epidemiology,\" says Paolo Vineis, an environmental epidemiologist at Imperial College London. Baranowski and Vineis are at the forefront of a movement among health researchers to develop measurements of environmental exposures that are more precise and objective than questionnaires. Some are working to develop personalized exposure profiles using blood-based tests. Others want their study subjects to trot around town with sensors dangling off their bodies capturing their movements, snapping photos of their lunch and taking samples of the air they breathe. \"We are getting to the point where you can conceive of doing a study with 500,000 people and giving them a cell-phone-sized device that they put in a charger every night,\" says David Balshaw, the exposure-biology programme manager at the National Institute for Environmental Health Sciences in Research Triangle Park, North Carolina. Some researchers foresee a day when they will keep track of the entire spectrum of environmental exposures for a single individual, dubbed the 'exposome' (see  'How to measure everything' ). That's a long way off. In the meantime,  Nature   takes a look at efforts to measure three key elements of the exposome: air pollutants, physical activity and diet. Each of these is bringing the exposome one step closer to reality \u2014 and the questionnaire, with all its flaws, a step closer to extinction.  \n                Breath by breath \n              The contraption fitted snugly inside a child's backpack. The tangle of green plastic tubes, filters, pumps, circuit boards and a hefty battery weighed about 3 kilograms and made a low hum when it was switched on and began sucking in air. Tiny filters were designed to collect continuous records of all the grit and grime a child in the Bronx would be exposed to during their pilgrimage from their apartment, through the New York City subway system to school and back again. For geochemist Steven Chillrud, whose team built the device in 2004, it represented the future of exposure biology. In the United States, environmental scientists have traditionally estimated human exposure to airborne pollutants by analysing data from building-mounted sensors. But the shortcomings of this approach became clear in a landmark study 1  published in 2005, in which researchers showed that levels of many hazardous compounds were higher inside homes than out. The findings made sense to Chillrud, who had already been thinking about the exposures of people living in New York City. \"People do not live on buildings,\" he says. To the New York City Police Department (NYPD), though, Chillrud's contraption was a potential terrorist threat. After four terrorists detonated bombs on London's public transport system on 7 July 2005, the NYPD had been conducting random searches on the subway system. When Chillrud stopped by the local police precinct to alert them to his planned study, officers were aghast, and even Chillrud admits his device looked intimidating. \"We put a lot of effort into it,\" Chillrud says now, as he hoists it onto his desk at Columbia University's Lamont\u2013Doherty Earth Observatory in Palisades, New York. \"Then, the police shut us down.\" But they also offered the team a way forwards. \"If we could shrink it to the size of a Walkman, we'd be back in business.\" Last November, after several iterations with his collaborators, the first of Chillrud's Walkman-sized environmental sensors finally arrived. When participants in the study leave the vicinity of a 'home' beacon, the device switches between two filters, making it possible for Chillrud to distinguish between exposures at home and elsewhere; a Global Positioning System (GPS) device helps to differentiate exposures during the commute from those during the school or work day. After several days of use, the filters can be chemically analysed to identify different sources of black carbon and other chemicals. And the NYPD will be pleased: the pared-down version slips neatly into a special vest with an air inlet near the collar. The first health studies with the contraption will be aimed at more accurately measuring passive contact with tobacco smoke. Chillrud will be studying 50 adults and a handful of children using portable sensors and a method developed by Avrum Spira, a pulmonary specialist at the Boston University School of Medicine in Massachusetts, which uses changes in gene expression in cells brushed from the nostril to assay smoke exposure 2 . Spira believes that a more precise measure of cumulative smoke exposure can pin down the reasons why some smokers \u2014 but not all \u2014 develop lung cancer and conditions such as chronic obstructive pulmonary disease. \"We are not measuring just exposure, but how you are responding to exposure,\" Spira says.  \n                Step by step \n             \n               boxed-text \n             Another aspect of daily exposure is charted in Kevin Patrick's maps of San Diego, California. They take a few minutes to understand. The blue Pacific lapping against the shore on the left is immediately recognizable, as is the city itself, a false-colour patchwork of highways, buildings and parkland. Finally, you begin to notice the green, yellow, orange and red dots, and it all starts to come together. The dots show an individual's heart rate at different points in time: widely spaced green dots represent the sedentary drive to work; a day at the office generates green dots layered on top of each other; and finally, a jog or bike ride along the bluffs appears as a string of heart-thumping orange and red (see  'Every step you make' ). Patrick, a director of the Center for Wireless and Population Health Systems at the University of California, San Diego (UCSD), says that these maps measure physical activity more accurately than the pedometers and questionnaires he and other researchers used for years. \"We realized we needed to know not only how active someone is, but where that activity occurs,\" he says. Such monitoring can help the researchers understand how the layout of a city \u2014 with its parks, hills and smog traps \u2014 influences physical activity and, ultimately, public health. Patrick launched the mapping project in 2007. Called the Physical Activity Location Measurement System, or PALMS, it combines heart-rate monitors, a GPS device and acceleration sensors to record body movements in detail. More than 1,500 people have already worn the US$60 devices, after which they sit down to explain what they were doing at different points during the day. Working with computer scientists, Patrick hopes to develop a pattern recognition system to automatically distinguish different activities. The first proposed health application of PALMS will measure physical activity among San Diego-based participants in the Hispanic Community Health Study/Study of Latinos, run by the US National Heart, Lung, and Blood Institute among other bodies. He also has plans to measure the effects of interventions, such as campaigns encouraging people to spend more time in parks than city streets. Patrick works on another project, CitiSense, led by software engineer William Griswold, also at UCSD. This aims to measure physical activity and airborne pollutants with gadgets similar to Chillrud's. In one planned study, Patrick will give these devices to San Diego cyclists, providing them with real-time feedback on the quality of the air they breathe during bike rides. Patrick says he looks forward to the day when researchers can link up the data that he is collecting with those on social networks, psychology and genetics to understand how these factors in combination contribute to disease. \"I don't think it's going to be very long before that happens,\" he says.  \n                Gulp by gulp \n              No study of human exposure will be complete without examining food. That's why, on a rainy afternoon in December, Baranowski was looking closely at a dinner plate of maize (corn) on a flat-screen monitor. In fact, it was one of eight plates of maize, identically positioned on a blue table, differing only in their portion sizes from a few spoonfuls to a few cups. \"The kids' job,\" Baranowski says, \"is to pick which size comes closest to the portion they consumed.\" Over the past few years, Tom and Janice Baranowski have taken serial pictures of foods prepared in the 'metabolic' kitchen on the third floor of the Children's Nutrition Research Center, ranging from breakfast cereal to chicken nuggets to grapes, amassing some 15,000 photographs altogether. The photos are part of an effort funded by the US National Cancer Institute to improve the Baranowskis' food intake recording software, called Automated Self-Administered 24-hour Dietary Recall (ASA24), and adapt it for use by children. During trials, the photo-prompts help children estimate portion sizes of meals they ate with about 60% accuracy, Baranowski says. The goal is to build a web-based tool that other researchers can use in place of food diaries to, for instance, link up dietary habits, genetic signatures and risk of disease. Electrical engineer Mingui Sun of the University of Pittsburgh, Pennsylvania, is trying to circumvent self-reporting entirely. He has built an all-purpose exposure-biology device that hangs around the neck and contains 5\u20138 sensors including a GPS device, an audio recorder, accelerometer and a digital camera programmed to take 2\u20135 pictures a second over the course of a week 3 . Image-processing software can automatically recognize dinner plates or a glass of milk, segmenting the video stream so that meals and cooking procedures can later be reviewed by dieticians. Sun says the devices will soon be used in a pilot study estimating the caloric intake and physical activity levels of people who are obese. Vineis, though, has taken a very different tack to measuring the dietary component of the exposome, as part of his work on the ten-country European Prospective Investigation into Cancer and Nutrition cohort. In November, his group published a proof-of-principle paper 4 , in which they compared blood-plasma analyses and dietary assessments of 24 people who went on to develop colon cancer over a seven-year period, compared with 23 healthy controls. They found one biomarker \u2014 a derivative of benzoic acid produced by fibre-digesting gut bacteria \u2014 that correlated with dietary fibre intake and a reduced colon cancer risk. Vineis calls this the \"meet-in-the-middle approach\" to discovering biomarkers that measure exposure at the same time as showing how the exposure might foreshadow disease. But fibre is just one of the known and unknown environmental exposures to which a human body is subjected, and colon cancer just one of its many downfalls. A comprehensive exposome is many years off \u2014 so for now, Vineis is just hoping for a better way to measure one exposure at a time. \"I don't think we'll completely give up on questionnaires,\" he says. Brendan Borrell is a freelance writer based in New York. \n                     Genes, Environment and Health Initiative \n                   Reprints and Permissions"},
{"file_id": "471428a", "url": "https://www.nature.com/articles/471428a", "year": 2011, "authors": [{"name": "Lizzie Buchen"}], "parsed_as_year": "2006_or_before", "body": "Why is it so hard to find a test to predict cancer? On 3 March, two studies appeared online that offered 19 pages of gloomy reading for anyone interested in cancer. They focused on biological molecules, or biomarkers, the presence of which in the blood might be used to detect the earliest glimmers of ovarian cancer \u2014 a disease not normally discovered until it has destroyed the ovaries and rotted other parts of the body. The researchers, coordinated by the Early Detection Research Network (EDRN) of the US National Cancer Institute (NCI), had assembled 35 protein biomarkers, including 5 panels of proteins, that had looked the most promising in early studies. They had carried out rigorous testing \u2014 screening blood samples from more than 1,000 women \u2014 to ask whether these seemingly breakthrough biomarkers were better at identifying women with early ovarian cancer than the one flawed biomarker that had been in use for almost 30 years, CA-125. None of them was 1 , 2 . \"CA-125 remains the 'best of a bad lot',\" read an accompanying perspective article 3 . \"The new candidates have fallen short of expectations.\" Tied in last place for its poor performance among the biomarker panels was one identified by Gil Mor, a cancer biologist at Yale University in New Haven, Connecticut. Mor's six-protein panel detected ovarian cancer in only 34% of the women who were diagnosed with the disease within a year. (CA-125, by contrast, detected 63%.) Mor's panel already had a tortured history. A primary research paper behind it had been criticized by other scientists for allegedly using inappropriate statistical calculations and for optimistically concluding that the test would help women before rigorous follow-up studies proved that it could. Yet for four months in 2008, the test was sold to patients by Laboratory Corporation of America (LabCorp) in Burlington, North Carolina, the company that licensed the panel from Yale. LabCorp had marketed the test under the name OvaSure until the US Food and Drug Administration (FDA) intervened and the company pulled it from the market. The panel offered \"invaluable object lessons\" for bringing a test prematurely to the clinic, wrote the authors of the perspective article. Similar lessons can be found in the stories behind many cancer biomarkers that have sputtered and failed on their way to the clinic. Those tests that are in clinical use \u2014 including prostate-specific antigen (PSA) for prostate cancer, mammogram-detected masses for breast cancer and CA-125 \u2014 fail to detect all cancers and sometimes 'detect' ones that aren't there. Genomics, proteomics and other such technologies promised to help by finding combinations of markers that are more powerful and cancer-specific than individual ones, but that promise has not been realized. Researchers using such technologies have published studies on thousands of panels, suggesting that they can detect early-stage disease, guide patient treatment and monitor recurrence. But only a tiny number of such tests have reached the clinic \u2014 and none for the early detection of cancer, the biggest clinical challenge of all. \"Much biomarker research has been done very badly for decades,\" believes Lisa McShane, a biostatistician at the NCI in Rockville, Maryland. \"Even when it was single markers. Now, as we're moving up to multiple markers, all our bad habits are coming back to bite us in a big way.\" These habits have been thrown into the spotlight by the EDRN's study, one of the largest and most systematic validation studies of biomarkers so far. It came just months after a high-profile decision at Duke University in Durham, North Carolina, to suspend clinical trials of a genomics-based biomarker panel designed to direct chemotherapy in patients with breast cancer. A number of scientists had raised concerns about the Duke group's data and analysis, and the trial was stopped after allegations came to light that the lead researcher, geneticist Anil Potti, had made false claims on his CV. Last September, the Institute of Medicine (IOM), part of the US National Academies, assembled a committee to discuss lessons for developing tests based on 'omics' technologies and bringing them to the clinic. \"Why don't we have assays out there, with this enormous promise?\" Dan Hayes, a breast-cancer researcher at the University of Michigan in Ann Arbor asked researchers at the first IOM committee meeting in December 2010.\"It's either because these things just don't work, or because we've used sloppy science to test them.\" It is too early to say whether either of these is true: the field is still young, and faces many challenges. It has drawn in many cancer biologists who are excited by the potential to translate their work to the clinic \u2014 but they sometimes lack the expertise or resources needed to pursue translational or clinical work. \"A lot of novices came in. They get in without realizing that the problem may be more complex than it appears,\" says Eleftherios Diamandis, a clinical biochemist at the University of Toronto in Canada. And although most experts agree that potential biomarkers for early cancer detection should be validated on samples taken before diagnosis \u2014 the stage at which the test would be used in the clinic \u2014 that is a step that few groups attempt and no biomarker for ovarian cancer has passed, as the EDRN study made clear. \"Sometimes the glamour of the technology or the sheer volume of omics data seem to make investigators forget basic scientific principles,\" said McShane at the IOM meeting. Mor agrees that the field has faced problems, and that it is important for markers to go through a careful process of design and validation, as he tried to do. \"There's been an enormous amount of hype and promise,\" sums up David Ransohoff, a cancer epidemiologist at the University of North Carolina in Chapel Hill. \"But after 10 or 15 years of intense work in these fields, there's simply not a lot to show for it. It's important for the whole field to step back and look at what is wrong.\"  \n                Making a difference \n              Mor began his career in Israel, where he trained as a clinician at the Hebrew University of Jerusalem. But an experience in the final years of his oncology residency compelled him to change course. A young woman arrived at the hospital with ovarian cancer, a disease that kills some 140,000 women worldwide each year. The oncology team removed the woman's ovaries and put her through several rounds of chemotherapy, which seemed to be successful. But 18 months later, she was back, her body riddled with tumours, and she soon died. \"Chemotherapy didn't do anything for her,\" Mor recalls. \"She was 29. She was a beautiful girl. An impressive girl. A medical student. And I never understood what happened to her.\" Mor decided to leave medicine, which had been unable to save her, for research, which one day might. He earned a PhD studying ovarian cancer at the Weizmann Institute of Science in Rehovot, Israel, before moving to Yale in 1997. He went on to start a programme called Discovery to Cure, aiming to speed cancer research to the clinic. The group began to build a bank of blood and tissue samples, including some from a Yale clinic for women with a high risk of ovarian cancer owing to a family history of the disease.\"There was a lot of excitement around that time for finding proteins specific to cancer,\" says Mor. In 2003, David Ward, then a geneticist at Yale, contacted Mor. Ward had co-founded Molecular Staging, a company in New Haven that had developed a 'high-throughput' technique for quantifying multiple proteins in the blood using arrays of antibodies 4 . He asked whether he could use Mor's samples to search for markers of early ovarian cancer. Mor had never been involved with biomarker research \u2014 \"I do biology of cancer, not biomarker development,\" he says \u2014 but he signed up, intrigued by the clinical potential of the technology. Ward had scoured the literature for proteins that had been associated with ovarian-cancer growth and malignancy, and had come up with 169 candidates. Using the protein-quantification technique, Ward's company screened blood samples in Mor's tissue bank that came from two groups: women with newly diagnosed ovarian cancer who had been enrolled in Yale's high-risk clinic, and women who had come to the hospital for routine gynaecological exams. Using additional cancer-patient samples, they whittled the list down to four proteins: leptin, prolactin, osteopontin and insulin-like growth factor II. Mor worked to develop an algorithm that could automatically classify women as having cancer or not, depending on levels of these four proteins. When the team ran a new set of blood samples through the algorithm, they got astounding results. The test showed a sensitivity of 95% (meaning it correctly detected 95% of the ovarian-cancer cases) and a specificity of 95% (it erroneously classified only 5% of healthy people as having cancer). \"I was delighted,\" says Mor. On equivalent samples, CA-125 tests typically have a sensitivity of 70\u201380% and a specificity of around 95%. In May 2005, the findings were published in the  Proceedings of the National Academy of Sciences   ( PNAS ), with Ward as a contributing author 5 . Before publication, Mor helped the Yale Office of Cooperative Research to prepare a patent application. \"A lot of companies expressed interest in licensing the panel,\" says John Puziss, director of technology licensing at Yale. LabCorp licensed the test in 2006, as did Millipore, a biomanufacturing company based in Billerica, Massachusetts. (Mor says that the royalties he and his co-inventors received \"were not a significant amount\".) The test's promising results had also caught the attention of researchers in the EDRN, who were just putting together their validation study. Up to that point, most biomarkers for detecting early ovarian cancer had only been shown to distinguish patients with diagnosed cancer from healthy controls, but they are intended to detect the disease in women whose cancer is just budding, before symptoms develop. What the field needed was a 'prospective' study, run on blood samples from apparently healthy women, to see whether the biomarkers could pinpoint those who would later be diagnosed with ovarian cancer. Such samples, from large numbers of women who are tracked over months or years, are extremely difficult to come by.  \n                Problem detection \n              The EDRN found what was needed in the Prostate, Lung, Colorectal, and Ovarian (PLCO) Cancer Screening Trial, sponsored and run by the NCI. Between 1992 and 2001, the trial had been collecting blood at regular intervals from 155,000 women and men, and screening them for cancer. By June 2006, 118 of the women had developed ovarian or closely related cancers, and the EDRN researchers were now in a position to use them to evaluate the most promising biomarkers for early detection. Ziding Feng, a biostatistician at the Fred Hutchinson Cancer Research Center (FHCRC) in Seattle, Washington, and coordinator of the EDRN, visited Mor to discuss whether his panel of four proteins could be included in the study. Mor was already in the process of refining the panel: he had more patient samples, and wanted to add more markers, including CA-125 and the protein macrophage migration inhibitory factor, to make the test more sensitive to cancer. LabCorp had been running his new samples on assay kits manufactured by Millipore. (Ward, meanwhile, had moved to the Nevada Cancer Institute in Las Vegas, and was not involved in data collection or analysis.) When Mor showed Feng how he was analysing his recent data, Feng was troubled. Mor asked him to go through the new results himself, and Feng agreed to collaborate. \"I do not do statistics,\" says Mor. \"That is not my field.\" The researchers also added the six-protein panel to the EDRN's validation study. Feng and Gary Longton, another statistician at the FHCRC, developed their own classification algorithms, and found that Mor's test had a sensitivity of 95% and specificity of 99%. They also calculated the positive predictive value (PPV) of the test \u2014 the proportion of patients who the test would diagnose with the disease and do in fact have it. A high PPV means that few people will be misdiagnosed, which is crucial when screening healthy people. Feng and Longton calculated the PPV at 6.5%, too low for the test to be of much use for screening. But separately, Mor was working with a different figure, of 99.3%. The huge disparity between the two values stemmed from the way that they calculated the figure and factored in the prevalence of ovarian cancer \u2014 an important variable in calculating the PPV. Following convention, Feng and Longton calculated the PPV using the accepted prevalence in post-menopausal women, 1 in 2,500 (0.04%). But Mor's figure was calculated solely from the study population, in which the prevalence was 46%. \"We calculated the PPV based on the population in the study, because we always intended the test for the high-risk population,\" says Mor. \"If you want to bring the test to the clinic, it has to be calculated based on the population you're going to study,\" he says, noting that other research studies work out the PPV for the study population in this way. It's a common mistake, believes McShane, who \u2014 like other statisticians \u2014 disagrees with Mor's logic. \"I see that a lot, but it is nowhere near the correct thing to do,\" she says. Even in high-risk populations \u2014 women who are screened every year because of their family history or because they have tested positive for mutations in tumour-suppressor genes  BRCA1   or  BRCA2   \u2014 the prevalence is around 0.5%, far below the 46% in Mor's study population. Similar battles over the correct use of statistics litter the cancer-biomarker field, said researchers at the IOM meeting last year. \"It's the type of thing where non-statisticians think statisticians are being uptight about something that's not going to matter anyway,\" says McShane. Mor prepared a paper reporting the latest work. But when Feng and Longton saw the page proofs, they noticed that the PPV value was reported as 99.3%. They asked Mor to change it to the 6.5% that they had calculated, and to correct a few other typographical errors in the tables. \"He agreed, so we signed off,\" recalls Feng. But there was a miscommunication: Mor thought that Feng had agreed to the use of the high PPV, and that everyone approved of the final manuscript. The paper was published online in  Clinical Cancer Research 6  in February 2008, and to Feng's shock it reported the high PPV. \"You can imagine how upset I was when I saw it in the paper,\" says Feng. Feng called Mor. \"I told him, those are errors, we told you those are not correct.\" Feng also contacted the journal, the editor of which asked Mor to submit a correction to fix the PPV and the other typos. Mor agreed, adding the lower PPV as a footnote to the table and in a written correction. A few weeks later, Feng received an e-mail with unwelcome news from a colleague: LabCorp was preparing to market the panel, and was \"hopeful that this test will be available to women by the end of the year\". \"I was shocked,\" says Feng. \"I had no idea this was coming.\" He thought that the markers should be validated further before they went to the clinic. In March 2008, Feng and Mor saw each other at a meeting in Washington DC. \"I told him, face to face, you cannot do this,\" says Feng. \"You have to wait until after the PLCO validation. What you have done is early discovery. If validation does not support your earlier claim, you're making a significant error.\" Mor does not recall this encounter, but says that Feng's \"role was to analyse the data, not to make judgements of a company decision\". Now, Mor says that if he were preparing the paper again, he would include both the low and high values for the PPV. And he vacillates about whether LabCorp's decision to offer the test to women before it had undergone more validation studies was the right thing to do. He says he thought that clinical use of the test might be a good way to do further validation. \"It's very difficult to do that on large numbers of patients,\" he says. \"It's extremely expensive. The only way to do the study is if LabCorp started distributing the test and enrolling patients.\" Mor notes that many tests, such as mammography, have been offered to patients as an aid to diagnosis even while data on the test are being collected. \"Was it the right time? I don't know,\" he says.  \n                Critical backlash \n              On 23 June 2008, LabCorp announced the availability of the OvaSure test, for between US$220 and $240. The press release said that it was being offered to women with a high risk of the disease, and quoted Mor as saying he was \"pleased that this test is available to help physicians detect and treat ovarian cancer in its earliest stages\". Excited chatter about the test spread through patient forums and support groups, but it was soon countered by cautionary tales. Jean McKibben, an ovarian-cancer survivor, rushed to take OvaSure on the first day it was available, and her results showed a 0.00 chance of cancer. A week later, scans showed that her cancer was back. She was crushed. \"I wanted this to work so badly,\" she wrote on a discussion board. One week after LabCorp's announcement, the Society of Gynecologic Oncologists in Chicago, Illinois, released a statement expressing concern about OvaSure, saying that \"additional research is needed to validate the test's effectiveness\". The paper in  Clinical Cancer Research   was also circulating at the Canary Foundation, a non-profit organization based in Palo Alto, California, that funds research on early cancer detection. Scientists there found other reasons for concern. One member, Nicole Urban, head of the Gynecologic Cancer Research Program at the FHCRC, had found that levels of prolactin, one of the proteins in the panel, are highly sensitive to stress \u2014 something very likely to affect women entering the clinic with symptoms of ovarian cancer 7 . After controlling for that, she says, \"prolactin gave no signal at all for malignancy. It was useless.\" Others pointed out that the high specificity and sensitivity figures reported in the paper's conclusions, and trumpeted in Yale and OvaSure press releases, were not present in any of the tables or figures. And they bristled at the positive tone of the discussion, which stated that the test \"will enhance the potential of treating ovarian cancer in its early stages and therefore, increases the successful treatment of the disease\". \"There were a lot of uncertainties, and evidence of biases,\" says Martin McIntosh, who researches markers for early-stage ovarian cancer at the FHCRC, and is a member of the Canary group, \"But the narrative only highlighted the best-performing analysis. It didn't mention caveats.\" Members of the Canary group wrote a letter to  Clinical Cancer Research , describing some of their complaints. Meanwhile, Feng agreed to co-author a second letter, criticizing the paper even though he was a co-author. The fuss was already reaching the FDA, which on 7 August 2008 sent a letter to LabCorp saying that the test \"has not received adequate clinical validation, and may harm the public health\". A second letter, sent by the FDA on 29 September 2008, alleged that LabCorp did not have the necessary marketing clearance or approval for the test from the FDA. LabCorp replied to the FDA on 20 October, disagreeing with the agency's assertions, but agreed to pull OvaSure from the market. It did so on 24 October 2008, just one day after  Clinical Cancer Research   published the critical letters from the Canary Foundation and Feng, as well as a third from the Centers for Disease Control and Prevention (CDC) in Atlanta, Georgia 8 , 9 , 10 . (Millipore continues to market the biomarker panel for use in research, not by patients.) Mor was surprised by all three letters. In his published response 11 , he disputed some of the criticisms and wrote that any concerns about commercialization should be taken up with LabCorp. Stephen Anderson, vice-president of investor relations at LabCorp, says that OvaSure was not marketed as a test for detecting cancer recurrence, which was how some patients used it. He says that LabCorp \"continues to believe OvaSure offers a valuable tool for ovarian-cancer detection in conjunction with other diagnostic techniques\", and that the assay is still in development. The company would not provide further comment.  \n                Doubts and lessons \n              Since then, Mor has worked hard to validate his panel. He and Ward have completed a study on a much larger set of samples including many from women diagnosed in the earliest stages of ovarian cancer 12 , and in which LabCorp again ran the assays. The test still performed well at distinguishing the patients from the healthy controls. Mor says he is puzzled by the PLCO trial results, and he hopes that further analysis of the trial data will help to explain why his biomarkers performed so poorly. He continues to express confidence in his panel, saying that the test could be most useful in high-risk populations, and when used regularly \u2014 every two to three months \u2014 to monitor rising and falling levels of the biomarkers. But the whole experience has made him reluctant to pursue biomarker work much further. \"I'm focusing on understanding cancer stem cells,\" he says. Others say that's just as well. The panel's poor performance in the PLCO study makes critics question its usefulness in any group, even a high-risk one. McIntosh says that the PLCO study's damning conclusions should serve as a wake-up call. \"The entire field has to cope with this,\" he says \u2014 including him, given that the most promising biomarkers discovered by his institution also failed to improve on CA-125 in the trial. \"It's hugely disappointing.\" The IOM committee, which is expected to release its results sometime in 2012, may help to find a way forward. At a meeting later this month, the members plan to draw lessons from the biomarker failures, as well as from the few success stories (see  'The gene collection that could' ). One of the most urgent lessons is the need to help researchers validate their biomarkers on appropriate samples before they reach the clinic. Feng says that the EDRN has been collecting its own high-quality tissue reference sets for ovarian, breast, lung, colon, liver and prostate cancers, from people who aren't yet showing symptoms and those in all stages of the disease. Investigators can apply to test their biomarkers on blinded tissue samples. Until this type of testing becomes commonplace, there is no way of excluding the possibility that, as Hayes suggested at the IOM meeting, \"these things just don't work\" \u2014 particularly when it comes to picking up cancer early on. \"People keep talking about early-detection biomarkers as if they are a fact, and we only need to find them,\" says McIntosh,\"when in reality their existence is a hypothesis that needs to be tested.\"\n \n                 See Outlook  \n                 p.450 \n               Lizzie Buchen is a freelance writer in San Francisco, California. \n                     Cancer \n                   \n                     Nature Reviews Clinical Oncology: focus on Biomarkers \n                   \n                     Institute of Medicine: Review of Omics-Based Tests for Predicting Patient Outcomes in Clinical Trials \n                   \n                     Early Detection Research Network \n                   \n                     Canary Foundation \n                   \n                     Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial \n                   \n                     Yale Discovery to Cure Program \n                   \n                     Genomic Health \n                   Reprints and Permissions"},
{"file_id": "470161a", "url": "https://www.nature.com/articles/470161a", "year": 2011, "authors": [{"name": "Amy Maxmen"}], "parsed_as_year": "2006_or_before", "body": "An obscure group of tiny creatures takes centre stage in a battle to work out the tree of life. Since the turn of the twentieth century, zoologists have set out from coastal marine stations at dawn to sieve peppercorn-sized worms from sea-bottom muck. These creatures, called acoels, often look like unremarkable splashes of paint when seen through a microscope. But they represent a crucial stage in animal evolution \u2014 the transition some 560 million years ago from simple anemone-like organisms to the zoo of complex creatures that populate the world today. There are about 370 species of acoel, which gets its name because it lacks a coelom \u2014 the fluid-filled body cavity that holds the internal organs in more-complex animals. Acoels also have just one hole for both eating and excreting, similar to cnidarians \u2014 a group of evolutionarily older animals containing jellyfish and sea anemones. But unlike the simpler cnidarians, which have only an inner and outer tissue layer, acoels have a third, middle tissue layer. That is the arrangement found in everything from scorpions to squids to seals, suggesting that acoels represent an intermediate form. That hypothesis has gained considerable support in recent years, but a report published in  Nature   this week 1  is causing scientists to rethink the storyline. The study by an international team of researchers, who used new analytical techniques and data, removes acoel worms from their position near the trunk of animal evolution and instead places them closer to vertebrates (see 'Competing views of animal evolution'). boxed-text The rearrangement has triggered protests from evolutionary biologists, who are alarmed that they may lose their key example of that crucial intermediate stage of animal evolution. Some researchers complain that the evidence is not strong enough to warrant such a dramatic rearrangement of the evolutionary tree, and claim that the report leaves out key data. In any case, the vehemence of the debate shows just how important these worms have become in evolutionary biology. \"I will say, diplomatically, this is the most politically fraught paper I've ever written,\" says Max Telford, a zoologist at University College London and last author on the paper. The debate focuses on where acoels fit in the family tree of bilaterians, three-layered animals with bilateral symmetry. Biologists divide these animals into two branches. The larger group, called protostomes, contains invertebrates such as earthworms, squids, snails and insects. The smaller group, known as deuterostomes, includes both vertebrates and invertebrates, such as sea urchins, humans and fish. Zoologists have generally placed acoels on the earliest branch of the bilaterians \u2014 before the split between protostomes and deuterostomes \u2014 because the worms lack so many key features such as a separate mouth and anus, a central nervous system and organs to filter waste. Although the position of acoels has moved around a bit over the decades, a DNA analysis in 1999 (ref.  2 ) and several since then have placed them back in their earlier spot. In particular, a genetic study of 94 organisms in 2009 solidified the conclusion that acoels belonged at the very base of the bilaterians 3 . That study, led by Andreas Hejnol, a developmental biologist at the Sars International Centre for Marine Molecular Biology in Bergen, Norway, confirmed that acoels and their kin occupied an intermediate spot between cnidarians and the more-complex bilaterians. \"I suddenly had the feeling that everything had finally fallen into place,\" says Claus Nielsen, an evolutionary biologist at the Natural History Museum of Denmark, who has followed acoels for 40 years as they wandered across the tree of life.  \n                Shaking the tree \n              But the study by Telford and his colleagues 1  has shaken the tree again and placed acoels within the deuterostome branches, next to the echinoderms (which include sea urchins) and acorn worms. Their genetic analyses suggest that the acoels \u2014 and a marine worm named  Xenoturbella   \u2014 descended from a more complex ancestor and lost many of the features seen in other deuterostomes. The researchers used several approaches and examined three independent data sets to come to their conclusions. First, they reanalysed data from Hejnol's 2009 study 3 , using 66 species instead of 94. Herv\u00e9 Philippe, a bioinformatician at the University of Montreal in Quebec, Canada, and first author of the  Nature   paper 1 , says that the team removed species that had incomplete genetic data or were 'fast-evolving' \u2014 meaning that some of their genes had accumulated many changes, when compared with genes from animal groups that emerged around the same time. Phylogenetic computer programs have a well-known problem with these kinds of species and tend to group them together even though they are not related. Philippe and his co-workers used a more sophisticated mathematical model to analyse sequence evolution, which helped to minimize this problem. Without this model and careful species selection, Philippe says, acoels can fall at the base of the animal tree. After analysing sequences from nuclear DNA, the group made a separate evolutionary tree based on genes in mitochondria. They also studied microRNAs, which regulate gene expression but do not code for proteins. According to co-author Kevin Peterson, a palaeontologist at Dartmouth College in Hanover, New Hampshire, microRNAs are particularly useful for studying deep evolutionary relationships. The team found that acoels have a type of microRNA known to be specific to deuterostomes, suggesting that they are related. The authors acknowledge that no single data set clinches the case for placing acoels within the deuterostomes. But taken together, says Telford, \"the fact that our evidence points in the same direction makes me think it's right\". If acoels do fit within the deuterostomes, the worms must have evolved from an ancestor with a central nervous system, a body cavity and a through-going gut that connected an anus and mouth \u2014 features seen in existing deuterostomes. So researchers would need to explain how acoels and  Xenoturbella   lost those and other characteristics. They would also be left to search for another primitive-looking lineage that represents the evolutionary step between jellyfish-like animals and bilaterians. (If one even exists. Peterson says that many complex features may have emerged all at once.) Some researchers are not ready to give up on the old ideas of where acoels fit. \"I'm sad about their paper, but I'm not upset,\" says Hejnol. \"I'd be upset if their analysis was excellent and it meant we lost a representative animal to bridge an important transition in the tree of life.\" Hejnol and his colleagues have doubts about the reliability of the tree that Telford and his team built from nuclear genes, which is their main evidence. Critics say that the key branches of the tree are not as statistically strong as they should be. Because of this, Brian O'Meara, a phylogeneticist at the University of Tennessee in Knoxville, calls the new tree \"suggestive, but not definitive\". The study has also come under fire for leaving out data that some scientists say would have weakened the researchers' conclusions. An author on the paper had previously analysed a species of worm closely related to acoels known as  Meara stichopi , and did not find deuterostome microRNA. But the authors defend their decision to keep  M. stichopi   out of their microRNA analysis owing to concerns about the quality of those data. Moreover, not everyone is convinced by the power of microRNA analysis, which has only recently been adopted for evolutionary studies. This report marks the method's most high-profile appearance yet as a tool to resolve relationships. Because microRNAs can be lost during evolution, it is possible that the deuterostome microRNA in acoels originated in the ancestor of all bilateral animals but was later lost in the protostome line. With so much at stake, researchers are keen to resolve the issue. The US National Science Foundation has been specifically soliciting proposals that target deep divergences in evolutionary history, as part of an initiative called Assembling the Tree of Life, says Tim Collins, a programme director at the foundation. \"We've done a good job within groups, but we've had a hard time reconstructing the deepest branches of the tree of life,\" he says. \"These are the events that happened in a relatively short time compared with the amount of time that has passed since then, which makes things hard.\" Last summer in Kristineberg, Sweden, Hejnol and Telford shared a room while teaching a class together. They debated their differences and discussed an ongoing joint project that might settle them: sequencing the full genomes of an acoel, a species of  Xenoturbella   and the controversial  M. stichopi . With that influx of new genomic information, the researchers are confident that they can reach an agreement about where acoels fit in evolutionary history. \"We're talking about a very close result with a humongous impact,\" says Hejnol, of the newly proposed tree. \"The good thing is, we know how to resolve this issue.\"  Amy Maxmen is a freelance writer based in New York City. \n                     Marc van Roosmalen \n                   \n                     Convention on Biological Diversity \n                   \n                     Max Telford \n                   \n                     Andreas Hejnol \n                   \n                     Assembling the Tree of Life \n                   Reprints and Permissions"},
{"file_id": "470323a", "url": "https://www.nature.com/articles/470323a", "year": 2011, "authors": [{"name": "M. Mitchell Waldrop"}], "parsed_as_year": "2006_or_before", "body": "The Templeton Foundation claims to be a friend of science. So why does it make so many researchers uneasy? At the headquarters of the John Templeton Foundation, a dozen kilometres outside Philadelphia, Pennsylvania, the late billionaire seems to watch over everything. John Templeton's larger-than-life bust stands at one end of the main conference room. His life-sized portrait smiles down from a side wall. His face peers out of framed snapshots propped on bookshelves throughout the many offices. It seems fitting that Templeton is keeping an eye on the foundation that he created in 1987, and that consumed so much of his time and energy. With a current endowment estimated at US$2.1 billion, the organization continues to pursue Templeton's goal of building bridges between science and religion. Each year, it doles out some $70 million in grants, more than $40 million of which goes to research in fields such as cosmology, evolutionary biology and psychology. As generous as the foundation's support is, however, many scientists find it troubling \u2014 and some see it as a threat. Jerry Coyne, an evolutionary biologist at the University of Chicago, Illinois, calls the foundation \"sneakier than the creationists\". Through its grants to researchers, Coyne alleges, the foundation is trying to insinuate religious values into science. \"It claims to be on the side of science, but wants to make faith a virtue,\" he says. But other researchers, both with and without Templeton grants, say that they find the foundation remarkably open and non-dogmatic. \"The Templeton Foundation has never in my experience pressured, suggested or hinted at any kind of ideological slant,\" says Michael Shermer, editor of  Skeptic , a magazine that debunks pseudoscience, who was hired by the foundation to edit an essay series entitled 'Does science make belief in God obsolete?' The debate highlights some of the challenges facing the Templeton Foundation after the death of its founder in July 2008, at the age of 95. With the help of a $528-million bequest from Templeton, the foundation has been radically reframing its research programme. As part of that effort, it is reducing its emphasis on religion to make its programmes more palatable to the broader scientific community. Like many of his generation, Templeton was a great believer in progress, learning, initiative and the power of human imagination \u2014 not to mention the free-enterprise system that allowed him, a middle-class boy from Winchester, Tennessee, to earn billions of dollars on Wall Street. The foundation accordingly allocates 40% of its annual grants to programmes with names such as 'character development', 'freedom and free enterprise' and 'exceptional cognitive talent and genius'. Unlike most of his peers, however, Templeton thought that the principles of progress should also apply to religion. He described himself as \"an enthusiastic Christian\" \u2014 but was also open to learning from Hinduism, Islam and other religious traditions. Why, he wondered, couldn't religious ideas be open to the type of constructive competition that had produced so many advances in science and the free market? That question sparked Templeton's mission to make religion \"just as progressive as medicine or astronomy\". He started in 1972, by endowing the Templeton Prize for progress in religion. He stipulated that the cash value should always be higher than that of the Nobel Prizes; it currently stands at \u00a31 million (US$1.6 million). Early Templeton prizes had nothing to do with science: the first went to the Catholic missionary Mother Theresa of Calcutta in 1973. By the 1980s, however, Templeton had begun to realize that fields such as neuroscience, psychology and physics could advance understanding of topics that are usually considered spiritual matters \u2014 among them forgiveness, morality and even the nature of reality. So he started to appoint scientists to the prize panel, and in 1985 the award went to a research scientist for the first time: Alister Hardy, a marine biologist who also investigated religious experience. Since then, scientists have won with increasing frequency. In 2010, the prize went to Francisco Ayala, a geneticist at the University of California, Irvine, and a former Dominican priest who has spent 30 years fighting the teaching of creationism and intelligent design in schools as alternatives to evolution. The prize has come in for some academic scorn. \"There's a distinct feeling in the research community that Templeton just gives the award to the most senior scientist they can find who's willing to say something nice about religion,\" says Harold Kroto, a chemist at Florida State University in Tallahassee, who was co-recipient of the 1996 Nobel Prize in Chemistry and describes himself as a devout atheist.  \n                Scientists as allies \n              Yet Templeton saw scientists as allies. They had what he called \"the humble approach\" to knowledge, as opposed to the dogmatic approach. \"Almost every scientist will agree that they know so little and they need to learn,\" he once said. The scientific attitude informed the motto that Templeton crafted for his foundation: \"How little we know, how eager to learn.\" The foundation began with just two employees in a room above the garage of his oldest son, Jack Templeton, in Bryn Mawr, Pennsylvania. The foundation's initial activities were also modest: administering the Templeton Prize, funding science and religion courses at universities and seminaries, and sponsoring essay contests. \"But the foundation was a research project in his mind,\" says Jack Templeton, who retired from his career as a paediatric and trauma surgeon in 1995 to become the organization's president. The slowly growing staff was bombarded with ideas and directives in a near-daily stream of faxes from Lyford Cay in the Bahamas, where the elder Templeton had lived since 1968. Templeton's interests gave the resulting list of grants a certain New Age quality (See ). For example, in 1999 the foundation gave $4.6 million for forgiveness research at the Virginia Commonwealth University in Richmond, and in 2001 it donated $8.2 million to create an Institute for Research on Unlimited Love (that is, altruism and compassion) at Case Western Reserve University in Cleveland, Ohio. \"A lot of money wasted on nonsensical ideas,\" says Kroto. Worse, says Coyne, these projects are profoundly corrupting to science, because the money tempts researchers into wasting time and effort on topics that aren't worth it. If someone is willing to sell out for a million dollars, he says, \"Templeton is there to oblige him\". But Templeton wasn't interested in funding mainstream research, says Barnaby Marsh, the foundation's executive vice-president. Templeton wanted to explore areas \u2014 such as kindness and hatred \u2014 that were not well known and did not attract major funding agencies. Marsh says Templeton wondered, \"Why is it that some conflicts go on for centuries, yet some groups are able to move on?\" At the same time, says Marsh, the 'dean of value investing', as Templeton was known on Wall Street, had no intention of wasting his money on junk science or unanswerables such as whether God exists. So before pursuing a scientific topic he would ask his staff to get an assessment from appropriate scholars \u2014 a practice that soon evolved into a peer-review process drawing on experts from across the scientific community. Because Templeton didn't like bureaucracy, adds Marsh, the foundation outsourced much of its peer review and grant giving. In 1996, for example, it gave $5.3 million to the American Association for the Advancement of Science (AAAS) in Washington DC, to fund efforts that work with evangelical groups to find common ground on issues such as the environment, and to get more science into seminary curricula. In 2006, Templeton gave $8.8 million towards the creation of the Foundational Questions Institute (FQXi), which funds research on the origins of the Universe and other fundamental issues in physics, under the leadership of Anthony Aguirre, an astrophysicist at the University of California, Santa Cruz, and Max Tegmark, a cosmologist at the Massachusetts Institute of Technology in Cambridge.  \n                The design Debate \n              But external peer review hasn't always kept the foundation out of trouble. In the 1990s, for example, Templeton-funded organizations gave book-writing grants to Guillermo Gonzalez, an astrophysicist now at Grove City College in Pennsylvania, and William Dembski, a philosopher now at the Southwestern Baptist Theological Seminary in Fort Worth, Texas. After obtaining the grants, both later joined the Discovery Institute \u2014 a think-tank based in Seattle, Washington, that promotes intelligent design. Other Templeton grants supported a number of college courses in which intelligent design was discussed. Then, in 1999, the foundation funded a conference at Concordia University in Mequon, Wisconsin, in which intelligent-design proponents confronted critics. Those awards became a major embarrassment in late 2005, during a highly publicized court fight over the teaching of intelligent design in schools in Dover, Pennsylvania. A number of media accounts of the intelligent design movement described the Templeton Foundation as a major supporter \u2014 a charge that Charles Harper, then senior vice-president, was at pains to deny. Some foundation officials were initially intrigued by intelligent design, Harper told  The New York Times . But disillusionment set in \u2014 and Templeton funding stopped \u2014 when it became clear that the theory was part of a political movement from the Christian right wing, not science. Today, the foundation website explicitly warns intelligent-design researchers not to bother submitting proposals: they will not be considered. The foundation's critics are unimpressed. Avowedly antireligious scientists such as Coyne and Kroto see the intelligent-design imbroglio as a symptom of their fundamental complaint that religion and science should not mix at all. \"Religion is based on dogma and belief, whereas science is based on doubt and questioning,\" says Coyne, echoing an argument made by many others. \"In religion, faith is a virtue. In science, faith is a vice.\" The purpose of the Templeton Foundation is to break down that wall, he says \u2014 to reconcile the irreconcilable and give religion scholarly legitimacy. Foundation officials insist that this is backwards: questioning is their reason for being. Religious dogma is what they are fighting. That does seem to be the experience of many scientists who have taken Templeton money. During the launch of FQXi, says Aguirre, \"Max and I were very suspicious at first. So we said, 'We'll try this out, and the minute something smells, we'll cut and run.' It never happened. The grants we've given have not been connected with religion in any way, and they seem perfectly happy about that.\" John Cacioppo, a psychologist at the University of Chicago, also had concerns when he started a Templeton-funded project in 2007. He had just published a paper with survey data showing that religious affiliation had a negative correlation with health among African-Americans \u2014 the opposite of what he assumed the foundation wanted to hear. He was bracing for a protest when someone told him to look at the foundation's website. They had displayed his finding on the front page. \"That made me relax a bit,\" says Cacioppo. Yet, even scientists who give the foundation high marks for openness often find it hard to shake their unease. Sean Carroll, a physicist at the California Institute of Technology in Pasadena, is willing to participate in Templeton-funded events \u2014 but worries about the foundation's emphasis on research into 'spiritual' matters. \"The act of doing science means that you accept a purely material explanation of the Universe, that no spiritual dimension is required,\" he says. It hasn't helped that Jack Templeton is much more politically and religiously conservative than his father was. The foundation shows no obvious rightwards trend in its grant-giving and other activities since John Templeton's death \u2014 and it is barred from supporting political activities by its legal status as a not-for-profit corporation. Still, many scientists find it hard to trust an organization whose president has used his personal fortune to support right-leaning candidates and causes such as the 2008 ballot initiative that outlawed gay marriage in California. Scientists' discomfort with the foundation is probably inevitable in the current political climate, says Scott Atran, an anthropologist at the University of Michigan in Ann Arbor. The past 30 years have seen the growing power of the Christian religious right in the United States, the rise of radical Islam around the world, and religiously motivated terrorist attacks such as those in the United States on 11 September 2001. Given all that, says Atran, many scientists find it almost impossible to think of religion as anything but fundamentalism at war with reason. They have a reflexive reaction against the idea, espoused by Templeton, that progress in spirituality can help to solve the problems of the world.  \n                The big questions \n             \n               Click here for larger image \n               Towards the end of Templeton's life, says Marsh, he became increasingly concerned that this reaction was getting in the way of the foundation's mission: that the word 'religion' was alienating too many good scientists. This prompted a rethink of the foundation's research programme \u2014 a change most clearly seen in the organization's new website, launched last June. Gone were old programme names such as 'science and religion' \u2014 or almost any mention of religion at all (See  'Templeton priorities: then and now' ). Instead, the foundation has embraced the theme of 'science and the big questions' \u2014 an open-ended list that includes topics such as 'Does the Universe have a purpose?' Under this umbrella come new programmes in such areas as mathematical and physical sciences, life sciences, and philosophy and theology \u2014 each, for the first time, with its own team of programme officers. The peer-review and grant-making system has also been revamped: whereas in the past the foundation ran an informal mix of projects generated by Templeton and outside grant seekers, the system is now organized around an annual list of explicit funding priorities. It remains to be seen how reassuring these changes will be for scientists still sceptical of the foundation \u2014 although Marsh notes that last year's inaugural announcement of 13 funding priorities drew some 2,500 submissions. The foundation is still a work in progress, says Jack Templeton \u2014 and it always will be. \"My father believed,\" he says, \"we were all called to be part of an ongoing creative process. He was always trying to make people think differently.\" \"And he always said, 'If you're still doing today what you tried to do two years ago, then you're not making progress.'\" M. Mitchell Waldrop is an editor for Nature in Washington DC. \n                     John Templeton Foundation \n                   \n                     Foundational Questions Institute \n                   \n                     The Edge: Debate over the Templeton Foundation \n                   \n                     Pew poll on Scientists and Religious Belief \n                   Reprints and Permissions"},
{"file_id": "470452a", "url": "https://www.nature.com/articles/470452a", "year": 2011, "authors": [{"name": "Daniel Cressey"}], "parsed_as_year": "2006_or_before", "body": "Nearly one-quarter of biologists say they have been affected by animal activists. A Nature poll looks at the impact. In the past five years animal-rights activists have perpetrated a string of violent attacks. In February 2008, the husband of a breast-cancer biologist in Santa Cruz, California, was physically assaulted at the front door of their home. In the same month, the biomedical research institute at Hasselt University in Diepenbeek, Belgium, was set on fire. In the summer of 2009, activists desecrated graves belonging to the family of Daniel Vasella, then chief executive of the pharmaceutical company Novartis, based in Basel, Switzerland, and torched his holiday home. A poll of nearly 1,000 biomedical scientists, conducted by  Nature , reveals the widespread impact of animal-rights activism. Extreme attacks are rare, and there does not seem to have been any increase in the rate of their incidence in the past few years, but almost one-quarter of respondents said that they or someone they know has been affected negatively by activism. More than 90% of respondents agreed that the use of animals in research is essential, but the poll also highlights mixed feelings on the issue. Nearly 16% of those conducting animal research said that they have had misgivings about it, and although researchers overwhelmingly feel free to discuss these concerns with colleagues, many seem less at ease with doing so in public. More than 70% said that the polarized nature of the debate makes it difficult to voice a nuanced opinion on the subject, and little more than one-quarter said that their institutions offer training and assistance in communicating broadly about the importance of animal research (see  'Assessing the threats' ).  \n                Cracking down \n              During the past decade, both the United States and the United Kingdom have enacted tough laws in response to violent tactics from activists. In 2005, the United Kingdom created the Serious and Organised Crime and Police Act, allowing stiff sentences to be imposed on those who intimidate companies and individuals that contract with animal-testing labs. Activists have since been found guilty of blackmail for terrorizing individuals and companies with financial ties to Huntingdon Life Sciences, a contract animal-testing company in Cambridgeshire, UK (see  page 454 ). In the United States, the 2008 Animal Enterprise Terrorism Act was brought in to combat property damage and threats that produce a 'reasonable fear' of death or injury for researchers or their relatives, although its enforcement has been challenged in the courts. \n               boxed-text \n             These laws do not seem to have driven down the rate of violence. The Foundation for Biomedical Research in Washington DC, which is in favour of animal research, and the anti-animal-research magazine  Bite Back , based in West Palm Beach, Florida, collect accounts of activism incidents from media reports and activist websites, respectively. Although not comprehensive, their data suggest that the worldwide incident rate has been stable for five years or more, with some regional variation. Activity in Britain seems to have dropped since the anti-Huntingdon campaign cooled. Protests have also been scaled back at the Biomedical Sciences Building at the University of Oxford, which opened in 2008 and houses research animals including primates. Although  Nature  's survey was not designed to measure the incidence of activism, it suggests a similar picture: 45% of respondents said they had not perceived an increase in activist activity in the past five years, with some regional differences. US scientists were more likely to say that activism had increased, whereas many UK scientists reported a perceived decrease. Sally Rockey, deputy director for extramural research at the US National Institutes of Health in Bethesda, Maryland, says that the responses probably reflect the publicity drawn by high-profile incidents, not real increases. \"There have been some life-threatening situations, arson and bomb threats for example. One of the things we've seen is some investigators have been targeted at their homes,\" says Rockey. Animal researchers who said that they or someone they knew had been affected by activism wrote about incidents ranging from anonymous threats and protests outside laboratories to vandalism, 'liberation' of animals, physical attacks by masked activists and bombs both real and simulated. \"Home damaged, young children terrorized, death threat, etc,\" reports one genomics researcher matter-of-factly. A small number, about 15% (26 respondents), who had been negatively affected by activism said that they had changed the direction or practice of their research as a result. After encountering violent protests, one US academic was \"much less willing to conduct any studies on non-human primates, despite their absolute critical relevance for neuro-protection research\".  \n                Primal concerns \n              Only 38 scientists working with non-human primates responded to the survey, but they were the group of respondents most likely to strongly agree that activism is a problem. Frankie Trull, president of the Foundation for Biomedical Research, says that in her experience, primate researchers are targeted more than those in any other type of animal work. Although more primate research is conducted in the United States, the ability to work with primates has been challenged in Europe. In 2009, the European Union considered legislation that would have restricted work on non-human primates to research investigating \"life-threatening or debilitating\" conditions. It took a concerted campaign by researchers to amend it to allow for basic research in addition to applied work. Hannah Buchanan-Smith, an animal-welfare researcher at the University of Stirling, UK, says, \"Primate laboratory researchers are finding it harder to justify their research to the public.\" Buchanan-Smith refuses to do any animal research that causes pain, suffering, distress or lasting harm, and says that basic research on primates presents a particular ethical challenge. She argues for alternatives to animal testing. \"Replacement is the ultimate goal and we are moving in that direction with certain groups of animals,\" she says. \"I very much hope in my lifetime that will be achieved in primate research.\" Stefan Treue, head of the German Primate Centre in G\u00f6ttingen, views primate research in a different light. He says that after lay-people have visited his laboratory and seen how work is conducted and why, \"something like 98% understand and accept that this is a small but important and irreplaceable part of biomedical science that is conducted to the highest ethical standards\". Treue rejects an ethical distinction between basic and applied research. \"It's not a logical argument to say, 'I accept applied research but I don't want the underlying basic research', because you can't have one without the other. I have to admit that partly the science community is to blame for not explaining that more clearly and more frequently in public,\" he says.  \n                Public consultation \n              Some results from the survey suggest that communication with the public might be improving. Fifty-five per cent of animal researchers said that their institutions encourage communication with the general public about their work, and only 7% said that this is actively discouraged. In a poll run by  Nature   on this subject in 2006, only 29% of researchers said that they were encouraged to discuss their work, and 11% had been discouraged (see   Nature  444, 808\u2013810; 2006 ). This is good news, says Rockey, but there is much to be done. More than half of the researchers who said they are encouraged to discuss their work indicated that their institutions offered no support or training on how to do so. \"It's important for institutions to have outreach programmes which engage the public in explaining the importance of the research,\" says Rockey. It can be challenging to explain the type of nuanced positions on animal research that the poll revealed: 33% of respondents had \"ethical concerns\" about the role of animals in their current work. Researchers wrote about their preoccupations with reducing pain, minimizing the numbers of animals used and showing respect for their subjects. Some 16% reported \"misgivings\" about work they have done, and half of these (54 researchers) said that they changed their research or practices as a result, suggesting that personal reflection may be more effective than activism at changing behaviour. \"I consider these issues virtually daily,\" wrote a US neuroscientist. \"The day I stop considering these issues is the day I quit. I know few scientists who don't feel similarly.\" Trull welcomes scientists thinking deeply about the issues involved in working with animals, and is glad that 93% of researchers said they feel free to discuss concerns about ethics with colleagues. \"There are a lot of those discussions and debates that go on in the research community. It's a privilege to use these animal models,\" she says. \"Scientists need to view it in this way and I think they do.\"   See Editorial    page 435 . \n                 For full survey results, see:  \n                 go.nature.com/pn9a4i \n               Daniel Cressey writes for Nature from London. Survey work was aided by Laura Harper. \n                     Animal research special \n                   \n                     Foundation for Biomedical Research \n                   \n                     NIH Office of Animal Care and Use \n                   Reprints and Permissions"},
{"file_id": "470454a", "url": "https://www.nature.com/articles/470454a", "year": 2011, "authors": [{"name": "Shanta Barley"}], "parsed_as_year": "2006_or_before", "body": "Researcher by day and activist by night, Joseph Harris was leading an untenable double life that eventually landed him in prison. Most British PhD students head straight to the pub after graduating. Joseph Harris had something else planned. On a freezing Friday night in December 2005, he packed a rucksack with a pair of bolt cutters, a hammer, a flashlight, latex gloves and a can of black spray paint. He drove to a deserted industrial estate just outside Nottingham, UK, and broke into a compound owned by a company that hires out refrigeration units. He slashed the tyres of a van and glued down its windscreen wipers. He smashed his way into an outhouse, tipped cans of lubricant and coolant onto the floor and cut through the electrical cables that powered the office's air-conditioning unit. On his way out he sprayed, \"Now you pay 4 your crimes\" on a wall.  The following Monday, Harris returned to work in Sue Watson's oncology lab at Queen's Medical Centre in Nottingham, where he was already committed to an irreconcilable double life. By day, he studied potential treatments for gastrointestinal cancer \u2014 work that invariably required the use of animal models. By night, he crusaded against such animal research, sabotaging companies with links to it. Within a month, Harris would be caught vandalizing another company. Ultimately, he would become the first person in the United Kingdom to be convicted under a law intended to crack down on activist extremism. The judge overseeing the case lamented a career destroyed and a scientist lost. \"It may well be that your future inability to continue your research into gastrointestinal cancer will be a great loss to those who suffer that disease,\" said Judge Ian Alexander when he sentenced Harris to three years in prison. Harris was released within a year, and has now agreed to speak to  Nature   about what drove him to commit his crimes.  \n                Rummaging for roly-polies \n              Most of Harris's childhood in southern England's New Forest was spent outdoors. He and his younger brother, Thomas, passed the hours rummaging under rocks for roly-polies \u2014 woodlice that roll into a ball when threatened. Joseph Harris showed an early affinity for science. At the age of ten he started compiling a list of every animal in the world. \"I didn't get very far,\" he says. His convictions were already firm, however. In the same year, his teacher asked him to write about what he wanted to be when he grew up. Instead, he wrote about what he didn't want to be: a scientist who did tests on animals. He excelled in biology and chemistry at school, and in 1998 began a degree in molecular biology at the University of Nottingham. While there, Harris joined the local Hunt Saboteurs Association group in fox-hunt sabotage, or 'sabbing'. Shadowing hunts from Land Rovers, he and his friends sprayed citronella to mask the scent of foxes and break pursuit. His actions were legal as long as he didn't injure anyone or trespass on private property. In 2001, Harris was interviewed for a PhD on oesophageal cancer at Queen's Medical Centre. \"I made it clear that I wouldn't do animal testing during the interview, even though I thought it would scupper my chances,\" says Harris. Instead, he got the job, with a grant from the UK Medical Research Council. He soon found himself in Watson's lab studying a class of drugs known as proton pump inhibitors. These drugs are commonly prescribed for gastro-oesophageal reflux disease, but tests on rats in the 1980s suggested that they might also boost the risk of developing a type of cancer known as oesophageal adenocarcinoma ( N. Havu  Digestion    35,   42\u201345; 1986 ). According to one hypothesis, they do this by raising the amount of the growth hormone gastrin in the bloodstream. Harris and his colleagues suggested how, by showing that gastrin can suppress a cell-death pathway in oesophageal cells, allowing them to survive and possibly become cancerous ( J. C. Harris  et al. Cancer Res.    64,   1915\u20131919; 2004 ). \"I wasn't doing any animal testing,\" says Harris, \"but I still felt that I was making important contributions towards the field.\"  \n                Petty insolence \n              As his involvement in research gained momentum, Harris says that his participation in activism waned, save for a single act of petty insolence. One day in a nearby lab, he spied a pair of axolotls \u2014 amphibians studied for their ability to regrow entire limbs. With no one looking, he decanted them into a jar and took them home. \"I just felt so sorry for them,\" he says. But as his doctoral studies neared an end, he was forced to confront the conflict between his career and his convictions. Watson urged Harris to take on the project of a postdoc who had quit the lab. This meant an early promotion for Harris \u2014 still technically a student \u2014 but the project involved working with mice. \"When I took over, she asked me, gently and then increasingly insistently, to continue the animal testing, even though she knew that I was utterly against it,\" he says. The pressure was real, says colleague Jacqueline Dickson, who also did her PhD under Watson's supervision. \"Sue is very used to getting her own way,\" says Dickson. Watson declined to comment for this story. Eventually, Harris acquiesced. He was studying whether a drug designed to prevent gastrin from binding to cell receptors would slow tumour growth in mice. Harris says that he reluctantly agreed to analyse biopsies. \"I didn't inject a single drug, or see a single animal die, but I still felt complicit in what was happening,\" he says. Worst of all, Harris was concerned that the animals were dying for nothing. Phase III clinical trials had shown that the drug, known as Insegia or G17DT, did not improve survival rates in patients with pancreatic cancer, and Harris reasoned that it probably wouldn't work for gastrointestinal tumours either. He was appalled, he says, that his own research into gastrin had been used to justify the drug's continued development. Tortured by his conscience, Harris felt the need to do something. Huntingdon Life Sciences (HLS), the largest animal-testing company in Europe, located in Cambridgeshire, UK, seemed like an appropriate target. He never forgot the highly publicized release of video footage secretly shot at HLS in 1997, showing several severe breaches of animal-research protocols, including beagles being beaten. The films had already riled the broader British activist community into a state of organized fury. Activists firebombed three cars owned by HLS staff and sent the managing director mouse traps allegedly tainted with HIV \u2014 he was later attacked by three men armed with pickaxe handles. The organization Stop Huntingdon Animal Cruelty (SHAC), which formed in 1999 and is based in London, targeted HLS investors relentlessly with forms of intimidation such as hoax bombs and abusive e-mails. One by one, financial backers of the company pulled out, fearful that their staff would be hurt. In early 2001, the Royal Bank of Scotland cancelled a \u00a322-million (US$47 million in 2001) loan, forcing HLS to near bankruptcy. But HLS and the BioIndustry Association \u2014 an organization based in London that promotes UK biotechnology \u2014 fought back, lobbying the government for tougher laws against activists. The London-based Association of the British Pharmaceutical Industry delivered an ultimatum: if the government did not do something to curtail activist attacks, Britain's drug industry would boycott those banks and investment companies that had caved under the pressure of SHAC. \"All of a sudden, the government was confronted with the real threat of pharmaceutical companies pulling out of future research-and-development investment in Britain,\" says Andrew Upton, who studied the animal-rights movement for his PhD at Liverpool John Moores University, UK. The UK government passed the Serious and Organised Crime and Police Act in 2005. Section 145 of the law explicitly prohibits \"interference with contractual relationships so as to harm animal research organisation\", and imposes tough sentences of up to five years for perpetrators of these crimes.  \n                Crime and punishment \n              The threat of punishment wasn't enough to dissuade Harris \u2014 but the armed patrols and barbed-wire perimeter now installed at HLS did convince him to go after a softer target. Like several activists before him, Harris went after companies that had contracts with HLS (and much less security). The SHAC website had identified several such companies, and for his first foray, Harris chose the Nottingham branch of York Refrigeration, which provided fridge units to HLS. The fact that he had graduated with a PhD that very day was \"just a coincidence\", says Harris. Sensing no repercussions from his acts of anarchy, he resumed his crusade one month later. On 15 January 2006, Harris targeted Atlas Material Testing Solutions, a company based in Bicester, UK, that provided HLS with equipment. He sliced through power cables, squirted superglue into locks and flooded the office. \"The sight of a hosepipe propped up against the front door proved too much to resist,\" says Harris. He pushed the nozzle through the letter box, twisted the tap and destroyed a shipment of computers still in their Royal Mail boxes. Lastly, he spray-painted, \"This company kills puppies\" onto a wall. Later that evening, he broke into Bullimore Plant Hire in Northampton, UK, which was helping HLS to build an extension. He was snipping air valves off the tyres of bulldozers when he heard a distant shout. Soon after, ten police cars arrived on the scene, responding to what officers thought was an attempted burglary at an adjacent electronics warehouse. Harris ran and the police gave chase. \"I knew it was all over, but I didn't want to just give myself up,\" says Harris. Defiant to the last, he buried his hands beneath him when he was slammed to the ground to avoid being cuffed. It took several months of police work to reveal that Harris was not, in fact, a burglar. DNA and minute droplets of copper on the blades of his bolt cutters eventually tied him to the crimes at the three companies, which had caused damages in excess of \u00a328,000 (US$49,000 at the time). Harris insists that these are the only serious crimes he has committed. After his arrest, the police raided his lab and took away his computer. He was subsequently fired. His actions shocked many of the staff at Queen's Medical Centre, but not Dickson. \"Joe's a 'straight-edged vegan' who makes no secret of his opposition to animal testing, doesn't drink alcohol or coffee, and donates most of his money to animal-welfare charities, so although I was surprised it kind of all made sense,\" she says. Harris was sentenced in September 2006 and spent his first month in prison in a maximum-security facility near Milton Keynes, alongside, he says, murderers, rapists and drug dealers. Harris dead-pans that he learnt two valuable lessons in prison: how to stick a poster to a wall using toothpaste, and how to take a compliment. \"A guy came up to me in the shower and told me I had J-Lo's bottom,\" says Harris. \"I thanked him and walked away.\" He shaved off his curly chestnut hair to look tougher in front of the other inmates, but his concerns about being bullied were unfounded. If anything, he was, it seems, universally liked by his companions. When his image appeared on the news one evening his fellow inmates cheered, he says. They also expressed bafflement over his three-year sentence. \"Meeting a member of the National Front who got the same sentence as me for stabbing a black man in the throat, killing him, made me wonder too,\" says Harris. He later appealed his sentence on the grounds that Judge Alexander was a fox hunter and Harris was a first-time offender. He was released in August 2007 after 11 months in prison.  \n                Palpable fear \n              David Jentsch, a neurobiologist at the University of California, Los Angeles, who uses vervet monkeys to study the effect of drug addiction on the brain, doesn't think that the punishments imposed on activists are too severe. In March 2009 his car was firebombed, and in November last year he received a parcel containing HIV-tainted razors \u2014 with an accompanying threat that his throat would be slashed. Whether psychological or physical in nature, the violence is terrifying, he says. \"You feel fear. Real, palpable fear.\" Harris describes the threats that Jentsch has received as horrendous, but justified. \"Vervet monkeys are very intelligent creatures that, like humans, feel psychological fear, so each time David goes into the room they're probably terrified too,\" he says. Harris represents a unique brand of animal-rights activist, according to Upton. \"He is the only example on record of an animal-rights activist actively contributing to the very thing he was trying to wipe out,\" says Upton. Harris also operated alone and had a 9-to-5 job. Most animal-rights activists who have broken the law since 2005, says Upton, have been full-time protesters embedded in campaigns. \"They were 'career' activists, for whom convictions were an occupational hazard,\" he says. Harris's brother Thomas fits this profile better. He dropped out of university after a year, and in 2007 took over running SHAC after several of its leaders were imprisoned. He is currently serving a five-year sentence for intimidating companies linked to HLS. Joseph maintains that he never spoke to his brother about his own escapades and that he acted independently of SHAC. He says that he found the members of the organization 'cliquey'. In some ways, however, Joseph Harris does fit the activist profile: white, middle class and well educated. James Jasper, a sociologist at the City University of New York, studies animal-rights protesters and says that people in this category are more likely to become animal-rights activists because they do not need to fight for their own basic rights, and are therefore at liberty to extend rights to less privileged groups. \"I didn't really think of myself as an animal-rights activist,\" Harris says now. \"I was just doing what I thought was right.\" Harris has tried to resume a normal life since being released on parole. Initially, he could only find a job driving forklift trucks in Newbury, UK. After eighteen months, however, he persuaded a well-known conservation organization to take him on as an unpaid intern. He asked that they not be named for fear that he might be forced out. Today, Harris speaks about his past double-life as though it happened a long time ago. He knows he can never return to biomedical research and regrets that he may never be able to do a PhD in conservation. \"It's hard to imagine that any professor would take on someone with my past,\" he says. But he is also unrepentant. Although he was caught, he views his crimes as a success. All three of the companies that he sabotaged pulled out of their contracts with HLS to prevent further attacks. \"You can't get much more effective than that,\" he says.   See editorial    p.435 .  For a podcast discussion with Joseph Harris, see:    go.nature.com/9ajhgc Shanta Barley is a freelance writer based in London. \n                     Animal-research special \n                   \n                     James Jasper \n                   Reprints and Permissions"},
{"file_id": "471020a", "url": "https://www.nature.com/articles/471020a", "year": 2011, "authors": [{"name": "Helen Pearson"}], "parsed_as_year": "2006_or_before", "body": "In 1946, scientists started tracking thousands of British children born during one cold March week. On their 65th birthday, the study members find themselves more scientifically valuable than ever before. On Tuesday 5 March 1946, Patricia Malvern was born in a small flat in Cheltenham, UK, near the boilers that her dad stoked to warm the building above. She weighed in at 9\u00a0pounds, 2 ounces (4 kilograms).  The next day, David Ward was \"one of the few Catholics born in a Jewish hospital\" opposite Hampton Court, near London. Ward doesn't know exactly what he weighed, although his dad said later that he looked \"like a skinned rabbit\". Throughout the rest of that week, just months after the end of the Second World War, 16,695 babies were born in England, Scotland and Wales. Health visitors carefully recorded the weights of the vast majority on a four-page questionnaire, along with countless other details including the father's occupation, the number of rooms and occupants (including domestics) in the baby's home and whether the baby was legitimate or illegitimate. Over subsequent years, the information files on more than 5,000 of these children thickened, then bulged. Throughout their school years and young adulthood and on into middle age, researchers weighed, measured, prodded, scanned and quizzed the group's bodies and minds in almost every way imaginable.  This week, the group has much to celebrate. They are turning 65, the age at which many in the United Kingdom retire and, as such, a milestone in British life. They will also celebrate being part of the longest-running birth-cohort study in the world. These ordinary men and women are now some of the best-studied people on the planet. And this makes them some of the most scientifically valuable, because it has allowed researchers to track their health and wealth throughout their lives, and to search for factors that could explain their trajectories.  The exercise has revealed some surprises. It has shown that the heaviest babies were most at risk of breast cancer decades later; that children born into lower social classes were more likely to gain weight as adults; that women with higher IQ reached menopause later in life; and that young children who spent more than a week in hospital were more likely to suffer behaviour and education problems later on.  \n                A generation under study \n              All told, the results from the 1946 birth cohort \u2014 now known as the National Survey of Health and Development and run by the Medical Research Council (MRC) \u2014 have filled 8 books and some 600 papers so far. Perhaps more than anything else, the survey has shown that early life matters \u2014 a lot. \"Ultimately, where you get to in early adulthood is strongly influenced by where you come from,\" says Michael Wadsworth, who led the study for nearly 30 years, until 2007.  Children who were born into better socioeconomic circumstances were most likely to do well in school and university, escape heart disease, stay slim, fit and mentally sharp and, so far at least, to survive. (Ward, whose father worked his way up in a Walthamstow-based dry-cleaning business, went on to university and built a career in journalism. Malvern, whose father left home when she was five and who wore third-hand clothes, left school at 16 and \"bitterly regrets\" the fact that her mother couldn't afford to pay tuition for her to train as a teacher.)  Those lessons are arguably more urgent today than they were in 1946 when, caught up in post-war optimism, Britain was introducing major educational reforms and a National Health Service (NHS) to ensure that good schooling and health were available to all. The contrast with the country's mood this winter couldn't be starker. Students have been rioting to protest against the government's plan to introduce \u00a39,000 (US$14,600) annual fees for universities; plans are afoot to drastically reform the NHS (eviscerate it, critics say); and sweeping budget cuts are threatening public services \u2014 including early childhood support centres, for which the cohort's data once helped provide impetus. \"I find these changes very worrying,\" says Diana Kuh, who now directs the survey and says she is saving up for her grandchildren to attend university.  \"It's unique and groundbreaking in the history of epidemiology. It's the only study to have chased an entire cohort across its life course \u2014 and it's not yet finished,\" says Ezra Susser, an epidemiologist who works with cohort studies at Columbia University in New York. He says that cohort research has been vital in seeding the idea that disease evolves as a result of events throughout life. \"You gain enormous depth of understanding in how that disease came to be by following someone over their life course.\"  Now, as the cohort members enter old age, the study offers a precious opportunity to understand how a lifetime of experiences might hasten or slow their decline \u2014 an urgent question for countries such as the United Kingdom and United States, whose populations are rapidly ageing and sickening. In the latest round of data collection, running from 2006 to 2010 and costing \u00a32.7 million, study members underwent almost every modern biomedical test, including echo\u00adcardiograms, measures of blood-vessel function, whole-body bone, muscle and fat scans, and tests of blood, memory and how quickly they could get up from a chair.  The data will provide a detailed starting point from which to measure the cohort members' inevitable decline, and the opportunity to analyse the information is already swelling an extensive network of collaborators. Some are testing how genes interact with a lifetime of experiences to lead to obesity or disease; others plan to scan participants' genomes for 'epigenetic' marks \u2014 molecular traces left, perhaps, by early birth weight or by life's inequalities \u2014 that alter gene expression and might provide a molecular explanation for effects in later life. Greg Duncan, an economist at the University of California, Irvine, who studies the impact of child poverty, hopes that follow-up studies could help to answer a question arising from the earlier findings on socioeconomic status and health: \"What are the active ingredients in social class?\" It is this ability to draw associations between biological data, from blood pressure right down to genes, and life as it is actually lived that makes the cohort study so unusual, say its leaders. \"These are real people,\" says Kuh. \"This is what it is to be human and normal.\"  \n                Next steps in making motherhood easier \n              The first few decades of the twentieth century found Britain acutely concerned about its falling birth rate and stagnant infant mortality. (The thought at the time, as Kuh puts it, was \"how are we going to maintain Britain and its empire?\") A Population Investigation Committee recommended a maternity survey to explore whether the social and economic costs of childbearing were discouraging prospective parents. James Douglas was appointed to head it.  Douglas, a physician, had spent part of the war conducting vast studies of air-raid casualties. He set about launching an investigation that today would be ethically difficult, logistically nightmarish and financially prohibitive: sending health visitors to interview the mothers of every child born in that March week. He reached 13,687 of them. \"It was crazily ambitious,\" says Wadsworth, who inherited the study leadership from Douglas more than three decades later. Yet \"he pulled it off\". In 1948, when Douglas's book about the study's results appeared, the baby boom was in full swing and concerns about birth rate had mostly dissipated. But the volume,  Maternity in Great Britain , made a stir by revealing shocking disparities between rich and poor in infant survival and women's care. One widely reported result showing that only 20% of women who gave birth at home were offered pain relief, and that the poor suffered most, spurred a parliamentary bill allowing more midwives to deliver gas and air.  Douglas decided to turn the study into a tool for documenting social inequality and gauging the impact of newly minted welfare reforms such as the NHS. In particular, he realized that he had the perfect weapon for testing the success of the 1944 Education Act, which had introduced a nationwide system of exams for 11-year-olds \u2014 the 11+ \u2014 intended to channel the brightest, regardless of background, into elite 'grammar' schools. He selected a sample of the original 13,687 children spanning geography and social class, ending up with 5,362, whose health, growth and other data were regularly recorded and then transferred onto punch cards. Douglas also tested the children's cognition as they reached 8, 11 and 15, and tracked their course through school.  \n                Britain's squandered treasury of talent \n              To the architects of the welfare state, the results were discouraging. Bright children from the middle classes were more likely to pass the 11+ and do well at school than were equally bright working-class children, although supportive parents and good teachers could better a child's odds. The attrition of smart but poor boys (girls counted for less) became known as the 'waste of talent', turning Douglas's next two books \u2014  The Home and the School   (1964) and  All Our Future   (1968) \u2014 into must-read educational references and contributing to the introduction of non-selective 'comprehensive' schools in the 1960s.  While Douglas was studying the group's diverging paths, the children were walking them. Malvern, who was cripplingly embarrassed by taking free school meals, failed her 11+. She blames a class teacher so violent that Malvern would sleep without covers in order to catch a cold and avoid school, and who \"walloped me across the head\" on the day of the exam. After she left school, Malvern went to learn typing at Government Communications Headquarters in Cheltenham. Ward's father, meanwhile, was planning to buy a house, and his mother tested him on Latin vocabulary over the ironing. He was one of 4 children out of 66 in his school's top two classes who passed the 11+ exam, and he and his sister were the first in their family to attend university.  As the 1970s rolled on and the participants entered their thirties, Douglas was losing steam. Most of his questions about the cohort members' education, occupations and social mobility had been answered, and Douglas was heading towards retirement. Medical epidemiologists thought that the cohort should be mothballed until its members got interesting again, when they started to sicken and die. The MRC, which had been funding the project since 1962, dithered about what to do with it; even Douglas thought the project was finished.  \n                Life's pattern decided \u2014 at the age of seven \n              For Wadsworth, a social epidemiologist who had joined Douglas's team in 1968, it was just getting going. \"I thought the changing pattern of health of these people would be interesting over life,\" he says.  After he took the helm in 1979, Wadsworth convinced the MRC to fund a new round of data collection as the cohort reached 36, then again at 43 and 53. He started assessing the group's physical capabilities and health, including blood pressure, heart and lung function, diet and exercise. He wanted to see how these indicators had been influenced by earlier life \u2014 and then chart them into the future.  Correlations tumbled out of the data. In 1985, Wadsworth and his team reported that cohort members whose birth weight had been low had higher blood pressure as adults 1 . It was an early hint that fetal and infant growth shape adult health, a link that became known as the Barker hypothesis after David Barker, an epidemiologist at the University of Southampton, UK, who published a 1989 analysis of birth weight and health in a different cohort 2 . He found that babies with the lowest birth weights had the highest risk of heart disease as adults.  Study after study from the 1946 cohort supported the link, showing a tangle of connections between infant and child growth or development and adult traits from cognitive ability to frailty, diabetes, obesity, cancer and schizophrenia risk. \"It isn't the same story every time, but we find an endless stream of long-term associations in quite 'noisy' data,\" says Kuh. \"Big babies were more likely to get breast cancer. Small babies were more likely to have poor grip strength. Those who grew fast postnatally have more cardiovascular risk.\" (Says Ward: \"I find that quite extraordinary, almost in a poetic way, that there is something that spans all those years, that something was set down, determined at that stage.\") A major question for scientists today is how to explain these connections: which biological systems in infants are so important, and how are lasting scars laid down on them? One possible answer lies in epigenetics: the chemical footprints, such as methyl groups, stamped on DNA by early life events that alter gene-expression patterns and might contribute to later disease. Martin Widschwendter, an oncologist at University College London (UCL), for example, is planning to analyse tens of thousands of possible methylation sites in the cohort's DNA, looking for changes that could explain the link between birth weight and breast-cancer risk. The detailed life-course information that can be combined with the DNA \"is really only available via these cohorts\", says Widschwendter.  \n                The doctor's son does better than a dustman's \n              Yet Kuh and others emphasize that fates are not fixed by early life. \"I don't ever want the findings to be interpreted as purely deterministic,\" says Kuh; she prefers the more optimistic idea that disease risks result from an accumulation of experiences throughout life, and that education, diet or other factors can shift poor trajectories to better ones. Marcus Richards, an epidemiologist who is leading the cognition studies on the group, points to evidence from the 1946 cohort \u2014 and supported by many other studies \u2014 that regular physical exercise in a person's thirties and forties can slow their cognitive decline with age. \"We can take that research and say, here is very clear-cut evidence of something you can do to protect your cognitive health as you get older, and this is how you should do it,\" says Richards. The 1980s brought a vivid lesson in the power of environment. Hardly any of the Douglas babies, nourished on post-war rations, were fat as children \u2014 a sharp contrast to those of today \u2014 and they had maintained a healthy weight throughout young adulthood. But now incomes were climbing, eating out was more affordable, and cars were the way to get around. As the cohort approached their thirties, the line plotting the proportion who were obese edged upwards; in their late thirties it soared 3 . And although those in lower socioeconomic brackets did get fatter faster, no social class was immune.  Somewhere on one of those curves is Malvern, who found her own weight creeping up when she moved to Luxembourg in 1992 and stopped work as a school bursar. She weighed 11 \u00bd stone (73\u00a0kilograms) when she moved. \"When I came back in 2000 I was horrified: I was 15 stone. It was the p\u00e2t\u00e9 and the baguettes and the cheese and having visitors,\" she thinks \u2014 on top of the menopause. Malvern has since lost weight, and Ward has kept himself trim, he says, by living in the Peak District, where \"you can't get anywhere without going up and down a hill\".  \n                Cleverness 'delays the menopause' \n              As women in the study reached their fifties, a more mysterious pattern emerged: those who had performed well on childhood intelligence tests tended to reach menopause several years later than those who had performed poorly 4 . \"We tested almost to destruction every social and behavioural pathway; we threw almost everything we had at that to see if we could make that association go away and it didn't,\" says Richards. But once the researchers considered the association, it began to make sense. Their theory now is that childhood cognition provides a readout of brain development, including that of some areas that respond to hormones or are responsible for hormone production. In short, high IQ scores could indicate a brain that was well-developed all round, and so was able to sustain reproduction for longer. Kuh says that she has been testing whether genes are responsible, \"so far without success\".  In 2005, as the cohort neared 60 and Wadsworth neared the end of his scientific career, the project's future was again in jeopardy. The MRC was pondering whether to keep paying for it and, if it did, who should lead it. \"We didn't know if the study would be closed down \u2014 and Mike was retiring. It was a very unstable period,\" says Kuh.  Kuh \u2014 who had trained in economics \u2014 wanted to build up the biomedical data that Wadsworth had been collecting. Until that time, all the examinations had been performed at the study members' homes, but by this stage the nurses were staggering under all the equipment. To really understand the participants' physiology and biology, Kuh argued, the study needed to get them to a clinic. \"People appreciate a free bone scan,\" she says. By 2008 she had convinced the MRC to pay for every willing cohort member to visit one of a number of clinics around the country and had established a dedicated research unit, now housed in a Georgian terrace in central London.  Ward went to a clinic in Manchester for his exam. He learned that he has signs of osteoporosis in his spine, and that he can no longer stand on one leg for long with his eyes closed. \"You wobble rather more and I ended up hopping about the place.\" He recalls the food diary he had to prepare as a \"serious challenge\". \"You don't want to admit that you had that extra glass of plonk or another slice of cake, but you say, hang on, this is science, I've got to tell the truth\". Kuh and her colleagues \u2014 the study now has about 25 full-time researchers and support staff and 100 collaborators \u2014 are still compiling such truths about their thousands of participants. \"Now the cohort is one of the most phenotyped in the world,\" says Kuh. Once her paper summarizing the latest data goes public 5 , Kuh is expecting the queue of epidemiologists, geneticists and other scientists who want to collaborate to lengthen, and last November she hired someone for three years especially to cope with the increased data sharing. As the cohort ages and falls ill, the study will continue monitoring participants' health and trying to tease out the influence of early experience. \"One big question we can ask is, are these life effects we see in mid life going to wane?\" says Kuh. Or will they, as some epidemiologists expect, get more dramatic with age?  Kuh is also thinking about how best to exploit genomic and other biomedical analyses. At least one study has hinted at the power of the cohort's life-course data combined with genetics. Last year, Rebecca Hardy, a statistician with the survey, published a study of two hot genes called  FTO   and  MC4R , variants of which have been identified as risk factors for obesity 6 . When she analysed DNA collected from the cohort in 1999, she found that the association of those variants with body mass index increased in early adult life, then weakened as the cohort grew older. Perhaps, Hardy speculates, any effects of the genes on appetite or fat storage were overwhelmed by that onslaught of fat-promoting influences in the 1980s, a possibility that might become clearer when she tests a further panel of obesity-linked genes.  Ever protective of her study members and the limited DNA samples she has, Kuh says that she views the latest molecular biology techniques with caution. \"I feel a huge responsibility to deliver,\" she says. Quite often, she says, outside researchers have an attitude of \"give us all the cohort data and we'll rush this through and find millions of associations. I say, well, that sounds very interesting; can you come back with a hypothesis?\" Even so, when Kuh compiles a plan for the MRC's five-yearly review of the survey in 2012, she knows that working out how to incorporate these technologies \"is going to be key\". The falling cost of DNA sequencing means that ploughing through participants' entire genomes is an almost inevitable step, she acknowledges. \"The questions are, when is the best time \u2014 and what would we learn from it?\"  \n                A survey taking on a life of its own \n              For now, Kuh has more immediate planning concerns: five 65th-birthday parties, at which the study members will meet each other for the first time (see  'Cards & calls' ). The parties are causing her some anxiety. Wadsworth had considered and rejected the idea of a 50th- or 60th-birthday bash, in case the get-together ended up influencing the participants' life course in some way. \"Basically, we thought people might leave their partners and get off with someone in the study,\" he says. But Kuh decided that recognizing and rewarding the members was worth the risk. (She even wrote to Buckingham Palace to request a garden-party invitation for the study members. \"I wrote such a nice letter. I learned all about how to address the Queen, and I'm still hoping to get a reply.\")  Ward and Malvern are pleased to have been part of the study. \"It gives me a fair old bit of pride in a way,\" says Ward. \"Just things like bed-wetting. What did I contribute to the nation's store of knowledge on bed-wetting?\" Neither is perturbed by the idea of the researchers watching them until they crumble and die. \"I suppose,\" says Ward, \"it helps you accept that you're mortal, you're not going to last forever.\"  Some 13% of subjects have died so far \u2014 and the study already has something to say about the fate of the rest. Kuh flips open some graphs of survival rates that she has calculated. They show the proportion of the survey members surviving up to age 60, separated by father's social class. And they reveal yet another curious correlation for Kuh and her colleagues to dig into. Kuh points out a blue line representing a group of women from better-off backgrounds, whose death rate is about half that of everyone else 7 . Kuh has not been able to attribute the effect to less smoking or other obvious factors, and she suspects that these women took advantage of the educational and health opportunities afforded by post-war Britain to improve themselves. \"They really changed their lives with education. The girls, if they got through, they did really well.\"  Yet the study is lending a touch of immortality to all its participants, whether men and women, born into comfort or poverty. Traces of them will live on in preserved DNA, cell lines frozen in liquid nitrogen \u2014 and in their records, now all transferred from punch cards to computers. \"You're very aware that your memory is going,\" says Ward. \"But you also know that in the archive is a version of you.\" \"I often call it an alternative biography in there,\" he adds, \"and that I'd quite like to get my hands on.\"  \n                     Ageing Insight \n                   \n                     Developmental plasticity and human health \n                   \n                     The MRC National Study of Health and Development \n                   Reprints and Permissions"},
{"file_id": "469282a", "url": "https://www.nature.com/articles/469282a", "year": 2011, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "For scientists, collisions at the world's most powerful particle collider are just the start.  Nature   follows the torrent of data on its circuitous journey around the world. \n               ATLAS particle detector, Switzerland, 30 March 2010, 13:06 local time  \n             Beneath gently rolling hills between the mountains of Switzerland and France, the world's greatest physics experiment starts its first real run. Two beams of high-energy protons meet head-on at almost the speed of light inside the Large Hadron Collider (LHC), a giant particle accelerator at CERN, Europe's high-energy physics lab. Nanoseconds after the protons crash together, their combined energy gives birth to heavier particles, which decay in an instant into a splatter of lighter debris. At the collision point, 92 metres underground, the 7,000-tonne ATLAS detector sees everything. The debris particles pass first through the detector's inner tracker \u2014 a sophisticated layer of silicon electronics that records their paths. Beyond that lie systems that measure the energies of the particles. Some drag to a stop there, but heavy cousins of electrons called muons barrel along, flying metres from the collision point before being picked up by giant, mustard-coloured sensors. Microprocessors convert the particles' paths and energies into electronic signals, and select a handful of promising collisions for a closer look. The data from the chosen collisions zip upstairs to a computer farm that discards the majority and creates a digital reconstruction of those that remain. Even after rejecting 199,999 of every 200,000 collisions, the detector churns out 19 gigabytes of data in the first minute. In total, ATLAS and the three other main detectors at the LHC produced 13 petabytes (13 \u00d7 10 15  bytes) of data in 2010, which would fill a stack of CDs around 14 kilometres high. That rate outstrips any other scientific effort going on today, even in data-rich fields such as genomics and climate science (see   Nature  455, 16\u201321; 2008 ). And the analyses are more complex too. Particle physicists must study millions of collisions at once to find the signals buried in them \u2014 information on dark matter, extra dimensions and new particles that could plug holes in current models of the Universe. Their primary quarry is the Higgs boson, a particle thought to have a central role in determining the mass of all other known particles. \n               Click here for larger image \n               The architects of the LHC decided in 2001 to deal with all that data by dividing and conquering. The results from the giant particle detectors get parcelled up and sent to a vast global network known as the Worldwide LHC Computing Grid, the most sophisticated data-taking and analysis system ever built. The network is as great a technological leap as the collider itself, and without it the project would quickly drown in its own data. The Grid consists of some 200,000 processing cores and 150 petabytes of disk space, distributed across 34 countries through leased data lines (see 'March of the data'). By combining these resources, the Grid enables scientists to run vast analyses that would push the world's most powerful supercomputers to the edge. \n               CERN computing centre, 30 March 2010  \n             Within minutes, the first collisions have made their way to a 1970s-era concrete building on the other side of CERN's campus. In a white, high-ceilinged room, racks containing 50,000 computing cores undertake a careful reconstruction of every selected collision. Details of each sub-detector's calibration, along with temperature readings and other environmental data from the cavern where ATLAS is housed, are used to piece each event back together. ATLAS scientists at CERN pull up reconstructions showing starbursts of narrow lines spreading from the collision points. In Grid terminology, the CERN computing centre is known as Tier 0. It undertakes an initial analysis of the data and stores one copy. The physics data from ATLAS on the first day of the March run total about 5.2 terabytes (5.2 \u00d7 10 12  bytes), enough to fill around ten laptop computers, or five of the digital storage tapes kept on the floor below the rows of processors. The first day's harvest is modest compared with what will follow, but the ATLAS experiment has more than a thousand collaborators waiting for results. If all of them logged into CERN and attempted to pull the data from the first collisions back to their home institutions, the network would grind to a halt. So instead, the Grid automatically spreads copies of the data geographically. Inside a small partitioned section of the computing centre, a wall of panels bristles with bright-orange fibre-optic cable. This is the heart of the system, and it routes data to sites across the globe at a blistering rate of 5 gigabytes per second. \n               Oxfordshire, UK, 30 March 2010  \n             After CERN finishes the initial analysis, a dedicated fibre-optic link carries some of the data from the first round of collisions more than 800 kilometres to the Rutherford Appleton Laboratory, a sprawling research park nestled among muddy fields in rural Oxfordshire. Here, in a modern office building, a computing farm receives the data through a yellow cable only slightly thicker than a phone line. The lab is one of 11 Tier 1 centres spread around the world, where the data are further refined and split. Particle physics is a bit like investigating a mid-air collision. Nobody is there to witness it; instead, the debris is painstakingly collected and reassembled to give investigators hints as to what happened. In this case, physicists divide up the different kinds of particles for study. One group looks at muons, for example, while another focuses on high-energy \u03b3-rays. The computers at the lab help by creating dozens of copies of the data, focusing on various aspects of the collision. They are given names like data10 7TeV.00152166.physics MinBias.merge.DESD_PHOJET. \u2217  \u2014 which contains data on photons and narrow jets of particles. \n               Chicago, Illinois, 15 May 2010  \n             A team of US researchers sends a request for data out on the Grid, and information on several subsets of the collisions from 30 March travels from Oxfordshire via New York to a post-war University of Chicago building just two blocks from the site of the Manhattan Project's first nuclear reactor. Rob Gardner, the physicist in charge of the computing facility, says, \"What we've assembled here is a data centre just about as cheaply as we can put silicon on the floor.\" It looks like a smaller version of the computing centres in Geneva and Oxfordshire, but with one importance difference: researchers can bring coffee into the Chicago site. \"It's not a clean environment,\" says Gardner. His cluster of computers is one of the Grid's 140 Tier 2 sites. Unlike Tier 1s, which undertake serious reconstructions of the data, Tier 2 centres mainly provide storage and computing resources and can be accessed by users all over the world. In an office above the cluster, postdoc Antonio Boveia sits at a metal desk with his laptop. His machine is at the far end of the Grid from CERN, with lines of code scrolling against the black screen. To conduct an analysis \u2014 such as one on the decay of the Higgs boson into heavy particles known as W-bosons \u2014 he types in commands in the common programming language C++. For just one of Boveia's analyses, he must study tens of millions of collisions. Even if his laptop's hard drive were 4,000 times its current size and could accommodate the data, his processor would still take a few years to complete the work. \"It would be impossible,\" he says. The Grid makes it possible by splitting the task. When Boveia enters his request, the Grid pulls data from sites such as the one in Oxfordshire, then parcels the analysis into thousands of separate pieces and spreads it across the network. The pieces might be processed at CERN, or at a facility in Italy, or, more likely, in many places at once. In a matter of days, Boveia receives an e-mail alert telling him that the analysis is complete. The operation does not always work so smoothly. The Tier 1 and Tier 2 centres are managed locally, which means that they each have their own protocols \u2014 and problems. In the summer of 2009, as simulated data was flowing through the Grid in advance of the first real collisions, fluff from local cottonwood trees clogged the Chicago centre's air-conditioning unit and forced a shutdown. The same year, road workers severed one of CERN's fibre-optic links in Switzerland, and a fire brought down the Tier 1 centre in Taipei, Taiwan, for months. When things go wrong, alerts are dispatched by e-mail or, occasionally, by phone to an assortment of emergency contacts around the globe. The system relies on goodwill, says Jamie Shiers, a group leader in CERN's computing department. \"We have no line management over these people whatsoever,\" he says. But somehow, the global cooperative produces results. \n               CERN, 24 December 2010, 11:54  \n             The ATLAS team posts an initial analysis from the Chicago group onto the pre-print server arXiv.org (ATLAS Collaboration. Preprint at  http://arxiv.org/abs/1012.5382;  2010). The report \u2014 on W-bosons produced through mechanisms other than the decay of Higgs bosons \u2014 includes collisions from the first day's run, along with many others. Measurements of the W-bosons produced show good agreement with existing theories. The physics data set from 30 March now makes up just 0.02% of the total data collected by the ATLAS detector. Most physicists on the collaboration are using that initial set without even realizing, as they acquire sections for analysis and combine them with other data sets. The first hints of a Higgs boson may already be stored on a computer disk in Mumbai, Melbourne or one of the many other sites to which LHC data are distributed. But even if it is there, the Higgs will stay hidden until many more petabytes have flowed through the Grid.  Geoff Brumfiel is a senior reporter for Nature based in London. \n                     Nature News LHC Special \n                   \n                     Travelling the petabyte highway is harder than it looks \n                   \n                     Watch the Grid live \n                   \n                     Dave the data set \n                   \n                     CERN \n                   Reprints and Permissions"},
{"file_id": "469462a", "url": "https://www.nature.com/articles/469462a", "year": 2011, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "Researchers in Panama suffered under a dictatorship and were overshadowed by the United States. Now the country is attempting a scientific renaissance. When Carmenza Spadafora Mej\u00eda left Panama in 1997, she walked away from a bloody history. In 1985, her brother Hugo had been decapitated by henchmen of the military dictator Manuel Noriega, after Hugo publicly denounced him. For years, Spadafora and her family fought to identify the killers. But the brutal murder helped to spark an uprising that culminated in a US invasion and the imprisonment of Noriega in 1990. After her family won convictions against some of those responsible for the murder, Spadafora felt ready to leave to start a PhD in Spain. Since 2008, when Spadafora returned to Panama, life for her and her country has been on the up. Now with a US postdoctoral position in infectious diseases under her belt, Spadafora coordinates a cellular and molecular biology centre at INDICASAT-AIP (the Institute of Scientific Advances and High-Technology Services) in Panama City. Last June, she and her colleagues revealed a long-sought receptor that the malaria parasite  Plasmodium falciparum   uses to invade red blood cells 1 , a discovery that could help development of vaccines. A few months later, they won a US$1-million grant from the Bill & Melinda Gates Foundation in Seattle, Washington, for a scheme that Spadafora concedes may sound outlandish: developing a full-body microwave scanner to cure malaria by killing the parasite. With developments such as these, \"we are showing we can compete with anyone worldwide\", she says. And that is exactly what the Panamanian government wants to do. The country is one of many in Central America that have struggled under dictatorship and political turmoil in the past few decades. Now it is the first to make a significant investment in science part of its recovery, thanks mainly to its past two democratically elected governments, which have seen research as a route to economic growth. \"Science and technology are key components for competitiveness,\" says Rub\u00e9n Berrocal Timmons, Panama's science secretary, adding that he wants to make Panama \"an international scientific hub\". To that end, the country's leaders are intent on increasing investment in science and technology from less than 0.2% of the gross domestic product, as it was in the mid-2000s, to 0.6% by 2014. The investment is already in evidence. In March, ground is to be broken on a $20-million science and technology innovation park near Panama City; construction of a $5-million vivarium for the country's research animals is planned for this spring; the government is funding about 100 Panamanians to undertake doctoral studies at universities abroad, with incentives to return to Panama for research careers; and the first complete in-country PhD research programme \u2014 in biotechnology \u2014 has just begun at INDICASAT-AIP. Observers say that the country's efforts at a scientific renaissance could even serve as a model for other nations seeking new life after conflicts. The change in Panamanian science amazes some, including Ira Rubinoff, who led the US Smithsonian Tropical Research Institute in Panama for 34 years until 2008. \"If you told me this would happen 25 years ago, I would say you were smoking something illicit,\" he says.  \n                Building an identity \n              Panama is inextricably associated with its canal, which bridges the Atlantic and Pacific oceans. It was built at the start of the twentieth century by the US Army Corps of Engineers, which had to battle malaria and yellow fever to complete its task (see  'A century in Central America' ). Ever since, Panama's research community has lived in the shadow of the United States, which \u2014 in governing the canal \u2014 virtually militarily occupied the nation until 1999, when Panama won full control of the waterway under a controversial 1977 US treaty. During much of the twentieth century, Panama was a US laboratory of sorts for secret military research on mustard and nerve gas, dioxin and depleted-uranium weapons. But after the Second World War, the US presence also allowed the blossoming of the Smithsonian Tropical Research Institute, a branch of the Smithsonian Institution in Washington DC and now a world-class ecological research facility. Panama's push to build its own science began with former President Mart\u00edn Torrijos Espino, who took power in 2004 and saw science as a way to forge a new era independent of the United States. He named Julio Escobar Villarrue, a computer scientist trained at Massachusetts Institute of Technology in Cambridge, as science minister. Escobar's team initiated a grant programme that awarded at least $15 million during his 5-year term, provided scholarships for several hundred graduate students to study abroad, and established a system that rewards well-published researchers with stipends worth $1,000\u2013$2,000 per month. \"He turned things around,\" says Rubinoff. Spadafora agrees. \"He is awesome,\" she says. \"He called me to say we are building a new enterprise, please come back. She did \u2014 and now can be found in her modern lab in the 'City of Knowledge', a former US military base at the Pacific canal entrance that the government is converting into a research centre, housing INDICASAT-AIP and other new agencies. The historic military buildings have been gutted and turned into labs, offices and a conference centre. The new science park and vivarium will be located there. Spadafora bubbles with excitement when talking about her 'ray-gun' idea for curing malaria, devised during a brainstorming session with fellow Panamanian Jos\u00e9 Stoute Zuriea, a physician at Pennsylvania State University in Hershey.  P. falciparum   invades red blood cells and digests their haemoglobin, then sequesters the iron-rich, toxic remnants in a crystal form called haemozoin. Their idea was to heat these crystals with low-frequency microwaves, fatally releasing their contents. After the pair won an initial $100,000 grant from the Gates Foundation in 2009, they demonstrated that the concept would work. They exposed infected red blood cells from mice to 2.45-gigahertz microwaves for four minutes, and showed that the crystals dissolved, killing  P. falciparum   but not the host cells. Now, armed with the new $1-million Gates grant, the researchers plan to create a mouse-sized microwave device to radiate infected animals. Meanwhile, the science drive started by the Torrijos administration continued with the 2009 election of President Ricardo Martinelli Berrocal and the appointment of Rub\u00e9n Berrocal Timmons, his cousin. The Martinelli administration also appointed a new head of INDICASAT-AIP: Jagannatha Rao, a neuroscientist who arrived in summer 2010 from a national lab at India's Council of Scientific and Industrial Research in New Delhi. In Rao, Panama has found a tireless advocate for his adopted nation's scientific community. Last year, Berrocal and Rao visited Singapore to learn about the country's major science push of recent years. Rao is now initiating a steady stream of conferences featuring international participants, and he has been adding programmes to INDICASAT-AIP's portfolio, including the country's new biotechnology PhD. \"Students must have three publications before they can graduate,\" says Rao. One of Berrocal's aims is to build up modern labs and recruit high-quality scientists, creating a thriving research atmosphere that can avert a brain drain of new doctorates. \"We don't want to lose these people,\" he says. Paul Collier, an economist who studies capacity building at the University of Oxford, UK, says that Panama's science drive \"sounds very positive\". The challenge, he says, is \"to concentrate resources in a niche \u2014 then be the quality escalator in that niche\". Panama wants its niche to be biotechnology, with a focus on infectious diseases and bioprospecting \u2014 the search for drugs developed from its own rich natural resources. (Its neighbour, Costa Rica, has taken a similar tack 2 .) Rao is organizing a drug-discovery meeting in May, which will include discussions of Panama's plans for a Bioprospecting Natural Product Bank, a resource for screening that already includes 8,000 samples of marine bacteria and fungi. The drive in biomedicine is evident across Panama City. At Hospital Santo Tom\u00e1s in the city's centre, researchers aim to collaborate with pharmaceutical firms on clinical trials, says Juan Miguel Pascale Bellagamba, a physician who directs programmes at the Gorgas Memorial Institute of Health Studies and Columbus University of Medicine and Science. Pascale is one of several researchers who also want to boost the country's ability to track and respond to outbreaks of infectious disease or to other public-health concerns. If there is one thing that might slow down these plans, Panamanians say, it is a lingering resistance to recruiting researchers born outside the country. Few seem concerned that the research investment could dry up; as long as ships continue to traverse the Panama Canal, the country's major revenue source will flow. A $5.25-billion project for larger locks on the canal, which will markedly increase its capacity, is set for completion in 2014. (Excavations for new locks proved a boon for palaeontologists, who unearthed fossils of camels, horses, rhinos and pigs 3  that are helping them to understand animal migrations between the North and South American continents.) Looking back at the Noriega era, Spadafora and Stoute both recall with sadness the country's lost opportunities. After her brother's death, Spadafora chained herself to the Vatican embassy in protest. Stoute, too, was traumatized. \"I lost many friends,\" he says. Things are better now, they agree, even if many outside the country are not yet aware of it. \"Our US colleagues have told us we're pretty wild down here,\" says Spadafora. But if Panama can emerge from the depths of tyranny, maybe a wild idea to fight malaria can emerge here too. Rex Dalton is a writer based in San Diego, California. \n                     Smithsonian Tropical Research Institute \n                   \n                     Grand Challenges in Global Health \n                   \n                     INDICASAT-AIP \n                   Reprints and Permissions"},
{"file_id": "469460a", "url": "https://www.nature.com/articles/469460a", "year": 2011, "authors": [{"name": "Jane Qiu"}], "parsed_as_year": "2006_or_before", "body": "After years of struggle on behalf of ocean science, Wang Pinxian is taking a key role in China's plans to expand marine research. Wang Pinxian began his scientific career in the shabbiest of conditions, spending long winters in an abandoned Shanghai workshop with no heat. He washed and screened samples of sea-floor sediment in a rice bowl, and struggled with a microscope that would barely focus. The most important book on his shelf was a Russian encyclopaedia of palaeontology, which helped him to identify microfossils in the samples pulled up from off the coast of China. \"It's an extraordinary way to start a career in oceanography,\" says Wang, laughing as he recalls those days half a century ago. He fell in love with the tiny creatures that served as windows to Earth's distant past, and he dreamed of using the fossils to help to develop the scientific capacity of his country. Wang, now a marine geologist at Tongji University in Shanghai, slowly rose up the ranks. He eventually earned a position in the Chinese Academy of Sciences and served as a member of the Chinese national legislature. For decades, the 74-year-old, outspoken scientist has used his position to lobby Chinese leaders to devote more resources to marine science, but those arguments have fallen on deaf ears \u2014 until recently. With China facing an increasing need for energy and minerals, it is now taking an interest in the deep sea. In its next five-year budget, which will be announced in March, the country will boost funding for oceanography, particularly in exploration, research and deep-sea technologies. That rising tide has also lifted Wang's fortunes. Last July, he was awarded a US$22-million grant from China's National Natural Science Foundation to lead studies into the geology and biology of the South China Sea. The project starts this week with an inaugural meeting in Shanghai. \n               Click here for larger image \n               \"The South China Sea is a haven for oceanographers and climate researchers,\" says Wang. Sitting between Asia and the Pacific Ocean (see map), the sea is a crossroads for currents that influence the climate of the entire globe. It also helps to control the Asian monsoon system, which feeds the water supplies for billions of people on the continent. Wang's project aims to uncover information about the prehistoric climate and investigate how the ocean basin formed. At the same time, it will study the microbial community in the deep sea, which has an important role in the cycling of carbon between long-term storage in sediments and its release into the ocean and atmosphere. Born in 1936 in Shanghai, Wang grew up during a tumultuous period in Chinese history \u2014 living first through the Sino\u2013Japanese War and then the Chinese Civil War from 1945 to 1949. After the Communist Party founded the People's Republic of China, the young government began to send promising students to Russian universities, and Wang earned the opportunity in 1955 to attend Moscow State University. There he studied geology, which China regarded as a priority because of its practical value in finding mineral resources and oil. The experience had an enduring effect on Wang. He excelled in Russian and thrived under the scientific training in Moscow, where he was fairly safe from the severe economic and social turmoil sweeping his home country. But when Wang returned to China in 1960, his tendency to speak his mind drew the ire of officials who publicly rebuked him. He voiced concerns about the widespread famine in China at a time when the economic policy known as the Great Leap Forward was widely portrayed as a great success. \"These things just didn't make sense to me at all,\" he says. Because of the extreme economic hardships at the time, Wang's training initially seemed irrelevant in China, especially during the Cultural Revolution \u2014 a violent political and social movement that began in 1966. But China was eager to find fossil-fuel reserves, so Wang was called up by the government in 1972 to analyse calcareous microfossils in marine samples, in the hope of identifying petroleum deposits. Although he had access to only basic equipment in his Shanghai workshop, Wang toiled away and eventually published the book  Marine Micropaleontology of China , first released in Chinese in 1980 and then in English 1 . Wang co-authored all of its 17 papers, and developed an international reputation for his scholarship. The book helped to connect Chinese oceanography to research elsewhere \u2014 at a time when little was known about marine geological science in China.  \n                Deep concerns \n              The work also attracted international interest in Chinese marine regions, and Wang helped to convince the international effort known as the Ocean Drilling Program (ODP) to conduct the first deep-sea drilling expedition in the South China Sea. Wang co-led the 1999 effort, known as ODP Leg 184, which drilled 17 holes on the southern and northern continental slopes of the sea to explore the history of the east Asian monsoon. The South China Sea occupies a crucial position between the world's highest mountains in the Himalayas and the deepest spot on the Earth's surface, the Mariana Trench in the western Pacific Ocean. Erosion of those nearby mountains causes sediments to accumulate rapidly on the sea floor, providing a detailed record of the regional climate over the past 45 million years, during which time India has collided with Asia and raised the Himalayas. \"The South China Sea promises some of the most fascinating geological records on Earth,\" says Carlo Laj, a palaeoceanographer at the Laboratory of Climate and Environmental Sciences in Gif sur Yvette, France. Laj met Wang during the ODP expedition and the two men have collaborated in subsequent French\u2013Chinese palaeoceanography cruises in the South China Sea.  \n                Eye on the tropics \n             The ODP expedition was a turning point in Wang's research career. On the basis of the South China Sea records, he and his colleagues found that the chemistry of the region has gone through significant changes in the past 1.6 million years. The sea-floor cores contain fossilized plankton shells that can be used to measure the ratio of the isotopes carbon-13 to carbon-12 in ancient sea water. This can be used to deduce information about different reservoirs of carbon, including atmospheric carbon dioxide and the organic matter from marine organisms. Wang found that the carbon ratio fluctuated in step with variations in Earth's orbit \u2014 such as the eccentricity of its route around the Sun. And the ratio peaked before two expansions of polar ice sheets, suggesting a possible connection 2 . These orbital cycles are the pacemakers of Earth's climate and are thought to trigger ice ages by reducing the amount of sunlight reaching high northern latitudes during summer. But Wang's work helped to focus attention on the tropics, and raised the possibility that orbital cycles could cool the planet by altering processes in the low-latitudes \u2014 such as the weathering of rocks \u2014 that in turn cause major changes in the carbon system. He also conducted research on the East Asian monsoon and its broader connections. He and other researchers have found the fingerprint of orbital cycles within monsoon records from the South China Sea and many other parts of the globe 3 . Instead of regarding monsoons as separate regional phenomena, Wang and others embrace a 'global monsoon' concept, which refers to a large-scale churning of the atmosphere throughout the tropics and subtropics. In studies of the palaeoclimate, \"Pinxian was one of the first to put monsoon in a global context\", says Laj. \"That was extremely original and insightful.\" Wang's work helped to earn him the Milutin Milankovic Medal for long-term climatic research from the European Geosciences Union in 2007. With the influx of funding and interest from the Chinese government, Wang has big plans for the future. Beyond the eight-year South China Sea project, his team is working with the government to establish a sea-floor observatory off the coast of Xiaoqu Shan, an island southeast of Shanghai. The observatory will record important features of the ocean, including temperature, salinity and sedimentation rates. Wang would ultimately like to build a network of ocean-floor observatories in the South China Sea similar to those off the coasts of the United States and Canada. \"This is the only way we can truly understand the oceans,\" he says. Looking back, Wang is surprised that he has managed to thrive under a political regime that usually deals harshly with dissent. Recently, he has openly criticized some entrenched powers in Chinese society. Three years ago, he complained that the election of Chinese Academy members was not based entirely on academic achievements, and that members enjoyed disproportionate authority and privileges. Wang's remarks sent shock waves across China's scientific community; even some of his good friends turned against him. And Wang has not limited his critiques to scientists. He was a member of the National People's Congress \u2014 China's top legislature \u2014 from 1986 to 1992, and has questioned the procedures through which some political decisions are made. Although he has enjoyed a fruitful career, Wang is concerned about the future of science in a society that has endured so many political upheavals. He says that excessive commercialization and a deficit of moral values have led to rampant scientific misconduct. China, he says, \"will need to make some hard decisions and instigate further reforms, especially to its science system\". Taking the long view, though, Wang recognizes that his country has made tremendous progress since the days he laboured in that frigid Shanghai workshop. \"Things will change with time,\" he says. \"Let's hope.\" Jane Qiu writes for Nature from Beijing. \n                     Nature Geoscience \n                   \n                     Nature Climate Change \n                   \n                     Nature Reports Climate Change \n                   \n                     Wang Pinxian \n                   \n                     South China Sea-Deep \n                   Reprints and Permissions"},
{"file_id": "470027a", "url": "https://www.nature.com/articles/470027a", "year": 2011, "authors": [{"name": "Lee Billings"}], "parsed_as_year": "2006_or_before", "body": "The search for planets outside our Solar System will always be pricey. But creative solutions are proving that it no longer has to break the bank. Astronomers searching for planets around stars other than the Sun have had much to celebrate over the past decade. The number of confirmed 'exoplanets' has soared from about 50 to more than 500 in that time. And although none of these planets closely resembles Earth, NASA's Kepler space telescope, launched in 2009, is now delivering candidates from distant stars by the hundreds \u2014 some of which may prove to be very Earth-like indeed (see  page 24 ). The exoplanet search itself has been wildly successful, but not so the searchers' quest for multibillion-dollar follow-up missions. Hopes for ambitious spacecraft such as a Space Interferometry Mission or Terrestrial Planet Finder have been dashed as missions have been cancelled or postponed owing to a combination of sluggish economic growth, deep cuts to space-science funding and programme difficulties with NASA's James Webb Space Telescope (JWST). In response, the planet-hunting community has got creative, devising ways to maximize the science and minimize the costs. An Exoplanet Task Force jointly commissioned by NASA and the US National Science Foundation accordingly issued a report 1  in 2008, supporting a new strategy for exoplanet research. Rather than waiting for the launch of costly, dedicated planet-hunting spacecraft, it calls for astronomers to press ahead with cheaper, ground-based surveys to discover worlds orbiting nearby stars, which appear brighter to us than do those farther away, and so are easier to study. The hope is that such low-cost surveys will yield at least a few worlds that can be studied using space-based resources such as the JWST. Such facilities would allow astronomers to spectroscopically search the exoplanets' atmospheres for ingredients such as carbon dioxide, water vapour and perhaps methane, oxygen and other trace gases, which could indicate that life is present. A 2010 report from the European Space Agency reached nearly identical conclusions 2 . \"The planets are out there, and it's relatively inexpensive to go after them,\" says Greg Laughlin, an astrophysicist at the University of California, Santa Cruz, who served on the NASA/National Science Foundation task force. \"There's an economic inevitability to this.\" Of the many ideas that astronomers have come up with for conducting exoplanet searches on the cheap, five stand out.  \n                M-dwarf transit surveys (US$2 million) \n              Central to the strategy is a focus on cool, red 'M-dwarf' stars close to the Solar System. Not only are there lots of them \u2014 M-dwarfs are the most abundant kind of star in the Milky Way \u2014 but they are much smaller and dimmer than the Sun, having less than half its mass. So any M-dwarf planet passing in front of the star, or 'transiting' it, would block a larger fraction of the light than it would of a larger star and would be easier to detect (see  'Planet-hunting for beginners' ). The comparative size of the transiting planet's silhouette would also make it easier for telescopes to gather light filtering through its atmosphere for spectroscopic analysis. \n               boxed-text \n             The first, and so far most successful, search for potentially habitable planets transiting M-dwarfs is the MEarth Project (pronounced 'mirth'): a cluster of eight 0.4-metre robotic telescopes at the Whipple Observatory on Mount Hopkins in Arizona. Unlike all previous transit surveys, which stare at a fixed patch of sky rich with stars, MEarth targets 2,000 nearby M-dwarfs; only if one of these displays a candidate transit will all telescopes observe it at once. The project is headed by David Charbonneau, an astronomer at Harvard University in Cambridge, Massachusetts, and was designed mainly by Philip Nutzman, now a postdoctoral researcher in astronomy at the University of California, Santa Cruz. MEarth announced the discovery of its first transiting planet in 2009 \u2014 a world dubbed GJ 1214 b, after the M-dwarf star it orbits some 13 parsecs from Earth 3 . The planet is too large and hot to harbour life as we know it, but was found in only the first six months of MEarth's proposed three-year running time, and so far remains the most easily studied Earth-like exoplanet known. A spectroscopic study of GJ 1214 b, undertaken last year at the European Southern Observatory (ESO) in La Silla, Chile, showed that the planet's upper atmosphere is either very hazy or is composed of water vapour 4 . \"MEarth shows that for a relatively modest investment of US$1 million or $2 million, you can put together a ground-based survey capable of finding habitable-zone super-Earths,\" says Charbonneau, referring to rocky planets that are larger than Earth and orbit their stars at a distance at which water can exist as a liquid. \"The answer to the question being asked is certainly worth a lot more than that.\" By October 2011, Charbonneau and his colleagues hope to have a copy of MEarth operating in Chile, where it will see parts of the sky not visible from Arizona. Several other M-dwarf transit searches are also under way, notably at the 0.6-metre TRAPPIST telescope in La Silla, and at the 1.22-metre Samuel Oschin Telescope at Palomar Observatory in California.  \n                Near-infrared spectrometers (US$5 million) \n              Although transit observations are a good way to determine the radii and orbital periods of exoplanets, other techniques, such as spectroscopy, are essential for learning more. Particularly important is the radial-velocity technique, the planet-hunting method that has produced the most hits so far. An orbiting planet tugs its star to and fro, generating periodic shifts in the wavelengths of the light the star emits; measurements of these shifts can not only allow independent confirmation of an exoplanet's existence, but also provide estimates of its mass. The method presents another good reason to focus on M-dwarfs. Earth's motion around the Sun causes the star to wobble with a radial velocity of some 10 centimetres per second over the course of a year \u2014 a tough signal for any alien astronomers to detect. But if a planet the size of Earth were located in the habitable zone of an M-dwarf, much closer to the star, its radial-velocity signature would be a metre per second, much easier to see. Unfortunately, M-dwarfs shine most brightly with infrared and near-infrared light, so that is the region of the electromagnetic spectrum in which planet-hunters must search \u2014 but astronomers have yet to build the infrared spectrometers required for such precise measurements. So only a handful of the myriad M-dwarfs close to the Solar System have been surveyed for habitable planets. Worse, planet-hunting near-infrared spectrometers are more costly than their optical counterparts, owing to basic physics: infrared photons don't have enough energy to easily excite electrons in an off-the-shelf silicon detector. So the instruments rely instead on detectors made from expensive, exotic materials such as indium gallium arsenide or mercury cadmium telluride, and must be either cryogenically cooled or thermally insulated against background infrared radiation. The most cutting-edge near-infrared spectrometer is ESO's Cryogenic High-resolution Infrared Echelle Spectrograph (CRIRES), which cost some \u20ac10 million (US$13.6 million) to build. Yet prices are dropping, and several major spectrometers may debut in the next few years, if they receive sufficient funding. Among them are the Calar Alto High-resolution Search for M-dwarfs with Exo-Earths with a Near-infrared Echelle Spectrograph (CARMENES), a German\u2013Spanish instrument slated for the Calar Alto observatory in Spain; a near-infrared spectropolarimeter (SPIRou) on the Canada\u2013France\u2013Hawaii Telescope in Hawaii; and a US project, the Habitable Zone Planet Finder, slated for the Hobby-Eberly Telescope at the McDonald Observatory in Texas.  \n                Laser frequency combs (US$100,000) \n              Another hurdle to the progress of M-dwarf planet discoveries is more subtle: the need for better ways to calibrate the spectrometers. The minuscule spectral-line shifts caused by an orbiting habitable planet can all too easily be mimicked by fluctuations in the stability of the instruments themselves. The obvious solution is to generate a reference spectrum with which the observations can always be compared. But the spectra generally used to calibrate optical radial-velocity surveys \u2014 those from iodine or a thorium\u2013argon mix \u2014 don't produce usable calibration lines in infrared. A number of other elements and mixtures are under investigation for calibration. But planet-hunters are most excited about an ultra-high-precision technology known as the laser frequency comb. At the core of such a device is a laser that emits rapid pulses that can be tuned across a wide range of wavelengths. Plotting the frequency of such a pulse train gives a distinct series of regular-wavelength peaks that resemble the teeth of a comb. When those pulses are fed through a spectrometer and synchronized with the ticking of an atomic clock, it becomes a powerful calibration source for spectroscopic measurements. Efforts are under way to test laser combs on spectrometers at observatories. In late 2009 and early 2010, for example, a laser comb developed at the Harvard\u2013Smithsonian Center for Astrophysics in Cambridge, Massachusetts, was linked to an optical spectrometer at the Whipple Observatory and used to calibrate spectroscopic measurements of a known planet-hosting binary star, HD 189733. In mid-2010, at the Hobby-Eberly Telescope, a comb developed at the US National Institute of Standards and Technology and mounted on a near-infrared spectrometer from Pennsylvania State University, obtained radial-velocity measurements of the planet-hosting star Upsilon Andromedae. And in December 2010, at ESO, a comb from the Max Planck Institute of Quantum Optics in Garching, Germany, made radial-velocity measurements of an exoplanet that were, for the first time, more accurate than the previous champion, thorium\u2013argon calibration. The results of these tests are unpublished. If all goes well, combs from each team will grace next-generation spectrometers at major observatories in this decade.  \n                Radial-velocity observatories (US$50 million) \n              Even armed with laser combs, planet-hunters could still be undone by the stars themselves, whose surface motions can masquerade as radial-velocity signals. \"A star reverberates like a bell, with millions of modes of harmonic oscillations covering its surface, a bit like the weird patterns you get from putting sand on a vibrating drum head,\" says Steven Vogt, an astronomer at the University of California, Santa Cruz. \"A few of these modes don't average out across the surface of the star, and they give you oscillations that can show up as noise in your observations.\" The technique that has emerged to counter such noise sources is to average together 10\u201315-minute time-exposures of the star taken on consecutive nights over a period of weeks. St\u00e9phane Udry, an astronomer at the University of Geneva in Switzerland, who hunts for planets at La Silla, says that it works. \"We have a little sample of ten nearby stars we've begun following in this way, and we've already found planets around three of them,\" he says. \"But a lot of observations are needed, because the stars are unlikely to have only one planet. So we have to cover all the potential periods for multiple planets, which takes time. As you try to make your measurements more precise, it quickly becomes expensive.\" So expensive, in fact, that Vogt says the best way to reduce the long-term cost is to spend more money in the short-term on building radial-velocity-dedicated observatories. \"The coin of the realm is observing nights,\" he says. \"It's not new technology; it's not laser combs or some newfangled near-infrared spectrometers that can take advantage of M-dwarfs. Take $50 million, which is chump change in the NASA regime, build a 6\u20138-metre telescope with enough light-gathering power to reach a large fraction of the nearest M-dwarfs, put a nice spectrometer on it and dedicate it to this work every single night of the year. You'd have these planets pouring out of the sky.\" Vogt and his colleagues have built a demonstration project, the Automated Planet Finder (APF): a 2.4-metre robotic telescope paired with a high-efficiency spectrometer at Lick Observatory on Mount Hamilton, California. The APF, according to Vogt, is \"built and bred only to find short-period rocky planets\" around nearby stars, including the brightest M-dwarfs in the sky. The project is now in its final installation phase, with commissioning scheduled for this month. Vogt expects it to rapidly discover a bevy of small, rocky worlds.  \n                ExoplanetSats (US$250,000 each) \n              Radial velocity's most important role in the future may be helping to verify and study promising transit discoveries. \"Transit searches are the most advantageous technique giving access to terrestrial planets in the habitable zones of stars,\" says Udry. \"We cannot beat that.\" Astronomers are already brainstorming successors to the Kepler mission, which would carry out transit surveys of nearby stars looking for worlds with the potential for life. In the meantime, a much cheaper proposal is the ExoplanetSat programme being developed by Sara Seager, an astronomer at Massachusetts Institute of Technology in Cambridge, and her team. The idea is to build on the existing framework for 'CubeSats' \u2014 miniaturized satellites, made up of varying numbers of cubes 10 centimetres on each side, designed to hitch low-cost rides into orbit on rockets launching larger spacecraft. Seager's plan calls for a fleet of dozens of CubeSats, each targeting an individual star and containing a small telescope and guidance equipment. Outside Earth's atmosphere, which interferes with observations, such a payload could detect transiting Earth-sized planets in the habitable zones of nearby Sun-like stars. Seager admits that engineering such 'nano-satellites' to have the necessary stability and thermal control will be challenging. But she and her team hope to launch a functional prototype as early as 2012, with subsequent satellites launching for as little as $250,000 apiece \u2014 a bargain-basement price for a space-science mission. \"On one hand, this seems risky because the probability of finding a transiting Earth-sized planet in the habitable zone of a nearby star is currently estimated at 1 in 200,\" she says. \"On the other, these satellites are modular and relatively cheap; launching one has low risks and what may be very high returns. Basically, this could be MEarth in the sky.\"  \n                 See Editorial  \n                 p.5 \n               Lee Billings is a freelance writer based in New York. \n                     Exoplanet Task Force Final Report \n                   \n                     ESA's European Roadmap for Exoplanets \n                   \n                     Extrasolar Planets Encyclopedia \n                   Reprints and Permissions"},
{"file_id": "471151a", "url": "https://www.nature.com/articles/471151a", "year": 2011, "authors": [{"name": "Tanguy Chouard"}], "parsed_as_year": "2006_or_before", "body": "If dogma dictates that proteins need a structure to function, then why do so many of them live in a state of disorder? Keith Dunker's life is a mess. His desk is so swamped with books, old chocolate bars, half-reviewed manuscripts, pens, coke bottles and \u2014 somewhere \u2014 a stray sock, that he ends up printing papers again rather than wading in to find the original. \"I'm so disorganized,\" he crows, \"some people have called me Dr Disorder.\" But he remembers with great precision the moment that disorder invaded his scientific life. It was 15 November 1995, at 12:40 p.m., halfway through a seminar by crystallographer Chuck Kissinger, at Washington State University in Pullman, where Dunker was then a biochemist. Dunker was staring at a slide showing the atomic structure of calcineurin, an enzyme targeted by immunosuppressive drugs. What caught his attention wasn't the intricate structure but something missing from it: a dotted line representing a string of amino acids with a position too variable to be determined by X-ray crystallography, as the rest of the protein had been. And Kissinger was insisting that this loop had to remain flexible for calcineurin to serve its crucial function in the human immune system. \"It hit me like a brick,\" says Dunker: this wayward piece of protein flouted a century of dogma. A central tenet in molecular biology is that the function of a protein depends critically on its fixed three-dimensional structure; by extension, enzymes bind to specific substrates because their shapes match perfectly, as immortalized in the 'lock-and-key' model proposed by chemist Emil Fischer as early as 1894. But this part of calcineurin seemed to disobey these rules, by providing function without structure. Now Dunker was wondering how many other proteins were ignoring the rules too. To find out, he and his colleagues wrote a bioinformatics program that predicted which protein segments are 'intrinsically disordered' \u2014 meaning that they do not fold spontaneously into a unique three-dimensional shape. Today, this and other similar programs predict that about 40% of all human proteins contain at least one intrinsically disordered segment of 30 amino acids or more, and that some 25% are likely to be disordered from beginning to end 1 . This part of the protein universe had largely been ignored because disordered protein segments impede crystal formation \u2014 a prerequisite for X-ray diffraction, the predominant way structures are deduced \u2014 and structural biologists clip them out whenever they can. Today, though, \"the recognition of disorder has grown dramatically\", Peter Wright, a protein biophysicist at the Scripps Research Institute in La Jolla, California, told the American Association for the Advancement of Science meeting in Washington DC last month. A large part of that recognition has come from studies using nuclear magnetic resonance (NMR) spectroscopy, which allows researchers to determine the structures of small proteins even as they twist and turn in solution. Such work has shown that disorder can actually be essential to function by helping a signalling protein to recognize and react to a protein partner, or by allowing a regulatory protein to interact with multiple targets. Still, says Wright, \"that hasn't got through to the textbooks\". Many structural biologists see no need for revision. \"My mantra has been: function requires structure,\" declares Tom Steitz, a crystallographer at Yale University in New Haven, Connecticut. \"Some flexibility can be required, it may be an essential part of the assembly process, but it's not interesting until the proteins get to do their job.\" Critics argue that the computer programs predicting high levels of disorder are fundamentally flawed because they identify proteins that are well-known to become perfectly ordered \u2014 and to crystallize \u2014 when bound to their proper molecular partners. They say that unfolded protein chains cannot persist for long in living cells, and some want the concept of intrinsically disordered proteins to be ditched altogether. That seems unlikely. Data are fast accumulating from all fronts \u2014 biophysics, bioinformatics and cell biology \u2014 in support of widespread disorder, and disorder aficionados are calling for a complete reassessment of the structure\u2013function paradigm. \"Biology uses disorder to bring about its various functions,\" Wright says. Since the late 1950s, newly made proteins have been assumed to fold up immediately and spontaneously into a unique three-dimensional shape \u2014 their most energetically stable conformation and the only functional one 2 . The few proteins known to remain unfolded \"were pointed to as oddities\", Wright says. But that started to change in 1999, when Wright and fellow NMR spectroscopist Jane Dyson, also at Scripps, wrote a review 3  pointing to the growing collection of proteins that seemed to function despite their disordered state. It has been \"the big-dog paper in the field\", says Dunker, now at Indiana University School of Medicine in Indianapolis. One burning question, then and now, is how a protein can function if it has no fixed shape. \"We all accept flexibility,\" says structural biologist Jo\u00ebl Janin at the CNRS Laboratory of Enzymology and Structural Biochemistry at Gif-sur-Yvette, France. \"The question is: how can you get recognition with flexibility?\" The whole concept of disorder seems incompatible with the lock-and-key model. You might as well try to open the door with cooked spaghetti. In 2007, postdoc Kenji Sugase in Wright's lab found an answer: the spaghetti uses the lock to mould itself into the shape of the key, rather than forming the key beforehand. Sugase focused on CREB, a gene-regulatory protein involved in many processes including learning and memory. Once bound to DNA, CREB also needs to recognize and bind a protein partner called CBP before it can switch on the gene. But the part of CREB involved with CBP enters the game in a disordered state. How could a thing like this possibly work? To find out, Sugase developed the equivalent of a super-fast NMR camera so that he could capture frequent snapshots of CREB's wriggling chain, at atomic resolution, as it tested out points of contact within itself and with CBP. What he saw was that several bonds had to form cooperatively within CREB and with CBP for the whole complex to snap into shape 4 . That's exactly how the average 'globular' protein folds: its internal segments need to establish long-range chemical bonds with one another, to pull the whole thing suddenly into shape 2 . CREB forms such interactions externally, by bonding to CBP \u2014 and if this bonding is just a little weaker, then the key cannot form and there is no binding with the lock at all. Wright and his colleagues think that disorder is therefore advantageous because it allows CREB to partner with CBP more exclusively than a rigid protein would. And Wright thinks that this type of process allows many signalling proteins to engage in speedy yet selective interactions.  \n                Disorderly conduct \n              The structure\u2013function mantra took an even bigger hit recently from a protein with parts that never seem to fold at all. The Sic1 signalling protein is a key regulator of the cell cycle that puts the brakes on DNA replication until the cell is ready to divide. In 2001, a team led by Mike Tyers, a yeast cell-cycle expert at the University of Toronto, Canada, began unpicking the mechanism of the switch. The group found that when phosphate groups are added to six sites on Sic1 it can then hook up with a second protein, Cdc4, which pushes Sic1 into the cell's protein-disposal pathways 5 . Once Sic1 is degraded, DNA replication can forge ahead. But unless that degradation occurs at precisely the right time, DNA replication goes haywire and the cell may eventually die. The cell achieves that precision by ensuring that it takes exactly six phosphates to flip the switch, not four or five. But there's a rub: Cdc4 has only one high-affinity binding pocket for a phosphate group. How can Cdc4 'count' up to six with effectively only one finger to count on? \n               Click here for larger image \n               After all attempts to crystallize the Sic1 complex failed, Tyers's team called NMR spectroscopist Julie Forman-Kay, also at the University of Toronto. In 2008, Tanja Mittag, a postdoc in Forman-Kay's lab, showed that Sic1 was disordered 6  \u2014 not only in its free state but, astoundingly, also when bound to Cdc4. The complex seemed to be a mixture of different conformations shifting around in constant, dynamic equilibrium. And the most stunning part was that each of the six phosphate groups on Sic1 could be found to occupy the single Cdc4 pocket, one after the other, as in a constant dance around the fire (see  'Orders of disorder' ). The researchers then developed a computer model and fed it with every scrap of experimental data about the proteins' structures that they could gather 7 . They concluded that, even though Sic1 is disordered when it is bound, it maintains a rather compact structure, which keeps all the phosphate groups sufficiently close together to form an average electrostatic field that glues Sic1 to Cdc4. Only when six phosphates are present is the glue strong enough for Cdc4 to hold Sic1 close and force-feed it into the cell's disposal machinery. And one reason that Sic1 has to be disordered during all this, the team proposes, is to enable the rather rigid disposal machinery to reach all parts of Sic1 and carpet it with the chemical tags that mark the protein for destruction. It takes a nimble protein indeed to make all these connections at once. \"The result is interesting,\" says structural biologist Stephen Harrison of Harvard Medical School in Boston, \"because interaction motifs are often found more or less repeated along unstructured segments, and the work shows how such multiplicity can function.\" And Forman-Kay thinks that adopting 'multi-structural' states could allow other proteins to constantly probe and sense signals from many partners at once. This is particularly important for 'hub' proteins, which are central to vast networks of rapidly changing molecular interactions. \"There is a complexity people haven't talked about,\" she says. \"These hub proteins need to very rapidly sample the complex cellular environment.\" One extreme example can be seen in the tumour suppressor p53, an extraordinarily well connected hub in multiple signalling networks, and the protein most frequently implicated in human cancer. Part of the explanation for p53's promiscuity seems to lie in its versatile structure, which features every possible conformation from order to disorder. The core domain is globular and binds to DNA and just a few other proteins; its two flanking wings are mostly disordered and can bind to hundreds of signalling partners; and a segment within one wing shows a 'chameleon' status which can flip between four different ordered states, depending on which partner it binds to 8 . Alan Fersht, a biophysicist and NMR expert at the University of Cambridge, UK, says that he is \"absolutely sure\" that long parts of p53 remain largely disordered in the cell: \"I don't think there's any doubt about that whatsoever.\"  \n                Prediction problems \n              Yet many researchers question how widespread disordered proteins can be. That is mainly because biochemists, over more than 100 years of preparing tissue extracts, have struggled to prevent proteins from unfolding and tangling into insoluble clumps or getting digested by enzymes called proteases. \"It's hard to believe that disordered proteins could produce anything else than a mess,\" says Janin. Researchers who study these processes, however, say that such fears are largely irrelevant. In human cells, for example, nonspecific proteases are locked away in compartments called lysosomes, which should allow disordered proteins to survive everywhere else, explains Ulrich Hartl, an expert on protein quality control at the Max Planck Institute in Martinsried, Germany. Disordered proteins should also be protected from aggregation because, unlike globular proteins, they contain few hydrophobic amino acids, which tend to stick together \u2014 and are instead rich in 'polar' amino acids that are happy swimming in water. Hartl thinks that natural selection against aggregation probably gave disordered proteins this particular amino-acid composition \u2014 in other words, it is not a signature for disorder per se. And this explains an apparent inconsistency of disorder predictors: that they do not pick internal segments from globular proteins, even though these too are incapable of folding on their own when sliced out of a protein 2 . The reason is that they are rich in hydrophobic amino acids and so do not show the signature sequence that the predictors detect. It also explains why the predictors select some proteins that are in fact ordered, another point of controversy surrounding these programs: because proteins can lack hydrophobic amino acids for reasons other than disorder. \"This makes perfectly good sense,\" agrees Dunker. Overall, the numerous programs claim an 80% success rate at predicting whether any individual amino acid in a protein will be surrounded by order or disorder 1 , as compared to the 50% success rate expected by chance \u2014 and crystallographers rely on them to sideline proteins expected to resist crystallization. \"Disorder predictors are a massive oversimplification, but they are very useful,\" says Adam Godzik, at the Burnham Institute in La Jolla, California, and head of the bioinformatics group for the Protein Structure Initiative, a large collaboration that aims to solve large numbers of protein structures. Nevertheless, debate about the prevalence and importance of disorder has probably slowed progress in the field. DisProt, a database of proteins whose disorder has been established experimentally, contains just over 500 proteins, a number dwarfed by the more than 60,000 structures in the Protein Data Bank 1 , the database for 3D structures. \"The main reason why DisProt is so small,\" says Dunker, \"is because it has taken so many damn years to get it funded.\" But in the past few years, major consortia aimed at exploring intrinsically disordered proteins have been set up in several countries. Interest in disordered proteins as drug targets is also on the rise because so many of them, like p53, are crucially implicated in disease. Little by little, a fundamentally new picture of the relationships between protein sequence, structure and function is emerging: a continuum running from the most rigid 'lock-and-key' enzymes and molecular machines at one extreme through to durably unstructured spaghetti such as Sic1 at the other, and spanning all degrees of structural ambiguity in between. Figuring out how all these disordered proteins really work is a long way off, if Sic1 is anything to go by: determining its mode of action involved several, often arcane, biophysical techniques, new computer tools and statistical physics theory \u2014 plus at least ten years of work by six labs. Multi-structural biology isn't going to be simple. Still, Dunker, Wright and other doctors of disorder are optimistic. As is Martin Blackledge, an NMR spectroscopist at the Institute of Structural Biology in Grenoble, France, who compares the excitement now to that surrounding the first crystal protein structures in the 1950s. \"Every new case is fascinating at the moment,\" he says. Blackledge looks forward to a day when it might be possible to predict where on the structural continuum a protein segment falls, from its amino-acid sequence \u2014 to crack the full code of disorder. \"This is exactly what I'm aiming for,\" he says, \"this is my dream.\" Perhaps the rules of disorder are needed, before disorder can rule.  Tanguy Chouard is an editor for Nature in London. \n                     Database of Protein Disorder \n                   \n                     European consortium on IDPs \n                   \n                     Japanese consortium on IDPs \n                   Reprints and Permissions"},
{"file_id": "471025a", "url": "https://www.nature.com/articles/471025a", "year": 2011, "authors": [{"name": "Zo\u00eb Corbyn"}], "parsed_as_year": "2006_or_before", "body": "American Indians have had some unhappy interactions with scientists in the past. Now, America's tribal colleges are rapidly expanding their own research. Katie McDonald had never given much thought to the trout in Flathead Lake \u2014 except when fishing with her family. She didn't wonder about heavy-metal pollution or how that might affect people eating the fish. But that was before the then-19-year-old student started a bachelor's degree in environmental science at Salish Kootenai College in northwest Montana and had to choose a research project. She saw that trout consumption was going up on the Flathead Indian Reservation, where she lived. Poor people, in particular, had begun to receive donated fish. So McDonald set out to see whether there was cause for concern. Her institution is a tribal college, one of 36 scattered around the United States (see  'US tribal colleges' ) and serving some of the least-developed communities in the country. But thanks to several federal programmes seeking to boost science within tribal colleges, McDonald had access to equipment such as a state-of-the-art mercury analyser. She ran samples of the lake trout ( Salvelinus namaycush ) and found surprisingly high levels of the toxic metal 1 . The results were compelling enough for the tribal government to advise women of childbearing age to avoid eating older, larger fish from the lake altogether \u2014 a more stringent recommendation than state guidelines that suggest eating no more than one a month, says Barry Hansen, the tribes' fisheries biologist. Douglas Stevens, head of life sciences at Salish Kootenai, says that McDonald's work shows students how their scientific research can serve the local community. \n               Click here for larger image \n               That message is big change for American Indians, who have typically been research subjects rather than investigators in studies ranging from anthropology to genetics. And like indigenous peoples around the world, American Indians have sometimes been treated poorly by the scientific establishment. In a high-profile case last year, Arizona's Havasupai Indian tribe settled a lawsuit it had filed against Arizona State University in Tempe for conducting genetic analyses that the tribe says were done without express permission. That case and others have fostered a climate of suspicion among some American Indians towards mainstream researchers. But tribal colleges are now trying to harness science for their communities' own purposes by building up their capacity for both training and research. With an influx of funding from several federal agencies over the past decade, these institutions have started to hire more faculty members with research credentials, develop better facilities and establish science degree programmes. Although there are difficulties, particularly in research quality and publication rates, supporters say that the increasing focus on scientific research at tribal colleges is helping both students and their communities. It can be seen \"as an act of resistance\" says Luana Ross, the president of Salish Kootenai. \"We are taking control of the research process.\"  \n                Demand for doctorates \n              The emphasis on research is part of a broader set of changes at tribal colleges, most of which operate in self-governed nations. Unlike mainstream US universities, where undergraduates typically pursue four-year bachelor's degrees, tribal colleges have traditionally offered only two-year degrees and vocational training. Because many of them serve relatively poor communities with struggling primary and secondary schools, tribal colleges must provide remedial education to make up for gaps in students' basic skills and knowledge. But several tribal colleges are also seeking to raise the level of their instruction by hiring teachers with more training. The percentage of staff with doctorates at tribal colleges rose by nearly 40% from 2003 to 2009, going from about 8% to 11% of the total, according to figures from the American Indian Higher Education Consortium, based in Alexandria, Virginia. The focus on science seems to be having an effect. Although enrolment at tribal colleges has been decreasing, the number of students pursuing degrees in science rose by more than 70% between 2003 and 2009, to about 1,200 students altogether. And eight tribal colleges now offer full four-year bachelor's degrees, with about 70 applied-science bachelor's programmes available. The initiatives at tribal colleges are aided by a collection of programmes totalling about US$20 million annually, from federal agencies such as the National Science Foundation (NSF). \"One of the reasons for the phenomenal growth in science enrolment at the tribal colleges is because they are able to provide undergraduate research opportunities,\" says Jody Chase, who manages the NSF's Tribal Colleges and Universities Program. Since 2001, that programme has provided $13.5 million a year in funding to strengthen science courses at tribal colleges and other institutions serving Native Americans in Alaska and Hawaii. Many of the research projects at tribal colleges focus on the local community. Researchers at Din\u00e9 College in the Navajo Nation of Arizona, for example, worked with scientists from the US Geological Survey in Reston, Virginia, to investigate why residents in the Shiprock area of the reservation have roughly five times the rate of respiratory illness seen in nearby communities, despite a relatively low incidence of smoking. The area is home to some of the largest coal-mining and electricity-generating operations in the United States. By examining hospital records and monitoring indoor air quality in more than 130 homes, the researchers linked respiratory problems to high concentrations of particulate matter from the burning of coal in stoves not designed for that purpose 2 . The coal is provided at low or no cost to Navajo living near coal mines, as part of reservation lease agreements. The study has led to a large community-education campaign emphasizing, for example, the importance of leaving a window open. The college has also recommended that the tribe support a stove-replacement programme. Scientists working outside the tribal colleges say that the focus on research is raising standards at these institutions. \"They have had some really good successes,\" says David Burgess, a Native American cell biologist at Boston College in Massachusetts, who is involved with the Society for the Advancement of Chicanos and Native Americans in Science. Burgess says that the presentations given by many tribal-college students at the society's annual conference are getting stronger.  \n                Global phenomenon \n              The research expansion has parallels in other countries such as Norway, New Zealand and Canada, where universities serving indigenous peoples are conducting studies on topics of local interest that would not otherwise be explored. \"It is a global phenomenon,\" says Boni Robertson, a professor of indigenous policy at Australia's Griffith University in Queensland, and co-chair of the World Indigenous Nations Higher Education Consortium. But along with their successes, America's tribal colleges have run into some hurdles in their scientific efforts. Enrolment in science programmes has climbed, but the number of students completing two-year or four-year science degrees remained essentially flat from 2004 to 2009. There are also concerns about the research at these institutions. Barbara Howard, a senior scientist at the non-profit MedStar Health Research Institute in Hyattsville, Maryland, has worked in Indian country since the late 1980s directing the Strong Heart Study, the largest epidemiological study of Native Americans. Howard welcomes the rise in undergraduate research at the tribal colleges, and says it is the best way to encourage students to go on to graduate school. But, she says, the tribal colleges need to improve in terms of their \"research quantity and complexity\". A major obstacle is that many faculty members don't have the necessary experience to undertake research \u2014 a problem that the American Indian College Fund (AICF), based in Denver, Colorado, is trying to rectify. It awards fellowships to faculty members at tribal colleges to start and finish PhDs and do their own research. Yet although the organization recruits intensively, each programme receives only a handful of applications. Doing a PhD on a local Indian issue can be a tough slog, says Valerie (Pretty Paint) Small, a faculty member at Little Big Horn College on the Crow reservation in southern Montana. She has an AICF science fellowship to finish her PhD at Colorado State University in Fort Collins, where she is studying the invasion of a non-native tree species on Crow tribal lands. \"So many people know so little about our contemporary issues,\" she says. \"University professors don't make much of an effort to see where you might be coming from if you are on the reservation.\" Faculty members at tribal colleges also struggle to publish their work, in part because of large teaching loads. To improve publication rates, colleges such as Salish Kootenai are forming writing groups for faculty members, and administrators are exploring ways to give staff time off for research. And the AICF hopes to start a peer-reviewed, interdisciplinary journal this year that would publish research undertaken at tribal colleges. Some scientists wonder whether tribal colleges would be better off expanding their partnerships with research-intensive universities rather than trying to do research on their own. \"Why recreate those resources when they can partner with other institutions and develop new kinds of synergy?\" asks Spero Manson, a Native American medical anthropologist who directs the Centers for American Indian and Alaska Native Health at the University of Colorado in Denver. But Daniel Wildcat, acting dean of natural and social sciences at Haskell Indian Nations University in Kansas, says that doing research within tribal colleges allows Native Americans to \"design their own research agendas\", in which tribal values rather than those of outsiders determine what gets studied.\n \n                 See Editorial  \n                 p.5 \n               Zo\u00eb Corbyn is a freelance journalist based in San Francisco. \n                     American Indian College Fund \n                   \n                     Society for the Advancement of Chicanos and Native Americans in Science \n                   \n                     NSF Tribal Colleges and Universities Program (TCUP) \n                   Reprints and Permissions"},
{"file_id": "469284a", "url": "https://www.nature.com/articles/469284a", "year": 2011, "authors": [{"name": "Corie Lok"}], "parsed_as_year": "2006_or_before", "body": "There is more to the eye than rods and cones \u2014 the discovery of a third photoreceptor is rewriting the visual rulebook. Russell Foster remembers his first human subject, an 87-year-old woman, as she sat in a dark room facing a backlit pane of frosted glass. A genetic disorder had destroyed the light-sensing rod and cone cells in her eyes, leaving her blind for the past 50 years. She was convinced that she would see nothing. But as the wavelength of light in the room shifted to blue, she reported \u2014 after some hesitation \u2014 a sort of brightness. \"That just blew us away,\" says Foster, a neuroscientist at the University of Oxford, UK, and one of the senior authors of a 2007 study reporting the finding 1 . Foster and his collaborators had done nothing to treat the woman's blindness. Instead, her awareness of light owed itself to a class of light-sensitive cells discovered in 2002. Studies of these intrinsically photosensitive retinal ganglion cells (ipRGCs) have since revealed many surprises. Scientists initially thought that, rather than contribute to vision, the cells simply synchronized the circadian clock, which sets the body's 24-hour patterns of metabolism and behaviour, with changing light levels. However, recent work suggests that ipRGCs have been underestimated. They may also have a role in vision \u2014 distinguishing patterns or tracking overall brightness levels \u2014 and they seem to enable ambient light to influence cognitive processes such as learning and memory.  \n                Rods and cones dethroned \n              During the past century, vision scientists focused mainly on rods and cones as the light sensors of the eye. It took Foster, an outsider coming from the circadian-biology community, to uncover some of the first evidence for a third type of photoreceptor. In the early 1990s, while at the University of Virginia in Charlottesville, his lab tested the circadian light responses in a mouse mutant with retinas that degenerate over time, and found that they were indistinguishable from the responses in mice with normal retinas. But light had no effect on the internal clocks of mice whose eyes had been removed 2 . Scepticism was strong. Foster recalls people walking out during a talk he gave. Critics of the research argued that the mutant mice probably retained some rods and cones that could be setting the clock. So in 1999, Foster, who had moved to Imperial College London, crossed transgenic mice that had no cone cells with mice that had degenerative rod cells, thus eradicating both cell types in the offspring. As long as the mice had eyes, they still had normal circadian rhythms 3 , 4 . The next year, Ignacio Provencio, a former graduate student of Foster's now at the University of Virginia, Charlottesville, identified the light-sensitive molecule melanopsin in the mouse and primate ganglion layer 5  \u2014 a network of retinal cells that was only thought to relay signals from rods and cones to the brain (see 'Light in layers'). The presence of this 'photopigment' suggested that some of these cells might also sense light and serve as a new class of photoreceptor. Researchers raced to isolate the cells and show that they could fire in response to light, without input from rods and cones. \n               Click here for larger image \n               The race ended in a tie in 2002. Samer Hattar, a neuroscientist at Johns Hopkins University in Baltimore, Maryland, and his colleagues found that as many as 1% of the cells in the mouse ganglion layer express melanopsin, which is most sensitive to blue light 6 . David Berson, a neuroscientist at Brown University in Providence, Rhode Island, and his lab showed that these cells, ipRGCs, detect light on their own and reach into the brain's pacemaker, the suprachiasmatic nucleus 7 . The two papers helped to win over the sceptics, says Russell Van Gelder, a neuroscientist and ophthalmologist at the University of Washington, Seattle. \"Things really took off in 2002,\" he says. Researchers began to develop mouse models in which they could selectively block input from each of the three photoreceptor types in the eye, to probe their individual contributions. But rather than distributing jobs neatly between cell types, the cells seem to swap roles under different conditions. It became clear that under low light conditions, rods can set the body's clock, but some groups have suggested that under different conditions cones can as well. Perhaps more surprisingly, researchers have found that ipRGCs may contribute to visual perception. Hattar and others fluorescently labelled ipRGCs in mice to trace the projections of these cells to the brain. They found that ipRGCs reach into more brain regions than expected, including centres involved in visual processing: the dorsal lateral geniculate nucleus (LGN) and the superior colliculus. Mice without functioning rods and cones, but with intact ipRGCs, could even discriminate patterns in a visual test 8 . This is puzzling. Melanopsin responds slowly \u2014 on the order of seconds \u2014 to changes in light, limiting its ability to signal changes in spatial information, says Robert Lucas, a neurobiologist at the University of Manchester, UK. He and his group found that in mice that lacked the gene for melanopsin, and therefore had non-photoreceptive ipRGCs, almost half of the neurons in the LGN had defective light responses. The mice were unable to track background light levels, especially in the daylight range, suggesting that ipRGCs could be encoding information about brightness 9 . Researchers now think that ipRGCs and rods compensate for each other and may collectively be allowing the eyes and brain to respond to light across a wide range of brightness levels. Why these different photoreceptors share the load in such specific ways is not clear. For example, the sensitivity of ipRGCs to blue light may make them better suited to detect the arrival of dawn and dusk.  \n                Beginning to see the light \n              The ipRGCs might influence phenomena beyond vision and circadian rhythms. Many physiological responses have been linked to light, such as sleep, migraine pain and seasonal affective disorder, and these have recently been associated with ipRGC activity. \"There's likely to be a whole array of physiology that, to some degree, is light sensitive,\" says Provencio. Learning and memory may be improved under certain light conditions. Provencio and his colleagues presented data last year showing the effects of light on a mouse model for learned fear. Mice were conditioned to associate a mild electric shock with a tone cue. Those that had learned fear in the presence of light froze for longer in response to the tone than those that had been conditioned in the dark. This effect did not appear in mice engineered to lack rods and cones, but did in melanopsin-knockout mice, suggesting that the rods and cones are driving this light-enhanced learning. Still, the researchers have not ruled out a role for ipRGCs. These cells route information from the eyes to the non-visual centres in the brain, including those involved in fear responses. Hattar has unpublished data to suggest that activating melanopsin with light at various points in the sleep\u2013wake cycle of mice impairs learning and memory, even when the animals have normal circadian rhythms. This could mean that exposure to light at times when the body isn't expecting it can be disruptive. And for humans, who have a smaller percentage of ipRGCs than mice, experiments are beginning to show how the cells might contribute to physiology and behaviour. Steven Lockley, a neuroscientist at Brigham and Women's Hospital in Boston, Massachusetts, and his colleagues tested the reaction times of 16 healthy volunteers while they were exposed to either blue or green light for 6.5 hours. Those exposed to blue light had faster reaction times and fewer attention lapses when they were asked to report when they heard a sound 10 . Lockley says that these different strands of research might eventually help to engineer 'healthier' light \u2014 using specific wavelengths, intensities or even patterns to activate brain pathways and improve mood, sleep or mental performance. \"This research opens up a whole new field in terms of light applications, both for use therapeutically and for the general population,\" says Lockley. Light of certain frequencies can have beneficial effects, but may also be detrimental to health. Lockley has been working with a group of light engineers, neuroscientists and ophthalmologists, who call themselves the Blue Light Group. They met for the first time this summer to discuss, among other things, any safety issues surrounding blue light, including the idea that excessive exposure to it might contribute to a type of vision loss known as macular degeneration. Many light-emitting diodes, a leading technology for energy-efficient lighting, are rich in blue light, points out Charles Hunt, a materials scientist at the University of California, Davis, who leads the group. Could their wider adoption lead to health problems for people? Humans have evolved to live under natural light, says Van Gelder. \"Could we be doing some damage to our health by poisoning the world with wavelengths that we're not evolved to live in?\" he asks. Given the emergence of new kinds of lighting, Hunt says that it is important to find out. \"We need answers quickly,\" he says.  Corie Lok is Nature's Research Highlights editor. \n                     Russell Foster \n                   \n                     Steven Lockley \n                   \n                     Samer Hattar \n                   Reprints and Permissions"},
{"file_id": "470024a", "url": "https://www.nature.com/articles/470024a", "year": 2011, "authors": [{"name": "Eugenie Samuel Reich"}], "parsed_as_year": "2006_or_before", "body": "Launched in 2009 to seek out worlds beyond the Solar System, the Kepler mission is exceeding expectations. Is it closing in on another Earth? Sitting for an interview in his office at the Harvard-Smithsonian Center for Astrophysics (CFA) in Cambridge, Massachusetts, the normally voluble astronomer Dimitar Sasselov looks nervous. Asked for his favourite among the many potential planets discovered by NASA's Kepler planet-finding mission, for which he is a co-investigator, he hesitates, then sidesteps the question entirely. \"Personally, I'm already beyond that point. It's not one. It's not a single planet. It's a whole family.\" Sasselov has good reason to be wary: his public lecture last July at the Technology, Entertainment and Design 2010 conference in Oxford, UK, earned him a stern rebuke from his colleagues on the mission. Not only had he presented numbers for possible planets greater than those released officially by the team, they said, but he had also used a careless phrasing that resulted in a raft of headlines proclaiming \u2014 incorrectly \u2014 the discovery of hundreds of other Earths. That hullabaloo became a distant memory this week, as the mission released 400 candidate systems, adding to the 306 released last June. Along with the candidates came a bunch of confirmed planets. The latest finds, posted on NASA's website last month (see  http://go.nature.com/aejd15 ) and published in  Nature   this week 1 , include a rocky planet orbiting so closely to its star that its starlit side must be a seething sea of lava; and a planetary system containing several large, rocky or icy planets in orbits of tens of days, just one order of magnitude faster than Earth's 365-day cycle. \"It's very exciting. It's a type of system we haven't seen before,\" says Jack Lissauer, a space scientist and Kepler co-investigator based at the NASA Ames Research Center in Moffett Field, California, and a lead author on the paper in  Nature . Nonetheless, most of the Kepler scientists continue to be cautious. By watching the light from some 150,000 stars for the dimming that could signal a planet crossing in front of them, Kepler is extraordinarily efficient at finding possible planets. But Kepler has yet to find another Earth \u2014 a small, rocky planet with an orbit of a few hundred days and well inside the habitable zone in which water can exist and life can arise. That is for a fundamental reason; the blips that Kepler detects show only the radius, and not the mass, of an observed planet, which means that the density and composition generally remain unknown. Moreover, the scientific objective of the Kepler mission is not to discover Earth-like planets. Instead, it is to estimate the fraction of Sun-like stars that have Earth-like planets \u2014 statistics that could greatly enhance astronomers' understanding of how planetary systems form. Determining which of the blips correspond to planets \u2014 rather than systems of stars in which one is eclipsed, causing a similar dimming \u2014 is what the researchers spend most of their time on, says William Borucki, a space scientist and Kepler principal investigator at NASA Ames. The only way to do that, he says, is the hard way: painstakingly sorting the real signals from the false positives.  \n                Observational bias \n             \n               boxed-text \n             Until Kepler, the leading detection method used to discover exoplanets \u2014 planets outside the Solar System \u2014 was much more likely to find giant planets, resulting in a sampling bias. Known as radial velocity or Doppler spectroscopy, the method depends on identifying the shift in a star's spectral lines as it wobbles around a mutual centre of gravity with a planet. The larger the planet and the closer to the star it lies, the faster the star's movement towards and away from Earth, and the easier it is to detect the shift in the spectral lines. Almost all of the planets found by this technique have been larger than Jupiter and very close to their stars, sometimes completing an orbit in just a few days. An alternative method was presented in 2000, when CFA astronomer David Charbonneau and his colleagues, working from a shed in a car park outside the National Center for Atmospheric Research in Boulder, Colorado, observed a planet passing across \u2014 or transiting \u2014 the face of its parent star 2 . Within days, another group had made a similar observation 3 . In this case, the researchers were confirming a transit predicted for a planet, HD 209458b, that had been spotted using the radial-velocity method. Before long, planets were being detected by their transits alone. Those early detections also yielded large, close-in planets, which were easier to see because they obscured larger portions of their host stars than their Earth-size counterparts would do. But researchers were thrilled to realize that, in principle, a space telescope could be made sensitive enough to see the transits of Earth-sized planets in Earth-like orbits \u2014 and the idea of Kepler was born. Kepler was designed as a 0.95-metre-diameter space telescope that would detect exoplanets by monitoring variations in the light from stars. Unlike most ground-based telescopes and the French Space Agency's COROT planet-finder mission, which monitor targets for months at a time at most, Kepler was intended to stare at the same, fixed field of view for 3\u20134 years.  This field  encompasses 150,000 stars in the Cygnus and Lyra constellations, chosen because of a prevalence of Sun-like stars. The commitment to the same star field made Kepler unique in being able to capture three or four repeat observations of transits by small planets in yearly, Earth-like orbits, even if it wouldn't be able to determine their mass and composition. The spacecraft was launched in March 2009, and the Kepler scientists announced their first few planets the following January. \"We were just skimming the cream off the top,\" says Natalie Batalha, an astronomer at San Jose State University in California and Kepler's deputy science team leader. At that point, the short timescale over which the mission had been operating continued to favour the discovery of giant planets with fast orbits that were too close to their host stars to be habitable. These included five giant planets with orbits of between 3.2 days and 4.9 days 4 . But the 306 planetary candidates released a few months later told a different story. Most of these correspond to planets that are Neptune-sized or smaller, and nearly 40 are smaller than twice Earth's size 5 . If confirmed, Batalha estimates, five would correspond to planets orbiting within the habitable zones of their stars. \n               boxed-text \n             The process of turning a candidate into a confirmed planet is tortuous. Every month, pixels representing a continuous flux of light captured from the target stars are downloaded from the spacecraft to computers at NASA Ames, where they are converted into light curves \u2014 graphs showing the intensity of light from the star as it changes with time. The software flags up about 2,000\u20133,000 dips in light curves automatically, and these are then sent to a committee headed by Batalha. Those not rejected as obvious false positives \u2014 owing to instrument noise, for example \u2014 are assigned a Kepler object of interest (KOI) number. Mission scientists estimate that 50% of the KOIs are real planets, but they have been able to confirm only 15 of the 306 KOIs announced so far (see  'No place like home' ) \u2014 including those published in this issue. The most obvious way to rule out a false positive is to detect the planet using another method. Charbonneau, now a participating scientist on Kepler, is working to follow up the KOIs with NASA's Spitzer Space Telescope, which is sensitive to infrared radiation. It is ideal for ruling out a type of false positive known as a blend, which consists of a much brighter star in the same line of sight as two dimmer, orbiting stars, so that one occasionally eclipses the other. To Kepler, a blend can look like the transit of a Jupiter-sized planet, says Charbonneau \u2014 but not to Spitzer, because the three stars will have different proportions of their light in the infrared and visible wavelengths. Kepler data alone can confirm a candidate when it is part of a system of several planets. Last year, for example, researchers found a planetary system with a pair of planets transiting the same star, at almost regular intervals. \"We started to lavish more individual attention on the system once we saw the transits were varying,\" says Matthew Holman, an astrophysicist at the CFA and a member of the Kepler team. It was a sign that the planets were real: planets can vary by as much as several minutes per orbit if they are interacting gravitationally with one another. In a rapid orbit, this is the equivalent of Earth's revolution around the Sun changing by a few hours each year. The new planets, dubbed Kepler-9b and -9c, had radii about 0.8 times the size of Jupiter's, and orbits of 19 days and 39 days, respectively 6 . By modelling the gravitational interactions of the planets, the team calculated that their probable masses are similar to that of Saturn. Knowing the mass and radius of each planet helped the astronomers to estimate that the composition of the planets was hydrogen- and helium-rich, making them very much like the gas giants Saturn and Jupiter. Kepler-9 was the first system in which several planets were found to transit the same star. The paper on  page 53  (ref.  1 ) announces a second, Kepler-11, in which as many as six planets transit, with orbital periods of 10, 13, 22, 31, 46 and 118 days, and masses between 2.3 and more than 300 times that of Earth. Although the outer four are gas giants, the inner two could be ice giants like Neptune. Or they could be super-Earths \u2014 planets several times larger than Earth but consisting of a mixture of rock and gas. The team was surprised to see as many as six planets transiting in the same system, says Lissauer, and further astounded to find the inner planets so densely packed that, were they any more so, their orbits would not be stable. \"It's an amazing system,\" he says. \n                Subtle effects \n              Kepler-11b\u2013g come hot on the heels of the January announcement of Kepler-10b, a dense planet circling a Sun-like star in a 0.84-day orbit. In this case, there were no observable transit-timing variations, but because of the closeness to the star, the researchers were able to confirm the planet using ground-based radial-velocity observations that also showed the planet's mass. At 4.6 times Earth's mass and only 1.4 times its radius, the planet is dense enough to be unambiguously rocky \u2014 although its closeness to the star means that one side will be constantly molten rock. After careful studies of the host star and analyses of a year's worth of data, the Kepler team was able to pick out not only the dimming due to the transit, but also the cycle of brightening and dimming as the orbiting planet alternately showed its day and night sides towards Earth. \"It's phenomenal\" that such a subtle effect was detectable, says Batalha. It also shows what unexpected insights can be gleaned from Kepler's observations. \"This is our first data point down in the rocky regime,\" says Batalha, \"it's a huge milestone.\" The Kepler mission has detected other planets for which the mass cannot be determined. One example is Kepler-9d, a super-Earth found in the same system as Kepler-9b and -9c. Computer simulations by Guillermo Torres, an astronomer at the CFA, and his colleagues showed that eclipsing stars could not produce as good a fit to the observed light curve as the match produced by a super-Earth planet passing in front of the Kepler-9 star. That, says the team in a paper in  The Astrophysical Journal 7 , is the first validation of a planet using a general method that could be applied to any of the Kepler candidates, even those that don't show transit-timing variations and are too far from the star to be studied using the radial-velocity method. \"The probability of a planet is higher than the probability of a false positive,\" says Torres, \"there is a statistical argument.\" The method was used again to validate the sixth of the Kepler-11 candidates, Kepler-11g, a gas giant with an orbit of 118 days, far enough from the rest of the cohort that any transit-timing variations are too subtle to have been observed.  \n                The Kepler legacy \n              Because of the time needed for repeat observations of planets in Earth-length orbits, it will be years before the Kepler researchers can establish the frequency of planets in the cosmos. But that hasn't prevented other scientists from making preliminary estimates. In 2010, for example, a group led by Andrew Howard, an astronomer at the University of California, Berkeley, took the size distribution of planets found by the radial-velocity method and, by extrapolating to lower masses, predicted that Kepler will find that roughly 22% of stars are orbited by Earth-size planets 8 . Borucki is sceptical. \"They extrapolated, which is not a mortal sin but it's close,\" he says. But Lissauer is more sanguine. Ultimately, he says, results from the radial-velocity method can be combined with results from the transit method to produce a measured frequency of planets at different sizes, masses and compositions \u2014 from rocky Earths to gaseous Jupiters. And those data, in turn, will be invaluable for helping astronomers to understand the origin and evolution of planetary systems throughout our Galaxy. \"There's a whole load of good science in there,\" says Lissauer. The promise, says Batalha, is that Kepler will deliver a massive roster of objects for future generations to follow up on. \"Kepler will leave this legacy. People are going to use these data for decades,\" she says.  \n                 See Editorial  \n                 p.5 \n               Eugenie Samuel Reich is a reporter for Nature based in Boston. \n                     Kepler Mission \n                   \n                     Harvard-Smithsonian Center for Astrophysics \n                   \n                     Keck Telescope \n                   Reprints and Permissions"},
{"file_id": "471154a", "url": "https://www.nature.com/articles/471154a", "year": 2011, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "As the oceans rapidly grow more acidic, scientists are scrambling to discover how marine life is likely to react. The Friday night beers made Sam Dupont forget all about his sea urchins. Earlier that day, in April 2010, the young Belgian eco-physiologist had put a batch of urchin larvae into a bath of highly acidic water to see how their skeletons would fare. When nothing obvious happened after a few hours, Dupont decided to join some friends at the pub and check on the experiment later in the evening. But he didn't remember until Sunday, at which point he was sure that the precious larvae would be dead. But when Dupont returned to work at the Sven Lov\u00e9n Centre for Marine Sciences in Kristineberg, Sweden, on Monday, he found the larvae still swimming around in their tank. Their internal skeletons had dissolved away, but otherwise the creatures seemed to be functioning well. Dupont's chance finding underscores how much scientists have yet to learn about the growing threat of ocean acidification, which is caused by rapidly rising atmospheric concentrations of carbon dioxide. The acidity of sea water has climbed by 30% over the past 150 years, and some regions have already become corrosive enough to inhibit the growth of corals and other species for part of the year. According to projections, most creatures with calcium carbonate shells, such as mussels and snails, could run into problems within a few decades. By the end of this century, the acidification could even impede the growth of important groups of plankton, thus endangering entire marine ecosystems, from fisheries to coral reefs. Although the urchin experiment hints that some organisms are able to survive brief exposures to highly acidic water, other studies are revealing unexpected problems that might threaten even creatures without hard shells, such as fin fish. Preliminary work suggests that responses could be highly variable, depending on factors such as water temperature, a creature's evolutionary history and the availability and quality of food. Countries are only now revving up the coordinated research programmes needed to assess how marine ecosystems will react to the increasingly acidic waters. \"We simply have not conducted the basic experiments,\" says Richard Feely, an oceanographer with the US National Oceanic and Atmospheric Administration in Seattle, Washington, which last year launched a US$5.5-million programme of research into the problem. But with the current pace of acidification, scientists do not have much time to come up with answers.  \n                Carbon sink \n              Without the oceans and their vast ability to absorb carbon dioxide, Earth would be warming up much faster than it currently is. The seas take up about 9 billion tonnes of the gas each year \u2014 almost one-third of the 30 billion tonnes emitted globally. Once it enters the ocean, CO 2  reacts with water to produce carbonic acid, which releases positively charged hydrogen ions. Acidity is measured in pH, a logarithmic scale on which low numbers mean high acidity; neutral water has a pH of 7, but sea water is naturally alkaline, owing to the salts dissolved in it. Since the mid-nineteenth century, the average pH of ocean surface waters has dropped by 0.1 units, to a current value of about 8.1. Unless nations sharply curb their emissions, atmospheric CO 2  is expected to at least double from its preindustrial concentration by sometime in the second half of this century, and scientists project that ocean pH will fall by a further 0.3\u20130.4 or so units. Sea water could then contain at least 150% more hydrogen ions than it did at the onset of the industrial era. Those extra ions cause problems by binding with dissolved carbonate ions to form bicarbonate. With fewer free carbonate ions in the water, organisms struggle to absorb enough to build shells and skeletons made of calcite and aragonite \u2014 two different forms of calcium carbonate. And if sea water becomes permanently undersaturated with respect to those minerals, hard parts made of them will start to dissolve. \"There is absolutely no doubt that calcifying organisms will calcify less if conditions become more acidic,\" says Jean-Pierre Gattuso, an oceanographer at the National Centre for Scientific Research in Villefranche-sur-mer, France, who coordinates the European Project on Ocean Acidification (EPOCA). This has happened before. Some 55 million years ago, during an episode of extreme global warming driven by a spike in atmospheric CO 2 , the pH of sea water is thought to have dropped to levels similar to those expected at the end of the twenty-first century. Ocean sediment deposited during that period contains very little carbonate and no fossils of microorganisms with calcium carbonate shells, indicating that the sea water became too corrosive for calcifying algae such as deep-sea foraminifera, driving many to extinction 1 . Today, acidification is progressing at least ten times faster than it did 55 million years ago. \n               Click here for larger image \n               Researchers expect to see problems pop up first in polar seas, because cold water absorbs more CO 2  than warmer water (and because the melting of sea ice dilutes the concentration of carbonate ions). In 2008, measurements showed that regions of the Arctic Ocean had become undersaturated with respect to aragonite for part of the year 2 , and scientists suggest that further portions of the Arctic and Southern Oceans will cross that chemical threshold within the next decade. If CO 2  continues to rise at current rates, half of the Arctic Ocean could be undersaturated with respect to aragonite year-round by 2050 (see  'Into the red zone' ). Even in temperate waters, pH changes may already be having an impact. In the United States, the West Coast shellfish industry has asked scientists to study a dramatic rise in oyster mortality seen in hatcheries off Oregon and Washington since 2005. During the summer, upwelling currents in these seas carry deep-ocean water, naturally under-saturated with respect to calcium carbonate, onto the continental shelf. Researchers wonder whether the acidification of surface waters has combined with these upwelling currents to cause some of the recent shellfish problems. At the moment, scientists can offer few conclusions. Although they can make broad predictions about the progress of ocean acidification, they know very little about how it will affect marine animals in different climate zones, alter the composition of ecosystems and, ultimately, influence the marine food web. To complicate matters, acidification is just one of many environmental changes confronting marine life. Organisms also face increasing stress from ocean warming, pollution, fishing pressure, sea-ice loss and shifting patterns of currents and mixing of deep and shallow water. Some scientists think that progressive ocean acidification will limit the ability of marine organisms to survive such stresses.  \n                Sea of variables \n              Dupont and his colleagues in Kristineberg tried to answer some of the basic questions about acidification by filling 264 tanks with a range of organisms, including scallops, halibut, brittle stars, sea urchins and lobsters. In a four-month lab experiment that ended this week, they observed the performance of the various animals in each combination of six temperatures (6\u201318 \u00b0C) and two pH values (8.1 and 7.7), measuring growth, respiration, shell and tissue structure, internal pH and survival rates. They are just starting to analyse the data. With his previous urchin test, Dupont says, \"we've seen that some species can cope with extremely low pH values, at least in the short term\". But that might not be true for longer exposures and higher temperatures. \"We expect that the response to combined stressors is very site- and species-specific.\" A separate study of two populations of spider crabs ( Hyas araneus ) suggests that how animals respond to acidification depends on their climate zone. In lab experiments, the growth rate and fitness of larvae from the North Sea decreased markedly in acidic waters, whereas an Arctic population from 3,000 kilometres farther north was more sensitive to warming than increased acidity 3 . Even individuals from the same species and climate zone can react quite differently. In one lab study 4 , blue mussels ( Mytilus edulis ) from the North Sea showed a 25% drop in calcification rates at values of atmospheric CO 2  of 740 parts per million, about what is expected by 2100 if emissions are not curbed. But a different population seemed to do just fine in such waters: these mussels live in the nearby Baltic Sea, where CO 2 -rich waters well up for parts of the year, causing the pH in the sea to drop as low as 7.5 (ref.  5 ). Frank Melzner, an environmental physiologist at the Leibniz Institute of Marine Sciences (IFM-GEOMAR) in Kiel, Germany, who led the Baltic study, suggests that the mussels can survive there because they have developed the physiological capacity to regulate the pH in their cells and build up a protective layer of proteins and carbohydrates that shelters their shells. But only well-nourished organisms can afford such defences, he says. \"It seems that some organisms can biologically control the effects surprisingly well \u2014 but it certainly requires energy.\" There is plenty of food in the Baltic. Where nutrition is less abundant, populations seem to decline when faced with increased acidity. That is one of the lessons from a study off the Italian island of Ischia in the Gulf of Naples, where underwater volcanic vents have been spewing CO 2  into the comparatively food-poor Tyrrhenian Sea for millennia. A survey of life around the site found that normally common calcifying organisms, including corals and sea urchins, were absent from the spots with low pH. Instead, the researchers discovered a thriving community of species that are immune to elevated CO 2  or even benefit from it, such as sea grasses and invasive algae 6 . Jason Hall-Spencer, a marine biologist at the University of Plymouth, UK, who oversees the research off Ischia, says that the massive difference between the responses of animals there and in the Baltic illustrates how little is known. To really understand the problem, \"you'd like to test the combined effects of ocean acidification and other stressors on hundreds of species and their interactions\", he says. \n               Click here for larger image \n               And calcifying organisms are not the only creatures at risk (see  'Future shocks' ). Even fish could be vulnerable: experiments have shown that elevated CO 2  impairs the sense of smell in juvenile clownfish ( Amphiprion percula ), which could make it difficult for them to find the sea anemones in which they like to live 7 .  \n                Growing urgency \n              In the past few years, nations have started to devote resources to the research challenge. Europe's \u20ac16.5\u2013million (US$22.9-million), four-year EPOCA project, which began in 2008 and encompasses 31 laboratories in 10 countries, aims to monitor the effects of ocean acidification on marine organisms at various scales, from cells to ecosystems and then across the entire globe. One of the programme's priorities is to determine whether there are any tipping points, beyond which any increase in acidity would hurl marine ecosystems towards catastrophic changes. In the United States, President Barack Obama's administration plans to submit a proposal to Congress in the next month or so for an integrated national research programme on ocean acidification, which would draw together researchers from across the federal government. The president's 2011 budget called for $11.6 million for research on the subject, but Congress has yet to pass a budget for the current fiscal year. National research programmes are also under way in Germany, Britain, Japan, China, South Korea and Australia. The largest field experiment conducted so far is an offshore study by EPOCA, involving algae and bacteria in large floating containers exposed to varying levels of CO 2 . The research took place between May and July last year, off the island of Spitsbergen in the Arctic Ocean. A group of 35 researchers collected daily measurements of 45 variables affecting the 'mesocosm' within the oversized containers, from nutrient cycling to trace-gas production by calcifying algae. The experiment is to be repeated in April and May this year off Bergen in Norway, where the team hopes to observe how acidification affects a bloom of coccolithophorids \u2014 important calcifying algae that produce dimethyl sulphide, a trace gas that seeds the formation of clouds. \"We need to understand much more about how ocean acidification affects real ecosystems than we can hope to learn from dose-response experiments on isolated species,\" says Ulf Riebesell, a biological oceanographer at the IFM-GEOMAR, who leads the study. A sense of urgency is propelling these studies. Governments have shown no signs of stemming CO 2  emissions any time soon, and there is talk of tackling the problem of methane and other greenhouse gases first, leaving the tougher issue of CO 2  for a later generation. That might slow the global temperature rise, but it won't keep the seas from growing ever more corrosive.  Quirin Schiermeier is a senior reporter with Nature in Munich. \n                     European Project on Ocean Acidification (EPOCA) \n                   \n                     NOAA Ocean Acidification \n                   Reprints and Permissions"},
{"file_id": "471282a", "url": "https://www.nature.com/articles/471282a", "year": 2011, "authors": [{"name": "Ewen Callaway"}], "parsed_as_year": "2006_or_before", "body": "When Judy Mikovits found links between chronic fatigue syndrome and a virus, the world took notice. Now, she's caught between the patients who believe her work and the researchers who don't. On a sunny January afternoon in Santa Rosa, California, a small crowd waits patiently for Judy Mikovits to arrive. She is scheduled to deliver a talk on a mysterious virus called XMRV, which she believes underlies chronic fatigue syndrome. Although she's two hours late \u2014 held up by fog at San Francisco International Airport \u2014 not a single person has left. And when she arrives, they burst into applause.  To a rapt audience, she gives a chaotic and wide-ranging talk that explores viral sequences, cell-culture techniques and some of the criticisms that have been thrown at her since she published evidence 1  of a link between XMRV and chronic fatigue in 2009. Afterwards, Mikovits is swarmed by attendees. A middle-aged woman who spent most of the talk in a motorized scooter stands up to snap pictures of her with a digital camera. Ann Cavanagh, who has chronic fatigue and has tested positive for XMRV, says that she came in part for information and in part to show her support for Mikovits. \"I just wish there were a hundred of her,\" Cavanagh says.  The event was \"surreal\", says Mikovits, a viral immunologist at the Whittemore Peterson Institute for Neuro-Immune Disease (WPI) in Reno, Nevada. She is discomfited by the attention from patients, which at times borders on adulation. But her reception among scientists has been markedly cooler. Numerous follow-up studies have found no link between the virus and the disease; no group has published a replication of her findings; and some scientists argue that XMRV is an artefact of laboratory contamination. Now, even some of Mikovits's former collaborators are having second thoughts. Mikovits has dug in, however, attacking her critics' methods and motives. She says that their distrust of her science stems from doubts about the legitimacy of chronic fatigue syndrome itself. Chronic fatigue, also known as myalgic encephalomyelitis, affects an estimated 17 million people worldwide, but it is extremely difficult to diagnose. Many with the disorder are told that their symptoms \u2014 which include exhaustion, joint and muscle pain, cognitive issues, and heart and respiratory problems \u2014 are psychosomatic. \"I had no idea there was that much bias against this disease,\" Mikovits says.  The stakes are high and many are taking the risks seriously. Several countries have barred people with chronic fatigue from donating blood in case the virus spreads (see  'Something in the blood' ). And the US government has launched a US$1.3-million study to investigate the link. Patients are already being tested for XMRV, and some are taking antiviral drugs on the assumption that the virus causes chronic fatigue by attacking their immune defences. Many say that such action is premature, but Mikovits is steadfast. \"We're not changing our course,\" she says.  \n                First findings \n              In October 2007, Mikovits attended a prostate-cancer meeting near Lake Tahoe, Nevada, where she met Robert Silverman, a virologist at the Cleveland Clinic in Ohio. Silverman co-discovered XMRV, which stands for xenotropic murine leukaemia virus-related virus 2 . While examining human prostate tumours, he and his collaborators found genetic sequences that resemble retroviruses found in the mouse genome. Like all retroviruses, XMRV rewrites its RNA genome into DNA on infection, then slips the DNA into the genomes of host cells. Ancient remnants of such viruses litter animal genomes. But the only active retroviruses conclusively linked to human disease are HTLV-1, which causes leukaemia, and HIV. At the meeting, Silverman was presenting research linking XMRV to deficiencies in a virus-defence pathway. Mikovits recalled that the same pathway was weakened in some patients with chronic fatigue. She wondered whether the prostate-tumour virus could also be behind chronic fatigue. After the meeting, Silverman sent Mikovits reagents to test for XMRV.  The idea excited Mikovits, but she had other priorities. After stints in industry and at the US National Cancer Institute (NCI) in Maryland, she had recently joined the WPI to lead its research programme. The WPI was founded in 2006 by physician Daniel Peterson, an expert on chronic fatigue, and by Annette Whittemore, the wife of a well-connected Nevada businessman, whose daughter Andrea has had chronic fatigue for more than 20 years. The Whittemores spent $5 million establishing the WPI, and several million more to support Mikovits's research, which has attracted few other grants.  At the WPI, Mikovits established a sample collection from Peterson's patients and began screening it for signs of an infection. A litany of pathogens has been linked to chronic fatigue over the years, including Epstein-Barr virus, Borna disease virus, human herpes virus 6 and HTLV-2. None panned out. Still, the disorder bears some hallmarks of an infection. Many patients report acute illness before chronic symptoms appear, and their bodies often show signs of an immune system at war. The disease can also crop up in apparent outbreaks, including one characterized by Peterson near Lake Tahoe in the 1980s.  Just before Christmas 2008, Mikovits turned her attention to Silverman's reagents. She and her postdoc, Vincent Lombardi, known as Vinny, asked a graduate student to test for XMRV DNA in white blood cells from some of the most seriously ill people being studied at the WPI. The first try turned up just two positives out of 20. But by tweaking the conditions of the test, Mikovits says her team found XMRV in all 20. \"Vinny and I looked at each other and said, 'Well, that's interesting',\" she says. They spent the next few weeks convincing themselves that they were onto something, and soon conscripted Silverman and Mikovits's former mentor at the NCI, Frank Ruscetti, to help prove that XMRV infection was behind chronic fatigue.  \"We really retooled our entire programme and did nothing but focus on that,\" she says. They kept the effort under wraps, dubbing it 'Project X'. Even Peterson and the Whittemores weren't clued in. Mikovits says that the secrecy was necessary because her team also found XMRV in the blood of some healthy people, raising concerns about blood products. She hoped to build an airtight case because she worried that sceptical public-health officials would undermine her work. In May 2009, the team submitted a paper to  Science   reporting the identification of XMRV genetic material in two-thirds of the 101 patients with chronic fatigue they had tested and in 3.7% of 218 healthy people. They also included data suggesting that infected white blood cells could pass the virus on to uninfected cells. Reviewers wanted more evidence: a clear electron micrograph of virus-infected cells, proof that patients mounted an immune response to the virus, an evolutionary tree showing XMRV's relationship to other viruses and the locations where viral DNA was integrating into patient genomes. Mikovits's team went to work. \"None of us took any time off, not even a weekend,\" she says. They resubmitted the paper in early July with everything the reviewers had asked for, except the DNA integration sites, which many scientists consider a gold standard in proving a retroviral infection. Later that month, NCI officials who had learned about the work invited Mikovits to give a talk at a closed-door meeting with other XMRV researchers and government scientists. \"When I finished speaking you could've heard a pin drop,\" she says. Mikovits says she thinks at least one of her manuscript's reviewers was at the meeting, because soon after, she got a call from a  Science   editor. Their paper had been accepted. Jonathan Stoye, a retrovirologist at the MRC National Institute for Medical Research in London, wrote a commentary about the paper for  Science 3 . He had never heard of Mikovits, but Frank Ruscetti's name on the paper gave him confidence, he says, and \"if it were true, it was clearly very important\". Stoye's co-author John Coffin, a retrovirologist at Tufts University in Boston, Massachusetts, says he was satisfied with the data and thought it was time to \"let the field and public chew on them\".  The BBC, US National Public Radio,  The New York Times ,  The Wall Street Journal   and dozens of other news outlets covered the research. \"Prostate cancer pathogen may be behind the disease once dubbed 'yuppie flu',\"  Nature   announced on its news website the day the paper came out. Phoenix Rising, a forum for patients with chronic fatigue that has become a hub for all things XMRV, called the work a \"game changer\", and patients flocked to learn more about a virus that they hoped would explain their condition. But others, including Britain's leading chronic fatigue patient group, urged caution until more research buttressed the link. The first negative findings started to arrive in January 2010 \u2014 failing to find XMRV in 186 people with chronic fatigue from the United Kingdom 4 . A month later, a team including Stoye published a paper 5  showing no evidence of XMRV in more than 500 blood samples from patients with chronic fatigue and healthy people. One day later, the  British Medical Journal   accepted a paper reporting more negative results in Dutch patients 6 . Studies began piling up so fast that Coffin made a scorecard to show at talks. \"I've lost count now,\" he says.  Mikovits says that the discrepancies can be explained by differences in the geographical distribution of XMRV or in the methods used.  The most common way to detect XMRV is PCR, or polymerase chain reaction, which amplifies viral DNA sequences to a level at which they can be identified. Mikovits and her team used this method to detect XMRV in some of their patients, but she contends that the most sensitive way to detect the virus is to culture patients' blood cells with a cell line in which the virus replicates more quickly. This should create more copies of the virus, making it easier to detect with PCR and other techniques. She says that none of the negative studies applied this method exactly, a fact that annoys her. \"Nobody's tried to rep-li-cate it,\" she says, sounding out each syllable for emphasis.  In summer 2010, some evidence emerged in Mikovits's corner. Harvey Alter, a hepatitis expert at the NIH's Clinical Center, and his team identified viruses similar to XMRV in 32 of 37 people with chronic fatigue and in 3 of 44 healthy people. They were preparing to publish their results in the  Proceedings of the National Academy of Sciences . But scientists at the Centers for Disease Control and Prevention (CDC) in Atlanta, Georgia, were about to publish a negative report. The authors delayed publication of both papers 7 , 8  for several weeks to assess discrepancies. The move agitated Mikovits as well as the chronic-fatigue community, who suspected that important data were being suppressed.  When Alter's work came out in late August 7 , Mikovits was ecstatic, and the WPI released a YouTube video of her touting it. For other researchers, however, the new paper had shortcomings. The viral sequences from Alter's paper differed from XMRV, says Greg Towers, a retrovirologist at University College London. \"He doesn't get variation, he gets a totally different virus.\" Towers says that mouse DNA, which is chock-full of virus sequences like those Alter's team found, probably contaminated their samples, which were collected in the 1990s. But Alter says that his team found no contamination from mouse DNA and recovered the same viral sequences from the same patients sampled a decade later.  Contamination became a dirty word for Mikovits. Just before Christmas 2010,  Retrovirology   published four papers 9 , 10 , 11 , 12  that highlighted laboratory contamination as a possible explanation for her findings. One showed, for example, that mouse DNA contaminates an enzyme from a commercial kit commonly used for PCR. Coffin, an author on two of the  Retrovirology   papers, urges caution against over-extrapolating. These papers do not say that contamination explains Mikovits's results, he says, just that extreme care is required to avoid it. Towers and his colleague Paul Kellam, a virologist at the Wellcome Trust Sanger Institute near Cambridge, UK, are less charitable, however. Their study 12  showed that the XMRV sequences that Mikovits and Silverman had extracted from patients lacked the diversity expected of a retrovirus that accumulates mutations as it passes between patients. \"This doesn't look like an onwardly transmittable infectious virus,\" says Kellam. A press release for the paper issued by the Sanger Institute put it more bluntly: \"Chronic fatigue syndrome is not caused by XMRV.\"  Mikovits is riled when the topic turns to Towers's paper over dinner one night in Reno \u2014 \"Christmas garbage\", she calls it. Contamination cannot explain why her team can reproduce its results both in her lab in Reno and at Ruscetti's at the NCI, she says. Her team checks for contamination in reagents and in the cells it grows the patients' samples with. She says that her team has also collected viral sequences that will address Towers's and Kellam's criticism but that it hasn't yet been able to publish them. Meanwhile, an unpublished study of patients in Britain with chronic fatigue bears out the link to XMRV, she says. \"I haven't for one second seen a piece of data that convinced me they're not infected.\"  Jay Levy, a virologist at the Univer\u00adsity of California, San Francisco, has a window in his closet-sized office that looks out into the laboratory where, in the 1980s, he became one of the first scientists to isolate HIV. After his discovery was scooped by other researchers, Levy turned his attention to chronic fatigue and started a long but fruitless search for an infectious cause. Now, Levy is putting the finishing touches on what could be the most thorough response yet to Mikovits's  Science   paper, adopting the same cell-culture techniques to detect the virus and using samples from the same patients. He's done this with the help of Daniel Peterson, who left the WPI in 2010 for what Peterson says are \"personal reasons\". Peterson has questioned the institute's singular pursuit of XMRV, a research direction that was pursued without his consultation. Mikovits says that she kept the XMRV work secret from Peterson over fears he would tell his patients, and left his name off the original  Science   manuscript until a reviewer questioned the omission. When asked whether that episode contributed to his departure, he says, \"I was surprised at the secrecy and lack of collaboration.\" As for his motivation to team up with Levy: \"I'm just trying to get to the truth. It's my only motive, because this is such a deserving group of patients who need to know what's going on.\"  Others, too, are rallying for a definitive answer. Ian Lipkin, a microbial epidemiologist at Columbia University in New York, has a reputation for getting to the bottom of mysterious disease\u2013pathogen links. His team debunked the association between Borna disease virus and chronic fatigue, for example. Now he is spearheading the $1.3-million effort funded by the US government. He is leaving the testing to three labs: Mikovits's at the WPI, Alter's at the NIH and the CDC. Each will receive coded samples of white blood cells and plasma from 150 patients with chronic fatigue and from 150 healthy controls. The labs will test for XMRV using their method of choice. Lipkin will crunch the data and unblind the samples. But even if a study confirms the link to chronic fatigue, it won't be able to determine whether the virus is the cause. XMRV could, for example, be an opportunistic infection affecting those whose immune systems are already dampened by chronic fatigue. Even Mikovits can only hypothesize as to how it might cause disease.  The virus might not even exist as a natural infection. At a retrovirus conference this month in Boston, Massachusetts, Coffin and his colleague Vinay Pathak at the NCI in Frederick, Maryland, presented data showing that XMRV emerged in the 1990s, during the development of a prostate-tumour cell line called 22Rv1. Developing the line involved implanting a prostate-tumour sample into mice, retrieving cells that might divide indefinitely and repeating the process. But looking back at DNA samples taken throughout the cell-line's development showed that human cells became infected only after passing through several different mice. Importantly, XMRV's sequence seems to have come from two different mouse strains. \"They just sort of snapped together like two puzzle pieces,\" says Coffin, an event extremely unlikely to have happened twice. XMRV sequences retrieved from patients with prostate cancer and chronic fatigue \u2014 including some who have had chronic fatigue since the mid-1980s \u2014 are nearly identical to the virus from 22Rv1 cells. The implication, says Coffin, is that this virus, born in a laboratory, has probably been infecting samples for more than a decade, but not people. \"Although people on the blogs aren't going to believe me, I'm afraid this is by far the most reasonable explanation for how XMRV came to be,\" says Coffin, who hoped that the association with chronic fatigue would pan out and still thinks some pathogen other than XMRV could explain the disease. Silverman, who no longer works with Mikovits, says that he wasn't using 22Rv1 cells when XMRV was discovered. Nonetheless, the work has rattled his confidence in XMRV's link to both prostate cancer and chronic fatigue. Mikovits, however, is undeterred. The WPI owns a company that charges patients up to $549 to be tested for XMRV, and Mikovits believes that patients who test positive should consult their doctors about getting antiretroviral drugs normally prescribed to those with HIV. Levy and others worry that she is overreaching. \"That's scary for me. These antiretroviral drugs are not just like taking an aspirin,\" he says. Mikovits argues that they might be some patients' only hope. \"The people who we know they're infected should have a right to get therapy,\" she says, \"They have nothing. They have no other choice.\"  \n                Context and debate  \n              Back in her Reno laboratory two days after the talk in Santa Rosa, Mikovits examines a stack of small plastic flasks under a microscope. Some contain patient cells that she hopes will turn into cell lines and churn out XMRV. \"On Wednesdays I get to take care of my cells, and that's where I'm the happiest,\" she says. She has just come off the phone from a sobbing patient infected with XMRV whose symptoms had worsened. \"They call me every single day,\" Mikovits says. \"I don't do science any more. I spend so much time trying to understand the patients, to understand this disease. People have moved to Reno to be here,\" she says. They've left gifts: stuffed animals, and stacks of bumper stickers that say \"Today's Discoveries, Tomorrow's Cures\" and, more boldly, \"It's the virus XMRV\". Mikovits clearly shares in the frustration of those with chronic fatigue who have been marginalized over the years and told that their disease is not real. She says that this disbelief in the disorder drives the criticism of her work. Kellam and the others say that this isn't true. They don't deny the existence of the syndrome or even the possibility of an infectious origin. \"What we're trying to understand is the aetiology,\" Kellam says. \"It's a scientific debate.\"  Mikovits says that she's analysed all the papers critical of her work and found flaws in each of them. Nevertheless, she's quick to endorse findings that support her work. She claims that Coffin and Pathak's study, for example, \"says nothing about human infection\". Yet new work presented at a different meeting that found XMRV using next-generation DNA sequencing offers \"no doubt it's not contamination \u2014 that the whole story's real\", she says. Despite the growing choir of sceptics, Mikovits says that she has simply seen too many data implicating XMRV and other related viruses in chronic fatigue to change her mind. For her supporters, that steadfastness offers legitimacy and hope. \"The scientists are moving forward,\" she announced at her talk in Santa Rosa, \"and I think the politics will go away shortly.\" The crowd responded with vigorous applause.\n Ewen Callaway writes for  Nature   from London. \n                     New challenges for viral link to XMRV \n                   \n                     FDA advised to turn away blood donors \n                   \n                     The confusion about chronic fatigue \n                   \n                     The Whittemore Peterson Institute for Neuro-immune Disease \n                   \n                     CDC Information on XMRV \n                   Reprints and Permissions"},
{"file_id": "471286a", "url": "https://www.nature.com/articles/471286a", "year": 2011, "authors": [{"name": "M. Mitchell Waldrop"}], "parsed_as_year": "2006_or_before", "body": "In 1861, James Clerk Maxwell unified electricity, magnetism and light. Experiments under way today could inch physicists closer to combining everything else. When it happens \u2014 if it happens \u2014 don't look for Hollywood-style drama. Physicists at the Large Hadron Collider (LHC) outside Geneva in Switzerland won't suddenly gasp with astonishment, and their monitors won't flash the message, \"Higgs boson detected.\" Instead, the discovery will unfold over the course of months. Computers will trawl through petabytes (10 15  bytes) of collision data in search of a handful of distinctive events that might signal their quarry's existence, while physicists cross-check every candidate. Only when they have accumulated enough events to be sure \u2014 maybe a dozen \u2014 will they publicly proclaim the discovery of the sought-after Higgs. Even so, the announcement will be dramatic \u2014 and timely. Exactly 150 years ago, the Scottish physicist James Clerk Maxwell showed that three apparently separate phenomena \u2014 electricity, magnetism and light \u2014 are different aspects of one phenomenon, today known as electromagnetism (see  page 289 ). The Higgs discovery could take that unification a giant step further by filling in the last and most critical piece of the 'standard model', an extension of Maxwell's equations that encompasses three of the four forces of nature: electromagnetism and the weak and strong forces that act on subatomic particles. The Higgs boson is thought to interact with electrons, quarks and other fundamental particles, endowing them with mass \u2014 and thus making it possible for the standard model to describe the Universe as we know it. This puts the standard model as it is today in the same position as Maxwell's theory before experiments demonstrated the existence of electromagnetic waves, says Frank Wilczek, a physicist at the Massachusetts Institute of Technology in Cambridge, and co-recipient of the 2004 Nobel Prize in Physics for his part in creating the model. \"It looks good, lots of its predictions have been verified, but the most dramatic new thing remains to be verified.\" But even if the Higgs boson is discovered as predicted, physicists will not be satisfied. The ultimate goal is a unification theory that would reveal how all observed particles and forces are just different manifestations of a single underlying system, which can be expressed within a common mathematical framework. Such an elegant result is not possible with the standard model, which includes the strong force that binds the atomic nucleus only as an afterthought, and has nothing at all to say about gravity. The standard model also has no explanation for dark matter, an invisible substance that outweighs the ordinary matter in stars and galaxies by a factor of roughly five. Although physicists agree that some kind of larger unification is needed, they don't know what form that should take. For four decades, nearly as long as the standard model has existed, researchers have been speculating about ways to extend it with exotic ideas such as supersymmetry, extra dimensions and holographic space-time. \"The situation is that there are a bunch of hypotheses on the table, most of them not new, with no experimental support for any of them,\" says Lee Smolin, a physicist at the Perimeter Institute for Theoretical Physics in Waterloo, Canada. \"The good news,\" says Smolin, \"is that the experiments are finally being done.\" Within a few years, thanks to the LHC and other experiments, physicists should have a much clearer idea of which theoretical notions are real and will take their place in the ultimate unification.  \n                Supersymmetry \n              If the Higgs boson turns out to be exactly what is predicted from the standard model, it will have zero internal angular momentum ('spin-0'), and a mass somewhere between 115 and 180 billion electron volts (GeV) in the energy units favoured by particle physicists. But such a match would be pretty boring, says John Ellis, a theoretical physicist at King's College London. It would be much more fun, he thinks, if the LHC physicists didn't find anything. \"After all those years of speculating, we finally look under the appropriate lamp post, and it's not there at all!\" says Ellis. That would force the theorists back to the drawing board, \"but there are various drawing boards to go back to\". There could be more complicated ways of generating mass, or something more unexpected. \"That would be very exciting,\" says Ellis. Alternatively, he says, it is entirely possible that the LHC will turn up not one Higgs particle, but a whole family of them. That would be a sign of supersymmetry, a theory that predicts a zoo of as-yet unobserved 'superparticles', one to match each of the 25 particles in the standard model \u2014 the force-carrying bosons such as photons, gluons and the Higgs, and the fermions, such as quarks and electrons, which make up matter. These superpartners would be heavy \u2014 at least 600 GeV. Supersymmetry appeals to physicists because it provides a unified mathematical description of bosons and fermions, which otherwise seem utterly unrelated. And the theory would greatly strengthen the case for a 'grand unification' of the strong, weak and electromagnetic forces, leaving only gravity unexplained. In the standard model, the strong force's interaction strength \u2014 expressed in terms of a constant analogous to electric charge \u2014 is very different from the strengths of the weak and electromagnetic forces. But if supersymmetry is assumed to be true, quantum corrections show all three strengths to be exactly equal \u2014 just as would be expected if the forces are actually one. Supersymmetry would also solve some problems of other grand unified theories, such as their predictions that the proton should be unstable. The presence of the superpartners in calculations tends to suppress proton decay, leading to a decay rate far below the limits currently set by experiment. Finally, and perhaps most importantly from an observational standpoint, supersymmetry might very well provide an explanation for dark matter. This invisible cosmic haze behaves like a swarm of massive particles that interact very weakly with ordinary atoms, and has so far been detected only by its gravitational influence on visible stars and galaxies. No particle in the standard model has the right properties \u2014 but several of their predicted superpartners do. If one of them is indeed the dark-matter particle, it may soon be observed not only at the LHC, but also in one or more of the dark-matter detectors now operating around the world (see    Nature   doi:10.1038/news.2011.125; 2011 ). But just because supersymmetry promises many wonderful solutions to current problems, that's no guarantee it is true. \"If supersymmetry is seen, that's absolutely fabulous,\" says Smolin. \"But if not \u2014 well, that will be fabulous, too. It's always better to know.\"  \n                Extra dimensions \n              Conversations with LHC physicists can become surreal \u2014 especially when they start talking with a straight face about finding that staple of science fiction, extra dimensions. One big reason that they take this prospect seriously is that extra dimensions are predicted by string theory, by far the most popular attempt at unification beyond the standard model. String theory posits that the fundamental particles are actually vibrating threads of energy. Since it was developed in the late 1960s, string theory has remained a mental exercise, with no physical evidence to back it up. But it has proved remarkably compelling even so. It predicts the existence of forces that look a lot like the strong, weak and electromagnetic forces of the standard model. It incorporates supersymmetry in a natural way. And it automatically includes gravity: string-theory equations show that closed loops of string would behave like gravitons, the particles postulated to carry the force of gravity. \"It combines all the principles of physics we know,\" says Nathan Seiberg, a string theorist at the Institute for Advanced Study in Princeton, New Jersey. \"That's huge. There's no other suggestion that even comes close.\" The extra dimensions arise because string theory has its most natural formulation in 11 dimensions, only 4 of which would be observable to us: the 3 dimensions of space and 1 of time. The missing dimensions are easy to explain, says Seiberg: the theory allows them to be so tightly rolled up that they're invisible under ordinary circumstances. The LHC could detect those extra dimensions if the particles generated by collisions have enough energy \u2014 and therefore short-enough quantum wavelengths \u2014 to start spiralling around those tightly curled dimensions. The energy of that spiralling would show up as mass, according to Einstein's famous relativity theorem. So LHC physicists could detect whole families of higher-mass duplicates of the standard-model particles. An alternative scenario, also an outgrowth of string theory, suggests that what we perceive as three-dimensional space is actually a kind of membrane floating in a higher-dimensional space. We never notice the extra directions because all the particles of the standard model are confined to the membrane. But LHC collisions might be energetic enough to let in a burst of gravitational energy from outside our membrane. The result would be a jet of collision products spraying off to one side of the collision point, but apparently with nothing to balance it on the other side \u2014 as if the jet had been hit by a bolt out of nowhere. LHC physicists have calculated the experimental signatures of all these extra-dimensional phenomena and more, says Albert de Roeck, deputy spokesman of the Compact Muon Solenoid experiment, one of the LHC's two big collision detectors. \"But the moving target is to know the scale\" of the extra dimensions, he says. If they are rolled up too tightly, on a scale smaller than 10 \u221219  metres, then the energy required to probe them will be beyond the LHC's reach. That is a distinct possibility, which is why physicists consider it a long shot to observe extra dimensions at the LHC. \"I'm not holding my breath,\" says Michael Duff, a physicist at Imperial College London. \"My bet is that if there are extra dimensions, they're at the Planck scale\" of 10 \u221235  metres, where quantum mechanics and gravity are thought to unite in some still-unknown way. Nonetheless, the pursuit of extra dimensions is considered well worth the effort. Their existence would not prove string theory correct; it is perfectly possible to have extra dimensions without strings. But confirming one of its major predictions would considerably bolster the case for string theory.  \n                Holography \n              As nice as that would be, says Seiberg, string-theory practitioners are left with a gnawing sense that something is missing. \"We know how to calculate a lot of things in string theory,\" he says. \"But we don't have a conceptual basis for it \u2014 a set of fundamental principles from which everything follows.\" This quest for deeper principles is shared by physicists of all kinds, not just string theorists. One idea that has drawn a lot of attention follows from a startling theoretical discovery made by Stephen Hawking at the University of Cambridge, UK, and others in the 1970s: quantum effects in the space around a black hole cause it to emit radiation as if it were hot, even though black holes are supposed to swallow mass and energy, not spit it out. \"That's an amazing result,\" says Carlo Rovelli, a physicist at the University of the Mediterranean in Marseilles, France. Somehow, the three apparently separate phenomena of gravity, quantum mechanics and thermodynamics \u2014 the science of heat \u2014 are intertwined, he says. \"And we have still not understood why in a deep way.\" Efforts to understand this result have led theoretical physicists in some strange directions. According to standard thermodynamics, for example, any object's temperature is related to its entropy: a quantity that measures the amount of information available to outside observers about the arrangement and motions of the object's constituents. But there is no way for outside observers to get any information from a black hole, because anything closer to the black hole than a surface known as the event horizon is cut off from the rest of the Universe. So if a black hole is governed by thermodynamics, as Hawking and others demonstrated, all the information about its three-dimensional interior must somehow be encoded on its two-dimensional event horizon. Furthermore, after decades of analysis and generalization of this argument, many physicists now believe that it applies to any three-dimensional volume, from black holes to empty space: the volume's entire information content can be encoded in its two-dimensional surface. Or to put it another way, the ultimate unified theory of everything should describe our apparently solid three-dimensional world in terms of a lower-dimensional reality. Our Universe would emerge from the theory like a three-dimensional optical image from a two-dimensional hologram. Although this 'holographic principle' might be an element of some ultimate unified theory, it does not by itself say what that theory should be. And not every physicist buys it. \"It's interesting and provocative, but extremely vague,\" says Wilczek. Mathematically, at least, a version of the holographic principle does apply in a string-theory model known as AdS/CFT duality, in which it has been studied extensively. And perhaps more importantly, it might be possible to test the idea. Craig Hogan, director of the Fermilab Center for Particle Astrophysics in Batavia, Illinois, has suggested that if the holographic principle is true, quantum effects could produce a kind of 'holographic noise' in light beams. The effect would be minute, says Hogan, but might be detectable by the kind of ultraprecise laser interferometers already used in gravitational wave experiments. He and his colleagues are already building hardware to test whether this effect will work as expected, says Hogan, and if it does, they hope to proceed next year with a full-scale experiment, projected to cost about US$2 million over three years. \"That's cheap, as these things go,\" says Hogan \u2014 and that is probably just as well. \"These are tests of physics we don't know, so by definition, it's exploratory\". Among all the theories that could help to unify physics \u2014 from strings to holography and even more esoteric concepts \u2014 there is the possibility that many may turn out to be the same idea, viewed from different perspectives. But the only way to find out is to do the tests. \"The LHC gives us hope for a huge leap,\" says Seiberg. \"In a few years we should be a lot smarter.\" \n               \n                 Further reading  \n               \n             \n                 See Editorial  \n                 page 265 \n               M. Mitchell Waldrop is an editor for Nature in Washington DC. \n                     Nature News Special: The LHC \n                   \n                     Large Hadron Collider \n                   \n                     LHC overview \n                   \n                     John Schwarz, Introduction to String Theory \n                   \n                     Ed Witten, Introduction to String Theory \n                   \n                     Raphael Bousso: The holographic principle \n                   \n                     The Fermilab Holometer \n                   Reprints and Permissions"},
{"file_id": "469148a", "url": "https://www.nature.com/articles/469148a", "year": 2011, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "Japanese hospitals are using near-infrared imaging to help diagnose psychiatric disorders. But critics are not sure the technique is ready for the clinic. In a room full of psychiatrists in downtown Tokyo, I prepare to have my mental health assessed. No probing questions are asked. Instead, I don an odd type of swimming cap, criss-crossed with cables and studded with red and blue knobs. At the flick of a switch, the 17 red knobs send infrared light 2 to 3 centimetres into my brain, where it is absorbed or scattered by neurons. Photoreceptors in the 16 blue knobs retrieve whatever light bounces back to the surface. Buried in the signals, say the researchers operating the system, are clues that can distinguish depression, bipolar disorder, schizophrenia and a normal state of mind. More than 1,000 people have already been subjects of the technique, called near-infrared spectroscopy (NIRS) and developed by Masato Fukuda, a psychiatrist and neuroscientist at Gunma University Hospital in Maebashi, and the Hitachi Medical Corporation in Tokyo. Most of those were research subjects. But since April 2009, when NIRS was approved by the health ministry as an \"advanced medical technology\" to assist psychiatric diagnoses, more than 300 people have paid \u00a513,000 (US$160) out of their own pocket to access the technique. The University of Tokyo Hospital, one of eight leading Japanese research hospitals now offering NIRS diagnostic neuroimaging, found demand for it to be so high that the hospital stopped taking appointments twice. Gunma University Hospital is fully booked to the end of March. \"We've been overwhelmed by requests,\" says Fukuda. The appeal of NIRS is its promise of fast, clear-cut diagnoses of psychiatric conditions which, with their messily overlapping symptoms, are frequently diagnosed wrongly or not diagnosed at all. US studies, for example, found that some 70% of bipolar patients were initially misdiagnosed 1 , 2 . As for patients, says Fukuda, \"They want some kind of hard evidence,\" especially when they have to explain absences from work. NIRS could offer an objective measure of mental health reliable and convenient enough for routine use in the clinic. Fukuda says that it can help point to a diagnosis much like a chest X-ray might be used to help diagnose pneumonia or an electrocardiogram to define a heart problem. Aside from Fukuda and a group of doctors in Japan, however, few scientists are persuaded. Critics charge that the studies so far have been too few, too small and too weakly designed to warrant the technique's clinical use. \"It's attractive as a research topic, but the data are not convincing enough,\" says Masahiko Haruno, a neuroscientist at Tamagawa University in Tokyo. John Sweeney, a neuroscientist at the University of Illinois, Chicago, who has spent two decades looking for connections between various brain-monitoring techniques and diseases such as schizophrenia, says that \"none has ever been validated to anyone's satisfaction\". And NIRS is the least developed of them all, he says, calling it \"the thinnest of ice to be treading upon. We are nowhere near ready to tell patients and families that they should have these kinds of tests.\"  \n                New kid on the block \n              Sporting the knobbled cap, I stare at a screen showing Japanese phonetic characters and say aloud words beginning with each sound. The words don't come readily, particularly in front of an audience of psychiatrists and neuroscientists. Coming from all over Japan, they meet every month to discuss NIRS results and strategies. Having entered the research scene some 15 years ago, NIRS is relatively new compared with functional magnetic resonance imaging (fMRI) or electroencephalography (EEG), but in Japan it has raced ahead to the clinic. (The two biggest suppliers of NIRS analytic devices \u2014 Hitachi and Kyoto-based Shimadzu \u2014 are both based in Japan and the country accounts for two-thirds of publications using NIRS analysis.) The technique takes advantage of the fact that compared with constituents of other tissues, haemoglobin in blood absorbs more light in the near-infrared spectrum. Blood flow to a particular brain region increases when neurons there are active. So monitoring the changes in haemoglobin concentration gives a site-specific read on blood flow and thus on neuronal activity 3 . Fukuda's NIRS device focuses on the prefrontal cortex and temporal cortex, regions that are implicated in many of the symptoms seen in psychiatric disorders; the signature pattern of blood flow associated with each disorder is used to help diagnose it. NIRS lacks the precision and depth of fMRI, which can pinpoint changes in blood flow throughout the brain and with much greater spatial resolution. But NIRS is relatively cheap and mobile, and subjects can sit upright without having to endure a spell in the large, loud and sometimes nerve-wracking tube of an fMRI machine. This means that NIRS is easier to use on fidgety subjects such as children, and people with psychotic conditions or anxiety. The advantages have made infrared imaging increasingly popular with brain researchers worldwide. Devices from the largest maker in the United States, NIRx Medical Technologies of Glen Head, New York, are being used to study areas ranging from autism to brain\u2013computer interfaces. Hitachi now offers a stripped-down version that allows the brains of four people interacting in a room to be analysed wirelessly. \n               Click here for larger image \n               Fukuda, though, has focused on applying the technology to diagnoses. Lean and grey-haired, he speaks thoughtfully and is confident in the technique and its potential to help people. He started using a basic NIRS device in 1997, when he was an associate professor at the University of Tokyo already working with EEG. Since then, he has been measuring the brain activity of people with a variety of disorders and has reported that those with depression, bipolar disorder or schizophrenia have, when averaged across groups of 10\u201320, a characteristic pattern of brain activation 4 , 5 . Fukuda boils the data down to graphs describing activity in the prefrontal and temporal cortices for the first 60 seconds or so of each task (see  'Traces of troubled minds' ). He says that the NIRS test on its own classifies patients correctly 80% of the time. These studies have not convinced other neuroscientists. Haruno says the patient numbers in the published studies are \"far too small\" to distinguish patterns, and that even if such patterns are found when signals are averaged across groups, this does not mean that one person's pattern can be used to assign them to a group. \"What does that mean for an individual patient? It's very misleading,\" Haruno says. Even Fukuda's collaborator, Andreas Fallgatter of the University of T\u00fcbingen in Germany, who has used NIRS for 14 years on about 1,000 patients and is now repeating Fukuda's language test in German, says \"NIRS is still a research method.\" Still, he says, \"Obviously, Dr Fukuda could successfully convince the Japanese authorities.\" That approval came via a fast-track process instituted by the national health ministry's Advanced Medical Technology programme in 2005 in an attempt to spur the development of biomedical technologies. Teruhiko Higuchi, a clinician and researcher specializing in depression and president of the National Center of Neurology and Psychiatry, led the evaluation of NIRS. He concluded that the technology was safe, effective and fast, and could help to make critical distinctions between different mental states (major depression, bipolar disorder or schizophrenia) at an early stage, when used with other diagnostic techniques. \"It is, in the end, only to assist diagnosis,\" he told the members of the evaluation committee, according to meeting minutes posted online. Higuchi's centre now offers the technique. Other committee members raised concerns about the small numbers of patients in the studies, and the fact that some were receiving drugs, but they did not object to its approval. Fukuda says that a larger study involving more than 500 patients will be submitted for publication soon and will answer many of his critics. He says that doubling the number of knobs and other methodological modifications reveal a much sharper distinction between the conditions, and that controlling for medicated versus non-medicated patients showed that drugs do not obscure a patient's NIRS profile. He acknowledges the validity of criticism about using group averages to diagnose individuals: \"Strictly speaking, this criticism is right.\" But he says the same is true for many other measurements, such as electrocardiograms and EEG, which vary from one individual to the next and thus require interpretation, but can still be clinically useful. \"Clinical diagnosis and NIRS examination are complementary to each other,\" he says. \"We stress this complementary nature to all the patients.\" But in at least some cases, NIRS seems to take the lead in diagnosis. For example, when Fukuda calculates his success rates, NIRS results that match the clinical diagnosis are considered a success. If the results don't match, Fukuda says he will ask the patient and patient's family \"repeatedly\" whether they might have missed something \u2014 for example, whether a depressed patient whose NIRS examination suggests schizophrenia might have forgotten to mention that he was experiencing hallucinations. Andreas Meyer-Lindenberg, an expert in neuroimaging and mental health at the University of Heidelberg, Germany, says that studies of patients without an existing diagnosis or psychiatric medication would be more persuasive. \"You would need a sample of unclear cases, as you would get in the clinic, classify them and then ascertain their diagnosis by following them up.\" Fukuda and his colleagues are already moving on to NIRS studies that might aid diagnoses of a range of disorders, including those centring on panic, attention deficit and post-traumatic stress. My own NIRS results, however, are short on clarity. Within 15 minutes, including the tests and a quick computer analysis, Fukuda is able to look at my traces and deliver a diagnosis: normal. When I later compared them to the patterns published in the literature, however, my trace seems to describe a brain somewhere between normal and bipolar. Later, Fukuda says that my pattern is not a normal 'normal' NIRS trace, perhaps because the observers in the room made me hesitate to speak. He also says that a subset of healthy subjects has the pattern that I do in the frontal lobe and that data he measured from the temporal lobe helped him reach a diagnosis. However they are reached, I suppose I should be happy about my results.  \n                 See Editorial  \n                 page 132 \n               \n                     Bipolar & Schizophrenia Network on Intermediate Phenotypes \n                   \n                     Shimadzu's near infrared spectrophotometer \n                   \n                     NIRx \n                   \n                     Alzheimer's Disease Neuroimaging Initiative \n                   \n                     National Center of Neurology and Psychiatry, Tokyo \n                   Reprints and Permissions"},
{"file_id": "469150a", "url": "https://www.nature.com/articles/469150a", "year": 2011, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "Climate change means that national parks of the future won't look like the parks of the past. So what should they look like? Imagine Montana's Glacier National Park without glaciers; California's Joshua Tree National Park with no Joshua trees; or the state's Sequoia National Park with no sequoias. In 50 years' time, climate change will have altered some US parks so profoundly that their very names will be anachronisms. Jon Jarvis, who became director of the US National Park Service in 2009, has called climate change \"the greatest threat to the integrity of our national parks that we have ever experienced\". The sentiment represents a dramatic shift from the position held during the Bush administration, when officials refused to fully acknowledge the existence of climate change. Now, park managers in the United States and around the world are working with researchers to map how the landscapes they care for might change. And they are coming to terms with the idea that the historical remit of most parks systems \u2014 to preserve a piece of land in its 'natural' state \u2014 is untenable. \"You can't fight the climate,\" says Ken Aho, an ecologist at Idaho State University in Pocatello, who studies non-native species at Yellowstone National Park in Wyoming. \"Eventually you have to throw up your hands,\" he says. Nowhere is attachment to historical fidelity more pronounced than at Yellowstone, the first US national park and the best example of the park as a landscape seemingly unchanged by the passage of time. Visiting it, one crosses paths with bison and wolves. It is not hard to imagine a party of explorers coming around the next bend. Much of Yellowstone's 900,000 hectares are high plateau, crossed by rivers and dotted with geothermal basins featuring pools and geysers. The park was created in 1872, to protect the geological wonders and safeguard a wild landscape emblematic of the American west. Since 1916, Yellowstone and the nation's other parks and monuments have been run by the National Park Service, which has aimed to preserve the land in its 'natural' condition \u2014 typically meaning how it looked before white people arrived. In the words of the influential  Leopold Report   of 1963: \"A national park should represent a vignette of primitive America.\" ( A. S. Leopold  et al .  Wildlife Management in the National Parks   Advisory Board on Wildlife Management; 1963. ) \n               boxed-text \n             But Yellowstone can no longer be kept as it was in 1872, if that were ever possible. Climate change has already begun to transform the park (see  'Under attack' ). The Yellowstone area has been plagued by tree pests, abetted by warmer temperatures. Fires are expected to become more frequent, animal populations are shifting and the landscape and ecology are being reshaped. Similar stories are playing out at all of the nation's parks, and the Park Service is beginning to react. In September 2010, it released a  Climate Change Response Strategy , which includes sections on science, adaptation, mitigation and communication (National Park Service Climate Change Response Program; 2010). The report hints that climate change may force the Park Service to change the way it defines its mission. Maintaining a natural state can no longer be the goal, or important tools for adaptation, such as moving species or selective breeding, would be forbidden. And besides, that battle may already be lost.  \n                Encroaching damage \n              A drive through the greater Yellowstone area reveals changes to the landscape. Most striking are the acres of trees standing dead, killed by an insect the size of a grain of rice. Mountain pine beetles, native parasites, burrow into and reproduce in the living wood of the trees. Winter temperatures of \u221240 \u00b0C kill the beetles, keeping their numbers down. But warm winters in the past ten years have allowed them to proliferate. More than half of the greater Yellowstone ecosystem's conifer forest has seen pine-beetle damage, and 10% of the forest has a 'high severity' infestation, in which more than 40% of trees are lost. Throughout the western United States, the Park Service has used insecticides to protect some trees, and removed a few dead ones. Mostly, however, it can only watch as an orange wave of dying trees ripples through the forest. The dried-out remains left in the beetles' wake look like an invitation to intense forest fires that would reshape the landscape even more, but that is not necessarily the case. Monica Turner, an ecologist at the University of Wisconsin\u2013Madison, has been modelling big blazes at Yellowstone, such as the inferno of 1988 that burned 321,000 hectares of forest. She and her colleagues have found that such fires burn in the living crowns of trees and don't rely on dead wood; furthermore, pine-beetle attacks kill trees, decreasing the amount of wood in an area and creating gaps that can stop a fire. Even so, Turner's group expects that the big fires that typically occur every few hundred years might happen more frequently in the future without the beetles' help, thanks to increased temperatures and changes in precipitation levels. That increased frequency might turn some parts of Yellowstone into a landscape of young forests, with less capacity than more mature ones to absorb carbon dioxide from the atmosphere. This could potentially turn the park at large from a carbon sink into a carbon source. But forecasting the future is complicated. Pines that don't get caught in a blaze may actually grow better in a warmer future; growth of lodgepole pines is limited by the length of the growing season and by cold temperatures. Meanwhile, species such as the larch, currently confined to lower altitudes than those at Yellowstone, may move up as temperatures rise. Other familiar tree species might go locally extinct. Whitebark pine, a high-altitude species, is facing a three-way attack from pine beetles, a fungal disease called blister rust and climate change. As Yellowstone's peaks heat up, the tree's range is expected to move upslope, but the higher it goes the less area it will have. \"The future of whitebark pine in Yellowstone is questionable,\" says Turner. Ecosystems won't move predictably. \"Migration in response to climate change can often be extremely messy,\" says Stephen Gray, a climatologist at the University of Wyoming in Laramie, who is among those working on scaling down global climate models to forecast changes at the level of individual parks. As ecologists scramble to predict changes, park managers are gearing up for a new management style, which will have to include at least one of two approaches traditionally anathema to the profession: letting things change, or intervening aggressively to keep them the same. In many cases, choosing between these strategies will be the challenge. If managers choose the former, they will need to create an environment conducive to change. For example, many conservation biologists argue for creating and maintaining corridors that connect parks to other natural areas. The bigger the connected area, the more room plants and animals will have to move and the larger the gene pools available for adaptation. Wolverines are one species that might benefit. Maps of future climates suggest that by 2040, lower-elevation parts of their current range may no longer have the deep spring snow that wolverines need to make their dens. So managers should perhaps focus on increasing the quality and connectivity of land in the Colorado Rocky Mountains, California's Sierra Nevada and parts of Wyoming and Utah to make sure that the wolverine population remains large enough to be genetically healthy. To maintain corridors in areas that fall outside national-park remits, the Park Service is participating in Landscape Conservation Cooperatives, an initiative begun in September 2009. These clubs of federal and state agencies, conservation organizations, university scientists and other interested parties focus on managing huge chunks of land as units, rather than having every park, forest and piece of private land managed independently.  \n                A firm hand \n              But tough choices loom in terms of how much to meddle. Aggressive intervention might be needed to conserve some of Yellowstone's larger mammals (see  'Four seasons of worry for the grizzly' ) and its iconic trees. To preserve the whitebark pine, some scientists from various agencies have begun identifying trees that are resistant to blister rust and collecting seeds to breed rust-resistant trees. They've also begun planting whitebarks in newly burned areas. They plan to use insecticides to protect individual trees from pine beetles, prune off infected branches and thin the vegetation around the trees to give them a competitive advantage. They also plan to stop fires from burning rust-resistant or particularly genetically diverse stands of whitebarks. Some of these planned actions sound a lot like landscape gardening. The alternative approach would be to let the whitebarks die out in Yellowstone, and plant them somewhere where they might flourish in a warmer future. Sally Aitken, a forest geneticist at the University of British Columbia in Vancouver, Canada, has planted whitebarks beyond their current range, in northwestern British Columbia. They seem to like it there. Jarvis has suggested the possibility of moving species outside their native ranges to give them a better chance of surviving \u2014 just not right away. \"The big point here is that we are willing to face these questions,\" he says. \"We are not afraid to talk about them.\" Other countries have ditched 'naturalness' for goals that encompass a range of acceptable states, and don't rule out aggressive intervention. Parks Canada uses an 'ecological integrity' approach, preserving a configuration of local flora and fauna that is likely to be robust. At a conference on the Yellowstone ecosystem last October, many speakers talked about managing for 'resiliency' rather than historical fidelity, promoting an ecosystem's ability to change with the times without changing in character. Rather than trying to sustain a stressed ecosystem that collapses at the first hot summer, the idea is to let things change gracefully. But the Park Service isn't planning to change its mission any time soon. \"I don't think resilience replaces our current management foundation,\" says Jarvis. \"Our goal has never been to freeze [parks] in some kind of stasis.\" Resiliency may be achievable for Yellowstone, even if it won't look like it did in 1872. Whitebark pines may not make it, but lodgepole pines are very resilient. Bears and wolves are clever. Yellowstone is big and Yellowstone is tough. \"As a scientist who really treasures the region,\" says Turner, \"I believe that Yellowstone will go on in the face of climate change. Yellowstone is very resilient. The 1988 fires are not a catastrophe. Bark beetles are not a catastrophe.\"  \n                 See Editorial  \n                 page 131 \n               \n                     The Greater Yellowstone Science Learning Center \n                   \n                     Climate Adaptation Knowledge Exchange \n                   Reprints and Permissions"},
{"file_id": "469014a", "url": "https://www.nature.com/articles/469014a", "year": 2011, "authors": [{"name": "Richard Van Noorden"}], "parsed_as_year": "2006_or_before", "body": "Researchers have spent 25 years exploring the remarkable properties of fullerenes, carbon nanotubes and graphene. But commercializing them is neither quick nor easy. In fairy tales, third place is often the best: it's usually the third casket that contains the treasure, and the third child who finds fame and fortune. And so it may be for graphene, the third and most recently discovered form of 'new carbon'. The football-shaped fullerenes 1 , discovered in 1985, and the hollow cylindrical carbon nanotubes 2 , first characterized in 1991, have so far had a limited impact on industry. But now graphene, a one-atom-thick flat sheet of carbon, seems to be surrounded by favourable omens \u2014 not the least of which is the speed with which groundbreaking experiments on its properties were rewarded with the 2010 Nobel Prize in Physics. It has been just six years since Nobel laureates Andre Geim and Kostya Novoselov at the University of Manchester, UK, first reported using sticky tape to peel atomically thin layers of graphene from lumps of graphite 3 . But the material \u2014 essentially just an unrolled nanotube \u2014 has turned out to have properties just shy of miraculous: a single layer of graphene is simultaneously the world's thinnest, strongest and stiffest material, as well as being an excellent conductor of both heat and electricity. Graphene has been showered with media attention as companies vie to bring those attributes to market. Last year, graphene was the subject of around 3,000 research papers and more than 400 patent applications. South Korea is planning a US$300-million investment to commercialize the material, and companies ranging from IBM to Samsung are testing graphene electronics \u2014 ultra-small, ultra-fast devices that might one day replace the silicon chip. The hype over graphene has reached such a pitch that a casual follower might wonder why it hasn't conquered the technological world already. The reality is not such a fairy tale. Graphene's carbon forebears were once hyped in much the same way. Yet fullerenes have found hardly any practical applications. And although nanotubes have done better, they are costly to produce and difficult to control. Their subdued industrial impact is a lesson in just how hard commercialization of a new material can be. Yet the story of nanotubes has some encouraging features. High-tech electronics applications are still years in the future, but a more low-tech application \u2014 nanotube-based conducting films for energy storage or touch screens \u2014 is much closer to commercialization. Another, comparatively straightforward use \u2014 nanotube-reinforced composite materials for aeroplanes and automobiles \u2014 is now reaching the market. Anticipating growing demand, nano-tube manufacturers have scaled up production to many hundreds of tonnes a year. \n               Click here for larger image \n               For that very reason, the graphene manufacturers following in their wake may have hit on the right moment to start mass-producing the sheets. Graphene is being considered for the same types of application as nanotubes, but it has some key advantages in ease of production and handling, and should benefit from two decades of research with nanotubes. That hindsight also means that graphene manufacturers have a better idea of which applications are worth chasing, and of how to avoid the false starts that nanotubes made in their first decade.  \n                A carbon playground \n              The remarkable properties shared by nanotubes and graphene arise from their common structure: an atomically thin mesh of carbon atoms arranged in a honeycomb pattern. Immensely strong carbon\u2013carbon bonds produce an exceptionally high strength-to-weight ratio. Such is the strength of graphene, for example, that according to the Nobel prize committee, a hypothetical 1-metre-square hammock of perfect graphene could support a 4-kilogram cat. The hammock would weigh 0.77 milligrams \u2014 less than the weight of a cat's whisker \u2014 and would be virtually invisible. The symmetry with which carbon atoms are arranged on the hexagonal lattice also allows both forms of nano-carbon to conduct electricity far more easily than the silicon used in computer chips. This means that they have much lower electrical resistance and generate much less heat \u2014 an increasingly useful property as chip manufacturers try to pack features ever more densely onto circuits. Furthermore, even small variations in carbon structure can create a multitude of new properties. In graphene, for example, electronic behaviour depends on the size of a given sheet, the presence or absence of defects in the sheet's lattice and whether it is lying on a conductive surface. In nanotubes, likewise, a given structure can be made semiconducting or metallic just by changing its diameter, length or 'twist' (the angle between the lines of hexagons and the direction of the tube). And there are differences between single tubes and those in which several cylinders are nested inside each other \u2014 called multi-walled nanotubes. These properties have long sparked hopes of game-changing electronics applications. And researchers have made great progress \u2014 in the laboratory. In 1998, for example, physicists demonstrated a transistor made from a single, semiconducting nanotube 4 . And in 2007, researchers reported the synthesis of a carbon-nanotube-based transistor radio 5 . But for industrial-scale mass production of such circuits, the great variability of nanotubes is a curse. They are most commonly produced in a reactor, in which catalysts guide formation of the tubes from a carbon-rich vapour. This typically leaves a jumble of multi-walled, single-walled, semiconducting and metallic tubes of various lengths and diameters, all with different electronic properties. \"Diversity is great until you have too diverse a population: then it becomes a real headache,\" says John Rogers, a physical chemist at the University of Illinois in Urbana-Champaign. Only in the past five years have researchers worked out how to sort nanotubes into semiconducting and metallic types 6 . But there are further difficulties in assembling selected nanotubes in predetermined places on a chip and connecting these separate tubes together without compromising performance, so most physicists have come to believe that it is impractical for carbon nanotubes to replace silicon. \"An integrated circuit would have to involve billions of identical carbon-nanotube transistors, all switching at exactly the same voltage,\" says Phaedon Avouris, who works on nanoscale electronics at IBM's Thomas J. Watson Research Center in Yorktown Heights, New York. This is not feasible with current technology. Graphene offers a bit more cause for optimism. The highest-quality sheets are currently made by heating a wafer of silicon carbide in a vacuum, leaving a layer of pure graphene on the top surface. This method has fewer problems with uncontrollable variety from batch to batch than does nanotube synthesis, and the flat sheets that result are bigger and easier to handle than nanotubes. But graphene has problems too. A single graphene sheet conducts charge so well that it is hard to make the current stop, something that must be solved if the material is ever going to be used in digital devices such as transistors, which control the flow of current like on\u2013off switches. To change the material's electronic properties in the appropriate way \u2014 creating a 'band gap', or break in electron energy levels, which essentially turns it into a semiconductor \u2014 the sheet must be sliced up into thin ribbons. This is probably easier than trying to place billions of nanotubes on a chip, says Avouris \u2014 but it is still not currently possible with commercial technologies. These processing difficulties suggest that graphene won't soon displace silicon chips. \"There have been millions of person-years and trillions of dollars put into the development of silicon electronics,\" notes James Tour, an organic chemist who specializes in nanotechnology at Rice University in Houston, Texas. \"Asking graphene to compete with silicon now is like asking a 10-year-old to be a concert pianist because we've been giving him piano lessons for the last six years.\" In the meantime, nano-carbon structures may be more competitive in less demanding electronics, such as conductive flat films for transparent electrodes in touch-screen displays or in solar cells. Bundles of dissimilar carbon nanotubes might very well provide enough conductivity for such electrodes, as might cheaper, lower-quality graphene sheets made by methods other than the silicon carbide process.  \n                Lowering the sights \n              In June 2010, for example, a team led by Byung Hee Hong at Sungkyunkwan University in Suwon, South Korea, reported using carbon-rich vapour to deposit graphene films measuring 75 centimetres diagonally on copper plates, which are then etched away and recycled 7 . South Korean electronics giant Samsung is already testing this technique for use in commercial touch screens, which Hong estimates could be just two to three years away. \n               Click here for larger image \n               The question is whether the graphene films can compete with existing touch-screen materials such as indium tin oxide (ITO). Hong is optimistic; the cost of ITO has been increasing rapidly, because indium is scarce. But again, carbon nanotubes offer a cautionary tale. Early on, it was hoped that the tubes would form the television screens of the future, thanks to their ability to emit electrons from their tips to excite phosphors on the screen. In practice, competing plasma and liquid-crystal displays got better faster \u2014 and these are the screens most commonly used today. One sweet spot for nano-carbon may be in the still-emerging market for flexible electronics. These are displays and sensors that could be worn on clothes, stuck to walls or printed on rollable sheets. Here, the only competition is from organic conducting polymers, because other materials cannot be printed on plastic. The performance of these polymers is quite low, says Rogers, so nanotubes and graphene circuits \u2014 which can be transferred to flexible substrates \u2014 could compete effectively. But even these specialist electronics are still in the future. For now, the hundreds of tonnes of commercial nano-carbon being turned out every year are mostly going into composites for sporting goods, lithium-ion batteries and cars. The aim is to disperse nano-carbon sheets or tubes within resins or polymers, so that they not only make the material tougher by blocking cracks that would otherwise spread, but also help to dissipate heat and electrical charge. For example, the Audi A4 car now has plastic fuel filters containing carbon nano-tubes, which protect against static electricity. And nanotube additives in lithium-ion battery electrodes were one of the first nanotube applications marketed by Showa Denko, a Tokyo-based chemical-engineering company.  \n                Cutting costs \n              Basic processing problems with nanotubes initially hampered progress. They tended to clump together like tangled string as they came out of the reactor, making it hard to disperse the nanotubes evenly through plastic or resin. Despite improvements, this limits nanotube content to 1\u20132% by weight in the final product, compared with the 20\u201330% typical of conventional carbon fibre. The other problem was, and still is, cost. Materials such as steel, aluminium and plastics, and fillers such as carbon black, sell for just dollars or cents a kilogram, says David Hwang at Lux Research, a technology-evaluation company in New York. Meanwhile, multi-walled nanotubes retail for $100 a kilogram. The price is coming down as production scales up, but will drop only to about $50 a kilogram by 2020, according to Lux forecasts. Composite-quality graphene has the potential to be a lot cheaper, although costs are currently similar. As Geim and Novoselov showed in 2004, graphene platelets of varying sizes can easily be peeled away from graphite 3 , a raw material that costs a few dollars a kilogram. Graphene is also easier to disperse in a resin than are nanotubes. But promising as this is, says Steve Hahn, a senior scientist with Dow Chemical's Ventures and Business Development Group in Midland, Michigan, the reality is that these applications are still niche. \"I've been trying to find outlets for graphene for a couple of years,\" he says. But there is usually something quite a lot cheaper that does the same job, says Hahn. Michael Knox, president of XG Sciences, a start-up graphene-manufacturing company in East Lansing, Michigan, agrees. Adding graphene platelets to composites is not a transformational application, \"it's an incremental improvement\", he says. Yet that is not to be sniffed at. \"If I could demonstrate a 10\u201320% improvement in a polypropylene composite at a reasonable price, I could probably sell a million tonnes of it a year \u2014 and car manufacturers would be pretty excited by that,\" says Knox. The trick for young graphene-manufacturing companies is to find specific applications and then work out how to scale up production capacity without overstretching themselves. Vorbeck Materials in Jessup, Maryland, for example, has decided to focus on making graphene-based conducting inks. John Lettow, co-founder and president of Vorbeck, says that the inks will be in smart cards and radio-frequency identification tags in retail stores in the first quarter of 2011. One near-term application may be supercapacitors, which use crumpled-up sheets of graphene to pack a massive surface area into a small space \u2014 to store more electric charge per gram than any other material. Other researchers are looking at using nano-carbon to make catalyst electrodes in fuel cells, or even to make water-purification membranes \u2014 but, as usual, finding clear advantages over existing materials such as activated carbon will be the problem. Carbon nanotubes do have one property that graphene sheets don't: they can be very long. The nanotubes currently mixed into resins and plastics are typically short stubs, but Nanocomp Technologies in Concord, New Hampshire, says that it can spin long nanotube fibres into lightweight, electrically conducting yarns or sheets that could replace copper wiring in some applications. \"There are about 60 miles of copper wire in an aeroplane,\" says Nanocomp chief executive Peter Antoinette \u2014 so replacing this with the much lighter nanotube wire could save substantially on weight and fuel use. Such activity is very encouraging for carbon nanotubes, says Hwang. \"There was a huge amount of research that had to be done before nanotubes got to be commercially viable. Now if you look at the next five years, the commercial trajectory will be very different.\" But have carbon nanotubes really taken a disproportionate time to get going? It takes 20 years or more for any new material to make an impact in industry, point out many nanotube makers. \"Research on carbon fibre started in the 1950s; it took 15 years or so before aerospace and military used it \u2014 and we didn't hear about that until much later \u2014 and it wasn't until the mid-1970s that you started seeing commercial aircraft with small quantities of structures made out of carbon-fibre composites,\" says Brian Wardle, who directs the nano-engineered aerospace structures consortium at the Massachusetts Institute of Technology in Cambridge. Nanotubes may simply be on the usual trajectory from discovery to industry \u2014 and graphene may find that it follows the same path. \"Graphene will have its place, but it will just take longer than people think,\" says Antoinette. And what will happen in the meantime? \"A lot of companies are bringing new capacity online at the same time right now,\" notes Hahn. \"They will either go out of business or find a market somewhere. Whatever happens, it'll be a great lesson to all of us in how new materials are commercialized.\" \n                     Nature Materials focus on graphene \n                   \n                     Selling graphene by the ton \n                   \n                     2010 Nobel Prize in Physics \n                   \n                     Andre Geim\u2019s research group \n                   \n                     Vorbeck Materials \n                   \n                     Angstron Materials \n                   \n                     XG Sciences \n                   \n                     SungKyunKwan University\u2019s graphene laboratory \n                   \n                     Nanocomp Technologies \n                   Reprints and Permissions"},
{"file_id": "469018a", "url": "https://www.nature.com/articles/469018a", "year": 2011, "authors": [{"name": "Katharine Sanderson"}], "parsed_as_year": "2006_or_before", "body": "In the past two decades, the green-chemistry movement has helped industry become much cleaner. But mindsets change slowly, and the revolution still has a long way to go. By the latter half of the 1980s, the worldwide chemical industry knew that it had to clean up its act: its environmental reputation was dismal. Still fresh in the public mind was the 1984 disaster at Bhopal, India, where at least 3,000 people died and hundreds of thousands were injured by a toxic gas leak at a Union Carbide pesticide plant. Also fresh were memories of the 1978 Love Canal incident in Niagara Falls, New York, where the discovery of buried toxic waste forced the abandonment of an entire neighbourhood, and the discovery of dioxin contamination a few years later that forced the evacuation of an entire town \u2014 Times Beach, Missouri. Even when companies did try to deal responsibly with their waste, which typically included volatile organic solvents and other hard-to-clean-up agents, the volumes were daunting. Global statistics were, and still are, fragmentary. But in the United States, according to the earliest systematic data gathered by the Environmental Protection Agency (EPA), some 278 million tonnes of hazardous waste were generated in 1991 at more than 24,000 sites. Not all of it came from chemical companies, but much of it did. More than 10% of the total, some 30 million tonnes, came from one firm alone: the Dow Chemical Company, headquartered in Midland, Michigan. And other firms, such as petrochemical giant Amoco, headquartered in Chicago, Illinois, and DuPont, of Wilmington, Delaware, were not far behind. The result, as chemical companies struggled to deal with increasingly stringent environmental regulations, was an industry-wide move towards what is often called 'green chemistry' \u2014 a term introduced in 1991 by  Paul Anastas , then a 28-year-old staff chemist with the EPA. The goal of green chemistry was never just clean-up, explains Anastas, who is currently on leave from Yale University to head the EPA's research division. In his conception, green chemistry is about redesigning chemical processes from the ground up. It's about making industrial chemistry safer, cleaner and more energy-efficient throughout the product's life cycle, from synthesis to clean-up to disposal. It's about using renewable feedstocks wherever possible, carrying out reactions at ambient temperature and pressure \u2014 and above all, minimizing or eliminating toxic waste from the outset, instead of constantly paying to clean up messes after the fact. \"It's more effective, it's more efficient, it's more elegant, it's simply better chemistry,\" says Anastas. If the green-chemistry ideal is simple to state, however, achieving it has been anything but simple. Yes, says Eric Beckman, a chemical engineer at the University of Pittsburgh in Pennsylvania, \"Companies these days are being very attentive to rendering their current processes greener.\" In 2009, for example, the total US output of hazardous waste was down by an order of magnitude over 1991, to 35 million tonnes. The largest generator in that year, DSM Chemicals in Augusta, Georgia, produced just 3.4 million tonnes. But the greening of any given process is always a trade-off among benefits, feasibility and cost, says Beckman \u2014 and green is not always the winner. Furthermore, he says, industry's adoption of green chemistry has so far been focused mainly on incremental improvements in existing processes. \"It's embryonic at best,\" says Beckman, who speaks for many observers when he says that the real 'green revolution', in the form of processes redesigned from scratch and plants rebuilt from the ground up, is only just beginning. The progress of green chemistry so far has been partly a matter of technical feasibility, as researchers have developed less toxic alternatives to conventional methods. A prime example is supercritical carbon dioxide: ordinary, non-toxic carbon dioxide that has been heated and pressurized above its 'critical point' of 31.1 \u00b0C and 7.39 megapascals, beyond which it behaves like both a gas and a liquid, and readily serves as a solvent for a wide range of organic and inorganic reactions. Other non-toxic replacements for solvents have been found among the ionic liquids: exotic cousins to ordinary table salt that happen to be liquid at or near room temperature.  \n                The E-factor \n              Green chemistry's progress has also benefited from an awareness campaign by Anastas and his allies. A key first step was the 1991 coining of the name itself, says John Warner, president of the Warner-Babcock Institute for Green Chemistry in Wilmington, Massachusetts, who at the time was director of exploratory research at the Polaroid Corporation in Minnetonka, Minnesota. \"Identifying green chemistry as a field of science differentiated it from a political and social movement,\" he says. Another key step was the drawing up by Anastas and Warner of a set of principles intended to help scientists define and practise green chemistry (see  'The twelve principles of green chemistry' ). And yet another came in 1995, when Anastas helped to persuade US President Bill Clinton to launch the Presidential Green Chemistry Challenge, which still awards five citations each year to companies and academics who have done an outstanding job of implementing the principles. Mostly, however, green chemistry's progress has been a matter of corporate buy-in, as epitomized by its promotion by the chemical industry's own voluntary initiative, Responsible Care ( http://www.responsiblecare.org ), which works with national industry associations to improve the industry's health, safety and environmental performance. Founded in Canada in 1985, membership has grown from 6 national associations to 53. The pharmaceutical sector has embraced green chemistry most enthusiastically, perhaps because it has the most to gain. Pharmaceutical plants typically generate 25 to 100 kilograms of waste per kilogram of product, a ratio known as the environmental factor, or 'E-factor'. So there is plenty of room to increase efficiency \u2014 and cut costs. At drug-maker Pfizer, for example, the first laboratory synthesis of its anti-impotence drug sildenafil citrate (Viagra) had an E-factor of 105. But long before Viagra went on the market in 1998, a team at Pfizer's plant in Sandwich, UK, was rigorously re-examining every step of the synthesis. The researchers replaced all the chlorinated solvents with less toxic alternatives, and then introduced measures to recover and reuse these solvents. They eliminated the need to use hydrogen peroxide, which can cause burns. They also eliminated any requirement for oxalyl chloride, a reagent that produces carbon monoxide in reactions and is therefore a safety concern. Eventually, Pfizer's researchers cut Viagra's E-factor to 8. After that success, Peter Dunn, the leader of the Viagra synthesis team, became head of the more systematic green-chemistry drive started by Pfizer in 2001. Dunn says he is not free to talk about specific cash savings, but can point to sweeping changes made across the company. Pfizer has reduced the E-factor of the anticonvulsant pregabalin (Lyrica) from 86 to 9, he says, and has made similar improvements for the antidepressant sertraline and the non-steroidal anti-inflammatory celecoxib. \"These three products alone have eliminated more than half a million metric tons of chemical waste,\" says Dunn.  \n                Creative chemistry \n              Nor is Pfizer alone; the pharmaceutical sector is so competitive that no company can afford to ignore green chemistry's potential savings. The Pharmaceutical Roundtable, first convened in 2005 by the American Chemical Society's Green Chemistry Institute, now has 14 member companies that jointly fund academic research in the field and share pre-competitive information. In 2002, the chemicals giant BASF, based in Ludwigshafen, Germany, introduced an industrial-scale process that uses ambient-temperature ionic liquids to remove acid by-products from reaction mixtures \u2014 a common chemical manufacturing step that had previously been much more cumbersome. But BASF's embrace of green chemistry (which the company prefers to call 'sustainable chemistry') goes much further, notes Pete Licence, a green chemist at the University of Nottingham, UK. \"You're getting sensible and joined-up thinking about the way that chemical plants are created,\" he says. \"They have this integrated reaction system where the products and the by-products of reactions are actually the starting materials for the plant that is next door.\" The plants are also designed to maximize energy efficiency, Licence says: \"Waste heat from one process is the warm-up for the feedstock for the next.\" But the comprehensive restructuring required illustrates why the shift to green chemistry has been comparatively slow among bulk-chemicals manufacturers. These firms deal with products that are made in much larger volumes than pharmaceuticals, and their industrial processes are already highly optimized, with E-factors typically in the range of 1 to 5. Although it is possible to go much lower \u2014 E-factors for petrochemicals are on the 0.1 scale \u2014 doing so is not always economic. \"Once you have a plant, it will run for 30 or 40 years because you have made a huge investment,\" says Walter Leitner at the Institute for Technical and Macromolecular Chemistry at the University of Aachen, Germany. Nor does it always pay to be green in the speciality chemicals sector \u2014 as Thomas Swan and Company in Consett, UK, learned the hard way. In 2001, building on the work of chemist Martyn Poliakoff at Nottingham University, it opened the world's first continuous-flow reactor using supercritical carbon dioxide as a solvent. \"It looked as if it could have been game-changing within the industry,\" says managing director Harry Swan. But when no government subsidies were forthcoming, the plant could not provide chemicals more cheaply than those made by the standard non-green methods, he says. So the facility was mothballed, and may soon be decommissioned and dismantled. Other roadblocks to the adoption of green chemistry are technical. For example, even after decades of research, green solvents are not always more efficient than the widely used chlorinated solvents. Nor have chemists completely eliminated the need for catalysts containing precious or toxic metals \u2014 although Dunn, for one, is optimistic that this may eventually be possible through advances in enzyme technology. And how to make bulk chemicals from biomass and other renewable feedstocks, rather than from crude oil, is still an open challenge. \"It's a different way of looking at a chemical synthesis,\" says Leitner, who points out that the conventional problem gets turned on its head. Instead of starting out with a relatively simple hydrocarbon extracted from oil, and then adding side groups to the molecule to give it the desired properties, chemists have to start with the incredibly complex mixture of biomolecules typical of most renewable feedstocks, and get to what they want by snipping off pieces in a controlled manner. But many advocates say that the most fundamental barrier to the wider adoption of green chemistry is mindset \u2014 which largely reflects the way chemists are taught. \"In the United States, chemists get trained rigorously in chemistry, but don't see any engineering, product design, or life-cycle analysis,\" says Beckman. Or, as Anastas puts it, \"you usually get the safety course that says, 'Wear your goggles and your coat, and don't blow things up \u2014 and by the way, here's the number to call if you do.' But I don't think that's the same as treating the consequences of what we do as an intrinsic part of our work\". This curricular conservatism may well reflect the often negative reactions of academic chemists to green chemistry. Especially in its early years, the field was seen as fuzzy and non-rigorous, recalls Neil Winterton of the University of Liverpool, UK, a former critic who has since become more accepting of the movement. The word 'green' conveyed the impression that certain techniques were being promoted for reasons of political correctness, he says. \"It needed a little bit more fundamental underpinning to establish whether what was being proposed was or wasn't a major contribution to improving the efficiency of chemical processes.\" Sceptics also questioned whether green chemistry was anything more than a trendy new buzzword used to get money for projects of dubious environmental value. \"It's something that can dupe the public, it can dupe other scientists working in the area, and much, much more importantly, it can dupe decision-makers\", concedes Licence. Scepticism hasn't gone away entirely; a mention of green chemistry in a gathering of chemists can still provoke sighs and eye-rolling, says Warner. But scepticism has lessened as research has improved. The EPA, for example, has made notable progress in lifting one barrier to effective green chemistry, which is that researchers trying to create a new, non-toxic manufacturing process often don't know if a given compound is 'green' or not. No one has had the time or money to gather the toxicity data, which typically requires expensive animal testing. The EPA's answer is a high-throughput screening project called ToxCast, which has been running at its Research Triangle Park facility in North Carolina since 2007. The ToxCast team has applied a battery of standard high-throughput biochemical assays, which measure such things as binding to cellular receptors, to 1,000 chemicals that already have animal toxicology data. These data have then been used to build statistical and computational models that attempt to predict any compound's toxicity from the assays alone. A ToxCast prediction costs US$20,000 per chemical, compared with the $6 million to $12 million typical of animal toxicology tests, says Robert Kavlock, who oversees the project as head of the EPA's National Center for Computational Toxicology at Research Triangle Park. So if these models can be made reliable enough, he says, \"then we've got a way to address the chemicals that we can't afford to test in animals\" \u2014 and in the process, help companies to choose compounds that will make their chemistry truly green. Now that Anastas is the EPA's research chief, he has been trying to spread the green-chemistry approach through staff meetings at the agency's labs across the country. He wants to move the EPA away from a culture of regulating and banning to one where products are designed to be synthesized in a way that reduces or eliminates the use of hazardous substances in the first place. As EPA chief Lisa Jackson puts it: \"It's the difference between treating disease and pursuing wellness.\" If that change in attitude happens, says Anastas, it will represent a \"seismic shift\" at the agency \u2014 \"the culmination of the work of my career\". But in a sense, he adds, it will also be just the beginning: \"I believe that the ultimate goal for green chemistry is for the term to go away, because it is simply the way chemistry is always done. Green chemistry should just be second nature, the default value.\" Click here  for a Q&A with the man who coined the phrase 'green chemistry', Paul Anastas. Katharine Sanderson is a writer based in Toulouse, France. \n                     EPA Office of Research and Development \n                   \n                     Warner Babcock Institute for Green Chemistry \n                   \n                     University of York Green Chemistry Network \n                   Reprints and Permissions"},
{"file_id": "469286a", "url": "https://www.nature.com/articles/469286a", "year": 2011, "authors": [{"name": "Apoorva Mandavilli"}], "parsed_as_year": "2006_or_before", "body": "Blogs and tweets are ripping papers apart within days of publication, leaving researchers unsure how to react. \"Scientists discover keys to long life,\" proclaimed  The Wall Street Journal   headline on 1 July last year. \"Who will live to be 100? Genetic test might tell,\" said National Public Radio a day later. These and hundreds of similarly enthusiastic headlines were touting a paper in  Science 1  in which researchers claimed to have identified a set of genes that could predict human longevity with 77% accuracy \u2014 a finding with potentially huge implications for medicine, health policy and the economy. But even as the popular media was trumpeting the finding, other researchers were taking to the web to criticize the paper's methodology. \"We expect that most of the results of this study will not have the same longevity as its participants,\" sniped a blog posted by researchers at the personal genomics company 23andMe, based in Mountain View, California. Critics were particularly perturbed by the genome-wide association study (GWAS) that the authors had used to identify their longevity genes: the centenarians and the controls in the study had been tested with different kinds of DNA chips, which potentially skewed the results. \"Basically anybody that does a lot of GWAS knows this [pitfall], which is why we all said it so fast,\" says David Goldstein, director of Duke University's Center for Human Genome Variation, who voiced his concerns to a Newsweek blogger the day the study appeared. This critical onslaught was striking \u2014 but not exceptional. Papers are increasingly being taken apart in blogs, on Twitter and on other social media within hours rather than years, and in public, rather than at small conferences or in private conversation. In December, for example, many scientists blogged immediate criticisms of another widely publicized paper 2  \u2014 this one heralding bacteria that the authors claimed use arsenic rather than phosphorus in their DNA backbone.  \n                A chorus of disapproval \n              To many researchers, such rapid response is all to the good, because it weeds out sloppy work faster. \"When some of these things sit around in the scientific literature for a long time, they can do damage: they can influence what people work on, they can influence whole fields,\" says Goldstein. This was avoided in the case of the longevity-gene paper, he says. One week after its publication, the authors released a statement saying, in part, \"We have been made aware that there is a technical error in the lab test used \u2026 [and] are now closely re-examining the analysis.\" Then in November,  Science   issued an 'Expression of Concern' about the paper 3 , in essence questioning the validity of its results. When asked for a comment by  Nature , the lead investigator on the paper, Paola Sebastiani, a biostatistician at Boston University in Massachusetts, said only that she and her co-authors \"feel it is premature for us to talk about our experience because this is still an ongoing issue\". For many researchers, the pace and tone of this online review can be intimidating \u2014 and can sometimes feel like an attack. How are authors supposed to respond to critiques coming from all directions? Should they even respond at all? Or should they confine their replies to the conventional, more deliberative realm of conferences and journals? \"The speed of communication is ahead of the sheer time needed to think and get in the lab and work,\" said Felisa Wolfe-Simon, a postdoctoral fellow at the NASA Astrobiology Institute in Mountain View, California, and the lead author on the arsenic paper. Aptly enough, she circulated that comment as a tweet on Twitter, which is used by many scientists to call attention to longer articles and blog posts. To bring some order to this chaos, it looks as though a new set of cultural norms will be needed, along with an online infrastructure to support them. The idea of open, online peer review is hardly new. Since Internet usage began to swell in the 1990s, enthusiasts have been arguing that online commenting could and should replace the traditional process of pre-publication peer review that journals carry out to decide whether a paper is worth publishing. \"It makes much more sense in fact to publish everything and filter after the fact,\" says Cameron Neylon, a senior scientist at the Science & Technology Facilities Council, a UK funding body.  \n                Fast feedback \n              In some fields, notably mathematics and physics, this sort of public discourse on a paper has long been the norm, both before and after publication. Most researchers in those fields have been depositing their draft papers in the preprint server  arXiv.org  for two decades. And when blogging became popular around the turn of the millennium, they were quick to start debating their research in that form. Scientists in other fields seem less willing to get involved in pre-publication discussion. Biologists, in particular, are notoriously reluctant to publicly discuss their own work or comment on the work of others for fear of being scooped by competitors or of offending future reviewers of their own work. Adding to the disincentive is the knowledge that tenure committees and funding agencies do not explicitly reward online activity. As a result, several journals \u2014 including, in 2005,  Nature   \u2014 have tried and mostly failed to interest scientists in various forms of open review. \"Most papers sit in a wasteland of silence, attracting no attention whatsoever,\" says Phil Davis, a communications researcher at Cornell University in Ithaca, New York, and executive editor of  The Scholarly Kitchen , a blog run by the Society for Scholarly Publishing in Wheat Ridge, Colorado. Journals have had a little more success with post-publication peer review in the form of comments to the online versions of their papers. But the discussion is hardly vigorous, largely because the journals have usually solicited these post-publication critiques on their own websites, rather than on popular social networking sites. \"Who in their right mind is going to log on to the  PLoS One   site solely to comment on a paper?\" asks Jonathan Eisen, academic editor-in-chief of  PLoS Biology , and a prolific blogger and tweeter. \"I guarantee that there are more comments on Twitter about a  PLoS   paper.\" The question for researchers is how to deal with this ad-hoc analysis of papers. Unstructured, unruly and often anonymous, online commenting can be exasperating for biologists used to more conventional means of discussion. Like Sebastiani, for example, Wolfe-Simon initially tried to stay out of the brouhaha over the arsenic paper. \"Any discourse will have to be peer reviewed in the same manner as our paper was, and go through a vetting process so that all discussion is properly moderated,\" she said when the controversy first erupted. She and a co-author later did provide answers to a few of the criticisms on her website. But Goldstein, who has also had publications on the receiving end of negative online reviews, tries to take the process in his stride. \"I think if the work is solid, it holds up over time and this chatter is not going to hurt solid work,\" he says. Nonetheless, he adds, \"there can be a herd mentality to this, which one wants to be really careful of\" \u2014 especially for examples such as the longevity and arsenic papers, for which neither the rapid spike in fame nor the equally sharp fall into disrepute may be fully justified. One solution may lie in new ways of capturing, organizing and measuring all these scattered inputs, so that they end up making a coherent contribution to science instead of just fading back into the blogosphere. Perhaps the most successful and interesting experiments of this type can be found at websites such as  Faculty of 1000  (F1000) and  thirdreviewer.com , and in online reference libraries such as  Mendeley ,  CiteULike  and  Zotero , which allow users to bookmark and share links to online papers or other interesting sites. F1000, which was launched in 2002 and evaluates papers from journals across biology, is among the best known of these websites. It now relies on a 'faculty' of more than 10,000 peer-nominated researchers and clinicians who select, evaluate and rate papers with a score of 6 ('recommended'), 8 ('must read') or 10 ('exceptional'). The individual scores are then combined using a formula to generate the paper's F1000 article factor. These scores, in turn, are making some appearances in tenure packages and grant applications. \"It's the only one we've been using in any systematic way,\" says Liz Allen, who leads post-award evaluation at the Wellcome Trust in London. \"It adds another dimension to the citation index.\" However, critics note that F1000 rankings tend to correlate closely with traditional citations, which suggest that they add little, if any, extra value. And most papers never attract the attention of the faculty members, so that they are never ranked at all. Even one as talked-about as the longevity paper garnered only a single rating on F1000: a must-read score of 8. For comparison, the currently highest-ranked paper on the site has an aggregate score of 62, and scores of 20 or more are common.  \n                Meta-Twitter \n              Given the vagaries of such measures, there is a growing interest in methods that would aggregate and quantify all of the online responses and evaluations of a paper \u2014 producing what Neylon and some others are referring to as 'alt-metrics' \u2014 and compare it with more conventional metrics. \"As scholars migrate to newer forms of communication, it becomes very important to measure what they're doing and to compare,\" says Jason Priem, a second-year graduate student in information science at the University of North Carolina in Chapel Hill, who is focusing his study on alt-metrics. Neylon is leading a \u00a330,000 (US$50,000) grant proposal to create and test a working alt-metrics prototype that would rapidly measure a paper's impact by assessing all the activity surrounding it online. In addition, he and many of his colleagues champion a completely online system of pre-publication peer review that would build on the arXiv.org model, and would replace what they see as a flawed process with a more egalitarian and transparent one. That last step, however, may be a bit farther than most scientists are willing to go \u2014 even the ones who energetically blog and tweet their post-publication reviews. Although the latter activity is \"a nice secondary mechanism for catching things\", says Goldstein, \"I think we do not want it to be just a commentary free-for-all as the only arbiter of quality.\" \"It's exactly like what's said about democracy,\" he adds. \"The peer-review process isn't very good \u2014 but there really isn't anything that's better.\"  Apoorva Mandavilli is a writer based in New York. \n                     Biotechnology@nature.com \n                   \n                     Cancer@nature.com \n                   \n                     Faculty of 1000 \n                   \n                     Zotero \n                   \n                     David Goldstein \n                   \n                     Mendeley \n                   \n                     CiteULike \n                   Reprints and Permissions"}
]