[
{"file_id": "444994a", "url": "https://www.nature.com/articles/444994a", "year": 2006, "authors": [{"name": "Erika Check"}], "parsed_as_year": "2006_or_before", "body": "The development of lactose tolerance in sub-Saharan Africa is a fascinating tale of genetic convergence, reports Erika Check. The Dinka people of southern Sudan, it is said, have 400 different words to refer to the cattle that they prize above all other things. The Maasai, who live in Kenya and northern Tanzania, have traditionally believed that all cattle on Earth were given to them by the gods, and value those in their possession accordingly. The Zulu, Xhosa and Swazi in South Africa devote themselves to their strikingly patterned Nguni cattle, whose hides are now prized by high-end interior designers. And in all these pastoralist or semi-pastoralist groups, which rely on herding for survival, people drink milk \u2014 which is something of a puzzle. Most adult humans (those of European stock are largely an exception) find the sugar in milk, called lactose, indigestible. The gene for lactase, the enzyme that breaks lactose down into the more digestible forms of glucose and galactose, is normally switched off as children are weaned. Without lactase, lactose is of little use to a milk drinker; but it is still a valid food for some stomach bacteria, which can have unsettling and unpleasant results. Thus, most adults tend not to drink much milk. Now, genetic detective work is showing how, in parts of Africa, evolution found a way round this problem \u2014 just as it did a few thousand years earlier in northern Europe. The results should help clarify the origins and spread of cattle rearing in Africa, and provide a textbook illustration of the ways in which the same social innovation can write its consequences into the human genome in different times and places. \n               A drinking game \n             Sarah Tishkoff of the University of Maryland, College Park, first heard the story of 'lactase persistence' \u2014 the ability to continue making lactase throughout adult life \u2014 in an introductory anthropology course she took at college in the late 1980s. In the 1960s, anthropologists first started noticing that groups of humans that raise cattle also tended to be the same ones that could drink milk as adults 1 . In the 1970s, a genetic basis for lactose intolerance was established, although the nature of the mutation has remained a mystery 2 . Drinking milk thus came to be seen as a metabolic, not merely cultural, proclivity. People whose ancestors had herded cattle had evolved to make use of their milk. In places where lactase persistence is rare, the milk that is drunk is frequently sour \u2014 its lactose content lowered by hungry bacteria. But it was not until 1997, when Tishkoff was doing postdoctoral research with Andy Clark at Pennsylvania State University, that she began thinking about lactase persistence as a focus for her own work. Tishkoff visited geneticist Trevor Jenkins at the University of Witwatersrand in Johannesburg, South Africa, to help him study an extensive collection of DNA samples from southern Africans. She began to wonder whether she could trace the genetic origin and history of lactase persistence by studying the DNA of Africa's cattle herders. If so, it would be a neat piece of genetics. And it would also be of interest to anthropologists interested in cattle domestication itself. Had the ancestral aurochs been domesticated just once, in the middle East, and the practice then spread to Europe and Africa? Or had Africans learned to domesticate cattle on their own? Tishkoff designed a project to collect DNA samples from 43 ethnic groups in three East African countries and to correlate the data with various physiological characteristics relevant to taste perception, disease susceptibility and other things \u2014 including lactose metabolism. It was an ambitious plan at best \u2014 a foolhardy undertaking at worst. Few researchers, if any, had ever collected such a large, diverse set of genetic samples from Africa. Many of the groups she wanted to study were in remote locations, isolated by choice or by circumstance. And although she started to apply for permission to research in Tanzania in 1999, she didn't get permission to work there until 2001. \n               Living on the edge \n             Tishkoff set out with a 30-year-old Land Rover full of cases of energy bars and camping gear. Over the next three years, on and off, she and her students roamed around Tanzania, Kenya and Sudan, collecting data in some dangerous as well as out of the way places \u2014 such as parts of Kenya where carjacking was rampant and everyone carried semi-automatic weapons. The team also had a close call once, on a journey near Lake Eyasi in Tanzania, when a bus rounded a corner and smashed head-on into the Land Rover; miraculously, no one was hurt. Tishkoff credits her students \u2014 especially Jibril Hirbo and Alessia Ranciaro, who collected samples in the most dangerous parts of Kenya \u2014 with persistence and courage. \u201cThose guys literally risked their lives for this work,\u201d Tishkoff says. At most stops, Tishkoff's group hired a translator to explain the purpose of her research, get permission from local leaders, and recruit people for their research. In all, they took samples from 470 people. To test for lactase, the team members stirred powdered lactose into cups of water, gave it to each person, and took a series of timed blood samples. That told the researchers how well each person could digest lactose, as well as providing samples of the individual's DNA. In 2002, while the work was under way, a team of Finnish researchers reported that it had found a genetic mutation that seemed to cause lactase persistence in North Europeans 3 . A small change in an 'enhancer' region upstream of the lactase gene seemed to keep the gene from being switched off after infancy. All of the 137 lactase-persistent Finns studied had the mutation in question, and studies of other populations showed that the frequency of the mutation matched that of the lactase persistence. But two years later, another team reported that the Finns' genetic variant was not found in East Africans, even though some are lactase persistent 4 . Tishkoff's team showed why this was. Ranciaro sequenced DNA from her subjects and noticed several mutations close to the lactase gene. A collaborator at the Wellcome Trust Sanger Institute in Cambridge, UK, Panos Deloukas, checked for the mutations in the full set of 470 people, and Tishkoff and a postdoctoral fellow working with her, Floyd Reed, noticed that one of these mutations was tightly linked to lactase persistence in Tanzanians and Kenyans. The team also found that two other mutations were associated with lactase persistence in people in northern Kenya and Sudan, though not as strongly 5 . It was immediately clear that these mutations had arisen independently from the one found in Finland. \n               Quick start \n             The DNA also showed evidence that the mutation seen in the Tanzanians had spread very quickly. DNA accumulates small, random imperfections over the generations, and yet the stretches of DNA surrounding the lactase persistence mutation were identical in most of those who had it. This uniformity shows that the gene evolved recently and spread rapidly \u2014 which in turn means that it must have conferred an advantage strongly selected for by evolution. Statistical models of Tishkoff's data suggest that the mutation arose between 3,000 and 7,000 years ago \u2014 a blink of an eye in evolutionary time. Tishkoff and Reed conclude that lactase persistence bears one of the strongest signatures of positive selection ever observed in the human genome. Mutations that favour malaria resistance, such as the sickle-cell gene and an inability to make the enzyme glucose-6-phosphate dehydrogenase, or G6PD, are also strongly favoured and spread rapidly. (They seem to have got going at about the same time, too, and are also related to domestication \u2014 malaria is thought to have first become a major problem for Africans when they started to live in settlements.) But the positive selection for lactase persistence seems even stronger, perhaps because the costs of the mutation are less severe than those for malaria. Tishkoff and Reed suspect that the advantage might go beyond the extra calories that could be gained from the lactose. Lactase persistence might also have allowed people to stay alive during times of drought, when those benefiting from the mutation would have been able to drink milk without the risk of diarrhoea, which exacerbates dehydration. The study, and subsequent follow-up, should help to elucidate the origins of East African cultures and traditions. Anthropologists such as Diane Gifford-Gonzalez from the University of California, Santa Cruz, one of the world's experts on the origins of herding, say that these genetic data are changing the way they think about human history. \u201cUntil the geneticists contributed to the data, the rest of us always thought about evolution happening very slowly and gradually. \n               Herding along evolution \n             And the genes provide data even in places, such as East Africa, where the archaeological data are poor. \u201cThat is why this article is so interesting to people like me,\u201d says Gifford-Gonzalez. \u201cThis gives us a totally independent line of evidence about the origins of dairying, and gives us a much better way of homing in on when these major nutritional transitions occurred.\u201d The human genetic data might thus in some ways complement the genetic data on cattle, which in the decade since Tishkoff began thinking about the issue have come down in favour of multiple domestications, including one in Africa. Tishkoff's work also highlights the incredible genetic diversity in African people, a diversity that as yet has been studied very little. For example, the African DNA samples in the Haplotype Map 6 \u2014 the catalogue of genetic diversity published last year \u2014 come from only one ethnic group: the Yoruba of West Africa. But, traditionally, the Yoruba don't herd cattle, and Tishkoff didn't discover any mutations for lactase persistence in them (although in some populations of West African pastoralists, fewer people have the 'European' lactase persistence mutation). Such blindspots could be a problem, because scientists hunting for the genetic causes of diseases often rely on HapMap data. \u201cThere's such a huge amount of genetic diversity in Africa that we're clearly going to have to look at all the ethnic groups in different regions to find all the variants,\u201d Tishkoff says. \u201cOtherwise, we could be completely missing things that are important in disease.\u201d She worries that the fragmentation and disappearance of traditional cultures will make it harder to access and understand that diversity in the future. The lactose work still leaves a lot of questions unanswered, and the possibility of more adaptations still to be found. A particularly intriguing question surrounds the Hadza of Tanzania, who show a surprisingly high level of lactase persistence despite having very little to do with cattle. One possibility is that, though they are now mainly hunter-gatherers, their ancestors might have been pastoralists. Another is that lactase may offer some other benefits besides the ability to drink milk \u2014 perhaps aiding in the digestion of some specific local foods. \n               A textbook case \n             But if there are still questions, there's also a significant achievement. Scientists have to date found little genetic evidence for convergent evolution in people: \u201cThis is the best example of convergent evolution in humans that I've ever seen,\u201d says Joel Hirschhorn, a geneticist and paediatrician at Children's Hospital Boston, Massachusetts. \u201cLactase persistence has always been a textbook example of selection, and now it'll be a textbook example in a totally different way. Convergent evolution is not unknown in humans; lighter skin colour seems to have evolved independently in Europe and Asia, and a range of different malaria adaptations are known. But lactase persistence offers a particularly simple and tractable example: there's a single gene involved, with different mutations in different parts of the world having similar effects. The challenge now is to learn from this textbook example how to spot more subtle convergences that have been forced on human biology by shared experiences and cultural innovations \u2014 or that are still under way today. \n                     Evolution, consequences and future of plant and animal domestication \n                   \n                     Genetic evidence for Near-Eastern origins of European cattle \n                   \n                     First steps to low-lactose milk \n                   \n                     HapMap project \n                   \n                     Tishkoff lab \n                   \n                     Divinity and Experience: The Religion of the Dinka \n                   Reprints and Permissions"},
{"file_id": "444262a", "url": "https://www.nature.com/articles/444262a", "year": 2006, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "When landfills overflow, governments need new ways to deal with garbage. David Cyranoski visits a plant in Japan where plasma technology is turning waste into energy. Michiaki Shigehiro has a rare problem. \u201cI've struggled to find enough garbage,\u201d he says, in dead seriousness. He already sees a lot of waste, on average 100 tonnes per day, but he'd be far happier with twice as much. Shigehiro is general business manager of EcoValley Utashinai, a company named after a remote city in Japan's northern island of Hokkaido. EcoValley converts heaps of refuse into energy using a plasma arc, a jolt of electricity that ionizes gas in a chamber and produces temperatures of up to 16,000 \u00b0C, or almost three times hotter than the Sun's surface. The technology is costly and must process massive amounts of trash to recoup EcoValley's \u00a57-billion (US$59-million) investment. Utashinai city may not generate enough waste, but the rest of Japan, and the world, generally churns out too much. Every year, Japan produces about 50 million tonnes \u2014 about one kilogram per person per day \u2014 of paper, food, plastics and other garbage collectively known as municipal solid waste (MSW). The United States is almost twice as wasteful per capita, generating a total 222 million tonnes in 2005. Most of the time MSW is either stuck in a landfill or burned at a considerable cost to the taxpayer as well as releasing pollutants into the soil or air. Americans pay between US$30 and US$80 per tonne, depending on the region, to dump their trash in landfills and US$69 per tonne on average for incineration. In a small country such as Japan, where land is more scarce, local governments pay US$200\u2013300 per tonne to local landfills and incinerators. Landfills and incinerators also have knock-on economic effects, such as lowering property values nearby and, when improperly handled, may create environmental hazards. The planners behind EcoValley Utashinai thought they could solve all these trash problems at once using plasma-arc technology. In theory, waste disposal would become a profitable and environmentally benign business by converting gasified waste into energy. On paper, MSW contains one-third to one-half the energy of coal per tonne \u2014 enough to power a plant and sell excess to the national grid. But the Utashinai plasma plant is the only major energy-recycling MSW facility in operation, and it has struggled to make ends meet since opening in 2002. Several companies, betting that they can improve on the Japanese experience, are planning their own plasma-arc facilities. Atlanta-based Geoplasma is finalizing a contract for a plant more than 10 times the size of Utashinai to be built in St Lucie, Florida, a wealthy community where a massive landfill has lowered property values. If it succeeds, by 2009 it would be processing 2,700 tonnes of waste per day. And in September, Startech Environmental based in Wilton, Connecticut, announced a contract for a 180-tonne-per-day plant in Panama. Plasco Energy Group of Ontario is negotiating similar-sized plants in Ottawa and Barcelona. \n               Torching garbage \n             If these plants are built, it will be a vote of confidence for plasma arc, which despite its promise has not yet turned trash into gold. Those in the industry are optimistic. According to Hilburn Hillestad, Geoplasma's president for environmental affairs, the technology will catch on because of a \u201cperfect storm of forces\u201d \u2014 which includes increasing political and public attention on pollutants and rising energy prices. Plasma arc is an old technology, although its use in large-scale waste disposal is new. Since the 1960s, NASA has used it to simulate the high temperatures faced by space vehicles re-entering the atmosphere. Now plasma torches are used widely for melting scrap metal or destroying hazardous materials. But it wasn't until the early 1990s that companies such as Startech and Westinghouse Plasma in Madison, Pennsylvania, which builds the plasma arcs used by Geoplasma, began developing torches for trash. The torch is created by ionizing the air in a chamber with a powerful electric arc to generate plasma, which is then used to heat MSW, coke and limestone in a second oxygen-starved chamber. Under these conditions, the plasma torch heats the mixture to temperatures above 1,500 \u00b0C that enable it to vitrify inorganic materials in MSW without combustion occurring. The innocuous slag that results can be used as a construction material, although it doesn't fetch a high price. More importantly, the heat breaks down organic molecules in the MSW. Whereas combustion generates lots of carbon dioxide, in an oxygen-limited environment the MSW is converted to a mixture of mostly carbon monoxide and hydrogen, called syngas. Syngas can be used like natural gas to power a gas turbine. Purified hydrogen could itself be used as fuel. The gas mixture is further processed to minimize the amount of other pollutants, such as nitrogen oxides and dioxins, that enter the turbine or escape into the atmosphere. The Japanese have had some success with this technology. Utashinai's plant pumps out 3,000 megawatts of power per year, all of which is used to run the plant. But its supply of trash is dwindling. With a population of 5,500, Utashinai holds the title of Japan's smallest 'city', an appellation it earned 50 years ago when 45,000 people lived there. Rather than respond to a trash crisis, the plasma-arc plant was intended to spur the local economy by charging fees for waste disposal, which it does, and selling electricity and slag, which it does not. On average, the plant only processes 60% of trash volume that the company expected. The facilities also suffer operational problems, though not with the plasma torch itself, and one of the two lines is often down for maintenance. When the lines are running at full capacity, there may not be enough trash. At other times, the facility turns it away. Getting more trash from elsewhere is one option. But few citizens want their home to become a dumping ground or processing plant for trash from other areas. \u201cNo one has a good image of garbage,\u201d says Shigehiro. Some states in the United States will import garbage, collecting lucrative tipping fees. Big cities such as New York and Toronto export most of their waste for processing. But in Japan there is little trade in trash, local governments prefer to process waste locally than dump it elsewhere. In Florida, St Lucie county already has a 4.3-million-tonne landfill in their backyard. Clearing that would open up 160 acres of land and increase local property values. Geoplasma will process 1,800 tonnes of new trash and 900 tonnes of landfill trash everyday, emptying the site in less than 20 years \u2014 the time Geoplasma says it will take to recoup its US$425 million investment. Hillestad says 80% of their income will come from energy sales. The company expects to produce 160 megawatts per day, of which 120 will be sold off. \n               Trash trade \n             One difference between the St Lucie and Japanese plants that may allow higher energy output will be the use of a gas turbine, a device that turns syngas into electrical energy. The Utashinai plant uses a substantially cheaper steam turbine that can only convert about 15% of the energy to electricity. Geoplasma will use a $40 million gas turbine that is almost 40% efficient. This higher efficiency should help it to generate more energy from their syngas. But the Utashinai syngas contains small amounts of hydrogen chloride and sulphur oxides, says Shinichi Osada, head of the environmental systems division at Hitachi Metals, which installed the Utashinai plant. \u201cThese might attack a gas turbine,\u201d explains Osada. With a steam turbine, the gas heats water, creating steam for the turbine. In a gas turbine, the gas flow itself turns the turbine \u2014 a more efficient process, but one which potentially exposes the machinery to any corrosive gases. Indeed, Utashinai had a gas turbine from a previous project and Hitachi decided to replace it with steam turbines for fear of degradation. Geoplasma says that, in Florida, pretreatment will reduce the amounts of corrosive gases to acceptable levels.\u201cThe scale of the St Lucie plasma plant allows for the investment in higher technology processes in order to avoid such problems,\u201d says Hillestad. When bidding for local government contracts, plasma companies have to compete with other waste-disposal options. Some of these also promise a measure of converting waste into energy, or lower levels of pollutants. \u201cWe researched incineration, anaerobic digestion, gasification, plasma-arc gasification, bio-reactors, and alternative landfilling methods,\u201d says Ron Roberts, assistant solid-waste director of St Lucie county. The result: a 6,000-page report and a conclusion that there was only one proven solution to the MSW problem that would end the need for a new landfill. \u201cThe solution was plasma-arc gasification,\u201d says Roberts. Getting rid of the landfill was a key factor in St Lucie's decision. Geoplasma, too, needed the landfill to guarantee it enough trash. Once the landfill is gone, the company may have to close the plant, depending on the size of the regular trash supply, but they should have recouped their installation costs by then. A lack of steady trash was one reason that Geoplasma's plan to establish a plant in Hawaii failed. The local government would only guarantee 300 tonnes per day. \u201cIt was not enough for us to compete with rates of existing incineration plants,\u201d says Hillestad. Although few could argue against removing an ugly landfill, environmental groups remain wary about trace pollutants in syngas. In a 2006 report on thermal conversion strategies for MSW, California-based Greenaction for Health and Environmental Justice calls plasma-arc and other high-temperature gasification technologies \u201cincinerators in disguise\u201d. The Utashinai plant says it has met all of Japan's stringent environmental standards (although it has yet to undergo the regional government's audit). Still Shigehiro is sceptical that the technology will catch on: \u201cMany foreigners come to see our plant, but no one builds their own,\u201d he says. Roberts, who has visited Utashinai, is far more upbeat. \u201cWe are not worried about it being too good to be true. We have seen the process in operation,\u201d he says. \u201cWe certainly hope that Geoplasma has done its due diligence on the economic side of the equation.\u201d The test of that will be St Lucie. \n                     Environmental health: Megacity, mega mess... \n                   \n                     news in brief \n                   \n                     Daedalus: Gas is for burning \n                   \n                     EPA on waste \n                   \n                     Geoplasma \n                   \n                     Startech \n                   \n                     Plasco \n                   Reprints and Permissions"},
{"file_id": "444814a", "url": "https://www.nature.com/articles/444814a", "year": 2006, "authors": [{"name": "Jane Qiu"}], "parsed_as_year": "2006_or_before", "body": "Coordination and integration of the results of animal research are an ever-increasing challenge. Jane Qiu finds out what happens when big biology meets a small rodent. If mice could build a high-rise supercity, it might look something like this. Stack upon stack of clean, shiny plastic cages gleam inside the air-conditioned suites at the Wellcome Trust Sanger Institute in Cambridge, UK. With a capacity to house thousands of mouse lines, the institute has the ambition of becoming the world's largest warehouse for knockout mice, in which individual genes have been selectively deleted, or 'knocked out', from the genome. There are about 25,000 mouse genes, so the scale of the project is enormous and the cost is astronomical. \u201cNo single institute will be able to accomplish this alone,\u201d acknowledges Allan Bradley, director of the institute. The Sanger Institute is part of an international effort to accomplish this feat, which promises to change the way mouse genetics is done for ever. \u201cBiology is not a cottage industry anymore. We have to move away from thinking about one gene and one project at a time,\u201d says Steve Brown, director of the Medical Research Council's Mammalian Genetics Unit in Harwell, Oxford. \u201cTo face the enormous challenge of understanding the genome, we should be thinking about projects that will allow us to access every single gene. It sounds ambitious, but such projects are very much a sign of the times. To crack the big questions about how something as complex as a mammalian genome works, biologists are increasingly turning to multi-million dollar projects that aim to capitalize on all the information coming out of genome-sequencing projects. Although researchers are certain that the mouse projects will yield a wealth of valuable new knowledge, questions remain about whether this approach is really the best one. \u201cAlthough there is no doubt useful models will result, would the outcome of these knockout projects justify the cost and colossal number of animals involved?\u201d asks David Threadgill, an expert on mouse models of human disease at the University of North Carolina at Chapel Hill. \u201cThat's the million-dollar question. Certainly, such large-scale approaches can pay off. Last week, the Allen Institute for Brain Science in Seattle, Washington, published the results of its three-year endeavour 1  that looked at the activity in the mouse brain of almost every gene in the genome \u2014 at a cost of US$40 million. \u201cThis is a relatively small cost for the huge amount of information we have gained,\u201d says Ed Lein, a director of the neuroscience division at the Allen Institute. The resulting atlas provides significant insight into the brain's function and lets scientists measure the differences in the level of a gene's activity in different parts of the brain. \n               Big ambitions \n             The mouse mutagenesis schemes, however, are the most ambitious projects yet. Two research initiatives \u2014 the European Conditional Knockout Mouse Mutagenesis (EUCOMM) and the Canada-based North American Conditional Knockout Mouse Mutagenesis (NorCOMM) programmes \u2014 were launched in October 2005. They were joined a few months ago by the Knockout Mouse Project (KOMP), which was spearheaded by the US National Institutes of Health (NIH). At a total cost of more than US$70 million, the immediate goal of these projects is to turn the first steps to making knockout mice into a high-throughput enterprise by disabling genes in embryonic stem cells. Japan and China have also indicated an interest in developing similar projects. The teams plan to establish centralized archiving systems to distribute reagents and mice to research communities around the world. This is only a prelude to the ultimate goal of generating and characterizing the entire cohort of 25,000 knockout mouse lines, which would cost at least another US$600 million. A further project, the European Mouse Disease Clinic (EUMODIC) will be launched next February with a handsome sum of \u20ac12 million (US$16 million) for creating another 300 knockout mice and analysing 600 mouse lines in total, including those that will be generated by EUCOMM. Meanwhile, the Mouse Genetics Programme at the Sanger Institute is planning to create and characterize 250\u2013500 knockout mice per year in the following decade. \u201cUltimately, we would like to have comprehensive genotype and phenotype databases for every single knockout,\u201d says Brown, who heads EUMODIC. Geneticists are no strangers to attacking genomes with high-throughput, production- line approaches \u2014 they have been doing it for years with yeast and fruitflies, and even vertebrates, such as the zebrafish 2 . But trying to pull off a similar trick in a mammal will be much more costly. Mutants of yeast and fruitflies are easy to produce and cheap to maintain. The generation of mouse knockouts is more involved and expensive, yet these costs are dwarfed by the price tag of maintaining the animals and characterizing their physical attributes, or phenotype. Another big issue concerns the implications for animal welfare. Depending on individual genes and the method used to produce them, the number of mice needed to establish a line stretches from 50 to several hundred. On top of this, another couple of hundred animals are needed for basic analyses of genetic make-up and phenotype. So, for the 25,000 genes in the mouse genome, more than 7 million animals would be needed to generate and characterize all the knockout lines. And the consequences of knockout mutations vary greatly, from small effects that are barely noticeable to debilitating conditions. \u201cTo closely monitor potential and actual suffering and distress in those animals on a large scale is very challenging,\u201d says David Morton, a biomedical ethicist at the University of Birmingham,UK. In Europe, bodies that regulate animal research are committed to the principle of the Three Rs \u2014 refinement (to minimize suffering and distress), reduction (to minimize the number of animals used) and replacement (to avoid the use of living animals). Is the spirit of the knockout projects in line with this principle? \u201cThat is the issue researchers, funding agencies and regulatory bodies have to consider very carefully,\u201d says Morton. \n               A balancing act \n             Proponents would argue that the benefits in terms of making large advances in biomedical knowledge balance these costs. And many, such as Karen Steel, director of the Mouse Genetics Programme at the Sanger Institute, say that even though a huge number of mice is needed, the coordination and technological efficiency of these projects mean that, in the long run, fewer mice would be used than if individual labs were left to their own devices. \u201cThis kind of high-throughput project will, in fact, reduce the number of animals used in research,\u201d she says. \u201cIt will be a lot more cost-effective than what can be done by individual laboratories.\u201d The idea of an international mouse knockout project sprang from the difficulties that researchers had in generating or obtaining mutant mouse lines. Not all laboratories have the capacity to create knockout mice. And finding out which mutant lines are available is not always easy, let alone getting hold of them. A recent survey conducted by the NIH 3  shows that researchers have knocked out 11,000 mouse genes, most of which are not reported either because they belong to the private sector or because they do not have obvious phenotypes. Of the 4,000 unique knockout mouse lines that have been published, only 1,000 are in public repositories. As a result, there is a large duplication of effort: more than 700 knockouts are generated three times or more; in an extreme case, a mouse line is replicated 11 times. \u201cThere isn't a common mechanism in place to enforce deposition of knockout mouse lines into public repositories,\u201d says Wolfgang Wurst, coordinator of EUCOMM and director of the Institute of Developmental Genetics at the German National Research Centre for Environment and Health (GSF) in Munich. Many knockouts were not generated in a standardized way or properly characterized, he adds. So, rather than trying to 'recover' mouse lines scattered around various laboratories, the researchers have come to the conclusion that the way forward is to generate a comprehensive and public resource of mutant embryonic stem cells \u2014 almost from scratch \u2014 by capitalizing on efficiencies of scale and a centralized production effort. The enormous value of the mouse in biomedical research is not in dispute. Mice provide useful models for human disease and have helped researchers to make important breakthroughs in basic research in fields ranging from molecular biology to behavioural science. \u201cNobody would work with animals unless it were the only way to address biological questions,\u201d says Steel. \u201cIt's much easier and cheaper to work with cells in a Petri dish, but that doesn't necessarily tell you how genes behave in living organisms.\u201d Instead, much of the debate about the mutagenesis projects centres around their cost-effectiveness and the validity of their scientific approaches. To many researchers, it is one thing to generate a useful resource for a wider scientific community, and yet another to create and characterize 25,000 knockout mouse lines as the starting point to understand gene function. Some researchers doubt whether this is the most efficient way to obtain useful information about how the genome works. \n               The complexities of function \n             A key issue is that the function of genes is context-dependent \u2014 it depends on the genetic make-up of individual animals. \u201cSo it is not a matter of which genes do what, but how they behave as part of a complex genetic network,\u201d says Allan Balmain, a cancer biologist at the University of California, San Francisco. Researchers often come across the problem that inactivating the same gene can yield very different phenotypes depending on the breed or strain of mouse. The strain used in research is therefore often crucial for the specific biological questions being addressed. So much so, in fact, that the Jackson Laboratory in Bar Harbor, Maine, launched the Mouse Phenome Project in 2000 to collect baseline physiological and behavioural information for 40 different inbred strains of mouse, so that researchers could choose the most appropriate strain for their area of interest. But it turns out that, for technical reasons, the different groups of the knockout project have used different strains of mice as their starting points. EUCOMM and NorCOMM are using the 129 mouse \u2014 the strain in which knockout technology was first developed. But this strain has some conditions that mean it is not usually chosen for a number of disciplines, including immunology and behaviour. Most researchers work with a strain called C57BL/6, the strain that KOMP has chosen to make its knockouts, although the technology to do this is less efficient than that for the 129 mouse. Once the technology is better established, EUCOMM and NorCOMM will switch to the C57BL/6 strain. It is not clear how easy it will be to compare results from the different strains. \n               A cog in the works \n             Another issue taxing mouse geneticists is whether these production-line systems will be sensitive enough to detect every aspect of a phenotype. In many cases, a knockout mouse does not show any obvious phenotypes, and researchers often have trouble knowing where to look for subtle phenotypes even when they have some idea what the gene does. This happens because genes often have overlapping functions, and can compensate for each other if one is lost. To get around this problem, researchers frequently need to breed several knockout strains to generate double- and triple-knockouts, in which two or three genes are disabled simultaneously. \u201cUsing the knockout approach to study genes with unknown function on such a large scale may not be the most effective strategy,\u201d says Threadgill. William Richardson, a neurobiologist at the Wolfson Institute, University College London, agrees: \u201cPhenotyping is not a straightforward business. We have moved a long way from the big dream that knockouts will explain everything.\u201d But not everybody agrees. Martin Hrab\u00e9 de Angelis, director of the German Mouse Clinic at the GSF, argues that knocking out a gene because you think it might be involved in a particular process can mean that new leads are missed. \u201cMany researchers got their working hypothesis wrong and therefore focused on the wrong pathways,\u201d he says. \u201cAnd few would do comprehensive phenotype assays to study areas outside their specialities.\u201d In his clinic, established in 2001 and a partner of EUMODIC, 240 parameters in 14 different areas are measured routinely as part of a high-throughput phenotype screen of mutant mice. Of the 50 knockout mouse lines studied, 42% of them have new phenotypes not detected before. Some researchers are not convinced. \u201cPhenotype studies are more than just getting a bunch of numbers,\u201d says Richardson. Indeed, to uncover the mechanism for a particular characteristic, one needs to ask the right question. \u201cIt is a concern that we are going to end up with a lot of superficial phenotypes across many domains, but none of them is deep enough to answer difficult biological questions,\u201d cautions Threadgill. At the end of the day, few would question the power of the knockout approach in deciphering gene function in the context of human disease. \u201cThe genome projects have generated a wealth of information in terms of gene sequence, but we have no clue what half of those genes do,\u201d says Colin Fletcher, programme director at KOMP. \u201cWe hope that the knockout projects will illuminate their function in the context of human disease.\u201d But whether knockout mice can accurately mimic the complexities of human disease remains to be seen. The International HapMap Project has revealed more than 10 million polymorphisms \u2014 different versions of genes \u2014 in humans. And the Mouse Resequencing Project, which completed sequencing the genomes of 15 strains of mouse, has shown similar levels of complexity. These results are key to understanding individual variation in disease susceptibility and the effect of gene\u2013environment interactions. \u201cThe main determinant of human genetic diversity is not which genes are or are not knocked out, but which genetic variants one carries in the genome. That is where the real interest of functionality lies,\u201d says Balmain. Indeed, many researchers are trying to build such variation into the animal models of human disease and to design screening strategies by targeting specific genetic variants. To those who have a taste for big biology, genome-wide association studies focusing on variations in genes may well be the next big thing. \n                 Nature \n                  has a commercial collaboration with the Allen Institute for Brain Science in the Neuroscience Gateway. \n               \n                     An open debate \n                   \n                     Animal research: A matter of life and death \n                   \n                     Animal research: Grey Matters \n                   \n                     Animal research: caught in the middle \n                   \n                     Animal Research: Primates in the frame \n                   \n                     Cancer: Off by a whisker \n                   \n                     Price of mice to plummet under NIH's new scheme \n                   \n                     Geneticists prepare for deluge of mutant mice \n                   \n                     Animal research special link \n                   \n                     Mouse genome special \n                   \n                     European Conditional Mouse Mutagenesis \n                   \n                     National Institutes of Health \n                   Reprints and Permissions"},
{"file_id": "444812a", "url": "https://www.nature.com/articles/444812a", "year": 2006, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "Primate researchers have long faced violent protests over their work. But in some countries, regulatory obstacles are taking a greater toll. David Cyranoski meets European scientists who feel that bureaucratic pressures are closing their labs. Kevan Martin's monkey research project has been running for ten years. This year it hit a snag. Martin studies the brains of macaques to find out why higher-processing areas \u2014 the neocortex that accounts for 80% of the human brain \u2014 have been such an evolutionary success story. \u201cIf we can arrive at a general solution of the structure and function of the neocortex, it has enormous consequences,\u201d he says. The National Centre of Competence in Research, a Swiss government initiative, has reviewed and approved his work, supplying him with generous funding. Martin's project at the Swiss Federal Institute of Technology Zurich has also been reviewed and approved every two or three years by the local animal experimentation committee. But this year the licence was delayed, forcing him to halt work in June. The eleven-member committee voted 5\u20134 against the licence, but the local chief veterinary surgeon overruled the decision, and the licence was granted in November. However, six members of the committee have now exercised their right under Swiss law to challenge the licence. The legal challenge has put Martin's work on hold indefinitely. Klaus Peter Rippe is a philosopher at the University of Zurich who heads the local animal experimentation committee and is one of those protesting against Martin's licence. He attributes the extra attention on Martin's work, in part, to a change in the law. In December 2005, Switzerland reformed a law on animal welfare to protect the \u201cdignity of creation\u201d of animals. The vague wording has real effect, says Rippe: \u201cThe legal framework changed; objections that were in the moral sphere are now in the legal sphere.\u201d Martin thinks that the battle for his studies to resume will take at least another six months. \u201cIt's a relentlessly slow, drip-by-drip method of making life uncomfortable,\u201d he says. Martin has already seen one lab disappear, and fears that his will be next. Indeed, the number of primates used in research in Switzerland has dropped from 536 in 2000 to 408 in 2005. The marmoset studies of Zurich colleague Joram Feldon, head of the behavioural biology laboratory, ended in April after questions were raised by the local committee. Feldon's study involved isolating marmosets from their mothers for 30\u2013120 minutes repeatedly during the first four weeks of their lives to induce stress \u2014 a model for human depression that, Feldon says, cannot be matched by non-primate models. The potential success of that model was what worried Rippe, whose local committee requested the ethical review of Feldon's work. \u201cWhat would be the consequences if the experiment went well?\u201d The worrisome answer: drug companies would start using more marmosets for research. So Rippe sought the advice of two national ethics committees, which formed a joint working group to evaluate Feldon's project. After weighing up the stress induced and the potential benefits to humans, the committees concluded that the severity level of the marmoset studies was ethically unacceptable. Although the report does not have any legal jurisdiction over Feldon's licence, the lead scientist on the project quit, and Feldon decided to focus on the experiments with rats and mice that make up the majority of his research. Martin, however, is not yet ready to give up: \u201cI am the only person in Europe doing this research. If I'm gone, it's over,\u201d he says. Martin and Feldon both point out that their lab's treatment of monkeys has not changed since their studies began some time ago. They found the unscientific nature of the ethics review particularly galling. The working group comprised a biologist, an animal-welfare activist, a veterinary surgeon and three philosophers. In response to criticism that it did not include a primatologist, Rippe argued that they consulted four primatologists, including a member of Feldon's lab who was in charge of the marmosets. He also says that the 24-page report, which included lengthy discussions of ethical positions by Kant and other philosophers and contained no scientific citations, \u201cis an ethical evaluation of primate research. The formal rules for ethical arguments differ from papers of the natural sciences.\u201d \n               Behind the scenes \n             Bombs, threats and arson perpetrated by animal-rights extremists grab the headlines, but behind-the-scenes manoeuvring may be taking a greater toll on primate research. Many researchers in European countries feel that they are being picked off one-by-one through regulatory mechanisms. Indeed, such indirect tactics may be more effective than direct campaigns. Klaus-Peter Hoffmann, a cognitive neuroscientist who also works on vision, says that by forcing longer reviews and approval processes, or by getting them denied outright, opponents of animal research are succeeding in restricting primate research to a few centres such as the one in T\u00fcbingen. \u201cNo new laboratories are opening up in Germany,\u201d he says. Hoffmann, who will retire in two years, has previously gone to court to protect his right to do research. But, he says, there is little motivation to replace researchers like himself. \u201cI don't think the faculty is keen on primate research. The opposition is not only from the outside. Scientists also ask whether it is really necessary.\u201d Martin, too, sees primate research slowly being strangled by regulations: \u201cIf you're 35 years old and you have to stop for 18 months for approval, why would you bother?\u201d Primate research has always been the most controversial arena for the battle between animal-rights activists and researchers, and the need for it is growing. Ageing populations prompt more neurological studies into Alzheimer's and Parkinson's treatments, drug and vaccine developers say they require larger primate samples to improve preclinical trials, and stem-cell studies will need primates to bring the much-hyped research to the clinic. In 2002, some 52,000 primates were used for research in the United States and another 10,000 in the European Union. \n               Worrying trends \n             Researchers in Europe worry that their experiments will need to move overseas if, as they suspect, retiring researchers are not replaced, those coming from other countries are blocked and young researchers are discouraged from entering the field. Alexander Thiele, a neuroscientist investigating vision at Newcastle University, UK, received a five-year \u00a31.4-million (US$1.8 million) fellowship from the Volkswagen Foundation to set up a laboratory in his native Germany. \u201cYou'd expect the university to be happy \u2014 they get a professor almost for free,\u201d Thiele says. But his plans to move to the Charite Medical School at Humboldt University in Berlin were dashed in July, when its ethics review committee voted 4\u20132 against accepting him. Thiele attributes the vote partly to university politics and partly to pressure from animal-rights activists. Before the vote, critical stories, based on leaked confidential information, ran in the local papers, says Thiele. Other universities have also turned Thiele down; he's not too hopeful that he will return to Germany. Meanwhile, in the United Kingdom, where protests against primate research have the longest, and most violent, history, researchers have seen a swelling of public support and legislative actions in response to the more extreme activities of animal campaigners. Simon Festing, director of the Research Defence Society, a group in favour of animal research, says that some of the more unsavoury actions of the animal activists are generating negative headlines for them. For example, activists dug up the remains of a member of a family that ran a guinea-pig farm, forcing it to close in August 2005. The public outrage was mostly directed against the activists. In May this year, UK Prime Minister Tony Blair signed a petition in support of animal research, which has to date been signed by more than 21,000 people. In a public statement, Blair noted that new powers for the police and courts would be used to counter the threat from animal-rights extremists. Animal activists in the United Kingdom harass not only researchers, but also janitors, security guards or third parties who do business with animal researchers. They claimed victory, for example, in 2004, when the University of Cambridge scrapped plans for a large primate research facility. A BBC television documentary last month reported on a similar battle over the construction of an animal research laboratory at the University of Oxford. The documentary featured Oxford neurosurgeon Tipu Aziz, who experiments on Parkinson's disease in monkeys and performs related surgery on humans. He says that he felt the need to speak out when construction at Oxford was halted last year. \u201cIt is my strong belief that the Cambridge site was not built because scientists didn't educate people about what they're doing,\u201d says Aziz. \n               A fighting chance \n             But not all primate researchers are as determined as Aziz. This summer, Dario Ringach, a neuroscientist at the University of California-Los Angeles, announced that he would stop doing research on monkeys after the Animal Liberation Front (ALF) tried, unsuccessfully, to detonate a bomb on his colleague's doorstep. The ALF victory may prove costly in the long run. In its wake, support for a pending Animal Enterprise Terrorism Act grew. The act, which will make it easier for law enforcement to crack down on economic harassment and impose greater penalties, was signed by US President George W. Bush at the end of November. Frankie Trull, president of the National Association for Biomedical Research, a nonprofit organization that supports the use of animals in research, says that the past four or five years have seen an increase in extremist activity. \u201cI hear many stories of researchers having difficulty in finding postdocs,\u201d she says. However, the new law \u201cwas a big victory for biomedical research\u201d. The battle is set to continue. Neither side looks ready to give up over the Oxford University facility, which is again under construction. This week, an independent UK review of the scientific case for primate research announced its support for such work, but suggested more can be done to improve animal welfare. Meanwhile, as Feldon watches his colleagues come under fire, he regrets that he did not fight to keep his experiment running. \u201cGiving up might have been a mistake,\u201d he says. \n                     An open debate \n                   \n                     Animal research: A matter of life and death \n                   \n                     Animal research: Grey Matters \n                   \n                     Animal research: caught in the middle \n                   \n                     Animal research: Mighty mouse \n                   \n                     News in brief \n                   \n                     Demo backs animal lab in Oxford \n                   \n                     Animal-rights militancy exported to US and Europe \n                   \n                     Lessons from Huntingdon \n                   \n                     Animal research special \n                   \n                     Swiss report on primates \n                   \n                     Swiss Federal Veterinary Office \n                   \n                     The People's Petition \n                   \n                     UK Home Office Statistics \n                   Reprints and Permissions"},
{"file_id": "444418a", "url": "https://www.nature.com/articles/444418a", "year": 2006, "authors": [{"name": "Tony Reichhardt"}], "parsed_as_year": "2006_or_before", "body": "Philosophers since Aristotle have puzzled over the meaning of happiness. Tony Reichhardt asks what scientists, psychologists and economists can bring to the topic. Are we any closer to being able to quantify joy? Feeling happy? As you read this, are you taking a well-deserved break from your work, confident that your latest experiment is going to produce the results you want? Or is reading  Nature  a guilty pleasure, snatched in the face of other pressures \u2014 your students, your department head, writing the next grant? Sociological surveys usually try to measure well-being by asking people to assess their current level of happiness. Such questions are a regular feature of surveys of the public, such as the 2005 Pew Research Center survey, which asked 3,014 adult Americans: \u201cHow happy are you these days in your life?\u201d Half reported being \u201cpretty happy\u201d, 34% \u201cvery happy\u201d and 15% \u201cnot too happy\u201d. But asking people if they are happy raises more questions than it answers, not least of which is how to define happiness. Is it a single emotion or a personality trait? A physical state, with characteristic brain-wave patterns and biomarkers? Is it simply the absence of unhappiness, or something else? Psychologists, economists and other well-being researchers don't have definitive answers, but they're beginning to approach the subject in a more rigorous way. In the process, they hope to learn more about the link between health and happiness, and to contribute to debates over eternal questions, such as: who's happier, the Americans or the French? Daniel Kahneman, a Princeton University psychologist who won the 2002 Nobel Prize for Economics for applying psychology to decision-making in the face of uncertainty, wants to develop surveys that ask more sophisticated questions. His work investigates how a person's sense of overall life satisfaction diverges from their everyday ups and downs. Early results suggest that the two do not necessarily correlate. Kahneman and his colleagues gave two questionnaires to women in Columbus, Ohio, and Rennes, France. The first assessed overall life satisfaction. The second asked for a diary of a single day, broken into discrete episodes (had lunch with a friend, did chores), and to rate specific emotions they'd felt during each episode, on a scale from 0 to 6. This Day Reconstruction Method (DRM) 1 , developed by Kahneman, Princeton economist Alan Krueger and others in 1994, is easier to administer, but yields results similar to the more intrusive Experience Sampling Method, where subjects are interrupted by a beeper at various points during the day and asked to report their feelings. \n               Back to normal \n             Kahneman found some differences between French and American women \u2014 for example, Americans spent more time on childcare and enjoyed it less \u2014 but in general, similar things made them happy (time with friends and family) and unhappy (commuting and work). And in both cases, overall satisfaction was not strongly correlated with experienced happiness. Rich and married women reported more satisfaction than poor and single women, for example, but weren't happier day-to-day. Although preliminary, these results provide a new way to investigate the widely held idea that our level of happiness is fixed. When social scientists compared survey reports of happiness with other factors, ranging from income to pet ownership, they concluded that people with more money report being happier, but only up to a point. Beyond a certain level (for Americans in 2004, an annual household income of $50,000 to $90,000), more money doesn't bring more happiness. And in several countries where median income has risen \u2014 including America, China and Japan \u2014 happiness levels remain stubbornly constant. These and other data led well-being researchers to conclude that life circumstances don't have much affect on long-term happiness. Surveys show that happiness increases after marriage, but only temporarily. An oft-cited 1978 study found that, a year after their life-changing event, both lottery winners and paralysis victims had reverted to close to their former level of happiness 2 . This contributed to the notion of a 'hedonic setpoint' to which people return no matter what life throws their way. And based on studies showing similar levels of reported happiness in twins, the setpoint appeared to be genetically determined. Pioneers of 'positive psychology' such as Martin Seligman of the University of Pennsylvania in Philadephia developed research-based practices for being happier at the margins \u2014 by consciously expressing gratitude, for example. But most experts agreed that we were stuck at our natural setpoint. Lately, however, psychologists such as Kahneman and Ed Diener, from the University of Illinois at Urbana-Champaign, have backed away from too rigid an interpretation of this 'hedonic treadmill'. Diener argues that although genetically determined personality factors may predispose people to a certain level of well-being, different kinds of happiness change over time 3 . For example, older people report declines in both positive and negative moods. In Diener's view, happiness is not a unitary concept with a single set point. \n               In the blood \n             Kahneman thinks a greater focus on measuring experienced happiness could change our view of the hedonic setpoint. The observation that married people revert to their previous levels of satisfaction within a few years of the wedding may be explained by marriage adding some sources of daily happiness (time with spouses and children) while decreasing others (time with friends). Future surveys may need to distinguish between daily and long-term happiness more carefully. In Kahneman's quest for a measure of happiness that \u201ceconomists can respect\u201d, he and Krueger came up with a U-index based on DRM reports of positive and negative episodes. The U-index is the fraction of time a person is unhappy, and relates more to experienced happiness than to life satisfaction 4 . Krueger has been working with the Gallup Organization to come up with a five-minute version of the DRM, making it suitable for large surveys. Measures such as the U-index averaged for the general population, he says, could be incorporated into National Well-being Accounts \u2014 already used in Canada and Australia \u2014 that try to measure quality of life. Krueger, among other economists, argues that well-being is a better social goal than economic growth. But first they need to agree on a simple measure of well-being. Well-being researchers are also going beyond questionnaires to borrow tools from medicine, neuroscience and genetics. Kahneman plans to supplement his DRM data with measures of cortisol, a hormone associated with stress, to see how it varies with mood. According to psychologist Carol Ryff, director of the University of Wisconsin's Institute on Aging in Madison, there's currently a boom in well-being studies measuring cortisol. Ryff is responsible for perhaps the most ambitious well-being research project ever conducted, the Midlife in the United States, or MIDUS II, study. With $28 million in funding from the National Institute on Aging, MIDUS II is five years into a six-year study to assess adult Americans' health and well-being. As well as the psychological and personality surveys collected in MIDUS I, conducted in the mid-1990s, researchers will gather data on several biomarkers. By measuring factors such as hormone levels (cortisol or epinephrine), blood pressure or immune-system functioning, scientists hope to learn more about the physiological underpinnings of psychological well-being and distress. Studies showing that people with positive emotions catch fewer colds 5  suggest a link between happiness and physical health. \n               Cut the fluff \n             Some biomarkers, including measures of brain activity and genetic factors, have been correlated with well-being in previous studies. In the MIDUS II study, twins and siblings will yield information on the role of genetics in well-being. And the project plans a smaller version of MIDUS II in Japan, to gather data from another culture. The researchers hope to tease out the role of some of these biomarkers as they process their results. Ryff hopes to find out whether well-being and ill-being (depression and so on) have distinct biological correlates, or whether they are at opposite ends of the same psychological spectrum. One of her previous studies on a group of 135 older women assessed on biomarkers such as cortisol and waist\u2013hip ratio, suggested that the biological correlates of well-being and ill-being are largely distinctive 6 . That's one issue MIDUS II will investigate more fully. In her own work Ryff likes to distinguish between hedonic well-being (moods and feelings) and eudaimonic well-being, which is more concerned with factors such as having purpose in life, continued personal growth and development, and good relationships with others. In fact, Ryff rarely uses the term 'happiness'. Perhaps that's because the more scientists learn, the less precise the term has become. That's roughly where the science of happiness stands right now \u2014 still wrestling with its own terminology. Ryff has little time for the fluffier aspects of positive psychology, which she dismisses as \u201ca lot of PR\u201d. But one thing she and other well-being researchers can agree on is the nature of the question, \u201cWe're going after it in a serious way,\u201d she says. \u201cIn the final analysis, it's an empirical question.\u201d See Editorial,  \n                     page 401 \n                   . \n                     Nuns go under the brain scanner \n                   \n                     A search for meaning \n                   \n                     A happy gathering \n                   \n                     American dream debunked \n                   \n                     Alan Krueger's well-being research \n                   \n                     MIDUS II \n                   \n                     Australian Centre on Quality of Life \n                   \n                     Ed Diener's Home page \n                   Reprints and Permissions"},
{"file_id": "444416a", "url": "https://www.nature.com/articles/444416a", "year": 2006, "authors": [{"name": "Michael Cherry"}], "parsed_as_year": "2006_or_before", "body": "Khotso Mokhele, formerly in charge of developing research in South Africa, talks to Michael Cherry about the role that science is playing in the nation's development. It feels slightly odd meeting Khotso Mokhele in Paris, where he is attending an International Council for Science meeting. Usually we meet on African soil, but this year I am on sabbatical, and Mokhele, who is 51, stepped down in September after ten years at the helm of Africa's largest research funding agency, now known as the National Research Foundation (NRF) in South Africa. He seems quite at home, however, in the 16th  arrondissement , domain of the city's diplomats and aristocrats. With the current resurgence of interest in Africa in the West, I wanted to hear his views on the role that science and technology are playing in the development of South Africa and the rest of the continent. In 1992, the relatively unknown microbiologist was plucked from his department at the University of Cape Town to become the vice-president of the NRF's predecessor, the Foundation for Research Development. After serving a four-year apprenticeship he took over the reins from Reinhard Arndt. But he is the first to admit that his appointment was not a foregone conclusion. Mokhele is not a member of South Africa's ruling African National Congress (ANC) \u2014 and therefore not an obvious potential candidate to run a government agency. His tiny party, the Azanian People's Organisation (AZAPO), has strong roots in the black consciousness movement, a political philosophy born in the late 1960s that emphasizes the value of black identity. \n               Conflicting interests \n             During Mokhele's tenure, the research foundation extended its funding to social sciences and arts \u2014 it was formerly confined to the natural sciences and engineering. South African science has enjoyed mixed fortunes over the past 15 years: although spending on research and development (R&D) as a percentage of gross domestic product (GDP) has risen recently to 0.81%, it fell drastically in the nineties from a peak of 1.03% in 1991 to a low of 0.68% in 1997. But studentships and postdoctoral fellowships are worth less in real terms, making recruitment into R&D more difficult. But what does Mokhele regard as his biggest achievement of the past decade? \u201cWe managed to retain quality as a cornerstone of the research enterprise,\u201d he replies, with very little hesitation. The South African science community is mostly white, and Mokhele was pressed to boost the number of black scientists. \u201cA research culture can die very quickly if you uncouple it from quality considerations. As a black South African, I would have been excused if I had addressed equity issues in a manner that resulted in the disappearance of quality.\u201d This is undoubtedly true. Eight hundred thousand white South Africans \u2014 about 15% of the white population \u2014 have left the country over the past decade, citing inequality of opportunity as a contributing factor. Doctors, accountants, engineers, teachers, nurses \u2014 but very few researchers. This is something that could be regarded as a tribute to Mokhele's efforts: for example, the NRF awards grants to the best research projects, as well as providing incentives for black researchers. \u201cThe crime is that 12 years after the advent of democracy, black education has made no strides, so we are still reliant on the white community for these skills,\u201d says Mokhele. \u201cWhites may complain about affirmative action, but in reality they experience very little competition for jobs because of this lack of progress.\u201d The seeds of his passion for education and valuing black identity were sown during his upbringing. Mokhele grew up in the township of Phahameng in Bloemfontein, capital of the Free State province. His mother ran a  shebeen  (an unlicensed pub) in their house from Thursday to Sunday. Her vocation had an unexpected benefit: as a young boy, Mokhele became an expert brewer, using a paraffin tin in the backyard, ultimately inspiring his interest in microbiology. \u201cI had the knack: when my brothers tried to do it, they got the temperature wrong.\u201d \n               Race relations \n             At the age of 16, Mokhele was sent to Moroka High, a mission school in Thaba Nchu in the eastern part of the province. His chemistry teacher suggested he apply to the University of Fort Hare in Alice, Eastern Cape province \u2014 alma mater of Nelson Mandela, Mangosuthu Buthelezi and Robert Mugabe \u2014 to study agriculture. \u201cAs a township boy, agriculture was the last subject I wanted to study,\u201d he laughs, \u201cbut I did it as a last option.\u201d By the end of his first year, he was hooked \u2014 not just on food science, but also on politics, as he had come under the influence of the black consciousness leader Steve Biko. Biko lived in nearby King William's Town, and, although banned from Fort Hare by the university's reactionary administration, he frequently visited the adjacent Federal Theological Seminary, where in Mokhele's words: \u201cWe listened to him think aloud. There were also a lot of parties, and Biko would leave the dancefloor to write down his thoughts when he had a moment of inspiration.\u201d At the end of his third year, Mokhele was expelled from Fort Hare for political activity. But like many adherents of black consciousness, Mokhele has a strong sense of his own destiny. Biko's philosophy, in a nutshell, was that black people needed to develop their own sense of identity, rather than take their cue from white, Indian and mixed-race intellectuals who dominated the democratic movement at the time. \u201cI wasn't going to let my expulsion determine my fate,\u201d Mokhele says. This self-assuredness is an attribute that has made him able to work effectively with both black and white communities. Fort Hare agreed to readmit him to complete the degree and he left South Africa in 1979 for the University of California, Davis, where he completed master's and doctoral degrees. But in 1987, he returned to Fort Hare to a temporary lectureship. \u201cIt was a tough decision,\u201d says Mokhele, \u201cas I felt very comfortable in the Afro-American community in the States.\u201d Looking back over his tenure at the NRF, Mokhele says there isn't really anything he would have done differently, even the decisions that didn't turn out as well as he had hoped. He served, for example, as a facilitator for President Mbeki's controversial advisory panel set up in 2000 to investigate the link between HIV and AIDS. Many panel members were 'dissidents', set on proving that there was no link. No one envied Mokhele's task of trying to achieve even a measure of consensus between these dissidents and an enraged and alarmed group of orthodox scientists. The planned experiments on which the panel had supposedly agreed were never concluded and Mokhele's critics felt he was left with egg on his face. \u201cI had no alternative but to accept that job,\u201d he counters. \u201cIt could have played itself out in a way that would have saved us from the situation we are in now. It didn't, but this outcome was not obvious from the outset, and we failed there.\u201d South Africa is currently providing antiretroviral treatment to only 13% of its inhabitants requiring it, compared with more than 50% in neighbouring Botswana and Namibia, for example. As for the future, Mokhele believes that South Africa should play to its strengths. The country is known, for example, for its contributions to oceanography, astronomy, geology and ecology. \u201cThere is little point in the country making tiny contributions to the same fields of science that are currently fashionable in the West. We need to do two things: concentrate on areas where we have a natural advantage; and interpret what is going on elsewhere and adapt it to our specific needs,\u201d he says. \n               Continental shift \n             I turn to Africa as a continent \u2014 why has it fared so badly in comparison with other post-colonial societies? \u201cApart from Botswana, non-military democracy in Africa is very new. Africa is also the only continent where most children study in a language that they don't use when they play in the streets.\u201d And what about the mooted concept of the African Science and Innovation Facility \u2014 could an Africa-wide research agency work, in reality? He's not sure: \u201cThere has been such a reluctance on the continent to establish national agencies for research funding, and it is the strong nations that stand to benefit from globalization. The only way that such an agency could work would be if beneficiary countries were obliged to spend a certain percentage of their GDP on research in order to qualify. If it's based just on hand-outs from the West, it is destined to fail.\u201d Mokhele's career to date has benefited from his natural charm, political savvy and the odd measure of good luck. Robert Kriger, who worked closely with Mokhele at the NRF over this period, says that Mokhele's great strength \u201cis his ability to get to the core of research issues \u2014 which reflects both his experience within the international science community, and his knowledge of the workings of its associated bureaucracies\u201d. When asked about his own future, Mokhele describes himself as \u201cbeing in retirement until such time as I decide what to do next\u201d. But he is not tempted to follow so many of his compatriots out of Africa, a position to which he remains unequivocally committed. \n                     Star of the south \n                   \n                     AIDS denialists back on the upswing \n                   \n                     South Africa names head of science ministry \n                   \n                     www.nature.com/news/specials/aids/index \n                   Reprints and Permissions"},
{"file_id": "444420a", "url": "https://www.nature.com/articles/444420a", "year": 2006, "authors": [{"name": "Michael Hopkin"}], "parsed_as_year": "2006_or_before", "body": "The biggest project in the history of ecology is nearing its dawn. Can its organizers pull off the seemingly impossible and unite a disparate field behind its vision to observe the ecosystems of the United States? Michael Hopkin reports. They call it the Hubble telescope of ecology. Like astronomy's best-known space telescope, the National Ecological Observatory Network (NEON) is planned to be big, expensive and able to see most of its universe. NEON, the biggest undertaking in the field's history, would blanket the United States with a network of observation stations \u2014 providing the most detailed picture ever of the state of its ecosystems. But like the Hubble telescope, NEON and its organizers have faced a long and bumpy ride. They have fought off long-running charges that the project is too expensive, too inflexible and too poorly designed. Now they are hoping for the same sort of miraculous turnaround that the Hubble has experienced several times \u2014 to launch their project on time and with the full support of the ecology community. Funding for NEON's construction remains at the discretion of Congress \u2014 the National Science Foundation (NSF) has requested $12 million as part of its 2007 budget for building major research installations, rising to a total spend of $100 million by 2011. Other costs, such as operating and maintaining the project, are expected to more than double this figure. \n               Grand challenge \n             Meanwhile, the project's members have just finished presenting the latest version of the project's blueprint to a board of NSF officials. Should their plans meet with the approval of the board \u2014 who have experience in evaluating huge projects such as telescopes and particle accelerators \u2014 the team will proceed in making the preliminary design a reality. According to the latest blueprint, NEON will involve setting up permanent measuring stations at locations in each of 20 'domains', which split the entire United States into zones representing every major climate and ecosystem type. Each location will boast an array of instruments, including a 10-metre tower to measure carbon dioxide flux, detect nitrogen oxides and record leaf wetness. The goal \u2014 the 'grand challenges' of ecology \u2014 is to assess how ecosystems respond to natural and human-induced changes in climate, land use and the presence of invasive species. It won't be easy. As the most expensive project in ecology's history, NEON has struggled for funding for years. Part of the problem is that it's difficult to put an overall price-tag on what is still a work in progress. \u201cIt's always in the hundred-million-dollar range,\u201d says Dan Johnson, communications director for NEON. \u201cBut it's frustrating because it's so hard to cost out.\u201d To make matters worse, early designs were poorly received by the community, and project organizers were accused of taking plans behind closed doors. Even now, some ecologists worry the project's vision still has not been clearly enough stated, despite having been on the drawing board for more than a decade. The project was first dreamed up in the mid-1990s by Jim MacMahon, now board chairman, and Bruce Hayden, an environmental scientist at the University of Virginia in Charlottesville. But it wasn't until 2003 that the NSF began to include NEON in its formal plans. Originally, NEON organizers pushed to have the system as standardized as possible, with identical hardware set up at some 60 locations. Each of the 20 domains was proposed to have three permanent research stations \u2014 one in a rural area, one in an urban one and a third in a transitional zone. But the design allowed very little scope for portable measuring equipment or flexibility to address local ecological issues on an ad hoc basis \u2014 such as the risk of wildfire in a region. Chris Field, an ecologist at Stanford University in California, helped lead a team in writing NEON's latest blueprint to address such concerns. \u201cIf there was a consistent criticism, it was that earlier designs didn't present an inviting enough edifice to ecologists,\u201d he says. Field also takes up the Hubble telescope analogy to explain why NEON needs local flexibility. \u201cWith a major telescope,\u201d he says, \u201cyou can improve flexibility by ensuring that it can point at different regions of the sky, or have different instruments built on.\u201d Similarly, ecologists want to be able to adapt their machinery to hit local hot spots of interest \u2014 such as a regional drought, or a flood such as that in New Orleans following Hurricane Katrina \u2014 with mobile measuring devices to improvise their studies. \n               Progress report \n             The latest blueprint seems to be soothing a lot of nerves and egos. In an open letter accompanying the document, MacMahon said that it is \u201cgratifying to put to rest the gloom and doom that I have heard from some in the community\u201d. And until recently the worries and doubts were still very much at the surface. As recently as August, ecologists packed a conference room for a somewhat irritable 'village meeting' that updated NEON's progress. As one attendee put it: \u201cIs this still going to attract money? Please tell me so I know how badly I'll sleep tonight!\u201d But since the release of the latest blueprint, researchers in the main seem to be more determined to make the most of the new opportunities. The new blueprint involves just a single 'core site' in each domain, located within a watershed area and ideally featuring a perennial river or stream. Each site will be equipped with a tower fully kitted out to measure gas fluxes, water vapour, solar irradiation, ozone concentrations, levels of pollen and bacteria, as well as air and soil temperature, wind speed and direction, precipitation and atmospheric pressure. Each site, which will take measurements from a few tens of square kilometres, will also feature terrestrial and aquatic sensors scattered over a wider area to measure temperatures and nutrient levels. Apart from the core sites, NEON also includes a range of mobile instruments to measure specific local soil or aquatic processes; access to Earth-observation satellites and aircraft to monitor vegetation from above; and a 'land-use package' that will allow researchers to address issues such as the encroachment of new roads and agriculture into pristine land. It's certainly a lot for ecologists to take on board. \u201cI think the scientific community is generally supportive and encouraging,\u201d says ecologist David Briske of Texas A&M University in College Station. \u201cBut the flip-side is that it's unprecedented and there's no model.\u201d Hence the feelings of frustration, among some in the community, that the whole endeavour may be getting too big for itself. Elizabeth Blood, one of NEON's organizers, tries to calm some of those fears. \u201cWe have never tried to do this before, to enrich biological science in this way,\u201d she says. \u201cWe are creating new ways of doing science \u2014 ways we can get only a glimpse of now.\u201d In its new incarnation, NEON has the flexibility to address questions not yet dreamed of, says James Collins of the NSF, another member of the project team. \u201cProjects can go for 10, 20, 30 years, but then after that people will be proposing new research,\u201d he says. \u201cPeople who aren't even born yet will be developing new idiosyncracies for it.\u201d Many ecologists are finally allowing themselves to become enthusiastic over NEON. Perhaps most tellingly, the leaders of the Consortium of Regional Ecological Observatories (COREO), a group set up to represent the ecologists who will use the infrastructure, now seem at peace with the NEON board. \u201cBefore, there was surprise and dismay at the inflexibility, and at the fact that there was little option for COREO members to comment. But most members are pleased with the re-review,\u201d says COREO chair Phil Robertson of Michigan State University in East Lansing. \n               Lingering concerns \n             Most concerns revolve around the ambition of the project's timeline: the system is planned to be up and running by 2013. NEON's public-relations team insists that the project is on schedule and, although Collins admits that this is a big task, he's confident it can be done. \u201cIt's ambitious, but the community has been involved in the research and development for years,\u201d he says. \u201cThe ideas have been proofed in concept \u2014 now we're at the point of knitting them together.\u201d Although the grand-challenge questions are a useful guide, it is ultimately up to ecologists themselves to design the research that will be done. \u201cThe nuts and bolts are not in place yet,\u201d says Collins. \u201cBut these ecologists are the top investigators in their fields, so we want them to write the proposals.\u201d So the debate about what questions the system will answer looks set to remain up in the air for a while to come. The NSF is currently in the process of a 'request for information' \u2014 consulting ecologists on the specific kinds of research they think they can perform with the new equipment. That process won't be complete for a few months. \u201cUntil the request for information comes out, all bets are off,\u201d says Robertson. America's ecologists aren't quite ready to breathe a sigh of relief yet. \n                     NEON to shed light on environment research \n                   \n                     NEON home page \n                   \n                     NEON page at NSF \n                   \n                     National Academy of Sciences report on NEON \n                   Reprints and Permissions"},
{"file_id": "444264a", "url": "https://www.nature.com/articles/444264a", "year": 2006, "authors": [{"name": "Trevor Stokes"}], "parsed_as_year": "2006_or_before", "body": "What makes the perfect protein purification or the right reagent reaction? Trevor Stokes investigates the weird world of good-luck lab charms. Daniel Nelson was getting desperate. His graduate thesis at the University of Georgia, Athens, depended on his ability to isolate periodontain, an enzyme from the bacterium that causes gum disease. Yet after two years of trial and error, his protein preparation still contained too many contaminants. Enter the lucky sombrero. For his 26th birthday, Nelson's lab mates took him to a Mexican restaurant. The birthday boy asked the waiter if he could wear a sombrero hanging on the wall \u2014 then, after sharing several pitchers of margaritas, Nelson wore the hat back to the lab. Luck finally struck: the new protein-isolation protocol he had started earlier that morning showed that he had finally isolated his favourite protein. The 'purification sombrero' quickly gained a following among postdocs and students in the lab. \u201cThey have all heard the story and know if they don't wear the hat, they will be cursed with two years of impure protein,\u201d says Nelson, who is now an assistant professor at Rockefeller University in New York. Similar good-luck charms populate laboratories worldwide. As childish as they may seem, such totems can benefit researchers on several levels: as a bonding activity to bring lab members together; a giver of hope when experiments are failing or a silly release to take the edge off serious research pursuits. Stuart Vyse, a psychologist at Connecticut College, New London, says superstitions can help lab members bond in a sort of \u201cpsychological hopefulness\u201d. \n               Magical mascots \n             Charms can be found in nearly every aspect of science, from biochemistry labs that depend on reagents working properly to observatories that require clear skies. To that end, dozens of fist-sized cloth ghosts line the ceiling of the control room at the Subaru Observatory at the summit of Mauna Kea, Hawaii. In Japan, elementary-school children typically use such charms, called  teru-teru bozu  \u2014 'sunshine monks' in English \u2014 to ensure good weather for class trips and holidays. What's good for trips is good for telescopes, say staff astronomers. \u201cOver a period of time, each astronomer who subscribes to that superstition will add one or more of these things,\u201d says Gary Fujihara, an educator at the University of Hawaii's Institute for Astronomy at Manoa. Scientists and technicians tend to pass down their charm traditions. John Gaskin, a former postdoc at Washington University in St Louis, Missouri, introduced graduate student Jason Londo to the 'amulet of power' \u2014 a fake-jewel-encrusted gold chain that students wore while conducting polymerase chain reactions (PCRs). \u201cIf it works, cool,\u201d says Londo. \u201cAnd if somebody has a problem with that, we'll just see where they go when their PCR stops working.\u201d The amulet disappeared last year, but Londo replaced it with a dragon made out of twine and ribbons. Some scientists call on their colleagues for ideas for charms, which suggests that the line between fiction and fact can be crossed in the name of social fun. On Protocol Online, a life-sciences protocol database, a recent forum read \u201cThe Gods of Molecular Biology. \u2014 Pay homage! Share your experience!\u201d Three authors fretted about choosing the right sacrifice for experiments to work. \u201cMy sacrifice of a KitKat and Diet Coke every morning results in having PCR reactions work (which, without the sacrifice, don't work at all),\u201d claimed author vetticus3. Hobglobin responded: \u201cOur PCR spirits are very busy. They appear to us as squirrels and in our PCR room several squirrel statues and posters are placed to honour them.\u201d \n               Superstitious minds \n             Failure can also bring out the lab charms. At the University of Utah, Salt Lake City, an assistant professor of human genetics arranges voodoo candles in her lab to pay homage to the gods of cloning. \u201cIt's a little bit of a humorous comfort to people,\u201d she says, \u201cto see that other people can make a joke out of failure.\u201d But her embarrassment over the practice \u2014 she asked not to be named, for fear of not getting tenure \u2014 suggests the charm of good-luck trinkets fizzles out as scientists become more established. Graduate students and postdoctoral fellows are more often the ones setting up good-luck charms, compared with older laboratory heads. But Rich Whitkus, a plant evolutionary biologist at Sonoma State University in California, has kept Squeaky, a high-pitched dinosaur, for more than a decade after graduate school. \u201cIt's one of the first things that gets set up in the lab,\u201d says Whitkus. He has witnessed other researchers squeeze the dinosaur before starting PCR reactions. \u201cOf course, we have no correlation that the number of squeaks that Squeaky gets is directly correlated with the success of PCR,\u201d he says. \u201cWe have no scientific data on that.\u201d \n                     Protocol Online \n                   Reprints and Permissions"},
{"file_id": "444808a", "url": "https://www.nature.com/articles/444808a", "year": 2006, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "Many scientists have nuanced views on animal research. But they are rarely heard, says Emma Marris. Many readers of  The Guardian , a British newspaper, will have been surprised at a recent online article in which Sophie Petit-Zeman, a neuroscientist and journalist, explained how she could at the same time be a vivisectionist and a vegetarian 1 . But they will not all have been surprised in the same way. Some will have been surprised at the existence of such a complex position in an area where much of the discussion is depressingly black and white. Others will have been surprised not by the position itself, but by its being discussed in public. As a poll of  Nature  readers working in the biomedical sciences reveals, many scientists who work on animals have complex takes on the issue. But they are not often willing, or encouraged, to express these feelings. Some of this is directly due to fear of animal-rights extremists; some is an indirect effect of the polarized atmosphere that surrounds the issue. In some labs, at least, scientists feel pressured to keep quiet about the grey areas of debate, lest they undermine the official mantra And how do they square the ethics of it all? As well as polling our biomedical readers (for full results see  http://www.nature.com/news/specials/animalresearch ),  Nature  set out to get some voices from the front lines \u2014 and found a lot of ducked heads. It would be fair to say that the average researcher prefers not to talk to the press about his or her work. And yet, there are those who are not only willing to talk, but have plenty to say. It quickly becomes clear that each researcher has his or her own system of ethical equations in place, but that the simplified pro\u2013con debate makes it very difficult to communicate this \u2014 or have any kind of calm conversation about animal research. So let's meet the researchers. Tom Burbacher runs an infant primate lab at the University of Washington in Seattle, which models the cognitive effects of prenatal exposures to environmental contaminants in macaques. He talks publicly about his work, but does not defend the entire enterprise of animal research, believing it is a waste of time to defend such a large, abstract concept. He adds that he feels that his work, which is clearly linked to human health, is easier to explain than some blue-sky research, such as mapping the brain or describing how vision works. \u201cThere is a lot of basic research going on that is harder to talk about,\u201d he says. Burbacher got involved in toxicology as an undergraduate, and has been working with animals ever since. \u201cI had a study that followed animals from the time they were born until they were more than 20 years old. I got old with them,\u201d he says. Then, they were killed. \u201cIt was tough,\u201d he says. Burbacher is not shy about the emotional toll that working with animals can sometimes take. He says the toll is two-fold: \u201cIt does wear on you sometimes. I have on different occasions thought about getting out. The death of an animal is an acute stress, but the activists are a chronic stress.\u201d \n               Fine lines \n             Chris Harvey-Clark, director of the Animal Care Centre at the University of British Columbia in Vancouver, has even greater day-to-day contact with animals, working with research subjects from sea lions and hummingbirds to transgenic mice. Harvey-Clark, like many in similar positions, was once a private-practice veterinarian, and remains a confirmed animal-lover, saying he often feels emotionally closer to his charges at the centre than to his old clients' pets. In the face of their inevitable deaths, Harvey-Clark must work to retain his humanity and empathy. \u201cHow do you keep caring for the animals without being scorched by the fact that you are using them up?\u201d he asks. His answer lies in the fact that suffering and death have long characterized the relationship between humankind and the animal kingdom \u2014 from animal predators that prey on humans, to the slaughter of wild and farmed animals by humans for food. As Harvey-Clark puts it, \u201cwithout a farm background, it is hard to understand that you can both care for things and also understand that they are going to wind up being food, or data\u201d. It's a perspective shared by many scientists, including Cynthia Otto, a clinical veterinarian and researcher, who is also a disaster-scene veterinarian with the Federal Emergency Management Agency. She has cared for search-and-rescue dogs in the immediate aftermath the terrorist attacks of 11 September 2001 in Manhattan, and has gone looking for pets left behind by Hurricane Katrina in New Orleans. She kills rabbits in research, but she also treats pet rabbits as a vet. \u201cI also eat meat. I respect what the animals can give me. If I was not going to do any animal research, I would not eat meat or wear leather, but in a certain way, I feel like that would be less respectful \u2014 not taking advantage of what they can give us.\u201d But somehow, the complexities of arguments such as these seem to have been lost. \u201cWhenever you talk about the research, the stock answer is to say that we are curing cancer or saving premature babies. You don't talk about finding out what a bit of the brain does just because you are quite curious to know,\u201d says one UK researcher, who works with rats, but prefers not to be identified because he worries that he might be targeted by animal-rights activists. \u201cI have heard animal-research advocates say that you have to say everything as nicely as possible, and that edges towards fabrication. They say that everything heads towards a cure for something and that all the experiments work. We should be more honest because a lot of people will still go along with it.\u201d \n               Finding a voice \n             The same researcher feels that some scientists go so far as to imply an anguish they don't really feel about the practice. \u201cI think the standard thing that people tell you is that they are some sort of tortured soul that can't sleep at night but have to do it to cure cancer. I do feel bad about using rats but I can sleep at night \u2014 we use the fewest possible and try our best to ensure they suffer as little as possible. People have been pushed into portraying themselves like that because of animal-rights activities.\u201d The silence \u2014 or spin \u2014 from the researchers may be most profound in the United Kingdom, where radical animal-rights activists have perhaps the longest and most disruptive history. Colin Blakemore, a neuroscientist who has long been the target of animal-rights extremists' activities and who now heads the UK Medical Research Council in London, says that this strategy is flawed. \u201cWe are perceived as being distant and unwilling to speak and therefore as having something to hide,\u201d he says. Blakemore has encouraged universities to let their faculty members talk freely about their work and supported more public scrutiny of animal research. But Blakemore doesn't necessarily believe that there is much hidden discussion or soul-searching about animal research inside the field, apart from complaints about the bureaucratic acrobatics involved in working with animals. \u201cIt is not the kind of thing that's always talked about in the coffee room,\u201d he says. And yet he has his own shades of grey. Although he vigorously defends the practice, he is relieved to be done with his years working directly with animals, despite his consistently vigorous defence of the practice. \u201cThe longer I used animals, the less comfortable I was with it,\u201d he says. It's a sentiment he believes is common in the research community. \u201cI don't know of a single scientist who would not prefer to use alternatives if they were available.\u201d Burbacher believes that talking about the details and benefits of his work is the best policy. Activists have left black roses on his doorstep and mobbed his office, but he keeps talking. \u201cThe more people know about what is going on in the lab the better,\u201d he says. But even he is circumspect when he first meets people. \u201cIf I don't know the people very well, I usually tell people I teach. You can usually tell whether the animal part should be brought up or not.\u201d Marin Stephens, vice-president for animal-research issues as the Humane Society in Washington DC, believes that the polarized debate has harmed communication between researchers and mainstream groups such as his organization, which favours a reduction in animal research. \u201cThere is not a lot of crosstalk,\u201d he says. \u201cThere is a lot of stereotyping and demonizing on both sides.\u201d He adds that the Humane Society co-founded a confidential group of stakeholders from both sides that is meeting to discuss the issues and to try to undo some of this polarization. \u201cIn my own experience, I have found a diversity of opinion in the research community,\u201d says Stephens. \u201cSome care greatly for animals and are very helpful in changing the status quo. Others see things in black and white and have no use for people like me.\u201d And researchers aren't the only ones being unfairly lumped into one position. Stephens says that the Humane Society has sometimes been grouped with disruptive extremists for political reasons. \u201cIf you pin down individual researchers, they would admit that there are different perspectives in the different groups,\u201d he says, \u201cbut what one hears quoted is that they are all of a piece.\u201d To find out more,  Nature  conducted an anonymous survey of 1,682 readers working in the biomedical sciences. What we found supported the claims quoted here: that there is a diversity of opinion within the community about animal research, with many respondents calling for more dialogue among scientists and with the public. The majority of respondents, just over 70%, believed that the animal-rights movement had made voicing a nuanced view of animal research in public more difficult for individual researchers. Some commented that it also hindered debate within the science community of how to best do animal research. Otto says she has experienced this dampner on discussion first-hand. She is trying to develop alternatives to using animal models in her speciality, the study of sepsis. She wants to move away from disease-model animals to real sick animals in veterinary practice. She believes they will be better models, but part of her motivation is to help individual animals. \u201cNaturally occurring diseases might be a better reflection of what is going on in a human than inducing something in a mouse or a rat. People's pets that spontaneously develop these overwhelming infections need the treatment, and we would be more responsible because we are not actually creating diseases in animals.\u201d Otto says she's had a hard time getting a hearing for her proposal from researchers resistant to change. And she's not alone. Ian Roberts, epidemiologist at the London School of Hygiene and Tropical Medicine, put forward the idea in a recent article in the  British Medical Journal  that all animal-research studies should be preceded by systematic reviews of their clinical utility, as human trials commonly are 2 . To illustrate the helpfulness of doing so, he and his co-authors surveyed some reviews that said many animal studies were of dubious worth. Although there were some considered objections to the paper, including one from Blakemore 3 , there were also some angry responses from both sides. \u201cOne of the things that made the argument more difficult,\u201d Roberts says, \u201cis that in animal experiments, views are so polarized that you can't get any serious discussion about the methodology. When we first published, we were accused of being anti-vivisectionists, and then of being vivisectionists. The whole debate seems to cause people to become emotional and irrational.\u201d \n               Rational responses \n             So what can be done? Discussion groups such as the one Stephens attends are one potential solution, as is the Boyd Group, an independent UK forum founded in 1992 to produce solid information about animal research and bring those of differing opinion together. The group was founded by Blakemore and Les Ward, an animal-rights activist. Ward has since left the group, which he says has become stalemated. But he believes the group was useful in that it was one of the few places where moderate activists and moderate scientists sat down and talked things over. \u201cI want to see the total end of animal experimentation, but I am not stupid enough to think that it is going to happen overnight,\u201d says Ward. \u201cEveryone has to be willing to move their position.\u201d Ward says that scientists rarely sought him out to talk about their qualms about research, but when he visited labs, he was swarmed. \u201cIt was clear to me that they wanted to speak,\u201d he says. \u201cThey were intimidated, but claiming intimidation is also a dodge to avoid having to speak.\u201d Another inclusive effort was the Nuffield report 4 , a two-year effort to capture the state of the debate run by the UK-based independent Nuffield Council of Bioethics. Barry Keverne, chair of the Royal Society's animal-research group, says it was one of the best. \u201cThey produced a huge document \u2014 it has all members of society in it, lawyers, historians, philosophers, scientists and anti-vivisectionists. Most scientists have no problems discussing this openly with people who have a genuine interest.\u201d In Europe, the moderate approach is more established. Vera Rogiers is a toxicologist at Vrije University in Brussels, Belgium, and the chair of ECOPA, the European Consensus-Platform for Alternatives, which promotes the development of alternatives to animals in research. She says that in the rest of Europe, the discussion is more \u201chealthy\u201d. \u201cMost people are very well aware of the dialogue and the development of alternatives. We ask that the people who are around the table sign a statement that they accept the three Rs \u2014 reduce, replace and refine. Usually all the people that we have around the table have no problem with that.\u201d The old adage may truly fit here: there are as many views about animal research as there are thoughtful people. But as long as the debate is played out as a ping-pong game between hard-core activists and hard-core defenders, anyone in the middle who stands up to be heard risks getting hit. \n                     An open debate \n                   \n                     Animal research: A matter of life and death \n                   \n                     Animal research: caught in the middle \n                   \n                     Animal Research: Primates in the frame \n                   \n                     Animal research: Mighty mouse \n                   \n                     Cancer: Off by a whisker \n                   \n                     Bioethics: An easy way out? \n                   \n                     Animal research special \n                   \n                     The Boyd Group \n                   \n                     The Nuffield Report \n                   Reprints and Permissions"},
{"file_id": "444811a", "url": "https://www.nature.com/articles/444811a", "year": 2006, "authors": [{"name": "Kerri Smith"}], "parsed_as_year": "2006_or_before", "body": "Researchers aren't the only ones who concern themselves with animal welfare in the lab. Vets are asked regularly to monitor and care for these animals \u2014 a role that can call for some difficult decisions. Kerri Smith talks to Sarah Wolfensohn, head of veterinary services at the University of Oxford, UK, about the challenges and conflicts presented by caring for experimental animals. \n               By law, UK institutions doing animal research must have a vet to oversee such work. What does your job involve? \n             The primary role is to look after the health and welfare of the animals. We have to do routine visits, and quality control of the animals' environments. We do a lot of health monitoring and advise licence holders on their project licences or experiments, refining them to reduce the impact on the animals. As part of that we are constantly advising on, for example, different anaesthetics or methods of collecting blood samples. We also run all the training courses for licence holders and do research projects, trying to find objective measures of animal welfare. We are not in the position of accepting or vetoing programmes of work \u2014 my team feeds into the ethical review process of how welfare can be improved, but our role is an advisory one. \n               You specialize in the welfare of primates \u2014 which in Oxford generally means macaques. How have you been improving their lot? \n             The most significant difference was when my group got agreement from Oxford's research groups to include foraging in the primates' environment. It's now official \u2014 the National Centre for the Replacement, Refinement and Reduction of Animals in Research has just published guidelines on primate accommodation and care, which state 'all primates should be given the opportunity to forage daily'. A primate in the wild will spend 75% of its time foraging for food \u2014 it's not natural to put it in a cage and give it a lump of food at meal times. Putting in wood shavings in which the food is hidden means they can forage around and look for it. It takes time, so they spend their time much more naturally, and they can use exploratory and social behaviours. It makes a huge difference to primate lifestyle. This followed on from a general shift in practice towards housing primates together in groups. Twenty years ago, you might have had eight single cages in a room, each housing one monkey. Now you have all eight monkeys having the whole room. They can indulge in social behaviours and interact, and they can use the room three-dimensionally. \n               How do you train scientists? \n             In a number of ways. For example, we've made a DVD about how you can improve primate welfare by taking them out of cages and putting them into open rooms. It has made a significant difference, especially in the United Kingdom, where the majority of animals are held in groups. \n               You must come up against some fairly negative opinions. \n             It's always a challenge. As a vet surgeon you take an oath when you qualify that says that your constant endeavour will be for the welfare of animals committed to your care, but when you're dealing with experimental animals, their welfare has the potential to be compromised. That inevitably puts you in a difficult position. We have to balance the welfare against the quality of the science, and there are occasions when that is challenging. The scientific benefit may be absolutely clear and outstanding or it may be rather more vague. \n               Are there other sources of conflict? \n             I went to a conference last year where somebody \u2014 not a lab animal vet \u2014 came to speak from the Royal College of Veterinary Surgeons, and he said: 'When I came here I was wondering why anybody would work in this field, and now I've spent a day talking to you all, I still don't understand why anyone would work in this field.' It's because you are caught in the middle all the time. The anti-vivisectionists don't like you, because you're on the other side, as they perceive it. Some scientists perceive you as trying to change the way they do their work. The whole ethical review process and the legislation is seen as a hurdle by many scientists. What you do have to protect against is the minority of over-zealous scientists who are single-mindedly pursuing their scientific goal. Those are the problem people \u2014 those few affect everybody else. \n               What impact does being squeezed from both sides have on your work? \n             One of the problems with the anti-vivisection movement is that because it targets everybody involved, it's really difficult to recruit people to this area. The net effect is that the anti-vivisection movement is bad for animal welfare. You constantly have to think, 'because I'm making a difference and I'm improving animal welfare, I have to ignore the anti-vivisection pressure, and think about the medical benefits that come out of this'. In order to have medical benefits, there will have to be some animal use, and therefore you have to do your best for the welfare of the animals that are being used, and that's why I do it. I know that over the past 20 years I have made a difference to the welfare of animals that have been used, despite the pressures from the anti-vivisectionists on one side and the scientists on the other. \n                     An open debate \n                   \n                     Animal research: A matter of life and death \n                   \n                     Animal research: Grey Matters \n                   \n                     Animal Research: Primates in the frame \n                   \n                     Animal research: Mighty mouse \n                   \n                     On their own \n                   \n                     UK animal labs still under siege \n                   \n                     Lab rats go wild in Oxfordshire \n                   \n                     Building firm pulls out of Oxford research lab \n                   \n                     Animal research special \n                   \n                     Oxford University Veterinary Services \n                   \n                     National Centre for the Replacement, Refinement and Reduction of Animals in Research \n                   Reprints and Permissions"},
{"file_id": "444997a", "url": "https://www.nature.com/articles/444997a", "year": 2006, "authors": [{"name": "Mark Schrope"}], "parsed_as_year": "2006_or_before", "body": "Kerry Black travelled the world in search of the best surf spots. Then he decided to build them himself \u2014 on land. Mark Schrope meets the maverick oceanographer. Lori Wilson Park, near Cape Canaveral on Florida's east coast, is not known as one of the world's best surf spots. On a typical weekday approaching sunset, a handful of surfers might be found catching some good, but not great, waves. Florida's finest surfing doesn't usually occur until a tropical storm or other suitable weather system passes the right distance offshore. Surfers here may grumble at times, but they accept they are at the mercy of the ocean's fickle moods and the beaches' idiosyncrasies. But surprisingly enough, that may be about to change. Soon, surfers at Lori Wilson Park could become the first ever to have a choice between two new surf options: catching natural waves that have been dramatically improved by an artificial reef, or driving an hour inland to ride great artificial waves at a newly developed surf park. When that happens, they can thank Kerry Black, a physical oceanographer and lifelong surfer who has spent the past decade trying to override the ocean's whims. He plans to install offshore artificial reefs at places to better control waves and, in the process, improve surfing, create new habitats for marine life and prevent beach erosion. At the same time, Black and his New Zealand-based company, ASR, are working with US partners to construct the first wave pool purpose-built for surfing \u2014 which would generate phenomenal waves rivalling those at the world's best natural surf spots. \n               Breaking technology \n             Black seems well qualified for both tasks. Born in Melbourne, Australia, he began surfing when he was 14 and eventually trained as an oceanographer. For decades, his research focused on topics such as larval dispersal along reefs and sand transport along beaches. But in 1995 he accepted a professorship at Waikato University in Hamilton, New Zealand \u2014 a position that, he says, offers a great deal of autonomy \u201cas long as you don't break any obvious rules\u201d. To the university's surprise, Black decided to shift his research focus. \u201cAbout two months in,\u201d he remembers, \u201cI was literally wandering through the grounds wondering what I wanted to do when I thought: let's see if we can understand surf breaks. And that's when we set up the programme.\u201d His wave-research programme included travelling to 44 of the world's best surfing spots \u2014 from Bingin in Bali, to Malibu in California \u2014 to gather sonar data on the topography of the reefs there. Black was flooded with applications from students who wanted to be involved, but faculty response ranged from shocked to cynical to confrontational. In time, the university community came to accept the research, says Black, but the larger academic world was not as malleable. \u201cI took the responsibility to actually explain to every student that if this is your degree course, you could be studying for a profession that does not exist,\u201d he says. Today, that profession has to some degree emerged. After seven years at Waikato, Black left to form his company with some of his graduate students. ASR is based in the surf town of Raglan, made famous in the 1966 documentary  The Endless Summer . The company's main interests are in designing and building surf pools and artificial reefs; dozens of the latter are in various stages of development in nations such as India, South Africa and the United Kingdom. And in August, the company sponsored the biennial International Surfing Reef Conference, a three-day event that drew some 50 researchers to Black's own surfing resort on Lombok, Indonesia. There, for the first time, many of the presentations were able to go beyond theory and into case studies of reefs that have actually been built. \u201cIf you add up all the surfing breaks in the world and count how many you would call classic surf breaks, you realize it's a very small fraction,\u201d says Black. This suggests that a nearly magical combination of conditions must align to create great natural waves. Black hopes to capitalize on his knowledge of the world's classic surf spots to build artificial reefs that create the same sort of surfing experience. \n               Wave theory \n             The exact details are proprietary, but in the simplest terms, the important factors are the slope of the reef on its ocean side \u2014 which helps determine when and how a wave will break on the landward side \u2014 and the angle of the reef in relation to incoming swells, known as the 'peel angle'. For good surfing, it is essential that waves do not break all at once but peel \u2014 or break in such a way that surfers can ride in front of the crest on an open wave face, or even inside the breaking portion, known as the barrel. Other critical wave characteristics are size and power; not surprisingly, the bigger and more powerful the wave, the better the surfing. Artificial reefs can control these factors, to some extent, by concentrating the waves' energy. The surfer's ideal wave usually requires a light wind blowing out from shore, to groom the wave face and make it smoother \u2014 but reef designers can't do much about that. Artificial reefs are not as scenic as natural ones; at ASR, they are made of huge sand-filled bags, up to 50 metres long, made from synthetic fibre sheets. Every location has different wind and ocean swell conditions, so each reef must be designed individually for that spot. Reef designers must also meet specific wave goals: for instance, whether to maximize the number of surfable days per year or the quality of waves when the very best swells come through. Reefs can also be tailored for intermediate or advanced surfers. \u201cThere is a gigantic juggling exercise that goes on in trying to balance all these different effects,\u201d says Black. Another factor to juggle is whether the artificial reefs can help protect the beach from erosion. Florida's Lori Wilson Park, like many beaches worldwide, has to be repeatedly replenished by sand dredged from offshore, a process called 'nourishment'. \u201cIt is basically buying you time, because you're putting the beach back to where it used to be,\u201d says Robert Dalrymple, a coastal engineer at the Johns Hopkins University in Baltimore, Maryland. \u201cOne thing it doesn't do is cure the original cause of the beach erosion. If it eroded before, it's going to erode again.\u201d Such nourishment projects can protect beachfront properties and restore vanishing beaches, but also have high financial and ecological costs. Black and his colleagues think artificial reefs could hold a solution. Artificial reefs could work in some instances, says Dalrymple, as anything placed offshore that reduces the wave energy hitting a beach could potentially reduce erosion. But he warns that the solution isn't necessarily simple. \u201cYou have to be really careful because you can also put a structure offshore that makes the erosion problem worse,\u201d he says. For instance, an artificial reef could capture sand that would have otherwise been transported to \u2014 and deposited on \u2014 another section of beach. The first test of one of Black's designs came at the Narrowneck Reef on Australia's Gold Coast, a massive 60,000-cubic-metre structure completed in 2000. The project, funded mainly to protect the beach from erosion, also included dumping nourishment sand in areas up and down the beach, either side of the reef, to balance erosion that might occur as sand shifted and the beach adjusted to the new structure. \u201cSo far things are going great,\u201d says John McGrath, an engineer with the Gold Coast City Council who oversees the monitoring work. The beach inside the reef has widened and nearby erosion has proceeded gradually at expected rates, although McGrath says problems could conceivably still arise given that the period since construction has seen fewer than normal severe storms. \n               Against the current \n             Very little is really known about how artificial reefs can affect erosion, says Muthukumar Narayanaswamy, a graduate student at Johns Hopkins who works with Dalrymple. He recently completed a white paper on the potential of artificial reefs to prevent erosion. For instance, a small artificial reef off El Segundo, California \u2014 not built by ASR \u2014 settled into the sea bed so deeply that it neither improved surf nor reduced erosion. \u201cA lot more work needs to be done before saying that these structures are the cure-all for coastal erosion,\u201d he says. Narayanaswamy's study was commissioned by the Surfrider Foundation, based in San Clemente, California. Chad Nelsen, the group's environmental director, is also cautious; he worries that artificial reefs could create a false sense of security that might promote even more coastal development than has already occurred in the United States. Nelsen thinks proposals for artificial reefs should be carefully scrutinized. Black, he says, \u201cis a salesman and also a talented scientist, but he's motivated to build reefs\u201d. At Narrowneck, the artificial reef hasn't been as good at enhancing surf as it has at preventing beach erosion. Waves have improved somewhat, but not as much as intended. Although designed by ASR, the company was not involved in its construction, which was handled by local engineers. The bags that make up the reef were dropped from a barge, and the reef ended up not being as high as planned. This kept the barge from grounding itself on the bags, but also reduced the reef's impact on incoming swells. But Narrowneck has worked well ecologically. The reef is covered with seaweed that supports marine creatures such as worms, crabs, lobsters and a wealth of baitfish. Corals cannot attach themselves to the geotextile material, but there are few nearshore reefs in the area, making it biologically valuable, says Steve Smith, a marine biologist at the University of New England in Coffs Harbor, Australia. Narrowneck is also popular with snorkellers. Elsewhere, ASR has had more luck with achieving both good waves and good ecology at its 6,000-cubic-metre Mount Reef in Mount Maunganui, New Zealand. To get around the barge depth problem, Black's team designed a system for connecting the bags using seatbelt material. Workers dragged a web of empty bags into place using anchors in the sea floor, then filled them with sand using dredging equipment. \u201cThis is the first reef we've had control over all the way,\u201d says Black. \u201cIt is really proof of all parts of the concept. Mount Reef saw its first significant swell in October. \u201cTo see some waves breaking as we knew they would, that's a pretty good feeling,\u201d says David Neilson, executive director of the Mount Reef Trust, which raised money for the project. Sand has also built up between the reef and the beach, and crayfish have colonized the reef, along with mussels, crabs and snapper. \n               Surfers' paradise \n             But even the world's best artificial reef relies on storms, winds and tides to create great waves. To get around that problem, Black's team set out to design a new kind of wave pool specifically for surfers. The first is under construction in Florida. There are countless wave pools around the world already, mainly for lazy, family fun in water parks. Although some of the pools produce surfable waves, none are specifically designed for surfing. Black realized that a few relatively simple changes to basic designs could dramatically improve the situation. Wave pools typically get wider as you move away from the wave machine, and they use fresh water. To improve surfing, ASR's design calls for narrowing pools to concentrate energy and salt water to improve buoyancy. More critically, wave pools don't contain reefs. So ASR designed a patented structure called the Versareef, made of steel triangles overlain with rubber, to sculpt pool waves to satisfy even the most advanced surfer. The shape of the Versareef can be shifted on demand to allow different configurations \u2014 thus producing waves that mimic those of the world's best surf breaks. The first Versareef pool is a beginner's model that will produce waves about 1 metre high, at the Ron Jon Surf Park in Orlando, Florida. If this is successful, a larger pool, for expert waves up to 2 metres high, will be built soon after that. A number of other projects, including in England and California, are on hold until the first pool is successfully completed. \u201cNo one will sign until this one works,\u201d says Black. In August, the pool was filled and the first waves created, but the prototype Versareef was unable to withstand the pressure of the waves. On a recent visit to the test pool, despite the setback, Black spoke confidently of the prospects for success. \u201cWe know it's going to work,\u201d he says. The reef structure has now been beefed up substantially, and the first successful waves were produced at the beginning of December. For Florida surfers, the pools clearly can't be completed soon enough; thousands are already on the waiting list for park membership. Black envisages that one day the pools will host unprecedented surf contests where competitors will be able to ride identical waves, which will enable more accurate judging. And he hopes the pools could even be used for wave research, as they will be larger than most wave tanks that are currently available to scientists. Dalrymple says the pools could be a valuable resource if they are not too expensive for scientists' research budgets. Back in Cocoa Beach, Florida, surfers may also see better surfing options soon. John Hearin, an engineer with NASA, is working with Black and others to push through plans for an artificial reef that Hearin designed for his master's degree. The plan, most likely for Lori Wilson Park, is still in its early stages, but local officials and business have been supportive. So, with natural waves enhanced by artificial reefs and unbelievable artificial waves available on demand, will surfers be willing to give up their famous global quest for good waves? Absolutely not, says Black. He views the pools and artificial reefs as a great addition to surfing, but not as replacements for the real thing. Surf beaches are his favourite place in the world to go, he says. \u201cI really need these places for my own health to get away and just surf.\u201d \n                     Ship endures record-breaking waves \n                   \n                     Oceanography: Noah's flood \n                   \n                     Life of the ocean wave \n                   \n                     Surf's too far up \n                   \n                     ASR Limited \n                   \n                     Surfrider Foundation \n                   \n                     Ron Jon Surf Park \n                   \n                     Space Coast Reef \n                   \n                     Mount Reef project \n                   \n                     Narrowneck Reef \n                   Reprints and Permissions"},
{"file_id": "444807a", "url": "https://www.nature.com/articles/444807a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "Few issues in biology have the power to fire up emotions like the topic of animal research. The public face of the debate is largely characterized by polarized positions: the use of animals in the laboratory is simply either 'all good' or 'all bad'. The imagery and arguments tend to be quite crude. On the one hand members of the public are confronted with images of angry animal-rights protesters picketing labs and offices, calling for an end to all animal experimentation. On the other, they are presented with claims from supporters of animal research that the use of such creatures in the lab is essential and should not be questioned. Somehow, the voice of the middle ground has been lost. With stories appearing in the news of scientists and their families being threatened by extremists, it is not surprising that many researchers are reluctant to talk publicly about their work. But, as Emma Marris reveals on  page 808 , those who are prepared to discuss the issue add a range of highly nuanced views to the debate. To explore this further,  Nature  conducted an anonymous survey of 1,682 biomedical scientists, and found a similarly broad range of opinions. Interestingly, the survey also suggested that many scientists feel that they receive insufficient support from their universities or institutions to allow them to speak on the subject more openly (see  page 789  and the full poll results online at  http://www.nature.com/news/specials/animalresearch ). Bench scientists are not the only ones who can find themselves in the firing line. On  page 811 , Kerri Smith talks to a vet, whose job it is to care for experimental animals, about the challenge of balancing the pressures from both anti-vivisectionists and researchers, while trying to improve the lives of lab primates. On  page 812 , David Cyranoski speaks to researchers working on primates who are worried that, in addition to violent animal-rights protests, regulations are an increasing threat to their work. And on  page 814 , Jane Qiu looks to the future of mouse research and how the advent of large-scale genetics projects will affect the use of this rodent. To voice your opinions in this debate, visit    Nature   's newsblog at  \n                   http://www.nature.com/news \n                 . \n                     Animal-research reporting set for shift \n                   \n                     UK panel urges animal researchers to go public \n                   \n                     Britain seeks compromise on animal research \n                   \n                     Defeated but not deterred \n                   \n                     Animal research special \n                   Reprints and Permissions"},
{"file_id": "444026a", "url": "https://www.nature.com/articles/444026a", "year": 2006, "authors": [{"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "Statistics on scientific investment and performance are lacking across the Muslim world. Declan Butler analyses the best of what is available. The full  Islam and Science special  is available from news@nature.com.   Stretching from Indonesia to Morocco, and from Uganda to Kazakhstan, countries with large Muslim populations are home to some 1.3 billion people. The Islamic world encompasses remarkable diversity in political systems, geography, history, language and culture (see  page 20 ). But science in these nations is weak, with spending on research and development far lower than the global average. This much is acknowledged to be true, but what of the details behind the broad picture? The official statistics database of the Organization of the Islamic Conference (OIC) reveals information on each of the 57 OIC nations on everything from arable land per tractor to Internet users, but you won't find any data on research 1 . Science indicators for OIC countries are also scarce among data collected by the World Bank and United Nations agencies \u2014 largely a reflection of many of these countries' low level of interest in science. To get a more detailed picture of how OIC countries measure up on science and technology, and of what patterns exist within OIC countries,  Nature  extracted science indicators from official sources and reanalysed them for the OIC group as a whole, creating an overall picture of science and technology indicators for OIC countries.  Fig. 1 Recognized authorities such as UNESCO or the World Bank Development Indicators have few reliable data on science spending in most OIC countries. But combining these data for 20 OIC nations covering 1996\u20132003 gives the average annual spend on R&D as 0.34% of GDP, much lower than the global average over the same period of 2.36% (refs  2 , 3 ). Although many OIC countries are among the world's poorest, with almost half being developing countries, their spending is consistently less compared with the national average across a range of income brackets. The exceptions are Malaysia and Turkey, whose spending is comparable to other moderately wealthy nations. Nowhere is the OIC deficit greater than in the oil-rich nations; Saudi Arabia and Kuwait, for example, spend less proportionately on research than the poorest OIC countries (see chart below, and  page 28 ). Part of the explanation lies in spending priorities. Many OIC countries, particularly the richest, spend more on armaments than on science, education or health 3 . Six of the world's top ten military spenders as a share of public spending are OIC countries: Kuwait, Jordan, Saudi Arabia, Yemen, Syria and Oman (each spending above 7% of GDP on arms in 2003). African OIC countries, in contrast, tend to spend proportionately less on the military. \n               A tough lesson \n             Although the science budgets of the OIC countries are all near the bottom of the world league, their spending on education is more variable. Malaysia, Saudi Arabia and Yemen's relative education budgets are among the world's highest. Morocco, Tunisia and Iran also spend respectable sums on education. All six were among the world's top 25 spenders on education in 2002 (ref.  3 ). The OIC countries' performance in education also varies more widely, according to the World Bank's 'education index', suggesting that many have the human resources that could exploit greater investment in science and technology 3 . But of the 20 poorest performers on this score, 15 are OIC countries, including many African nations, Bangladesh and Pakistan. The OIC countries' low investment in science and technology is also reflected in a poor scientific output, including low levels of scientific articles and numbers of researchers. World Bank Development Indicators for 1996\u20132003 record numbers of researchers per million people for 19 OIC countries 3 . These OIC nations cluster at the bottom end of the global scale. The top global performers (Finland, Iceland, Sweden and Japan) all have above 5,000 researchers per million people. The highest scoring OIC country is Jordan, with 1,927 researchers per million people; the OIC average is 500. Of the 28 lowest producers of scientific articles, as recorded by the US National Science Foundation 4 , half are OIC countries. In 2003, the world average for production of articles per million inhabitants was 137, whereas none of the 47 OIC countries for which there were data achieved production above 107 per million inhabitants 4 . The OIC average was just 13.  Fig. 2 Moreover, over the past two decades the number of papers produced by 24 OIC nations has remained flat or declined, albeit with some striking exceptions 4  (see chart, right). Turkey's publication rate per year has grown from around 500 in 1988 to more than 6,000 in 2003. The other rising star is Iran, which from a low base of less than 100 articles per year a decade ago now produces nearly 2,000. Both countries have eclipsed Egypt, previously the most prolific of all OIC states in scientific publishing, which grew only slowly between 1988 and 2003. \n               Scientific divide \n             The articles published by OIC countries show striking disciplinary and geographical differences 3 . Health and social sciences get little attention except in south Asia and African countries. Chemistry and physics papers make up greater shares of total output in central Asian countries, life sciences in North Africa, Indonesia and Malaysia, as well as Saudi Arabia, and engineering predominates in most Middle Eastern and north African countries. The OIC countries produce so few patents that they are invisible on a bar chart of comparison with other countries 3 . This lack of technological competitiveness translates into low rankings in terms of high-tech exports as a percentage of total exports \u2014 with one exception, Malaysia, which ranks fifth worldwide with 58% high-tech exports, alongside Singapore and the Philippines. Otherwise, Indonesia (14%) and Morocco (11%) are the only OIC countries with high-tech exports greater than 10%. Given such diversity, it is hard to identify trends across the Islamic world as a whole. But there are signs of hope when looking at the OIC's top and bottom performers. Turkey, for example, is not rich in oil, but is the most scientifically successful of the Muslim states. Turkish intellectuals attribute this to the 1923 revolution, which led to the creation of a constitutionally secular state. For decades, Turkey has aimed for membership of the European Union (EU); formal negotiations began last year. Negotiations require even closer structural alignment with EU member states, and since 2003 science funding has more than trebled. Modern Turkey grew from the ruins of the Ottoman empire. Mustafa Kemal Ataturk, Turkey's founder, was keen for his country to catch up with the West. Universities switched from using Arabic to the Roman alphabet, ensuring easier access to Western writings, and Ataturk initiated a nationwide campaign to raise literacy rates. \n               Balancing act \n             The Kemalist legacy has fostered Western-style scientific organizations and policies. But the insistence on removing religion from public life introduced restrictions of its own. For example, women are banned from wearing headscarves in government-funded universities \u2014 something that may have to change if Turkey joins the EU. Yet Turkey has respectable participation by women in academic life \u2014 higher than in some EU countries. As for the bottom performers, sub-Saharan Africa accounts for 21 of the 57 OIC countries and, with the exception of Cameroon and Gabon, are among the poorest. The ongoing economic recovery of sub-Saharan Africa is \u201cone of the most remarkable stories of the past five years\u201d, according to the 2006 World Bank Development report 3 . The latest available data, for 2004, show five years of growth, after two decades of decline. Most was down to oil, but agriculture was also important. Among the OIC countries, Chad's economy grew more than 10% per year and Nigeria's 6%. But overall economic levels remain poor, with growth rates usually far below the 7% minimum considered necessary to begin meeting the Millennium Development Goals. Several countries, including C\u00f4te d'Ivoire and Gabon, still show negative growth. Not surprisingly, science is correspondingly weak, and although hard data are few, Islamic countries in sub-Saharan Africa have about 20 researchers and engineers per million people, compared with about 250 in Latin America. One potential bright spot is Nigeria, which announced in July that it is considering ploughing its oil revenues into a US$5-billion endowment fund for science and technology, which would make it Africa's biggest science spender by far 5 . If others follow where Turkey and Nigeria lead, these science indicators might look very different in ten years' time. As a first step, more OIC governments need to gather data on science, producing figures that are sufficiently reliable to appear in international statistics databases. See Editorial,  \n                     page 1 \n                   ; Commentaries,  \n                     pages 33 \n                   ,  \n                     35 \n                    . \n                     Science in the Arab world \n                   \n                     Arab state pours oil profits into science \n                   \n                     Time for 'enlightened moderation' \n                   \n                     The scientific impact of nations \n                   \n                     Academies wrestle with issue of Islam's flagging science base \n                   \n                     Arab science: Blooms in the desert \n                   \n                     Organization of the Islamic Conference (OIC) \n                   \n                     World Development Indicators 2006 \n                   Reprints and Permissions"},
{"file_id": "444020a", "url": "https://www.nature.com/articles/444020a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "The 57 countries in the Organization of the Islamic Conference are home to 1.3 billion people. The attendant diversity in culture, geography, economics and politics can be seen in these snapshots of five different approaches to science. The full  Islam and Science special  is available from news@nature.com.  The decline of scientific knowledge in Persia, now Iran, began in the fifteenth century, as the entire Islamic world lost touch with its intellectual roots. In the 1970s, the last Shah dynasty attempted to reverse the trend by building new universities and sending students abroad to do PhDs, after which many returned to teach and research in Iran. The number of scientific publications crept up from 125 in 1970 to nearly 400 in 1979, the year of the Islamic revolution. The revolution halted this modest progress. Many of the scientific \u00e9lite fled the country as the new regime closed universities and turned against 'Western science'. The Iran\u2013Iraq war in the 1980s further drained resources, leaving little money for non-military education and research. With peace, more public money became available and Islamic fervour moderated to accept the intrinsic value of science. Universities expanded again, and were allowed to award PhDs. The publication rate also rose, more sharply than in the 1970s: by 2003 the annual output of papers reached close to 2,000. With the election last year of President Mahmoud Ahmadinejad, a hardline Islamist, every aspect of Iranian life \u2014 from its supreme leader, to the judiciary, and parliament \u2014 is now in religiously conservative hands. Iran's reformers have gone underground. Many areas of science continue to be well funded, including stem-cell research. Iran was the first Middle East country to develop a human embryonic stem-cell line, using spare embryos from  in vitro  fertilization. And despite increasing restrictions on free speech, higher education continues to expand. Whether the new university presidents \u2014 all appointed by Ahmadinejad \u2014 will yield to conservative demands for greater controls at universities remains to be seen.  Alison Abbott Asked the greatest achievement of his institute in the past five years, Sangkot Marzuki, director of the Eijkman Institute for Molecular Biology in Jakarta, says: \u201cStill being here.\u201d The 1990s were a boom time for Indonesian science. In 1993, the Eijkman Institute was resurrected after political, social and economic unrest closed its doors in 1965. But the Asian financial crisis of 1997 threatened to reverse the gains. In 1998, following two years of social unrest and demonstrations, Indonesia's autocratic leader, General Suharto, stepped down, ushering in free elections and reform. Since 2004, the president has been freely elected. Religion does not seem to have the same link with power as in other predominantly Muslim countries. Nearly 90% of Indonesians are Muslims, but neither of the main political parties is strongly linked with Islam. Still, opposition by many smaller Islamic parties was crucial in quashing the bid by a woman, Megawati Sukarnoputri, to retain her position as president in 2004. Religious friction has hurt science by threatening international collaborations. Marzuki says that before 1998 his institute had six Australian scientists on three-year grants from the Australian government. \u201cWith the travel restrictions frequently issued, especially after the Bali bombing, no collaboration has been possible,\u201d he says. Islam itself is very flexible in relation to science, says Marzuki: \u201cOur institute performs prenatal diagnosis against common genetic diseases, in particular thalassaemia. The government is against reproductive cloning but supportive of therapeutic cloning, a position adopted on the recommendation of the Indonesian Academy of Sciences.\u201d With stability, Indonesia could be a fertile ground for science. The government, when cash flow allows, supports research. But no one welcomes the misfortunes that have forced research in some fields \u2014 earthquakes, tsunami monitoring, avian influenza \u2014 on Indonesian scientists.  David Cyranoski Malaysia has one of the most technologically advanced societies in the world, thanks in part to its consumer-electronics industry. It outshines the rest of the Muslim world in high-tech exports, but is not resting on its laurels. The government, led since the Malaysians received their independence in 1957 by the United Malays National Organisation, is keen to build on its successes in making semiconductor components and to develop new technologies. But government policies have driven away some of the country's best talent and kept Malaysia isolated from international science. Too often, pleasing the Islamic Malays, who make up the majority of the population, takes precedence over rewarding scientific merit. Historic tensions between the Malays and the Chinese minority, who have long held the economic reins, explain some of these policies. In 1969, riots followed general elections in which Chinese parties made gains. The government has faced a difficult balancing act since then. The most controversial policy is a university quota system that favours ethnic Malays (some 60% of the population), over Chinese (25%) and Indian (7\u201310%). Minority students, many of whom have top grades, struggle to get into the nation's best universities, and often end up going to the United States, Singapore or Australia. Such policies also inhibit interactions with the international community. And attempts to reverse a mainly Chinese brain drain have failed. The government has long invested in large projects intended to benefit high-tech industry, but with little success. And ongoing privatization of government operations in various sectors, including roads, energy and technology, is slanted to help the Malays. Such 'Malay first' policies will fail to attract the best overseas talent and continue to leave Malaysia isolated.  David Cyranoski Pakistan's history of perpetual military coups is a disaster for democracy. But the men in tanks have been more generous than civilians when it comes to funding science and technology. Pakistan's national biotechnology institute, for example, owes its existence to former President General Zia ul-Haq (military coup, 1977). In contrast, a decade of civilian rule after the general's death saw research funding drop to critically low levels \u2014 former Prime Minister Benazir Bhutto did not even appoint a science minister. Credit for current science and technology funding \u2014 the highest it has ever been \u2014 goes to President General Pervez Musharraf (military coup, 1999). Musharraf has handed the task of reorganizing research and higher education to Atta-ur-Rahman, a chemistry professor at the University of Karachi. Rahman's many reforms include increasing the number of universities and sending more students abroad to train. Schemes to attract foreign faculty members to work in Pakistan's universities, and performance-related pay for the country's own academics, have been more controversial. Critics of the reforms include Pervez Hoodbhoy, physics professor at Quaid-i-Azam University in Islamabad. He says that formerly cash-starved ministries lack the management capacity to spend their windfall wisely, and that the expanding university numbers have not been matched by a commitment to quality. Pakistani researchers, meanwhile, are unhappy that foreign faculty members get higher salaries for the same work. Rahman says he understands the criticisms. But he is a man in a hurry. General Zia's life ended in a plane crash in 1988. There have already been two attempts on Musharraf's life. Rahman knows the boom will end when the general leaves office.  Ehsan Masood A year after the birth of independent Saudi Arabia in 1932, the kingdom's rulers faced a problem with the public acceptance of technology. The dilemma was caused when US oil companies prospecting in the region requested permission to take aerial images of the desert. Newly crowned King Ibn Saud, founder of the present Saudi dynasty, had two concerns. First, that indigenous tribes might shoot at something they considered to be extraterrestrial. Second, if religious authorities believed that the onboard cameras could glimpse the face of God, it would invite divine wrath. This story, told in  Oil, God, and Gold  by Anthony Cave Brown, helps to explain why the world's largest oil producer still remains one of the world's lowest producers of scientific knowledge. It took 20 years for a ministry of education to be created. The country's science ministry did not emerge until 1977. Even today, Saudi Arabia spends just 0.25% of its gross domestic product on science and technology. Despite this, there are some bright spots. The number of specialist science and engineering colleges has doubled to 64 in the past decade, and the number of students enrolled in related degrees has also doubled to 76,000. Moreover, Saudi society has progressed considerably in its acceptance of new technology. According to Saleh Al Athel, president of the science ministry, genetic modification in agriculture is permitted within conventional biosafety limits. But pre-implantation genetic diagnosis is forbidden as it is seen as interfering with divine will.  Ehsan Masood \n                     Seismology: Shaking the foundations \n                   \n                     Malaysian biotechnology: The valley of ghosts \n                   \n                     Iran's long march \n                   \n                     Iranian neuroscience: The brains trust of Tehran \n                   \n                     Women at work \n                   \n                     Good and bad in Pakistan \n                   \n                     Malaysia puts biovalley under wraps \n                   \n                     Arab science: Blooms in the desert \n                   \n                     Bird flu outbreaks in Indonesia going unstudied \n                   \n                     Organization of the Islamic Conference \n                   Reprints and Permissions"},
{"file_id": "444019a", "url": "https://www.nature.com/articles/444019a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Science in the Muslim world \n             The full  Islam and Science special  is available from news@nature.com.  The war in Iraq, the price of oil, the deadlock over Iran's nuclear ambitions, the terrorism of al-Qaeda and the tensions surrounding immigrant communities in Europe ensure that Islam is rarely far from the headlines. But you would have to be an avid student of Muslim affairs to come across any discussion of science and technology not linked to the development of nuclear weapons. In this week's issue,  Nature  offers an unprecedented look at the prospects for science and technology in the Muslim world (see  page 20 ). We have never before collected together such a range of voices and analysis in one issue. In ignoring Muslim science, the West follows the lead of the Muslim world itself. Low investment and a low profile combine to keep the scientific community small, marginalized and unproductive. This is not simply a matter of underdevelopment; the oil-rich Gulf states invest pitifully in R&D (see  page 28 ). In our Commentary section, on  pages 33  and  35 , Nader Fergany, the lead author of the Arab Human Development Reports, and Herwig Schopper, president of the council for the Middle East laboratory SESAME, offer their own critical analyses of what needs to change to allow science to take off in Muslim countries. The poor scientific track record of Islamic countries might suggest that there is something about Islam inherently inimical to research. Muslims bristle at this idea, pointing to the major achievements of Muslim scholars under the Islamic caliphate (see timeline,  page 23 ). But what of the present? Our News Feature on  page 22  looks at the attitudes to science in the various Islamist organizations growing in power in key states ranging from the Occupied Palestinian Territory to Malaysia. The secular regimes and one-party states that have ruled many Muslim countries are being replaced, or directly challenged, by voices calling for a more directly political Islam. The conditions in which knowledge flowered a millennium ago are hardly those that today's Islamists say they favour. Back then, support for scientific enquiry was matched by an openness to other cultures and sources of knowledge. But when Islamists come to power the picture is more nuanced than it may first appear. Restrictions on freedom of speech and a high level of investment in military technology are distressing to outsiders, but greater attention to higher education is a trend that could offer hope. Mostafa Moin, an Iranian reformer and scientist, lays out his hopes and fears for the future on  page 29 . Greater attention to the challenges of the present is sorely needed. Too few Muslim governments collect data on the status of science and innovation (as our analysis on  page 26  shows), and so the problems facing scientists are not even on their agenda. Muslim nations wanting to invest in science as a broad cultural activity need to extract the right lessons from their glorious past and their politically charged present. \n                     Islam and Science: The Islamic world \n                   \n                     Islam and Science: An Islamist revolution \n                   \n                     Islam and Science: The data gap \n                   \n                     Islam and Science: Oil rich, science poor \n                   \n                     Islam and Science: Q&A The reformer \n                   \n                     Islam and Science: Steps towards reform \n                   \n                     Islam and science: Where are the new patrons of science? \n                   Reprints and Permissions"},
{"file_id": "4441000a", "url": "https://www.nature.com/articles/4441000a", "year": 2006, "authors": [{"name": "Helen Pearson"}], "parsed_as_year": "2006_or_before", "body": "Ultraendurance racers torture their bodies and minds to achieve near-impossible physical feats. Is it an exceptional genetic make-up or the vestiges of human evolution? Helen Pearson reports. Trek 125 kilometres, and cycle 250 more. Kayak 131, rappel through canyons for another 97, and swim 13 in churning whitewater. Throw in some horseback riding and rock climbing; spread it all over six days in the blistering Utah heat; and never stop to sleep. That's the punishing formula for the annual Primal Quest adventure race, considered one of the most extreme tests of human endurance. Of the 90 four-person teams that competed this summer, only 28 finished the course. \u201cIt was more like adventure torture,\u201d says Michael Tobin of the winning team. Yet a growing number of athletes are opting to push the outer limits of their body's abilities. If a marathon or Ironman triathlon isn't enough, athletes can enter 'extreme ultraendurance' events, which spread the agony over several days (see  'The toughest races on Earth' ). And alongside the racers \u2014 figuratively and sometimes literally \u2014 are exercise physiologists. Researchers see these radical events as a testbed for ideas on how some people manage to perform physical feats at which others can only marvel. Some scientists argue that ultraendurance athletes have special genes or physiology that allow them to perform beyond ordinary limits. Others claim that these athletes are little different physically from fit people such as marathon runners \u2014 but have something unique about their brains that allows them to keep exercising when the body screams 'stop'. Either way, many find it hard to resist the opportunity to study human physiology as it is stretched towards breaking point. \u201cScientists are naturally drawn to the extreme,\u201d says Euan Ashley, a cardiologist at Stanford University in California who has studied the genetics of endurance racers. Yet much of the research is still in its infancy. Most of the focus in exercise physiology remains on prestigious professional and Olympic sports, not the protracted and gruelling competitions of amateurs. In addition, ultraendurance is difficult to study in the lab because few subjects are willing to run on a treadmill for 24 hours or more. But some key differences between ultraendurance athletes and other athletes are beginning to emerge. For instance, short races, such as 10-kilometre competitions, are usually won by athletes in their 20s. But ultraendurance races tend to be won by people in their early to mid-30s \u2014 because the body takes a longer lifetime of training to achieve the toughness of muscle and efficiency of metabolism that such a race demands, experts say. Ultraendurance winners also are good at performing for sustained periods of time at 60\u201370% of their maximum output. The strongest conventional athletes, by contrast, are those who use oxygen most efficiently when working at close to 100% of their maximum. \n               Pushing the limits \n             Additional insights are starting to come from studies led by Mikael Mattson and Jonas Enqvist, researchers at the Karolinska Institute in Stockholm, Sweden. Their team had nine world-class racers run, cycle and kayak on lab machines nonstop for 24 hours while the researchers recorded various measures of heart function and metabolism. The group is still analysing the data, but one preliminary result suggests that ultraendurance exercise makes the energy-generating mitochondria in the athletes' muscles more efficient at using fat rather than glucose as fuel. Fat generates more energy than glucose per kilogram, but the body can't normally burn fat when exercising very hard. The bodies of endurance racers may have found a way to go faster using fat as fuel, saving stores of glucose for later on in the race. \u201cIt's one reason these athletes are physiologically different from other athletes,\u201d Mattson says. He calculates that the athletes burned around 20,000 kilocalories (84,000 kilojoules) during the day of exercise \u2014 an amount almost impossible to replenish by eating. What is not clear from Mattson's work is whether the ultraendurance athletes were born with special physiological abilities or whether they gained them through training. But genes probably play at least some part, according to studies of more than 400 participants in the South African Ironman competition. A team led by Malcolm Collins, a molecular biologist at the University of Cape Town, has examined the gene that makes angiotensin-converting enzyme (ACE), a peptide that helps narrow blood vessels. In a separate study, one form of this gene had already been found to be associated with endurance performance in mountaineers and army recruits 1 . More recently, Collins' group found that more than 77% of the fastest South African-born Ironman triathletes carried one or two copies of this endurance variant, compared with 67% of non-athletes 2 . The effect didn't hold for non-South African athletes, but it triggered Collins to start looking at other potential gene variants that could affect Ironman performance. In particular, he is now scrutinizing two genes that work along the same biochemical pathways as ACE. Variants of these genes affect the blood flow and metabolic efficiency of muscles: one makes a receptor for a peptide called bradykinin, and the other makes an enzyme called nitric oxide synthase 3. The researchers found that athletes with a particular combination of 'fitness' variants of these two genes finished the Ironman in 12 hours 30 minutes, on average, compared with 13 hours 4 minutes among those who carried the 'slow' variants 3 . Tim Noakes, a sports physiologist who works with Collins at the University of Cape Town, argues that the bodies of ultraendurance athletes are actually little different from those of other sports people. The brain, he says, is the oft-overlooked organ that sets ultraracers apart. These people \u201care mental freaks\u201d, he says, not physiological ones. Conventional thinking in exercise physiology holds that the muscles, lungs and cardiovascular system set the limits of performance. But the brain coordinates biochemical signals from these peripheral organs, such as those indicating fatigue and tissue damage, and will unconsciously stop us before we wreck our bodies. Perhaps, Noakes suggests, ultraendurance athletes have brains that allow them to override warning signals that would bring others to their knees. Noakes is now examining what conscious and unconscious information the brain uses to set the ideal exercise pace. In one recent study, he and his colleagues showed that athletes use oxygen more economically and feel less exertion if they know the length of time they will be running, compared with those deceived about the time but who run at exactly the same pace 4 . But even couch potatoes may have something of the endurance racer in them. Daniel Lieberman, a biological anthropologist at Harvard University in Cambridge, Massachusetts, argues that the human body is well-adapted to long-distance running, as an evolutionary hangover from our hunting and scavenging days 5 . Ultraendurance racers \u201care able to be freaks because evolution has enabled us\u201d, he says. A body capable of jogging tens of kilometres at a time helped our ancestors survive, he says. Fuelled by plentiful water, energy bars and yet more training, that body can complete the 90 or more kilometres of an ultramarathon. If this is true, then many of those who study ultraendurance racers have also embraced their evolutionary past. Paul Laursen, for instance, was kicked out of kinesiology studies at Simon Fraser University in British Columbia, Canada, after becoming so absorbed in triathlon competitions that he flunked his first year. Now he has a remarkably lean 10.6% body fat, a 48-beat resting heart rate and, in early December, swallowed a miniature thermometer and ran an Ironman in Western Australia as the subject of his own study into muscle damage. He ran a worse time than any of his previous Ironmans \u2014 but says the experiment was a success. \n                     Extreme sports push hearts \n                   \n                     Distance running 'shaped human evolution' \n                   \n                     A breed apart \n                   \n                     The medals and the damage done \n                   \n                     Special: What are the Limits? \n                   \n                     Primal Quest \n                   \n                     Race Across America \n                   \n                     Badwater Ultramarathon \n                   \n                     UK Sleepmonsters adventure racing \n                   Reprints and Permissions"},
{"file_id": "444029a", "url": "https://www.nature.com/articles/444029a", "year": 2006, "authors": [{"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "Mostafa Moin is a paediatrician and medical researcher who has served as Iran's minister for higher education and for science. He was a reformist candidate in Iran's presidential election last year, which was won by religious conservative Mahmoud Ahmadinejad. Declan Butler asks Moin about the prospects for science in Iran. The full  Islam and Science special  is available from news@nature.com.  \n               How do you see the interplay between science, religion and reform in Iran? \n             Iranian society has long been deeply religious, even before the Islamic era began in 651. But over the past 150 years, Iran has also pioneered struggles for freedom and opposition of dictatorship. In the past few decades, reformers and religious neo-intellectuals with a common attachment to the principles of a civil and democratic society have cultivated democratic structures in Iranian society. These will undoubtedly prepare the ground for greater scientific development and help a knowledge-based society to materialize. Islam itself is not anti-science, but I have always been concerned that superficial, narrow-minded and non-democratic interpretations of Islam \u2014 and the political behaviour of certain traditional administrators \u2014 risk having a negative impact on both scientific development and social reforms in Iran. \n               What is your assessment of Mahmoud Ahmadinejad's track record on science, academic freedom and social reforms? \n             The new government has overlooked the science and higher-education sectors in the 14 months it has been in power. It has also replaced almost all the university chancellors, and senior research and higher-education officials. When I was minister of science, research and technology, senior university officials were elected by the academic staff; the new government appoints them directly. There is no doubt that this political control has resulted in purging, restrictions and criticism of independent forces. Renowned academics have been forced into retirement, and repression of politically active students and student organizations is escalating. International scientific exchange has not been immune from these narrow-minded approaches: students are no longer sent abroad and sabbaticals are restricted. The government has cracked down on reformist newspapers, activists and political parties. The  Shargh Daily , for example, one of the highest-circulation reformist newspapers, was shut down in September. I've also heard that admission of female students to universities is being restricted; I hope my information is incorrect. \n               How optimistic are you that Iran can find a route to political reform? \n             In the short term, it can't be predicted whether things will get better or worse. But I'm optimistic for the medium term. There is a global movement towards greater civil rights, and an expansion of democracy. It is this, and the growing awareness of the Iranian people of these issues, the vigilance of its youth and women, that will shape Iran's future. \n               What are the biggest obstacles to improving Iran's international isolation? \n             International scientific cooperation with Iranian universities and scientists has greatly increased in the past few years. But it's not surprising that the nuclear crisis and the unscientific and unsustainable policies of the new government may have overshadowed this process. Current foreign policy is based on confrontation, and is shifting away from former president Mohammad Khatami's 'dialogue of civilizations'. This is the main obstacle at present. \n               What should colleagues elsewhere think about Iran's nuclear programme? \n             I am sure that my learned academic colleagues abroad would not accept discrimination against Iran's legitimate right to the peaceful use of nuclear energy within the framework of global regulations. The problem is the accusations by the ruling neo-conservative radicals in the United States over programmes of weapons of mass destruction in Iran, while proposing double standards within the Middle East and the rest of the world. The same accusations have been made before, but experts, scholars and international institutions proved them unfounded. If the United States were to implement unilateralist, violent and non\u2013negotiable policies against Iran, this would be a great catastrophe, increasing regional and global crises, expanding terrorism and consolidating dictatorships. \n               What were the major defining events in Iranian science over the past two decades? \n             The expansion of higher education in the country, with university students increasing from 400 to more than 3,000 students per 100,000 people, between 1979 and 2000. The student\u2013lecturer ratio has also improved from about 36:1 in 1989 to 18:1 in 2006. I regret that the structural reform in higher education in Iran has remained unfinished, and that the autonomy of universities and academic freedom were not institutionalized. \n               What are your own plans for the future? \n             In the past two to three years I have founded two non-governmental organizations, the Association for Scientific Development of Iran and the Iranian Association for Ethics in Science and Technology. As president of the Immunology, Asthma and Allergy Institute at Tehran University of Medical Sciences, I'm active in teaching and research. I have respected the commitment I made to the Iranian people during last year's election, with the creation of the Democracy and Human Rights Front. I want more than ever to strengthen civil and scientific institutions and structures, particularly for the young. \n                     French confusion leaves Iranians locked out of meeting \n                   \n                     Revival in Iran \n                   \n                     Science in culture: The zenith of Islamic science \n                   \n                     Iran's long march \n                   \n                     Iranian neuroscience: The brains trust of Tehran \n                   \n                     Iranian physicist locked out of laboratory by energy department \n                   \n                     Immunology, Asthma & Allergy Research Institute, Tehran, Iran \n                   \n                     Wikipedia: Science in Iran \n                   \n                     Ministry of Science, Research & Technology (in Persian) \n                   \n                     Comstech Iran profile \n                   \n                     Atomic Energy Organization of Iran \n                   \n                     Nature podcast; Interview with Iranian cosmologist, and former deputy science minister, Reza Mansouri \n                   Reprints and Permissions"},
{"file_id": "444138a", "url": "https://www.nature.com/articles/444138a", "year": 2006, "authors": [{"name": "Jenny Hogan"}], "parsed_as_year": "2006_or_before", "body": "Most astronomers head for remote mountain-tops or deserts to study the cosmos. Jenny Hogan meets a confident team set up on a patch of farmland in a crowded corner of mainland Europe. In the lobby of Astron, the Netherlands Foundation for Research in Astronomy, a clear plastic column almost full of dirt commemorates an unusual aspect of the institution's struggle to study the cosmos. Over the past three years, the institute's astronomers have been negotiating with 42 Dutch farmers to buy up the 400 hectares of land they need for the first stage of a remarkable new radio telescope \u2014 the low frequency array, or LOFAR, a \u20ac148-million (US$188-million) attempt to open up a new part of the spectrum to astronomical inspection. Every time a stretch of land was acquired, a bit of soil was added to the column in the lobby, making a layer cake of peaty soil and sand. Out in the fields, the business end of LOFAR consists of sites dotted with low-slung, leggy wire antennas liberally spattered with the droppings of the birds that perch on them and a few grey metal Portaloo-like cabins. But the fields aren't the right place to look for LOFAR's grandeur: the telescope's impressive heart is being built in cyberspace. It is there that the faint noise of the Universe may eventually be separated from the background roar of Holland's air-waves and the Milky Way. 'Software telescopes', which couple a vast capacity for absorbing information with a matched capacity for analysing and reassembling it, are widely seen as the future of radio astronomy. Such technologies will be crucial to the Square Kilometre Array (SKA) with which the world's radio astronomers hope to revolutionize their field by the 2010s. At LOFAR, though, the technology faces an extra problem. To secure funding, the array's core had to be built in the Netherlands. Nowhere on Earth is completely free of radio signals, but the Netherlands, the most densely populated country in one of the most built-up regions of the world, is quite remarkably noisy. To tune in to the early history of the Universe, LOFAR must studiously ignore TV shows, FM radio stations, air traffic control and local taxi drivers, not to mention purely accidental emissions. Heino Falcke, LOFAR's international project scientist, gestures to a tractor trundling past one of what will be 77 fields of antennas. \u201cWe would probably measure that as a series of sparks going past our telescope.\u201d LOFAR's rivals, cloistered in a remote part of China and the Australian desert, enjoy far quieter environments. Whether LOFAR's software cleverness and number-crunching prowess will allow it to overcome the disadvantage remains to be seen. Get close enough to the 'Portaloos' in the field and you can hear them hum. Unlock their doors and you find tangles of cable and racks of electronics and blinking lights. This is where the wire roots of the antennas end up, and where the software telescope begins to take shape. Although the antennas may look simple, they can produce data at an alarming rate. One antenna site, consisting of 96 spindly antennas collecting low-frequency waves and 1,536 squat structures detecting higher frequencies, will produce about 500 gigabits of data every second \u2014 far more than even the most sophisticated optical telescopes. \n               Cyber optics \n             The humming Portaloos are there to make sure that most of the data are left in the field, reducing a flood to a mere torrent. But that torrent still outdoes the data rate for the detectors on the LHC, the vast particle collider currently being completed at the European Centre for Particle Physics (CERN) near Geneva \u2014 instruments whose vast data flows have led to a world-wide effort in GRID computing. The torrent from the fields, channelled through optical fibres, ends up in a windowless strip-lit room at the University of Groningen. Here, an IBM Blue Gene/L supercomputer called Stella compares the data from the 50,000 channels measured at each station with the equivalent data from all of the other 76. At Astron, they think that this 34-trillion-operations-a-second wonder could restore them to the glories of the 1950s, when the 25-metre Dwingeloo dish, now rusting away behind their institute, was briefly the largest steerable radio telescope in the world. In the 1990s, the institute decided to follow a path that concentrated less on dishes and more on digital. \u201cWe thought that Moore's law would be a good thing to hook onto,\u201d recalls Harvey Butcher, Astron's director since 1991. To maximize the effort that could be put into data processing meant that the cost of picking up the signals in the first place had to be minimized, leading to the use of cheap dipole antennas. LOFAR's dipoles pick up long-wavelength signals at frequencies from 30 to 240 MHz. This long-wavelength radio is a little-explored part of the spectrum, not only because it is widely used on Earth, but also because it is affected by the ionosphere \u2014 another source of noise that has to be removed with the help of computers. What's more, the long-wavelength sky is filled with the synchrotron emission given off by electrons being whipped around by magnetic fields in the Galaxy, the glare from which masks intriguing cosmological signals some 10,000 times as faint. In 2001, the Astron team calculated the computing requirements for a long-wavelength system big enough to find those faint signals. Then they had a look at the Top 500 list that ranks the world's quickest supercomputers. It looked as though, given the way that performance was improving over time, the power they needed might be purchasable in the near future. And so it was. The Blue Gene/L in Groningen is three times as powerful as the most powerful machine in the world was back then \u2014 and doesn't quite make it into the Top 500's top ten today. Perhaps the most interesting of the feeble signals that the computer will be sifting out are those emitted in the dark ages of the first few hundred million years after the Big Bang, a chapter in the Universe's history that remains obstinately closed. The dark ages' darkness stems from the fact that the glowing plasma of the Big Bang, details of which are still visible in the cosmic microwave background, had cooled down into neutral atomic hydrogen. It was only after the stars and galaxies had come to life that their light burnt off this hydrogen mist, reionizing the gas into a transparent plasma. Huge sums of money are being spent on various telescopes that promise to peer back at this epoch of reionization. The $650-million Atacama Large Millimetre Array, under construction in the Chilean Andes, aims to detect the infrared glow of hot dust around the very first galaxies, radiation that will have been stretched, or 'redshifted', by the expansion of the Universe to millimetre wavelengths. NASA's $5-billion James Webb Space Telescope, scheduled for launch in 2013, will look for the ultraviolet light of these first galaxies, now redshifted into the infrared wavelengths that the telescope is optimized for. \n               Shedding light \n             LOFAR's approach, shared by the Mileura Widefield Array (MWA), planned for the outback of western Australia, and the 21CMA installation in China, is to look at the neutral hydrogen itself. Neutral hydrogen emits radiation with a wavelength of 21 cm \u2014 hence the name of the Chinese array. That 21-cm radiation from the epoch of reionization is redshifted down into the metre-long wavelengths the new telescopes can detect. What the arrays will actually be looking for are the holes in the 21-cm radiation caused as the reionization centred on stars and galaxies took hold, creating a Swiss-cheese effect. In principle, different stages in the growth of these holes could be teased out by finding the 21-cm hydrogen line at different redshifts \u2014 the longer the wavelength, the older the radiation. By moving up and down the frequency dial, it would be possible to watch these bubbles expanding and merging as the Universe opened itself up to inspection. Simulations of the early Universe produce beautiful pictures of this process. But images from this first generation of telescopes will be, at best, very fuzzy. The head of LOFAR's reionization project, Ger de Bruyn, demonstrates what might be seen by bringing up the theorist's beautiful simulations on his laptop and then removing his glasses, scooting his chair to the far side of his office, and squinting: \u201cThis is more likely.\u201d The first detection of reionization is more likely to be statistical, giving a measurement of how the brightness of the glow varied from one time to another, than a resolvable picture of specific bubbles. And even a statistical measurement is going to be tricky. Xiang-Ping Wu, chief scientist for 21CMA, says that it could take five years to be confident of seeing a reionization signal. With 21CMA already taking data, construction on LOFAR started this summer and the first antennas of Australia's MWA expected to be installed next year, it could, says Butcher, \u201cbe a really interesting race\u201d. The Chinese array, built for just US$3 million, is the smallest and cheapest. It is located in the west of the country, in a remote valley in Xin Jiang province. When the project was approved in August 2004, \u201cwe pitched a tent in the Ulastai valley and started the construction on the same day\u201d, says Wu. The completed array consists of 10,287 log-periodic dipoles \u2014 television antennas, more or less \u2014 arranged along two perpendicular arms roughly four and six kilometres long. Unlike LOFAR and the WMA, which have other scientific fish to fry, 21CMA is essentially interested only in the reionization signal. Its field of view is restricted to a patch of sky around the North Celestial Pole and its resolution rather coarse. Although it has been collecting data for three months, no money has yet been allocated to continue operations. But Wu isn't worried. The Chinese Academy of Sciences and National Astronomical Observatories of China, which funded the array's construction, are likely to continue to support the project, he says, and concentrating on a small patch speeds up data acquisition. \n               Universal challenge \n             Close behind is LOFAR, which will use two types of antenna, one gathering low-frequency radiation, between 30 and 80 MHz, the other higher frequencies, between 110 and 240 MHz. Because of its geographical situation, LOFAR has a whole suite of systems to deal with radio interference. It will use software filters to flag and discard signals that are too strong to be coming from space. Some noise will be fleeting, and so average out over a long observing time, or be at a single frequency that can be filtered out. This will leave gaps in the data, but the team estimates that they can use about 90% of the spectrum at any one time. The exception is the commercial FM band, which LOFAR will simply discard. This is why there's a gap between the ranges of the two antennas. And that could pose a problem for the study of reionization. Current models predict reionization would have been getting into its stride by the time the Universe was 500 million years old. If they're correct, \u201cthe frequency will be right in the middle of the LOFAR band. Then LOFAR can really have a go at it,\u201d says de Bruyn. But if galaxies formed a bit more quickly than predicted, the hydrogen would have burnt up a few million years earlier. In that case, its redshifted signal will lie in the impenetrable realm of Exxact FM and Radio Veronica. Three years ago this blank spot, and other concerns about the effects of noise, led to a split in the international collaboration of astronomers planning the telescope. The US and Australian members left after the Astron team and their Dutch collaborators committed themselves to a site in the Netherlands. Some of the Americans opted for a Long Wavelength Array in New Mexico, which is not pursuing the 21-cm signal from reionization, the others for the MWA in Australia. \u201cWe didn't agree with the decision to build in northern Europe. We didn't think it was wise,\u201d says Jacqueline Hewitt, director of the Kavli Center for Astrophysics and Space Research at Massachusetts Institute of Technology in Cambridge, now part of the collaboration working on the MWA in Australia. \u201cI don't personally think you can do the epoch of reionization measurement there \u2014 I might be wrong, but I wasn't prepared to spend the next ten years of my life trying.\u201d The site chosen for the MWA \u2014 a 12-hour truck drive from Perth, the nearest big city \u2014 is beset by very little interference. But the Dutch part of the collaboration was tied to building their telescope in the Netherlands by a generous dollop of funding. LOFAR was granted \u20ac52 million in 2003 from the national Fund for Economic Structure, which reinvests money made from sales of the Netherlands' natural gas. This money has to be used to strengthen the national economy, and so couldn't be spent on building a telescope in another country. The Dutch contingent wasn't going to walk away from this kind of cash (see  'Making it pay' ). \n               Space race \n             The MWA sees itself more as an experiment \u2014 and as a forerunner to the SKA \u2014 than as a dedicated piece of infrastructure for many users, which is what LOFAR aspires to be. It has fewer antennas, of only one type, all in one place, and a much smaller project team relying on postdocs and graduate students for a lot of labour. And there are far fewer farmers to negotiate with. This makes it much cheaper than LOFAR, with an expected initial cost of just US$10 million \u2014 although Rachel Webster, a member of at the MWA team at the University of Melbourne, says she expects the team will actually spend much more. The initial money is being provided by the US National Science Foundation, the Australian Research Council and the government of Western Australia, among others, but the project is still at the prototype stage. The first 32 of its 500 antenna elements are due to be installed in the outback in 2007. Hewitt notes that, regardless of who sees the reionization signal first, it will be good for the measurement to be repeated. \u201cWe're going to need to compare our results. If one group sees it, I don't think people will believe it until another group has too.\u201d Butcher, while regretting the break-up of the project team, is matter-of-fact about its consequences. \u201cOn the one hand, there are some bits of the spectrum that we can't use. On the other hand, we have a telescope and they don't.\u201d LOFAR and the MWA will use their all-sky views to study more than just the epoch of reionization. \u201cThe epoch of reionization is the lighthouse. It is the project that most people are interested in,\u201d says Falcke. But the fact that they will be looking through a new window of the spectrum offers a prospect that he sees as just as tantalizing. \u201cThere's at least a chance that there is something out there that we haven't found yet.\u201d Estimates based on the number of objects detected in surveys at higher frequencies suggest that LOFAR's surveys will turn up some 100 million radio-emitting objects. The view at the lowest frequencies should include bright radio galaxies more distant than any seen before. The data stored in LOFAR's buffers will let it zoom in on transient flashes in the radio sky, which could be the radio counterparts of poorly understood \u03b3-ray bursts, or even bursts of radio emission from extrasolar planets, like those that come from Jupiter. Searches for transient events will 'piggyback' on all the other projects, commandeering the telescope if something particularly spectacular shows up. Long wavelengths also have implications for the search for extraterrestrial intelligence \u2014 a topic that stirred up significant interest among the farmers selling Astron their land. Working at wavelengths where the earthly background noise makes things difficult becomes a plus when looking for radiation leaking out from civilizations around other stars \u2014 at least it does if they have FM radio stations and the like 1 . \n               Network building \n             Closer to home, flashes of lightning and radio showers from cosmic rays will also be picked up by the array. Falcke's pet project looks at both of these, testing theoretical suggestions that cosmic rays might sometimes trigger lightning. A prototype LOFAR station in Germany, called LOPES, has already managed to image cosmic-ray showers with a time resolution of 30 nanoseconds 2 . The sharp resolution needed to study individual sources such as galaxies and even planets makes the LOFAR team keen to spread their antennas beyond Holland's borders; whereas the core set of elements doing the reionization studies will be clumped near the Dutch town of Exloo (see map), other antenna elements could be hundreds of kilometres away. Universities in Germany have expressed interest in at least six stations, and a UK group is interested in three. There are also discussions going on with astronomers in Sweden, France and Italy. Falcke hopes that, as the project gets under way, a me-too effect will lead to interest snowballing and the array spreading further, expanding from the initial 77 stations to more than 100. Long baselines, though, exacerabate a problem of calibration. Incoming radio waves are distorted on their way through Earth's ionosphere. LOFAR will have to correct the signal from every station separately, using the results from a model of the ionosphere running on a Linux cluster. This model will be updated every ten seconds by comparing the apparent positions of 200 sources on the sky with their known positions, with the results being fed into the Blue Gene/L. There are now so many cables plugging into the computer's vitals that the doors of its case barely close. Nor do the challenges end with the inputs. While the various filters and calculations compress the immense amount of data the system gathers, the Blue Gene/L will still be outputting correlated data at an astonishing 39 gigabits per second. That can't be stored for long in its unprocessed form, and the team plans to keep the data for only a week or so. After that, the data will only be available in the form of star maps from which quite a lot will have been lost. Astronomers who want to preserve the primary correlated data will have to take them away while they are still fresh, either through high-capacity fibre or saved to high-density storage media that can be shipped. And LOFAR is relatively simple compared to the demands of the SKA, with its square kilometre of collecting area and far greater frequency range. When Kjeld van der Schaaf, who is overseeing development of LOFAR's computer systems, makes presentations on the SKA, he includes a slide in which the trends of the Top 500 computer list are extrapolated through to 2015. The SKA's correlating system, which needs to be 3,000 times faster than LOFAR's Blue Gene/L, sits at the top of this ranking. While the participants in the SKA wait for computing power to catch up with them, design their technology, plead for their funding and choose their site (it will be a radio-quiet spot in South Africa or Australia), LOFAR will forge ahead with its ground-breaking work. And although winning the race to detect the 21-cm signal from reionization is not the project's be-all and end-all, it does have a certain sentimental significance. The very existence of the 21-cm hydrogen line was first predicted by a Dutch astronomer in 1944, but US astronomers beat his compatriots in the race to find it. This time round, they want to get there first. \n                     Giant telescope offered choice of homes \n                   \n                     US astronomy: Is the next big thing too big? \n                   \n                     Radio astronomy: High and dry \n                   \n                     Insight: Early Universe \n                   \n                     In focus: Future computing \n                   \n                     The Netherlands' LOFAR \n                   \n                     Australia's Mileura Widefield Array \n                   \n                     China's 21CMA (formerly known as PAST) \n                   \n                     Wikipedia on the 21cm hydrogen line \n                   Reprints and Permissions"},
{"file_id": "444028a", "url": "https://www.nature.com/articles/444028a", "year": 2006, "authors": [{"name": "Jim Giles"}], "parsed_as_year": "2006_or_before", "body": "The wealthy Arab states offer scant support for science and technology. Jim Giles finds out whether this indifference to research is likely to change. The full  Islam and Science special  is available from news@nature.com.  When  Nature  surveyed the prospects for science in the Arab world in 2002, our reporter picked out three subjects in which the region excelled 1 . One was, and still is, important: desalination technologies to combat water shortages. But the other two highlight the region's threadbare research record. Camel reproduction and falconry research might excite Arab sports enthusiasts, but they are unlikely to set the scientific world on fire. The monarchies of the Gulf are the richest of all Muslim nations, but little of that wealth is spent on research. Saudi Arabia, Qatar and Kuwait spend about 0.2% of their gross domestic product (GDP) on science \u2014 less than one-tenth of the developed-country average of 2.3% and about a third of that spent by less wealthy Iran. The oil monarchs have the financial clout to launch major research efforts, but have yet to do so. \u201cThe very rich countries are less concerned because they are sitting pretty on oil reserves,\u201d says Nader Fergany, director of the Almishkat Centre for Research in Cairo, an independent social-sciences research organization. \u201cThe nature of wealth from natural resources is that it does not require a great level of ingenuity.\u201d Fergany notes that even in directly relevant science such as petroleum technology, most innovation happens outside the Gulf. \n               Oil futures \n             But some Gulf leaders do see investment in science and technology as a way of creating an economic future when their oil reserves dry up. Among scientists trying to invigorate science in the Gulf, there is a sense that change is possible. \u201cWe are now at an inflexion point,\u201d says Samir Hamrouni, director of research and development at the Arab Science and Technology Foundation in Sarjah in the United Arab Emirates. \u201cScience is being seen as an alternative to natural resources.\u201d The origins of the current underspend are easy to see. The European colonial powers that ruled much of the Gulf until the middle of the twentieth century invested almost nothing in indigenous higher education or research. Oil revenues transformed the region, but the money kept flowing without the need for major investment in education and science. The latest statistics collected by COMSTECH, the science and technology committee of the Organization of the Islamic Conference, show little change 2 . The annual output of scientific papers from Saudi Arabia, which generates almost as many papers as the other monarchies combined, was static between 2000 and 2005. Even in desalination technology, investment has been limited. The Middle East Desalination Research Center in Muscat, Oman, set up in 1996 to encourage research cooperation in the region, is currently limping along with a budget of just US$2 million a year. The next five years might see more change. In Qatar, the country's head of state, Emir Hamad bin Khalifa Al-Thani, has created an endowment that generates millions of dollars in research funding every year. He has also imported Western science policies, such as competitive grant systems based on external peer review, and is forming partnerships with universities in the United States and Europe 3 . Environmental science, computing and biomedicine are priorities. If Qatar's new research centres attract scientists and students, they might prompt its neighbours into action. \u201cOnce it develops, other countries will start to think about it,\u201d predicts Mohamed Hassan, executive director of the Academy of Sciences for the Developing World (TWAS), based in Trieste, Italy. Of those neighbours, Saudi Arabia is making a slow start, having approved a new national science and technology development plan in 2002. Its priorities are defence, and oil and gas technology, but there is also a commitment to devote 1.6% of the nation's GDP to R&D by 2020. Both Saudi Arabia and Kuwait are each investing around $2 billion in higher-education institutes that include research centres. Such initiatives generate excitement \u2014 and some scepticism. Fergany questions whether the oil monarchies are willing to make the economic and structural changes needed to translate research into innovation. It is also unclear whether the oil-state rulers want to foster the atmosphere of critical enquiry that science needs. Only in the long run, say advocates of reform, will it become clear whether the current commitment is genuine. \u201cIs it just for the moment, or is it really important?\u201d asks Hamrouni. \u201cIt depends on our politicians.\u201d Commentaries  \n                     pages 33 \n                    and  \n                     35 \n                   . \n                     Arab state pours oil profits into science \n                   \n                     Arab science: Blooms in the desert \n                   \n                     Science in the Arab world \n                   \n                     Helping hands for Arab science \n                   \n                     New biotech oasis? \n                   \n                     Egypt boosts its academic entrepreneurs \n                   \n                     ASTF \n                   \n                     Qatar foundation \n                   \n                     Comstech \n                   \n                     Australian whitefly/soybean report \n                   Reprints and Permissions"},
{"file_id": "444259a", "url": "https://www.nature.com/articles/444259a", "year": 2006, "authors": [{"name": "Helen Pearson"}], "parsed_as_year": "2006_or_before", "body": "There's more than one way to read a stretch of DNA, finds Helen Pearson \u2014 and we need to understand them all. \u201cItwasknownthattheywerealittleacquaintedbutnotasyllableofreal\u00a0informationcouldemmaprocureastowhathetrulywas...\u201d Reduce it to just a sequence of letters, and even a delicate phrase from Jane Austen's  Emma  becomes virtually impenetrable gobbledygook. So it was something of a triumph for Simon Shepherd when, in 2001, an algorithm he had written reconstructed all of  Emma , word for separated word, from just such an uninterrupted string, despite being unacquainted with English vocabulary or syntax. The software worked out which groupings of letters were most likely to appear together, and thus have distinct meanings. Shepherd, a researcher at the University of Bradford, UK, picked up much of his expertise during ten years cracking Russian codes in British Naval Intelligence. But he was not really interested in  Emma  \u2014 that was just a demonstration. His real goal was the far longer sequences of As, Gs, Cs and Ts that make up the world's genomes. Within those strings there is information that no one knows how to extract \u2014 codes that regulate, control or describe all sorts of cellular processes. And if the information is there, Shepherd thinks that number crunching should be able to pry it loose. \u201cWe are treating DNA as we used to treat problems in intelligence,\u201d he says. \u201cWe want to break the code at the most fundamental level.\u201d That DNA contained at least one code was realized as soon as the molecule's structure was discovered. That code, cracked in the 1950s and 1960s, parses passages of DNA into three-letter combinations that correspond to particular amino acids. This is a code in the strictest sense; input determines output. But researchers now know that there are numerous other layers of biological information in DNA, interspersed between, or superimposed on, the passages written in the triplet code. Human DNA contains tissue-specific information that instructs brain or muscle cells to produce the suite of proteins that make them brain or muscle cells. Other signals in the sequence help decide at what points DNA should coil around its scaffolds of structural proteins. These are the codes that computer buffs such as Shepherd want to crack with raw processing power \u2014 and that mainstream biologists are attacking, too, although using a rather more lab-based approach. \u201cWe need all these codes together to understand the dynamics of the cell,\u201d says computational biologist Manolis Kellis at the Massachusetts Institute of Technology in Cambridge. The DNA sequence contains information not just about the make-up of proteins but also about the interactions of DNA with some of those proteins, and the diverse antics of RNA. The analysis of DNA sequences is revealing patterns that have meanings at all of these levels. \u201cBiology has probably figured out a way to squeeze every bit of information from that molecule it can,\u201d says Jason Lieb, who studies DNA\u2013protein interactions at the University of North Carolina at Chapel Hill. The code that is currently most exercising the minds of geneticists is the 'regulatory code' that directs the production of suites of proteins tailored to specific cell types and used at specific times. The idea is that many of the genes switched on in DNA contain signature sequences in 'promoter' regions nearby and 'enhancer' regions that may be millions of base pairs away. In a blood cell, say, these signature sequences might be bound by proteins A, B, C and D, whereas genes switched on in skin may be regulated by signature sequences that bind proteins B, C, Y and Z. \u201cThe biggest obstacle after the sequencing of the genome has been to understand how genes are regulated and how we can see that from the sequence,\u201d says Jussi Taipale, who studies gene regulation at the University of Helsinki, Finland. \u201cIt's a more complex code than the genetic code.\u201d The first difficulty is the sheer scale of the problem. Human cells contain more than 20,000 protein-coding genes, roughly 1,500\u20132,000 transcription factors, which switch genes on and off, and numerous other regulatory proteins and RNAs that direct their production. The possible permutations and combinations are bewildering. \n               Lost in translation \n             One way to start solving the regulatory code en masse would be to find all the positions where each of the regulatory proteins binds within the genome. Many transcription factors show a penchant for binding specific short motifs in DNA, such as a six-letter sequence. In theory, a computer could scan for any such motifs that occur more often than might be expected by chance. But there are drawbacks. For one thing, a given six-base-pair sequence will sometimes be a binding site and sometimes not, probably depending, in part, on whether the DNA is folded up in a way that prevents transcription factors from gaining access. For another, the way that these sites are recognized is not as specific as the binding between the bases that translate the triplet code into protein. Transcription factors recognize DNA sequences from the effects of the sequence on the outside of the helix, and although this recognition is still sequence dependent, it is not quite so precise. Some of these proteins will bind to a range of related sequences \u2014 sometimes more tightly, sometimes less so \u2014 and those subtleties of affinity, like the nuances of a social embrace, may themselves have biological meaning.\u201d Len Pennacchio at the Lawrence Berkeley National Laboratory in California and his colleagues have begun to fathom some of these subtleties by identifying a rudimentary tissue-specific code for the human brain 1 . They teased out the relevant enhancers from the human genome by comparing the human sequence to those of distant relatives such as the pufferfish ( Takifugu rubripes ), pulling out regions that didn't describe proteins but that evolution had nevertheless deemed important enough to keep intact. They then systematically inserted 167 such regions into mouse embryos and found that 45% of them provided tissue-specific ways to switch on genes. The team identified four enhancers that boost gene activity in the developing forebrain and share several short-sequence motifs that are presumably binding sites for control proteins. By searching for similar signature sequences in the human genome, they located other forebrain enhancers, suggesting that they have found some of the sequence information that 'means' brain-specific in the regulatory code. Taking a slightly different tack, Richard Young at the Whitehead Institute for Biomedical Studies in Cambridge, Massachusetts, and his colleagues have come up with a preliminary code that distinguishes human embryonic stem cells 2 . They extracted human DNA bound by three key transcription factors and determined all the sequences to which those proteins chose to bind. The proteins recognize sequences near genes that need to remain active for stem cells to stay stem cells; they also recognize other sites where they seem to help shut down the genes needed for the stem cells to differentiate into other cell types. So these proteins, in combination with others, seem to stop stem cells from becoming other cell types. Many researchers are now talking about a coordinated effort to identify the regulatory code for all human transcription factors in multiple tissues. But they are unlikely to resolve this code without simultaneously extracting other layers of overlapping information in DNA. A section of DNA can contain two or more layers of information that are used at different times or in different ways depending on the cell's requirements. So whether a given sequence is read as a binding site for a transcription factor to some extent depends on how the DNA involved is packaged at that point in the chromosome \u2014 and that packaging depends on a different code stored in the DNA. \n               Wrap stars \n             A human cell has to fit about two metres of DNA into a nucleus a few micrometres in diameter; that requires packing it together with proteins in a complex hierarchy of folding back and wrapping round. The fundamental element underlying all this packaging is the nucleosome \u2014 147 base pairs of DNA wrapped around a globule of eight proteins called histones. Up to 90% of DNA is bundled up into nucleosomes, and their position influences the DNA's activity. Sequences wrapped up in nucleosomes are often less accessible to transcription factors and so less likely to be transcribed. It has been known for more than two decades that in the test-tube certain sequences are more likely to be packaged up in nucleosomes. But in the real hustle and bustle of the cell, it was unclear to what extent such preferences get honoured. Earlier this year, Eran Segal at the Weizmann Institute of Science in Rehovot, Israel, Jonathan Widom at Northwestern University in Evanston, Illinois, and their colleagues came the closest yet to defining a code for the position of nucleosomes 3 . They took DNA wrapped up in nearly 200 yeast nucleosomes, and 177 from chickens, and exposed it to enzymes that would eat up all sequences in between the nucleosomes. They then sequenced the DNA left intact in the nucleosomes, and used computational methods to align the sequences and search for common patterns. The team came up with a set of rules that could predict where more than 50% of nucleosomes lie in yeast and chicken DNA. \u201cIt's much less than perfect but way better than random,\u201d Widom says. The main rule is that the sequences AA, TT or TA are more likely to be found where the spiralling DNA backbone grazes the histone \u2014 they seem to help the DNA bend around the protein core. But Segal and Widom's rules can't predict the position of a significant fraction of the nucleosomes. DNA's overlapping codes mean that an individual nucleosome might be usurped if regulatory proteins are already tightly bound there. The nucleosome code depends on the regulatory code, just as the regulatory code depends on the nucleosome code. In addition, the position of a nucleosome might be influenced by the way in which the nucleosome-wrapped sequence is folded and condensed yet further. \u201cThe code specifies the initial state and the cell can mess with what happens afterwards,\u201d says Oliver Rando, who studies nucleosome positioning at Harvard University. The goal now is to find codes that govern those larger-scale features of DNA packaging, such as how the nucleosomes are twisted up into a cable of chromatin and eventually coiled into the tightly interwoven ropes of the chromosome. As yet, though, researchers have not found landmarks equivalent to nucleosomes that can guide the search for meaning \u2014 nor is it clear that they will. \u201cThere could be diffuse information spaced at hundreds of kilobases that helps package even larger pieces of the genome together,\u201d says Lieb. \u201cOr it could be that the exact position of those structures is not important.\u201d \n               Room for manoeuvre \n             DNA seems well adapted to supporting a number of codes. For a start, only 1\u20132% of the human genome is occupied with protein-coding sequences, which leaves plenty of intervening DNA to hold other information. But many stretches of DNA in humans and other organisms manage to multitask: a sequence can code for a protein and still manage to guide the position of a nucleosome. This is possible because the triplet code is 'degenerate'. Several slightly different triplets can code for the same amino acid, and many positions in a protein can be filled by different amino acids \u2014 so different sequences can effectively mean the same thing. This allows other signals to be imprinted on top of the first \u2014 especially when those other signals are themselves encoded with some slack. This elegance is surely the handiwork of evolution \u2014 and if the way in which that hand had worked to solve these problems were clearer, the simultaneous decoding of all the messages involved might become easier. Perhaps ancestral organisms had simpler sequence patterns that evolution has optimized, taking advantage of its degeneracy to layer in additional information that helped organisms acquire extra complexity. Hanspeter Herzel, who specializes in statistical analyses of DNA at Humboldt University, Berlin, speculates that the space constraints of the cell may have favoured the development of nucleosomes that wound up unruly DNA \u2014 and that their existence then encouraged the evolution of a nucleosome code in the sequence because this lowered the energetic cost of coiling up DNA. But as yet such ideas, and any help they might offer, remain tentative. \u201cWe don't really have a phylogeny of these signals,\u201d he says. And in some cases, it seems that evolution may have generated patterns that have no clear biological function. In 1992, Gene Stanley at Boston University, Massachusetts, and his co-workers created waves when they suggested that there were patterns in DNA that spanned hundreds and thousands of base pairs 4 . Stanley used the types of statistical techniques that identify correlations in climate and financial data and applied them to all the DNA sequences available in databases at the time. Essentially, the study showed that a region with a particular chemical composition, such as one loaded with the bases A and G, is likely to be followed by a similar region hundreds or thousands of base pairs away, and that the probability of this pattern declines in a predictable way with distance. It also found that this correlation existed predominantly in DNA that did not code for protein, leading Stanley to propose that DNA previously written off as junk actually carries biological information. The findings were controversial at the time because several other groups could not repeat aspects of the analysis, and they prompted huge interest in DNA from mathematicians and physicists. Today, these correlations are thought to be real \u2014 but interest in them has faded because, despite researchers' best efforts, the patterns have not revealed anything biologically important. Perhaps, suggests Ivo Grosse of the Leibniz Institute of Plant Genetics and Crop Plant Research in Gatersleben, Germany, the patterns could simply be traces of random evolutionary processes, such as the erosion patterns elegantly but accidentally carved into sandstone by the wind. \u201cLong-range correlations definitely do exist, but I don't think it's some supercode imprinted in DNA,\u201d Grosse says. \u201cWe just stumbled on a feature with probably no deep biological meaning. But to some people the thought of order with no meaning is an affront. To such minds, the idea of teasing out nature's secrets with little more than mathematical cunning and processing power will never lose its allure. When Shepherd and his graduate student Natalie Kay, in unpublished work, ran the software that they had tried out on  Emma  over the (admittedly small) genome of Ebola virus, it identified as meaningful some sequences that, at the time, bore no annotations in genetic databases. Only later, Shepherd says, were these motifs recognized by biologists as passages that control the activity of genes or mark their ends. He thinks that approaches based on almost pure number crunching will go on to rock the field: \u201cI firmly believe that major advances in this over the next 20, 30, 50 years will be made by the theorists, not the medics.\u201d But researchers versed in the complexities of how DNA and proteins actually work remain convinced that their type of knowledge will remain vital to sorting the meaningful from the circumstantial. When the triplet code was first being studied, there were any number of fanciful mathematical and logical approaches to it \u2014 but the approaches that paid off were the ones informed by the greatest degree of biological insight. \u201cComputer scientists think they can just walk in the door and solve things,\u201d says bioinformatics expert Wyeth Wasserman at the University of British Columbia in Vancouver, Canada. \u201cBut they come to realize you need biology too.\u201d \n                     Genetics: What is a gene? \n                   \n                     Epigenetics: Unfinished symphony \n                   \n                     Human genome: Patchwork people \n                   \n                     DNA: Beyond the double helix \n                   \n                     50 Years of DNA \n                   \n                     ENCODE project \n                   Reprints and Permissions"},
{"file_id": "444143a", "url": "https://www.nature.com/articles/444143a", "year": 2006, "authors": [{"name": "Jane Qiu"}], "parsed_as_year": "2006_or_before", "body": "Working out whether premature babies feel pain has important implications for child development, says Jane Qiu. It was a precarious beginning. Teresa was born 14 weeks earlier than she should have been. Having spent 26 weeks in the womb, she now lies motionless in an incubator in the neonatal intensive-care unit, relying on mechanical ventilation for breathing and intravenous tubes for nutrition. Her tiny, fragile body is covered with sensors to monitor blood pressure, heart rate, breathing and temperature. Teresa's destiny is far from certain. She is suffering from the common complications associated with premature birth, such as breathing difficulties, anaemia and infections. She has to go through many routine medical procedures every day, and also faces a number of major operations in the future. Teresa is a fictional but representative composite of hundreds of thousands of premature babies born each year. Thanks to medical and technological progress, babies born after 23 weeks' gestation in developed countries are now routinely kept alive. But many of them have life-threatening complications. It is estimated that each baby in a neonatal intensive-care unit is subjected to an average of 14 procedures a day \u2014 a number that can go up to 50 in extreme cases 1 . Although clinicians and nurses intuitively feel that most of these procedures may be painful, there is no easy way to discover whether the babies are feeling anything. It's a vexed question. Medics rely on self-reporting for pain assessment, but babies can't use words to explain exactly what they are feeling. For a long time, many health professionals assumed that newborns, and those born prematurely in particular, were too young to perceive pain in the same way that older children and adults do \u2014 or if they did, were not able to remember it. They were wary of giving painkillers to infants 'just in case', as the effects and dosing of these drugs had been tested only in adults. Some neonatal clinics started to give painkillers to premature babies during certain medical procedures in the late 1980s but, as yet, there is no consensus on best practice. \u201cMost intensive-care units have no guidelines for managing pain in preterm babies, so it's all very patchy and up to the individual clinicians and nurses on duty,\u201d says Judith Meek, a paediatric consultant at the Elizabeth Garrett Anderson and Obstetric Hospital in London. The question of whether preterm babies feel pain goes beyond the obvious ethical and humane issues. In recent years, neuroscientists have unearthed evidence suggesting that the nervous systems of premature babies who experience repeated procedures that are potentially painful develop abnormally. Such children may be either excessively susceptible or desensitized to pain as they get older. The problem is likely to increase in the future, as the number of premature births has been rising in westernized countries 2 . This is partly because of the increased number of multiple births resulting from more widespread use of fertility treatments. There are further implications for neuroscientists too. They hope that, as well as answering the question of whether premature babies feel pain, their results will also help them understand more about how an infant's nervous system develops. This could form the groundwork for the development of much-needed drugs specifically designed for children, whose physiology is often different from that of adults. \n               Pain barrier \n             At the moment, practitioners use 'pain-coding scales' to assess the amount of pain babies might feel. Each scale consists of a set of behavioural criteria, such as crying, withdrawal and facial expressions, as well as physiological indicators, such as heart rate, blood pressure and breathing patterns. \u201cAll those measures have their strength,\u201d says Maria Fitzgerald, a neuroscientist at University College London. \u201cBut they may not be true reflections of pain experience, as they can also change under other stressful conditions, such as hunger and cold.\u201d Part of the problem is that the experience of pain is far more than just a straightforward physiological response to stimuli. Being a psychological state, pain is always subjective, and is coloured by a number of cognitive and emotional factors. It can result from nociception \u2014 the physiological detection of noxious stimuli by pain receptors and nerves \u2014 but can also occur independently of this, as a sufferer of phantom-limb pain will attest. Animal studies suggest that nociception is likely to occur in premature infants 3 . But to work out whether premature babies experience pain, neuroscientists need to tackle two key questions. The first is whether the signals generated by the stimuli actually reach the cerebral cortex, the part of the brain that is crucial for the perception of bodily sensation. The second is: to what extent are premature infants conscious, that is, sufficiently self-aware to experience and be emotionally distressed by pain? Have they yet developed minds? The debate over whether pain signals reach the cortex in premature babies was fuelled by work first published in 2002 by Tim Oberlander, a paediatrician at the University of British Columbia in Vancouver, Canada. He showed that premature babies with severe brain injuries that disable much of the cortex have the same facial expressions and behaviours in response to potentially painful stimuli as normal babies of the same age 4 . This suggested that the behavioural and physiological responses of premature babies to such stimuli are merely reflexes put together at the level of the spinal cord and brain stem. If signals of pain do not go beyond the brain stem, it is unlikely that preterm infants can feel pain. Recently, however, a number of scientists have attempted to see inside the brains of premature babies to find out if this is the case. Two teams of researchers, one led by Fitzgerald, the other by Kanwaljeet Anand at the University of Arkansas for Medical Sciences in Little Rock, have independently shown that the cortex of premature babies is activated in response to potentially painful stimuli 5 , 6 . Both groups used near-infrared spectroscopy (NIRS), a non-invasive imaging technique that can measure subtle changes in the concentration of the blood's oxygen-carrying molecule haemoglobin. Neuronal activity in the cortex is coupled with changes in the blood flow to different regions of the brain, and this is reflected by the ratio of oxygenated haemoglobin to its deoxygenated counterpart. Because of this, NIRS can be used to monitor neuronal activity in the cortex. It is not as sharp or accurate as other imaging systems, such as functional magnetic resonance imaging (fMRI) and positron emission tomography (PET). But these systems would involve moving a very sick and fragile infant into a noisy and stressful environment. NIRS can be performed by simply placing sensors on a baby in its cot. Using this method, Fitzgerald's team measured cortical activity in response to a heel lance, a widely used procedure for collecting small blood samples, in 18 babies of between 25 and 45 weeks of gestational age, and postnatal ages ranging from 5 to 134 days 5 . Unexpectedly, even in the youngest babies, there was a small but very clear evoked activity in the somatosensory cortex, the part of the cortex that processes bodily sensation. A brief, non-invasive mechanical stimulation of the heel did not produce detectable cortical activation, even when babies withdrew their feet in reflex action. The researchers found that the older the baby, the bigger the response in the cortex. \n               Conscious thoughts \n             In a parallel study, Anand and his team analysed 40 preterm babies, born at 28 to 36 weeks of gestation, at 25 to 42 hours after birth 6 . They used vein puncture, another way of collecting blood samples, to generate potentially painful stimuli. Following either tactile (skin disinfection) or painful (vein puncture) stimuli, there was an increase in blood flow to the somatosensory cortex, but not to other areas of the cortex. The increase induced by vein puncture was much larger than that by skin disinfection, supporting the findings made by Fitzgerald's team. But unlike Fitzgerald, Anand found that the more premature the baby at birth, the bigger the response to painful stimuli. This difference between the two studies may be explained by the fact that the teams used different stimuli and babies of different ages. Even so, they do have the same central finding in common: there is activation of the cortex following acute pain in premature babies. The finding is met with great enthusiasm from other pain researchers. \u201cTo make this kind of breakthrough in human neonates is extremely difficult,\u201d says Ruth Grunau, a paediatric psychologist also at the University of British Columbia. \u201cIt presents a major step forward in the field.\u201d But proof of cortical activation in babies is not exactly the same as proof of consciousness. So how sure are the researchers that those preterm infants do have the same subjective experience that adults recognize as pain? There is simply no proof of consciousness, even in wide-awake adults. And recent studies have raised the question of whether comatose adults, who are also unable to speak, might have some level of consciousness, although this remains highly controversial 7 . So proving what a baby feels is currently impossible. \u201cHow can you take any biochemical or biophysical measures and equate that to a subjective, conscious experience?\u201d asks Anand. \u201cIt is a conundrum.\u201d Anand thinks that the best way forward, for now, is to infer that premature babies can feel pain. \u201cIf a baby manifests the same behavioural and physiological responses to pain as seen in adults, together with activation of the cortex, then we can be fairly confident that he is feeling pain, even though that's no proof of consciousness,\u201d Grunau agrees. \u201cWe would seem to be holding an extraordinary standard if we didn't infer pain from all those measures.\u201d That being the case, the issue of alleviating pain and suffering becomes paramount, especially now that scientists know that repeated painful experiences can alter the development of the nervous system. Some studies show that children born prematurely become less sensitive to pain, whereas others demonstrate that children, especially those who were born prematurely and who have had multiple operations early in life, need a higher dose of analgesics when they need further surgery 8 . In other words, early repetitive pain experiences can either increase or dampen sensitivity to pain later on in life. \u201cThere are a lot of confounds here,\u201d Fitzgerald acknowledges. This is not surprising, given the complexity of the phenomenon and the difficulties with human neonate research. \n               Subjective responses \n             Studies with newborn rats also show that, depending on the timing, nature and duration of the painful stimuli, the animals become either under- or over-sensitive to pain later on. Both are potentially problematic: children who are less sensitive to pain may be more prone to injury, whereas those who are hypersensitive may withdraw from everyday activities. Grunau is also concerned about changes in such children's stress responses. She and her colleagues have conducted detailed studies to correlate the amount of potentially painful procedures premature babies undergo in intensive-care units with their levels of stress hormones at various stages of early postnatal development. They found that premature babies of 8 and 18 months have much higher levels of stress hormones than their full-term counterparts 9 , 10 , although how long these effects persist remains unknown. A large body of animal and human studies has shown that prolonged exposure to stress hormones can result in changes in the brain, especially in areas important for learning and memory 11 . \u201cPreterm babies are vulnerable because the structure of their brains is still being formed,\u201d says Meek. \u201cThis is really worrying.\u201d Detailed analysis of a large number of children between the ages of 5 and 14 indicates that children who were born prematurely are about 2.6 times as likely to have attention-deficit disorder as their full-term equivalents 12 . Such children also tend to have various learning difficulties, increased levels of anxiety and poorer cognitive outcome. It is difficult to pinpoint the precise causes of these differences \u2014 after all, prematurity brings a host of problems that could all contribute in one way or another. But, says Grunau, pain is certainly on this list of potential factors. So researchers are trying to develop ways to measure pain more accurately in premature babies. Fitzgerald is now testing how well cortical activity in response to potentially painful stimuli correlates with subjective assessment of pain by medical practitioners. She is also using imaging to monitor the efficacy of painkillers in the babies. To refine the findings with NIRS, her team is now using electroencephalography, which will allow the group to map how input from nerves is transmitted and processed in various regions of the preterm baby's cortex. Anand and his colleagues are carrying out similar kinds of studies including the effects of prolonged pain, such as post-operative pain and discomfort from being on a ventilator. Anand's team is also using fMRI and other imaging techniques to broach the highly controversial and politically sensitive question of whether the fetus can feel pain. Researchers agree that studies on premature infants won't address this question. The process of being born alters an infant's brain physiology so radically that it cannot readily be compared to that of a fetus still in the womb. Even with the help of sophisticated imaging technology, tackling this question will be far from easy. Studies of pain in premature babies could, however, fuel an initiative to develop painkillers designed especially for babies and children. Drug regulatory bodies, government funding agencies and charities have now realized this is an important issue. In the United States, the Best Pharmaceuticals for Children Act of 2002 authorizes government agencies, in particular the National Institutes of Health and the Food and Drug Administration, to help develop drugs specifically for children. Task forces of scientists, clinicians and ethicists specialize in various conditions affecting newborns, including pain as well as neurological, cardiovascular and respiratory diseases. The initiatives aim to set priorities for scientific research and to establish ethical guidelines for clinical trials using babies. One such task force, the Neonatal Pain-Control Group, \u201cis one of the top priorities among all the task forces,\u201d says Anand, its chairman. \u201cWe expect to see some major clinical advances in neonatal pain management within the next few years.\u201d \n                     Europe backs trials on drugs for kids \n                   \n                     UN surveys ecological ravages of tsunami \n                   \n                     IVF health risks pinpointed \n                   \n                     NIH Newborn Drug Development Initiative Workshop \n                   Reprints and Permissions"},
{"file_id": "443502a", "url": "https://www.nature.com/articles/443502a", "year": 2006, "authors": [{"name": "Jonah Lehrer"}], "parsed_as_year": "2006_or_before", "body": "We're selfish and rational \u2014 that's what classical economics says. But play parlour games with brain scanners and you'll find we're pulled in different directions when it comes to money. Jonah Lehrer reports. Read Montague spent the summer of 2003 thinking about soft drinks. His teenage daughter was working as an intern in his lab at Baylor College of Medicine in Houston, Texas, and Montague, a neuroscientist, wanted to find an experiment that she could \u201cwrap her head around\u201d. After much deliberation, he came up with the perfect research topic: recreating the Pepsi Challenge. In a brain scanner 1 . Pepsi launched this advertisement, one of the most famous of all time, in the early 1980s. Television ads showed people on the street being asked to sip cola blindly from two different glasses. Not surprisingly, the ads featured Coca-Cola aficionados who, much to their astonishment, found they preferred the taste of Pepsi. But if Pepsi really tasted better, Montague wondered, then why would Coke still be more popular? When we are standing in the supermarket, faced by cans of Coke and Pepsi, what is happening inside our brains? Montague is at the cutting edge of a new scientific field known as neuroeconomics, which uses the experimental techniques of neuroscience to understand how the brain makes economic decisions. Biology, of course, has long been used to explain human nature; evolutionary biology seeks the causes of behaviour in terms of its fitness benefit; and cognitive psychology aims to model decision-making. Neuroeconomics is different: it seeks to understand the most immediate causes of economic choices by seeing how the brain makes them. By studying the brain at work, neuroeconomists hope to resolve well known anomalies such as why stock markets are sometimes gripped by 'irrational' exuberance, or why people rack up huge credit-card debts to buy things they don't need. The stubborn persistence of these perplexing phenomena defies classical economics, founded as it is on two assumptions about human nature: that we are rational and that we are selfish. When confronted with a variety of options, traditional economists expect us to evaluate the possibilities (rationality) and choose whichever best matches our personal preferences (selfishness). Their mathematical models require this predictable behaviour. What the eighteenth-century economist Adam Smith called the \u201cinvisible hand\u201d of the marketplace is just the collective result of lots of reasonable people going about the business of trying to maximize their own advantage. Such pure rationality is disconcertingly rare, however. Neuroeconomists want to explain why, and their research promises to affect everything from what cola we drink to how we save for retirement. \n               Fair play \n             The story of neuroeconomics began in the early 1980s with a parlour game, the ultimatum game, devised to investigate economic behaviour 2 . The rules are simple. An experimenter puts two people together and hands one of them $10. This person (the 'proposer') has to offer some of the money to the other. The second person (the 'responder') can either accept the offer, in which case both players pocket their respective shares, or reject it, in which case the $10 is taken from the proposer, and both players walk away empty-handed. According to the predictions of classical economics, the game should always generate the same outcome. The proposer should offer the responder a minimal amount, $1, and the responder should always accept it. After all, $1 is better than nothing, and a rejection leaves both players worse off. If the ultimatum game played out this way, it would be a clear demonstration of our rational self-interest. When the game is played, however, that doesn't happen. Instead of taking a small profit, responders typically reject any offer that they think unfair. Proposers tend to anticipate this 'irrational' rejection and, instead of offering a minimal amount, typically propose around $4. This isn't a cultural prejudice: people around the world play the ultimatum game the same way. The only ones who obey the expectations of classical economics are autistic adults: because they don't take the feelings of the other player into account, they typically offer the minimum amount. Why would someone make the seemingly irrational decision to reject free money? Evolutionary game theory provides some insight. In the real world, losing out on the money in the short term could mean getting a social benefit in the long term: a reputation for not being a pushover, for one thing 3 . Players in the ultimatum game, however, don't have to worry about this: they play each other only once and have no information on their partner's reputation. But fairness still trumps reason. So what is going on inside the brain? In 2003, Alan Sanfey, Jonathan Cohen and their colleagues at Princeton University, New Jersey, used a technique called functional magnetic resonance imaging, or fMRI, to look inside the minds of people playing the ultimatum game 4 . fMRI scans highlight areas of the brain that are more metabolically active than others and are therefore thought to have increased neuronal activity. The team found that 'unfair' offers led to brain areas such as the anterior insula, associated with strong, negative emotions such as disgust and pain, becoming more active. At the same time, there was activation of brain areas associated with information processing and long-term planning, such as the dorsolateral prefrontal cortex (DLPFC). \n               In two minds \n             When subjects were struggling to decide whether or not to reject an unfair offer, those whose anterior insula showed more activity than their DLPFC tended to reject unfair offers, whereas those whose brains exhibited the opposite pattern tended to accept them. This, says the team, suggests that competition between these areas influences decision-making \u2014 and emotions usually win. \u201cThe platonic metaphor of the mind as a charioteer driving twin horses of reason and emotion is on the right track,\u201d wrote the neuroeconomists Colin Camerer of the California Institute of Technology in Pasadena and George Loewenstein of Carnegie Mellon University in Pittsburgh, Pennsylvania, in an unpublished working paper. \u201cExcept that cognition is a smart pony, and emotion a big elephant.\u201d This interpretation, in which the brain is capable of both deductive logic and irrational emotion, often simultaneously, is known as the 'dual-process' model and it remains controversial. \u201cImagine showing this model of cognition to an economist and a neuroscientist,\u201d Camerer says. \u201cThey'll both think it's wrong, but for opposite reasons. The economist will say it is too complicated: the brain only needs a rational system. The neuroscientist will say it's too simple, and that our brain regions can't be parcelled out into rational and irrational categories. Neuroeconomics is trying to find the middle ground.\u201d Some researchers don't think that middle ground has been found. Paul Glimcher, a neuroeconomist at New York University, warns that the dual-process model is not biologically accurate. \u201cExperiments done on monkeys have never supported this notion of there being two fully independent decision-making systems,\u201d Glimcher says. \u201cThis doesn't mean that emotions don't exist or that they don't influence our behaviour. What it does suggest is that our emotions are not just a negative impulse that gets in the way of our rationality. They are much more integrated than that.\u201d Rather than focus on brain-imaging research \u2014 which Glimcher dismissively calls \u201cspots-on-brain\u201d experiments \u2014 his lab records from neurons in specific areas of the cortex while monkeys are making 'economic' decisions, such as trying to maximize a reward of fruit juice. Glimcher argues that rigorous neuroeconomics will require this sort of reductionism \u201cto construct a general theory of neural decision-making\u201d. Other neuroeconomists counter that, although fMRI has serious limitations \u2014 Camerer admits it's a \u201cvery imperfect tool\u201d with some \u201cserious signal-to-noise problems\u201d \u2014 it remains useful for giving insight into what the brain is doing. Many neuroeconomists, however, judge their field by what it adds to economic theory rather than by its neurological precision. In these terms, some argue that the dual-process model has already been a success because it can explain behaviour that economists have not been able to. According to Andrew Lo, director of the Massachusetts Institute of Technology laboratory for financial engineering, \u201cEconomics has hit the wall. It has explained about as much as it can with the tools it has. There are too many inconsistencies between theory and data.\u201d \n               Thinking ahead \n             One anomaly that continues to confound economists is humanity's often irrational approach to the future. Instead of saving money for retirement, people tend to impulsively splurge on the present. Neuroeconomists are beginning to understand the neural roots of this behaviour. In a 2004 brain-imaging experiment led by Samuel McClure of Princeton, people were asked whether they wanted a low-value Amazon gift voucher now or a higher-value voucher in two to four weeks 5 . McClure wanted to test a specific assumption of classical economics: the idea that we apply the same calculus to the future and the present. If that were true, then the same brain regions should become active whether we are thinking about the results of economic decisions in the future or in the present. This isn't what McClure found. When his subjects contemplated receiving gift vouchers in the future, brain areas associated with rationality (such as the prefrontal cortex) became active. These cortical regions seemed to urge people to resist temptation and wait for the more valuable vouchers. On the other hand, when people started thinking about getting a gift voucher right away, brain areas associated with emotion \u2014 the midbrain dopamine system, for instance \u2014 were also turned on. By manipulating the value of vouchers in each situation, the researchers could compare the levels of activation in the different regions. They discovered that the relative amount of activity was \u201cdirectly associated with subjects' choices\u201d. People whose 'emotional' brain areas were more active opted for the spoils of immediate gratification. This discovery has important implications. For starters, it helps explain why people often fail to save enough for their retirement. Because our emotions warp our better judgement, we delay saving. Loewenstein, who collaborated on the McClure paper, thinks that understanding how we make such decisions will help us develop better economic policies: \u201cOur emotions are like programs that evolved to solve problems in our distant past. They are not necessarily well suited to modern life. It's important to know how they lead us astray so that we can design incentives and programmes to help compensate for our irrational biases.\u201d What might a savings scheme informed by neuroeconomics look like? In March 2004, behavioural economist Richard Thaler of the University of Chicago, Illinois, testified before the Senate on ways to increase the national savings rate \u2014 US consumers currently have a savings rate close to zero 6 . His plan was simple: rather than asking people if they want to start saving right away, companies should ask people if they want to opt into a savings plan that begins in a few months' time. This allows people to make decisions about the future without contemplating the present, bypassing our irrational emotions. McClure's brain research suggests that should be a smart approach, and indeed trial studies of Thaler's plan have been a resounding success: after three years, average savings rates jumped from 3.5% to 13.6%. \n               Credibility gap \n             Many experts remain sceptical of neuroeconomics, however. The Princeton economists Faruk Gul and Wolfgang Pesendorfer think it is based on a faulty premise. They argue that economic models should be judged by their success at explaining phenomena such as inflation or unemployment, not by \u201cpsychological realism\u201d. In an attempt to close that credibility gap, neuroeconomists are trying to bring their experiments closer to the decision-making models of microeconomics, which studies individual behaviour. If they can't scan people's brains in the real world, they can at least bring a little bit of the real world into the lab. Take, for example, Montague's Pepsi Challenge experiment. Rather than playing parlour games in an fMRI machine, he monitored people's brain activity as they swallowed sips of soda 1 . When the Coke and Pepsi were offered unlabelled, people showed no measurable preference for either brand. Most of the time, they couldn't even tell them apart. Montague's second observation was telling: people strongly preferred drinks that were labelled as Coke, no matter what cola was actually delivered through the tubes. Brand trumped taste. The fMRI scans revealed that when the drinks were offered unlabelled, the ventromedial prefrontal cortex (VMPFC) became active. This makes sense, because the VMPFC is involved with the processing of appetitive rewards such as sugary drinks. However, when the subjects drank a cola with a Coke label, more brain areas were turned on. The hippocampus and midbrain all reacted strongly to the red cursive of Coke but not to the blue Pepsi logo. This happened even when subjects were given Pepsi with a Coke label. Montague notes that the brain regions triggered by Coke have all been implicated in 'affective' \u2014 emotional \u2014 influences on behaviour. Brand power exerts a more powerful force over our emotions and decisions than we might like to think. \u201cAdvertising is a deeply biological game,\u201d Montague says. \u201cThe idea of Coke clearly affects our judgement.\u201d \n               Trust me \n             People rarely make economic decisions in isolation like this: most involve interacting with others. So Camerer, Montague and Steve Quartz, also at the California Institute of Technology, decided to link their fMRI machines together and monitor different brains simultaneously 7 . This allowed them to measure brain activity during social interactions, as subjects were learning to trust each other. They invented a simple trust game in which an 'investor' has the option of entrusting money to a 'trustee'. Invested money gets tripled, and the trustee can then keep it all, or give some or all of it back to the investor. Because the game is typically played for ten rounds, however, with the investor receiving more cash to play with each round, each player has a selfish incentive to trust each other. The researchers discovered that increased activity in the caudate nucleus \u2014 a region involved in the brain's reward pathway \u2014 of the trustee was directly correlated with the trustworthiness of the investor's behaviour. Furthermore, this activity appeared much more quickly in later rounds of the game, indicating that the trustees were forming an opinion of their partners. Why the caudate nucleus? The neuroeconomists speculate that a decision to trust someone else depends upon our expectation of getting a reward in the end. The caudate nucleus gets excited by the anticipation of material pleasures such as food, drugs and money, so it makes sense that it would also measure the rewards of our social interactions. Trust is therefore an admirable trait with selfish origins. Neuroeconomists are bullish about their own potential. \u201cEconomics provides us with all sorts of elegant theories, and neuroscience gives us a new way of testing their predictions,\u201d says Loewenstein. \u201cAll we need is time.\u201d Daniel Kahneman, a psychologist at Princeton, agrees. \u201cThese researchers have the chance to come up with a general theory of decision-making that is both biologically and behaviourally accurate. They have the necessary experimental tools and are asking the right kind of questions. Now they just have to find some answers.\u201d \n                     Lure of lie detectors spooks ethicists \n                   \n                     Brain imaging ready to detect terrorists, say neuroscientists \n                   \n                     Ethicists urge caution over emotive power of brain scans \n                   Reprints and Permissions"},
{"file_id": "443629a", "url": "https://www.nature.com/articles/443629a", "year": 2006, "authors": [{"name": "Erika Check"}], "parsed_as_year": "2006_or_before", "body": "It started life as an anaesthetic, then became a psychedelic club drug. Now researchers think ketamine could hold the key to understanding and treating depression, says Erika Check. \n               B held my hand and we started our roller coaster out... I can't feel my body anymore except this overriding general fuzziness. The lines on the ceiling become a tunnel and I am flying down it faster than sound... Then the room does a somersault and I with it... I'm scared... this is a thrill ride and the car is the dimensions of existence. \n               Such are the wonders of the drug ketamine, according to U.S. 9, who described this experience with the drug on an online forum called the Erowid Experience Vaults 1 . Many others around the world use ketamine illegally, going \u201cdown the K-hole\u201d to abandon reality and alter their consciousness. Now neuroscientists are getting in on the act. They are finding that although ketamine makes some lose their minds, it might help others to find their sanity. Ketamine was invented in the 1960s by chemists at the drug company Parke-Davis in Detroit. The drug was a powerful anaesthetic, but also caused 'dissociative' effects, such as the feeling of leaving one's body and entering other planes of existence. Although popular with recreational drug users, ketamine is only used medically as an anaesthetic in animals and children, who are less prone to its psychedelic side effects. But back in the 1960s, a handful of scientists used these side effects to explore human consciousness (see ' Exploring the dark '). Today, scientists are using ketamine as a tool to study mental illness, and perhaps even treat it. Clinical trials suggest that the drug might help patients with severe depression. And these experiments are changing the way scientists think about the nature of the disease and how it might be treated. The World Health Organization estimates that depression is the world's leading cause of disability. But doctors and scientists are still mystified by what the disease actually is. The label 'depression' is a catch-all term for a condition with a huge array of possible symptoms and causes. Some depressed people feel sad, guilty or tired; others cycle into hyperexcited mania, or feel worried and anxious. Genes, stress and negative thought patterns have all been linked to depression. But it is not clear what chain of events causes a particular set of symptoms in an individual. Scientists also aren't quite sure why modern antidepressant drugs succeed or fail to cure depression in different patients. The drugs act on neurotransmitters, the chemicals that brain cells use to communicate. Most of today's drugs target a particular class of neurotransmitter called the biogenic amines. These include serotonin, which is targeted by selective serotonin reuptake inhibitors, or SSRIs. Fluoxetine and sertraline, sold in the United States as Prozac and Zoloft, are members of this enormously popular category of drugs. But the drugs that act on biogenic amines take weeks to work, and fail to help at least 40% of depressed patients 2 . So neuroscientists suspect that these drugs don't hit depression at its source and are searching for other approaches. The search took a dramatic turn in August this year, when a team led by Husseini Manji, at the National Institute of Mental Health in Bethesda, Maryland, published a study 3  looking at the effects of ketamine on 18 severely depressed patients, all of whom had failed to respond to standard treatments. In the double-blind trial, doctors gave the patients intravenous ketamine or placebo saline drips, and then scored the responses. After taking ketamine, 12 of the patients improved by at least 50% on a depression rating scale. Patients felt better as little as two hours after treatment. And one-third of the patients still felt better a week later. Although unexpected, Manji's results are not the first to hint that ketamine has unexpected benefits. In 2000, John Krystal at Yale University and his colleagues published the results of a similar trial on eight severely depressed patients who didn't respond to standard medications 4 . This too suggested that ketamine might work as an antidepressant. Four of the patients felt much better after the ketamine treatments \u2014 their depression scores dropped by more than half, by one rating scale. And it worked fast. The patients felt better just three days after their treatment. \n               Special forces \n             At the time, these results astonished other psychiatrists. \u201cTen years ago, I would have said there's no way in hell this drug would work as an antidepressant,\u201d says Nuri Farber, a psychiatrist at Washington University in St Louis. Many remained sceptical, and were wary about the whole idea of treating mentally unstable people with a psychoactive drug. Psychiatrists have on occasion used ketamine to deliberately destabilize normal subjects: by giving it to stable people, psychiatrists can induce a schizophrenia-like state and study the brain chemistry that may underlie the disease 5 . So psychiatrists didn't exactly rush to try ketamine in their own clinics. \u201cIt was such a small study, and nobody else was following this path. It was sort of a novelty,\u201d Krystal says. \u201cPeople didn't take it as seriously as they might have.\u201d The findings languished in the literature until one of Krystal's colleagues, Dennis Charney, left Connecticut to work at the National Institute of Mental Health. There, he began working with Manji, who had been studying chemical relatives of ketamine in mice. Manji, Charney and a psychiatrist named Carlos Zarate decided to find out whether the ketamine study was more than a fluke. When they took ketamine, Zarate's patients did experience trippy side effects such as dizziness and euphoria. But most of these effects wore off after 80 minutes 3  \u2014 before the drug's antidepressant effects kicked in. That is surprising: usually, drugs that trigger highs, such as cocaine or ecstasy, are followed by a depressive low. \u201cIt's not what I would have expected,\u201d says Farber. \u201cIf anything, I would have expected these patients to crash and burn.\u201d One possible explanation for this is that ketamine uses different pathways to trigger its psychedelic and antidepressant effects. Another possibility is that patients have to go out of their minds before they can get back to normal. \u201cWhat ketamine does is briefly make people crazy,\u201d says Eric Nestler, a neuroscientist and psychiatrist at the University of Texas Southwestern in Dallas. \u201cThe question becomes: is that disruption in cognitive function what's creating this improvement in mood?\u201d Right now, it is impossible to answer that question. But Zarate, Krystal and other researchers who have studied ketamine's link to depression have one idea about what's going on. Their hypothesis has to do with the way ketamine works in the brain. Ketamine hinders the activity of a complex molecular machine called the  N -methyl- D -aspartate receptor, the NMDA receptor for short. It is one of the receptors for a neurotransmitter called glutamate. That activity could relate to one theory of depression: that the disease occurs when brain cells are just too stressed to thrive. This link is based on two facts and one highly controversial idea. First, scientists know that the NMDA receptor can control brain-cell growth and survival. Second, they know that excessive levels of glutamate kill brain cells in some conditions, such as stroke and Alzheimer's disease 6 . Here's the controversial part: there is some evidence supporting the idea that depression is caused by brain-cell death. So, by jamming the NMDA receptor, could ketamine be correcting a toxic glut of glutamate that is harming brain cells and causing depression? It is already known that stress floods the brain with glutamate, says Zarate. \u201cIt might be that these neurons are struggling to regulate glutamate, and if you stress them over and over, they become injured.\u201d This hypothesis is still very new 7 . There is some evidence linking glutamate, and the NMDA receptor, to depression. Twenty-five years ago, for instance, scientists at the National Institute of Diabetes and Digestive and Kidney Diseases in Bethesda, Maryland, showed that chemicals that target the NMDA receptor have antidepressant effects in animals 8 . Scientists have since found that deceased depressed human patients, such as suicide victims, have abnormal numbers of NMDA receptors. And brain-imaging studies have found that depressed people have much higher levels of glutamate in one region of their brains than healthy people 9 . But Nestler and other psychiatrists caution that it is impossible to know yet whether ketamine affects brain-cell survival. Glutamate is the most common neurotransmitter in the brain, used by perhaps half of all brain cells. And the NMDA receptor is involved in a huge array of different processes, such as learning and memory, as well as cell growth and survival. So it is difficult to pin down the precise reason why tweaking glutamate through the NMDA receptor would influence human happiness. And although it is true that the NMDA receptor is involved in cell survival, this takes a long time, whereas ketamine's antidepressant effects seem to kick in within hours. Until the mechanism can be clarified, psychiatrists say, Zarate's and Krystal's studies are important for one major reason: they provide evidence that biogenic amines such as serotonin don't tell the whole story about depression. \u201cWe've had a preoccupation over so many years with biogenic amines,\u201d says John Olney, a psychiatrist at Washington University who has been studying neurotransmitters for 35 years. \u201cThe idea that glutamate might be involved in depression has evolved very slowly. We're still trying to understand it.\u201d \n               What's the use? \n             Nestler agrees. \u201cAll our available drugs act on the serotonin and noradrenaline systems, and this drug clearly does not,\u201d he says. \u201cIt's very important as a proof of principle that a drug acting on a different neurotransmitter system can have a mood-elevating effect.\u201d Even if ketamine works, it's not an ideal drug. Clinical trials have studied the drug in only 26 patients, and no one has investigated how long its beneficial effects might last; there are also those psychedelic side effects. So neuroscientists are continuing to look into whether other drugs that hit the glutamate system could help mentally ill patients. A trial of one NMDA blocker \u2014 memantine, used to treat Alzheimer's patients  \u2014  found the drug didn't cure depression 10 . But the study's authors think that is because memantine does not bind as tightly to the NMDA receptor as ketamine does. Another candidate, riluzole \u2014 already used in patients with Lou Gehrig's disease \u2014 seems more promising. It works by preventing brain cells from making excess glutamate and dumping it into surrounding brain tissue. Patients taking the drug have reported some improvement, but riluzole has yet to undergo large clinical trials for depression. Other drugs are entering tests. Whether or not these trials prove successful, at least the fresh take on depression is finally helping neuroscientists climb out of the serotonin rut. And maybe an ageing drug with a tarnished reputation will help them find their way. \n                     Club drug finds use as antidepressant \n                   \n                     Ecstasy eases Parkinson's in mice \n                   \n                     Psychedelic drugs: The ups and downs of ecstasy \n                   \n                     Antidepressants in focus \n                   \n                     National Institute of Mental Health \n                   \n                     The Vaults of Erowid \n                   Reprints and Permissions"},
{"file_id": "443622a", "url": "https://www.nature.com/articles/443622a", "year": 2006, "authors": [{"name": "Mark Schrope"}], "parsed_as_year": "2006_or_before", "body": "What can pirates' journals and centuries-old cookbooks teach modern-day ecologists? Mark Schrope meets the researchers who trawl history books for deeper insights into marine ecosystems. In the late 1600s, Alexandre Olivier Exquemelin was busy living up to his extravagant name. In his book  The Buccaneers of America , the French-born pirate describes a host of battles and \u201cbarbarous inhumanities\u201d through the Caribbean and along the South American coasts. He writes about some pirates' habit of taking Cuban sea-turtle fishermen as slaves, and he personally owns up to stealing victuals left ceremonially by grieving widows at their husbands' graves. But between all the sacking and pillaging, careful readers can also find glimpses of natural history. A description of a green sea turtle reads: \u201cTheir eggs... are found in such prodigious quantities along the sandy shores of those countries that, were they not frequently destroyed by birds, the sea would infinitely abound with tortoises... Certain it is that many times the ships, having lost their latitude through the darkness of the weather, have steered their course only by the noise of the tortoises swimming that way, and have arrived at these isles.\u201d Today, a growing number of marine scientists argue that researchers focus too narrowly on recent decades and don't make proper use of historical records to put their work in context. Clearly, the most recent data are also the most thorough. But some argue that findings from such studies are improperly informed at best, and at worst support fundamentally flawed management schemes. What if, for instance, an ecosystem had been severely degraded a century earlier, but the general understanding of the ecosystem \u2014 including restoration targets for fish and other marine creatures \u2014 was based on observations made over only a few decades? \n               Time team \n             One way to resolve this is to look at the past, and detectives in the emerging field of historical marine ecology are doing just that, scouring archives, museums and archaeological and palaeontological records. They root through relatively straightforward data, such as how many fish were caught in a particular harbour. But they also have to get creative, delving into obscure sources such as pirate logs, medieval cookbooks and restaurant menus that track the availability of seafood. Most historical marine-ecology projects fall under the Census of Marine Life, a ten-year global initiative with the ambitious goal of cataloguing everything that lives in the ocean. An arm of the programme \u2014 History of Marine Animal Populations (HMAP) \u2014 aims to determine what used to live there. By 2010, researchers will have examined more than a dozen key sites around the globe; historical studies are already under way in southwest Africa, Australia, Europe and the United States, including the latest, a study of the Florida Keys. Further work is planned in southeast Asia and New Zealand. HMAP was designed to correct the \u201chistorical myopia\u201d of fisheries scientists, says Tim Smith, one of the programme's leaders and a fisheries biologist retired from the National Marine Fisheries Service in Woods Hole, Massachusetts. In 1995, a seminal paper appeared describing the basic problem as \u201cshifting baseline syndrome\u201d 1 . In it, Daniel Pauly of the University of British Columbia argued that researchers base their understanding of healthy fish populations on what fish stocks are like during their lifetimes, with no framework for incorporating how much more plentiful fish might have been in generations past. Although historical documents can shed light on this, there are limits, says David Starkey, another programme leader and historian at the University of Hull, UK. \u201cYou can never revisit the past and replicate what went on \u2014 you can only get glimpses of the past,\u201d he says. \u201cThat's an inevitable challenge.\u201d But not facing that challenge is a mistake, he believes. The one-year Florida Keys project began in June and is designed to shift baseline observations of the ecosystem as close as possible to their positions before Europeans showed up. Graduate student Loren McClenachan, working with marine ecologist Jeremy Jackson at the Scripps Institution of Oceanography in San Diego, California, and others, is combing Spanish and British archives for records dating back to the 1600s, and US libraries for accounts from the 1800s onwards. She hopes to learn how far coral reefs used to extend around the Keys, as well as the population history for 14 important species including mangrove trees, grouper fish and the now-extinct Caribbean monk seals. \n               Rogues' gallery \n             Some sources are richly detailed. In 1803, US commissioner Andrew Elicott wrote in his journal: \u201cAlong the Florida Reef, and among the Keys, a great abundance and variety of fish may be taken: such as hog-fish, grunts, yellow tails, black, red, and gray snappers, mullets, bonefish, amber fish, margate-fish, baracoota, cavallos, pompui, groopers, king-fish, siberfish, porgys, turbots, stingrys, black drum, Jew fish, with a prodigious variety of others, which in our situation we found excellent.\u201d In an earlier related study 2 , McClenachan and Jackson looked at historical populations of sea turtles throughout the Caribbean. They found the late-seventeenth-century writings of a buccaneer named William Dampier particularly useful. For instance, he described the location of turtle nesting areas and aggregations in enough detail for modern scientists to map them accurately. Pirates may seem notably suspect as sources of information. Many pirate accounts exaggerate their exploits, says Starkey, because interesting tales from their world travels could make them money and also deflect attention from their illegal activities. But Dampier was one of the most scientifically significant and reliable, says Starkey, who has studied privateers, pirates and buccaneers. And later voyagers often confirmed Dampier's observations. Even Charles Darwin recognized the value of Dampier's work. He carried some of the buccaneer's century-old writings on his voyage on  The Beagle , and referenced them in his account of the trip. He described Dampier as \u201cso accurate a person\u201d, noting that a volcanic peak in the Galapagos Islands that he found overgrown must previously have been barren, simply because Dampier described it as such. Even some of Darwin's early work on evolution and natural selection was inspired in part by Dampier's thoughts on \u201cbastard\u201d species, such as Galapagos sea turtles, that he thought must be a mix of species from different geographical areas. \n               Net gains \n             In Florida, some of the best descriptions of the historic Keys come not from pirates, but from surveyors, captains and doctors. Not surprisingly, the sources paint a collective view of the Keys quite different from today. In 1796, Scottish surveyor George Gauld wrote that \u201camong the roots of the mangroves and about every old log or piece of rotten work, there are such quantities of the largest crayfish [spiny lobsters] that a boat may be loaded with them in a few hours\u201d. Today, divers spend hours filling just one bag. Other accounts spoke of plentiful fish and coral reefs where now only sand or rubble are found. Tracking down such stories takes a combination of creative thinking and patience. \u201cIt's a little bit of a treasure hunt,\u201d says McClenachan. Her greatest find to date is a 1775 map by Gauld that includes details about coral reefs to avoid and views of the approaches to specific islands, giving a snapshot of the area covered by mangrove. \u201cIt's just a complete gem,\u201d she says. Based in part on similar analyses of historical sources, Jackson and his colleagues have concluded that coral reefs are in severe decline worldwide, largely as a result of human activities such as overfishing 3 . Such far-reaching extrapolations have fuelled an ongoing debate over reef health. Richard Grigg, a coral-reef biologist at the University of Hawaii at Manoa, says that although he favours the historical-ecology approach in general, he thinks Jackson's conclusions are too broad. \u201cYou have to be extremely careful with that kind of information,\u201d he says. \u201cI think it's up to us as scientists to be extremely critical and not fall into sweeping generalizations about how the sky is falling.\u201d For his part, Jackson argues that historical analyses can provide powerful tools for quantitative assessments. Anecdotal evidence, too, can be scientifically valuable, he says. If, for instance, records suggest that sea turtles in a region were once plentiful but are now scarce, that provides important ecological information about the past \u2014 even without details of exactly how many there were. Like the Keys project, an HMAP study led by Heike Lotze, a marine ecologist at Dalhousie University in Halifax, Nova Scotia, has taken a broad view of a complex ecosystem \u2014 in this case the Wadden Sea in northwestern Europe 4 . \u201cIf you go there today, most of the area is protected as a national park, so I think a lot of people envision it as a natural ecosystem,\u201d she says. But \u201c500 or 1,000 years ago it was a completely different system. If you have this historical knowledge, you see how much humans have transformed it.\u201d During medieval times, fish populations in rivers and lakes connected to the Wadden Sea began to drop as people overfished it and built dams, which blocked the spawning migrations. Later dyke building also cut off estuarine nursery areas. Archaeologists have documented how fish populations plummeted \u2014 for instance, digs of waste piles from old fish markets show that sturgeon scales became increasingly less common. But cookbooks also tell part of the tale. Sturgeon was once popular even for common folk, but by the fourteenth century they were so rare that they were only served at the king's table. Later French cookbooks held clues to the species' drop in numbers, such as instructions on how to prepare veal as a substitute in recipes calling for unavailable sturgeon. As the more easily accessible freshwater fisheries collapsed, medieval fishers turned to coastal waters and then moved farther afield as fish populations there dwindled. The Wadden Sea isn't the only place where overfishing hit hard, of course. A team led by Andrew Rosenberg, a marine ecologist at the University of New Hampshire in Durham, has done extensive research to determine historical cod populations off the coasts of the United States and Canada. \n               Record catch \n             In one study, for instance, the group calculated the cod population on Canada's Scotian Shelf in 1852, based on detailed logs for schooners operating out of Beverly, Massachusetts 5 . The researchers think the information is reliable because the reporting system offered little incentive for fishers to misreport, and because there is some overlap among the logs. Overall, the study suggests that the 1852 cod population was two orders of magnitude larger than today. And more than four-and-a-half centuries before the 1990s collapse of the Atlantic cod fisheries, there was an astonishingly rich population. In 1623, fisherman Emmanuel Altham wrote in a letter to his brother: \u201cIn one hour we got 100 great [very large] cod... and if we would have but stayed after the fog broke up, we might quickly have loaded our ship... I think we got 1000 in all... When we had nothing to do my people took great delight in catching them, although we threw them away again.\u201d Rosenberg says it is too soon to say whether fisheries managers will apply such historical records to their efforts. But Thomas Hill, acting chairman of the New England Fishery Management Council and a long-time fisherman based in Gloucester, Massachusetts, believes fisheries management in his region has indeed been hampered by a dependence on modern data. \u201cI think any historical perspective is certainly important in developing long-term public policy,\u201d he says. \u201cIn the absence of that, you're shooting in the dark.\u201d Hill adds that a better understanding of the historical populations could help fishermen appreciate their collective impact. One HMAP study that could have controversial management implications involves estimates of world whale populations. Smith has looked through sources such as customs-house records and period trade journals to estimate global catch rates for whale species including the humpback and the North Atlantic right 6 . The problem is that newer genetic analyses estimate total population sizes in the 1800s to be at least an order of magnitude larger than those based on historic records 7 . In this case, pirate activities may have complicated the historical analyses, as whale catches for ships they captured were not recorded. Known sources of underreporting, though, would likely change estimates only by a few per cent, says Smith, and work continues to reconcile the discrepancy. Despite such controversies, the HMAP researchers believe that their work is gaining momentum and acceptance. \u201cThe hybridization of history and ecology has really put a set of new questions on the table that, together, the scholarly community has been able to answer,\u201d says Jesse Ausubel, an ecologist at Rockefeller University in New York and programme director for the Census of Marine Life at the Sloan Foundation. And overall, says Jackson, the work has potential to make a real impact. \u201cIt's a whole new way of viewing the world,\u201d he says. \u201cIf in some small way this can shape ecological research for the future and help us to conserve biodiversity, then that will be wonderful.\u201d Indeed, if Darwin is any indicator, even a pirate could help shape that future. \n                     Fisheries science: How many more fish in the sea \n                   \n                     History of Marine Animal Populations \n                   \n                     Gulf of Maine cod project \n                   \n                     Shifting Baselines video contest \n                   Reprints and Permissions"},
{"file_id": "443626a", "url": "https://www.nature.com/articles/443626a", "year": 2006, "authors": [{"name": "Natasha Bolognesi"}], "parsed_as_year": "2006_or_before", "body": "AIDS treatment in South Africa is often a tug-of-war between clinicians and traditional healers. Natasha Bolognesi meets a woman who is uniquely qualified to heal the rift. For most people in the developed world, the words \u2018traditional healer\u2019 conjure up the image of a figure cloaked in beads, animal pelts and an air of impenetrable mystery. Someone, in short, whom Western-ers find difficult to understand or trust, and who has rejected biomedical science in favour of mysticism and magic. This kind of distrust is problematic anywhere, but especially so in African countries struggling with the HIV epidemic. South Africa, the worst affected, is home to an estimated 5.5 million HIV-positive people, many of whom visit traditional healers, or  sangomas  as they are known locally. Many  sangomas  have earned themselves a bad reputation among doctors for not referring their HIV patients to clinics for testing and treatment. For their part, many traditional healers fear that Western medicine will harm their patients. Amid this welter of misunderstanding, there is one  sangoma  who finds herself in the unique position of being able to understand and relate to both sides of the divide. She is finding ways to reconcile the two and to build bridges of trust between doctors and  sangomas . And now, her efforts are enriching and informing a pilot project to improve the health and quality of life of HIV patients in South Africa. British born and bred, Jo Wreford is a doctor of social anthropology and a research fellow for the AIDS and Society Research Unit at the University of Cape Town. She is also one of a small number of white people in South Africa who have qualified as  sangomas , and is known to her  sangoma  colleagues and clients as Thobeka, which means 'she who can be trusted because she is grounded' in the Xhosa language. This training, which involved undertaking a spiritual journey with a  sangoma  mentor, makes it easier for other healers to trust her as one of their own. Her training in anthropology and Western cultural background reassures doctors that she understands and appreciates scientific method. \n               Different vision \n             Wreford wears beads, throws the bones, burns the herb imphepho to invoke the guidance of the ancestors, and experiences visions. She says that unlike many of her colleagues, she is comfortable with reconciling her traditional beliefs with science, arguing that biomedicine can treat the body, while traditional healing can help treat the soul. \u201cNeither cures,\u201d she says. \u201cWe must therefore use the best of what both systems have to offer to alleviate the AIDS burden, which is both physical and psychological.\u201d Wreford wants to see doctors and traditional healers working harmoniously side by side. \u201cBoth play vital roles in healing the majority of AIDS-afflicted South Africans,\u201d she says. \u201cARVs (antiretrovirals) are the only medical intervention available to alleviate the physical effects of AIDS. The traditional healer, in addition to using herbs, also works on the spiritual level, which is an essential part of the African healing process that Western medicine does not address.\u201d But this will be a huge challenge. Del Kahn of the department of surgery at the University of Cape Town explains the obstacles: \u201cMost doctors still regard  sangomas  with suspicion because they don't have training in treating serious organic disease, and the general feeling is that traditional healers do more harm than good in patients with organic illness.\u201d Sangomas  are just as wary of Western physicians. Phillip Kubukeli, founder and president of the Western Cape Traditional Healers and Herbalists Association based in Cape Town, says that during the apartheid regime in South Africa many  sangomas  believed that doctors administered poison instead of medicine to black patients in the hope of killing them off. \u201cThis fear persists today,\u201d says Kubukeli, \u201calthough it is not as prevalent as it was.\u201d Kubukeli adds that traditional healers strongly believe that physicians are out to get their hands on herbal remedies to sell to drug companies. \u201cMany Western medicines are derived from indigenous plants,\u201d says Kubukeli. \u201cThis makes traditional healers very suspicious. For example, when I am trying to bridge the gap between  sangomas  and doctors, the  sangomas  will often accuse me of trying to sell their remedies to the physicians.\u201d Driving a deeper wedge between this mutual medical divide is the South African government's perception that natural remedies can treat AIDS \u2014 a view it vigorously promotes. At the International AIDS Conference in 2000, South African President Thabo Mbeki caused an international uproar when he questioned the link between HIV and AIDS. And at this year's AIDS Conference in Toronto \u2014 where South Africa's stand, covered in beetroot, garlic and lemons, seemed seriously out of place \u2014 the South African health minister Manto Tshabalala-Msimang further discredited her government's AIDS policy by saying people must have a choice between ARVs and traditional remedies (see  Nature   443 , 134\u2013135; 2006). \u201cAs a result of this persistent denial, even at the highest government levels,\u201d says Monika Esser, a paediatrician at Tygerberg Academic Hospital in Cape Town, \u201cWestern medicine of predominantly white origin continues to be met with an element of suspicion by black patients and traditional healers.\u201d It is here, into the turmoil of the AIDS healing conflict in South Africa, that Wreford hopes to throw a lifeline, in the form of a willingness to share her acceptance of both scientific and traditional beliefs with doctors and  sangomas.  Her hope is that by trusting Thobeka, the one who can be trusted, they can learn to trust each other. Wreford qualified and practised as an architect in London before deciding to explore the spiritual aspect of her life more, and use this to help people through spiritual healing in Africa. Before leaving London, she embarked on a course of jungian psychotherapy, which emphasizes exploring the unconscious through the use of visualization and dreams. Wreford says it helped to prepare her for the intense spiritual demands made on her during her  sangoma  training. What Western medicine needs to understand, says Wreford, is that many Africans believe that their ancestors live in a separate realm and carry with them answers to the deep questions about the cause of illness. \u201cThis knowledge is accessed by the  sangoma  through ritual, visions, dreams and herbs, and communicated to the patients, who then feel they have redressed the situation, which prompts more complete healing,\u201d she explains. \u201cLike psychotherapy, it can also help people cope with stigma and emotional strain in the face of a disease such as AIDS.\u201d \n               Spreading HOPE \n             Wreford plays an equally vital role in explaining to her  sangoma  colleagues the importance of biomedicine in fighting AIDS. \u201cHere my role is specific,\u201d she says. \u201c Sangomas  believe that the disappearance of symptoms through the administration of a herb means that they have cured the patient of AIDS. I have to explain to them that this is not so and that the application of ARVs, which do not cure either, at least enables the patient to live life normally.\u201d But alone, her efforts are a drop in the ocean. According to the Western Cape Traditional Healers and Herbalists Association, there are more than 200,000  sangomas  in South Africa \u2014 vast numbers spread over a vast country. So Wreford was invited to join forces with HIV Outreach Programme and Education (HOPE), a non-profit, non-governmental organization based in Cape Town. HOPE is currently running a pilot study in five townships outside Cape Town to build mutual trust and acceptance between  sangomas  and doctors. It aims to encourage them to collaborate on HIV/AIDS intervention, to avoid disruption of ARV treatment through mistaken herbal administrations and to persuade more male clients to volunteer for HIV testing. The project operates on a cross-referral system between nine traditional healers and primary-health-care clinics in the townships. The healers, having been taught to recognize the symptoms of HIV infection and the importance of ARVs, refer these patients to a clinic for testing, counselling and treatment. The patients then come back for traditional spiritual counselling from the healer. HOPE's recruitment and training of  sangomas  for the pilot study in October 2005 sparked misgivings at first. According to HOPE training officer Pauline Jooste, one healer said: \u201cWe were very scared \u2014 the facilitator was a white person.\u201d Now the response seems more positive. Nomsisi Stefans, a practising  sangoma  from Mfuleni township outside Cape Town, says: \u201cI am happy with HOPE and, if I think they should, my clients are happy to go to the clinic to be tested, especially if I go with them.\u201d The project is closely monitored and aided by Wreford. She participates in a monthly support and supervision day for all participants, regularly visits the clinics and  sangomas  to ensure commitment and quality, and lectures medical students and doctors on the need to recognize and respect the value of spiritual healing. Much help is necessary if the project is to work. Nocawe Frans, a HOPE member and social worker at Tygerberg hospital, points out that many traditional healers are reluctant to spread HOPE's message because they resent Western interference in African traditional medicine. \u201cIt is also difficult for  sangomas  to refer patients to a hospital because in African culture the hospital carries a strong association with death \u2014 parents often take their children out of hospital and to a  sangoma  instead,\u201d she says. Despite these challenges, the five-month-old project is beginning to show a slight increase in patient referrals to clinics from traditional healers, although exact numbers are hard to come by, says Esser, who is also a HOPE management member. Patients tend to go to clinics outside their community owing to the persistence of an enormous AIDS stigma in South Africa. Also encouraging are signals from both camps that Wreford's and HOPE's efforts are working. \u201cI think we underestimate the spiritual needs of patients \u2014 the 'healing' rather than the management of treatment and cure that we are familiar with in Western medicine,\u201d says Helena Rabie, a specialist in HIV/AIDS in children at Tygerberg hospital. \u201cIf we can succeed in tapping into the traditional healers' influence as a resource to fight the spread of HIV, it would be wonderful.\u201d Confidence is rising among  sangomas  too, according to Kubukeli. \u201cThobeka is truly great,\u201d he says. \u201cShe has helped my colleagues understand Western medicine. We want to see the HOPE project grow.\u201d HOPE chairman, Reverend Stefan Hippler, is optimistic that, if the project succeeds, it could inform HIV policy in other African nations as well. \u201cTraditional health practitioners are important role models and leaders in their communities,\u201d he says. \u201cThey are indispensable for all national efforts in the fight against HIV and AIDS.\u201d \n                     Traditional healers fight for recognition in South Africa's AIDS crisis \n                   \n                     Health minister ignites row over drugs for HIV mothers \n                   \n                     AIDS special \n                   \n                     HOPE Sangoma Project \n                   Reprints and Permissions"},
{"file_id": "443746a", "url": "https://www.nature.com/articles/443746a", "year": 2006, "authors": [{"name": "Henry Nicholls"}], "parsed_as_year": "2006_or_before", "body": "Darwin is the latest eminent scientist to get an online archive. How do these undertakings change our understanding of history, asks Henry Nicholls. When exactly did Charles Darwin first use the iconic and ideology-laden phrase \u201csurvival of the fittest\u201d? On page six of the first issue of the first volume of the first edition of  The Variation of Animals and Plants Under Domestication , published in 1868. The phrase only makes it into  On the Origin of Species  in that book's fifth edition, published the following year. If you are James Moore, or Janet Browne, or Adrian Desmond, or David Quammen, or any of the other people who have written biographies of Darwin and analyses of his thought, you probably already knew that. Now, though, you don't need years of scholarship to dig out as many such intriguing nuggets as you can imagine. With the launch on 19 October of Darwin Online 1 , answering this kind of question becomes a doddle. The service makes the complete works of Darwin \u2014 in the form of both scanned-in pages and searchable text \u2014 freely available to anyone with an Internet connection. It also contains a handful of his key unpublished manuscripts, such as the jottings he made during the voyage on  The Beagle . This kind of resource, allowing in-depth interrogation of a scientist's entire body of work, is becoming increasingly common. Its proponents believe that it will do more than just cut down on bike rides, train trips or even flights to visit physical archives; they expect it to inject new life into historical scholarship. The ambitious idea for Darwin Online came to John van Wyhe, a historian of science at the University of Cambridge, UK, back in 2002. Although there were plenty of Darwin's works on the web, he recalls, they were spread across many sites with no obvious editorial standards. \u201cIt was just utter chaos.\u201d Given the many other great men of science (for they are almost exclusively men) who boast digital archives of their work and correspondence, this was a gap that needed to be filled (see  'Best of the rest' ). \u201cIt just didn't seem right that other figures should have these rich databases and not Darwin,\u201d says van Wyhe. Van Wyhe aims to add every edition and translation of Darwin's published work to the website by 2009, the bicentenary of his birth and the 150th anniversary of the publication of  On the Origin . The ultimate plan is to scan in and transcribe a mountain of some 35,000 Darwin manuscripts dispersed around the world, including drafts of  On the Origin , descriptions of his experiments and even his domestic accounts. The aim is to host everything Darwin ever wrote except for his private letters, which the long-running Darwin Correspondence Project, also in Cambridge, is dealing with on its own website 2 . By the standards of scholarship in the humanities, as opposed to those of the well-equipped lab, these projects do not come cheap. Van Wyhe has a grant of \u00a3286,000 (US$532,000) from Britain's Arts and Humanities Research Council (AHRC), although he will need more to finish off the project as he wants it. The Newton Project, which aims to be the definitive, open-access repository for Isaac Newton's writing and correspondence, has received grants from the AHRC that total \u00a3900,000. This funding should be enough to transcribe, footnote and publish online his vast collection of theological writings (almost half of what he wrote) and more than half of his unpublished optical writings by 2010. As with Darwin Online, the long-term vision is to publish everything Newton ever wrote \u2014 although coding up the mathematics will be a particular challenge, notes Robert Iliffe, the historian from Imperial College London who acts as the project's editorial director. Iliffe is adamant that the venture offers serious value for money. In the past, thousands of pounds would be thrown at producing a print edition of Newton's work, with only a few institutions prepared to stump up the cash to buy a copy. \u201cThe online medium brings with it such a huge audience,\u201d he says. \n               Heavy hitters \n             That's certainly the experience at the Einstein Archives Online 3 . This site currently boasts more than 3,000 scanned-in pages from almost 1,000 of Einstein's scientific and non-scientific works. Even though he wrote almost exclusively in German, there is huge interest in these documents, says Diana Buchwald, director of the Einstein Papers Project at the California Institute of Technology in Pasadena. When the Einstein Archives Online went live back in 2003, there were nearly 250,000 unique user sessions in the first five days, she says. Darwin Online is ready for a similar surge of activity as both supporters and distorters of Darwin's ideas make use of the ability to search through the published material. There are several items that should be of particular interest to Darwin fans, notably the previously unpublished  aides-m\u00e9moires  he made as  The Beagle  explored the Galapagos Islands. The original notebook has been lost, probably stolen in 1983, but the University of Cambridge's library has a microfilm of it. It is also possible to inspect all his published illustrations; the visually impaired can download audio files of all the texts. The creationist faithful would do well to take a look, says van Wyhe. \u201cIf people feel so strongly about Darwin, they should actually take the time to read his own words rather than relying only on the interpretations of others.\u201d Even if this doesn't convert them to evolution by natural selection, it should expose the popular misconception that Darwin had an anti-Christian agenda, he says. \u201cThis was not what he was about,\u201d says van Wyhe. \u201cHe was simply a scientist trying to explain how the world works.\u201d The same cannot be said for the defiantly irreligious Francis Crick who, enraged by the decision of Churchill College, Cambridge, to build a chapel, wrote a letter to the college's namesake Winston enclosing \u00a310 towards the building of a brothel to go with it. Unfortunately, this letter is in the Churchill archive rather than the Crick archive now being catalogued at the Wellcome Library in London. As yet, only a sliver of the Crick archive is accessible online, by means of the website of the US National Library of Medicine. \u201cIt's a fantastic new tool not available to previous generations of researchers,\u201d says Matt Ridley, who has just published a biography of Crick 4 . \u201cI longed for even more correspondence to be there.\u201d \n               Contemporary conundrums \n             But Crick and other contemporary giants raise archival problems that Darwin and Newton avoid. \u201cIndexing becomes more difficult as the corpus gets larger,\u201d says Darwin Stapleton, executive director of the Rockefeller Archive Center in New York, whose father named him after the great nineteenth-century naturalist. The archive of a contemporary scientist such as Crick's colleague James Watson may contain more than a million items \u2014 including e-mails, spreadsheets, slide presentations, audio recordings and TV appearances, all of which present their own indexing challenges. \u201cWhat can be done with the Darwin collection cannot be done with a twentieth-century figure,\u201d says Stapleton. Contemporary history raises other challenges, most notably that of copyright. In many instances, it's simply not clear who owns it. \u201cYou may own the physical letter, but the person that wrote it owns the copyright,\u201d says Peter Hirtle, intellectual-property officer for Cornell University Library in Ithaca, New York. In an archive with many correspondences, \u201cyou have potentially hundreds and hundreds of copyright owners\u201d. Consequently, publishing outgoing and not incoming letters is the only practical option for many correspondence projects. But even if near-complete, searchable archives are to remain the privilege of the long dead, they still offer new scholarly possibilities. Last year, Albert-L\u00e1szl\u00f3 Barab\u00e1si, professor of physics at Harvard University's Dana-Faber Cancer Institute in Boston, and a colleague used data from the Darwin Correspondence Project and Einstein Papers Project to compare how long these men took to reply to incoming letters 5 . The ability to use the dates of documents to map a scientist's writing and correspondence patterns might give new insights into how he organized his life and work, says Barab\u00e1si. As van Wyhe will be able to tell something about who is reading which of Darwin's texts online, research into the researchers will be possible, too, through studies of the way the resource is used. Will everyone look for survival of the fittest, or will people dig into ever more intriguing byways? What grandeur may be found in this new view of a life? Reprints and Permissions"},
{"file_id": "443744a", "url": "https://www.nature.com/articles/443744a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "Representative Rush Holt is a rare thing in the US Congress \u2014 a bona fide scientist building a promising political career. Since his election for the 12th district of New Jersey \u2014 the one containing Princeton \u2014 eight years ago, this former physicist and son of a West Virginia senator has garnered several powerful committee slots. Holt has emerged as one of the Democratic Party's most prominent spokesmen on science, education and security. Colin Macilwain asked him about the life of a scientist on Capitol Hill, and what the mid-term elections could mean for science and education. \n               What difference would it make to science, or to scientists in America, if the Democrats took control of the House of Representatives? \n             The atmosphere in Washington is more politically partisan than I have seen in half a century, and it even affects things like science. I've never believed that science is completely removed from policy or politics. But many scientists would say they are appalled at the way a political game has been made of science, such as intelligent design in the schools, where both the president and some in Congress have said that both this and evolution should be taught. And climate change \u2014 until very recently it's been difficult to get anyone to acknowledge that there is climate change and that there is any connection with human activity. \n               Are scientific issues arising as issues in campaigns around the country? \n             Not as major issues, but in my district there is a kind of frustration that we've been unable to deal with energy problems \u2014 it might be high fuel prices, but somewhere in the voters' minds it is connected with a failure to find alternatives to fossil fuels, and a failure to listen to scientific analysis on climate change and that sort of thing. \n               Do the Democrats have a programme for science, technology and education \u2014 and, if so, what is it? \n             It may not be as well known or as well understood as we would wish. We do have a good message ['The Innovation Agenda'] released six or eight months ago. It calls for nationwide broadband, a greater investment in research and greatly increasing the number of trained science teachers in the schools. \n               But isn't it true that, historically, Republicans are likely to spend more money on research and development? \n             The president has acknowledged that the physical sciences have languished, but in the latest budget, not much has come through. So I'm not willing to elevate him to the hall of fame. \n               What about involving the public in decision-making? A lot of discussion happens in Europe but it doesn't seem to get much traction in the United States. \n             I think it is fair to say, and unfortunate to note, that the public is not driving the science agenda. I wish they were. In the United States we have found ourselves in a position where the public says, 'science is for the scientists, but not for me'. Not often do any non-science or non-engineering constituents come to me with science or technology on their list, and I imagine that's true for other members of Congress as well. \n               You've said that most people in Congress tend to view science as a special interest, albeit an intelligent one. Have you seen much change in how Congress views science? \n             The public's appreciation of science is no better, and maybe a little worse, than a decade ago. In official Washington, scientific subjects have become really politicized. There should be debate about the policy that is derived from science. But, historically, if science puts limits on the choices that are possible, the politicians would accept that. Now, by treating science as just another topic to be dealt with ideologically, or to be part of political trades, they will even ignore the laws of science. \n               You decided not to seek a berth on the science committee but to look to more general committees instead. Was that a good choice? \n             The greatest need here is for scientific expertise in those areas that are not obviously scientific. On funding for NASA or for Antarctic research, we get pretty good scientific advice. But on 'how do we get reliable elections' or 'what is the effect of good transportation planning', which are to most Americans not obviously scientific, we have the greatest need. \n               Do you find your fellow congressmen receptive to a bit of scientific knowledge? \n             Yes. People will listen to me on some subjects more than just an average colleague. Am I as influential as I'd like to be? No \u2014 but I work at it. \n               What is your proudest achievement in the Congress? \n             It has nothing to do with science, and it is not even easy to describe. But it is building a sense of respect for government, or to put it another way, beating back the cynicism about government, at least within my own district. \n                     US election: Showdown for Capitol Hill \n                   \n                     US Election 2004 special \n                   Reprints and Permissions"},
{"file_id": "443740a", "url": "https://www.nature.com/articles/443740a", "year": 2006, "authors": [{"name": "Geoff Brumfiel"}, {"name": "Meredith Wadman"}, {"name": "Emma Marris"}, {"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "Can science influence politics in the forthcoming US elections?  Nature  investigates how Democrats and Republicans are striving to win the hearts of voters. Heather Wilson has something she wants the voters of New Mexico's first congressional district to know about her: unlike President George W. Bush, she supports embryonic stem-cell research. In a local television advertisement last month, Wilson told viewers in Albuquerque and its environs: \u201cThe president vetoed the stem-cell bill, and I voted to override his veto because it was the right thing to do.\u201d It is not that surprising for a candidate to say an unpopular president is wrong, or that a popular biomedical cause is right. It is rather more surprising when the candidate and the president are members of the same party. But Wilson, the only woman in Congress who has served in the military, is a moderate Republican in a close fight to keep her seat. The stem-cell issue is one that she thinks may help her in the struggle to a fifth term in the House of Representatives, despite the fact that in other parts of the country some of her fellow Republicans are making opposition to the research a strong part of their campaign. Most of the time, American voters couldn't care less about science. \u201cIf you look at what people are hot and bothered about, it's health care, Iraq, taxes, education, things of that sort,\u201d says Daniel Greenberg, a science-policy expert based in Washington DC. \u201cScience and technology \u2014 they don't know anything about it.\u201d (See 'Q&A',  page 744 .) As a result, there's not much room for science in the typical US campaign \u2014 including the upcoming 7 November mid-term elections, which will decide who holds all 435 seats in the House of Representatives; one-third of the Senate's 100 seats; and 36 governorships. \u201cScience plays a very little role because facts play a very little role,\u201d says Tony Massaro of the Washington-based League of Conservation Voters. \u201cIssues, such as they are, are indicators for the values of the candidates.\u201d Some liberal groups think that the Bush administration's record on science can be seen as a reflection of its values. They hope to exploit this, painting a picture of an administration opposed to objective truth and intellectual progress. Over the past six years, activists have assembled a litany of issues where, they say, the Bush administration has either ignored scientific evidence or sought to manipulate it \u2014 from delayed decisions on the Plan B emergency contraceptive to altered documents on global warming. This 'Republican war on science', as it was called in the title of a 2005 book by journalist Chris Mooney, has proved a powerful rallying point for scientists disillusioned with the current administration. And there are many. But despite the fervour of some of its devotees, there is little evidence that this radical thesis is having any more effect in this race than similar ideas had in the 2004 presidential election. Back then, a group of respected scientists, including two dozen Nobel laureates, publicly accused Bush of \u201cmisrepresenting and suppressing scientific knowledge\u201d (see  Nature  427 , 663; 2004 ) and went on to support John Kerry's presidential bid. This year's race has seen the formation of a political advocacy group called Scientists and Engineers for America, which includes many of the same researchers. The group says it will work to raise public awareness of perceived abuses of science, and hopes to persuade voters to elect like-minded candidates. But the way the organization is set up constrains it from endorsing any specific candidate in any specific election. And despite charges that it could be just a Democratic front, its leaders insist the group is nonpartisan. \u201cThere have been very strong advocates for science from both parties,\u201d says founding member Peter Agre, a Nobel-prizewinning chemist and vice-chancellor for science and technology at Duke University Medical Center in Durham, North Carolina. \u201cThis is not a Republican issue or a Democratic issue. It's an American issue.\u201d John Marburger, President Bush's science adviser, agrees \u2014 and argues that the Bush administration has done a fine job of advancing various scientific initiatives. \u201cScience has wide bipartisan support in this country,\u201d he says. \u201cScientists benefit from that.\u201d He points to the 'competitiveness initiative', meant to keep the United States at the forefront of science and technology, which is currently winding its way through the Republican-led Congress. If brought into effect, it would mean an 8% increase for the National Science Foundation's budget and a 15% boost to funds for the Department of Energy's Office of Science in 2007 (see  Nature   439 , 644\u2013645; 2006). But if science as an overriding issue is hard pushed to gain traction, specific issues \u2014 in particular, stem cells and energy \u2014 could play an important role in some tight races. In a neck-and-neck national election, with the Democrats enjoying a lead in public opinion but requiring 15 seats to take control of the House and 6 seats to take control of the Senate, it's possible that the tactical use of these issues in specific races could make a difference. \n               Culture shock \n             Meanwhile, in various states, scientific issues are turning up on important ballot initiatives. In Missouri, a high-profile measure would amend the state constitution to protect stem-cell research (see  'Running on science' ). In California, leading researchers are pushing for a clean-energy ballot initiative that would put more than $1 billion towards research on alternative energy sources (see  'Good times for green energy' ). These initiatives do not necessarily lend themselves to the big-picture analysis of a 'war on science', but they should reveal how Americans are thinking about various ways in which research can have a practical impact on their lives. And that could, perhaps, influence candidates as they begin strategizing for the presidential race of 2008. More so than any other line of research, stem cells have brought candidates from both parties to the front lines of science. Wilson is far from the only Republican now distancing herself from Bush on the issue. \u201cYou won't see any Republicans in competitive races touting the fact that they supported Bush on human embryonic stem-cell research,\u201d says Sean Tipton, president of the Coalition for the Advancement of Medical Research, a Washington-based advocacy group that supports the research. \u201cA lot of candidates recognize that it plays into real weaknesses for Bush and his party.\u201d In several governors' races, Republican candidates are reaching the same conclusion, sometimes moving to support generous state funding for stem-cell research. In Maryland, governor Robert Ehrlich stood quietly by in April 2005 as a bill providing $25 million in research funds for such research died in the face of conservative opposition in the state legislature. But this January, he came out vocally in support of the work, and in April he signed into law a bill providing $15 million in state funding. In Massachusetts, lieutenant governor Kerry Healey (Republican) has been at pains to show her support for stem-cell research in her bid to become governor. In August, she broke publicly with the current governor when his administration announced a restrictive interpretation of a 2005 state law governing stem-cell research. A spokesman for Healey called the rules a \u201cmistake\u201d that \u201ccould have a chilling effect on those individuals at the forefront of this emerging field\u201d. In some states, the issue is economic as well as political. Jim Doyle, the incumbent Democratic governor of Wisconsin, has pointed at his challenger's congressional vote against expanded federal funding for stem-cell work. Doyle has also emphasized his own role in bolstering research in the state where human embryonic stem cells were first isolated. That's a canny strategy in a state that is \u201cterrified that it's losing jobs and people, and needs biotechnology\u201d, says Arthur Caplan, director of bioethics at the University of Pennsylvania, who has advised several Democratic and Republican campaigns on stem cells. \u201cIt's an economic argument dressed up in a stem-cell costume.\u201d In other states, though, Republicans are using opposition to stem-cell research as a traditional pro-life values issue. In the Missouri senate race (see  'Running on science' ) Republican incumbent Jim Talent hopes that opposition to a stem-cell measure on the state ballot will increase turnout among the conservative voters he needs. Politicians know that stem-cell issues appeal to many voters. So does the promise of 'energy independence', a catch-phrase that promises no more reliance on foreign oil. Voters have been put off by the ongoing war in Iraq and unrest in the Middle East, as well as record-high oil and gas prices this summer, although these have eased somewhat in recent months. \u201cUsually, we are fighting to get issues up in the forefront,\u201d says Massaro. \u201cThis year, everyone is thinking and talking about energy.\u201d Many gubernatorial candidates, including Wisconsin's Doyle and Ted Kulongoski, Democratic governor of Oregon, are advancing some version of the '25 by 25' pledge \u2014 the broad-based push to produce 25% of the country's energy from renewable sources by 2025. The issue has come into play in closely fought Senate races as well \u2014 and again on both sides of the partisan divide. In Washington state, Democratic senator Maria Cantwell is posing with wind turbines even as her Republican opponent proclaims his support for heavy investment in alternative energy. In Tennessee, Democratic candidate Harold Ford runs adverts wherein he strides across fields of soya beans grown for biofuel. In New Jersey, Republican challenger Tom Kean says that, \u201cunlike President Bush\u201d, he doesn't think America can \u201cdrill its way to energy independence\u201d. The interest in energy issues runs deep. Earlier this year, the liberal citizens' group MoveOn.org staged more than 1,000 house parties, asking attendees to name the issues they thought the group should press hard on for the elections. \u201cThere were just two issues that came up at every one of those house parties,\u201d says Eli Pariser, executive director of MoveOn's political action committee. \u201cOne was health care and one was energy.\u201d For Pariser, the issue is about more than oil prices and geopolitics: \u201cThere is this sense of a grand scientific exploration in the style of the campaign to put a man on the Moon. People are hungry right now to be asked to be part of a big project.\u201d But some routes towards energy independence involve extracting non-renewable energy sources \u2014 such as drilling in the Arctic National Wildlife Refuge, a plan pushed heavily by Ted Stevens, a Republican senator from Alaska. The drive for energy independence shouldn't eclipse the message of preparing for climate change, argues Alden Meyer, director of strategy and policy for the Union of Concerned Scientists in Washington DC. \u201cUnless you work in the global-warming message as well, there are some proposals \u2014 such as turning coal into liquid fuel \u2014 that could wreak havoc with the environment,\u201d he says. \u201cYou have to bring in the longer-term fossil-fuel dependence as well.\u201d Climate change offers less political mileage than energy independence. That may reflect the current American view: in June, a poll conducted on a number of issues by the Pew Research Center for the People and the Press found that, although 64% of respondents thought energy policy was \u201cvery important\u201d to them, only 44% said the same of global warming. Nevertheless, in tight governors' races in Rhode Island and Massachusetts, candidates have divergent stances on the Regional Greenhouse Gas Initiative, a seven-state scheme for limiting greenhouse-gas emissions. \n               Capitol gains \n             Back in Washington DC, where several powerful representatives have notoriously sceptical views on climate change, the elections could significantly shift the balance of who gets listened to the most. If the Democrats take back either house of Congress, the chairmanship of all committees will switch from Republicans to Democrats. And chairmen and chairwomen have the power to call hearings on topics of particular interest \u2014 or to call witnesses such as novelist Michael Crichton to criticize the current state of climate-change research, as happened last year in the Senate's Committee on Environment and Public Works. If Republicans lose control of the House, accusations of scientific politicization could gain a higher profile. \u201cI think there would be more investigations if the House changes,\u201d says Kurt Gottfried, president of the Union of Concerned Scientists in Cambridge, Massachusetts. For instance, California Representative Henry Waxman \u2014 a Democrat who has been active in pursuing conflict-of-interest issues at the National Institutes of Health and other agencies \u2014 is in line to gain the chairmanship of the House Committee on Government Reform. Representative Bart Gordon (Democrat, Tennessee), meanwhile, is in line to gain control of the House Committee on Science if the House switches majority. As such, he might call hearings on the accusations of scientific censorship at NASA and the National Oceanic and Atmospheric Administration, says Joel Widder, a policy adviser at the lobby group Lewis-Burke Associates in Washington DC. \u201cThe politicization of science and politics affecting scientific decision-making would clearly be issues that he would explore,\u201d Widder says. If the Republicans maintain the majority, the same committee might be headed by former Democrat Ralph Hall of Texas, global-warming sceptic Dana Rohrabacher of California or physicist Vernon Ehlers of Michigan. Which party wins may also influence how science budgets are distributed among agencies and across disciplines, as Congress is in charge of doling out money for scientific research. But the total pot of money for science isn't likely to grow, as the United States continues to struggle to pay for the war in Iraq and unexpected expenses such as Hurricane Katrina, on top of a growing deficit. \u201cIt's not like the Democrats are going to open the treasury and fix all the budget problems that all the science agencies are screaming about,\u201d says Widder. \u201cI think that the budget environment is likely to be so constrained that it doesn't matter who's in charge.\u201d No matter what happens on 7 November, the face of US science is likely to change. And on 8 November, campaigners from both parties will be picking themselves up, preparing for the new Congress to convene in January \u2014 and realizing it's never too early to start planning for 2008. \n                 See Editorial, page 724. \n               \n                     Q&A \n                   \n                     A long week in stem-cell politics... \n                   \n                     Election fever inflames the US stem-cell debate \n                   \n                     Alternative energy plan criticized \n                   \n                     Governors take the initiative over US carbon dioxide emissions \n                   \n                     US energy bill pushes research but fails to cut consumption \n                   \n                     Senate hearings strengthen calls for US action over climate \n                   \n                     Science shares the limelight as election battle enters final phase \n                   \n                     Scientists slam Bush record \n                   \n                     US Election 2004 special \n                   \n                     UCS petition \n                   \n                     Scientists and Engineers for America \n                   \n                     25X'25: America's Energy Future \n                   \n                     Pew Center for the People and the Press poll \n                   \n                     League of Conservation Voters \n                   \n                     Patricia Madrid's campaign \n                   \n                     Heather Wilson's campaign \n                   \n                     Claire McCaskill campaign website \n                   \n                     Jim Talent campaign website \n                   \n                     Stem cell polls \n                   \n                     California Proposition 87 \n                   \n                     Yes on Proposition 87 \n                   \n                     No on Proposition 87 \n                   \n                     Steve Bing profile \n                   \n                     Renewable and Appropriate Energy Laboratory \n                   \n                     Nathan Lewis website \n                   Reprints and Permissions"},
{"file_id": "443898a", "url": "https://www.nature.com/articles/443898a", "year": 2006, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "The cheerful leaves of the poinsettia could be hiding an unwelcome visitor this festive season. Rex Dalton goes in search of the whitefly, a potentially devastating pest. For many Americans, the festive season is hardly seasonal without a centrepiece of poinsettias. But for some entomologists the annual flood of red foliage is not such a welcome sight. When Timothy Dennehy lifts up a potted plant in a store or a nursery to inspect it \u2014 as he will be doing more and more over the next couple of months \u2014 he's not admiring the shape or the colour with an eye to how it might look sitting in his home. He's looking for little greenish-white harbingers of agricultural chaos. Every autumn, Dennehy, an entomologist at the University of Arizona, Tucson, hits the state's flower stalls searching for whitefly ( Bemisia tabaci ) on the poinsettias shipped in for the festive holidays. In December 2004, his horticultural gumshoe work paid off in a Tucson market with the first US identification of the whitefly variant in question \u2014 the pesticide-resistant Q-biotype 1 . The following year, the Q-biotype was found in stores across Arizona, spurring a nationwide survey that found the superfly in 22 states. In the 1990s, the B-biotype whitefly swept through North American crops, inflicting more than a billion dollars' worth of damage on farmers in the United States and Mexico; it had hitch-hiked from Israel to Florida to California, and from there it seems likely to have been spread nationwide via the imported poinsettias. Cotton crops were devastated, melons withered on stunted vines, lettuces wilted. US farmers scrambled for scientific assistance, successfully beating the pest with a new class of insect-growth regulators \u2014 such as buprofezin and pyriproxyfen, which were rushed through the approval process \u2014 and other pest management measures. Now the spreading of whitefly by poinsettia is at risk of repeating itself in an even more devastating way. The Q-biotype, originally observed in Spain in 1997 (ref.  2 ), \u201cis resistant to every pesticide we've tested\u201d, says Dennehy, who co-chairs a scientific panel on the pest convened by the US Department of Agriculture (USDA) 3 . So far, the superfly has been found only in retail shops or nurseries. But the fear that it will one day find its way into the fields is growing. \u201cIt scares me to death; it is one pest that could completely bury us,\u201d says Larry Antilla, an entomologist with the Arizona Cotton Research and Protection Council in Phoenix. \n               Festive pests \n             America is not the only country concerned about whitefly. Some scientists rank it as one of the world's most destructive pests to crops. Robert Gilbertson, a plant pathologist from the University of California, Davis, says damage caused by whitefly makes it \u201cthe worst [agricultural pest] problem in regions of Africa, Asia and South America\u201d. The flies' increased resistance to pesticides and indifference to drought \u2014 many actually prefer things hot and dry \u2014 make them a grave threat to crops in parts of the developing world. Elsewhere, international trade has put tomatoes in Japan, cassava in Africa and soya beans in Australia at increased risk. In the worst infestations, the flies can form visible clouds, coating windscreens and clogging the mouths and nostrils of field workers. Not only does the fly kill flowers, vegetables and cotton, it spreads viruses that are equally deadly in plants (see  'At the sharp end' ). The high doses of pesticides used in attempts to control them can do more collateral damage to the insects that feed on whitefly \u2014 such as ladybirds \u2014 than to the whitefly themselves. The best way for a country to fight the whitefly is to stop them entering in the first place. But with today's one-world agriculture, there are enormous economic and political pressures that can hamper effective inspection and regulation. In the United States, for instance, there has been a heated behind-the-scenes battle over the invasion of the Q-biotype. The cotton industry, fearing for its fields, has fought for more aggressive control methods; the ornamental flower trade, dependent on imports that could be quarantined at borders, has opposed them. The former is a $6-billion industry, the latter is worth $19 billion. After Dennehy and his colleagues found the Q-biotype in Arizona \u2014 a state where the B-biotype cost cotton growers $180 million in the first half of the 1990s \u2014 representatives of the flower industry fought against plans for a nationwide survey to check for the new variant. Dennehy, who receives some funding from cotton growers, was displeased. As he wrote to the task force on the subject at the USDA: \u201cHolding off on surveys is tantamount to promoting ignorance over enlightenment regarding the biotype issue.\u201d Meanwhile, Joe Chamberlin, a North Carolina-based entomologist representing ornamental flower growers, told the USDA task-force members that his clients were concerned that the survey's \u201cprimary purpose is not to help manage the whitefly, but instead to justify imposition of regulatory action and to generate research dollars or publications for university scientists\u201d. As in the B-biotype outbreak, there has been pressure from poinsettia growers to play down their product's links to the fly. Pressure from one flower grower led to a researcher at the University of California, Riverside, having to refer to \u201csilverleaf whitefly\u201d rather than \u201cpoinsettia whitefly\u201d in public statements. \n               On the fly \n             Now a nationwide survey has been established, thanks in part to the diplomatic skills of Osama El-Lissy, an entomologist at the USDA's plant-inspection service in Riverdale, Maryland. \u201cEl-Lissy masterfully cajoled many key people,\u201d says Dennehy. For his part, El-Lissy stresses that the different affected industries \u2014 cotton, ornamental flowers and vegetables \u2014 \u201cmust work together to combat this pest.\u201d As a cautionary tale, he points to the spread of the B-biotype, introduced into Florida in 1986 (ref.  4 ). \u201cBack then, it was a disaster,\u201d says El-Lissy, who was conducting research in Arizona at the time. \u201cWhy? Because the industries didn't work together.\u201d The Arizona team's investigations suggest that the Q-biotype came into the country not through Florida but through California. Poinsettia cuttings taken in central America, mostly in Guatemala, Honduras and southern Mexico, are shipped north to be grown up into plants. The Q-biotype-infected poinsettias found in Arizona were traced to a grower near Salinas, California, then south to Ecke Ranch in Encinitas, California, which handles the cuttings for a massive 70% of the United States' seasonal poinsettias. A family-owned operation run from an old, plantation-style California home, Ecke Ranch is spread over about two dozen hectares in foothills just inland from the Pacific, largely surrounded by newly built homes for San Diego commuters. To Paul Ecke, who has run the farm in recent years, worries about the Q-biotype are \u201coverblown\u201d. \u201cThese guys in Arizona went off the deep end, paranoid because of the cotton growers,\u201d says Ecke. To undertake a nationwide survey, Ecke says, \u201cwas a paranoid reaction that wasn't very effective\u201d. Ecke adds that integrated pest-management controls permitted his firm to ship poinsettia cuttings last spring into the United States \u201cwith zero whitefly problems\u201d, and that the clean-up operation after the Q-biotype was first reported at his Encinitas headquarters was successful. Although his company does not publish results of its internal tests in California or Central America, Ecke says he believes things are now under control: \u201cThe Q-biotype whitefly is a non-existent problem.\u201d Farmers must hope he is right, as there are no restrictions on Ecke \u2014 or any plant grower from California to Florida \u2014 continuing to import plants from whitefly-affected regions, even those with the Q-biotype variant. The USDA takes no action to block the import or movement of plants infected with  B. tabaci  because it has determined that the insect meets the agency's standard for a pest that can't be regulated \u2014 which is to say, one that can't be contained or eradicated. Other nations have taken a more proactive stance. In Australia, the authorities scrupulously watched for the B-biotype after it hit the United States, although it still sneaked into the country in 1994. Earlier this year, the European Union augmented its controls on whiteflies, which have been in effect since 1992. But some Mediterranean countries still have infestations. US flower importers fear they will lose their crops if attempts are made to regulate whiteflies because of the Q-biotype. Because the genetic analysis needed to distinguish the Q- from the B-biotype can take weeks 5  \u2014 the two can't be distinguished visually, even under a microscope \u2014 individually packaged poinsettia cuttings being imported would die, because they need water and nourishment that couldn't readily be provided for quarantined products. \n               Under the strain \n             Meanwhile, the B-biotype still presents problems of its own. Lance Osborne, an entomologist at the University of Florida's research facility in Apopka and Dennehy's co-chair on the USDA science panel, points out that it is widespread in crops in the southeast, from cotton in Georgia to ornamental flowers and vegetables in Florida. It takes just two or three B-biotype whiteflies to do their damage to a plant, sucking out sap and leaving leaves with silver-coloured sooty mould; the Q-biotype seems to require hundreds of flies to do equivalent damage, Osborne says. Osborne's worry is that farmers could promote resistance in the B-biotype by using more pesticides \u2014 as they may well do if the Q-biotype gets into fields. \u201cI'm very concerned the B-biotype will become pesticide resistant,\u201d he says. \u201cThe B-biotype fly lays more eggs than the Q; it could be more of a superfly than the Q-biotype.\u201d To avoid over-reliance on pesticides, Osborne and other scientists champion integrated pest-management programmes. Along with distinct applications of the pesticides, these also involve denying the flies food by not planting some crops in hot weather, and devoting significant amounts of land to crops the flies won't infect \u2014 they love melons, peppers and leafy vegetables, but not alfalfa, wheat or corn. But these courses can cause economic strain. In the Mexican state of Sonora, where the Yaqui River Valley has produced bumper grain crops, farmers have largely halted the planting of soya beans that mature in late summer. With fields fallow, revenue and jobs disappear. Without summer crops with labour-intensive harvests, many of the poorest Mexicans lost their jobs, transforming rural cultures as people moved to cities looking for work. In lower Sonora alone, agricultural economists estimate the losses from not planting their 70,000 hectares of summer soya beans at about $75 million. Now entomologists are looking at the possible responses to the pesticide-resistant Q-biotype, such as management schemes that make use of new members of the neonicotinoid family of pesticides. Far better, though, if the whitefly could indeed be kept out in the first place, however pretty its Trojan horses. \n                     Transgenic cotton drives insect boom \n                   \n                     Harnessing nature in Africa \n                   \n                     Pest resistance feared as farmers flout rules \n                   \n                     Leaf me alone \n                   \n                     Plant Defence insight \n                   \n                     Whitefly USA \n                   \n                     Whitefly Europe \n                   \n                     Q-Biotype page \n                   \n                     Australian whitefly/soybean report \n                   Reprints and Permissions"},
{"file_id": "443901a", "url": "https://www.nature.com/articles/443901a", "year": 2006, "authors": [{"name": "Nick Lane"}], "parsed_as_year": "2006_or_before", "body": "There's a fight going on inside all our cells for each breath of air. Nick Lane sheds therapeutic light on the implications for cancer and degenerative diseases. Seventy-five years ago, Otto Warburg's star was at its zenith. The pioneering German biochemist delivered his Nobel address in December 1931. He described the ingenious experiments by which he had unmasked the enzyme responsible for the critical step of cell respiration, the process that turns the energy in chemical compounds into energy the cell can use. His work on respiration in the early 1930s nearly earned him a second Nobel, ultimately denied him by Hitler. Then his star began sinking. His ideas on the importance of cell respiration in cancer led many to dismiss him as a crank. And the rise of molecular genetics in the 1960s put such ideas into a far distant orbit. But now, Warburg's star is rising again. A new generation of researchers is returning to his ideas about respiration in cancer cells. Recent findings suggest that the enzyme he identified, cytochrome oxidase, is a key player in a new understanding of how the cell's energy metabolism affects health and disease. And surprisingly they show that light has a profound effect on how the enzyme works \u2014 and could even be used to treat degenerative disease. To extract energy from molecules, the cell first breaks down glucose into simpler molecules via a process called glycolysis. It then feeds these molecules into energy-producing structures called mitochondria, which strip electrons from them to produce energy with the help of oxygen. As Warburg showed, cytochrome oxidase governs the last reaction in this process. Perhaps the most surprising aspect of the renaissance of Warburg's ideas is that the methods he used to make this discovery matter again. They exploit two chemical quirks: carbon monoxide (CO) can block respiration by binding to cytochrome oxidase in place of oxygen; and a flash of light can displace it, freeing up the site for oxygen to bind again. \n               Self control \n             By measuring oxygen consumption at different wavelengths of light, Warburg worked out that the enzyme belonged to a group of proteins that include haemoglobin and chlorophyll. But for Warburg, the binding of CO to the enzyme was just an oddity he could put to good use. He had no inkling that biology might use the same trick. Yet over the past decade, researchers have come to appreciate that cells often use CO, and to an even greater extent NO (nitric oxide), to block respiration. Not only that, but light has striking counter-effects on cytochrome oxidase. And all these suitors to the enzyme turn out to be critical to our understanding not just of cancer, but practically all degenerative diseases. Nitric oxide is emitted by nerve endings and can act on an enzyme called guanylate cyclase to relax blood vessels \u2014 the impotence drug Viagra manipulates this system. For a long time, scientists thought that guanylate cyclase was NO's only target. But in the mid-1990s, they found that the molecule can also bind to cytochrome oxidase and hinder respiration 1 . What's more, it could do so at levels found in the body's tissues. The finding that the body could poison one of its own enzymes was initially shrugged off as an imperfection \u2014 an example of how evolution cobbles organisms together with no forethought. But a few years later, several groups reported that mitochondria harboured an enzyme that synthesizes NO (ref.  2 ). Why would cells go out of their way to cook up NO right next to the respiratory enzymes? According to cell biologist Salvador Moncada of University College London, evolution really has crafted cytochrome oxidase to bind not only oxygen but also NO. \u201cOne effect of slowing respiration in some locations is to divert oxygen elsewhere in cells and tissues,\u201d he says. This prevents oxygen levels sinking dangerously low. Moncada, for example, has shown that NO blocks respiration in the cells lining blood vessels and that this helps to transfer oxygen into smooth muscle cells in these vessels. Fireflies use a similar trick to flash light (see  'Flashing the gauntlet' ). But the consequences of blocking respiration go beyond diverting oxygen. Respiration, Moncada argues, is not just about generating energy, it's about generating feedback that allows a cell to monitor and respond to its environment. Blocking respiration generates chemical signals, in the form of highly reactive molecules called free radicals. These are normally associated with cell damage, but now it seems they can interact with the proteins that control gene activity and adapt cells to changing circumstances. \u201cFree radicals had a bad reputation,\u201d says pathologist Victor Darley-Usmar, of the University of Alabama, Birmingham. \u201cBut we now see them as signals.\u201d In the past few years, researchers have compiled a list of these proteins, or transcription factors, the activity of which depends , at least in part, on interactions with free radicals 3 . These include many proteins known to be linked to cellular life and death, such as P53, a protein that kills cells if they show signs of turning cancerous. \u201cThe question is,\u201d says Darley-Usmar, \u201chow is the whole system controlled?\u201d The answer lies in the tens of thousands of protein machines that line each mitochondrion. Organized into chains, they pass electrons extracted from broken-down glucose on to each other. The end-point of the chain is cytochrome oxidase, which catalyses the final step of respiration, in which electrons and protons are transferred onto oxygen to form water. Energy released by this process is used to pump protons over a membrane, creating an electrical charge. The cell can then use this charge to power an enzyme that makes ATP, a molecule that fuels chemical reactions in the cell. The cell can suppress the number of free radicals coming from these respiratory chains by allowing protons to leak back though the membrane without driving the synthesis of ATP, a process known as uncoupling 4 . \n               Undercover radicals \n             But if uncoupling doesn't bring free-radical leak under control, the signal may be amplified. \u201cThere's a cross-talk, known as the retrograde response, between the mitochondria and genes in the nucleus, which we're only just beginning to explore,\u201d says cell biologist Keshav Singh of the Roswell Park Cancer Institute in Buffalo, New York, who helped uncover this mechanism. \u201cIf we can modulate this signalling, we might be able to influence the life or death of cells in many pathologies, even ageing.\u201d In cancer cells, something goes awry with this pathway. Free radicals can trigger two steps in carcinogenesis, chromosomal instability and metastasis 5 , by hindering DNA repair and promoting the activity of genes involved in cancer spread. \u201cWe've found that damaged mitochondria, which overproduce free radicals, undermine the integrity of the nuclear genome in otherwise normal cells,\u201d says Singh. Ultimately, however, the most intractable problem with cancer cells is their failure to undergo apoptosis (cell suicide). There are echoes of Warburg here, for he argued that cancer cells revert to a primitive type that has little or no need for oxygen or mitochondria, depending instead on anaerobic forms of energy generation such as glycolysis. Whether cells really 'revert' is a moot point, but Moncada has shown that cells' ability to switch to glycolysis is critical to their survival 6 . Moncada's findings suggest that cells that can do without mitochondria \u2014 many stem cells, for example, which have been implicated in cancer \u2014 can have their resistance to apoptosis stiffened by NO binding to cytochrome oxidase, making cancer more likely. At the same time, cells that depend on mitochondria for energy, such as neurones, may be pushed to apoptosis by NO binding, making degenerative disease more likely. Any solution to excessive NO binding might lower the risk of both cancer and degenerative diseases, as it would make apoptosis more likely in cancer cells and less likely in normal cells. And the second essential feature of Warburg's experiments \u2014 light \u2014 might do just that. Light has long been known to promote wound healing, but the detailed molecular mechanisms have only recently been studied. Light's effects are more than skin deep: at long wavelengths, in the near infrared (NIR) spectrum, photons may penetrate several centimetres into the body. And according to photobiologist Tiina Karu, at the Russian Academy of Sciences in Moscow, NIR rays in exactly this range modulate cell respiration and the signals it generates. \n               Light works \n             Karu's group has explored a number of changes taking place in cultured cells in response to NIR rays. The immediate effect is an energy 'buzz', in which ATP levels are stoked up and the electrical charge across the mitochondrial membrane is strengthened. A few hours later, the activity of as many as 110 genes shifts in concert 7 . These genes orchestrate a prolonged rise in mitochondrial energy production, as well as stress resistance. They also prompt cells to cling more strongly to their surroundings, an important factor in wound healing 8 . Exactly how NIR light interacts with the enzyme to bring about these changes is unclear, and difficult to measure. One idea is that phototherapy might work by dissociating NO from the enzyme, so reversing the signalling consequences of excessive NO binding. \u201cWe have shown that light can indeed reverse the inhibition caused by NO binding to cytochrome oxidase, both in isolated mitochondria and in whole cells,\u201d says biochemist Guy Brown, at the University of Cambridge, UK. \u201cAnd what's more, we found that light can protect cells against NO-induced cell death.\u201d He has a reservation, however. These experiments used light in the visible spectrum, with wavelengths from 600 to 630 nm. Although Brown acknowledges that cytochrome oxidase absorbs NIR photons from 700 to 900 nm, he points out that the absorption takes place in part of the enzyme not involved in NO binding. NIR also seems to have effects on cytochrome oxidase in conditions where NO is unlikely to be present. Presumably then, says Brown, NIR must also have a direct effect on the enzyme. Regardless of the exact mechanism, the effect is to relieve a blockade of the enzyme, whether by NO or any other molecules. According to toxicologist Janis Eells, at the University of Wisconsin, Milwaukee, such relief lowers the likelihood of apoptosis in many conditions. Eells and her colleagues found that NIR phototherapy counters methanol poisoning, which injures the retina and optic nerve, often causing blindness 9 . The toxic metabolite is formic acid, which inhibits cytochrome oxidase. \u201cIn a rat model, NIR phototherapy is able to restore virtually normal retinal function, at least as judged by the electroretinogram,\u201d says Eells. And neurobiologist Margaret Wong-Riley and her colleagues at the Medical College of Wisconsin in Milwaukee have shown that NIR phototherapy can also oppose the effects of cyanide on cell cultures 10 . Cyanide poisons by binding to cytochrome oxidase. Wong-Riley's team showed that phototherapy could halve the rate of apoptosis in cultured neurones, even when given before cyanide treatment. But can NIR phototherapy relieve not just acute toxicity, but more chronic inflammatory conditions? The signs augur well. Eells and her colleagues have shown that NIR phototherapy could cut the rate of apoptosis by 50% in a rat model of retinitis pigmentosa, in which photoreceptors die by apoptosis during postnatal development causing retinal degeneration and blindness 11 . The use of NIR phototherapy on other conditions in which cells die by apoptosis, including acute ischaemic stroke and myocardial infarction, has also shown promise in animal models, and should soon enter clinical trials. So will it work? \u201cThe trouble is that if you enthuse about light as a therapy, clinicians and even researchers tend to back away,\u201d says Eells. \u201cPerhaps if the physical interactions of photons with cytochrome oxidase and NO are better known, people will begin to appreciate the huge potential benefits of this technique.\u201d \n                     Microbiology: Batteries not includedWhat can't bacteria do? \n                   \n                     Mitochondrial disease: Powerhouse of disease \n                   \n                     A finger on the pulse \n                   Reprints and Permissions"},
{"file_id": "443386a", "url": "https://www.nature.com/articles/443386a", "year": 2006, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "US astronomers are renowned for getting together and choosing their projects as a group. But just as these tactics are catching on in other fields, some say the astronomy process is grinding to a halt. Geoff Brumfiel investigates. Some of us are impulse buyers, some chaotic shoppers, and some of us head out with a carefully thought through list of what we want. Collectively, US astronomers fit in that last category. Since the 1960s, a who's who of the field has gathered together every ten years to draw up a table of what instruments they want to buy \u2014 or rather, to have bought for them. Their reports, known as decadal surveys, cram radio dishes, ground-based telescopes and ambitious satellite observatories into a tidy, page-long, prioritized list. Compiled through the US National Academy of Sciences, these surveys can do much to set the field's agenda, especially when they prioritize a ground-breaking instrument. The 1964 recommendation to build the 27-dish array that went on to become the Very Large Array in New Mexico led to an explosion in radio astronomy. A 1972 recommendation for research towards the goal of a large space telescope did not, in itself, bring about the birth of the Hubble Space Telescope, but it helped show the community's support during the early years of the project. Cosmic-ray facilities in the 1980s and infrared astronomy in the 1990s both received fruitful bursts of interest and investment thanks in part to earlier decadal surveys. The astronomers' orderly and concise sorting of their needs has won them the praise of Washington politicians and the imitation of their peers in other disciplines. In recent years, space physics, high-energy physics, atomic and molecular physics, and planetary science have all compiled their own decadal surveys in the hope of galvanizing researchers and winning funding from agencies. Even Earth scientists and botanists have taken a stab at the process. Meanwhile, across the Atlantic, European astronomers are trying to develop a Europe-wide set of priorities (see  'Continental drift' ). \n               Cloudy skies \n             But at precisely the time others are rushing to emulate them, the astronomers have run into trouble. It is now five years since the publication of the most recent report, and not one of the instruments it recommended is yet operational. That is not unusual: none of the 1991 report's recommendations was up and running in 1996. And if all goes well, the first of 2001's list \u2014 the Gamma-ray Large Area Space Telescope \u2014 will launch on 7 October. But in stark contrast with previous surveys, many of 2001's other recommendations have yet to leave the starting blocks and are a long way from being launched or built (see  'Where are they now?' ). Only one other space mission from the survey is likely to fly this decade. The really large space missions are all deferred to the 2010s, as are most of the ground-based projects. As telescopes and satellites languish in the queue, the designs to which the 2001 report gave its blessing run an increasing risk of becoming technologically outdated. And the survey's price estimates are proving to be a long way wide of the mark. The cost of its chief priority, a successor to Hubble known as the James Webb Space Telescope (JWST), has gone from $1 billion to $4.5 billion (see  Nature   440 , 140\u2013143; 2006); and the Constellation-X quartet of X-ray telescopes has risen from $800 million to $1.6 billion. \u201cI think the last decadal survey for astronomy was a failure,\u201d says Webster Cash, an astronomer at the University of Colorado, Boulder. \u201cWe basically just have the JWST seven years from now, and until then we're surviving on existing missions. Space astronomy is grinding to a halt.\u201d \n               Get back on track \n             It is against this background of minimal success that the community will begin work on the next decadal review this winter. And as it discusses what should go into the report, it will also be assessing how the surveys can be made to work as they once did. Michael Turner, a cosmologist at the University of Chicago, was until recently head of the National Science Foundation's Mathematical and Physical Sciences Directorate, in which capacity he set up a decadal review for particle physics. \u201cThe philosophy of the astronomical decadal survey was 'one blessing with holy water' \u2014 you touch a project and by the end of a decade it should be done,\u201d he says. In today's environment \u201cthat philosophy really doesn't make sense\u201d, he adds. Turner is one of several scientists calling for reform of the decadal-review process. He wants to see the survey focus on scientific goals rather than equipment and to give the projects realistic price tags. Astronomy has always been an easier discipline to organize than most. \u201cWe're all building experiments that go either in space or on the ground, look at the sky and make measurements of distant objects,\u201d says Steven Kahn, deputy director of the Kavli Institute for Particle Astrophysics and Cosmology at Stanford University, California. \u201cMost astronomers have some comprehension of what other astronomers are doing. You feel like you can make comparisons or make rankings.\u201d But the surveys are rooted as much in political economy as in epistemology. The first survey was completed in 1964, following efforts to gain a lead in the space race with the Soviet Union. The eight-member panel was convened to work out how best to channel a flood of new funding into ambitious projects that might be beyond individual universities and foundations. Confined to ground-based projects, its priciest recommendation was for a $40-million 'pencil-beam array' of radio dishes, which eventually became the Very Large Array. The next review also considered instruments in space, with its 23 authors recommending 11 space-based and ground-based facilities in a growing number of fields, including infrared and high-energy gamma-ray research. Unlike the 1964 survey, the 1972 panel ranked the projects. NASA, which funds most space-based projects, has its own planning process that runs in parallel with the decadal reviews, but their priorities often coincide. Over subsequent decades, the size of the committee grew further, as did the number of projects it recommended (see graph). The most recent survey, completed in 2001, involved more than 100 individuals and was drawn up by a central committee of 15, flanked by 9 subcommittees, each devoted to a separate field of astronomy. It recommended a total of 27 projects, including 7 'major initiatives' \u2014 more big ones than any previous decadal review. \n               Inflation from all angles \n             The increasing size of the panel reflects the growth of astronomy, says Stephen Strom, an astronomer at the National Optical Astronomy Observatory in Tucson, Arizona, who has served on four of the five decadal reviews. \u201cAstronomy was not a terribly large field when the first of these came out, and it was possible to more or less name the leaders,\u201d he says. The process now includes panels, public meetings and white papers from subdisciplines. The fortunes of subdisciplines can be determined by a nod from a decadal-review committee. For example, the 1991 report chaired by John Bahcall endorsed several major infrared projects. According to Harley Thronson, a chief scientist for exploration at NASA's Goddard Space Flight Center in Greenbelt, Maryland, infrared astronomy was \u201ca respectable field\u201d before the 1991 report, but progress was slow. The Infrared Astronomical Satellite, a joint programme between the United States, the Netherlands and Britain, had provided a tantalizing look at long infrared wavelengths in the Galaxy in the early 1980s; but plans for the next US space-based observatory, known today as the Spitzer Space Telescope, were caught up in NASA's bureaucracy. Then the Bahcall committee gave it high marks. \u201cThe Bahcall committee gave it the official, highest-level imprimatur,\u201d says Thronson. This endorsement, he says \u201cestablished the credibility, the substance, the importance of observations at long wavelengths\u201d. \n               Making the grade \n             It's that kind of power that has left astronomers enamoured of the decadal review. \u201cIt builds a consensus in the field and it prioritizes,\u201d says Turner. \u201cAny group of scientists can produce a wish list that exceeds the GNP of the Milky Way,\u201d adds Roger Blandford, director of the Kavli Institute for Particle Astrophysics and Cosmology. The decadal-review process helps astronomers to whittle the list down to a realistic level and \u201cmake tough calls\u201d. If the surveys appealed only to scientists they would be of limited value, but they appeal to Washington too. Agencies such as NASA and the National Science Foundation often depend on the survey to help them determine which programmes to back. And the tidy list of priorities makes it a favourite with the White House Office of Management and Budget, which recommends yearly spending for those agencies. It also steadies the United States' famously chaotic congressional budget process, according to David Goldston, chief of staff for the House Committee on Science. \u201cIn general, on real science questions, Congress tends to defer to the community,\u201d he says. \u201cIt's an area where Congress really recognizes that it doesn't have the expertise to make a decision on its own.\u201d The survey's popularity in Washington has led other disciplines to copy it. \u201cA babble of voices is not useful,\u201d says Berrien Moore, a climatologist at the University of New Hampshire in Durham, who is co-chairing the Earth science community's first decadal survey. \u201cThe community has got to come together and decadal surveys force that.\u201d Not all of the surveys are carried out the same way, or necessarily with quite the same ends in mind. The broad field of atomic, molecular and optical science shunned specific projects in favour of endorsing research areas such as physics at ultra-short time periods or ultra-high temperatures. The survey for high-energy physicists, which Turner helped to organize, on the other hand, was geared towards convincing outsiders that the discipline was worth investment, says its chair, Harold Shapiro, an economist at Princeton University in New Jersey. \u201cThey felt the field badly needed an assessment from the outside, to find out if the day of the accelerator is really over,\u201d he says. Shapiro and his colleagues concluded it wasn't, which may be of some help to US physicists when they ask for money to host the Next Linear Collider. This imitation may give astronomers a warm glow, but it doesn't get their mirrors mounted, their balloons inflated or their spacecraft launched. The lack of progress on the 2001 committee's recommendations has become a source of anxiety, and any agreement on what to do differently in the upcoming survey depends on working out what went wrong last time. Christopher McKee of the University of California, Berkeley, one of the co-chairs of the 2001 survey, believes that the current impasse has nothing to do with the survey process. \u201cThere have been two things that occurred in this decade that did not occur in the previous ones,\u201d he says. First, the 2003 crash of the space shuttle Columbia put a heavy financial strain on NASA. Second, President George W. Bush's redirection of the space agency towards manned exploration of the Moon and Mars drastically and unexpectedly cut its science budget. \u201cWe did not make the assumption that somehow NASA funding would increase, but we very definitely did make the assumption, at least in constant dollars, that there would be no cut in support for astrophysics,\u201d McKee says. Instead, the budget for astrophysics in 2010 is projected to be substantially below the funding level in 2000. \u201cThat is having some very major effects,\u201d McKee says. This all comes at a time when the National Science Foundation is also over-extended. Blandford is chairing a committee with the rather more challenging task of reaching agreement on which of the National Science Foundation's ground-based facilities to close down. Others believe that the 2001 survey has to take some share of the blame itself. It's true that the budget outlook was considerably rosier during 2000, when the survey committee was finalizing its plans, says Cash. But that still doesn't explain what he describes as \u201ca severe low-balling\u201d of mission costs. \u201cThere was a tremendous amount of undercosting going on,\u201d agrees Chris Reynolds, an astrophysicist at the University of Maryland in College Park. \u201cAnd some of it was completely apparent at the time.\u201d McKee defends the committee's choices, noting that it was wholly dependent on NASA estimates of mission costs. \u201cWe did not have any separate funding to do an independent cost analysis,\u201d he says. \n               Future tense \n             Whatever the reason for the problems, they mean that the survey that gets under way this winter will be setting a different kind of agenda, because it will have to deal with the large number of incomplete projects in the previous survey, says McKee. \u201cI think that the survey will have to explicitly prioritize projects that have already been recommended,\u201d he says. \u201cAnd it's quite possible that it will have to recommend termination of some projects.\u201d More care must also be taken to ensure that costs are accurate, adds Strom. After the dramatic price hikes of projects in the last survey, the committee must learn to be more sceptical, he says, or it is likely to lose political credibility in Washington. \u201cA key element of trust is a reliable cost estimate and good management tools,\u201d Strom says. \u201cI'm not sure that kind of culture has been sufficiently inculcated by the community.\u201d But should there be more radical changes to the way the survey is done? Some scientists think so. Reynolds believes that part of the problem is that the review depends on senior members of the community, and they may have outdated scientific viewpoints. \u201cThose views can drive basic decisions on missions,\u201d he says. He thinks that the review should hold more public forums to get community-wide opinion and that there should be a clear and direct process for submitting project proposals. These steps would give a voice to younger researchers at the beginning of their careers. Others criticize ranking. \u201cI feel it's a terrible mistake to rank missions,\u201d says Cash. \u201cIf you implement it right away it's fine, but if everything sits on hold for more than three or four years, it freezes in one approach to one piece of science.\u201d Both equipment and scientific knowledge itself are changing rapidly, Cash adds. Instead of listing missions with specific costs, the survey should list the science goals of each decade, and allow missions to be chosen independently. \u201cFor example, instead of saying our number two priority is Constellation-X, we should say our number two priority is to watch blobs of plasma fall into black holes,\u201d Cash says. But Kahn and many others continue to stand by the traditional decadal survey. \u201cThe perception within the community, which is backed by what the agencies tell us, is that the reason we've had such success is that we made hard decisions and made a list,\u201d Kahn says. \u201cThe alternative is just chaos.\u201d \n                     US astronomy: Is the next big thing too big? \n                   \n                     US space scientists rage over axed projects \n                   \n                     National Academies Board on Physics and Astronomy \n                   \n                     National Academies Space Studies Board \n                   \n                     The 2001 Decadal Review \n                   Reprints and Permissions"},
{"file_id": "443392a", "url": "https://www.nature.com/articles/443392a", "year": 2006, "authors": [{"name": "Mike Shanahan"}], "parsed_as_year": "2006_or_before", "body": "Science journalists in the developing world face unique stumbling blocks, even as some of the biggest science stories unfold around them. Mike Shanahan reports. Bennen Buma Gana, a freelance science journalist in Yaound\u00e9, Cameroon, says he often learns about discoveries in his country from media outlets thousands of miles away. \u201cAfrican scientists are unknown at home,\u201d says Buma Gana, who has covered topics from tuberculosis to tropical agriculture in his three years of reporting. \u201cWe only hear of scientific research carried out in Africa from the Western media, because the studies are published first in the West.\u201d Science journalists like Buma Gana face similar difficulties throughout the developing world, yet there is no shortage of science issues to report on. The H5N1 avian influenza virus is rampaging through southeast Asia and Africa. Malaria and HIV are entrenched. And biodiversity in many of the world's tropical regions is diminishing fast. \n               Sparse coverage \n             Reporters in the developing world face challenges that would keep many of their counterparts in the West from even trying. They are often untrained in both science and journalism, lack support and resources, and have an uneasy relationship with the scientists and officials on whom they rely for news and comment. \u201cIf science were given a conspicuous place in the media,\u201d argues Buma Gana, \u201cmuch awareness would be created and policy could change on many issues.\u201d A spate of recent initiatives aims to bridge the gap between science and society in the developing world (see  'Spreading the word' ). Buma Gana is participating in one, Science for Life (SciLife), which launched in Cameroon in April. This organization hopes to improve the quality of local science journalism by encouraging reporters and researchers to meet and work together. It plans to run seminars, training sessions and field trips to keep journalists aware of scientific developments in both Africa and the West. While it looks for an outside donor, SciLife is relying on contributions from its members \u2014 22 so far \u2014 to fund activities such as 'science caf\u00e9s', at which the public can meet scientists and listen to talks on topics such as malaria and bird flu. Sometimes reporters have to begin their battle in the newsroom itself, where editors are reluctant to run stories on science topics. \u201cScience is still not on the media agenda here,\u201d says Colombian freelance journalist Lisbeth Fog. \u201cConvincing the editor that your stories are interesting or at least easy to 'sell' is hard.\u201d When Colombian newspapers do cover science, Fog says, it is usually to report \u2014 often sensationally \u2014 on oddities such as strange animals or on controversial international issues such as genetically modified crops or cloning. \u201cLocal science rarely reaches the media,\u201d she says. That silence is sometimes maintained even when the science involves critically important local issues. In July, the International Federation of Journalists released a study on media reporting of HIV and AIDS in some of the countries most affected by the epidemic. Coverage in South Africa was \u201cminuscule\u201d and in India \u201cinfrequent\u201d, the study found. In Cambodia, even in the two weeks surrounding World AIDS Day in 2005, reporting on HIV and AIDS occupied just 3% of the news. Major media outlets in the West often have dedicated science editors, but in many developing countries there are few. It is not unknown for editors whose usual beats are sports, politics or entertainment to rework science stories out of recognition and into inaccuracy, without consulting the reporter. \u201cThere are times where you read your story the following day and just shake your head with anger at the distortion,\u201d says Ochieng' Ogodo, a freelance reporter in Nairobi, Kenya. \n               Secrets and mistrust \n             Finding news to report in the first place is challenging enough, he says, because few scientists understand the need to communicate their findings to the public. The stiff bureaucracy at most research institutions doesn't help, he adds: \u201cIt can take as long as two months to get a piece of information that should have taken just minutes to supply.\u201d Brazilian science journalist Luisa Massarani agrees. \u201cTrying to find good stories in Brazil or any other Latin American country sometimes feels like being in a black hole, especially when you are young and don't yet have your own contacts.\u201d Freelance science writer Marcelo Leite says this is because few Brazilian research centres have press offices that publicize their findings by issuing news releases or bringing scientists and journalists together. \u201cWe depend a lot on the rare times that researchers themselves get in touch to tell us what is newsworthy,\u201d he says. Part of the problem is that even when scientists are keen to have their views reported on, they aren't sure how the process might play out. \u201cScientists fear that reporters will modify their comments in an alarmist way that could create political or professional problems,\u201d says Egyptian geneticist and freelance science journalist Wagdy Sawahel. The result, he says, is that few scientists trust reporters. Yet scientists are usually easier to deal with as sources than government officials, who \u201ctreat all information as a military secret\u201d, says Sawahel. \u201cThis way of thinking makes it extremely difficult to get hold of the facts about science initiatives, policies or strategic research plans.\u201d Even in China, whose leaders regularly stress the importance of popularizing science, reporting can be difficult. Because government officials decide science policies and allocate funding mainly behind closed doors, scientists rarely feel the need to deal with the media, says Jia Hepeng, a Chinese freelancer based in Beijing. For instance, journalists were strictly forbidden from attending the World Congress of Bioethics in Beijing in August, at which policymakers and scientists were present. When science becomes politicized, reporting becomes even tougher. Journalists who cover HIV in South Africa, which has one of the highest prevalences of the disease in the world, sometimes receive threatening phone calls from people who believe the HIV virus does not cause AIDS. The AIDS 'dissidents' have tacit \u2014 and in some cases open \u2014 approval from senior government figures including health minister Manto Tshabalala-Msimang, who has endorsed the use of vitamin pills, lemon juice, garlic and olive oil over antiretroviral drugs. An organization that favours vitamin supplements to treat HIV recently threatened to sue South African newspapers and an online news agency for critical reporting. \u201cThis has polarized newsrooms and diminished journalists' ability to report accurately,\u201d says one South African reporter, speaking anonymously. Another source of potential controversy is the coverage of genetically modified (GM) crops, particularly in countries \u2014 such as Zambia and Zimbabwe \u2014 that have banned imports of GM grain as food aid during famines. \u201cThere are very few instances where the independent media reports on the latest developments in GM crops,\u201d says Talent Ngwande, a freelance journalist based in Zambia's capital Lusaka. A 2005 study by the UK-based Panos Institute concluded that reporting on GM crops in Brazil, India, Kenya and Zambia tended to follow the government line, whether pro- or anti-GM, and lacked a critical approach. \n               Long-term training \n             Training journalists may help. A few fortunate reporters attend international science journalism workshops each year, but those are usually one-time events. Some argue that governmental support is needed for science communication. \u201cNo university here offers science journalism courses,\u201d says Pakistani science journalist Aleem Ahmed. He, like some, has launched into science journalism on his own, creating a monthly Urdu-language magazine called Global Science that has published 100 issues over the past eight years. To improve the lot of working journalists, the World Federation of Science Journalists is about to launch a peer-to-peer training programme. It will match 60 journalists in Africa and the Middle East with one of 16 experienced journalists, either in the same region or in the West, who will act as mentors for two years. Many believe this could make a difference. \u201cWhat makes me hopeful about this programme is, firstly, that it is long-term,\u201d says Nadia El-Awady, an Egyptian journalist and managing science editor of IslamOnline.net. \u201cSecondly, since each mentor is responsible for only four journalists, each mentee will hopefully receive a good deal of attention.\u201d The programme, funded by Canada's International Development Research Center and the UK Department for International Development, is due to start this month. In November, the mentees are likely to meet with their mentors face-to-face in Nairobi while reporting on the UN Framework Convention on Climate Change. The scheme will also twin established associations of science writers with fledgling organizations in the developing world, such as the Arab Association of Science Journalists and SciLife. Buma Gana thinks that SciLife will benefit from the scheme and, in return, bring real benefit back to society. \u201cTalking about science from a more African point of view,\u201d he says hopefully, \u201ccould encourage our governments to invest more in scientific research.\u201d \n                     Journalism with a worldview \n                   \n                     Cuban science: \u00c2\u00bfVive la revoluci\u00c3\u00b3n? \n                   \n                     Arab science: Blooms in the desert \n                   \n                     SciLife \n                   \n                     International Federation of Journalists \u2013 report on HIV/AIDS \n                   \n                     Panos report on GM reporting \n                   \n                     Science and Development Network \n                   \n                     World Federation of Science Journalists \n                   Reprints and Permissions"},
{"file_id": "443498a", "url": "https://www.nature.com/articles/443498a", "year": 2006, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "What drives environmental activists to fire-bomb laboratories? Emma Marris investigates a radical fringe of the US green movement. One day in June 1998, three young environmental activists of a radical bent drove from Eugene, Oregon, to Olympia, Washington. Their route took them through some of the loveliest country in the Pacific Northwest: up Interstate 5, through the Willamette Valley, between dark green forested mountains and misty hillside vineyards. Their van shared the road with logging trucks carrying immense trees hung with lichens and mosses. According to the Federal Bureau of Investigation (FBI), the three were on their way to take part in fire-bombing two research facilities. Things, however, didn't go according to plan. They picked up one friend and lost another to a shoplifting bust. They lost contact with their second vehicle. But something compelled them to set fire to their target anyway. On the night of 21 June, they drove down quiet Blomberg Road in Olympia, past a dairy and a few houses. They stashed 5-gallon buckets of fuel around a wooden one-storey building, then lit them with barbecue lighter sticks. A national wildlife research facility, run by the federal government's Animal and Plant Health Inspection Service, was utterly destroyed. It served as lab and office space for government scientists who study animals that eat trees and invent ways to keep them from doing so. At the time of the arson, the lab was working on chemical repellents for beavers and a long-term study of what induces bears to dine on Douglas fir bark. The attack wouldn't be the last, or the largest, on a scientific facility. Seven years later, in December 2005, the US government indicted 11 men and women on charges of conspiracy and arson for 17 fire-bombings whose results included $12 million in damage at a ski resort in Colorado as well as the destruction of the Olympia lab. The 11 had come together loosely under the banner of the Earth Liberation Front (ELF) and the Animal Liberation Front (ALF). Their trial, for those not dead, fled or pled out, will be held in Eugene. For a certain segment of the radical environmental movement, science is seen as both useful and oppressive. Activists sometimes use scientific findings to support their arguments but often view science, and especially technology, with deep distrust. To them, science is a string of proclamations issued by men in white coats and does not respect their deeply felt connection to individual places, animals and trees. Researchers, particularly environmental scientists, should be aware of such thoughts \u2014 not least because to some of these activists, science is a legitimate target in a war in which the very future of Earth is at stake. \n               Terror list \n             Although it is impossible to know how large such pockets of radicalism are, the FBI considers them a serious threat. Spokesman Bill Carter says, \u201cFrom January 1990 to June 2004, animal and environmental rights extremists have claimed credit for more than 1,200 criminal incidents, resulting in millions of dollars in damage and monetary loss.\u201d The bureau has officially labelled both groups terrorists \u2014 a word freighted with emotion, rhetoric and law-enforcement response. Dale Nolte, who ran the Olympia facility at the time of the arson, remembers an ELF communiqu\u00e9 claiming credit. \u201cThey said they had targeted us because of work we had done with beaver traps and cougars,\u201d says Nolte, now mammals programme manager at the National Wildlife Research Center headquarters in Fort Collins, Colorado. \u201cI don't know where they got their information, because we had never worked with cougars and we were working with repellents, not traps.\u201d Nolte has never understood the arsonists' motives. He lost a personal library in the fire, as well as several crates' worth of original data and observations about the mountain beaver going back decades. \u201cOur goal was to establish a better understanding of the relationships between the animals and forest, and to enhance the establishment of trees,\u201d he says. \u201cI have a strong sense of protecting the environment.\u201d To the activists, a lab that served the interests of lumber companies was fair game. Many of them came from the anti-logging community in Eugene. \n               Among the outsiders \n             Quiet, pretty and populated with anti-establishment characters, Eugene is a magnet for the kind of activists who end up in the ELF. On a fine day at the downtown Eugene market, one might see the following clues that one has stumbled deep into the heart of a west-coast radical paradise: people bristling with clipboard petitions on a range of left and anarchist issues; others wearing fairy wings; kids in home-made nappies being toted behind bicycles; tie dye, drum circles and the sweet reek of marijuana. The taxonomy of subcultures here is nearly impenetrable to an outsider. There are anarchists, feminists, Marxists, primitivists, old-school hippies, libertarians and socialists. The groups tend to spend their time debating with each other, more or less ignoring the mainstream. In June, friends and sympathizers of the Eugene-based eco-prisoners held a benefit at a local bar. The night was mellow, the band good, the beer organic \u2014 just the sort of ideological cocoon that can make radical ideas seem pedestrian. Primitivist writer Derrick Jensen gave a lecture on the futility of working within any system to save Earth. For those in the room, his contention that humanity is in \u201cthe thrashing endgame of civilization\u201d seemed obvious. A man in a cowboy hat and black overalls, smoking a cigarette on the back porch, said he was ready to give up civilization completely: \u201cEverything, man, the whole gig.\u201d Jensen, a middle-aged man in a T-shirt reading \u201cBetter dead than domesticated\u201d, proved a good rhetorician. Unlike many radical speakers, he was also very funny. But the logic of his arguments sometimes fell apart, and he displayed a strong anti-science streak. He referred to \u201cthe myths of science\u201d and said it doesn't take a rocket scientist to see that industrial civilization is not sustainable. \u201cIn fact,\u201d he continued, \u201cit probably takes everybody but a rocket scientist.\u201d Jensen's attitudes are not unusual among ELF-sympathizing eco-activists. These people are deeply sceptical of anything having to do with that entity variously known as 'the dominant culture', 'the white supremacist capitalist imperialist patriarchy', 'the man' or 'the machine'. Science is part of 'the machine', based, in their view, on hard rationalism and cold technology. Philosophers of science have produced reasonable arguments against science as the pure, objective, single road to truth. But something bad happens when you mix half-baked versions of these critiques with a soul-shaking environmental fervour. What you get is anti-intellectual spasms of violence. This approach is a long way from the earlier tradition of direct action in favour of the environment, pioneered by the group Earth First! Many scientists were among the first members of that group, formed in 1980 in the US southwest. As Susan Zakin describes them in  Coyotes and Town Dogs , her 1993 book on the movement, they were boozing, redneck 'desert rats' \u2014 men who had seen the mesas mined, the Colorado River dammed and the great American West becoming tamed. They stressed ecosystems over scenery, unlike many environmentalists of the time. They never resorted to arson: rather, they dismantled construction equipment, trashed billboards and spiked trees with nails to break chain-saws \u2014 but they generally told loggers where they had done this to avoid nasty accidents. Later in the 1980s, a schism opened between the ecologists in the group and those who were more interested in \u201cmonkey-wrenching\u201d \u2014 sabotage or other forms of troublemaking. The scientists began to drift off. \u201cBy the time we began sitting in redwood trees in northern California, the pagan vegan pacifists outnumbered the biologists ten to one,\u201d says founding member Mike Roselle. The non-scientists that were left, he says, spawned the ELF. They were radical and committed. \u201cThese folks are often the unsung heroes, the true foot soldiers of the movement,\u201d he says. \u201cThe ones in jail were among our best, some of the most skilled and committed members of our movement. Their incarceration is a tragic loss for all of us.\u201d Earth First! co-founder Dave Foreman doesn't quite agree. \u201cWe wanted to use the science of ecology to guide us,\u201d he says. \u201cThe current group of people I don't consider conservationists, but part of the international anarchist animal-rights movement. It becomes sort of an inarticulate yowl against the establishment \u2014 revolution for the hell of it.\u201d The yowl often starts young. Chelsea Gerlach, one of the recent indictees, was a 16-year-old high-school student in Eugene when she was arrested at an Earth First! logging blockade in Idaho, according to her support website. At school, she ran an environmental club, and, according to Seattle's alternative weekly,  The Stranger , was quoted in the yearbook as saying, \u201cOur generation was born to save the Earth. If we wait until we're out of school it might be too late.\u201d She was 23 when, the FBI indictment alleges, she helped set fire to the Jefferson Poplar Farms in Clatskanie, Oregon, spray-painting \u201cELF\u201d and \u201cYou cannot control what is wild\u201d. That was 21 May 2001. For maximum effect, the FBI says, on the same night three others \u2014 William Rodgers, Stanislas Meyerhoff and Briana Waters \u2014 torched the University of Washington's centre for urban horticulture, about 230 kilometres away in Seattle. They took time to remove some cages for pet snakes from the building, Merrill Hall, before setting it aflame. \n               The Merrill fire \n             The hall housed lab space for botanists and ecologists, and served as a meeting place for Seattle's horticulturalists. Assistant professor Sarah Reichard, a specialist in rare plants, remembers the day well. \u201cIt was a beautiful morning,\u201d she says, until she got the message that her lab was on fire. She rushed to the hall, which sits just down the hill from the main University of Washington campus and is surrounded by demonstration gardens, meadows, and a grove of native trees. \u201cThere was Merrill Hall with flames leaping 30 or 40 feet into the air,\u201d she remembers. The young academic lost everything, including a tissue culture lab where she was growing the highly endangered showy stickweed, a blue-white flowering plant in the forget-me-not family. Losses of specimens, along with irreplaceable books and slides, put her career back at least a year, she estimates. It wasn't just the blow to her road to tenure that worries her. That morning, standing by the blazing building, she feared that someone had been killed. \u201cAcademic units are not nine-to-five places,\u201d she says. \u201cIn fact, it was very unusual that there was no one there at three in the morning. One graduate student told me that the only reason he wasn't there was because I gave an extension on an assignment.\u201d No one was hurt in the blaze, but the building was a total loss. Reichard may have been the hardest hit but she wasn't the target of the arson. \u201cWe could see Toby's office was black \u2014 a big black hole,\u201d she says. \u201cEveryone realized immediately that it was not an accident.\u201d Toby is Toby Bradshaw, a plant geneticist who at the time ran the Poplar Molecular Genetics Cooperative, a group working on finding genes in hybrid poplars that code for traits useful in a crop tree \u2014 fast-growing, disease-resistant and straight. The team used traditional breeding techniques, ultimately aiming to make productive tree farms more attractive than logging big trees from old-growth forests. Somehow, however, the radical greens got the idea that Bradshaw was genetically engineering trees. Bradshaw says he has never done so, although his colleagues have. He'd been a target before. Someone had tried to overturn his potted seedlings during the protest against the World Trade Organization meeting in Seattle in 1999. And two weeks after the Merrill Hall fire, a communiqu\u00e9 was issued through Craig Rosebraugh, a former spokesman for the ELF who has not been indicted. It reads, in part, \u201cBradshaw, the driving force in G.E. tree research, continues to unleash mutant genes into the environment that is certain to cause irreversible harm to forest ecosystems. As long as universities continue to pursue this reckless 'science', they run the risk of suffering severe losses. Our message remains clear, we are determined to stop genetic engineering.\u201d Bradshaw likes to joke that the fire actually helped his career. He got tenure and a new lab shortly afterwards, and has since moved on to unrelated work. But the attitudes behind the crime trouble him. \u201cAs social commentary, these kinds of arsons are ineffective because they are so misguided,\u201d he says. \u201cScience at its heart is a rational enterprise and, at its heart, this kind of terror tactics with fire-bombing is an irrational enterprise.\u201d \n               The crackdown \n             The FBI last interviewed Bradshaw at the time of the arson, and he was surprised last December when the arrests were announced. In fact, the FBI had spent years putting together information for the indictment; the most recent of the 17 arsons listed in the charges dated to October 2001, at a Bureau of Land Management wild-horse facility in Litchfield, California. The small group that had acted under the ELF banner seems to have more or less broken up after that, scattering to Virginia, Arizona, and around the Pacific Northwest. Reportedly, the FBI laid the groundwork for the charges by getting activist and heavy-metal guitarist Jake Ferguson to call his old pals for some nostalgic, and wire-tapped, conversations. The bust was important enough to bring out the top law-enforcement brass. On 20 January, FBI director Robert Mueller gave a press conference on the indictments in Washington DC. \u201cTerrorism is terrorism, no matter the motive,\u201d he said. \u201cThe FBI becomes involved, as it did in this case, only when volatile talk crosses the line into violence and criminal activity.\u201d Terrorism, however, is not defined as a crime in the United States; the group was charged with arson and associated crimes. Most of the indictees are now in jails in Oregon, with some under house arrest or out on bail. Lauren Regan, head of the Civil Liberties Defense Center in Eugene, calls the arrests the 'green scare', a play on the 'red scare' of the 1950s in which US citizens with communist ties were persecuted. Regan says that many of the indictees don't deserve the sentences they are facing \u2014 life several times over for each. \u201cA lot of these people were at the time very young,\u201d she points out. \u201cYou are easily swayed, you've got a lot of passion. You hold a radio while some genetically modified trees are burned down. Chances are that they were not thinking, in that pre-9/11 time, that they were looking at life in jail.\u201d \n               Repentance and escape \n             Many of those arrested have since pleaded guilty and are cooperating with the authorities. Gerlach made a public statement at the time of her plea, saying, \u201cThese acts were motivated by a deep sense of despair and anger at the deteriorating state of the global environment and the escalating inequities within society. But I realized years ago that this was not an effective or appropriate way to effect positive change.\u201d Those outside jail have reacted harshly to those cooperating with the government. In Eugene, Jensen began his speech with a message: \u201cWhat you are doing is wrong, and I plan on seeing you brought to justice. And fuck you.\u201d Ferguson, who never faced charges, is known in some activist circles as 'Jake the Snake'. A few of the indictees are still at large. Their wanted posters reveal a bit about them. Justine Overaker's, for example, paints a portrait of her as a seeker; it mentions that she may seek work as a \u201cfirefighter, a midwife, a sheep tender or a masseuse\u201d and that she can speak Spanish and has been known to use narcotics. She has a tattoo of a bird across her back. Rodgers, also known as Avalon, was arrested at his bookstore in Prescott, Arizona. He suffocated himself to death with a plastic bag in his jail cell. He left a note that said, \u201cHuman cultures have been waging war against the Earth for millennia. I chose to fight on the side of bears, mountain lions, skunks, bats, saguaros, cliff rose and all things wild. I am just the most recent casualty in that war. But tonight I have made a jail break \u2014 I am returning home, to the Earth, to the place of my origins.\u201d At 41, Rodgers was the oldest of the indictees and by some reports the ringleader. His lover, Katie Rose Nelson, says, \u201cLife mattered to him \u2014 and that meant all life\u201d. She and Rodgers both felt the environmental cause was urgent \u2014 too urgent to spend time on research. \u201cIn our hearts we can all see what is happening around us,\u201d she says. With most indictees not talking, missing or dead, they can't explain the motivations and justifications for their alleged crimes. But a rare glimpse of three of the activists, including Ferguson and Rodgers, can be seen in the 1999 documentary film  Pickaxe , which chronicles an 11-month battle in the mid-1990s to keep an area in Oregon called Warner Creek from being logged. Here, they engage in legal protests, such as hunger strikes, and the kind of non-violent illegal protest that many condone, such as blockading the road into the area. And they won. At least that one patch of forest is still there, old and moist and green. So why were these same people drawn to anonymous arson attacks late at night \u2014 and why target scientists? \u201cScience does not have a lock on truth,\u201d says David Agranoff, an animal-rights and veganism activist in San Diego, California, who knows some of the defendants. He says he is sure they considered their actions carefully: \u201cI would guess they had a pretty good reason.\u201d Rosebraugh, the former ELF spokesman, notes that each would have had their own motives. \u201cFor all the people involved,\u201d he says, \u201cyou would probably give a different answer.\u201d \n               The 'pure ones' \n             Donna Haraway, who studies feminism and science at the University of California, Santa Cruz, met some of the radical-activist set in Eugene when she went to speak there in 2001. Because Haraway, author of  Simians, Cyborgs and Women: The Reinvention of Nature , embraces technology as an agent for positive change, the group saw her as an enemy. Their protest included flyers that she found distinctly threatening. \u201cAt least one felt that the rape of nature justified the rape of anyone who supported it,\u201d she says. \u201cThese people don't go for complexity; they believe that they are the only pure ones who can defend nature.\u201d The ELF indictees are by no means the last of the self-anointed environmental 'pure ones'. Jeffrey Luers of Eugene is appealing a 22-year sentence for setting fire to sports utility vehicles as a protest. Activist Trey Arrow is fighting extradition from Canada to the United States, where he faces charges of having burned logging and mining trucks. In January, three people were arrested in the parking lot of a Kmart while buying supplies, allegedly to bomb a forest genetics lab in Placerville, California: they are charged with conspiracy, and two have pleaded guilty, although many activists blame an FBI agent provocateur for the plot. In Canada, a group of eco-saboteurs are currently on a fire-bombing tour of construction sites across Ontario. By its nature, the movement is decentralized, non-hierarchical and open. Anyone can be an environmental arsonist. From jail, Luers publishes a magazine called  Heartcheck , with an estimated circulation of 1,000. For the latest issue, he has penned an essay called 'Time's up' in which he summarizes recent facts and figures on species extinction in various ecosystems, glacier melt rates in Alaska and carbon dioxide levels. He ends: \u201cI could go on, but you get the point. Or do you? We are not running out of time, we are out of time! We have to act now just so it doesn't get any worse. Smash it, break it, block it, lock down to it. I don't care what you do or how you do it. Just stop it. Get out there and stop it.\u201d \n                     Re-wilding North America \n                   \n                     Dangers of going underground \n                   \n                     The Rewilding Institute \n                   \n                     Animal Liberation Front \n                   \n                     The rebuilding of Merrill Hall \n                   Reprints and Permissions"},
{"file_id": "443505a", "url": "https://www.nature.com/articles/443505a", "year": 2006, "authors": [{"name": "Britta Danger"}], "parsed_as_year": "2006_or_before", "body": "Can an advertising executive write an accurate thriller about science? Britta Danger talks to a German author who thinks he has pulled it off. Marine biologist Sigur Johanson is wary when Tina Lund, an oil-company scientist, visits his remote Norwegian home. With mixed feelings \u2014 he has long been secretly attracted to her \u2014 he agrees to help her employer with a serious problem: swarms of  Hesiocaeca methanicola , or ice worms, at potential North Sea oil-drilling sites. Other scientists join in too, as the invasion turns out to be related to the mysterious breakdown of methane hydrate reservoirs on the sea bed off Europe. But worms are not the only creatures swarming. Mussels, sea wasps, and Portuguese men-of-war also gather in untold numbers, killing seafarers and beach tourists. A tsunami hits northern Europe. Death and destruction is everywhere. As crisis becomes global catastrophe, the US government \u2014 represented by the ambitious, single-minded General Judith Li \u2014 takes over the worldwide effort to unravel the mystery. If  The Swarm  sounds destined for the silver screen, it is: Hollywood actor and producer Uma Thurman has already snapped up the film rights to the best-selling German thriller, now hitting the shelves in an English-language translation. The book's author, Frank Sch\u00e4tzing, says he based his plot on a dream. \u201cSea life was flocking together, threatening us,\u201d he remembers. On waking, he began to wonder: \u201cWhat would happen if...?\u201d Yet much of  The Swarm  is based in reality \u2014 indeed, it contains a level of scientific verisimilitude unusual in any novel, let alone an airport thriller. Sch\u00e4tzing, a self-assured, well-coiffed 49-year-old, seems at first glance an unlikely figure to have authored such a book. He is an advertising executive in Cologne, and none of his previous five novels dealt with science. Yet readers have found themselves absorbed not only in the tensions and romances of a tough mystery, but also in the details of up-to-the-minute research in fields such as neurocomputing, seafloor oceanography and cell signalling. With no formal scientific education, Sch\u00e4tzing has managed to cover a swathe of scientific territory without significant error. Which is not to say that the novel is a truly realistic portrayal of the oceanographic world. The antagonist, after all, is pure fantasy: \u201cI created an environment as real as possible and added only one fictional element \u2014 a deep-sea, non-human intelligence crucial to the plot,\u201d says Sch\u00e4tzing. He has Johanson dub this intelligence Yrr by striking a computer keyboard three times at random. Sch\u00e4tzing developed the plot over three years of devouring popular science books, research reviews and internet sources. His studies led him to the dozen or so researchers with whom he hammered out the details of how the science could be made to serve the plot without becoming distorted. \u201cWith them, I tried to see how far I could stretch my ideas,\u201d he says. \u201cWe developed methods together, for example the way the Yrr communicated\u201d \u2014 through pheromones in the water. Sch\u00e4tzing visited scientists across Germany and further afield, including in Vancouver, Canada. He developed a particularly close relationship with those at the Leibniz Institute of Marine Sciences at the University of Kiel in Germany. There he spent hours discussing his ideas with researchers including Erwin Suess, a prominent methane-hydrate researcher; marine biologist Heiko Sahling; and marine geologist Gerhard Bohrmann. All showed up in the novel under their own names, to their initial consternation. But once they had read the book, none of them minded. \u201cI was worried when I started reading, but in the end I found myself nicely characterized,\u201d says Bohrmann, who has plenty of adventures in real life on research vessels in stormy seas \u2014 albeit not on the scale of his fictional namesake, who escapes a vicious shark attack while trying to save humanity from the ice worms. Sch\u00e4tzing smiles, recalling how he originally intended Bohrmann to play a tiny role. \u201cBut then he ended up as the Bruce Willis of marine science.\u201d Bohrmann says that Sch\u00e4tzing had done his research well before coming to Kiel for an interview: \u201cHe already knew everything about gas hydrates.\u201d Sahling was also impressed: \u201cHis questions were very intelligent and he was a good listener \u2014 he adopted much of what we said word-for-word in his novel.\u201d Suess was less charmed, however, remarking that Sch\u00e4tzing claimed as his own ideas that were generated in discussion with colleagues. Science communicator Thomas Orthmann was similarly unimpressed and tried to sue Sch\u00e4tzing for plagiarising from his website. His case was unsuccessful. Sch\u00e4tzing did not speak to all the scientists he characterized. Ryo Matsumoto, a gas-hydrate expert from the University of Tokyo, was tipped off about his role by his German colleagues. Curious yet nervous, Matsumoto had to wait two years for the English translation to be published to find out how he appeared in the novel. But he was pleased with his modest role as the scientist who confirms that ice worms have reached the Japanese Pacific. \u201cI am surprised at his knowledge of biogeosciences,\u201d he says. Despite his interest, Matsumoto has yet to finish the book. At more than 900 pages, even avid supporters admit that it is simply too long. Descriptions of the science \u2014 often disguised as discussions between researchers, or as lectures \u2014 can run on for pages. This is both the fascination of the book and its literary weakness, as it drastically slows the plot. As a novel  The Swarm  may be unlikely to repeat its phenomenal German success out in the English-speaking world. But its future in Hollywood looks brighter \u2014 it is, after all, a thriller packed with technology, danger, spectacle, romance and a watery hint of apocalypse. The prospect certainly has Sch\u00e4tzing in dream mode again: George Clooney and Lucy Liu in the main roles \u2014 and a cameo for himself, Hitchcock-style. \n                     The Swarm website \n                   \n                     Thomas Orthmann\u2019s site \n                   Reprints and Permissions"},
{"file_id": "443904a", "url": "https://www.nature.com/articles/443904a", "year": 2006, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "Is the cure for cancer lurking beneath the waves? Emma Marris plunges into the chemistry of marine natural products. Walk downhill from the University of California, San Diego, towards the ocean, and you'll see bungalows draped in bougainvillea, tanned youths unloading surfboards from station wagons, and at the bottom, William Fenical's lab at the Scripps Institute of Oceanography. The chemist has white sand and palm trees both on his screensaver and outside his window. And beyond them \u2014 the sea. Fenical and a global group of like-minded scientists believe that the sea hides a mermaid's grotto of useful chemicals. And they have been diving for corals, grinding up sponges, fermenting microbes, and poking around inside cells for decades in an effort to find them. Chemicals from these organisms \u2014 natural products \u2014 can have pharmaceutical potential. But before scientists could deliver any useful natural products, the drug industry largely lost interest in the field, slowing its growth. Fenical and others now say that the problems of the past have mostly been solved. And they may no longer need the drug industry's support. Many of the best-known drugs are extracted from living things. A famous example is the actinomycetes, soil microbes so good at producing antibiotics that in the 1950s and beyond, pharmaceutical companies sent people tramping all over the globe to collect dirt. Most of our antibiotics still come from  Penicillium  mould or soil microbes. \u201cAntibiotics were an astounding discovery,\u201d says Fenical from his seaside office, \u201cwithout question the most important medical discovery in history.\u201d The ocean covers two-thirds of Earth, and many branches of life \u2014 such as the group containing jellyfish and corals \u2014 are exclusively marine. Yet until relatively recently, chemists were confirmed landlubbers. \u201cWe are not marine organisms,\u201d says Fenical, \u201cso until about 1970, no one even thought of the ocean. It was left as a deep secret.\u201d Fenical, and a few others, were the first to get their toes wet. \u201cIt seemed ridiculous to me that the ocean \u2014 with such a vast habitat \u2014 had escaped anyone's notice,\u201d he says. \u201cBut there are good reasons. People fear the ocean; it has been considered a very hostile, inhospitable place.\u201d Thirty years later the field is having a mini renaissance 1 , with strong research labs around the globe, including China. But natural-products chemists have struggled to convince pharmaceutical companies to share their enthusiasm. Most drug companies dismantled their natural-products research units in the 1990s, when combinatorial chemistry was heralded as the next wave of drug discovery. At that time, there were no drugs based on marine natural products on the shelves. Today, there is one: Prialt, a pain reliever derived from the hunting venom of the cone snail (see  'Marine medicines' ). \u201cNot to say that there aren't powerful leads in nature,\u201d says Kate Robins, a spokeswoman for Pfizer in New London, Connecticut, which closed its natural-products programme in the 1990s, \u201cbut it has been our experience that promising leads come faster and more frequently out of combinatorial chemistry and synthetic techniques.\u201d \n               Backwater \n             Fenical bemoans what he perceives as the drug companies' lack of vision. \u201cWe have done so much to prove the rich resource of the ocean. The big pharmas are apathetic and uninterested. They are risk averse.\u201d This irks him, he says, not just for professional reasons. \u201cThe fact is that no really new antibiotic has come out since the late 1980s. We are on a very dangerous collision course with a plague.\u201d Marine natural products are attractive sources for new antibiotics because they are mostly secondary metabolites. That is, they are not essential to an organism's growth and development, but are compounds that do something else, such as deter predators \u2014 and could be re-engineered to aid our fight against infectious disease. But infectious diseases are unfashionable with many drug companies, and the legal uncertainties around bioprospecting for natural products hasn't helped its cause. Another problem is scarcity. A compound made in tiny quantities by a sponge that lives hundreds of metres down, for example, poses challenges to traditional drug testing and development. So scientists are now working out how organisms make natural products. To their surprise, many compounds are generated by an associated microbe, rather than the organism itself. This means they can either grow the microbes alone in flasks, or they can identify the microbial genes responsible for making the compound, and insert them into an organism researchers find easier to work with, such as  Escherichia coli . In addition, more and more natural products are being made from scratch as synthetic chemists get better at their art 2 . David Newman, director of the natural-products division at the US National Cancer Institute in Bethesda, Maryland, sums it up: \u201cIn the 1980s, the field was hyped, but we had not solved the supply problem. By the end of the 1990s, what came together was molecular biology, which allowed you to go looking for the producing genes, the ability to grow microbes, and synthetic chemists who could either do total synthesis or work from a structure.\u201d Fenical's research followed a similar pattern. He previously studied invertebrates on coral reefs, where low nutrient levels create competition for food and may favour the evolution of toxins. Resilience, a skin cream made by Est\u00e9e Lauder (selling for $50 an ounce), contains an anti-inflammatory that Fenical found in soft corals during those years. But he eventually left the invertebrate field: \u201cIt was almost impossible to develop a drug, because we could not provide lots and lots of it.\u201d Instead, he turned to microorganisms. With patience and luck, marine microbes are fermentable \u2014 they can be grown in large flasks, if the right mix of nutrients is provided. The lab made its first big discovery in deep-ocean sediment. There the team found lots of actinomycete species, despite the received wisdom that there were none in the sea. The team named the first genus of these  Salinospora 3 . And from one of them, in 2003, came salinosporamide A, a compound that binds extremely selectively to the proteasome in tumour cells 4 . It is now in clinical trials for multiple myeloma, a cancer of the blood. Recently, the lab has discovered another new genus,  Marinispora , which produces compounds with promising antibiotic and anticancer properties 5 . Fenical says the limiting factor now is time and the logistics of getting samples from deep-sea mud. Working with Scripps technicians, Fenical has devised probes that fall to the sea floor, drive a corer into the mud, release weights and rise to the surface. The samplers take 45 minutes to fall 6,000\u20137,000 metres to the ocean floor and return \u2014 if they come back at all. Technical advances are changing the field in other ways. Studies using nuclear magnetic resonance imaging need only tiny quantities of compounds, making the work easier and more ecologically sound, says John Blunt, a chemist at the University of Canterbury in Christchurch, New Zealand. \u201cThe other area that is developing is assays,\u201d says Blunt. \u201cWe used to use simple cytotoxic and antibiotic assays. Now specific enzymes are being used.\u201d So instead of finding out whether a compound has any biological activity, researchers can find out whether it binds to a particular target. \n               Find a niche \n             Whether big drug firms renew their interest in marine products may not matter \u2014 small institutional labs or companies can take them on. \u201cMany of our big-pharma competitors who have moved out of this effort are getting back in through collaborations with smaller companies,\u201d says Guy Carter, vice-president of research at drug firm Wyeth in Pearl River, New York, which, unlike most drug companies, retains a natural-products unit employing 50 people. For 20 years, the Madrid-based biotech company PharmaMar has sought cancer drugs in marine natural products. It investigates compounds found by academics, and has its own explorers. It now has a library of more than 40,000 compounds, six of which are in clinical trials. Fenical helped found a spin-off, called Nereus, after the Greek god of the Mediterranean. The eight-year-old company has two compounds in preclinical trials for cancer. Despite the scarcity of new drugs on the market, marine drug discovery still attracts newcomers. Robert Jacobs, a marine natural-products biologist at the University of California, Santa Barbara, has a tip for new recruits. \u201cWhen we started out in the early 1970s, just about anything you picked up, you'd find something new. These days we recommend that people think ecologically.\u201d That is \u2014 find a niche where secondary metabolites are likely to be abundant and focus on it. In the vastness of the oceans, there's still much to explore. \n                     The evolving role of natural products in drug discovery \n                   \n                     Lessons from natural molecules \n                   \n                     Natural resources: Bioprospects less than golden \n                   \n                     EMEA struggles with need to restructure \n                   \n                     Nature Reviews Drug Discovery guide to drug discovery \n                   \n                     Antibiotics in Focus \n                   \n                     Prialt \n                   \n                     Nereus \n                   \n                     PharmaMar \n                   \n                     Fenical Research Group \n                   Reprints and Permissions"},
{"file_id": "444534a", "url": "https://www.nature.com/articles/444534a", "year": 2006, "authors": [{"name": "Jo Marchant"}], "parsed_as_year": "2006_or_before", "body": "The ancient Antikythera Mechanism doesn't just challenge our assumptions about technology transfer over the ages \u2014 it gives us fresh insights into history itself. Hear the sound  of the Antikythera Mechanism recreated on the 30 November  Nature Podcast .  It looks like something from another world \u2014 nothing like the classical statues and vases that fill the rest of the echoing hall. Three flat pieces of what looks like green, flaky pastry are supported in perspex cradles. Within each fragment, layers of something that was once metal have been squashed together, and are now covered in calcareous accretions and various corrosions, from the whitish tin oxide to the dark bluish green of copper chloride. This thing spent 2,000 years at the bottom of the sea before making it to the National Archaeological Museum in Athens, and it shows. But it is the details that take my breath away. Beneath the powdery deposits, tiny cramped writing is visible along with a spiral scale; there are traces of gear-wheels edged with jagged teeth. Next to the fragments an X-ray shows some of the object's internal workings. It looks just like the inside of a wristwatch. This is the Antikythera Mechanism. These fragments contain at least 30 interlocking gear-wheels, along with copious astronomical inscriptions. Before its sojourn on the sea bed, it computed and displayed the movement of the Sun, the Moon and possibly the planets around Earth, and predicted the dates of future eclipses. It's one of the most stunning artefacts we have from classical antiquity. No earlier geared mechanism of any sort has ever been found. Nothing close to its technological sophistication appears again for well over a millennium, when astronomical clocks appear in medieval Europe. It stands as a strange exception, stripped of context, of ancestry, of descendants. Considering how remarkable it is, the Antikythera Mechanism has received comparatively scant attention from archaeologists or historians of science and technology, and is largely unappreciated in the wider world. A virtual reconstruction of the device, published by Mike Edmunds and his colleagues in this week's  Nature  (see  page 587 ), may help to change that. With the help of pioneering three-dimensional images of the fragments' innards, the authors present something close to a complete picture of how the device worked, which in turn hints at who might have been responsible for building it. But I'm also interested in finding the answer to a more perplexing question \u2014 once the technology arose, where did it go to? The fact that such a sophisticated technology appears seemingly out of the blue is perhaps not that surprising \u2014 records and artefacts from 2,000 years ago are, after all, scarce. More surprising, to an observer from the progress-obsessed twenty-first century, is the apparent lack of a subsequent tradition based on the same technology \u2014 of ever better clockworks spreading out round the world. How can the capacity to build a machine so magnificent have passed through history with no obvious effects? \n               Astronomic leaps \n             To get an idea of what the mechanism looked like before it had the misfortune to find itself on a sinking ship, I went to see Michael Wright, a curator at the Science Museum in London for more than 20 years and now retired. Stepping into Wright's workshop in Hammersmith is a little like stepping into the workshop where H. G. Wells' time machine was made. Every inch of floor, wall, shelf and bench space is covered with models of old metal gadgets and devices, from ancient Arabic astrolabes to twentieth-century trombones. Over a cup of tea he shows me his model of the Antikythera Mechanism as it might have been in its pomp. The model and the scholarship it embodies have consumed much of his life (see  'Raised from the depths' ). The mechanism is contained in a squarish wooden case a little smaller than a shoebox. On the front are two metal dials (brass, although the original was bronze), one inside the other, showing the zodiac and the days of the year. Metal pointers show the positions of the Sun, the Moon and five planets visible to the naked eye. I turn the wooden knob on the side of the box and time passes before my eyes: the Moon makes a full revolution as the Sun inches just a twelfth of the way around the dial. Through a window near the centre of the dial peeks a ball painted half black and half white, spinning to show the Moon's changing phase. It's a popular notion that technological development is a simple progression. But history is full of surprises. Fran\u00e7ois Charette On the back of the box are two spiral dials, one above the other. A pointer at the centre of each traces its way slowly around the spiral groove like a record stylus. The top dial, Wright explains, shows the Metonic cycle \u2014 235 months fitting quite precisely into 19 years. The lower spiral, according to the research by Edmunds and his colleagues, was divided into 223, reflecting the 223-month period of the Saros cycle, which is used to predict eclipses. To show me what happens inside, Wright opens the case and starts pulling out the wheels. There are 30 known gear-wheels in the Antikythera Mechanism, the biggest taking up nearly the entire width of the box, the smallest less than a centimetre across. They all have triangular teeth, anything from 15 to 223 of them, and each would have been hand cut from a single sheet of bronze. Turning the side knob engages the big gear-wheel, which goes around once for every year, carrying the date hand. The other gears drive the Moon, Sun and planets and the pointers on the Metonic and Saros spirals. To see the model in action is to want to find out who had the ingenuity to design the original. Unfortunately, none of the copious inscriptions is a signature. But there are other clues. Coins found at the site by Jacques Cousteau in the 1970s have allowed the shipwreck to be dated sometime shortly after 85  BC . The inscriptions on the device itself suggest it might have been in use for at least 15 or 20 years before that, according to the Edmunds paper. The ship was carrying a rich cargo of luxury goods, including statues and silver coins from Pergamon on the coast of Asia Minor and vases in the style of Rhodes, a rich trading port at the time. It went down in the middle of a busy shipping route from the eastern to western Aegean, and it seems a fair bet that it was heading west for Rome, which had by that time become the dominant power in the Mediterranean and had a ruling class that loved Greek art, philosophy and technology. The Rhodian vases are telling clues, because Rhodes was the place to be for astronomy in the first and second centuries  BC . Hipparchus, arguably the greatest Greek astronomer, is thought to have worked on the island from around 140  BC  until his death in around 120  BC . Later the philosopher Posidonius set up an astronomy school there that continued Hipparchus' tradition; it is within this tradition that Edmunds and his colleagues think the mechanism originated. Circumstantial evidence is provided by Cicero, the first-century  BC  Roman lawyer and consul. Cicero studied on Rhodes and wrote later that Posidonius had made an instrument \u201cwhich at each revolution reproduces the same motions of the Sun, the Moon and the five planets that take place in the heavens every day and night\u201d. The discovery of the Antikythera Mechanism makes it tempting to believe the story is true. And Edmunds now has another reason to think the device was made by Hipparchus or his followers on Rhodes. His team's three-dimensional reconstructions of the fragments have turned up a new aspect of the mechanism that is both stunningly clever and directly linked to work by Hipparchus. One of the wheels connected to the main drive wheel moves around once every nine years. Fixed on to it is a pair of small wheels, one of which sits almost \u2014 but not exactly \u2014 on top of the other. The bottom wheel has a pin sticking up from it, which engages with a slot in the wheel above. As the bottom wheel turns, this pin pushes the top wheel round. But because the two wheels aren't centred in the same place, the pin moves back and forth within the upper slot. As a result, the movement of the upper wheel speeds up and slows down, depending on whether the pin is a little farther in towards the centre or a little farther out towards the tips of the teeth (see illustration on  page 551 ). The researchers realized that the ratios of the gear-wheels involved produce a motion that closely mimics the varying motion of the Moon around Earth, as described by Hipparchus. When the Moon is close to us it seems to move faster. And the closest part of the Moon's orbit itself makes a full rotation around the Earth about every nine years. Hipparchus was the first to describe this motion mathematically, working on the idea that the Moon's orbit, although circular, was centred on a point offset from the centre of Earth that described a nine-year circle. In the Antikythera Mechanism, this theory is beautifully translated into mechanical form. \u201cIt's an unbelievably sophisticated idea,\u201d says Tony Freeth, a mathematician who worked out most of the mechanics for Edmunds' team. \u201cI don't know how they thought of it.\u201d \u201cI'm very surprised to find a mechanical representation of this,\u201d adds Alexander Jones, a historian of astronomy at the University of Toronto, Canada. He says the Antikythera Mechanism has had little impact on the history of science so far. \u201cBut I think that's about to change. This was absolutely state of the art in astronomy at the time.\u201d Wright believes that similar mechanisms modelled the motions of the five known planets, as well as of the Sun, although this part of the device has been lost. As he cranks the gears of his model to demonstrate, and the days, months and years pass, each pointer alternately lags behind and picks up speed to mimic the astronomical wanderings of the appropriate sphere. \n               Greek tragedy \n             Almost everyone who has studied the mechanism agrees it couldn't have been a one-off \u2014 it would have taken practice, perhaps over several generations, to achieve such expertise. Indeed, Cicero wrote of a similar mechanism that was said to have been built by Archimedes. That one was purportedly stolen in 212  BC  by the Roman general Marcellus when Archimedes was killed in the sacking of the Sicilian city of Syracuse. The device was kept as an heirloom in Marcellus' family: as a friend of the family, Cicero may indeed have seen it. So where are the other examples? A model of the workings of the heavens might have had value to a cultivated mind. Bronze had value for everyone. Most bronze artefacts were eventually melted down: the Athens museum has just ten major bronze statues from ancient Greece, of which nine are from shipwrecks. So in terms of the mechanism, \u201cwe're lucky we have one\u201d, points out Wright. \u201cWe only have this because it was out of reach of the scrap-metal man.\u201d But ideas cannot be melted down, and although there are few examples, there is some evidence that techniques for modelling the cycles in the sky with geared mechanisms persisted in the eastern Mediterranean. A sixth-century  AD  Byzantine sundial brought to Wright at the Science Museum has four surviving gears and would probably have used at least eight to model the positions of the Sun and Moon in the sky. The rise of Islam saw much Greek work being translated into Arabic in the eighth and ninth centuries  AD , and it seems quite possible that a tradition of geared mechanisms continued in the caliphate. Around  AD  1000, the Persian scholar al-Biruni described a \u201cbox of the Moon\u201d very similar to the sixth-century device. There's an Arabic-inscribed astrolabe dating from 1221\u201322 currently in the Museum of the History of Science in Oxford, UK, which used seven gears to model the motion of the Sun and Moon. But to get anything close to the Antikythera Mechanism's sophistication you have to wait until the fourteenth century, when mechanical clockwork appeared all over western Europe. \u201cYou start to get a rash of clocks,\u201d says Wright. \u201cAnd as soon as you get clocks, they are being used to drive astronomical displays.\u201d Early examples included the St Albans clock made by Richard Wallingford in around 1330 and a clock built by Giovanni de'Dondi a little later in Padua, Italy, both of which were huge astronomical display pieces with elaborate gearing behind the main dial to show the position of the Sun, Moon, planets and (in the case of the Padua clock) the timing of eclipses. The time-telling function seems almost incidental. It could be argued that the similarities between the medieval technology and that of classical Greece represent separate discoveries of the same thing \u2014 a sort of convergent clockwork evolution. Wright, though, favours the idea that they are linked by an unbroken tradition: \u201cI find it as easy to believe that this technology survived unrecorded, as to believe that it was reinvented in so similar a form.\u201d The timing of the shift to the West might well have been driven by the fall of Baghdad to the Mongols in the thirteenth century, after which much of the caliphate's knowledge spread to Europe. Shortly after that, mechanical clocks appeared in the West, although nobody knows exactly where or how. It's tempting to think that some mechanisms, or at least the ability to build them, came west at the same time. As Fran\u00e7ois Charette, a historian of science at Ludwig Maximilians University in Munich, Germany, points out, \u201cfor the translation of technology, you can't rely solely on texts\u201d. Most texts leave out vital technical details, so you need skills to be transmitted directly. But if the tradition of geared mechanisms to show astronomical phenomena really survived for well over a millennium, the level of achievement within that tradition was at best static. The clockwork of medieval Europe became more sophisticated and more widely applied fairly quickly; in the classical Mediterranean, with the same technology available, nothing remotely similar happened. Why didn't anyone do anything more useful with it in all that time? More specifically, why didn't anyone work out earlier what the gift of hindsight seems to make obvious \u2014 that clockwork would be a good thing to make clocks with? Serafina Cuomo, a historian of science at Imperial College, London, thinks that it all depends on what you see as 'useful'. The Greeks weren't that interested in accurate timekeeping, she says. It was enough to tell the hour of the day, which the water-driven clocks of the time could already do fairly well. But they did value knowledge, power and prestige. She points out that there are various descriptions of mechanisms driven by hot air or water \u2014 and gears. But instead of developing a steam engine, say, the devices were used to demonstrate philosophical principles. The machines offered a deeper understanding of cosmic order, says David Sedley, a classicist at the University of Cambridge, UK. \u201cThere's nothing surprising about the fact that their best technology was used for demonstrating the laws of astronomy. It was deep-rooted in their culture.\u201d I find it as easy to believe that this technology survived unrecorded, as to believe that this was reinvented in so similar a form. Michael Wright Another, not mutually exclusive, theory is that devices such as the Antikythera Mechanism were signifiers of social status. Cuomo points out that demonstrating wondrous devices brought social advancement. \u201cThey were trying to impress their peers,\u201d she says. \u201cFor them, that was worth doing.\u201d And the Greek \u00e9lite was not the only potential market. Rich Romans were eager for all sorts of Greek sophistication \u2014 they imported philosophers for centuries. Seen in this light, the idea that the Antikythera Mechanism might be expected to lead to other sorts of mechanism seems less obvious. If it already embodied the best astronomy of the time, what more was there to do with it? And status symbols do not follow any clearly defined arc of progress. What's more, the idea that machines might do work may have been quite alien to slave-owning societies such as those of Ancient Greece and Rome. \u201cPerhaps the realization that you could use technology for labour-saving devices took a while to dawn,\u201d says Sedley. There is also the problem of power. Water clocks are thought to have been used on occasion to drive geared mechanisms that displayed astronomical phenomena. But dripping water only provides enough pressure to drive a small number of gears, limiting any such display to a much narrower scope than that of the Antikythera Mechanism, which is assumed to have been handcranked. To make the leap to mechanical clocks, a geared mechanism needs to be powered by something other than a person; it was not until medieval Europe that clockwork driven by falling weights makes an appearance. \n               Invention's evolution \n             Bert Hall, a science historian at the University of Toronto in Canada, believes a final breakthrough towards a mechanical weight drive might have come about almost by accident, by adapting a bell-ringing device. A water clock could have driven a hammer or weight mechanism swinging between two bells as an alarm system, until someone realized that the weight mechanism would be a more regular way of driving the clock in the first place. When the new way to drive clocks was discovered, says Hall, \u201cthe [clockwork] technology came rushing out of the wings into the new tradition\u201d. Researchers would now love further mechanisms to be unearthed in the historical record. \u201cWe hope that if we can bring this to people's attention, maybe someone poking around in their museum might find something, or at least a reference to something,\u201d says Edmunds. Early Arabic manuscripts, only a fraction of which have so far been studied, are promising to be fertile ground for such discoveries. Charette also hopes the new Antikythera reconstruction will encourage scholars to take the device more seriously, and serve as a reminder of the messy nature of history. \u201cIt's still a popular notion among the public, and among scientists thinking about the history of their disciplines, that technological development is a simple progression,\u201d he says. \u201cBut history is full of surprises.\u201d In the meantime, Edmunds' Antikythera team plans to keep working on the mechanism \u2014 there are further inscriptions to be deciphered and the possibility that more fragments could be found. This week the researchers are hosting a conference in Athens that they hope will yield fresh leads. A few minutes' walk from the National Archaeological Museum, Edmunds' colleagues from the University of Athens, Yanis Bitsakis and Xenophon Moussas, treat me to a dinner of aubergine and fried octopus, and explain why they would one day like to devote an entire museum to the story of the fragments. \u201cIt's the same way that we would do things today, it's like modern technology,\u201d says Bitsakis. \u201cThat's why it fascinates people.\u201d What fascinates me is that where we see the potential of that technology to measure time accurately and make machines do work, the Greeks saw a way to demonstrate the beauty of the heavens and get closer to the gods. \n                     Science in culture \n                   \n                     Antikythera mechanism Wikipedia entry \n                   \n                     The Antikythera mechanism research project \n                   \n                     National Archaeological Museum of Athens \n                   Reprints and Permissions"},
{"file_id": "444022a", "url": "https://www.nature.com/articles/444022a", "year": 2006, "authors": [{"name": "Ehsan Masood"}], "parsed_as_year": "2006_or_before", "body": "Islamist political parties are taking over from secular ones across the Muslim world. What does this mean for science at home and scientific cooperation with the West? Ehsan Masood investigates. The full  Islam and Science special  is available from news@nature.com.  At Peshawar University on the Grand Trunk Road linking Pakistan, India and Bangladesh, there is much talk of growth. Its national centre for excellence in geology is to get 11 new labs, a library and a new museum. The provincial government, moreover, has handed the university the job of running a botanical garden and a 40.5-hectare national park. Peshawar is the capital city of Pakistan's northwest frontier province, the border region with Afghanistan where the Taliban first emerged among the Afghan refugee population in the 1990s. None of the university's activities is unusual for a leading institute in a developing country. But what might seem surprising to outsiders is that, after many years of neglect, the university's expansion comes at a time when local people have elected an alliance of political parties which, like the Taliban, want to base most laws on the Koran. Unusually for Pakistan, the current provincial government has forbidden male doctors from attending to female patients and has banned music on public transport. The university is run by Haroon Rashid, a professor of chemistry who was appointed vice-chancellor in January 2006. In common with the majority of Pakistanis, Rashid is a Muslim, something that he is proud to make known. Could a university vice-chancellor in Peshawar be of any other faith? In today's Peshawar, a non-Muslim vice-chancellor would be next to impossible. Pakistan, along with the Islamic Republic of Iran and Sudan, has been run by governments that put Islam at the centre of politics for many years. As more Muslim countries give their citizens the right to vote, Islamist political groupings have taken power, or form the main opposition, in national or regional assemblies in Iraq, Kuwait, the Occupied Palestinian Territory, Bahrain, Egypt, Afghanistan, Jordan, Morocco, Malaysia and Turkey. Islamist is a term used to denote those committed to the application of Islamic principles and Islamic law in politics. What can Muslim scientists expect from the new Islamist parties that are seeking power across the Muslim world? Will there be more support for science and for research infrastructure, as in Peshawar, but an environment where basic freedoms continue to be denied? The mostly secular, although undemocratic, regimes that have hitherto ruled for decades across the Muslim world have rarely paid more than lip-service to investment in science and technology. Consequently, today's Muslim states barely register on indices of research spending, patents and publications, and only Turkey has universities in the global top 500. Much of this is candidly documented in the four volumes so far of the  Arab Human Development Report  from the United Nations Development Programme, written entirely by Arabic-speaking social and natural scientists (see  page 33 ), which lays bare how knowledge-based activities such as science, innovation, book publishing, art and literature in Arabic-speaking countries are among the weakest in the world. The report does not consider non-Arab member states of the 57-strong Organization of the Islamic Conference (OIC), such as Indonesia, Pakistan and Turkey. But, as the data on  page 26  show, the picture in the broader Muslim world is not much better. The situation for Muslim science has been bad, and one assumption, based on current trends, is that things can only get worse. One fear is further restrictions on freedom of expression. Political leaders in the Muslim world, even in countries run on strict secular lines, are famously intolerant of dissent, as last year's attempted prosecution in Turkey of Orhan Pamuk, this year's winner of the Nobel prize for literature, demonstrates. Pamuk was accused of insulting Turkishness. Even today, few universities enjoy much autonomy, and appointments to research posts are opaque and prone to corruption. If secular governments did little for science, can Islamist ones be any worse? In the search for answers, Egypt's Muslim Brotherhood is a good place to start. The grandparent of Islamists, the brotherhood is a political party founded in Egypt in 1928. Its original aims included taking power, opposing Western influence in Egyptian politics, and governing using the Koran as the basis for lawmaking. \n               Mixed message \n             The party's presence and influence has expanded across the Muslim world \u2014 from the Middle East to Africa and Asia. In the absence of basic infrastructure in many countries, the brotherhood and its sister organizations run schools and hospitals, and its members include many scientists. But officially it does not exist \u2014 it is banned everywhere, and membership can be punishable by long spells in prison. To avoid censure its members stand as independents at election time, or as members of alternative parties. In Egypt, 88 brotherhood members of parliament together form the largest grouping after that of the government. Kamal El Helbawi, who now lives in London, is a one-time senior official in the Muslim Brotherhood, and its former spokesman in Europe. In common with, arguably, most Muslims, Helbawi sees science and Islam as being in harmony, and he says that any government led by the Muslim Brotherhood will reverse decades of underinvestment in R&D. Is this a rose-tinted view or a genuine commitment? The answer may depend on the resonance of science and technology with the wider debates occurring in Muslim society. It may also depend on whether Islamist parties lean towards the Shia or Sunni schools of thinking (see  'A long tradition' , page 24). For Helbawi, science has three functions in society. First, it is a set of tools to help humankind enjoy a higher quality of life through new technologies or by solving problems that afflict the poor. Second, science and technology can be used to deter aggression, a justification, Helbawi believes, for developing a nuclear deterrent. And third, Helbawi believes that science has a role in strengthening religious belief. In his view, the Koran, in addition to being the word of God, was designed by God to convince doubters of the truth of Islam and of creation. \u201cI urge all scientists to read the Koran, from which they will learn much about so many scientific topics,\u201d he says. Like many Islamists, Helbawi peppers his explanations with quotes from the Koran. He does so to underline that these are not his opinions \u2014 they have divine endorsement. For example, in explaining support for a nuclear deterrent he quotes chapter 8, verse 60. \u201cHence make ready against them whatever force and war mounts you are able to muster, so that you might deter thereby the enemies of God.\u201d Listening to Helbawi, it seems that although science investment may go up, the space to disagree with the official line will go down. Yet within the brotherhood itself, there is much debate on literalism, reason and rationality, suggesting that totalitarianism is not the only option. Among the rationalists, for example, is Tariq Ramadan, a philosopher of religion at the University of Oxford and the maternal grandson of Hassan Al Banna, the Muslim Brotherhood's founder. Ramadan says that the Koran should not be quoted outside of its religious and historical context. He also worries that Helbawi's literalism amounts to an invitation not to think, and to assume, for example, that if all science is contained in the Koran, there is no place in society for new knowledge. For Muslim societies, a literal interpretation of the Koran would present as many barriers to science and to freedom of thought as did the secular governments of the past. But the picture becomes more nuanced the closer one looks at Islamist governments once they are in power. Using Sudan, Pakistan and Iran as examples of countries where Islam is prominent in politics and which may foreshadow what may follow elsewhere, certain trends are clear. In the case of Iran and Pakistan, there has been a substantial expansion in higher education and more spending on research, measures to improve scientific quality, and some opening up of labs to scientists from overseas. Iran's university population has swelled from 100,000 in 1979 to 2 million today. Pakistan's university population has increased from 276,000 in 2001 to 423,000 in 2004. Sudan's public-sector universities, too, increased from 5 in 1989 to 26 in 1996. In each country, there are equal numbers of women and men entering many faculties. Indeed, in Iran some 70% of science and engineering students are women. This university expansion is, however, creating its own tensions as the economies are not large enough to absorb so many new graduates, particularly women. \n               Call to arms \n             Second, each country has directed funds towards military R&D, money that could, for example, have been spent on R&D towards alleviating poverty. Why the neglect of the poor? For many Islamists, achieving independence from Western nations, defence and national security are higher priorities than the Islamic duty to care for society's poorest. Iran, like Pakistan, insists on maintaining a capability to enrich uranium to weapons grade. Egypt and Turkey also both recently announced plans to develop nuclear power. Abdul Qadeer Khan, former director of Pakistan's nuclear programme, was a keen proponent of spreading nuclear technology to other Muslim nations. He is now under house arrest in Islamabad for selling uranium-enrichment technology to Iran, Libya and North Korea, A third trend suggests that Islamist governments are likely to restrict academic freedoms as much (if not more) than the secular regimes they want to replace. Saudi Arabia, Sudan, Iran and Pakistan are very restrictive environments for certain kinds of researchers, especially social scientists, to work in. Research into the role of government in public life, for example, requires governments to open up to the research community \u2014 something that these countries do not do. Because of this, the field of science and technology policy in all four countries is weak or non-existent. Although academic freedom continues to be limited in Muslim countries, the field of Islamic theology is rife with debate and disagreement on many science-related topics. Moreover, thanks to cable television (in particular the Al Jazeera channel based in Qatar) and the Internet, this debate is beginning to be seen in public as never before. One keenly contested area for theologians is that of the ethics of new technologies. Another is evolution. Islamic opinion on bioethics varies widely, and different countries regulate in different ways. But on this issue, as others, public debate is not as free as it is in more open societies. Although theologians and scholars of religion debate among themselves, it needs a brave lay person or scientist (who is also conversant with theology) to challenge them in public. Where do the differences in opinion lie? Saudi Arabia (an Islamic monarchy) and Iran, for example, have very different ideas on medical ethics. Saudi Arabia bans third-party  in vitro  fertilization on the grounds that sex and procreation is limited to husbands and wives. But third-party sperm donors are allowed in Iran because the alternative (a couple splitting up if they cannot have children) is considered worse for society. Similarly, Pakistan is practically alone in the Muslim world in banning organ donations from cadavers. This is because the country's Islamic authorities view the human body as being on loan from God, and when a person dies, the body needs to be returned to its creator close to its original state. But this view is not shared by other Muslim states. \n               Freedom to think \n             How literally they interpret the Koran will clearly influence how the new Islamist governments regulate science and technology. One of the Muslim Brotherhood's leading thinkers, the Egyptian scholar Yusuf Al-Qaradawi, who now lives in Qatar, is controversial in the West, but has mass support in the Arabic-speaking world, as well as among Muslims in Europe and North America. His book  Priorities of the Islamic Movement in the Coming Phase  (Awakening Publications, Birmingham, Alabama, 2002) is in effect a manifesto for the next wave of Islamist governments. At one level, Qaradawi is a literalist in that he regards every word of the Koran as the word of God, which he sees as applicable for all times to come. But he also understands that an environment that supports critical thinking was one hallmark of Islam's golden age of scientific development (see  'Islamic era science' ). Significantly, he has recently moved closer to philosopher Ramadan in his belief that Islamist governments should encourage self-criticism, that they should learn from failure, and that they have a duty to protect freedoms, including academic freedom and the freedom of any citizen to disagree with the state. \u201cWe want scientific thinking and the scientific spirit to guide our life in every way,\u201d he says. \u201cIt is against the scientific way of thinking to oversimplify complicated issues, or to view difficult problems with an alarming superficiality. Belief to us Muslims is not against reason or intellect.\u201d Qaradawi is concerned that Islamist opposition movements are too literalist and are not doing enough to encourage independent thinking using reason, known in Arabic as  ijtihad . \u201cMy worst fear for the Islamic movement is that it opposes free thinking for its followers and closes the door to  ijtihad ,\u201d he says. \u201cIf my fear turns into reality, then capable minds that can renew and innovate will escape from our ranks, leaving behind those conservatives who can only imitate and who would like everything to stay as it is, regardless of how ancient it is.\u201d  Ijtihad  is sometimes called Islam's forgotten pillar. To others, it poses a threat to Islam by weakening its teachings. Islamists have a reputation for looking inwards and shutting out the outside world, but they can look west when they need to, says Abdelwahab El Affendi of the University of Westminster's Centre for the Study of Democracy, in London, and chronicler of the rise of Islam in Sudanese politics. \u201cIslamists that come to power on the back of 'we-don't-need-the-West' rhetoric end up becoming more pragmatic,\u201d he says. Some Islamic thinkers are reaching out to the West in surprising ways. The prominent Turkish writer and columnist Mustafa Aykol has creationist views and publishes translations of US proponents of intelligent design. He has been building alliances with US faith-based groups such as the Discovery Institute in Seattle, Washington state. In an article for the US  National Review  last year he wrote: \u201cIntelligent Design can be a bridge between these two civilizations. Muslims are discovering that they share a common cause with believers in the West.\u201d In the late nineteenth century, Darwin's  On the Origin of Species  had a favourable reception in Muslim countries. But that is history, as books, pamphlets and films on creationism are now more popular in Muslim countries, and pro-evolution scientists are afraid to speak out. Adults in Turkey, for example, are even less accepting of evolution than are those in the United States. Nick Matzke of the National Center for Science Education, a not-for-profit organization based in Oakland, California, has debated intelligent design with Aykol in a Muslim online forum \u2014 a first for all concerned \u2014 but he thinks that Aykol's enthusiasm for the United States is unlikely to be reciprocated. American conservatives, he says, are not about to reconsider their views on Islam any time soon. \u201cI find it peculiar that Muslims are adopting a doctrine from US groups that regularly bash Islam in a fairly vicious way,\u201d he says. At Peshawar University, meanwhile, vice-chancellor Rashid is looking to increase direct links with foreign universities, having concluded an agreement to carry out teaching and research jointly with the University of Leicester, UK; the city of Leicester has a large British Asian population. Excellence in teaching, research and creative endeavour are the highest priority, Rashid says. But for him, Peshawar University's ultimate aim has to be a higher one. This is: \u201cto love and serve the entire creation of the creator\u201d. See Editorial,  \n                     page 1 \n                   . \n                     Muslim council phases in lunar calendar \n                   \n                     Science in the Arab world \n                   \n                     Science in culture: The zenith of Islamic science \n                   \n                     Turkey's evolution \n                   \n                     Turkish rectors rally in support of university head thrown in jail \n                   \n                     Iran's long march \n                   \n                     Iranian neuroscience: The brains trust of Tehran \n                   \n                     Women at work \n                   \n                     Islamic science: Rebuilding the past \n                   \n                     Time for 'enlightened moderation' \n                   \n                     Good and bad in Pakistan \n                   \n                     Arab science: Blooms in the desert \n                   \n                     Helping hands for Arab science \n                   \n                     Academies wrestle with issue of Islam's flagging science base \n                   \n                     Pakistan's plutonium \n                   \n                     AHDR reports homepage \n                   \n                     Islamic online debate between Aykol and Matzke Part 1. \n                   \n                     Islamic online debate between Aykol and Matzke Part 2. \n                   \n                     US National Library of Medicine exhibit on Islamic Culture and the Medical Arts  \n                   \n                     1001 Inventions: \n                   \n                     Islamic-World Academy of Sciences  \n                   \n                     Science pages of islamonline.net. \n                   Reprints and Permissions"},
{"file_id": "443390a", "url": "https://www.nature.com/articles/443390a", "year": 2006, "authors": [{"name": "Virginia Hughes"}], "parsed_as_year": "2006_or_before", "body": "Hagia Sophia has stood four-square in Istanbul for more than 1,500 years. Virginia Hughes finds out how this venerable building has resisted the area's numerous earthquakes. Istanbul is a city in motion. In just the past four decades, its population has exploded from 2 million to 10 million people, with migrants flowing in from the countryside for work. Most of them now live in multi-storey concrete structures, thrown up in haste and without concern for earthquake resistance. That's a problem, because Istanbul is also literally in motion \u2014 perched as it is near the North Anatolian fault (see map below). In 1999, a magnitude-7.4 earthquake killed some 18,000 people and destroyed more than 15,000 buildings in and around Izmit, a town lying 100 kilometres to the east 1 . If such an earthquake were to strike the capital \u2014 and that's worryingly likely \u2014 twice as many might die, suggests a recent analysis led by Mustafa Erdik, an earthquake engineer at Bo\u01e7azi\u00e7i University in Istanbul 2 . And some 40,000 buildings could be destroyed. Yet one of Istanbul's most famous buildings has already survived 15 centuries of earthquakes. It is a 55-metre-high, brick-and-mortar domed construction \u2014 Hagia Sophia. For decades, historians have debated how the building has withstood such seismic stress, and whether its architects planned it that way. Now, computer models and chemical analyses are providing clues to fuel these long-running debates. Were the Byzantine builders inventing new technologies, or just lucky? And when Istanbul is hit by the next big one, will Hagia Sophia collapse \u2014 or be the only thing left standing in the rubble? \n               Being flexible \n             Hagia Sophia has endured in a land notorious for geophysical, political and religious instability. When the Muslim Ottomans invaded in 1453, they transformed it from a Christian basilica into a mosque; in 1935, the Turkish government secularized it by turning it into a museum. The original building got its start in  AD  532, at the command of Emperor Justinian I. Meaning 'holy wisdom' in Greek, Hagia Sophia was the first structure to combine the rectangular plan of traditional basilicas with the central dome of imperial buildings such as the Pantheon in Rome. Justinian spared no expense; the church cost 145,000 kg of gold (worth US$3 billion today) and is one of the most expensive structures ever built. And who better to build what was then the greatest church in the world than the two greatest experts of the time: Anthemius of Tralles and Isidorus of Miletus. \u201cAnthemius was the best military engineer that Justinian had,\u201d says Ahmet \u00c7akmak, a professor emeritus in earthquake engineering at Princeton University. \u201cIsidorus was the director of the biggest scientific academy in the world,\u201d he adds. \u201cIt's like you hired Oppenheimer to build your house.\u201d \u00c7akmak, who spent his childhood visiting Hagia Sophia, has always been fascinated by earthquakes. At 23, he left Turkey to teach at Princeton in New Jersey, and by the time Istanbul hit its population boom a decade later, he was chair of the engineering department. But it wasn't until the mid-1980s that the theoretically minded \u00c7akmak turned his attention to practical studies of the monument from his childhood. At the time, his engineering colleague Robert Mark was creating computerized models to show how buildings bear their loads. Mark's work explained how medieval builders' observations of mortar cracks probably led to the invention of flying buttresses 3 , for example. \u00c7akmak encouraged him to make the same kind of studies of Hagia Sophia. \u201cThe technology was just beginning,\u201d Mark recalls. Together, they made computer models that could simulate how the church moved under various conditions, such as earthquakes. They have been arguing over the results ever since. Today's Hagia Sophia is a hodgepodge of domes, buttresses, supporting walls and minarets, added at various times in the name of religion or restoration. But, as the computer models show, the church's structural strength comes from its original square core. \u201cVirtually all domed structures before this time were essentially domes on cylinders,\u201d says architectural historian Rabun Taylor of Harvard University in Cambridge, Massachusetts. In contrast, Hagia Sophia is built on the crowns of arches, which support the dome and then extend to piers that form the corners of a square. \u201cThat was new,\u201d says Taylor. Also new were Hagia Sophia's pendentives \u2014 the concave triangular sections of brick and mortar that made smooth structural transitions between the curved tops of the four arches and the bottom of the dome. But how did Anthemius and Isidorus plan their structure, before the discovery of calculus or Newton's force laws? Although engineers and historians disagree about the extent of their knowledge and innovation, most agree that the architects must have relied heavily on simple geometric ratios, and on the example of existing buildings such as the Pantheon. \n               Cracking idea \n             Until Mark modelled the Pantheon's dome in the late 1980s, most historians believed the windows in Hagia Sophia's dome were added solely for visual effect. There are 40 of these windows, one between each of the ribs of brick and mortar that support the 31-metre-diameter dome. Mark's research suggested that the windows were added to avoid cracking 4 . \u201cThey knew from looking at the Pantheon that that region would want to crack anyway along the axis of the windows,\u201d he explains, \u201cso they used windows to, in a way, put the cracks in themselves.\u201d That would have helped in seismically active Constantinople, as the city was then known. Hagia Sophia has stood, sustaining only partial damage, through more than a dozen major earthquakes. Part of this success can be attributed to its arches and pendentives, which when shaken distribute the dome's weight equally among the four underlying pillars. But the very bricks and mortar that make up the church have also helped. Modern earthquake-resistant buildings are constructed to be light and flexible, in order to withstand shaking. Hagia Sophia was both, far ahead of its time. Its bricks are much lighter and more porous than the bricks used elsewhere in the empire in that period. \u00c7akmak's studies have shown that the bricks must have been baked at relatively low temperatures, less than about 750 \u00b0C, to get the right reaction between sand and lime 5 . \u201cIf it becomes higher than that,\u201d he says, \u201cthe sand becomes glassy and dense.\u201d The original builders also used a special kind of mortar. Working with his colleagues at the National Technical University of Athens, Greece, \u00c7akmak has found that the mortar contains a calcium\u2013silicon compound similar to that used in today's Portland cement 6 . The high tensile strength of this mixture has allowed the church to absorb the shaking from large earthquakes, he says. In 2002, the team reported that, amazingly, after 1,500 years the calcium and silicon in the mortar could still react 7 . \u201cThe mortar cures itself,\u201d he explains. \u201cAfter each shaking, there are microcracks, which are healed over time.\u201d (Mark, for his part, argues that the mortar's consistency may vary throughout the building, and thus cannot explain everything.) The ratio of brick to mortar used in the building may also contribute to its strength. To build faster, the masons used extremely thick mortar joints, often thicker than the bricks themselves. The thick joints make the material \u201cmore like reinforced concrete\u201d, says \u00c7akmak \u2014 although the technique is only possible with such a strong mortar. Did the builders include such earthquake-resistant features intentionally? Before Hagia Sophia, says \u00c7akmak, architects simply created very heavy buildings if they wanted them to survive earthquakes. But Anthemius was interested in the topic and even, reportedly, built his own earthquake-simulation machine. \u201cHe realized that the forces in a dynamic system are proportional to mass,\u201d argues \u00c7akmak. \u201cSo his concept of using lighter brick and flexible mortar instead of stone makes good sense.\u201d But Mark disagrees with his friend, coming down on the side of most architectural historians. \u201cAhmet thinks that they had knowledge that I don't think they had,\u201d he says. \n               Old reliable \n             Such queries aside, many wonder how Hagia Sophia will fare in the next great earthquake \u2014 the region just south of Istanbul is expected to experience two tremors of equal or greater magnitude to the Izmit earthquake in the next few decades 8 . In 1991, a team of Turkish and US researchers fitted Hagia Sophia with several vibration sensors. From data gathered during tiny earthquakes, \u00c7akmak created three-dimensional computer simulations that could predict how the building might move during a large earthquake 9 . The model shows that when hit by a magnitude-7.5 tremor, the walls of Hagia Sophia will tremble and sway dramatically back and forth. The tops of its arches will feel the most stress. But the dome will remain unscathed, and the church will stand. And if the earthquake is greater than magnitude 7.5? \u201cIf it's greater than that, there's very little we can do,\u201d \u00c7akmak says. Indeed, if the worst earthquake predictions come true, Hagia Sophia \u2014 standing or not \u2014 will be the last thing anybody is worried about. \n                     Quake threat rises after tsunami slip \n                   \n                     Earthquakes in focus \n                   \n                     Earthquake Engineering at Bogazici University \n                   \n                     Seismicity in Turkey \n                   Reprints and Permissions"},
{"file_id": "444543a", "url": "https://www.nature.com/articles/444543a", "year": 2006, "authors": [{"name": "Trevor Stokes"}], "parsed_as_year": "2006_or_before", "body": "Research suggests that consuming soil may have more health implications than one might expect. Trevor Stokes sieves through the reasons why people include dirt in their diet. At Calabash, a West African speciality grocery store in Newark, New Jersey, plastic containers line the impulse-purchase aisle next to the cash register. Hungry shoppers can choose from a wide range of traditional edible offerings, including sacks of caffeine-rich kola nuts, packets of vanilla sugar \u2014 and thumb-sized rolls of chalky clay. In such markets, edible clay isn't a strange thing to find. Joyce Corletey, a Ghanian customer in the store, says she ate fire-cured clay, known as  shra , during two of her pregnancies. The dirt became a food substitute that gave her the illusion of eating, without the fear of vomiting food during morning sickness. Corletey says she doesn't eat clay for medical reasons. \u201cIt doesn't give you energy, it doesn't make the baby grow faster, it doesn't make the baby healthy,\u201d she says. \u201cIt doesn't have any benefit, it's just clay.\u201d Several recent scientific studies bear her out \u2014 but others suggest otherwise. Anthropologists and biologists have long tried to explain geophagia, a practice named after the Greek words for earth-eater. Some researchers say that eating clay helps supplement a person's diet with much-needed minerals. Others argue that the stomach's acidity would remove any of the nutritional benefits. Still others have been searching for any potential evolutionary advantage to the practice, which falls under the general term of pica \u2014 the consumption of non-nutritive substances ranging from paper to cigarette butts. \n               Dirty past \n             Despite these recent studies, geophagia has received relatively little attention from the research community. \u201cIt's one of those topics that, paradoxically, everyone sees as being important, but nobody actually has it as their specialized area of research,\u201d says Jeya Henry, a human nutritionist at Oxford Brookes University, UK, and one of the editors of the forthcoming overview book  Consuming the Inedible 1 . Records of geophagia exist in all corners of the world, stretching back to 1800  BC  in Sumeria, Egypt and China, according to Rudolph Reinbacher, a historian in California 2 . For instance, at least 2,000 years ago, he says, Greek markets sold  terra sigillata , clay specially minted into \u201chealth\u201d coins that had supposed medicinal properties. \u201cPeople must have felt there was something to it,\u201d he says, \u201cso they ate it.\u201d What that 'something' is remains in dispute. According to one leading theory, the dirt serves as a sort of multivitamin. Clay, after all, consists of a lattice of silicon dioxide and aluminium oxide \u2014 similar to pill filler \u2014 mixed with trace minerals such as calcium, iron and zinc. In the 1990s, nutrition researcher Susanne Aufreiter of the University of Toronto, Canada, decided to test this 'nutrition hypothesis'. Her team analysed clays that people had eaten during the latter part of the twentieth century in various parts of the world: in China, eaten during famines; in North Carolina, eaten for general health; and in Zimbabwe, eaten to treat diarrhoea. Each of the clays contains levels of trace minerals that could supplement a poor diet, Aufreiter says 3 . Earth eating might also provide other benefits, she adds. Vegetables wouldn't have been scrubbed the way they are today, and Aufreiter argues that residual earth would have added roughage to the diet. \u201cWe didn't evolve with this wonderful hygienic life that we have. But chemical analyses on clay can miss the nuances of the digestive system through which clay moves. Digested clay may not only hold on to trace elements, but may actually leach them from intestinal tracts, other work suggests. At Kingston University in Surrey, UK, a team led by soil scientist Peter Hooda created a slurry of clay and simulated gastric acids and nutrient solution, and incubated it at body temperature to simulate conditions in the stomach. The team found that nutrients can become tightly bound to the lattice structure of clay particles, resulting in lower levels of iron, zinc and copper in the slurry 4 . And the higher the acidity, the more tightly the metals became bound to the clay \u2014 bad news for the nutritional hypothesis. Eating soils rich in iron and zinc, it seems, may actually worsen malnutrition, other research suggests. At Cornell University in Ithaca, New York, nutritionist Sera Young has been trying to unravel whether geophagia or malnutrition comes first. Working on the African island of Pemba, off mainland Tanzania near Zanzibar, Young has been studying pregnant women's perceptions of iron-deficiency anaemia. During her field-work in 2001, Young asked women in Swahili what caused anaemia. One Pemban woman responded that eating dirt from the walls of her home caused anaemia. Young thought she had misunderstood the answer, as Swahili was new to her; but no, the woman pointed and confirmed that her dirt walls caused anaemia. Other Pembans claimed the opposite \u2014 that eating earth during pregnancy actually treated their anaemia, rather than caused it. Intrigued, Young began searching for other cases. Certain that a search of PubMed, the medical journal archive, would clear up the underlying cause of geophagia, she says: \u201cIt just was not that simple.\u201d One of the biggest difficulties is that geophagia embraces several facets of science. \u201cYou need to dabble in soil science, you can't shy away from ethnography, you have to have some epidemiological skills and some biochemical knowledge,\u201d she says. \u201cAnd that's a tall order.\u201d Young and her PhD adviser, nutritional scientist Rebecca Stoltzfus, hope to meet the challenge. In an unpublished epidemiological study of 2,500 Pembans, women who told the researchers that they ate clay had significantly lower levels of haemoglobin, and thus were anaemic. It's not clear yet, though, whether geophagia caused the anaemia, anaemia caused the geophagia, or if a third factor is causing both of them. To test this last option, Young plans to look at the elemental and mineralogical profiles of the clays, to see if there are other bioavailable minerals that could be affecting the women. Some hints may come from work performed as far back as the late 1950s, which suggested that zinc deficiency may cause geophagia. Ananda Prasad, now a professor at Wayne State University in Detroit, Michigan, encountered several young Iranian men who suffered from stunted growth and slower sexual development. One 20-year-old man physically resembled an eight-year-old boy. All the patients ate clay every day. But when given zinc, they matured sexually and lost their desire to eat clay 5 . Plants absorbing nutrients from soil is different from humans absorbing nutrients from soil. Ananda Prasad Decades of subsequent work by Prasad and his colleagues showed that a lack of zinc leads to hypogeusia, a taste-diminishing disorder. Prasad reasons that hypogeusia makes dirt if not enjoyable, at least not unattractive, which reinforces the geophagia, leaches more zinc and starts the cycle all over again. And he also doesn't agree with the nutrition hypothesis: \u201cSoils contain nutrients, that's true, that's how we get our nutrients from plants,\u201d he says. \u201cBut plants absorbing nutrients from soil is different from humans absorbing nutrients from soil.\u201d \n               Ground rules \n             In perhaps the only randomized, controlled study to see whether anaemia causes geophagia or vice versa, Mbiko Nchito from the University of Zambia and her colleagues have found that supplementing the diet of geophagiac, anaemic children with iron did not stop them eating earth 6 . The geophagic children were also more often infected with hookworm, which causes anaemia; but that alone could not explain all the geophagic cases, as several non-infected anaemic children also ate clay. Similarly, Young's work among Pemban women shows that the clays they eat contain no detectable levels of hookworm. So the simple explanation that hookworm-riddled clay causes nutrient deficiencies doesn't seem to pan out. The question remains: what possible evolutionary benefits could arise from eating clay? To address this question, Young has joined forces with Paul Sherman, a social behaviourist at Cornell University in Ithaca, New York, and Julius Lucks, a biophysics graduate student at Harvard University in Cambridge, Massachusetts. They scour popular and scientific literature for first-hand accounts of pica, in order to build up a comprehensive database of the circumstances under which it occurs. To date, they have 1,738 such accounts. Sherman had previously studied morning sickness as a potential evolutionary adaptation, and looks at geophagia the same way. Clay could serve to help detoxify the body during pregnancy, he argues, by binding to plant toxins such as solanin in raw potatoes or nicotine in tobacco. Morning sickness is most prevalent during the first trimester of pregnancy, which is also when geophagia most commonly shows up. \u201cSo putting two and two together,\u201d he says, \u201cperhaps there is some advantage in terms of detoxifying plants with strong secondary compounds to protect the fetus against carcinogens and mutagens.\u201d Losing nutrients to clay, Sherman argues, is probably negligible in terms of nutrition compared to the cost of throwing up the same toxins. So far, the group's preliminary results support the idea that those most susceptible to toxins \u2014 young children and women in the early stages of pregnancy \u2014 most commonly practise geophagia. And, consistent with clay's possible detoxification role, researchers have observed that both men and women eat dirt to treat 'gastrointestinal stress', presumably cases of food poisoning. Over-the-counter stomach remedies, such as Kaopectate and Maalox, contain clay-like ingredients that settle the stomach. According to Sherman's findings, dirt-eaters self-medicate with every chunk of clay they chew. Back at the Calabash grocery store, Joyce Corletey picks a tiny corner of clay off a roll and gnaws it matter-of-factly. \u201cYou just put it in your mouth so that you have something in there,\u201d she explains. But once she was no longer pregnant, her experiment with clay ended. \n                     Evolutionary biology: Dirty eating for healthy living \n                   Reprints and Permissions"},
{"file_id": "444539a", "url": "https://www.nature.com/articles/444539a", "year": 2006, "authors": [{"name": "John Whitfield"}], "parsed_as_year": "2006_or_before", "body": "Understanding the trade-offs involved for plants making leaves promises fresh insights on every scale from the plant to the planet, finds John Whitfield There are about 250,000 different types of plant \u2014 and almost as many types of leaf, from blades of grass through downy beech leaves to the needles of cedars or the fronds of palms. But the differences hide a basic similarity, says ecologist Ian Wright of Macquarie University in Sydney, Australia. \u201cWithin any habitat, you'll find that each square centimetre of leaf will process a roughly similar amount of carbon per unit area over its lifespan.\u201d Wright and his colleagues have discovered that most of the variation in the physical and biochemical properties of leaves can be represented on a single axis running, to put it crudely, from quick and juicy to slow and tough. They call the axis the 'worldwide leaf economics spectrum' 1 , and it embodies many of the trade-offs that govern how plants deploy their resources within the limits that physics places on biological possibility. According to the Thomson in-cites website, the work is the second-most cited paper on ecology and the environment published in the past two years. It has attracted the attention of everyone, from plant physiologists studying how leaves work to biogeochemists looking at the cycling of nutrients on a global scale. In part, the paper is so popular because of the size and scope of the database that underlies the work; but the popularity also reflects the intellectual excitement that surrounds the discovery that so much can be explained by so little. This has given some ecologists hope that by looking at the large-scale patterns in how organisms work, they can gain a general understanding of why species live where they do, and why some are common and others are rare. Such findings are not of purely academic interest: climate researchers are using them to improve their models of the consequences of global warming. \n               From rainforest to tundra \n             The spectrum was discovered by compiling and analysing a database of the leaf biology of 2,548 species from 175 locations, ranging from tropical rainforests to the Arctic tundra. For every species, six key traits were recorded: leaf mass per unit area (affected by a leaf's thickness and tissue density); nitrogen and phosphorus content; the lifespan of the leaves; and the rates of photosynthesis and respiration. Wright and his colleagues compiled the data from their own and other people's studies \u2014 \u201cmuch of it was languishing in bottom drawers\u201d, he says. When the database was analysed statistically, its first principal component \u2014 the most informative way of looking at all the traits using a single axis \u2014 turned out to account for 74% of all the variation in all six traits. This principal component is the worldwide leaf economics spectrum. It ranges from leaves that are cheap to manufacture, highly active (ie, they have a high nitrogen content and photosynthetic rate) and short-lived, to those that are expensive and less active but make up for that in durability. In terms of an English woodland, think of going from flimsy short-lived beech leaves to tough old holly. One definition of economics is the study choice under the constraint of scarcity, and the narrow range of choices in the leaf economics spectrum provides a vivid illustration of the various scarcities that dominate plants' lives. The fact that all leaves lie fairly close to the axis of the spectrum shows that, despite the vast diversity of foliage produced over hundreds of millions of years of evolution, plants have little room for manoeuvre in how they build their leaves. \u201cMost textbooks of ecology project the idea that there's an almost infinite diversity of organisms,\u201d says plant ecologist Philip Grime of the University of Sheffield, UK. \u201cBut if you look at the core biology of what organisms do with resources, you find severe constraints and trade-offs.\u201d One tack taken in following up on this ground-breaking work has been to ask what the constraining trade-offs are. When Wright and ecologist Bill Shipley of Sherbrooke University in Quebec, Canada, used a mathematical technique that can find the direction of causality in the correlations 2 , they found that \u201cthere was no way to explain the correlations based only on the measurements in the data set\u201d, says Shipley. Some not yet measured factor was at work. \n               The missing link \n             Shipley thinks that the missing fundamental trade-off is between the volume of a leaf's cell and the thickness of its cell wall. Leaves at the tough end of the spectrum have cells like \u2014 well, like cells: small spaces surrounded by thick, solid walls. Cell walls require a lot of materials to make and do not photosynthesize. Sturdier cell walls make the leaves more opaque and less permeable to gases, hindering the capture of light and carbon dioxide. But these leaves also live longer, and the investment in the cell wall is thus paid off with a low rate of photosynthesis that can be maintained for a very long time. Another way to take the work forward is to include some aspects of a leaf's size and shape. The factors measured in the spectrum are usually expressed per unit mass. But Charles Price of the University of Arizona, Tucson thinks that a leaf's absolute size is also important. He has sought to explain how a leaf's surface area and mass are linked in terms of the geometry of the vein network that carries resources to, and away from, its cells 3 . Large networks supply cells more slowly: this, he predicts, will cause large leaves to photosynthesize more slowly, per unit area, than small ones, and might be another reason why leaves with a large mass per area photosynthesize more slowly. Perhaps the most striking aspect of the trade-offs to be investigated further, though, is that different plants in the same places make very different 'decisions'. \u201cThe dogma was that plants differed between places that are dry and wet, and those that have low and high levels of nutrients,\u201d says Wright. \u201cBut frequently, the range of trait values within any one habitat is as large or larger than that seen between sites.\u201d This illustrates, he says, that the various ways of making a leaf are equally good routes to the same place. Variation within sites will also result from small-scale changes in soil, shade and moisture, and from the fact that a plant's best strategy depends on what everything else is doing \u2014 in a wood full of fast-growers, tolerating shade might be better than trying to compete directly. Climate, though, does play a part in the shape of the spectrum. In the coldest places, doubling the leaf mass per area lengthens the lifespan by five times, whereas for the hottest sites, the same doubling lengthens lifespan by only 2.5 times. About 20% of the variance not captured in the basic leaf economics index is explained once climate is added to the equation 4 . \n               Onwards and upwards \n             The implications of a leaf's-eye view go beyond the plants themselves. Leaves are the gateways through which energy enters ecosystems, and choices made on the leaf economics spectrum affect detritivores, herbivores and, indirectly, carnivores. Plants with short-lived, rapidly photosynthesizing leaves are less tolerant of self-shading, and so have broader, flatter canopies than those with tougher, slow-growing leaves, such as conifers, which tend to grow more compactly. And the litter from plants that are fast-living decomposes more quickly than the tough foliage of the slow-coaches, controlling the speed at which nutrients cycle through the ecosystem. This sort of thinking is bolstering some ecologists in their belief that the study of traits is central to many aspects of their discipline 5 . To try to understand things such as what determines the number of species that can coexist in a place, how numerous each species is, and how productive the system as a whole is, ecologists have traditionally looked at what species are present, how they interact, and how their abundance affects that of the others. Such an approach is an extension of ecology's roots in natural history, says Brian McGill of McGill University in Montreal, Canada. \u201cPeople become ecologists because they love to go outdoors and look at the woods. They get attached to putting names on things, and get focused on knowing lots about particular organisms.\u201d But this approach soon becomes intractably knotty, as the number of possible interactions between species rises geometrically with the number of species. \u201cWe don't have the capacity to learn as much as we need to know by studying one species at a time. Studying interactions between species, and then trying to build that up, hasn't panned out. It's too complicated,\u201d explains McGill. Seeing ecological communities in terms of their constituent traits, rather than their species, might be less intuitive. But it makes measuring and comparing places a lot easier, says McGill. The trick is to find traits that are closely related to evolutionary fitness. The leaf economics spectrum is a prime example of this. Another example is the spectrum between plants that make a lot of small seeds and those that make a few big ones. Wright is now looking for other important axes, such as between plants that use water profligately to grow quickly, and those that are slow-growing but intolerant to drought. Once researchers think they have identified an axis of variation, they can see, for example, whether plants cluster tightly at one point on the axis, suggesting that they have evolved similar responses to a common environmental challenge, or whether they spread out along its length as they do on the leaf economics spectrum, suggesting that they have adopted strategies to avoid competing with one another. In October, Shipley and his colleagues published a study that tested how predictive such an approach might be 6 . Working in 12 French vineyards abandoned 2\u201342 years ago, they looked at the ways the traits of the plants change as the vineyards revert to an uncultivated state. As perennials replaced weedy species, the height and mass of the plants increased, and the number of seeds per plant dropped. The leaves of the perennials also had a greater mass per area, in a shift towards the slow/tough end of the economic spectrum that seemed to go along with the community's ageing. By looking at plant traits, you can go from physiology up to ecosystem functioning. Bill Shipley From a knowledge of the average trait values for all 30 species looked at in the study and the age of each field, the researchers then predicted the abundance of each species in each vineyard by borrowing a technique from statistical mechanics and assuming that resources were spread between the species as randomly as possible. They predicted their number of species in each plot and their abundance with 94% accuracy. \u201cBy looking at plant traits, you can go from physiology all the way up to ecosystem functioning,\u201d says Shipley. \u201cThe potential that I see is in finally being able to integrate different levels of plant ecology together.\u201d A species'-eye view might still reveal things about ecosystems that looking at traits cannot, says David Tilman of the University of Minnesota in St Paul. Plots with more species, for example, are more productive, and recover more quickly from perturbation than less diverse places 7 . Part of this trend is that more species mean a greater diversity of traits, Tilman says, but interactions between species are also important. \u201cLooking at traits won't let us ignore the number of species. Both are equally important determinants of how an ecosystem functions.\u201d \n               Predicting the future \n             But the role a trait-based approach could play in prediction is exciting some of the scientists studying the greatest environmental problem of the age at the largest possible scale\u2014the planet-wide effects of global warming. A wide array of feedbacks ties the terrestrial biosphere into the changes in the atmosphere's carbon dioxide content. Making sense of these relationships requires models that predict what plants to expect where, on the basis of environmental factors such as temperature and water availability. Today's models typically classify vegetation as belonging to one of 5\u201320 groups \u2014 evergreen, deciduous, woody, herbaceous and so on. Trait values, such as leaf longevity and photosynthetic rate, are then assigned to this group as a whole. This summer, researchers working on plant traits and those working on the interaction between plants and climate met in Sydney to try to see how a 'trait-first' approach to modelling might improve things. Either we watch the world change, or we gain a truly predictive knowledge of nature. David Tilman It turns out that models that replace a fixed trait value assigned to each vegetation type with a spread of values taken from the leaf economics spectrum give values of plant productivity closer to those seen on the ground. They should thus yield more accurate predictions of future change, particularly at a regional or continental scale, says modeller Ian Woodward of the University of Sheffield. \u201cLooking at Amazonia, we tend to find that incorporating these data into our models leads to changes in leaf longevity and primary productivity of about 10\u201315%. That's a significant difference, and an exciting eye-opener,\u201d he says. In general, says Tilman, if we are to understand and mitigate the effects of processes such as climate change, nitrogen deposition, species introductions and habitat loss, ecology needs to home in on the most important ways in which species differ. \u201cThese broad global patterns greatly simplify what we need to understand about species. They let us sort through a hopeless morass of information, and go from conceptual models to real models that say how ecosystems will respond to environmental change,\u201d he says. \u201cIf we can't be mechanistic and predictive, we'll be unable to provide society with explanations of the impacts of our actions. Either we watch the world change, and explain it in retrospect, or we gain a truly predictive knowledge of nature.\u201d \n                     Biology's big idea \n                   \n                     Complex systems: Order out of chaos \n                   \n                     Plant science: Tall storeys \n                   \n                     Ecology: Neutrality versus the niche \n                   \n                     50 Years of DNA \n                   \n                     Ian Wright's homepage \n                   \n                     Bill Shipley's homepage \n                   \n                     Brian McGill's homepage \n                   \n                     QUEST \n                   Reprints and Permissions"},
{"file_id": "442862a", "url": "https://www.nature.com/articles/442862a", "year": 2006, "authors": [{"name": "Apoorva Mandavilli"}], "parsed_as_year": "2006_or_before", "body": "Can everyone use technology creatively? Engineers at the Massachusetts Institute of Technology think so and have launched 'Fab Labs' around the world to prove it. Apoorva Mandavilli reports. Neil Gershenfeld has been teaching a class called \u201cHow to make (almost) anything\u201d to some of the brightest young adults in the United States for years. But it took an eight-year-old girl in a small village in Ghana to show that anyone, anywhere, really can make just about anything. One evening in June 2004 \u2014 the day after Gershenfeld had left Ghana having taught a week-long version of his class in the village \u2014 little Valentina Kwofie began cobbling together a circuit board. Hours passed. Several times Kwofie's parents stopped by the lab Gershenfeld had set up, imploring, \u201cValentina, let's go home, let's have dinner, let's go to bed.\u201d It was the first time anyone in the village, Takoradi, had made a circuit board: people crowded around, watching her nimble fingers manoeuvre parts half the size of a grain of rice. Finally, long past her bedtime, she crafted a board that worked. Gershenfeld, director of the Center for Bits and Atoms at the Massachusetts Institute of Technology (MIT), hadn't known what to expect when he put the fabrication laboratory, or 'Fab Lab', technology that he worked with at MIT into the context of rural Africa. What he got was inspiration. \u201cThis eight-year-old girl in Ghana was making microcontroller circuit boards for the love of it, for the joy of discovery,\u201d he recalls. \u201cThat ordinary people can do it and want to do it was very surprising.\u201d The Fab Lab in which Kwofie made her circuit has been followed by others around the world, each equipped with the same key machines. Today, there are ten Fab Labs \u2014 above the Arctic Circle in Norway, in Costa Rica, India and South Africa \u2014 and within the next year, fifteen others are planned, of which five will be in Africa. Together, these labs are showing that giving people the ability to make things for themselves can be the fastest way to solve their problems, particularly in communities with little access to education or technology. \n               It began in Boston \n             Gershenfeld began teaching \u201cHow to make (almost) anything\u201d at MIT in 1998 as part of an outreach requirement attached to his centre's grant from the US National Science Foundation (NSF). It quickly became the most popular class he taught. Students with no prior technical skills, he says, created the \u201cmost wildly expressive things\u201d: a web browser for bored parrots, for instance; a container for screams; even a 'defensible dress' that sprouts wires when someone invades its wearer's personal space. Watching the students' passion for teaching others what they had learned, Gershenfeld toyed with the idea of starting a lab somewhere that was far removed from MIT in its expectations about technology. So he opened a Fab Lab in working-class South Boston. Heartened by this experience, he decided to start going far afield geographically as well. He put together a collection of industrial machines, including a laser cutter that makes two-dimensional and three-dimensional objects from non-metal materials, a vinyl cutter that can make flexible antennas, among other things, and a high-resolution milling machine for circuit boards. All are run by simple computers using open-source software written by MIT scientists. This kit today costs about $25,000; materials cost another $10,000. The first labs, like the class that inspired them, were funded by the NSF grant. \u201cTechnical empowerment, problem-solving, invention \u2014 that wasn't at all the agenda,\u201d he says of the first few labs. \u201cAt my end, it wasn't much deeper than curiosity about what would happen.\u201d One of the things that happened was that each new lab took on a character of its own. Whereas the MIT students made fanciful gadgets, many of the developing-world communities seized the opportunity to implement practical ideas they already had but had not been able to realize. In Ghana, for example, villagers built large 'collectors' to harness solar energy and made machines to grind cassava into fufu powder. In Pabal, India, farmers created sensors, based on a design developed by MIT staff, to measure the fat content in milk and help them get a fair price. Farmers in Norway created wireless radios to help them track sheep. Some labs are on the verge of spinning off small businesses. \u201cFabs are now moving from experiments to becoming serious tools in addressing developing nations' problems,\u201d says Sushil Borde, executive director of the non-profit Fab Foundation, who has helped launch Fab Labs in both India and South Africa. \u201cI really feel that the deepest form of aid that you can give a country is to allow them to design and make their own things,\u201d says Hod Lipson, a mechanical aerospace engineer at Cornell University in Ithaca, New York. \u201cYou're letting them choose what to make. That's an amazing empowerment.\u201d \n               A machine that can make anything \n             Lipson and Gershenfeld are collaborating on the Fab@home project. Their aim is to create a cheaper and more accessible version of existing Fab Labs by scaling things down to just one machine that can make all sorts of things. This machine will fill space with droplets of matter as an inkjet printer fills a page with droplets of colour: a Fab Lab-in-a-box. This sort of equipment, mostly used for prototyping, is not new and has become steadily cheaper. Most versions work with only one material, however, so Lipson and others are working on printers that can make complex objects from an array of polymers. They have already made simple items, including a battery, from five different materials. The Fab Lab in Pretoria, South Africa, is the first to have one of the single-polymer '3D printers', but the goal is to make more of them for other Fab Labs, for schools and homes. \u201cOne of the nice things about this printer is that you can make it entirely in a Fab Lab, so it's really scalable,\u201d notes Lipson \u2014 although a printer alone couldn't make another printer. The self-replicating Fab Lab has yet to materialize, but Gershenfeld and Lipson dream of the day such personal fabricators are as common as a personal computer. Others are not so sure. \u201cIt's hard to believe that everybody would want one,\u201d says Paul Wright, chief scientist at the Center for Information Technology Research in the Interests of Society at the University of California, Berkeley. Wright teaches a class similar to Gershenfeld's, and his students too have been creative \u2014 making, among other things, a pillow alarm clock that can rouse the sleeper without waking a partner or roommate. But, says Wright, \u201cnot everyone wants to build things. One has to be realistic.\u201d More likely, he says, is that high schools or community groups would want personal fabricators. \n               From Norway to South Africa \n             MIT fields almost daily requests to build new Fab Labs. The ones that make the cut are inevitably driven by passionate local champions. The Norway lab, for example, is led by Haakon Karlsen, a charismatic chief herder, who helped it grow from a room in a barn to a beautiful 11-building Fab Village that has had 4,000 visitors since it opened last year. The village has support from the Norwegian government and local industrialists, and is set to launch a few local businesses. One of the newest labs is in Soshanguve, a poor township about 45 kilometres north of Pretoria, and is run by a local group, the Bright Youth Council. When the MIT researchers arrived in Pretoria in September 2005, they found that the Soshanguve group already had its own printing centre to help community members with their resum\u00e9s. The group had pooled money to buy a computer and those who knew how to use it taught everyone else. Convinced by the council's enthusiasm, Amy Sun, a graduate student at MIT, set up a Fab Lab between the printing centre and the health clinic with support from South Africa's Department of Science and Technology. Many who come to the clinic stop by out of curiosity. When I visited the lab on a warm afternoon in mid-February, just two weeks after it had opened, it was humming with nearly 20 people. About a dozen Youth Council members, with an average age of 25, work there. Because of the demand, the day is organized into two-hour slots allotted to different age groups \u2014 and that doesn't mean just boys and men. One participant was a pregnant 18-year-old high-school dropout. Another was a 40-year-old housewife who had never seen a computer before the lab opened but is now using one to design decorative cutout designs. People at the Soshanguve lab have made a light switch controlled by a cellphone, a motion-sensor light and an alarm system \u2014 useful devices in the many unsafe neighbourhoods in the area. Some choices are surprising. \u201cThere are boys there at 1 a.m. stringing beads cut on the laser cutter and really liking it,\u201d Sun says. A lab can be up and running within a week of delivery, but the tough part is helping people make things. Sun spends much of her time teaching physics and electronics to people with no knowledge of either who don't speak English. \u201cPointing helps,\u201d she says, \u201cI show by example.\u201d In places such as South Africa, where there are 11 languages and a severe teacher shortage, the labs rely heavily on students to show the next set how to use machines. \u201cThat turns out to be a really powerful tool for Fab Labs,\u201d she says. And why not? There is little else for these students to do. In many African countries, there are not many jobs for people with these mid-level skills. At one of Rwanda's few technical schools, for example, secondary-level students \u2014 about a third of whom are orphans of the 1994 genocide \u2014 earn a three-year diploma in topics such as automotive mechanics, electronics or construction. But of the 105 students at the Gitarama school who graduated last year, says the principal, Mark O'Kane, only 20 have found jobs, and 8 of those are with the school. School officials are now talking to Rwandan companies to try to train more employable students. Gershenfeld has something similar in mind. A virtual Fab Academy could equip students with skills that would make them more attractive to local companies, he says, particularly in countries with growing technology sectors such as India and South Africa. Local companies interested in employees with certain skills could also sponsor a lab. A handful of companies, including Honeywell India's training centre at Madurai in Tamil Nadu, and Neuron Bio of Pune, Maharashtra, are negotiating support for new labs there. Raising funds to maintain the labs will be the project's biggest test. Infrastructure challenges occur at every level, says Sun, from renting a truck to move the machines and keeping the labs stocked with consumables, to ensuring electricity or Internet access, and providing technical support if a machine breaks down. Lab staff are usually either volunteers \u2014 akin to a \u201ctechnology peace corps\u201d, says Gershenfeld \u2014 or shared with other community projects. But each lab still costs as much to run as a small business in the host country. So far they have been supported by a mixture of funds from MIT, local governments and grassroots groups. But as the number of labs grows, Gershenfeld has struggled to convince traditional development agencies to finance them. He now has interest from several private donors, however, and is launching the Fab Foundation to match these donors to labs and coordinate the growing network so that existing labs can learn from each other and new ones can grow more easily. It's already possible, he points out enthusiastically, for a Fab Lab to make buildings and furniture, and so \u201cwe're at the point of physically making the lab in a Fab Lab,\u201d although new machines would still have to be shipped from MIT. \n               Into business \n             After much debate, the Fab Labs have together decided that anything made in one lab should be available to the others. In this way, the focus is less on intellectual property \u2014 which is hard to protect in many developing nations \u2014 and more on making the business model suit the people doing the work, the invention and the lab's location. In theory, an invention from one lab could be made by several others, essentially creating a global collection of local businesses. Gershenfeld hopes to help by providing small amounts of capital from a 'Fab Fund'. The main expectation, he says, is that there is a return for the community. But ensuring that return is the tricky bit. Even when they have good ideas, \u201ctaking them to the next level takes a lot of business understanding\u201d, notes Behrokh Khoshnevis, director of the Center for Rapid Automated Fabrication Technologies at the University of Southern California in Los Angeles. \u201cCompeting with industry is a big, big challenge.\u201d Khoshnevis says that for the Fab Labs, \u201cthe focus should be on education more than anything else\u201d. Kohshnevis can appreciate their value better than most in the West: as a nine-year-old in Iran, he couldn't afford fancy toys and built himself a four-storey polystyrene building equipped with a working elevator the size of a cigarette box, complete with doors and lights. If there had been a Fab Lab to teach him, he says, his inventions could have matched his imagination. So far the labs have excelled at recreating the outreach experiment that got them started. The Fab Village in Norway has been so successful at engaging the community that there are plans to start others in Iceland, Sweden, Finland and Russia. The lab in Ghana is similarly set to help launch labs in Kenya and Rwanda. There is more to this story than philanthropy, though: the innovatory echoes of the outreach project have bounced back to MIT. Again, it was an eight-year-old girl who showed the way: Gershenfeld's daughter, Grace. Last year Gershenfeld was showing her how to use a laser cutter in the MIT lab, but Grace thought the shapes he was cutting out were boring. Instead she made a cardboard 'Lego kit' of two-dimensional pieces that snap together to form three-dimensional objects. Playing with it, Gershenfeld realized the flat shapes let him build space-filling parts, \u201cThat hadn't occurred to us as a way to make stuff at MIT.\u201d He and his colleagues are now testing ways to scale this approach up or down, by using parts of various sizes and in different materials. \u201cThere are now up to four grad students developing the work of an eight-year-old kid,\u201d says Gershenfeld. As Karlsen says, \u201cWe are clever people wherever we live. You see that we can do anything.\u201d \n                     Making progress? \n                   \n                     Do it yourself \n                   \n                     Fab central \n                   \n                     Interview with Neil Gershenfeld \n                   \n                     Edge.org article \n                   \n                     SciDev.Net R&D Dossier \n                   Reprints and Permissions"},
{"file_id": "443265a", "url": "https://www.nature.com/articles/443265a", "year": 2006, "authors": [{"name": "Amanda Haag"}], "parsed_as_year": "2006_or_before", "body": "Interdisciplinary research is the new buzzword, but does a grounding in different disciplines really make you better at solving problems? Amanda Haag joins an experiment to find out. Even the bastions of academia are no longer immune to reality television. In late August, 48 doctoral students arrived in the resort village of Snowbird, Utah, for a collaborative weekend of solving environmental problems \u2014 and found themselves the subject of a social-science experiment uncannily like MTV's  Real World . First gasps and uneasy silence, then nervous laughter spread through the Cliff Lodge conference room when students learned that their hotel-suite working areas were equipped not only with laptops, Post-its, and flip charts, but also with microphones, a video camera, a voice recorder and a silent observer. Within hours, the students were ensconced in groups of six, setting out to define a pressing environmental question, design a study and write a grant-worthy proposal to solve it \u2014 all in two and a half days. Giant sheets of scribbled notes, flow charts and diagrams soon plastered every available surface, from mini-bars to bathroom doors. Science, it seemed, could progress even with a video camera watching. \u201cIt was a little Big Brother with the cameras and everything, but you get used to all that,\u201d says Nicole Czarnomski, a PhD student in water resources engineering at Oregon State University in Corvallis. Others initially felt a bit like lab animals. \u201cI kept thinking, 'Don't cross your arms, that means you're not open to an idea!'\u201d says marine ecologist Suzanne Olyarnik of the University of California, Davis. Being open to ideas was, after all, part of the point of spending a weekend brainstorming different approaches to environmental problem-solving. But the students were also an experiment in themselves \u2014 part of a social-science study to investigate interdisciplinary education. By sorting students according to whether they came from traditional, single-discipline graduate programmes or the new wave of interdisciplinary research, the organizers aim to learn what creative processes drive scientific thought today. The National Science Foundation (NSF), which funds a graduate interdisciplinary programme, is not interested just in producing scientific knowledge, says Ed Hackett, a sociologist at Arizona State University in Tempe: \u201cThey're increasingly interested in understanding how it's produced.\u201d The impact of Hurricane Katrina on the US Gulf Coast \u2014 the first anniversary of which occurred as the students set to work \u2014 is a classic example of the problems that occur when human social dynamics, natural processes and complex environmental systems collide, says Diana Rhoten of the Social Science Research Council in New York City. As a final stage of their four-year project on the nature of scientific collaboration, she and Hackett designed the weekend as a 'charrette', a nineteenth-century concept now used to describe a short burst of intense problem-solving. \n               Mission: flexible \n             The charrette began more like a movie than a television show \u2014 with students finding packets in their working suites, \u201ca bit like  Mission: Impossible \u201d, says Hackett. Each packet contained not a scientific challenge, but an open-ended research design with a few defined parameters. For instance, the students might have to propose a research question that would cost roughly $10 million over five years and support a full team of scientists. How the students defined the question would be as closely scrutinized as the way in which they went about solving it. The questions had to deal with the intersection between human activities and ecosystem services, as defined by the 2005 Millennium Ecosystem Assessment. Each question hinged on comparing two places with different levels of human activity, such as urban New Orleans and the less developed Euphrates River in the Middle East. A suitable subject might be a region where economic growth and human health depend on fishing and a clean water supply, yet where overpopulation threatens the aquatic ecosystem. Once the problem was defined, students worked as a group to develop the seed of a written grant proposal and give a 15-minute PowerPoint presentation to a panel of experts. The model for the charrette dates back to the nineteenth century, when students at the architecture school of the \u00c9cole des Beaux-Arts in Paris were asked to draft a solution to a design problem under a strict deadline. Faculty members would often hand out problems so ambitious that few students could crack them before the cart \u2014  charrette  \u2014 rolled by the drafting tables to retrieve the work, finished or not. Today, professions ranging from urban planning to graphic design use charrettes to inspire fresh approaches to problems. \n               No survivors \n             For this experiment, Rhoten and Hackett divided the group into eight teams of six students. Each was composed as nearly as possible of three ecologists plus one other natural scientist (such as an agronomist or a biologist), a physical scientist (such as a hydrologist or an atmospheric chemist) and a social scientist or policy analyst. There were four groups each of early- and late-stage students, divided further into students from disciplinary and interdisciplinary programmes. That last distinction was key. One of the charrette's goals was to determine whether students trained in traditional graduate programmes work together to solve problems differently from their interdisciplinary peers. \u201cWe wanted to create an environment in which we could say, 'if we put these guys in teams, do they interact with individuals differently from their colleagues who haven't had this intervention?'\u201d Rhoten explains. She and Hackett also wanted to see whether interdisciplinary programmes are training students to produce science that informs science policy. In 2005, 75 institutions across the country hosted 120 Integrative Graduate Education and Research Traineeships (IGERT), a National Science Foundation fellowship, to the tune of $67 million and involving around 1,800 students. But did Rhoten and Hackett expect more from the interdisciplinary students because of their experience in such programmes? \u201cI think there was a little bit of feeling at first that we're the non-IGERT people, we're from the wrong side of the tracks,\u201d says Ryan Atwell, a doctoral student in landscape ecology at Iowa State University in Ames. But Rhoten and Hackett stress that they aren't out to demonstrate the superiority of interdisciplinary programmes. \u201cThe upshot of this may not be flattering to IGERTs,\u201d Hackett says. \u201cWe have evidence from some of our fieldwork that there are some IGERT students whose disciplinary technical skills are not up to the level the discipline requires.\u201d Such problems can arise as late as when a student is submitting a dissertation. \u201cWe haven't found the right balance point between enough breadth and enough depth,\u201d he says. Rhoten and Hackett assured the students that they were not competing with anyone, either within or between groups. And they tried to play down any  Survivor -like aspects of the weekend. No one would be ejected from a group, although anyone could leave at any time. But the rules were complex. \u201cIt's like a Russian doll,\u201d remarked Chris Bail, a social-science doctoral student from Harvard, who served as one of the group observers. \u201cIt's got layers within the inner layers.\u201d Students could draw only on the resources available within their group. \u201cYou don't call your faculty, you don't call your friends, and you don't ask somebody at MIT to run some data for you,\u201d Rhoten said at the opening session. To this end, each working suite contained just two laptops with Internet access and a basic software package. The researchers plan to analyse the web searches and other information students gathered during their work. \n               Under observation \n             Each group was assigned an observer, a social-science graduate student, who would sit in a corner taking notes on the group's activities. Observers were instructed not to interact with the students, and vice versa. Every 15 minutes, the observer would note exactly what was happening in the group at that moment. Was someone making a declarative statement? Was a certain person expressing scepticism? Has the group reached a low point? Have sub-groups broken off? By analysing these notes, the researchers plan to look for what they call interaction hot spots, to be studied further through the video recordings. It wasn't easy to catch up with the students, as many were tucked away in their working suites \u2014 practically around the clock. But some took time off for cocktails, hikes up the steep slopes of Little Cottonwood Canyon and even group trips to the hot tub. So between gulps of mountain air and after-hours refreshments, students relaxed and discussed their experiences. All eight groups said that they gelled remarkably well. But many students expressed frustration at the initial stages, when they were bogged down in defining their research question. Some arrived at a consensus on the first morning. For others, the entire first day was filled with frustration over not being able to choose a particular ecosystem service or geographical area. \u201cTo be honest, when I went to bed Friday night, and when I got up Saturday morning, I wasn't excited about coming back,\u201d Atwell says. \u201cI thought it was going to be one of those group experiences where you see and respect the people you're working with, but we just don't come together.\u201d But Saturday brought a breakthrough. By then, Atwell says, \u201cwe knew who was excited about what study systems and what questions\u201d. \n               Semantic quibbles \n             When students disagreed, it was mostly over terminology. For instance, Czarnomski's group got stuck discussing what is meant by ecological resilience \u2014 a fundamental concept in ecology. \u201cWhen I think about it, I think of it in more of an applied fashion and not necessarily in the theoretical constructs that a community ecologist would,\u201d Czarnomski says. Florence Bocquet, a doctoral student in atmospheric chemistry at the University of Colorado in Boulder, says that her group also quibbled over semantics. \u201cPeople were explaining the same concept using their own words, phrasing the idea differently, and re-explaining the idea,\u201d she says. \u201cI did not say, 'what you guys are saying is the same darn thing, so let's move on'. But I really wanted to.\u201d In the end, most groups settled on proposals involving land-use change, water-resource management, and patterns of urbanization. One presentation, \u201cThe American Dream \u2014 Have a Lawn and Eat it, Too\u201d, looked at how suburban development is straining the ability of surrounding ecosystems to provide the services, such as fresh water, that the communities rely on. New housing developments are being designed and marketed as having a reduced environmental footprint, although there is no comprehensive study of the true environmental impacts of individual developments. So the group proposed a comparative study between conventional and tightly grouped 'cluster' housing communities. \n               Horses for courses \n             Another group proposed comparing the Hudson River watershed in New York State and the Han River watershed in South Korea. The two basins are geographically and climatically similar, with agricultural activities and development upstream that can contaminate water downstream. The group proposed to study how human impact on the way nutrients move through the ecosystem can be taken into account in urban and rural planning. Each team presented their findings to a seven-member panel of scientists, who ranged from pure mathematicians to ocean managers. The students pulled off relatively strong proposals, most of the experts agreed. \u201cWere they NSF-quality proposals?\u201d asks panel member Ann Kinzig, who studies urban ecology at Arizona State University. \u201cNot quite yet. But did they accomplish a huge amount? Absolutely. These students only had two and a half days to work on what in some ways is a very difficult problem that senior researchers are still trying to crack.\u201d To everyone's surprise, students tackled the problem similarly, irrespective of whether they had interdisciplinary or traditional training. Rhoten and Hackett hesitate to draw conclusions before all the data are in, but both agreed that it was difficult to spot which groups were which. Rhoten says that the early observations bring into question whether interdisciplinary programmes train students to think or approach problems differently. \u201cThis is not a criticism of IGERTs,\u201d she says, \u201cbut it raises the question of whether they're actually changing students or just housing students who have these preferences.\u201d Although this is a long-standing question, few opportunities exist to test these hypotheses properly in group settings. Rhoten stresses that such a finding would not diminish the importance of programmes such as the IGERT. \u201cWithout the IGERT, perhaps those currently enrolled in them would have a graduate school experience that did not respond to, support or accommodate their learning preferences,\u201d she says. \u201cThat would be a huge loss.\u201d \n               Crossing the line \n             Temporary forays into interdisciplinary work, such as immersion in a charrette, may be valuable in extending the opportunity to more students without the need for multi-year fellowships. Steve Gaines, a marine ecologist from the University of California, Santa Barbara, who was not involved in the study, teaches a week-long course dealing with the interface between science and policy. \u201cIt's dramatic how students change in just a week,\u201d he says. \u201cSo I'm not surprised at all that you can have a lot of progress over the course of a couple of days in an intensive exercise like this.\u201d Knowing when and how to bring interdisciplinary work into one's career is a question for many researchers. Kinzig notes that many scientists feel strongly that students should become expert in one discipline before crossing boundaries. But, she adds, \u201cI think we have an increasing number of students who aren't that interested in being disciplinary. I think if I had had to focus narrowly within a particular discipline, I would not have finished graduate school. I just would have gotten bored.\u201d By the end of the charrette, many of the participants shared that sentiment. Ramona Walls, a doctoral student at the State University of New York, Stony Brook, was a fashion designer before she chose to become an ecologist. \u201cIt never occurred to me that I could set up a collaboration with an economist or some other social scientist, and now that I've done it, I think it's a great idea,\u201d she says. Perhaps it is in this balance between traditional disciplinary science and synthesis-driven approaches that solutions to today's environmental and social issues will be found. \u201cThere's still glory in that other kind of science that's not problem-oriented, that's just completely curiosity-driven,\u201d Kinzig says. \u201cWe can't lose sight of that.\u201d \n                     Recruiters & Academia \n                   \n                     Charrette website \n                   \n                     Social Science Research Council \n                   Reprints and Permissions"},
{"file_id": "442742a", "url": "https://www.nature.com/articles/442742a", "year": 2006, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Is targeting cancer stem cells a way to finish tumours off once and for all \u2014 or just the latest in a long line of false dawns? Alison Abbott looks at a debate that's generating both heat and light. \u201cYour cancer has returned.\u201d The words that patients most dread are the ones that most of them eventually hear. Why tumours that had been shrunk to nothing by a first round of treatment should recur has long been a mystery. Now, a growing number of researchers think it may be because biologists have overlooked a key part of how cancers grow. It's all down to stem cells, they say. Normal stem cells repair and renew our tissues. Recently, research has focused on the idea that a kind of evil twin, a cancer stem cell, could stock a tumour with cancer cells, rather like the way a weed can grow back from its roots. If the idea is right, then cancers return because existing treatments cut down the leaves and shoots but leave the roots intact. According to the theory, cancer stem cells have a physiology that makes them particularly resistant to typical treatments. They lurk for months or years, then re-seed the cancer, usually in a more aggressive, often fatal, form (see  'Why the drugs don't work' ). Find a treatment that targets the cancer stem cell, without harming normal stem cells \u2014 as researchers are already trying to do \u2014 and scientists might be able to protect against recurrence. It's a controversial idea. The theory is promising, but the field is awash with hype. Researchers observe wryly that more reviews seem to have been published than primary papers. Ideas abound about how the theory explains every aspect of cancer, and about where and how to find therapeutic targets. Hard data are emerging only slowly. Sean Morrison, a stem-cell biologist at the University of Michigan, Ann Arbor, who reported one of the first key differences between normal and cancer stem cells in May 1  remains downbeat. \u201cRemember apoptosis, remember angiogenesis?\u201d he asks rhetorically. These properties of cancers \u2014 respectively, their ability to bypass the normal cell-suicide response to damage, and their ability to create their own blood supply \u2014 were once seen as routes to powerful therapies, but have yet to live up to their initial promise. Yet whatever their clinical potential, the idea of cancer stem cells is prompting biologists to think about cancer in a new way. Many different tissues harbour adult stem cells. The cells don't normally divide much, but when prompted by death in their realms, they do so asymmetrically. One of the two daughter cells is a replica of the parent cell \u2014 the property of self-renewal \u2014 and the other is a 'progenitor' cell that goes on to differentiate into more specialized cells. Neural stem cells, for example, can replace both of the brain's major cell types, neurons and glia. Blood stem cells can replace myriad red and white blood cells. Cancer stem cells share the ability to self-renew and to spawn many different kinds of cell. They were identified in leukaemias in the late 1990s. John Dick at the University of Toronto harvested leukaemic cells from patients and found that only a small proportion could reproduce the same cancer in immune-deficient mice 2 . The ability to recapitulate the same cancer is what defines a cancer stem cell. \n               In the blood \n             Leukaemia is relatively easy to work with. You can access the cancer cells by drawing blood, and blood stem cells are well-characterized, particularly in terms of the presence or absence of specific marker proteins that define 'stemness'. Getting hold of fresh a solid tumour is more difficult, and much less is known about the relevant markers. That said, cancer stem cells have now also been demonstrated in breast 3  and brain 4  tumours. They bear convincing markers of stemness, and they can recapitulate the cancer. There are also many early reports of stem-like cancer cells in other solid tumours such as ovarian, lung and skin cancers that have yet to meet all the criteria of proof. \u201cThere is no question in my mind that cancer stem cells will prove to be the rule rather than the exception,\u201d says Michael Clarke, of the Stanford Comprehensive Cancer Center. It's just a question of time to get the results, he thinks. On the other hand, oncologist William Hahn of Harvard University \u2014 who describes himself as \u201ca sceptical observer who likes the idea of cancer stem cells\u201d \u2014 is more circumspect. \u201cThere is no real molecular definition of what makes a cancer stem cell,\u201d he points out. The search for compounds that selectively kill these cells will only speed up when the molecular details are known. And the details are still sketchy. At the moment it is not even certain that a cancer stem cell derives directly from a normal stem cell, although this will probably often be the case. Cells become cancerous when they have accrued a critical number of mutations in genes in the interacting signalling pathways that control growth, allowing the cells to evade the normal mechanisms that restrict growth. Unlike many cells, stem cells live long enough to acquire the required changes. But researchers have shown that a non-stem cell that picks up a mutation giving it the ability to self-renew might also develop into a cancer stem cell 5 . Different cancers probably contain cancer stem cells of different origin. But the origin of cancer-initiating cells is less important to those seeking therapeutic targets than finding molecules in them that they do not share with tissue stem cells. Given that so little is known about marker molecules in any sort of stem cell, at the moment it's a bit like trying to do a crossword without all the clues. But a couple of groups have identified two molecular signalling pathways that are regulated differently in the two stem-cell types. And these present the first leads for potential therapies targeting cancer stem cells ( see graphic ). \n               Flower power \n             Craig Jordan, a molecular biologist at the University of Rochester, New York, has identified a molecular signalling system that is permanently switched on in leukaemia stem cells but not in blood stem cells. He found that the molecule parthenolide, the main active ingredient in feverfew, a herbal medicine used to treat migraines, blocks a key component of this system. This prompts leukaemic stem cells to commit suicide, whereas normal stem cells are unaffected 6 . Jordan says he hopes to have a derivative of the molecule in clinical trials by the end of the year. Morrison's discovery was that deleting a gene called  Pten , commonly inactivated in human leukaemia, stimulates the growth of leukaemia stem cells but inhibits the growth of normal blood stem cells. And he found that the drug rapamycin reversed the effect of the gene deletion in mice, leaving the cancer stem cells unstimulated, and reactivating the growth of blood stem cells 1 . Rapamycin is used clinically as an immunosuppressant to prevent transplanted organs being rejected, and several variants happen to be in clinical trial for cancers. His group's findings are sure to affect how the trial results are interpreted, says Morrison. Other pathways that are often aberrantly activated in cancer may or may not be differently regulated in normal and cancer cells. These pathways have been pillars of mainstream cancer research for years, and are now acquiring a stem-cell spin. Two of the many contenders are the  Wnt  and  Sonic hedgehog  pathways which, in their healthy manifestations, play a key role in embryonic development, stem-cell renewal and tissue repair. \u201cThere is no proof that  Sonic hedgehog  is involved in cancer stem cells but my intuition is that it will be,\u201d says molecular biologist Michael Dean of the National Cancer Institute in Frederick, Maryland. He is already looking for candidate drugs, initially using cyclopamine, a molecule found in the poisonous lily  Veratrum californicum . Cyclopamine interrupts  Sonic hedgehog  signalling in prostate, gut and brain cancers. The hard core of cancer-stem-cell researchers view this tendency to hang a new collar on an old dog with caution. \u201cThere are a lot of hypotheses, but you have first to distinguish between stem cells that generate tumours and those that do not,\u201d says Morrison. Also getting a cancer stem-cell spin are the 'drug pumps'. These membrane proteins, which have long been therapeutic targets, remove drugs and other foreign molecules from a cell. They are believed to be a major cause of the resistance that cancer cells develop to drugs that initially shrink tumours. \n               Resistance cell \n             Stem cells are assumed to be built like fortresses, with multiple mechanisms to protect them from toxic environmental agents, so they don't replace tissue with damaged goods. Cancer stem cells might also be packed with drug pumps, explaining their presumed extreme resistance to therapies. Some suspect the unexpected occurrence of resistance to Gleevec \u2014 the first anticancer agent targeted to a specific signalling-pathway component, called BCR\u2013ABL \u2014 could be down to cancer stem cells' ability to pump the drug out. Dean, for example, is screening compounds in the hope of targeting such drug pumps in cancer stem cells. A different approach to hitting cancer stem cells is to make highly specific antibodies that can seek out and kill them. This requires a deep understanding of the proteins found only in cancer stem cells, which antibodies could target. It seems a tall order, but a number of biotech companies are trying to develop such methods. Outside the search for therapeutic targets, established themes of cancer are also being recast in the light of the stem-cell concept. Could it for example, give insights into metastasis, given that normal stem cells can relocate to repair distant tissue? Could it explain why chronic injury frequently results in cancer, as in such circumstances stem cells are called upon to continuously replace tissue and thus increase their chances of acquiring mutations? Even those scientists who fear that the cancer stem cell may prove to be a sidetrack don't believe it will be a dead-end \u2014 simply because such a lot of cancer biology is being reconsidered, and new things uncovered, en route. And if the concept performs beyond expectation, then the root-killing generation of therapeutics will emerge to ensure permanent remissions, to protect patients from deadly cancer recurrence. \n                     Cancer: New fronts in an old war \n                   \n                     Cancer: Caught in time \n                   \n                     Cancer: Off by a whisker \n                   \n                     Stem and cancer cells have something in common \n                   \n                     Cancer stem cells produce brain tumours \n                   \n                     Stem cells web focus \n                   Reprints and Permissions"},
{"file_id": "442735a", "url": "https://www.nature.com/articles/442735a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "Like so many wars, the war on cancer can cause despair, with final victory sometimes seeming inconceivable. In the 35 years since President Richard Nixon declared his battle on everybody's enemy, the immense progress in scientific understanding of the disease has yet to deliver the crushing blow that was hoped for. Indeed, the problem seems to have become less tractable, not more so. The concept of cancer as a single disease has been replaced by a view of it as many different ones, each with its own molecular process, each requiring its own therapeutic approach. Thanks to advances in molecular biology, therapies targeted at specific disease processes are continually being developed. But many believe they are likely to roll back the cancer mortality figures slowly, rather than making a quick and dramatic impact. If we leave the military analogies aside, we can see that there are possibilities for progress \u2014 not for final victories, but for saving lives in the fairly near future, and developing new approaches to stubborn problems. This special section of  Nature  looks at three of these new angles on cancer research and what they promise to contribute. One area where science could improve survival rates is by finding ways to detect cancers early. As Laura Spinney explains on  page 736 , existing treatments are far more effective when delivered early, and technologies to detect disease earlier than ever before are becoming available. The problem is choosing the right signs and understanding what they mean. Even if we catch cancers early, are the drugs targeting them correctly? A new theory suggests that they could be missing something, says Alison Abbott (see  page 742 ). A small subset of cells, called cancer stem cells, could re-seed tumours after treatment. Researchers are trying to develop drugs aimed at cancer stem cells, in the hope of preventing the recurrence of tumours \u2014 the leading cause of cancer deaths. New drugs need to be tested. One long-standing criticism of cancer research is that about nine out of ten drugs that look good in preclinical tests fail to fulfil that promise in human trials. Many point the finger at the mouse, the stalwart animal model of cancer, for being too mouse-like to be a good model for humans. But that is changing, says Carina Dennis (see  page 739 ), as researchers find ever-more ingenious ways to engineer mice to be more like us. \n                 For more on cancer see the Editorial on  \n                 \n                     page 720 \n                   \n                 , and visit our web focus at\u00a0\u2192  \n                 \n                     http://www.nature.com/nature/focus/cancerhorizons \n                   \n               \n                     Cancer: Caught in time \n                   \n                     Cancer: Off by a whisker \n                   \n                     Cancer: The root of the problem \n                   Reprints and Permissions"},
{"file_id": "442865a", "url": "https://www.nature.com/articles/442865a", "year": 2006, "authors": [{"name": "Kendall Powell"}], "parsed_as_year": "2006_or_before", "body": "Changes in the structure of children's brains may account for some of the risky business of adolescence, Kendall Powell finds. The 14-year-old has a very simple decision to make. When he sees a light out of the corner of his eye he is supposed to ignore it and keep looking straight ahead. It seems extraordinarily easy \u2014 even eight-year-olds can do it correctly half of the time \u2014 but it requires reigning in a natural impulse to look. And every parent of a teenager knows that reigning in impulses is not their strong suit. In this simple test, the teenager performs as well as adults do. But a peek inside his head reveals that he puts a lot more work into it. His brain uses a whole host of frontal regions \u2014 those involved in planning and executing actions \u2014 that adults ignoring something in their peripheral vision just don't need. \u201cThe adolescent brain is acting like an adult brain doing something much more difficult. An adolescent can look so much like an adult, but cognitively, they are not really there yet,\u201d says Bea Luna, a neuroscientist at the University of Pittsburgh Medical Center in Pennsylvania. It is her brain scans that have revealed this tendency for teenagers to 'overuse' their frontal brain regions when stopping themselves from looking at the light 1 . Work such as Luna's begins to explain why a teenager can behave maturely on Monday and do something unutterably foolish on Tuesday. Neuroscientists probing the teen brain have found that it undergoes a major remodelling that may be responsible for the teenager's propensity to take risks, seek out new experiences and fail to restrain inappropriate responses. Their work suggests that, even before you add raging hormones and peer-group-driven rebelliousness-without-a-cause to the mixture, teenagers may simply be unable consistently to make decisions the same way adults do. This could well be one of the reasons that, although most people are healthier during their adolescence than at any other time in their lives, adolescents are three or four times more likely to die than children past infancy 2 : they take risks, have accidents and pay the prices. The teenage years turn out to be a complicated time in the brain, with cells fighting it out for survival and the connections between different regions being rewired and upgraded. Some abilities, such as quashing offensive behaviour and empathizing with others, keep maturing well into the twenties. The passage from childhood to adulthood is not straightforward: some researchers now see the teenage remodelling as analogous to the 'developmental window' that allows the brain to be moulded by experience in infancy. There are ways in which teenage brains perform quite differently from either childish or adult ones. No one is saying that a desire to yak endlessly on the phone, sneak out of the house or throw yourself off the side of a mountain on a snowboard in the gnarliest way possible is purely a function of neural architecture. But the brain changes tracked by neuroscientists offer parents, teachers and any other caring adult insights in how to help teenagers avoid too much danger. And such studies look likely to enrich scientific explanations of teenage vulnerability to depression, addiction, eating disorders and schizophrenia. By around the age of 12, a child's brain has the size, folding, weight and regional specialization of an adult's. But a decades-long study conducted by the US National Institute of Mental Health (NIMH) in Bethesda, Maryland, has shown that such a brain still has a long way to go to reach adulthood. Begun in 1991, the study has followed 2,000 people who were then aged from 3 to 25, taking brain scans every 2 years. The study uses magnetic resonance imaging (MRI) to measure the water-to-fat content of tissues: in the brain, this distinguishes grey matter, which is mostly water-filled nerve cell bodies, from white matter, which is mostly made up of the nerve connections sheathed in fatty insulation called myelin. The NIMH research team, led by Jay Giedd, has made a movie of normal brain changes from ages 5 to 20 (ref.  3 ). It reveals that the grey matter thickens in childhood but then thins in a wave that begins at the back of the brain and reaches the front by early adulthood (see graphic, below). The process completes itself sooner in girls than in boys. This corresponds to a long-held assumption that adolescence sees the prefrontal cortex regions that handle executive functions 'waking up' and to the conventional wisdom that girls mature faster in this respect. But, Giedd says, \u201cthe real message is that what is important is the journey, not the destination\u201d. His studies and others show that measuring the shape or size of a certain region in an adult can mislead: what really counts is the developmental trajectory that gets you there. Earlier this year, the team showed that a more pronounced wave of grey-matter thinning corresponds to higher intelligence 4 . \n               Use it or lose it \n             Giedd and many other neuroscientists think the grey-matter thinning seen during adolescence is probably due to 'synaptic pruning' \u2014 the process of eliminating overabundant, unnecessary nerve cell connections. If synaptic pruning is accelerated during adolescence, says Giedd, it follows that this is a time of 'use it or lose it' in the brain. The more environmental input there is to guide that pruning, he says, the better. On the same basis he argues that less guidance could result in a brain less able to react to complex situations, as could uncontrolled pruning: preliminary studies show that childhood schizophrenics have an exaggerated loss of grey matter during adolescence 5 . Giedd takes this to imply that teens should be exposed to rich environments full of sports, travel, music and foreign languages. In the past decade, most efforts to provide brains with extra stimulation focused on the first five or six years of life. This new work, says Giedd, should argue for an equally critical period of brain plasticity in adolescence. However, Elizabeth Sowell, a neuroscientist at the University of California, Los Angeles, cautions against making direct connections between brain changes and specific teen behaviours. \u201cJay likes to say 'use it or lose it' and that we should put kids in enriched environments. That makes perfect intuitive sense, but we just don't have the data to say that.\u201d Tom\u00e1\u0161 Paus, director of the Brain and Body Centre at the University of Nottingham, UK, adds a further note of caution, warning that researchers should be careful not to simplify the brain\u2013behaviour relationship into a one-way street. Behaviour almost certainly influences final brain structure, too; indeed, this is what Giedd's current research is looking at. But Paus agrees the research has shown that the \u201cbrain continues to mature way beyond the first 3 to 5 years of life\u201d. Referring to a popular pre-school 'enrichment' programme in the United States, he says: \u201cWe sorely need a Head Start programme at the beginning of adolescence. Things are not carved in stone by that age. Our field's observations show that you can induce long-lasting changes that are entered into the brain.\u201d As grey matter thins, white matter is being gained, as layers of insulating myelin are added to the axon connections between nerve cells. George Bartzokis, a neuroscientist at the University of California, Los Angeles, has found that such 'myelination' follows an inverted 'U' shape over our lifetimes, peaking at around age 50 (ref.  6 ). The teen years are on the early stages of the steep upward curve of myelination. Bartzokis see this as facilitating connections between different parts of the brain: if you want to retrieve pertinent information quickly to make a decision, he argues, you don't want a supercomputer \u201cbut rather a fast Internet\u201d. Information held in different centres of the brain has to be 'online' or retrievable, and retrieving lots of it quickly requires increased processing speed and bandwidth. Myelination provides this by increasing the speed of signals travelling along axons and decreasing the time to the next nerve impulse. \u201cTwo years from now, my daughter will be 16, and my car insurance bill will double,\u201d notes Bartzokis, even though she probably knows the rules better and has quicker reflexes than he does. \u201cBut she will still have a much higher risk of getting into an accident because she can't instantly bring knowledge to bear in a situation like I do.\u201d What we call wisdom, Bartzokis says, requires maximum myelination. Abigail Baird, director of the laboratory for adolescent science at Vassar University, in Poughkeepsie, New York, and her graduate student Craig Bennett mapped white matter in the brains of first-year undergraduates and then looked again six months later to see if anything had changed. They found subtle but significant additions: five brain regions gained white matter, including frontal areas that prepare for action and form strategies, and other areas that integrate sensory input, emotions, body state and context 7 . A control group of postdocs showed no such changes. \u201cIt's the stuff that allows you to put yourself in another's shoes or have empathy in the broad sense,\u201d explains Baird. But she adds a strong note of caution about her work \u2014 it only shows a correlation between brain development and the ability to empathize. \u201cWe can't prove that the changes we saw do anything to cause certain behaviours. It's important to remember that this is a little bit of a 'Just So' story.\u201d \n               Brain at work \n             If the adolescent brain differences were only a matter of getting the correct grey-matter-to-white-matter balance, the story might end here. But the plot thickens with the work of Luna's group, which uses functional MRI (fMRI) to measure activity in different regions of the brain by tracking blood flow. Connecting her work on the 'overuse' of the teenager's frontal lobes to Giedd's ideas, Luna associates the onset of adulthood with an integration of the frontal regions with other areas of the brain \u2014 a move from local-area networks to wide-area networks, in computer terms. Myelination is probably part of the process, but not all of it: if it were, peak brain efficiency would not be seen until late middle age. Other functional studies link changes in the brain to teenagers' increased appetite for fast cars and other dangerous thrills. B. J. Casey's group at Weill Medical College of Cornell University in New York has measured brain activity in subjects who perform a simple task and then get small, medium or large rewards for performing correctly. In adolescents given a medium or large reward, a centre in the brain called the nucleus accumbens reacted more strongly than in children or adults 8 . That looks like an exaggeratedly positive reaction. When given the small reward, the teenage accumbens response decreased below that of children and adults \u2014 as if the small reward represented no reward at all in the teen's view. Like Luna, Casey sees a more diffuse pattern of activity in the teenage frontal regions than in those of adults. A reward centre on overdrive coupled with planning regions not yet fully functional could make an adolescent an entirely different creature to an adult when it comes to seeking pleasure. \u201cIf it were just that the prefrontal cortex was lagging behind, then children would be as risk-taking as teens, and we don't see that,\u201d says Casey. She goes on to speculate that the lag between the frontal regions and the reward centre is an evolutionary feature, not a bug. \u201cYou need to engage in high-risk behaviour to leave your village and find a mate,\u201d and risk-taking soars at just the same time as hormones drive adolescents to seek out sexual partners. Linda Spear, a behavioural neuroscientist at the State University of New York at Binghamton, says that in rodents, primates and even some birds, adolescence is a time of risky business, seeking out same-age peers and fighting with parents 9 , which \u201call help get the adolescent away from the home territory\u201d. The growing evidence that increased risk-taking is wired into the adolescent brain is beginning to shift the way psychologists approach 'troubled' kids. \u201cI don't think we can fight the biology of wanting to take risks and try on different identities. Those are good things,\u201d says Giedd. \u201cBut how do you have outlets that don't give teens STDs, car accidents, drug abuse or jail? As a society, we can give kids creative, positive outlets that do not lead to irreversible mistakes.\u201d Attempts to push kids towards safe sex or pharmaceutical temperance shouldn't be expected to succeed if they simply explain consequences. \u201cAdolescents have some fundamental qualities to them that are not voluntary and not easily modified by rational, information-based interventions,\u201d says Laurence Steinberg, a developmental psychologist at Temple University in Philadelphia. \n               Can't wait \n             And for some kids 'just saying no' will be even harder than for others. Casey, in collaboration with Inge-Marie Eigsti at the University of Connecticut in Storrs, has measured the ability of 4-year-olds to defer gratification by offering them extra cookies if they could leave a first one uneaten for a set period of time. When tested 10 years later, the kids who resisted temptation as pre-schoolers were better at restraining themselves as teenagers 10 . When children are predisposed to listen to their pleasure centres \u201cand then the same system becomes elevated during adolescence\u201d, says Casey, \u201cit's a double whammy. Those are the kids we are trying to identify.\u201d Children who show early problems with focusing attention, Casey thinks, may need stronger parental guidance later. But in general she and her colleagues shy away from making specific recommendations on the legal responsibilities, parenting or education of teenagers. Instead, they tend to emphasize the newfound appreciation for adolescent behaviour their work offers. The next time your quiet reverie in a caf\u00e9 is interrupted by a gaggle of teenagers getting their after-school caffeine and sugar fix, as this writer's has just been, take a moment to remember the teenage boy in the MRI scanner, his earnest brain working overtime just to restrain a simple flick of the eyes. Hard work is going on inside those heads. \n                     Science in the movies: From microscope to multiplex - An MRI scanner darkly \n                   \n                     Early sleep marks the end of adolescence \n                   \n                     Brain imaging could spot liars \n                   \n                     Laboratory for Adolescent Science \n                   \n                     UCLA's Laboratory of Neuro Imaging \n                   \n                     NIMH Child Psychiatry Branch \n                   \n                     Society for Research on Adolescence \n                   Reprints and Permissions"},
{"file_id": "442975a", "url": "https://www.nature.com/articles/442975a", "year": 2006, "authors": [{"name": "Helen Pilcher"}], "parsed_as_year": "2006_or_before", "body": "IVF isn't something most Westerners associate with Africa. But low-cost methods are urgently needed to treat the misery of infertility rampant on the continent, says Helen Pilcher. Several years ago, Betty Chishava was thrown out of her family home in Harare, Zimbabwe, because she failed to fall pregnant and didn't want to sleep with her husband's brother. Desperate for an heir and a cure for the stigma of infertility, her husband Herbert took a new wife. Betty was left penniless and alone. Betty's story is played out in millions of homes across sub-Saharan Africa, where up to one-third of couples are infertile 1  and the pressure to produce children is immense. \u201cIn Africa, a woman's worth is defined by her fertility,\u201d says Chishava. As the years roll by and a couple's lack of children becomes all too apparent, personal tragedies turn into public humiliation and shame. Perceived as evil or cursed, a woman without a child may be beaten and is commonly ostracized by family and friends. Some risk the threat of HIV to conceive through sex with multiple partners. Some fake pregnancies and steal newborn babies. Some just can't bear it any more and take their own lives. \u201cIf you're a woman in sub-Saharan Africa and you don't have a child, you're worth less than a dog,\u201d says fertility specialist Willem Ombelet from the Genk Institute for Fertility Technology in Belgium, who has worked in Africa for more than two decades. Treatments for infertility are becoming available in the developing world \u2014 but slowly. In 1989, little more than a decade after the world's first baby conceived through  in vitro  fertilization (IVF) was born in Britain, western Africa hailed its first IVF success: a boy born at Lagos University Teaching Hospital in Nigeria. Since then a sprinkling of private fertility clinics has sprung up, offering high-tech treatments from the developed world. But the therapy is too expensive for most Africans. Initiatives are afoot to make fertility treatment more accessible. A number of scientists have proposed methods for developing simpler, low-cost alternatives to the high-tech drugs and equipment currently used for fertility treatments. And others are working to prevent the sexually transmitted infections that account for most cases of infertility. But even as campaigners strive to make these approaches a reality, they face daunting prejudice \u2014 in both Africa and the wider world. Perhaps the biggest stumbling block is the insidious conviction (in Western circles) that sub-Saharan Africa simply cannot have an infertility problem. \u201cGovernments worldwide put money into family planning in the developing world, but no one wants to focus on infertility,\u201d says Ombelet. The average couple in that region has five or six children 2 , so many governments think that being too fertile should be the focus. And they find it hard to justify expensive fertility treatments in settings with few resources that have more obvious problems, such as malaria and HIV. But scientists such as Ombelet insist that the problem needs to be addressed. \u201cInfertility is consistently overlooked in aid and development work,\u201d he says. \u201cWe have to convince the Western world that infertility in Africa is a real problem.\u201d The sad thing is that much of Africa's infertility could be easily prevented, as infections are the main cause of infertility on the continent. Infections such as gonorrhoea and chlamydia often go untreated, spreading to the reproductive tubes where they cause blockages and scarring. The acceptance of one simple thing \u2014 the condom \u2014 could change it all. But cost and cultural taboos restrict condom use. Women, most of whom depend upon men for economic security, find it hard to negotiate safe sex and difficult to refuse intercourse; those who do may risk a beating. Less than 2% of married women in Africa use condoms 3 . Bodies such as the World Health Organization have programmes aimed at improving gender equality and the availability of contraception. And results are shortly expected from clinical trials of microbicide gels, which may be effective against a range of sexually transmitted diseases 4 . \n               Botch jobs \n             Meanwhile, the unplanned pregnancies that result from a lack of contraception pose another threat to a woman's fertility. General contraceptive uptake among women remains low, averaging 23% across sub-Saharan Africa as a whole, reaching just over 50% in South Africa, and dropping to less than 10% in Nigeria 3 . All too often, deliveries and abortions turn into botch jobs that cause infection, says obstetrician and gynaecologist Osato Giwa-Osagie from Lagos University Teaching Hospital. He says this is the second biggest factor in the region's female infertility. Female genital mutilation, which is more common in Africa than anywhere else, carries similar risks. Up to 140 million women have had part or all of their external genitalia removed 5 . The practice is usually performed by traditional doctors with crude instruments and without anaesthetic. Resulting infections can spread internally. Men, of course, can also be infertile. \u201cBut it's always the women who are blamed,\u201d says Chishava. Male infertility accounts for up to 40% of cases of childless couples 6 . Men are usually born with their fertility problems, although infections are also a factor. And yet male infertility is so taboo that no one will admit it exists. Families go to great lengths to cover it up: some resort to the traditional practice of getting a husband's brother to impregnate his wife, something Chishava refused to agree to. Most men in Zimbabwe, and some other countries including Nigeria, would rather change their wife than admit to an infertile marriage, she says. \n               Cheap tactics \n             Changing age-old prejudice is going to take a while. In the meantime, scientists and doctors in Africa and elsewhere are turning to IVF and other fertility treatments to help. Africa has a more-than-respectable history in assisted reproductive technologies (ARTs). After Nigeria's IVF success in 1989, Giwa-Osagie expected African governments to increase public spending on ARTs. \u201cIn terms of technology, we were just a few years behind Britain,\u201d he says. But the people with the purse strings prioritized other health concerns, such as malaria and diarrhoea. Africa's public infertility clinics began to feel the pinch and close down. \u201cThe service became very fragmented and many couples ended up going from doctor to doctor,\u201d says Nigerian fertility specialist Richard Ajayi. Those who could afford it, travelled abroad for treatment. Ajayi and others like him turned to the private sector for funding. Venture capital secured, in 1999 Ajayi founded the Bridge Clinic in Lagos. With its recently founded sister clinic in Port Harcourt, Ajayi's clinics perform more than 500 cycles of IVF treatment a year. The Bridge Clinic is a polished outfit with state-of-the-art laboratories and new equipment; other places have more humble beginnings. In 2003, gynaecologist Edward Sali set up the Kampala Gynaecology, Fertility and Maternity Centre in Uganda. Funds were tight, so Sali turned his bedroom into the laboratory. \u201cWe had to move out,\u201d he says. \n               Clinical excellence \n             There are now more than two dozen private fertility clinics scattered across nine or more sub-Saharan African countries. Virtually all forms of ART practised in the West are available, with IVF and artificial insemination by a husband's sperm the most common. Thanks in part to collaboration with sister clinics in the developed world, success rates are approaching those seen in the West. But there's a problem \u2014 the cost. In Nigeria and other countries in sub-Saharan Africa, a single IVF treatment costs around US$2,500. But the minimum wage in Nigeria is just US$52\u201360 a month 6  and there are a great many people scraping by on a dollar a day, or less. This makes IVF and other techniques too pricy for most. Ajayi estimates that only 5\u201310% of those who could benefit from fertility therapy can afford private treatment. So what is to be done? Fertility expert Alan Trounson from Monash University in Melbourne, Australia, thinks it's time to go back to basics. He believes that the cost of IVF could be slashed by replacing expensive drugs and high-tech equipment with safe, low-cost alternatives. And he's drawing on experience from veterinary medicine, paediatrics and the early days of IVF to do just that. Normally when a woman undergoes IVF, she is first injected with hormones called gonadotropins, to help her produce more eggs. Shortly before the eggs are harvested, the woman receives another hormone injection to help the eggs mature. The eggs are then collected using ultrasound guidance, and fertilized in the lab. The work is carried out inside a sterile cabinet called a laminar flow hood, before being transferred to a humidified, gas-filled incubator where the fertilized eggs are left to divide for a few days before being implanted. \u201cOver the years, IVF has become tailored to treat the Harley Street end of the market,\u201d says Trounson. \u201cBut there's no reason it can't be tweaked and simplified to create something that is affordable and safe, with a reasonable output.\u201d Costly hormone injections could be replaced with cheaper alternatives. Most women use 30 ampoules of gonadotropin per treatment cycle, resulting in about a dozen eggs: the total cost is US$300\u2013450. But the dose could be reduced or replaced with clomiphene citrate. This drug, which was routinely used in IVF treatments in the late 1960s, also stimulates ovulation but produces fewer eggs. And 15 tablets cost about US$1. \n               Softly, softly \n             Critics of clomiphene citrate caution that it can sometimes damage the uterus lining, making embryo implantation less likely. But advocates say that the 'softer' drug is less likely to trigger ovarian hyperstimulation syndrome, a rare side effect of gonadotropin therapy that can cause diarrhoea, vomiting and breathing difficulties. In the early 1980s, Trounson managed a roughly 5% live birth rate using clomiphene citrate, and antenatal care and tissue-culture techniques have come a long way since then. Research is ongoing, but Ombelet estimates that three cycles with clomiphene citrate should be as effective as two cycles with gonadotropins. Slashing the price of drugs means nothing, however, if doctors cannot afford the equipment needed to fertilize and nurture the eggs. Western IVF laboratories are replete with technology that costs tens of thousands of dollars, but much of it can be done away with. In place of the laminar flow hood, Trounson suggests using a 'humidicrib' \u2014 a plastic box more commonly used for keeping newborns snug. It's a tenth of the price and can be modified to create a portable, near sterile environment in which to handle embryos. And instead of incubating the embryos with carbon dioxide from an expensive cylinder, Trounson recommends exhaling across the culture media before sealing it in a plastic bag, a technique commonly used in veterinary IVF. Then remove the need for an incubator by dropping the bag containing the the Petri dish into a warm water bath. Such 'submarine incubators' have been used for cow embryos for more than a decade. \u201cPeople didn't think to use it in an IVF setting because it's not seen to be sophisticated enough,\u201d says Trounson. Pilot studies are needed to assess the safety and efficacy of such low-cost IVF protocols. Unfortunately, a lack of awareness, cash and political push means this is not happening. So Trounson and others are lobbying hard to put African infertility on the international agenda. Ombelet is organizing a meeting in Arusha, Tanzania, in February 2007, where scientists, clinicians, ethicists, policy-makers and women's organizations will draw up a plan of action. And the Nigerian Fertility Society is drafting guidelines on infertility treatments, which it hopes will be accepted by its government. As awareness increases, it is hoped that government money will trickle back to fund public infertility clinics. Such low-cost IVF could help millions of women. But government support and publicly funded clinics will mean little if the social stigma is not tackled. \u201cWomen have no voice in Africa,\u201d says Ombelet, \u201cso raising the status of women may be the hardest job of all.\u201d \n               Home truths \n             That's where Chishava comes in. When she was thrown out of her family home, she realized that there was a need for female counselling and education. So six years ago, she set up the Chipo Chedu Society (meaning 'our gift' in her language). The organization aims to help childless women become financially independent, teaching them practical and business skills such as batik and bookkeeping. It puts women in touch with medical experts and fights prejudice through rural workshops on infertility. The society has more than 500 members and Chishava hopes to see her network spread across Africa. Chishava has since been reunited with her husband. His second and third marriages failed to produce children, and he gradually accepted that he was infertile. He apologized to Chishava and fully supports her work. Now 54, Betty is mother to five children \u2014 all given to her by family members \u2014 but she would dearly love to have a child of her own and is intrigued by IVF. Until that happens, Chipo Chedu is her true baby. \u201cI will only find peace of mind when the programme flourishes,\u201d says Chishava. \u201cIf my neighbour has no children, then their problem is my problem.\u201d \n                 See also Editorial, page 957. \n               \n                     Health effects of egg donation may take decades to emerge \n                   \n                     Drug firms donate compounds for anti-HIV gel \n                   \n                     Starting to gel \n                   \n                     World Health Organisation \n                   Reprints and Permissions"},
{"file_id": "442978a", "url": "https://www.nature.com/articles/442978a", "year": 2006, "authors": [{"name": "Jacqueline Ruttimann"}], "parsed_as_year": "2006_or_before", "body": "The rising level of carbon dioxide in the atmosphere is making the world's oceans more acidic. Jacqueline Ruttimann reports on the potentially catastrophic effect this could have on marine creatures. It's not hard to imagine a tonne of water: it is a week's worth of not-very-deep baths. Getting to grips with a billion tonnes of water is more of a challenge. That would be a similar bath for every man, woman and child on the planet; a week's worth of flow for the Nile. To really expand your mind, go further still, to a billion billion tonnes \u2014 enough water to give every human a day's worth of the Nile instead of a shallow bath. There are dwarf planets that weigh less than a billion billion tonnes. Yet Earth's oceans weigh more. If it is hard to imagine something so vast, it is perhaps even harder to imagine changing it. But humanity is changing the oceans. From the tropics to the Arctic, the seas are sucking up human-driven emissions of carbon dioxide \u2014 about half of the excess belched into the atmosphere over the past two centuries from fossil-fuel burning and cement manufacturing plants 1 . When carbon dioxide dissolves in water, carbonic acid is produced: as a result the oceans are becoming more acidic. \u201cIt's basic chemistry,\u201d says Joanie Kleypas, a marine ecologist at the National Center for Atmospheric Research in Boulder, Colorado. \u201cIt's hard to say that this is not happening.\u201d Over the past few years, scientists have documented how increasingly acidic seas could eat away the armour of many creatures \u2014 blunting the spikes on sea urchins and dissolving the covering on corals. In artificially acidified waters, some animals, such as squid, have problems swimming because the corrosive water affects their respiration rate. Others, particularly tiny organisms with carbonate shells, lose their protective shields as the acid eats away at them 2 . But research into low-pH oceanography is, as yet, sparse. \u201cWe're just starting to grapple with what low pH will mean for ocean communities,\u201d says Jim Barry, a marine biologist at the Monterey Bay Aquarium Research Institute (MBARI) in Moss Landing, California. This ocean acidification is unlike the atmospheric warming also being caused by carbon dioxide in that it is fairly predictable; plotting its future course requires little more than school chemistry, as opposed to sophisticated modelling. The rate of acidification is pretty much unprecedented. Before the industrial revolution, the rise in the level of carbon dioxide in the atmosphere was relatively slow \u2014 giving oceans time to circulate the waters being made more acidic in the shallows with acid-neutralizing carbonate sediments in the depths. In the past few decades, carbon dioxide has been building up far more quickly, and the ocean is becoming acidified at a rate that outpaces the action of sedimentary antacids. The rate of change is perhaps 100 times anything seen in the past hundreds of millennia, as suggested by isotope studies of ancient sediments. In the century to come, sea creatures will find themselves in conditions that their ancestors never had to face. These organisms have never been forced to adapt to lower pH, says Ulf Riebesell, a marine biogeochemist at the Leibniz Institute for Marine Sciences in Germany. \u201cThey've never seen this before in their evolution.\u201d \n               Acid attack \n             The acidified waters eat away at the carbonate skeletons that protect many marine organisms. By some estimates, calcification rates will decrease by up to 60% by the end of this century. If so, carbon dioxide in the ocean could represent a chemical threat to the biosphere as severe as that posed by the build up of carbon dioxide in the atmosphere. A 2005 report from the Royal Society in London called for more research to help quantify the threat. Another report, released last month by three US research agencies, the National Science Foundation, the National Oceanic and Atmospheric Administration, and the US Geological Survey, laid out research strategies for tackling the problem. And the Intergovernmental Panel on Climate Change (IPCC) \u2014 the international body tasked with quantifying the effects of climate change \u2014 is flagging ocean acidification as a problem for the first time in its next report, due in early 2007 In 1800, the carbon dioxide in the atmosphere was 280 parts per million, and the oceans' pH averaged 8.16. Today there are 380 parts per million of carbon dioxide in the atmosphere, and the pH of the oceans is on average 8.05. Estimates suggest the pH could drop to 7.9 by the end of the century. The drop from 8.16 to 7.9, says Kleypas, \u201cdoesn't sound like much, but it's a lot.\u201d Each one-step change on the pH scale indicates a tenfold change in acidity. Just as terrestrial ecologists experiment by growing plants in high concentrations of carbon dioxide, so marine biologists are beginning to investigate what life in the less-alkaline ocean may be like. In a fjord in southwest Norway, Riebesell has set up an outdoor laboratory consisting of a raft with what look like giant milk cartons moored to it. The containers, known as 'mesocosms', are 50-litre vessels filled with coccolithophores \u2014 photosynthesizing plankton, or phytoplankton, with carbonate coverings. Riebesell immerses the coccolithophores into tanks that are aerated with the projected levels of carbon dioxide in the next 50 and 100 years. He calls them \u201cthe oceans of the future\u201d. The coccolithophores' outer casings \u2014 tiny hubcaps known as coccoliths \u2014 are made of the carbonate mineral calcite. Riebesell found that exposing coccolithophores to three times the present-day atmospheric level of carbon dioxide caused nearly half of their protective coating to disintegrate 3 . Such changes don't bode well for one of the ocean's most abundant types of phytoplankton, and what's bad for phytoplankton is likely to be bad for the food webs they sustain. By aggregating on the surface of waste from the upper levels of the ocean (mainly fish droppings), the coccoliths help it to sink down to the seabed communities, which recycle its nutrients into the ocean. Weakening the coccoliths could have knock-on effects on nutrient cycling. \n               Most at risk \n             Riebesell is now looking at the effects of increasing acidity on plankton eggs and larvae; other researchers are studying the dangers posed to larger animals, such as sea urchins and snails. In one experiment, marine biologist Yoshihisa Shirayama, at Kyoto University in Japan, placed sea urchins and snails separately into 30-litre tanks filled with water containing 550 parts per million of carbon dioxide. Within three months, the creatures dropped in weight by roughly 8%. Their calcite spines became blunted, says Shirayama, and so brittle that they broke off easily when handled. If carbon dioxide levels in the tanks were quickly lowered, the animals eventually recovered. If they were exposed to high levels over the long term, such as for a year, most of them died 4 . \u201cIt's obvious that animals cannot adapt to large change in carbon-dioxide concentration,\u201d says Shirayama. Other disturbing news comes from experiments led by MBARI's Barry. For most of his career, carbon dioxide has just been a tool: he used to carry a fire extinguisher to intertidal zones so he could bubble the gas through the sea water, put the animals living in it to sleep and thus capture them more easily. Now he's looking at carbon dioxide as a threat. Working with his post-doctoral fellow Eric Pane, Barry has been studying how two crab species respond to elevated levels of carbon dioxide. Preliminary results suggest that the shallow-water Dungeness crab, ( Cancer  species), does much better than the deep-sea Tanner crab ( Chionoecetes  species), as measured by how long it takes their metabolism to recover from a high dose of carbon dioxide. That difference points out a key point about the acidifying oceans, says Barry: \u201cDeep-sea animals are probably much more sensitive to changes than animals found in shallow water.\u201d Barry and Pace presented their findings on the crab studies in July, at a deep-sea biology symposium in Southampton, UK. In related work, Barry has found that microscopic animals living in seafloor sediments seem to decline after being exposed to a drop in pH of just 0.1 for one month. Oddly, they seem to recover after a week or two \u2014 perhaps, Barry says, because fewer of them are being eaten by larger animals, which may have died off, or because they have more to eat themselves as the dead bodies of other small creatures, killed by the pH change, drift to the sea floor. \n               Mass die-off \n             Also at risk in the deep sea are cold-water corals, some of the slowest growing corals on Earth. Corals are particularly vulnerable to acidification because reefs are built out of aragonite, a carbonate mineral that is more soluble than the calcite used by coccolithophores and sea urchins. \u201cDeep-water corals could soon be in trouble,\u201d says James Orr, an oceanographer at the Laboratory for Climate and Environmental Science in Gif-sur-Yvette, France. By the end of the century, two-thirds of deep-water corals \u2014 as opposed to virtually none today \u2014 could be exposed to sea water that is corrosive to aragonite 5 . Because carbonate is more soluble in high pressures and cold water, increased acidity has a greater effect at depth. But Orr's studies suggest that aragonite will be unstable at all depths throughout the Southern Ocean by 2100. Once the corals go, so too may other species that rely on them for resources such as shelter. Despite such dire models, not all scientists are convinced that ocean acidification will be a major problem. In May, a paper in  Geophysical Research Letters  suggested that the rising carbon dioxide would have minimal biological impact in the ocean 6 . Hugo Lo\u00e1iciga, a hydrologist at the University of California, Santa Barbara, argued that sediment carbonates would be able to buffer the predicted acidification. But it seems most experts disagree. A response, signed by 25 leaders in the field of ocean acidification, rebutted each of Lo\u00e1iciga's points in detail. The experts also argued that many marine organisms would be sensitive to a drop of 0.2 pH units \u2014 a change Lo\u00e1iciga had said would be essentially unimportant. Ken Caldeira, an Earth-systems modeller at the Carnegie Institution of Washington in Stanford, California, who led the response, says the public needs to know now that this is a problem, and that it is unprecedented in its scale. In one model, Caldeira and his colleague Michael Wickett from the Lawrence Livermore National Laboratory, California, simulated the effects of an unregulated pulse of fossil-fuel burning that peaks around the year 2100. The atmospheric carbon-dioxide level peaks shortly after the highest rate of burning. Then it slowly subsides as the carbon is picked up by the ocean \u2014 which slowly becomes more acidic at depth. At the surface, the pH might stop falling by 2750. A kilometre or so deep, however, the pH would still be dropping at the beginning of the next millennium 7 . \u201cPeople should know that the consequences of what we're doing in next decade will last for thousands of years,\u201d says Caldeira. In the meantime, researchers have a great deal to do trying to understand what the first decades of change could bring about. \u201cSo few studies have been done,\u201d says Victoria Fabry, an oceanographer at California State University in San Marcos. \u201cWe've really just scratched the surface.\u201d As an example, Fabry says lab studies on calcifying plankton are in such short supply that researchers have investigated only about 2% of all species. The July report produced by the US research agencies suggests paths for future studies. These include merging experiments done in the laboratory with those in the field, as well as identifying the effects of acidification on keystone species such as coral and plankton and how changes to such species could affect the rest of the ecosystem. The report also notes the need to study the problem at all spatial scales; currently, most studies are focused on either the large-scale biogeochemical level or effects on individual organisms. Studies of ecosystems are in short supply. \n               A world of problems \n             To try to rectify that, some researchers are pushing for large-scale field experiments, such as marine versions of the open-air experiments in carbon-dioxide enrichment that have been taking place in forests for many years. Barry and his MBARI colleagues are working to develop one of these experiments in the free ocean \u2014 it will be essentially a deep-sea cage set in a plume of water enriched in carbon dioxide. In addition, disciplines are starting to merge. Oceanographers are beginning to collaborate with chemists to quantify the acidification reactions, and with engineers to develop robotic submersibles to collect deep-sea organisms for study. Social scientists are starting to get involved as well. The next Pacific Science Congress, to be held in Okinawa, Japan, in June 2007, will bring together natural scientists and social scientists to discuss how residents of coastal areas might adapt. With more attention on the problem, a new possibility has raised its head. Ocean acidification might not just run in parallel with global warming \u2014 it could amplify it. The chalky coccolithophores, when blooming, lighten the surface of the oceans, which means more sunlight is reflected into space. Reduce their number and even if other phytoplankton take their place, that lightness will be gone. Coccolithophores are also responsible for many of the clouds over oceans. They produce a lot of dimethylsulphide, which accounts for much of the aerosolized sulphate in the atmosphere above the oceans. Sulphate particles act as 'seeds' around which cloud droplets grow 8 . Remove them, and you could remove a significant fraction of the world's clouds, warming the planet yet further 9 . Human imagination cannot easily cope with the vastness of the oceans, or the complexities that change within them can bring about. But human industry faces no such obstacle in making change unavoidable for centuries to come. \n                 Additional reporting is by Alexandra Witze. \n               \n                     Oceans in trouble as acid levels rise \n                   \n                     Researchers seek to turn the tide on problem of acid seas \n                   \n                     Project probes impact of waste carbon dioxide on marine life \n                   \n                     NOAA report from July 2006 \n                   \n                     Royal Society report from June 2005 \n                   \n                     2004 UNESCO symposium \n                   \n                     Ulf Riebesell \n                   Reprints and Permissions"},
{"file_id": "443023a", "url": "https://www.nature.com/articles/443023a", "year": 2006, "authors": [{"name": "Carina Dennis"}], "parsed_as_year": "2006_or_before", "body": "Sunlight is a ubiquitous form of energy, but not as yet an economic one. In the  first  of two features, Oliver Morton looked at how interest in photovoltaic research is heating up in California's Silicon Valley. In this, the second, Carina Dennis talks to Australian researchers hoping to harness the dawn Sun's heat. \n               Australia receives more solar radiation per square metre, on average, than any other continent. Although turning this radiation into electricity is one option, another is finding ways to make use of its heat. We spoke to Australian proponents of two very different solar-thermal systems, both rather confusingly known as solar towers: Roger Davey, executive chairman and chief executive of EnviroMission, based in Melbourne; and Wes Stein, from the Energy Technology Division of the Commonwealth Scientific and Industrial Research Organisation, based in Newcastle. \n               \n               How does your solar-thermal project work? \n             Roger Davey:  In our solar tower, solar radiation is used to heat the air captured under a large greenhouse. The roof of the greenhouse is sloped towards the centre, where there are turbines and a very tall tower that creates a chimney effect. Hot air rises up and out of the tower, rushing through turbines at the base of the tower to produce power. Wes Stein:  We have 200 small mirrors that track the Sun during the day and concentrate its rays onto a single point on the tower. This provides temperatures hot enough to drive a chemical 'reforming' reaction. Methane and steam go in, energy and a catalyst are added; the resulting gas [carbon monoxide and hydrogen] is called syngas in the industry \u2014 we call it SolarGas. This gas has 26% more energy per kilogram than methane \u2014 it is solar energy embedded in chemical bonds. So the plant improves the energy value of the gas, and provides a precursor for other energy products, such as liquid fuels. We could link this process with carbon burial so that the carbon dioxide from the fossil-fuel component was sequestered. \n               How big is your solar tower? \n             RD:  The original concept design made it 1 kilometre high and 7 kilometres in diameter, but we have developed new technologies that will make it substantially smaller and more powerful. A 50-megawatt demonstration plant will be built at Tapio Station, about 22 kilometres northeast of Mildura in New South Wales. This is not necessarily the optimum size, but it is the optimum size to build in the first instance to show the robustness of the technology. The front-end engineering and design are currently under way, and it would be premature of me to talk heights and sizes now, and then alter them in a week's time. WS:  The tower reactor is 15 metres above the mirrors, and the tops of the mirrors are about 3 metres off the ground. The total ground area is 40 \u00d7 40 metres, but that is because of our site constraints \u2014 it doesn't necessarily represent the ideal module size. So I don't want to make a big deal of the specific dimensions. It will produce up to 250 kilowatts of electricity. However, this is a research-size system. We designed this tower to represent a single module that could be replicated again and again to make it up to any desired capacity. \n               How does your system fit into the existing energy infrastructure? \n             RD : It operates as a power plant and feeds into the normal electricity grid. WS : The gas can be used in gas turbines to make electricity or we can turn it into liquid transport fuels. We are investigating the use of existing gas pipelines to transport SolarGas. One idea is to do the solar-reforming reaction out in the desert region, where many gas pipelines originate, putting the SolarGas into the pipeline and transporting it downstream. We can have all the convenience of gas but with all the benefits of solar energy. SolarGas can make liquid transport fuels and also fertilizer. After power plants, transport and agriculture are the biggest greenhouse-gas emitters in Australia, so it gives us an opportunity to target more than electricity. \n               How are you trying to improve the technology? \n             RD : We can now create far greater heat under the collector roof. It captures more heat and retains more heat \u2014 it's a bit like double-glazing. We now also have a method of storing heat in brine ponds, so that on days of lower solar radiation we can bring in that stored heat to create the temperatures we need. That means we have a system that could operate 24 hours a day, 7 days a week, 365 days a year. WS : We believe cost reductions will come from improving the efficiency of the reaction and reducing the cost of the mirror field through mass production. We have two areas of research we are working on. One is to reduce the reaction temperature to around 550 \u00b0C [from around 800 \u00b0C]. That greatly reduces the capital costs of the mirror field, because fewer mirrors are required. We are using a novel arrangement of membranes to allow a much lower reaction temperature. We are in the middle of a patent application, so that's all I can say at present. The second concept uses carbon dioxide as the reforming agent rather than steam, so we would be using a waste stream. Coal-seam methane is a rapidly growing resource in Australia. Methane from coal seams comes out with a fair amount of CO 2 . Normally, that CO 2  is stripped off before the gas goes into the downstream pipeline \u2014 so we would make use of that discarded CO 2 . But we need to develop new catalysts for that reaction. \n               How much does it cost? \n             RD : The original-concept 200-megawatt plant would have cost around Aus$800 million (US$610 million). The 50-megawatt plant will be substantially less, owing to a reduction in the structural dimensions, with final costings anticipated at the completion of the front-end engineering and design. We are working on the engineering of the optimally sized power plant with the optimum output, incorporating the new technologies. I could guess, but it's better to be sure than to mislead. WS : This solar tower has cost us Aus$1.5 million to build. But we think that if we apply the same learning curve that turbines have experienced over the past 15\u201320 years, this same system would cost us around Aus$300,000 to build in the future. \n               What are your advantages over rival solar technologies? \n             RD : We can guarantee output to meet the demand, just like coal-fired plants. WS : The advantages are that we can use SolarGas in a broader range of sectors, not just electricity; the very-low-cost potential; and the fact that it overcomes the transport and storage issues of solar energy. \n               Solar thermal has been around for a while \u2014 why has it not caught on? \n             RD : I think it has caught on. That's why we're here. WS : One of the difficulties it has had is the concept that it has to be built large for cost reductions to occur. It's one thing to build a system at the small demonstration level and enthuse everyone with that, and then say 'now I need to build 100,000 of these to be cost-effective in a 50-megawatt system'. I think the jump from that small to large scale has been too much for investors to handle, even though the modelling and calculations have always shown that it would be cost-effective. \n               Worldwide, investment in solar-thermal research is fairly low; why do you think that is? \n             RD : If you take the solar tower as an example, the original design had to be big to produce enough power to make enough money to pay for the capital costs. But with the additional technology we have added, we have created far more power out of a smaller plant. WS : It is because of the difficulty investors have in biting off big chunks compared with the easier task of biting off small chunks. Photovoltaics are very sexy \u2014 they don't move, they don't make any noise sitting on top of a roof, and they can be made to look good on buildings. They are still very expensive, but they come in small chunks. It is much easier for investors to go with that concept than with a one-off, large, solar-thermal power station. It's only now, as a result of initiatives and incentives around the world, that solar-thermal technology is starting to take off globally. \n               What stops power companies from making big investments in solar thermal? \n             RD : Power companies have investments in other assets, don't they? WS : Purely cost. At the end of the day, it comes down to cost of the technology. \n               How much of Australia's electricity needs could solar thermal provide in 20 years \u2014 or in 50? \n             RD : We plan to develop approximately 2,100 megawatts of installed capacity by 2020. By 2030, we plan to have installed in excess of 6,500 megawatts of power, which would be the equivalent of the electricity usage of about 10 million households based on current energy use. WS : I see no reason why, by 2050, solar energy couldn't be supplying at least 25% of Australia's energy mix. \n               Is your technology really only suitable for countries with big, empty deserts? \n             RD : No. Our solar tower operates on temperature differentiation \u2014 the creation of an environment with strong differentiation to ambient temperature will regulate where development is able to occur. Unfortunately, big, empty deserts do not serve electricity grids and have inherent supply-chain issues. WS : Sensibly, yes. Technically, there is no reason why you can't do it in lower solar regions but the cost will be higher. I'm not advocating solar being a single solution for all Australia's energy needs. Theoretically it could be, but practically I don't think that will happen. \n                     Solar energy: A new day dawning?: Silicon Valley sunrise \n                   \n                     Sunlight used to smelt zinc \n                   \n                     Leapfrogging the power grid \n                   \n                     Energy for a cool planet web focus \n                   \n                     EnviroMission Limited \n                   \n                     CSIRO energy centre \n                   \n                     Solar thermal overview \n                   \n                     Wes Stein profile \n                   Reprints and Permissions"},
{"file_id": "442507a", "url": "https://www.nature.com/articles/442507a", "year": 2006, "authors": [{"name": "Carina Dennis"}], "parsed_as_year": "2006_or_before", "body": "There's more to ecology than ringing birds, and in this special section Nature explores how the molecular sciences are transforming the field. In the  first  of two features, Sharon Levy explored how atoms in feathers can reveal the secrets of rare warblers. In this, the second, Carina Dennis unveils a technique that aims to make killing whales for science a thing of the past. The day started much like any other. Researcher Daniel Burns and his colleagues were keeping a respectful distance behind a pair of courting humpback whales. Then a third whale made an unexpected appearance. The new arrival beganto press his attentions on the increasingly harassed female. Taking fright, she manoeuvred to place the research boat between herself andher suitors, leaving Burns and his team sandwiched between three 35-tonne whales, all shoving in different directions. \u201cThe adrenaline was pumping,\u201d says Burns, recalling how he braced himself on the bow of the six-metre vessel as it lurched precariously between the massive animals, each measuring about twice the length of the boat. But he and his team got what they came for: a sample of whale 'dandruff'. Using a kitchen sieve strapped to the end of a stick, Burns and his team trail behind whales and quickly scoop up flakes of skin \u2014 some as big as the palm of a hand \u2014 before they sink. The flakes are naturally shed by the animals when they launch themselves out of the water (see above) or slap their tail fins on the surface. Scooping up skin might not have the glamour normally associated with whale watching, but Burns and his colleagues think the flakes could offer a non-invasive way of working out a whale's age. If they are right, one of the key arguments in favour of killing whales for scientific purposes will be dead in the water. Burns, a PhD student at the Southern Cross University Whale Research Centre in Lismore, New South Wales, goes out in virtually all weather, from dawn until dusk. He is part of a research team, led by Peter Harrison, that is developing genetic techniques to age humpback whales ( Megaptera novaeangliae ) from their sloughed skin. Currently, the most accurate way to age baleen whales such as humpbacks, which lack the teeth often used to age whales, is to count the lamination rings that form in their ear wax. Because the ear canal is closed to the outside world, wax accumulates in layers. Such laminations are conventionally thought to form twice a year in humpbacks. But counting them requires killing and dissecting the whale. \n               Showing the years \n             \u201cBeing able to age whales is something of a holy grail,\u201d says Phil Clapham, a whale biologist at the National Marine Mammal Laboratory in Seattle, Washington. Knowing whales' ages would enable biologists to model population dynamics, to better understand whale behaviour and assess how well the creatures are recovering from the devastating slaughter of the past century. It could also reveal whether mating tactics change with age, why individuals associate with each other at certain stages, and answer basic questions, such as exactly how long humpback whales live. Earlier reports suggested half a century, but researchers now suspect they could live for twice as long as that. Harrison's team \u2014 and other groups around the world \u2014 hope that clues to a whale's age lie in the tips of its chromosomes, where there are stretches of short, repeated DNA sequences called telomeres. Like countdown timers, telomeres gradually shorten with age in some species, including humans. Studies of humans and birds suggest that individuals can be assigned to broad age classes 1 , 2 , based on the length of their telomeres \u2014 although variability between individuals and in the rate of telomere loss during an individual's lifetime make it difficult to determine age precisely. Whale researchers are optimistic that, if whales are also shown to lose telomere sequences with age, telomere analysis could provide a non-lethal means to assign individuals to broad age groups. The need is urgent, say researchers, because last year Japan declared that it will double its catch of minke whales for scientific research. It will also include, for the first time since the International Whaling Commission (IWC) imposed a moratorium on commercial whaling in 1986, humpback and fin whales in its annual catch for research \u2014 with a yearly take of 50 of each species 3 . One of Japan's main scientific arguments for increasing its catch is the need to collect samples to determine population structure. Harrison's team and others hope that the telomere method could scotch this claim. \u201cIf the technique works, it would put a large nail in the coffin of Japan's argument for a scientific whaling programme,\u201d says Clapham. Using telomeres to estimate the age of whales \u2014 and other cetaceans, such as dolphins and porpoises \u2014 is untested water. Telomeres shorten with age in some animals, but not in others 4 ; some even lengthen 5 . To show that the telomere ageing method will work in whales, researchers need a sample of known age. Harrison's team is fortunate to have access to an extensive photo library of whales created by Trish Franklin, a historian, and her husband Wally, a retired airline executive. The couple have been meticulously photographing and documenting the animals since 1989. \u201cIt started off as an interest, which then become a passion \u2014 and later an obsession,\u201d says Wally, who, along with his wife, is now undertaking a PhD under Harrison's supervision. The photo library, comprised of pictures of nearly 3,000 individual whales, is invaluable as it provides the minimum age of many animals. The researchers are now systematically genotyping each animal \u2014 that is, obtaining an individual genetic 'fingerprint' \u2014 using the DNA extracted from sloughed skin samples. \n               Signs of success \n             While Burns braves unpredictable waters to collect the skin samples, his colleague Martin Elphinstone processes their telomeres in the quiet calm of the lab environment. Some early results are encouraging. According to Harrison, preliminary data from a handful of humpback whales hint that calves and adults can be distinguished on the basis of telomere length. \u201cIdeally, in the long term, we'd like to refine it to a 5- to 10-year band width,\u201d says Harrison. Although unlikely to be as accurate as counting ear wax laminations, this range would be sufficient for population modelling, he says. \u201cIt's better to have the approximate age of a live whale than the exact age of a dead whale.\u201d Support for these data comes from Per Palsb\u00f8ll, a population geneticist at the University of California, Berkeley, who is also studying telomeres in humpback whales. His team has examined the DNA in skin biopsies taken from a dozen whales, ranging from calves to 20-year-old animals. \u201cFrom this tiny data set, we see longer telomeres in younger animals,\u201d says Palsb\u00f8ll. But he cautions that the method is far from reliable at this stage. \u201cWe have tremendous problems with reproducibility, even within samples from the same individuals,\u201d he says. Telomere work is a complicated business. To assess how the rate of telomere loss changes over an individual's lifetime, Harrison's team is testing multiple samples from the same individual at different times in its life. To do this, the group will tap into a treasure-trove of skin biopsies collected over several decades by Scott Baker, a molecular ecologist at the University of Auckland in New Zealand. Baker has samples from northern-hemisphere humpback whales, ranging from 2 to 30 years old, which were resampled 14 years after the initial biopsies were taken. Harrison is collaborating with Baker to measure telomere length in these animals to see how it varies within and between individuals. Baker also has a collection of biopsies taken ten years ago from southern right whales ( Eubalaena australis ) from the Auckland Islands; working with Nick Gales, of the Australian Antarctic Division of the Commonwealth Scientific and Industrial Research Organization, based in Canberra, he is resampling the same animals for telomere analysis. Harrison's method for analysing whale telomeres from sloughed skin samples is unique in that it is non-invasive; getting DNA from skin biopsies involves taking a plug of tissue from near the dorsal fin using a biopsy dart from a cross-bow or modified rifle. But using whale skin flakes is not without problems. The skin is often damaged and dying tissue. \u201cMany samples don't provide enough high-quality DNA,\u201d says Elphinstone. It is also difficult to target specific whales using this technique, as skin flakes from different whales can get mixed up in the water. Harrison thinks this problem will be overcome once they have got a DNA fingerprint, or genotype, for every animal in their photo library to act as a reference. If telomere analysis can be validated in humpbacks, it could be extended to other whale species. Gales is examining telomeres in animals ranging from those that are relatively short-lived, such as harbour porpoises that live less than 20 years, to bowhead whales that may live for more than 200 years. \u201cWe are looking at animals of quite different lifespans to see whether telomere length correlates with longevity,\u201d says Gales. Gales is also analysing the method in a resident bottlenose dolphin population in Sarasota Bay, Florida \u2014 one of the world's best-studied dolphin populations. \u201cThe advantage is that a tremendous amount is known about their age and population structure, so telomeres can really be put to the test,\u201d says Gales, who is collaborating on the project with Randall Wells at the Mote Marine Laboratory in Sarasota. The developments can't come soon enough for the Australian researchers. Japan plans to catch humpback whales in the Antarctic feeding grounds in the south-latitude summer of 2007\u201308, which includes the population that Harrison's team is studying. They dread the thought that one of their whales, many of which have been given nicknames, will turn up on a whaling vessel. Even if whale researchers can develop a non-lethal method to age the creatures, whether it would immediately halt scientific whaling is questionable. Japan's Institute of Cetacean Research declined to comment when approached by  Nature . Gales, who heads the Australian delegation for the scientific committee of the IWC, thinks it wouldn't. \u201cBut at least we can use it to apply more political pressure,\u201d he says. \n                     Conservation at a distance: Atomic detectives \n                   \n                     Japan's whaling bid frustrated \n                   \n                     International Whaling Commission \n                   \n                     Provincetown Center for Coastal Studies \n                   Reprints and Permissions"},
{"file_id": "442504a", "url": "https://www.nature.com/articles/442504a", "year": 2006, "authors": [{"name": "Sharon Levy"}], "parsed_as_year": "2006_or_before", "body": "There's more to ecology than ringing birds, and in this special section  Nature  explores how the molecular sciences are transforming the field. In this, the first of two features, Sharon Levy explores how atoms in feathers can reveal the secrets of rare warblers. In the  second , Carina Dennis unveils a technique that aims to make killing whales for science a thing of the past.  In a scrubby jack-pine thicket in Michigan,Peter Marra and his colleagues set up a net,bait it with a plastic lemon, and play the song of one of the planet's rarest birds, the Kirtland's warbler. Soon after, a male warbler, resplendent in his bright yellow and grey breeding plumage, comes bombing into the net, mistaking the plastic lemon for a competitor on his turf. To reach this place, the tiny bird may have flown 2,400 kilometres from the island of Eleuthera in the Bahamas. The Kirtland's warbler  Dendroica kirtlandii,  (pictured below and right) belongs to a family of small, colourful birds that nest in the forests of the United States and Canada, and winter far to the south. In the 1970s and 80s, destruction of its breeding habitat brought the species to the brink of extinction. During this period, observers found fewer than 200 singing males in Michigan. Intensive management of their breeding habitat \u2014 replanting jack pines and removing cowbirds, which parasitize warbler nests \u2014 has helped the Kirtland's population rebound to more than 4,000 birds. But events in their largely unknown wintering grounds may still be harming warblers. Marra, a biologist at the Smithsonian Migratory Bird Center in Washington DC, is among a pioneering group that is trying to identify threats to warblers and other vulnerable creatures 1  \u2014 using not just bird nets and binoculars, but also mass spectrometers. The tool relies on the fact that elements such as carbon, hydrogen and nitrogen have more than one isotope. The isotopes differ in their mass, so a mass spectrometer can reveal how much of each isotope a sample contains. Because different foods have different isotopic make-ups, the ratios of carbon and nitrogen isotopes in the tissues of animals can reveal what they have been eating, and suggest the type of area they've inhabited. And thanks to the fact that some elements, such as hydrogen, have isotope ratios that vary over the globe, researchers can also see where an animal has been, by tracing its path across the isotopic landscape, or 'isoscape' (see ' Written in the elements '). Such studies are allowing researchers to reconstruct the behaviour of many endangered species, yielding information that could prove invaluable to efforts to conserve them. Many songbirds that make long annual migrations between northern latitudes and the tropics have been declining for decades. The reasons have confounded researchers, who until recently have not been able to connect breeding populations in the north with specific wintering populations in the south. \u201cThe links between breeding and wintering grounds represent a huge hole in our knowledge,\u201d says Marra. \u201cIt doesn't matter how much conservation work we do in the United States. We need to protect animals year-round, in both breeding and wintering areas.\u201d Marra's earlier study 2  of the American redstart ( Setophaga ruticilla ), a common warbler, was the first to show that habitat conditions in distant tropical wintering grounds could influence the birds' reproductive success in US forests. He exploited the fact that plants in cool, moist habitats tend to use a different form of photosynthesis from that used by most dryland plants. This means that plants in damp habitats have lower levels of heavy carbon (carbon-13) in their tissues. The different carbon ratios transfer up the food chain to the insects on which the redstarts feed. Using the ratio of carbon-13 to carbon-12 in the birds' blood, Marra inferred that the healthiest males arriving at his study site in New Hampshire had wintered in damp mangrove forests, where they had fattened up on abundant insects. Because these birds had become fit to take their long journey north sooner than birds wintering in the drier scrublands of Jamaica and elsewhere in the Caribbean, they arrived first on their northern breeding grounds, laid claim to the choicest nesting territories, and fledged more chicks. Another ground-breaking study 3  used isotopes to reveal the wintering grounds of the black-throated blue warblers ( Dendroica caerulescens ). These warblers breed in forests throughout the eastern United States and Canada, but over the past 30 years or so the population in southern states has dwindled while numbers in the north have held steady. Dustin Rubenstein of Cornell University in Ithaca, New York, and his colleagues analysed hydrogen isotope ratios, which vary predictably in rainfall at different latitudes. They showed that birds breeding in the southern United States wintered on the island of Hispaniola, which includes Haiti, a country that has suffered drastic deforestation. Those breeding farther north wintered in more intact habitats in Jamaica and Cuba. \n               What counts \n             \u201cIn North America, we're very good at running around counting birds, and knowing whether populations are going up or down,\u201d says Keith Hobson, a research biologist for the Canadian Wildlife Service who is based in Saskatchewan. \u201cThe problem is we don't often know why these changes are happening. So the insights that stable-isotope studies give us into causes are priceless.\u201d The stable-isotope technique itself is not new; archaeologists have long used isotope ratios in bone to reconstruct the diets of ancient peoples. Forensic scientists are catching on to their use too (see  'Written in the elements' ). But the high cost and relatively low efficiency of mass spectrometers meant that wildlife biologists were slow to take up the tool. This all started to change in the 1990s, when improved equipment became available. \u201cThere's been a great improvement in the speed with which samples can be run,\u201d says Hobson. \u201cPeople can suddenly afford it, and the throughput of samples has become huge with continuous-flow isotope mass spectrometry.\u201d Recent work reveals the changes that can reverberate throughout whole ecosystems when an animal population is decimated. In Monterey Bay, California, numbers of sardines and anchovies began to crash in the 1940s \u2014 possibly reflecting a natural oscillation exacerbated by decades of overfishing. This year, Ben Becker of the Point Reyes National Seashore in California and Steven Beissinger of the University of California, Berkeley, published a study in which they compared isotope ratios in the feathers of endangered modern marbled murrelets ( Brachyramphus marmoratus ) to those from museum specimens collected before the sardine crash 4 . Because the heavier isotope of nitrogen, nitrogen-15, concentrates in organisms' tissues as it moves up food chains, the isotope ratio offers a guide to what position the animal occupies in the chain. Becker and Beissinger found that today's seabirds are forced to feed lower on the food chain than their ancestors, surviving on krill rather than more nutritious small fish. This may explain why, in some years, as little as 10% of local marbled murrelets attempt to nest. \n               In at the krill \n             Steven Emslie of the University of North Carolina in Wilmington and Bill Patterson of the University of Saskatchewan in Saskatoon are also using isotopes to look at krill. They are asking whether penguins shifted to eating krill after whalers wiped out most of the Southern Ocean whales \u2014 major consumers of the crustaceans \u2014 some 200 years ago. Hobson first applied isotope techniques to wildlife in the 1980s, when he used nitrogen isotope ratios to gather information on the feeding patterns of seabirds, including the long-extinct great auk ( Pinguinus impennis ) 5 . He now thinks the technique has most potential as a probe into the mysteries of migration. He and his colleagues are using carbon and hydrogen isotopes to reveal the surprising long-distance travels of dragonflies and bats, and to probe the fates of some once-abundant North American migratory waterfowl. The technique could even help answer the long-standing puzzle of how new travel routes evolve. Stuart Bearhop of Queen's University in Belfast, UK, studied hydrogen isotope ratios in the claws of a community of European blackcaps ( Sylvia atricapilla ). All European blackcaps breed in Germany and Austria, but this population diverged from its parent population, which winters in Spain and Portugal, and began wintering in Britain a few decades ago 6 . Birds that winter in Iberia have significantly higher levels of hydrogen-2, allowing researchers to tell the two groups apart even though they mix on their breeding grounds. Blackcap males from Britain arrive at the nesting grounds earlier, tend to mate with females who have also come from the United Kingdom, and produce more chicks than birds that come from the south. \n               The mechanics \n             The competitive advantage of British blackcaps almost certainly arises from a change in the timing of their travel to the nesting grounds. \u201cMigratory behaviour in this species is triggered by changes in day length, and the critical day length is reached in Britain about ten days earlier than it is in Spain and Portugal,\u201d says Bearhop. The altered timing gives males the first choice of breeding territories, and helps ensure that UK-wintering birds find like-minded mates. Although there is as yet no evidence that the two populations are speciating, isotope ratios have made clear what Bearhop calls \u201ca nice mechanism by which two populations could become reproductively isolated.\u201d Even the hidden physiologies of animals can be revealed by isotope analysis. The heavier isotope of nitrogen occurs in much higher concentrations in marine than in terrestrial or freshwater food webs. Hobson is exploiting this pattern to study birds that winter on the ocean but fly far inland to breed. \u201cThe dogma has been that large-bodied, high-latitude breeding birds carry all their nutrients with them when they fly inland, that they use marine nutrients stored in fat to make their eggs,\u201d he says. This idea led to concerns that the chicks of many waterfowl species that breed in the high Arctic were suffering from heavy loads of contaminants, such as mercury and selenium, that pollute nearshore waters where the adult birds forage in winter. But Hobson's studies of the feathers of new-born birds show that most marine ducks and geese create their eggs with nutrients from food they find on their nesting grounds. That finding relieves some concerns over marine contaminants, but also points out potential problems. \u201cIf these birds rely on local conditions to favour reproductive success,\u201d says Hobson, \u201cthen climate change in the north may have a much greater influence on them than expected.\u201d Although they face the important challenge of improving and updating hydrogen-isotope maps \u2014 across latitudes and elevations \u2014 stable-isotope techniques have opened up a world of possibilities for wildlife researchers. Isotope ratios in different types of tissue offer a way to analyse food sources over varying periods of time: ratios in faeces or blood plasma reflect food eaten over a day or two; those in cellular blood cover diet over about a month. And the feathers or claws formed on breeding grounds retain the same isotope signature many weeks later when birds arrive to winter in an entirely different part of the globe. All this represents a huge improvement on traditional methods: hoping that birds banded near their nests will be sighted on distant, unknown wintering turf, or killing a research subject to get a snapshot of its stomach contents. The ultimate beauty of stable-isotope studies is that they allow information on whole populations of animals to be collected without injuring a single individual. \u201cThe exciting thing is that the isotope approach can be entirely non-destructive,\u201d says Hobson. \n                     Conservation at a distance: A gentle way to age \n                   \n                     Carbon in teeth helps to identify disaster victims \n                   \n                     Ornithology: A wing and a prayer \n                   \n                     Lasetron to produce zeptosecond flashes \n                   \n                     In focus: climate change \n                   \n                     Smithsomian ornithology \n                   Reprints and Permissions"},
{"file_id": "443141a", "url": "https://www.nature.com/articles/443141a", "year": 2006, "authors": [{"name": "Jeff Kanipe"}], "parsed_as_year": "2006_or_before", "body": "Physicists and climate scientists have long argued over whether changes to the Sun affect the Earth's climate? A cloud chamber could help clear up the dispute, reports Jeff Kanipe. The cloud chamber, invented by C. T. R. Wilson at the start of the twentieth century, opened a new microcosmos to human examination. Droplets of water coalescing around ions in a chamber saturated with water vapour provided physicists with a way of visualizing particles; tracks left by the particles were preserved like contrails across the sky (see  'The first cloud maker' ). In the second half of the century, though, the technology was eclipsed \u2014 first by bubble chambers and then by a range of other devices. Which is why it is surprising that an international team of scientists at the biggest particle-physics laboratory in the world, CERN in Geneva, Switzerland, is hard at work on a cloud chamber remarkably like those used by the physicists who founded the lab in 1954. The difference is that scientists will not use the clouds in this chamber to look for subatomic particles; they will use the particles in the chamber to try and understand clouds. The experiment, called CLOUD (for Cosmics Leaving Outdoor Droplets), is designed to shed light on a sometimes acrimonious debate between a small number of physicists and astronomers, who believe that cosmic rays have a substantial influence on Earth's climate, and many in the mainstream climate community who don't. In CLOUD, a beam of particles from CERN's Proton Synchrotron will stand in for the cosmic rays. And a team of atmospheric physicists, chemists and space scientists from nine countries will try to see how they affect cloud formation 1 . The CLOUD chamber has various bells and whistles that its particle-focused forerunners lacked, such as control systems to simulate parcels of air rising through the atmosphere and a 'field cage' to generate an electric field similar to that found in fair-weather clouds. The chamber can generate a range of water-vapour supersaturations (relative humidity values above 100%) that will allow droplets to form on particles between 0.1 nanometres (the size of small ions) and 100 nanometres (the size of the nuclei around which drops form in natural clouds). \n               Sun trap \n             The CLOUD prototype currently under construction will see its first subatomic particles from the synchrotron in October, according to team leader Jasper Kirkby. The machine will be fully operational in 2010, unless there are hitches with its \u20ac9.1-million (US$11.6-million) budget. \u201cAlthough this is a much simplified version of the final CLOUD experiment,\u201d says Kirkby, \u201cwe feel confident that, as this will be the first experiment of this kind on a particle beam, there will be very interesting new physics results.\u201d One of the earliest connections between clouds and cosmic rays was made in 1997 in the  Journal of Atmospheric and Solar-Terrestrial Physics  by Henrik Svensmark and his colleague Eigil Friis-Christensen 2 . The two researchers were then with the Danish Meteorological Institute in Copenhagen, Denmark, and are now at the Danish Space Research Institute, also in Copenhagen. Svensmark and Friis-Christensen noted that from 1987 to 1990, global cloudiness fell by approximately 3% and that the number of cosmic rays reaching the Earth dipped by 3.5%, a fluctuation that coincided with a peak in sunspot numbers. Looking at sea and land temperatures and cosmic-ray data from 1970 to 1989, they concluded that variations in cosmic-ray intensity that depended on the sunspot cycle might be influencing global cloud cover 2 . The idea that changes in the Sun's activity can influence Earth's climate is a long-standing one. In the 1970s, US astronomer Jack Eddy found a link between the Maunder minimum \u2014 a dearth of sunspots in the seventeenth and eighteenth centuries, which reflects a change in solar activity \u2014 and the 'little ice age', as indicated by the width of tree rings. But the problem with such links was that the changes in the Sun's overall energy output seemed much too small to account for the change in the climate 3 . The cosmic-ray hypothesis provided a way around that problem. It was not, the Danes argued, a change in the Sun's output of photons that so affected Earth, it was a change in the way the stream of particles ejected from the Sun, the solar wind, interacted with Earth's magnetic field. The Sun's activity waxes and wanes in regular cycles, as revealed by changes in sunspots. Decades of data from balloon-borne radiosondes and particle detectors show that the number of cosmic rays reaching the Earth goes down when this activity is at its height, because the strengthened magnetic field associated with the solar wind deflects them. During quiescent periods, the solar wind weakens, allowing more cosmic rays to penetrate to Earth's lower atmosphere. There the rays ionize atoms. They also occasionally hit the nuclei of atoms hard enough to create new isotopes such as carbon-14 and beryllium-10. These isotopes, stored in layered receptacles such as tree rings and ocean sediments, provide a record of solar activity going much further back than human observations \u2014 a record that, at some points, does seem to fluctuate in step with the climate 4 . \n               Clouding the issue \n             Svensmark's idea was that trails of ionization in the atmosphere caused by cosmic rays created condensation nuclei on which cloud droplets could form. More cosmic rays would mean more cloud condensation nuclei and either more numerous clouds, whiter clouds, or both. These clouds would increase the rate at which sunlight was reflected away from the planet and so cool it down. Svensmark and his colleagues cite studies 5  showing that, since 1964, galactic cosmic-ray intensity has declined by about 3.7%, and that the trend indicates that this decline could have been going on since the early twentieth century. This, they argue, could account for a significant amount of the global warming that's occurred over the past 100 years 6 . This cosmic-ray connection drew a lot of media attention for several years, but never found favour with the mainstream of climate science, which holds that the twentieth century's global warming was caused by people, not particles. To many in the community, the attention paid to Svensmark and Friis-Christensen seemed to be at best a diversion, at worst a counter-attack. The connection with the Sun was played on by organizations with connections to oil companies, such as the right-wing George C. Marshall Institute in Washington DC. There were also questions about Svensmark's use of data. In a 2004 article published in  Eos , Paul Damon of the University of Arizona in Tucson and Peter Laut of the Technical University of Denmark discussed several examples of what they called \u201cunacceptable handling of observational data\u201d by Svensmark and Friis-Christensen which exaggerated the correlation 7 . Among several flaws, including arithmetical errors, they noted that the cloud data that had been used originally did not represent total global cloud cover, and that when the correct data were used the correlation broke down. Svensmark began to use a different measure of cloudiness, justifying this by arguing that the new measure made more sense than the original one as something that the cosmic rays might be influencing. CLOUD, which Kirkby has been trying to get off the ground for years, is an attempt to move beyond such arguments. It is not a completely neutral attempt \u2014 Kirkby gives talks that put a strong emphasis on the cosmic-ray interpretation of climate history, and Svensmark is a member of the CERN team. But it will produce fresh data. \u201cCLOUD can compare processes when the 'cosmic-ray' beam is present and when it is not,\u201d says Kirkby, \u201c and all experimental parameters can be controlled and measured.\u201d One of those who will be watching is Nir Shaviv, an astrophysicist at the Racah Institute of Physics in Jerusalem, Israel. Shaviv bases a belief that global warming may not be solely the result of anthropogenic causes on studies of climate over extremely long timescales \u2014 hundreds of millions of years. From an analysis he has made of 80 iron meteorites, Shaviv claims to have discovered a periodicity in cosmic-ray exposure, with a peak every 143 million years, give or take 10 million years. He says this corresponds to the mean time between four crossings of the Sun through the Galaxy's spiral arms 8 . The spiral arms of a galaxy hold the greatest concentrations of massive stars, stars which over the course of tens of millions of years explode as supernovae. As cosmic rays are thought to be generated in supernova shockwaves, the Sun's passage through a spiral arm would mean greater cosmic-ray exposure, more clouds, and colder temperatures for Earth. Shaviv thus argued that these periodic crossings and the high cosmic-ray flux they bring with them correspond to four major ice-age epochs on Earth. If this could be confirmed it would link changes in the solar neighbourhood directly to climate. This idea was first considered by the great American astronomer Harlow Shapley, who speculated in 1921 that an 80% change in solar radiation due to the irregularity of interstellar clouds \u201cwould completely desiccate or congeal the surface of the Earth\u201d 9 . Shaviv's findings have found favour with some astronomers, such as Douglas Gies, an astrophysicist at the Center for High Angular Resolution Astronomy at Georgia State University in Atlanta. Based on the Sun's current position as determined from the distances and motions of nearby stars, Gies and a colleague, John Helsel, tracked the path of the Solar System through the Galaxy for the past 500 million years. Their findings indicate that the Sun could indeed have crossed four times through segments of the spiral arms, each passage corresponding with four major ice ages 10 . Again, climate scientists disagree. They think that the history of Earth's glaciations is pretty well accounted for by changes in the amount of carbon dioxide in the atmosphere. And in this instance, too, the climatologists contend that the astrophysicists' claims, specifically Shaviv's, are unfounded and even misleading. A paper 11  coauthored by Shaviv and J\u00e1n Veizer, now an emeritus professor at the University of Ottowa in Canada, was dressed down by no less than 11 scientists led by Stefan Rahmstorf of the Potsdam Institute for Climate Impact Research in Germany. The scientists contended among other things that Shaviv and Veizer had misinterpreted the cosmic-ray data derived from their meteorite samples 12 . \n               A century on \n             A follow-up analysis of the same data set a year later by Knud Jahnke, from the Max-Planck-Institute for Astronomy in Heidelberg, Germany, showed no evidence of significant changes in cosmic-ray exposure with a 143-million-year period \u2014 or any other between 100 million and 250 million years 13 . Again, there was a heated to and fro (see Shaviv's blog at  http://www.sciencebits.com ). Gies doesn't think the controversy changes Shaviv's findings relating Earth's spiral-arm crossings to ice-age epochs or taints the idea that cosmic rays may influence Earth's climate. \u201cI think the findings in our paper are still valid and add weight to arguments about the possible cosmic ray\u2013climate connection advocated by Shaviv and colleagues.\u201d Shaviv and Veizer, too, stand by their findings. Rahmstorf, who led the charge against Shaviv and Veizer, agrees that solar variability has an influence on climate, but he is adamant that galactic cosmic rays cannot explain the global warming of the past decades. \u201cWe know that the observed climate response is the sum of several forcing factors, including the Sun, which is probably responsible for some part of the warming up to about 1940, but not for warming after that. The Sun simply shows no trend there, and neither does the cosmic-ray flux.\u201d Perhaps the irony here is that the current emphasis on global warming is encouraging people from outside the field of climate studies to try out ideas, perhaps exaggerating their promise. At the same time those within the climate community are mindful of lobbies that seek to discredit their work, and believe enough doubt has been cast on the cosmic-ray theory to discredit it, if not ignore it completely. Yet for a connection between cosmic rays and climate to be interesting, it does not have to account for the already well-explained climate history of the past 100 years. Even a small effect, to which Earth is only sensitive under some conditions, would be an exciting find. The CLOUD experiment does not have to overturn the consensus of the world's climatologists to be a success; it just has to throw a little light on some physics. \u201cIn a nutshell,\u201d says Kirkby, \u201cwe want to go after the microphysics between a cosmic ray and a cloud droplet or ice particle. How significant are they in the atmosphere, or in certain parts of the atmosphere?\u201d And even if the results prove undramatic, or inconclusive, they will still carry a certain sense of historical redress. Wilson did not set out to invent a tracker for particles; he wanted to understand the nature of clouds. For his technology to be returned to that aim more than a century on would undoubtedly have pleased him. \n                     The dark side of the Sun \n                   \n                     Galactic dust cooling Earth? \n                   \n                     Flickering sun switched climate \n                   \n                     Researchers bask in reflected glory \n                   \n                     Cloud proposal documents \n                   \n                     Real Climate blog on cosmic rays \n                   \n                     Nir Shaviv's blog \n                   Reprints and Permissions"},
{"file_id": "442617a", "url": "https://www.nature.com/articles/442617a", "year": 2006, "authors": [{"name": "Erika Check"}], "parsed_as_year": "2006_or_before", "body": "Some feared that widespread use of AIDS treatments in Africa would encourage drug resistance, with globally disastrous consequences. But there's no crisis yet, reports Erika Check. The argument was simple, even brutal: there was no point giving anti-AIDS drugs to patients in Africa, because they wouldn't be able to stick with their treatment plans. Worse, the resulting rise of resistant strains of HIV could jeopardize lives elsewhere. This counsel of despair was first expressed five years ago, after some 17 million people in sub-Saharan Africa had already died of AIDS. It emerged while researchers and politicians debated whether to roll out antiretroviral medications to the millions who needed them in developing countries. Aid agencies and governments found the argument unconvincing, even distasteful, and pressed on. As a result, the epidemic has now reached an important turning point. For the first time, more people are taking anti-AIDS medications in poor countries than in rich ones. They number more than a million \u2014 three times the number at the end of 2001 (ref.  1 ). So, is a crisis looming? Not so as you'd notice. Despite some important exceptions, there is strong evidence that patients in the poorest countries are just as conscientious about taking their medications as those in the West, and widespread resistance has not yet emerged. But scientists warn that the next step \u2014 expanding drug programmes to reach a greater proportion of those who need treatment \u2014 will be much more challenging than anything faced so far. Getting patients to comply with their drug regimens has always been a major issue in the treatment of AIDS. The medications work by blocking HIV during various parts of its life cycle in human cells. But the virus has a notoriously error-prone process of replicating itself, so it can easily mutate to become resistant to treatment. Taking combinations of medicines suppresses the virus and cuts the chances of new, treatment-resistant mutations. But if a patient doesn't take all of his or her drugs on a tight schedule, resistant strains can emerge, and be passed on to other people. This is already a serious and growing problem in the developed world. A 2005 UK study showed, for example, that, depending on the drug, up to 20% of new patients were infected with strains that were already resistant2. Problems with adherence were a factor, but such strains mostly arose in patients taking drugs that were less effective than the combination therapies that later became available. Back in 2001, Africa was seen as a worst-case scenario when it came to adherence to combination therapies. Even if drugs could be made affordable, poor nations wouldn't necessarily be able to coordinate effective mechanisms to deliver them. Most patients didn't have access to regular medical services. And it was unclear whether they would trust Western medicine enough to stick with a long-term course of medicine with toxic side effects. Some had more controversial concerns. There were warnings that the very existence of drug programmes in Africa could breed complacency and encourage risky behaviour such as unprotected sex, thus increasing the spread of any resistant strains that arose (see ' Do drugs lead to risky behaviour? '). And most famously, Andrew Natsios, then the director of the US Agency for International Development, argued against giving antiretroviral drugs to patients in Africa, because these patients \u201cdon't know what Western time is\u201d and so would be incapable of staying on their treatment plans. Ask patients to stick to a complex regime, Natsios argued, and they \u201cdo not know what you are talking about\u201d. Such criticisms might sound prejudiced, if not downright racist, but those advocating them pointed out that if drug programmes in Africa failed, the whole world could come to regret it. \u201cIf treatments are not adhered to consistently and correctly, there could be disastrous consequences both for individuals on antiretroviral therapy, and for the HIV epidemic as a whole,\u201d wrote social scientists Danielle Popp and Jeffrey Fisher of the University of Connecticut, Storrs, in a 2002 article 3 . \u201cDeveloping countries could become a veritable 'petri dish' for new treatment-resistant strains,\u201d they warned. Such strains could spread around the globe, rendering antiretroviral drugs useless elsewhere. Researchers involved in rolling out treatments at the time say they were acutely aware of the dangers. \u201cWe were all going into this with our eyes open to resistance issues,\u201d says Marc Wainberg, who will co-chair the XVI International AIDS Conference, to be held in Toronto over 13\u201318 August. Some suggested modelling HIV programmes on principles similar to the directly observed therapy (DOTS) strategy for tuberculosis, in which patients take their medication under the eye of an observer 4 . But this isn't so practical for HIV, because patients have to take drugs every day for the rest of their lives. So researchers who set up some of the earliest sites for antiretroviral treatment in sub-Saharan Africa designed other measures to help patients stay straight. For example, in 2002, the Botswanan government became the first in Africa to promise AIDS treatment to everyone who needed it. The programme built on a pilot project supported by the government and international funders, including drug companies, universities and the Bill & Melinda Gates Foundation. Patients were asked to nominate a family member or friend as an 'adherence assistant', to help them monitor side effects and stick to their treatment courses. It seemed to work. Scientists found that nearly 80% of the first 153 patients treated on the programme knocked the virus down to undetectable levels after 48 weeks on drugs \u2014 a good sign that they had stuck to their medication 5 . Richard Marlink, director of care and treatment at the Elizabeth Glaser Pediatric AIDS Foundation in Santa Monica, California, and one of the scientists behind the project, sees such results as a stinging rebuttal of those who doubted Africans' ability to stick to complex schedules. \u201cAll the indications from studies we've done or anecdotally show that you can obtain levels of adherence you'd attain anywhere else in the world,\u201d he says. \u201cAnd there are some indications that they are better.\u201d Other studies support the idea that patients in even the most challenging environments can manage AIDS treatment regimens. In South Africa, the non-profit group M\u00e9decins Sans Fronti\u00e8res has been working since 1999 with local government to deliver HIV drugs to the residents of the Khayelitsha township. In 2004, doctors with the programme reported that of the first 287 patients treated, 70% had undetectable levels of virus after two years of treatment 6 . The results echoed earlier findings from a widely publicized study performed in Cape Town, which found that 66% of patients still on medication after 48 weeks had undetectable virus loads 7 . Other studies have found good, although not exceptional, results in places such as Senegal and Uganda 8 , 9 . One forthcoming analysis even suggests that, so far, African patients have a better combined record than those in rich nations. Edward Mills and an international group of investigators compared the results of studies on populations in Africa and North America. They were surprised to find that whereas the studies on North Americans reported that 55% of the populations covered stuck to their treatment regimens, 77% of the populations in African studies did so 10 . \u201cThe common expectation was that poverty was a risk factor for incomplete adherence,\u201d says David Bangsberg, director of the Epidemiology and Prevention Interventions Center at San Francisco General Hospital and senior author on the study. \u201cThat's wrong, in retrospect.\u201d Bangsberg says his team's analysis suggests that other factors may be more important than poverty \u2014 such as lack of social support, addiction to drugs or alcohol, bad relationships with the healthcare system and homelessness, which in Western settings have all been shown to make people more likely to stop taking their drugs. \u201cIt isn't poverty that's a risk factor for adherence,\u201d says Bangsberg. \u201cIt's some of the things that go with poverty in North America and western Europe that aren't necessarily part of poverty in southern and western Africa.\u201d Others aren't sure things are quite so rosy. Last year, Christopher Gill and his colleagues at the Boston University School of Public Health, Massachusetts, released a study in which they surveyed conference abstracts for unpublished reports on adherence in Africa. One abstract reported that only 48% of patients in a C\u00f4te d'Ivoire study stuck to their treatments, and although another found 68% adherence in a population in Cameroon, that rate declined over time 11 . \n               Bad bias \n             Overall, the authors found that adherence rates were much more variable than those published in the literature, which has led them to ask whether bad news just isn't getting reported. \u201cThis would seem to be a classic example of publication bias,\u201d says Gill. \u201cPeople are blowing their horns when things are great, and keeping quiet when things aren't.\u201d Bangsberg, however, argues that the low adherence rates in these unpublished studies often weren't the fault of the patients. He contends that adherence rates in one Senegalese study were poor because patients had to buy their own drugs. So if they ran out of money, they couldn't stick to the regimen. And he says that in the Cameroonian study, the drug supply was interrupted by factors beyond the patients' control12. To Bangsberg, these results don't show that patients can't stick to their drugs, but demonstrate the pressing need to strengthen drug-distribution networks. Indeed, scientists say that fixing such structural problems will be key to keeping adherence rates high, as drug programmes now move beyond pilot projects into the wider population. Despite progress so far, the majority of patients in sub-Saharan Africa who need antiretroviral drugs still aren't getting them. Botswana is perhaps furthest ahead, but according to conservative estimates, only about half the patients who need treatment there are on it. The rest of the region lags far behind; from South Africa, where about one-third of patients who need drugs are on them, to Zimbabwe, where just 5\u201310% of those who need treatment can get it. \u201cSadly, with all the effort and money being put into treatment access, there is still a holocaust going on,\u201d says David Katzenstein, drug-resistance specialist at Stanford University, California. He estimates that, globally, 5\u201310% of the people who will die from AIDS during the next year are actually being treated. Expanding programmes to reach more people looks likely to be much tougher than anything faced so far. Patients in clinical trials receive many incentives to stay on their treatments, and are carefully followed by specialist teams. But as treatment programmes make the transition from pilot sites to general public-health programmes, not everyone will get such close attention. And the patients who started treatment in the first roll-out programmes may start experiencing problems as the long-term toxicity of some of the drugs kick in. \u201cIt's still early days in the roll-out,\u201d says Dan Kuritzkes, director of AIDS research at the Brigham and Women's Hospital in Boston. \u201cI'm sure we'll enter a phase two, which is going to be more challenging than the early phase.\u201d \n               A grand plan \n             Because of this, scientists say it's important to make plans now to monitor the emergence of large-scale drug resistance. In rich nations, doctors are recommended to genotype individual patients' viruses before beginning therapy, to check whether they are resistant to any particular drugs. But such tests cost hundreds of dollars, and it's not feasible to deploy them for every patient starting therapy in sub-Saharan Africa. So a World Health Organization effort called the Global HIV Drug Resistance Surveillance Network has set recommendations for measures countries should take as they scale up treatment \u2014 including tracking an easily accessible group of newly infected patients, such as young women pregnant for the first time, and a cohort of patients who stop responding to drugs. If patients do fail first-line therapy, there's little that can be done, because second-line treatments are still too expensive to be widely available in Africa. So as well as trying to increase access to those second-line drugs, the main concern is keeping everyone adherent. But support costs money, and poor countries will have to make trade-offs, Gill warns. \u201cDo you want to have a good programme for fewer people, or a mediocre programme for a lot of people?\u201d he asks. \u201cSomeone has to make that decision.\u201d To stand any chance of success there must be full commitment to providing a robust drug supply and to supporting patients, say scientists, and that means moving on from the idea that Africans aren't capable of taking their drugs properly. \u201cThe hard work ahead of us is to go beyond pilot projects, to taking the health of Africa seriously,\u201d says Marlink. \u201cIt's time to put these concerns about adherence to bed.\u201d \n                     Drop in HIV infection rate used to justify focus on morality \n                   \n                     HIV infection in Zimbabwe falls at last \n                   \n                     New York draws fire over case of drug-resistant HIV \n                   \n                     Drop in HIV infection rate used to justify focus on morality \n                   \n                     HIV drug resistance triggers strategic switch \n                   \n                     Accrued HIV evidence turns treatment dogma on its head \n                   \n                     One in ten new HIV cases in Europe is drug-resistant \n                   \n                     AIDS Special 2004 \n                   \n                     WHO HIV ResNet \n                   \n                     UNAIDS Epidemic Update 2005 \n                   Reprints and Permissions"},
{"file_id": "443019a", "url": "https://www.nature.com/articles/443019a", "year": 2006, "authors": [{"name": "Oliver Morton"}], "parsed_as_year": "2006_or_before", "body": "Sunlight is a ubiquitous form of energy, but not as yet an economic one. In the first of two features, Oliver Morton looks at how interest in photovoltaic research is heating up in California's Silicon Valley. In the  second , Carina Dennis talks to Australian researchers hoping to harness the dawn Sun's heat. The Sun provides Earth with as much energy every hour as human civilization uses every year. If you are a solar-energy enthusiast, that says it all. No other energy supply could conceivably be as plentiful as the 120,000 terawatts the Sun provides ceaselessly and unbidden. If the tiniest fraction of that sunlight were to be captured by photovoltaic cells that turn it straight into electricity, there would be no need to emit any greenhouse gases from any power plant. Thanks to green thoughts like that, and to generous subsidies from governments in Japan and Germany, the solar-cell market has been growing on average by a heady 31% a year for the past decade (see chart, below). One of the most bullish industry analysts, Michael Rogol, sees the industry increasing from about US$12 billion in 2005 to as much as $70 billion in 2010. Although not everyone predicts such impressive growth, a 20\u201325% annual rise is widely expected. The market for shares in solar-energy companies is correspondingly buoyant. And yet in the projections of energy supply made by policy analysts and climate wonks, solar remains so marginal as to be barely on the map at all. At the moment, the world's total installed solar cells have a capacity of about five gigawatts. That looks small compared with almost 400 gigawatts for nuclear power and much more than 1,000 gigawatts for coal. And that's before taking into account the fact that solar cells do not produce electricity at their peak rating all the time. Even within the world of renewables, solar is dwarfed by wind power and hydroelectricity, simply because the technology is much more expensive. And expert opinion does not expect growth in the field to change the picture very much: a 25% annual growth in installed capacity for the next 15 years would still see solar photovoltaics producing just 1% of the world's energy. \n               Points of view \n             Reconciling the solar-cell industry's optimism with global indifference is basically a matter of perspective. Seen from the viewpoint of a small industry, solar's recent decade of expansion is indeed extraordinary. But even heady growth is not enough to spur a radical overhaul of energy infrastructure when you start such a long way behind your competitors. So, although no one doubts that solar electricity will become cheaper in the future, few expect it to do so fast enough to force radical change. Few worldwide, that is. In California's Silicon Valley, the corridor of land along the southwest side of San Francisco Bay, the outlook is more optimistic. Home first to the semiconductor boom and then to the Internet boom, the valley is perhaps the most fertile environment for new technologies in the world. As well as an extraordinary density of successful technology-based companies, it boasts world-class research universities, abundant capital, and a cultural fixation on getting to the future first and making money from it. The dot.com bust of 2000 did relatively little to dent the valley's fundamental strengths and attitudes; instead, it left the area's entrepreneurs and venture capitalists looking for somewhere else to put their millions. 'Cleantech' of all sorts, from water purification to biofuels, is currently the place they want to be. In 2005, $1.6 billion in venture capital went into cleantech, a growth of 35% year on year according to the Cleantech Venture Network, an umbrella group. The high-profile venture-capital firm Kleiner Perkins Caulfield and Byers is putting $100 million of its latest $600-million investment fund into cleantech start-ups. Bill Joy, a partner at Kleiner Perkins who used to be chief scientist at Sun Microsystems, says that the firm will look at perhaps 1,000 different cleantech ideas in the next year. And amid all these opportunities, photovoltaics seem to resonate most with Silicon Valley's history and culture. One attraction is technological familiarity. Solar power has grown up in the shadow of the chip industry, using its cast-off materials and technologies. The silicon in traditional solar cells comes from the same suppliers who feed the chip market; new techniques to make solar cells often use processing technology, such as chemical vapour deposition, that is already widely used in the production of integrated circuits. Miasol\u00e9, a Silicon Valley solar start-up in which Kleiner Perkins has invested, uses expertise derived from the manufacture of computer hard drives. But there is a broader cultural attraction, too. The potential of solar power to decentralize energy generation \u2014 a potential shared, to a lesser extent, by wind power \u2014 appeals to a culture that places huge societal significance on the empowering spread of the Internet. And a business community that saw personal computers go from hobbyists' workshops to almost a billion of the world's desks in 30 years is not fazed by the small size of the solar market today, but energized by the possibilities of tomorrow. It's also a help that Silicon Valley is sunny not just in its outlook; a solar cell in California can produce almost twice as much electricity a year as one in the Ruhr. \n               Catching the rays \n             The poster child for Silicon Valley's interest in solar power is a company called Nanosolar, based in a distinctly unimposing one-storey building next to Palo Alto's municipal airfield. Disappointingly, it has no solar panels on its roof, although there is a smattering of Prius hybrids in its parking lot. Nanosolar was founded in 2001 by Brian Sager, a biotech veteran with expertise in intellectual property, and Martin Roscheisen, an Austrian entrepreneur who mixes grandiloquence, enthusiasm and edginess. Like many Silicon Valley entrepreneurs, Roscheisen had a good track record: companies he had had a founding role in had sold for more than a billion dollars. And Nanosolar quickly attracted 'angel' investors with powerful reputations, including the founders of Google. In 2002, it became the first solar company to raise money on Sand Hill Road, Palo Alto's superconcentration of venture-capital firms. Nanosolar was not founded with one specific solar technology in mind, says Roscheisen. That's just as well, because the range of technologies it spent its first years investigating have not as yet panned out. The 'nano' in the company's name reflected an early belief that the use of very small structures would allow novel photovoltaics made of organic molecules to overcome certain difficulties, such as being able to transfer charge only over very short distances. But the founders soon concluded that \u201cthe organic part was going to require a number of years to mature\u201d, says Chris Eberspacher, the company's vice-president of engineering. \u201cAnd even when mature it would not be very efficient, and not be very durable.\u201d That was where Eberspacher came in. He had been a solar aficionado since a school trip to the University of Texas, Austin; instead of being awestruck by the Texas Turbulent Tokamak fusion experiment being shown to potential students, he found himself drawn to the ramshackle alternative-technologies centre across the street. Eberspacher went on to become head of research and development at Arco, once the largest maker of solar cells in the world, before starting a company of his own. When Nanosolar had trouble getting its organic technologies to work, the company leaders looked around for something less risky that they could get to work in the medium term. They had capital; Eberspacher had technologies that, while still innovative, were considerably more tried and tested than those Nanosolar had been working on. He licensed the technologies to them and joined up in 2005. That technology is now being scaled up for production at Nanosolar's first factory, which aims to produce more than 200 megawatts of solar cells in its first year and 430 megawatts a year later. That would make it among the largest solar-cell fabrication facilities in the world, and by far the largest devoted to this sort of 'thin-film' solar cell. Traditional silicon solar cells are made out of chunks of silicon 200 micrometres thick or more, but slivers a single micrometre across can suffice for a 'CIGS' thin film. CIGS cells are made up of copper, indium, gallium and selenium. Even though some of those elements are increasingly expensive \u2014 the price of copper has more than tripled in the past four years and the price of indium has shot up by a factor of ten \u2014 they are used in such sparing amounts that this is not too great a problem. What is a problem is that making very thin layers of CIGS has often been a complicated and expensive business, typically involving carefully controlled vapours being laid on to surfaces kept in vacuums. \u201cThe silicon [photovoltaic] industry got founded on a wafer technology we inherited from the integrated-circuit industry,\u201d says Eberspacher, \u201cand the thin-film photovoltaic industry got founded on deposition techniques inherited from them in the same way.\u201d But expenses that are reasonable for materials that process information are too high for materials that process energy. \n               On a roll \n             Decades of development have made CIGS cells as efficient as mass-market silicon cells; they can convert about 15% of incoming solar radiation into outgoing electrical current. They are also durable \u2014 the National Renewable Energy Laboratory in Golden, Colorado, has been running some since 1988 without any significant degradation. But they are not yet cheap to produce. The leaders of Nanosolar think that Eberspacher's techniques offer a way around that. They make tiny CIGS particles and mix them into a sort of ink, printing them on to a substrate of metal foil and curing that foil in such a way that the particles condense into a continuous semiconductor. The cells should hit the market in 2007, and the fact that the particles are nanometres across means that the company is still accurately named \u2014 although more by luck than judgement. Perhaps the most attractive aspect of the Nanosolar process is that it can be carried out on foil being continuously pulled off one roll on to another, allowing very high throughput. Such 'roll-to-roll' technologies make it possible to build a large factory with a relatively small investment and make cells cheaply. Roscheisen boasts that the production costs for Nanosolar's cells are so low that even if you subtracted the costs of materials, manpower and energy from a traditional silicon factory, its cells would still be more expensive than those of Nanosolar. Whether Nanosolar can live up to that boast remains to be seen. The fact that it has just raised a further $75 million in private capital suggests that some fairly serious investors believe it will. Whether or not it succeeds, many other companies are trying the same thing. Miasol\u00e9 in Santa Clara is starting to produce CIGS films made with its hard-drive technology. Earlier this year, Applied Materials \u2014 a far larger Santa Clara company that sells the machinery used to make chips and flat-panel displays \u2014 acquired the rights to ways of making thin-film silicon-based photovoltaics developed in Germany. And Nanosys, a Palo Alto firm, is working on nanostructures that could minimize current difficulties with the sort of organic polymer-based solar cells that Nanosolar was looking at in its early days. The valley does not have a monopoly on innovation. DayStar, based in Halfmoon, New York, is also pursuing CIGS thin films, as is Wurth Solar in Germany. In Austin, Texas, B. J. Stanbery has founded a company called Heliovolt. Stanbery, who has been working with CIGS thin films since the early 1980s, when they were first under development at Boeing, has developed techniques for printing such films on a variety of substrates, speeding up their manufacture. In Lowell, Massachusetts, a company called Konarka is working on a novel system for using dyes to produce solar power from flexible plastics. The interaction of sunlight with these organic dyes produces solar power in a way that is perhaps more similar to photosynthesis than to the semiconductor processes in normal solar cells. \n               Measuring up \n             But even if one or more of these companies manages to make solar cells a great deal cheaper, it will be only the beginning. Manufacturing the cells accounts for just half of the roughly $6 per watt it costs to get a solar-cell system up and running. The remaining cost is needed to put them into a protective, mountable module, tune their output from direct current to alternating current, and install them. This has various implications. One is that cells below about 10% efficiency have a hard time making economic sense, because the costs of mounting and installing cells in traditional models get bigger the larger the area involved, and low-efficiency cells require larger areas. Another is that even if you gave away 15%-efficient cells for free, systems using modules such as today's would still be too expensive for many applications. This is why Nanosolar and almost all the other recent solar start-ups take a strong interest in new ways of mounting their cells \u2014 ways that take advantage of their light weight or flexibility. Eberspacher hopes, for example, that such light-weight systems could be used on Nanosolar's own roof, which is too flimsy to take the load from a traditional array. The ultimate aim, says Stanbery, is to integrate the cells straight into building materials of all sorts. New houses, he points out, need roofs anyway. Photovoltaic tiles could be wired into the house from the start. \u201cIntegrating the photovoltaics as a coating,\u201d he says, \u201cis frankly the only practical and cost-effective way to do it.\u201d Heliovolt's printing process is meant to help make that integration possible. And Konarka talks of adding its dye-based 'Power Plastic' to more or less anything, from windows (where it would just cream off a bit of the light) to wind sheeters. None of these technologies, however cleverly mounted, will get the costs of generating electricity low enough for solar power to compete directly with coal, gas, wind or nuclear. But because solar panels are inherently easily decentralized, they do not have to compete with the cost of generating electricity; they just have to compete with the price consumers pay for it. This is four or five times more than the cost of generation, because the power companies need to pay for transmission networks, build new plants and please shareholders. So the industry's aim is to get significantly below 'grid parity'. This is the point at which the cost of borrowing the money to buy and install a solar-power system is more than covered by savings on your electricity bill. At the moment, grid parity is not quite within reach; in most places with a lot of solar cells there is or has been a great deal of government subsidy. In Germany, a particularly powerful subsidy is a government requirement that electric utilities be willing to buy electricity generated by small photovoltaic installations, such as those in homes and small businesses, at more than 50 cents a kilowatt-hour. Although this rate decreases over time, it is still a costly subsidy, and some wonder how long it can last in its present form. In its favour is popularity with the electorate \u2014 and, of course, with Germany's producers of solar cells. Reaching grid parity is not in itself enough. But if a mixture of much cheaper cells and adaptable, easily installed modules could bring the total cost of installation down by a factor of three, solar energy would start to look prudent, analysts say. \n               Spending to save \n             There is a lively research agenda in basic materials science that a thriving solar industry could use to drive costs down further still. Michael McGehee of Stanford University, the young investigator whose work on organic photovoltaics was part of the original inspiration for Nanosolar, is developing a proposal for a major initiative in solar energy research. \u201cWe are going to apply for a large centre here, funded by the Department of Energy,\u201d he says. \u201cWe have a team of people who will work on using sunlight to excite a semiconductor, or to split water to make hydrogen.\u201d If fully funded, it would have 16\u201320 principal investigators and be one of the biggest research groups with a specific target at Stanford. Steve Chu, the Nobel prizewinning Stanford physicist who now runs the Lawrence Berkeley National Laboratory on the other side of San Francisco Bay, has ambitious plans for an initiative called Helios. This would integrate new photovoltaic research with studies into other ways of capturing and storing sunlight, such as biofuels; the idea is currently under review at the Department of Energy. Also testimony to the research interest in this area is the way that it is being presented at meetings. Organizing a session on solar applications at last November's meeting of the Materials Research Society, McGehee found himself swamped with hundreds of abstracts; it was the third most popular of the meeting's 40 sessions. One potential source of funding for all this research is Proposition 87, which will be on the California ballot this November. The proposition, which is strongly opposed by oil producers, would increase the cost of drilling fees in order to raise $4 billion for clean-energy initiatives, including research. Vinod Khosla, a former partner at Kleiner Perkins, is a main backer of the initiative, and other venture capitalists are also on board. But the fact that Silicon Valley is abuzz with solar enthusiasm doesn't necessarily mean that all the activity there will trigger a revolution. Someone elsewhere might come up with the key breakthrough technology. And just because venture capitalists are successful in making money doesn't mean they will effect major economic changes. As Stephen Levy of the Center for the Continuing Study of the California Economy points out, venture capitalists have been saying that biotech would be the big new growth sector for years, \u201cand it is still 'just about to explode', with an emphasis on the 'just about'.\u201d Solar enthusiasts can respond that solar cells have no Food and Drug Administration to face, and that they don't need to invest hundreds of millions to get a product to market, as drug developers do. Yet a decade's growth, however buoyant, doesn't by itself mean that much. That growth needs to last for several decades to change an economy, and needs to accelerate to an even higher level to change the world. The difference between growing at a more than respectable 25% a year and at 44% a year \u2014 the rate at which volume grew in 2005 \u2014 is the difference between doubling in size in just over three years and in just over two. That may not sound a great deal, but over 15 years it means something growing at 44% would outdo something growing at 25% by a factor of eight. Between now and 2050, the difference is a factor of 500. And that could be the difference between providing just 2% of Earth's energy needs \u2014 and 10 times those needs. The remarkable thing is that the products of the semiconductor industry have grown at a yet faster rate for a similar length of time. If Silicon Valley can apply Moore's law to the capture of sunshine, it could change the world again. See Editorial,  \n                     page 1 \n                   . \n                     Solar energy: Radiation nation \n                   \n                     Resolving the energy crisis: nuclear or photovoltaics? \n                   \n                     Leapfrogging the power grid \n                   \n                     Applied physics: Solar cells to dye for \n                   \n                     A power that's clean and bright \n                   \n                     Nature Insight into materials for clean energy \n                   \n                     Nanosolar \n                   \n                     Solar buzz \n                   \n                     Cleantech blog \n                   \n                     The energy blog \n                   \n                     The Princeton Carbon Mitigation Intiative \n                   Reprints and Permissions"},
{"file_id": "443138a", "url": "https://www.nature.com/articles/443138a", "year": 2006, "authors": [{"name": "Henry Nicholls"}], "parsed_as_year": "2006_or_before", "body": "No one has seen a dodo in three and a half centuries, but that hasn't stopped the bizarre speculation about this extinct bird. Henry Nicholls investigates whether recent excavations in Mauritius could reveal the real creature. The dodo is perhaps the most famous symbol of extinction, yet no one knows exactly when or how it went extinct. In fact, no one knows much of anything about the flightless creature, which lived on the Indian Ocean island of Mauritius until soon after Portuguese and Dutch sailors showed up in the sixteenth century. Popular images perpetuate the notion that the dodo ( Raphus cucullatus ) was a tubby, pigeon-like bird with a huge bill, gaping nostrils and bulging eyes. Yet this clumsy apparition may be pure fiction, created from secondhand stories and poorly preserved specimens (see  'Dodo fiction' ). Thousands of dodo bones, hundreds of dodo studies and countless dodo enthusiasts have done little to illuminate how the dodo really lived and died. \u201cWe still know virtually bugger all about the dodo,\u201d says Julian Hume, a bird palaeontologist at the Natural History Museum in London. That may soon change, thanks to new excavations in a Mauritian swamp called the Mare aux Songes. For the second year running, a team led by Dutch geologist Kenneth Rijsdijk has been visiting the most important site ever to turn up dodo bones. The spot has languished untouched for decades, but in the years to come, mud from this dig could increase the number of known dodo bones by an order of magnitude. And there is an entire ecosystem, from bacteria to giant tortoises, preserved alongside the dodo remains. Together, the studies promise an unparalleled glimpse into the life and death of this enigmatic bird \u2014 perhaps even providing, for the first time, a faithful picture of the long-dead dodo. It may seem surprising that scholars disagree about so many basic facts, including what actually drove the dodo to extinction. There are many possibilities. The newly arrived sailors may have eaten so many birds that they caused the population to crash. Or perhaps the sailors destroyed its habitat beyond repair, or brought mammals \u2014 notably rats, pigs and goats \u2014 that trashed the landscape. Or a natural disaster, such as a tropical cyclone, may have pushed the bird to near-extinction before the sailors even arrived. The smart money is on the rats, but without the luxury of travelling back in time, it's impossible to be sure. Opinion is also divided over the date the dodo finally disappeared 1 . Hume has argued in  Nature  that detailed hunting records from a chief of Mauritius suggest his men bagged at least a dozen dodos between 1685 and 1688 (ref.  2 ). Others disagree, including Hume's friend and colleague Anthony Cheke, leader of a 1970s expedition to Mauritius by the British Ornithologists' Union. Cheke says that the word in the hunting records \u2014  dodaersen  \u2014 was used by the 1660s to describe another flightless bird on the island, the red rail ( Aphanapteryx bonasia ) 3 , 4 . The last reliable dodo sighting, he argues, dates from 1662, when a band of shipwrecked sailors waded out from Mauritius to a small islet and hunted down several birds \u201clarger than geese\u201d. \n               Up in smoke \n             Whenever and however it vanished, the dodo did not leave much of itself behind. Sailors brought back stories, sketches and stuffed birds. Only one bird is known to have been brought to Europe alive; it may be this creature whose skeleton wound up in the Ashmolean Museum in Oxford, UK. Tragically, in 1755, all but the head and one foot of this precious specimen went up in flames. One account of the story involves a heroic curator, who dived into the flames and hacked off the bits, saving them from certain destruction. The more widely accepted version is that the fire was no accident, but was deliberately lit to destroy a deteriorating specimen. The salvaged body parts remained the world's most important dodo relic for more than a century. Then in 1865, engineers were carving out a railway line alongside the Mare aux Songes on the east coast of Mauritius (see map). Local schoolteacher George Clark chanced upon dodo bones exposed during the digs, and he organized local workers to feel their way through the nearby swamp in search of more material. Clark eventually uncovered an impressive cache of dodo bones, which he sold to the Natural History Museum in London. It is largely this and other Mare aux Songes material extracted towards the end of the nineteenth century \u2014 a few thousand bones in total \u2014 that informs modern understanding of dodo anatomy. But because of the crude techniques used by Clark's crew, the Mare aux Songes has never produced a complete skeleton of a bird. Instead, almost all the dodo skeletons on display in the world's museums are the remains of several birds unearthed from the swamp by Clark and his immediate successors, and cobbled together into a dodo-like form. There is one exception, and it stands inside a six-foot glass case in the Mauritius Institute in the capital city of Port Louis. This specimen, found in the 1890s by a hairdresser and dodo enthusiast named Louis Etienne Thirioux, is the only known fully articulated specimen. It probably came from a cave at Le Pouce just south of Port Louis, says Hume. But Thirioux seems to have kept the exact location a secret, and the few people who have bothered to retrace his steps have turned up nothing as the valleys are now overgrown with impenetrable weeds. \n               Scant remains \n             Others have continued to search for dodo remains elsewhere on the island. In 1974, a handful of dodo material came out of a bore-hole at Mare S\u00e8che, but the landowner would not allow palaeontologists to excavate. \u201cYou can find bones in other places, but in very limited quantities,\u201d says Anwar Janoo, an ornithologist and consultant for the National Heritage Fund in Mauritius. He has found, for instance, some dodo fragments in cliffside caves at Baie du Cap at the southern tip of the island. Slaves and convicts used these caves as refuges, he says, and the bones are probably the remains of dodos captured and eaten by fugitives. Janoo has also found some bones at the bottom of a collapsed lava tunnel on Plaine des Roches in the north 5 . He claims to know of another dodo site in the north where French palaeontologist and filmmaker Didier Dutheil exhumed the back of a dodo skull in 1999. But neither Janoo nor Dutheil are letting on precisely where it is. The secrecy is frustrating, says Cheke: \u201cIt's amazing they haven't gone back or at least told somebody else where it is\u201d. A few thousand bones from Mare aux Songes, the Thirioux skeleton and Janoo's fragments are clearly not a lot to go on. But it wasn't the need for more specimens that led to the recent digs at the Mare aux Songes. Instead, a series of coincidences brought Rijsdijk, who is based at the Netherlands Institute of Applied Geoscience in Utrecht, and his colleague Frans Bunnik there in October 2005. They were looking not for dodo, but for pollen samples that might help them reconstruct the vegetation and climate of Mauritius before the Dutch settled the island in 1638. Rijsdijk and Bunnik identified several spots of interest, but neither of them realized that one of their sites \u2014 a marshy valley not far from the sea and surrounded by sugar-cane plantations \u2014 was the swamp Clark had excavated 140 years earlier. In the 1940s, the then British authorities ordered the Mare aux Songes to be filled with volcanic rubble, to combat the spread of mosquitoes and malaria on the island. Rijsdijk and Bunnik realized where they were when a local landowner told them that five boreholes had been drilled into the swamp in 1992, at the behest of a dodo devotee from Japan. The person who commissioned the work never claimed the cores, and the Dutch scientists were the first to see them. They were stunned by the richness of material in the cores. \u201cThey had an alternation of different types of sediment that showed we were dealing with a really dynamic environment,\u201d says Rijsdijk. There was one small problem: knowing where the cores had come from. A crude diagram made by the drill engineers sketched out the sea, a road, four palm trees and the boreholes \u2014 with no scale or direction. \u201cIt was like a treasure map,\u201d he says. \n               Dodo hunt \n             Locating a likely spot, they began to drill a speculative core. \u201cIt took us one and a half hours to get through 80 centimetres of rocky rubble,\u201d says Rijsdijk. But beneath they found dozens of dodo bones. This past July the team returned, armed with radar to locate the most promising areas, a digger to remove the top layer of rubble, and drills to bore into the peaty soil to depths of some eight metres. The cores came back up chock full of microbes and plant matter, along with bones from skinks, giant tortoises, parrots, owls and bats. Radiocarbon dates should soon pin down the age of these deposits, but the cores were completely free from introduced species \u2014 suggesting that they represented the dodo's world before humans arrived. \u201cThis provides us with a great opportunity to reconstruct this world,\u201d says Rijsdijk. In fact, the Mare aux Songes may capture more than just a moment in the life of the dodo, says David Burney, a palaeoecologist at Fordham University in New York. \u201cI'm predicting it's not just a snapshot,\u201d he says. \u201cIt may be a kind of movie, sampling the landscape more or less continuously back in time.\u201d Burney works in Hawaii, studying how the ecosystem was changed by the arrival of the Polynesians; similarly, the studies on Mauritius may reveal the effect that natural forces and human colonization have on island ecosystems. This summer's excavations focused on an area of swamp about the size of a small room. It yielded about 4,000 bones, of which a few hundred belonged to the dodo; early results will be presented at a conference in Oxford later this month. All told, there are three basins in the Mare aux Songes with palaeontological promise; they cover about 5 hectares and could contain tens of thousands of dodo relics, says Rijsdijk. Such bounty could plug some of the holes in scientists' understanding. Were male and female dodos different sizes? Did males sport an exaggerated bill or another trait to attract partners? How big were the dodos' eggs? And how did the Mare aux Songes end up with so much interesting material \u2014 was it a natural disaster? If so that would lend credence to the idea that dodos had been battered to near-extinction before humans arrived. Some hope that the swamp could even yield dodo DNA. An earlier study of dodo genetics, with DNA taken from the foot of the Oxford dodo, illustrates the potential of this research. It suggested that the dodo's closest living ancestor is the Nicobar pigeon ( Caloenas nicobarica ), and that the two diverged more than 30 million years ago 6 . But this date is long before volcanic Mauritius emerged from the sea. New studies could help clarify this relationship, or even yield new information, says Beth Shapiro, a specialist in ancient DNA with the University of Oxford. \u201cIf we were able to get ancient DNA from a lot of dodos from a long time period, we might be able to see how the population size changed as it approached extinction,\u201d she says. But the hot climate and acidic conditions of the Mare aux Songes will not have helped DNA preservation. Early analysis of the mud yielded DNA sequences from several different plant species, but so far nothing that looks as if it could have come from a dodo. \u201cWe have not yet given up,\u201d says Shapiro. But the dodo will remain the star of the show even without DNA. Some 350 years since the last dodo picked its way through the Mauritius undergrowth, opinions on the bird continue to change. \u201cIt may be dead, but it is very much alive,\u201d says Rijsdijk. The dodo attracts money even now. Rijsdijk has created the nonprofit Dodo Research Foundation, hoping to fund an expedition to the Mare aux Songes next year. He plans to excavate with painstaking care, avoiding the destructive mechanical techniques used this summer. And in the years to come, there could be boxes full of dodo bones to study \u2014 and perhaps, finally, enough material to dispel the many myths that have latched onto this bird. \n                     Flock of Dodos \n                   \n                     Dodo flew to its grave \n                   \n                     Naturalis dodo expedition blog \n                   \n                     The dodo at the Natural History Museum in London \n                   \n                     The Oxford dodo \n                   \n                     American Museum of Natural History on the dodo \n                   \n                     Species data on the dodo \n                   Reprints and Permissions"},
{"file_id": "442624a", "url": "https://www.nature.com/articles/442624a", "year": 2006, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "One way to keep carbon dioxide out of the atmosphere is to put it back in the ground. In the  first  of two News Features on carbon sequestration, Quirin Schiermeier asked when the world's coal-fired power plants will start storing away their carbon. In the second, Emma Marris joins the enthusiasts who think that enriching Earth's soils with charcoal can help avert global warming, reduce the need for fertilizers, and greatly increase the size of turnips. In 1879, the explorer Herbert Smith regaled the readers of  Scribner's Monthly  with tales of the Amazon, covering everything from the tastiness of tapirs to the extraordinary fecundity of the sugar plantations. \u201cThe cane-field itself,\u201d he wrote of one rum-making operation, \u201cis a splendid sight; the stalks ten feet high in many places, and as big as one's wrist.\u201d The secret, he went on, was \u201cthe rich  terra preta , 'black land', the best on the Amazons. It is a fine, dark loam, a foot, and often two feet thick.\u201d Last month, the heirs to Smith's enthusiasm met in a hotel room in Philadelphia, Pennsylvania, during the World Congress of Soil Science. Their agenda was to take  terra preta  from the annals of history and the backwaters of the Amazon into the twenty-first century world of carbon sequestration and biofuels. They want to follow what the green revolution did for the developing world's plants with a black revolution for the world's soils. They are aware that this is a tough sell, not least because hardly anyone outside the room has heard of their product. But that does not dissuade them: more than one eye in the room had a distinctly evangelical gleam. The soil scientists, archaeologists, geographers, agronomists, and anthropologists who study  terra preta  now agree that the Amazon's dark earths,  terra preta do \u00edndio , were made by the river basin's original human residents, who were much more numerous than formerly supposed. The darkest patches correspond to the middens of settlements and are cluttered with crescents of broken pottery. The larger patches were once agricultural areas that the farmers enriched with charred trash of all sorts. Some soils are thought to be 7,000 years old. Compared with the surrounding soil,  terra preta  can contain three times as much phosphorus and nitrogen. And as its colour indicates, it contains far more carbon. In samples taken in Brazil by William Woods, an expert in abandoned settlements at the University of Kansas in Lawrence, the  terra preta  was up to 9% carbon, compared with 0.5% for plain soil from places nearby 1 . From Smith's time onwards, the sparse scholarly discussion of  terra preta  was focused mainly on the question of whether 'savages' could have been so clever as to enhance their land's fertility. But Woods' comprehensive bibliography on the subject now doubles in size every decade. About 40% of the papers it contains were published in the past six years. \n               Loam ranger \n             The main stimulus for this interest was the work of Wim Sombroek, who died in 2003 and is still mourned in the field. Sombroek was born in the Netherlands in 1934 and lived through the Dutch famine of 1944 \u2014 the  Hongerwinter . His family kept body and soul together with the help of a small plot of land made rich and dark by generations of laborious fertilization. Sombroek's father improved the land in part by strewing it with the ash and cinders from their home. When, in the 50s, Sombroek came across  terra preta  in the Amazon, it reminded him of that life-giving 'plaggen' soil, and he more or less fell in love. His 1966 book  Amazon Soils  began the scientific study of  terra preta . Since then trial after trial with crop after crop has shown how remarkably fertile the  terra preta  is. Bruno Glaser, of the University of Bayreuth, Germany, a sometime collaborator of Sombroek's, estimates that productivity of crops in  terra preta  is twice that of crops grown in nearby soils 2 . But it is easier to measure the effect than explain it through detailed analysis. Everyone agrees that the explanation lies in large part with the char (or biochar) that gives the soil its darkness. This char is made when organic matter smoulders in an oxygen-poor environment, rather than burns. The particles of char produced this way are somehow able to gather up nutrients and water that might otherwise be washed down below the reach of roots. They become homes for populations of microorganisms that turn the soil into that spongy, fragrant, dark material that gardeners everywhere love to plunge their hands into. The char is not the only good stuff in  terra preta  \u2014 additions such as excrement and bone probably play a role too \u2014 but it is the most important factor. Leaving aside the subtleties of how char particles improve fertility, the sheer amount of carbon they can stash away is phenomenal. In 1992, Sombroek published his first work on the potential of  terra preta  as a tool for carbon sequestration 3 . According to Glaser's research, a hectare of metre-deep  terra preta  can contain 250 tonnes of carbon, as opposed to 100 tonnes in unimproved soils from similar parent material. The extra carbon is not just in the char \u2014 it's also in the organic carbon and enhanced bacterial biomass that the char sustains. \n               Ground control \n             That difference of 150 tonnes is greater than the amount of carbon in a hectare's worth of plants. That means turning unimproved soil into  terra preta  can store away more carbon than growing a tropical forest from scratch on the same piece of land, before you even start to make use of its enhanced fertility. Johannes Lehmann of Cornell University in Ithaca, New York, has studied with Glaser and worked with Sombroek. He estimates that by the end of this century  terra preta  schemes, in combination with biofuel programmes, could store up to 9.5 billion tonnes of carbon a year \u2014 more than is emitted by all today's fossil-fuel use 4 . \n               Mud pack \n             The year before he died, Sombroek helped to round up like-minded colleagues into the Terra Preta Nova group, which looks at the usefulness of using char in large-scale farming and as a carbon sink. The group was well represented at the Philadelphia meeting, although Glaser was not there. Their aim is to move beyond the small projects in which many of them are involved and find ways of integrating char into agribusiness. After all, wherever there is biomass that farmers want to get rid of and that no one can eat, char is a possibility. That means there are a lot of possibilities. One problem is that there is a new competitor for farm waste. Plant are largely made up of cellulose, indigestible material in cell walls. Recent technological advances make it likely that quite a lot of that cellulose might be turned into biofuel. At the moment, ethanol is made from corn in the United States and from sugar in Brazil; if it were made directly from cellulose, producers could work with a wider range of cheaper biomass. Given the choice of turning waste material into fuel or into charcoal, farmers might be expected to go for fuel, especially if that is the way that policy-makers are pushing them: US President George W. Bush promised $150 million for work on cellulosic ethanol in his 2006 state of the union speech. But Lehmann and his colleagues don't see biofuel as an alternative to char \u2014 they see the two developing hand in hand. Take the work of Danny Day, the founder of Eprida. This \u201cfor-profit social-purpose enterprise\u201d in Athens, Georgia, builds contraptions that farmers can use to turn farm waste into biofuel while making char. Farm waste (or a crop designed for biofuel use) is smouldered \u2014 pyrolysed, in the jargon \u2014 and this process gives off volatile organic molecules, which can be used as a basis for biodiesel or turned into hydrogen with the help of steam. After the pyrolysation, half of the starting material will be used up and half will be char. That can then be put back on the fields, where it will sequester carbon and help grow the next crop. \n               Negative thinking \n             The remarkable thing about this process is that, even after the fuel has been burned, more carbon dioxide is removed from the atmosphere than is put back. Traditional biofuels claim to be 'carbon neutral', because the carbon dioxide assimilated by the growing biomass makes up for the carbon dioxide given off by the burning of the fuel. But as Lehmann points out, systems such as Day's go one step further: \u201cThey are the only way to make a fuel that is actually carbon negative\u201d. Day's pilot plant processes 10 to 25 kg of Georgia peanut hulls and pine pellets every hour. From 100 kg of biomass, the group gets 46 kg of carbon \u2014 half as char \u2014 and around 5 kg of hydrogen, enough to go 500 kilometres in a hydrogen-fuel-cell car (not that there are many around yet). Originally, Day was mostly interested in making biofuel; the char was just something he threw out, or used to make carbon filters. Then he discovered that his employees were reaping the culinary benefits of the enormous turnips that had sprung up on the piles of char lying around at the plant. Combining this char with ammonium bicarbonate, made using steam-recovered hydrogen, creates a soil additive that is now one of his process's selling points; the ammonium bicarbonate is a nitrogen-based fertilizer. \u201cWe don't maximize for hydrogen; we don't maximize for biodiesel; we don't maximize for char,\u201d says Day. \u201cBy being a little bit inefficient with each, we approximate nature and get a completely efficient cycle.\u201d Robert Brown, an engineer at Iowa State University in Ames, has a $1.8-million grant from the United States Department of Agriculture (USDA) to fine-tune similar technology, although being in Iowa, he uses corn stalks not peanut hulls. \u201cWe are trying an integrated approach: we are trying to evaluate the agronomic value, the sequestration value, the economic value, the engineering,\u201d he says. Brown thinks a 250-hectare farm on a char-and-ammonium-nitrate system can sequester 1,900 tonnes of carbon a year. A crude calculation on that basis suggests the US corn crop could sequester 250 million tonnes of carbon a year. At the moment, no one knows how long this could go on; no one has yet found a ceiling for char addition. Stephen Joseph of Biomass Energy Services and Technology in New South Wales has built a number of char-producing machines in Australia that work at fairly large scales (the models have grown from an original 'Piglet' through a larger 'Daisy' to a positively bullish 'El Toro'). Joseph looks for companies with a waste problem such as a paper mill with spare scraps or a dairy with old bedding and manure, and then integrates char production into the business so that the heat produced in pyrolysis is used where the firm needs it. So far, Joseph's company is being brought in to solve waste-management problems, but he hopes the value of the char will become a selling point in itself. For that to happen, however, he needs some help. His machines can be tuned to make char of various sorts: different sized particles with different sized pores and different amounts of other elements. Which is the best? It's a question he asks in Philadephia, and one of the things that Brown's research in Iowa aims to find out. \n               The right protocol \n             Such technical unknowns are not the only obstacles on the road to a black revolution. One problem is that the purported benefits of char do not slot easily into the framework of the Kyoto Protocol, an international agreement to reduce carbon emissions. Lehmann hopes to see the process get going under the aegis of the protocol's Clean Development Mechanism, in which rich countries sponsor green projects in poor countries and get credit for the reduced emissions. To this end, he is amassing evidence that modern char techniques can actually keep the carbon involved locked up for centuries. His Cornell colleague John Gaunt is working on ways to present the technique as the sort of 'change in practice' that could count as a tradeable carbon-emission reduction of the sort allowed under Kyoto. Then there are your risk-averse farmers. They haven't heard of char. And they aren't going to buy it \u2014 let alone buy a strangely named machine to make it \u2014 unless they know it will make them money. It is no good pitching it to them with a mouthful of scientific caveats about not knowing the right kind of char for each type of soil or exactly how it works. You have to be able to sell specific benefits and real attractions. \u201cA lot of farmers are environmentalists,\u201d says John Kimble, a USDA man who has just retired from the National Soil Survey Center in Lincoln, Nebraska. \u201cBut they look at the bottom line, as we all do.\u201d After the afternoon coffee break in Philadelphia, Kimble takes the podium and the wind out of everyone's sails. He is sympathetic to the terra pretans goals \u2014 indeed he was a good friend of Sombroek's \u2014 but that doesn't stop him asking hard questions. \u201cCan you do this in a no-till way?\u201d is one tricky query. Kimble and many others have been pushing no-till farming, which basically means doing without ploughs, as a partial solution to erosion, pesticide run-off and fuel costs. The idea is that the less you mess with the soil, the less its components separate and wander away. But biochar is light and fine, like the black grit left in a barbecue. If you don't physically insert it into the soil, it might just blow away. Everyone listens politely. But while watching their responses, it was hard not to worry that the same enthusiasm that has brought them together might also trap them in a cul de sac. They obviously respect economics and pragmatic requirements. But these are not people principally moved by practical politics or bottom lines; they are people moved by ideals. They start from the basis that the answer lies in the soil, more or less whatever the question is, and can't quite understand why this isn't self-evident to everyone else. Faced, for example, with the suggestion that all corn matter be turned into ethanol, they tend simply to say \u201cWell it could be \u2014 but we hope, of course, it will go into the soil.\u201d They know they ought to be marketing  terra preta  as a resource, or a policy instrument; but they can't stop seeing it as a wonder. Policy is not always, or even often, dictated by pure rationality. Perhaps  terra preta 's compelling history and rich, earthy smell will go to the heads of that diffuse band of policy-makers who hand out the cash. The enthusiasts need to be more down to earth; but the policy people might benefit from getting their hands dirty. \n                     Barren soil is starving Africans \n                   \n                     Earthy bacteria faced with climate rap \n                   \n                     Reduction of soil carbon formation by tropospheric ozone under increased carbon dioxide levels \n                   \n                     Organic Farming in focus \n                   \n                     International Union of Social Sciences: obituary of Wim Sombroek \n                   \n                     Eprida \n                   \n                     Johannes Lehmann's page on Terra Preta \n                   Reprints and Permissions"},
{"file_id": "442739a", "url": "https://www.nature.com/articles/442739a", "year": 2006, "authors": [{"name": "Carina Dennis"}], "parsed_as_year": "2006_or_before", "body": "Much of what we know about cancer comes from studying mice, and potential therapies are tested in the animals. But the differences between the species can scupper the best laid plans of researchers and drug companies, reports Carina Dennis. It was in 1991 that Bob Weinberg first realized he had a problem with mice. He and his postdoc Tyler Jacks were trying to develop a mouse model for retinoblastoma, a childhood cancer of the retina. It results from the loss of a gene called  Rb , so the team genetically engineered mice to lack the same gene. But the mice didn't get retinoblastoma. Instead, they developed tumours in their pituitary glands. The finding shocked Weinberg. \u201cUp until then, I had always believed that all mammals were biologically equivalent,\u201d he says. \u201cThis planted the seeds of doubt in my mind.\u201d Weinberg, based at the Whitehead Institute for Biomedical Research in Cambridge, Massachusetts, is one of the pioneers of the molecular age of cancer research. He was involved in the early work on the first human cancer-causing and cancer-suppressing genes in the early 1980s. But when he saw that mutations in such genes didn't cause the same kind of cancer in mice and humans, he began to ask himself why. He became aware of other examples that challenged researchers' faith in how accurately mice could replicate human tumours, and has since sought to bring this to his colleagues' attention 1 . \u201cThere is a laundry list of problems with mouse models of cancer,\u201d he says. Although some genetically engineered mice have come a long way since the early days, many researchers concede that other kinds of mouse model remain especially problematic. Of the potential anticancer drugs that give promising results in tests on mice with cancer, only about 11% are ever approved for use in people 2 . It is also possible that drugs that would have worked in people failed in preclinical mouse trials, although there is no way of knowing. \u201cI'm not a naysayer who thinks we should discard valuable mouse models, thereby throwing the baby out with the bathwater,\u201d says Weinberg. \u201cMy rallying cry is that we need to be working on this problem.\u201d Now many researchers, Weinberg included, are doing just that, by making mouse models more faithful replicas of human cancer development. \n               Different strokes \n             Mice are still crucial for cancer research. After all, there is only so much one can learn from studying cells in a lab dish. Cancer is a complex, three-dimensional disease that changes, evolves and spreads through the body. Mice, easy to breed and genetically manipulate, and with a completed genome sequence, are the obvious choice for scientists wanting to pick apart these processes and test new drugs. And, as mammals, they share enough biology with us to make some useful comparisons possible. But despite having broad similarities, mice have significant differences that can scupper cancer experiments ( see graphic ). For one thing, most mouse tumours originate in different types of tissues from ours and, unlike us, their healthy cells can maintain the ends of their chromosomes, a key factor influencing which mutations tumour cells develop. One of the key mouse models, especially in drug testing, involves grafting human cancer cells into mice and seeing how the resulting tumours respond to treatment. But human cells are likely to behave differently in a mouse than in a human body, making results hard to interpret. Some of these problems are insoluble, arising as a result of fundamental differences between the two animals. But others can be fixed. In recent years, researchers have developed tricks to genetically engineer mice, or tinker with the human cells they graft into them, that more accurately reflect cancer in humans. And now both academia and industry are keen to put these mice to the test and see whether they can better predict how humans will respond to new drug compounds. \u201cIt took a while to make mouse models that accurately reflect human diseases but now that we have them, now is the time to use them,\u201d says David Tuveson of the University of Pennsylvania in Philadelphia. Early attempts to genetically engineer mice to develop cancer were, in retrospect, simplistic: turning a cancer-associated gene on or off in every cell in every tissue of the mouse. These mice often developed tumours, but they rarely looked anything like the human cancer, as Weinberg discovered. What's more, the technology used to alter mouse genes took effect early in development, whereas most human tumours develop later in adult life, following a stepwise series of mutations that turn a normal cell into an invasive cancer. Over the past decade, researchers have been able to recreate these steps more accurately. A key breakthrough was being able to control when and where the cancer-causing mutations occurred 3 . \u201cWe're getting the right combination of mutations at the right place at the right time,\u201d says Jacks, who now heads his own lab at the Massachusetts Institute of Technology in Cambridge. \n               Human touch \n             But recreating the sequence of genetic mishaps in a cluster of cells isn't necessarily enough to replicate human cancer. The physical differences between humans and mice can be a major obstacle, says Glenn Merlino of the National Cancer Institute (NCI) in Bethesda, Maryland. Merlino works on melanoma, which is induced by ultraviolet (UV) irradiation of the skin. Melanocytes, the pigment-producing cells that become cancerous in melanoma, are found in the outermost layer of human skin, but in mice, they are confined to hair follicles, making it harder to study the effects of UV radiation. What's more, the resulting tumours show a different cellular structure under the microscope. Merlino's team engineered mice to over-express a cell signal that causes melanocytes to be found in the outer layers of the rodents' skin, as in humans. They went on to show that very young, but not adult mice, exposed to UV radiation develop malignant melanoma, giving strong evidence to back up previous epidemiological studies highlighting the potential dangers of sun exposure in children 4 . Humanized mice such as these could help researchers tackle some fundamental questions \u2014 such as where and how cancer begins. \u201cThe problem is we don't know much about the cell-of-origin in human cancers,\u201d says Jacks. Some suspect that it is the stem cells that repair and renew adult tissues (see 'The root of the problem',  page 742 ). \u201cThose are hard questions to get to in humans. This is an example of the value of mouse models \u2014 they are highly manipulable, for which there is no comparison in human,\u201d says Jacks. The latest generation of mouse models mimics the human disease process more faithfully. But, in therapeutic terms, the real test is how well they reflect the human response to drugs. Early signs are encouraging 5 . \u201cNew mouse models hold a lot of promise. But now they have to show what they are worth,\u201d says Anton Berns, of the Netherlands Cancer Institute in Amsterdam. \n               Foreign bodies \n             The pharmaceutical industry has typically used 'xenograft' models to screen new drugs. In these, human cancer cells are injected under the skin of a mouse with a deficient immune system, to prevent rejection of the tumour. Although virtually every successful cancer drug on the market will have undergone xenograft testing, many more that show positive results in mice have had little or no effect on humans, possibly because the human tumours are growing in a foreign environment. \u201cIn many cases, the mouse is just a container for the human cells,\u201d says Roberto Weinmann, the director of oncology at pharmaceutical company Bristol-Myers Squibb in Princeton, New Jersey. Many companies therefore are interested in the new genetically engineered mouse models. \u201cWe are in a transition,\u201d says Giulio Draetta, head of basic cancer research at drug company Merck, based in Boston. \u201cWe are now building a substantial operation using transgenic models and are investing in licensing genetic models that currently exist as well as building our own models,\u201d he says. \u201cWe still rely very heavily on xenografts, as do most companies. But, as the models get better, we are watching the field very carefully,\u201d adds Richard Gaynor, vice-president of cancer research at Eli Lilly in Indianapolis. Some argue that a major factor preventing industry fully embracing transgenic mice are the patents associated with certain kinds of mice engineered to develop cancer, in particular the OncoMouse patents owned by chemical company DuPont, which constrains the commercial development of such mice 5 . Some think the patents have deterred drug companies from embracing transgenic mouse models. The licensing arrangements that commercial groups need to negotiate with DuPont have made using such models \u201cmore arduous and expensive\u201d, says Gaynor, although academics and non-profit groups can use the technology freely. Intellectual-property issues aside, xenografts will remain many companies' model of choice for the foreseeable future. They can be ready within weeks, whereas transgenic mouse models can take months to develop tumours and are often unpredictable in where and when they develop them. As hundreds of mice are required for screening, xenografts are quicker and cheaper. \n               Making connections \n             One possible way to improve xenografts is to manipulate the cellular environment into which cancer cells are injected. Connective tissue cells, or stromal cells, play an important role in the development of a tumour, often supplying it with the cell signals and nutrients it needs. But interactions between tumour cells and their neighbours are often lost in xenografts, because proteins from one species can't interact with their counterparts in the host. \u201cThe consequence is that the tumour often looks nothing like that seen in the patients,\u201d says Weinberg. Weinberg is among a number of scientists who have addressed this problem by putting human connective tissue cells into mice along with the cancer cells. The resulting tumours have a more human-like structure and metastasize more like human cancers do 6 , 7 . \u201cIt gets us closest to the human situation,\u201d says Ronald DePinho of the Dana-Farber Cancer Institute in Boston, Massachusetts, who has co-founded the company AVEO Pharmaceuticals to develop tissue-specific cancer models, including breast cancer. Adding human cells into the mix is one approach. Another way to make mice respond to drugs in a more human-like fashion is to give the mouse human genes. The driving force behind this is the considerable differences in the way that mice and humans metabolize drugs. For instance, two human enzymes CYP2D6 and CYP3A4, which together metabolize more than 70% of drugs on the market, have markedly different activities compared with their rodent equivalents 8 . The consequence is that mouse models may be of limited value in predicting the effectiveness or toxicity of drugs in humans. \n               Mine of information \n             Researchers have tackled this problem by engineering genes encoding some of the human enzymes into the mouse 8 . Studies of these 'humanized' mice indicate they can process some drugs in a more human-like manner. Combining these humanized mice with revamped mouse cancer models could be a powerful way to test drug candidates. In the future, researchers may broaden the types of human genes they add in to the mouse. \u201cMy hope is that we will be able to create mouse cells and tissues that may closely resemble human tissues,\u201d he says. But there is a limit to how far scientists can get mice to be like humans. So other researchers are mining human clinical data for insights into the cancer process, then using this to inform mouse studies. In the past, clinical data have often been collected on an ad hoc basis. But now researchers are tackling the process in a systematic and comprehensive fashion. Assisted by the Internet and better bioinformatics tools, there are large-scale efforts to collect and catalogue data on a wide range of human tumours. Howard Fine of the NCI is leading one such initiative. The aim is to perform genetic and molecular analyses of samples of human brain tumours sent to the NCI. Data are correlated with each patient's clinical course. The study is a pilot for the NCI's larger cancer Biomedical Informatics Grid, which will provide a global network for researchers to input information and access bioinformatics tools for mining cancer data. The study, which currently has data for 700 tumours and will eventually contain information for 2,000 tumours, is already yielding results, says Fine, who hopes to publish the findings shortly. Researchers are increasingly shuttling between human and mouse, using information from human data to refine mouse models and using insight from the mouse to uncover new disease processes and test predictions for clinical responses. But industry and the scientific community shouldn't expect too much from the humble rodent. \u201cMice are valuable but they are, after all, still mice,\u201d says Fine. \u201cThe best study subject will always be the human.\u201d (See  box ) \n                     Cancer: New fronts in an old war \n                   \n                     Cancer: Caught in time \n                   \n                     Cancer: The root of the problem \n                   \n                     Neonatal sunburn and melanoma in mice \n                   \n                     Mice mimic pancreatic cancer \n                   \n                     Mouse sequencing plan aims to boost models \n                   \n                     Mouse genome special \n                   \n                     Mouse models of human cancers consortium \n                   Reprints and Permissions"},
{"file_id": "442620a", "url": "https://www.nature.com/articles/442620a", "year": 2006, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "One way to keep carbon dioxide out of the atmosphere is to put it back in the ground. In this, the first of two News Features on carbon sequestration, Quirin Schiermeier asks when the world's coal-fired power plants will start storing away their carbon. In the  second , Emma Marris joins the enthusiasts who think that enriching Earth's soils with charcoal can help avert global warming, reduce the need for fertilizers, and greatly increase the size of turnips. Ketzin, a dozy village of 4,000 people west of Berlin, hardly looks like a vision of the future. Nestled in the Havel countryside \u2014 an idyllic mix of rivers and forests \u2014 it has a small tourist industry and, as is typical for such parts of eastern Germany, a not-so-small unemployment problem. \u201cThere's no news at the moment\u201d, says the community's website. But there could be news soon. In 2004, Ketzin was chosen as the site of mainland Europe's first large-scale carbon storage demonstration project. By the end of the year, drilling will start at a former gas storage facility on the edge of town. In the next two years, some 60,000 tonnes of carbon dioxide (roughly the annual carbon dioxide output of 40,000 cars) will be injected into an aquifer of salty water 700 metres beneath the surface. The Ketzin project will test the 'storage' part of carbon capture and storage (CCS), a strategy designed to allow energy to be generated from fossil fuels without the carbon dioxide produced in the process ending up in the atmosphere. Little more than a fringe idea five years ago, CCS was singled out at the 2005 G8 summit as a technology that could make a difference to climate change. Experts see it as a central part of any strategy for maintaining the generation of energy at today's levels: as Vassilios Kougionas, a European Commission officer in charge of clean-coal initiatives and international energy relations, puts it, \u201cWithout CCS there is no point in continuing with fossil fuels.\u201d And yet, for all this enthusiasm, there is a distinct lack of urgency in government approaches. The countries most interested in CCS have, at best, preliminary plans for it; most haven't even got that far. Meanwhile, the number of power stations whose carbon dioxide is neither captured nor stored is rising inexorably, as is the atmospheric level of the gas. To some observers, this represents a failure not of science or technology, but of will. \u201cIt does require quite substantial research and technology efforts to make CCS better and more efficient \u2014 but we could still start today if desired,\u201d says Hans Ziock, a physicist at Los Alamos National Laboratory in New Mexico, who has worked on carbon dioxide capture technologies for more than a decade. David Hawkins, director of the National Resources Defense Council's Climate Center in Washington DC, goes further: \u201cGlobal efforts are completely out of scale with what is needed.\u201d \n               Act now or pay later \n             The International Energy Agency (IEA) predicts that by 2030, global energy demand will grow by 1.7% a year. Although contributions from nuclear and renewable sources may increase, the IEA predicts that 85% of the rise in demand will be met by greater use of fossil fuels. This means the overall capacity of coal-fired power plants will have to double in that time, from 1,100 gigawatts to 2,200 gigawatts 1 . Taking into account the existing plants that will be shut down, the world is looking at 1,400 gigawatts' worth of new coal plants (see graphic on page 623). This doubling of capacity is the greatest expansion of power generation in the planet's history. Since 1750, humanity's burning of coal has released about 150 billion tonnes (gigatonnes) of carbon into the atmosphere. During their lifetimes, the new generation of plants will release 140 gigatonnes. Meanwhile, climate scientists are arguing that carbon dioxide levels should not be allowed to get much higher than 550 parts per million (p.p.m.); the current level is 380 p.p.m., which compares with 280 p.p.m. in the eighteenth century. Some argue that the ceiling needs to be a lot lower. Robert Socolow is a physicist and co-principal investigator of the Carbon Mitigation Initiative at Princeton University in New Jersey. He calculates that if carbon dioxide levels are to be kept in the desired range, then humanity needs to avoid about a third of the emissions expected in the next 50 years. That means finding a way to not release 175 gigatonnes of carbon. \u201cIf we get going now,\u201d he says, \u201cthe job will be less than half as difficult. If we don't it means we're running a very costly strategy.\u201d The good news is that, in principle, such vast amounts of gas could indeed be tackled by CCS. Deep aquifers in the world's sedimentary basins have a total storage capacity estimated at between 1,000 and 10,000 gigatonnes 2 . Pumping carbon dioxide into them is a straightforward matter. Oil companies already pump carbon dioxide into petroleum reservoirs on a routine basis as a way of flushing out the hydrocarbons. And experience with oil shows that such reservoirs can keep their contents stored away for geological lengths of time, points out G\u00fcnter Borm, director of geo-engineering at Germany's National Research Centre for Geosciences in Potsdam and coordinator for the European Union-funded Ketzin project. Not all reservoirs are as well adapted to CCS as a 100-million-year-old oilfield might be. Some may leak, and in some there might be a risk of sudden, catastrophic releases of gas. In low-lying land, such leakages could suffocate people because carbon dioxide is heavier than air, so will fill up valleys and basins. Under the sea, gas-filled reservoirs could potentially trigger landslides and thus tsunamis. But provided storage sites are chosen carefully, designed for safe operation, and properly monitored, the risks are manageable, says Lynn Orr, director of the Global Climate and Energy Project at Stanford University in California. At Ketzin, scientists will keep track of any undesired chemical interactions between carbon dioxide and minerals, which could in principle dissolve the 'cap-rock' that seals a storage site, or contaminate drinkable ground water. Scientists monitoring a smaller storage project, the Frio Brine Pilot Experiment in Texas, recently reported that the injection of carbon dioxide had made the brine 1,500 metres down substantially more acid 3 ; such acidic brine could potentially eat through the surrounding rock and escape into higher aquifers. But Hawkins counsels against too much pilot-project research of this type. He thinks that each reservoir will have unique geological characteristics that will need to be assessed  in situ . He also points out that even if some reservoirs leak, it still makes more sense to use them, and thus spread out emissions over time rather than do nothing. Everyone agrees that large-scale carbon dioxide storage would be a gargantuan technical feat. Locking away 250 million tonnes of carbon per year \u2014 equivalent to 4% of annual global emissions \u2014 would require an injection of 25 million to 35 million barrels per day, depending on compression density. That's equivalent to about a third of the flow of oil currently coming from reservoirs. According to Orr, if the infrastructure used to pump carbon dioxide into the ground was roughly the same size as the infrastructure currently deployed to bring oil to the surface, it could deal with about a seventh of the world's production of fossil-fuel-generated carbon. That is less than half the amount produced at power stations and large factories \u2014 the sources for which CCS is best suited. \n               Ground work \n             \u201cIt's a big enterprise,\u201d says Socolow. But pipeline building and well-drilling are mature and remarkably inexpensive technologies, and running costs would be extremely low. Experts calculate that setting up storage facilities, each capturing several million tonnes of carbon per year, for the carbon dioxide produced from hundreds or thousands of plants, might cost as much as $80 billion. In a world set to invest $16 trillion in energy by 2030, $80 billion is not an unthinkable amount; the cost of deep disposal for Britain's nuclear waste has recently been estimated as \u00a311.3 billion ($21 billion). What's more, the figure might come down as the technology matures and economies of scale cut in. As yet, industrial-scale CCS activities are limited to just three sites \u2014 in Norway, Canada and Algeria \u2014 and to megatonnes rather than gigatonnes of carbon. Since it began in 1996, the Norwegian project has pumped around 10 million tonnes of carbon dioxide 1,000 metres beneath the North Sea bed into the Utsira Sand formation; the carbon dioxide is a contaminant in natural gas from the Sleipner West field. In the Canadian and Algerian projects, the gas being stored away is also being used to enhance the productivity of oilfields, which covers some of the costs. All these sites have been continuously monitored for possible leakages, but none seems to have occurred. In total, 11 or so full-scale CCS projects are planned, or have been proposed, in the United States, Canada, Britain, Germany, Norway and Australia. But given present trends, experts think it unlikely that CCS will be employed at any substantial level before 2030, by which time the 1,400-gigawatts' worth of power stations will already have been built. For CCS to make a difference, the mechanisms it requires must be available to a substantial fraction of those plants after they have been built. Conventional coal-burning technology, which will probably be used to produce 1,200 of the 1,400 gigawatts, produces flue gases that are about 14% carbon dioxide. If the carbon dioxide is to be captured and stored it must first be 'scrubbed' from the gas stream, typically by running the gases through an amine solution. This takes up the carbon dioxide and then, when heated, releases it in pure form. The problem with using this technique is that the equipment needed not only costs money \u2014 it also takes up a lot of space. Fitting such equipment into a plant not designed for it is expensive. The main alternative to the conventional plant is the integrated gasification combined cycle (IGCC). The capital costs for IGCC plants are 20% greater than for conventional plants. But they have environmental benefits \u2014 among which is being cheaper to kit out for CCS. In IGCC plants, the fuel \u2014 coal, fuel oil or biomass \u2014 is introduced into a hot gasifier along with oxygen and steam. This produces a fuel gas consisting mainly of carbon monoxide and hydrogen. The carbon monoxide then goes through a second 'shift' reaction with steam, making carbon dioxide and more hydrogen. The carbon dioxide can be relatively easily separated at this point. Four IGCC plants are currently in operation \u2014 two in the United States, one in the Netherlands and one in Spain. Although more expensive, and less profitable, than conventional plants, they have very low emissions of sulphur dioxide and other pollutants. Carbon dioxide capture from plants such as these would be significantly easier and cheaper than from conventional plants. \n               Making it pay \n             At the moment, the efficiency of IGCC plants is about 40%, which is roughly the same as that for good conventional plants. CCS would drop the efficiency of both sorts of plant to about 30%. But IGCC plants use two thermodynamic cycles; the hydrogen from the gasifier and the shift reaction drives a gas turbine while the heat from that turbine and the gasifier drives a separate steam turbine. Having more than one cycle means that, in principle, it should be possible to push the overall efficiency much higher, to the point where, even when paying the energy penalty associated with CCS, the plants would still be competitive. \u201cFirst comes efficiency, then CCS,\u201d says Jacek Podkanski, a senior energy technology specialist with the IEA. The US 'FutureGen' initiative, a $1-billion public\u2013private partnership to design, build, and operate a coal-fuelled zero-emissions power plant by 2015, aims to prove the technical and economic feasibility of a commercial-scale IGCC plant fitted with CCS. In Europe, the German energy company RWE Power has recently announced that it will invest e1 billion (US$1.3 billion) in the construction of a 450-megawatt IGCC plant in Germany. The plant, fully equipped for CCS, could become operational in 2014. Other companies, such as Swedish Vattenfall, are also experimenting with ways to get more efficiency out of traditional plants, and with 'oxyfuel' plants in which the coal is burned in pure oxygen, producing a flue gas that is richer in carbon dioxide and so better suited to CCS. \u201cWe're by no means proceeding as though there were all the time in the world, but new technologies need time,\u201d says Johannes Ewers, head of new power-plant technologies at RWE. \u201cWe do know that if we want to burn coal we have to reduce carbon dioxide.\u201d In theory, the EU's emissions trading system should be providing incentives for such developments by putting a price on emissions. Such incentives have been shown to work: the sequestration of carbon dioxide from the Sleipner platform in Norway is the oil and gas company Statoil's response to the country's $50 per tonne of carbon dioxide tax on emission-intensive industries. The cost of capturing, transporting and storing a tonne of carbon dioxide is estimated by the Intergovernmental Panel on Climate Change to be between $20 and $70, which is why the Statoil sequestering project makes sense. But emitting a tonne of carbon dioxide currently costs less than $20 on the European exchange, and analysts doubt that the price is likely to increase much in the future. \u201cThe price of emitting carbon dioxide must rise, otherwise it just won't work,\u201d says Borm. \u201cCCS is perfectly doable at modest costs,\u201d says Jon Gibbins, a CCS expert at Imperial College London and project manager with the UK Carbon Capture and Storage Consortium. \u201cBut politically we're unfortunately in a very chaotic situation where it's hard to look forward for more than one or two years. If you want to tackle climate change you should do it aggressively or not bother at all.\u201d \n               The road ahead \n             For Gibbins, the best way forward is to ensure that every new plant is 'capture-ready' \u2014 that is, designed in such a way that CCS mechanisms can be relatively easily incorporated at a later date. The UK government is considering requiring that all new coal-fired plants are capture-ready. The idea is also part of an EU\u2013Chinese 'memorandum of understanding' on near-zero-emissions power generation technology, and was highlighted in the 2005 G8 action plan. But some fear that capture-readiness will just provide a cheap way of doing a small amount to cut emissions. \u201cThe term 'capture-ready' is pretty meaningless, because its definition includes subsequent installation of unidentified equipment,\u201d says Hawkins. To him, it would make more sense for governments to set up well-defined performance standards for power-generating facilities, such as a maximum carbon dioxide output allowed per unit of electricity. Such standards could become gradually more strenuous as better technologies become available. Another possibility, Socolow points out, would be simply to subsidize the technology. He thinks 2\u20133 cents per kilowatt-hour would make CCS a profitable route for a new coal-fired plant using today's technologies. That is only a little more than the 1.8-cent subsidy the United States currently gives producers of wind-generated electricity. No matter which technological options and economic or regulatory incentives are used, CCS will cost society money. \u201cElectricity will clearly become more expensive,\u201d says Ewers. How expensive depends on technology choices and geographical circumstances, but rough estimates suggest that the average production costs of electricity will double, from 4 to 7 cents per kilowatt-hour. This might be bad news for coal producers, as, all other things being equal, it would make the fuel they sell much less desirable, tilting the market towards natural gas. But it would not necessarily be a terrible burden for consumers, says Socolow. As the costs of distribution and transmission are hardly affected, he calculates that the retail cost of electricity would increase by just 20%. These costs, says Gibbins, are trifles compared to the long-term benefits of allowing companies to go on generating power with proven technology but without adding to the greenhouse effect. He also points to a peculiar long-term advantage. If fossil fuels are left unburned, they could be used in the future. If burned with CCS, they are gone for good, with the environment unchanged. \u201cIt doesn't make a lot of difference in which form carbon stays in the ground,\u201d he says. \u201cBut with carbon dioxide rather than desirable fossil fuels in the ground you have a very different path ahead.\u201d \n                 See Editorial, page 601 \n               \n                     Carbon credits for the Joneses \n                   \n                     Can biofuels finally take center stage? \n                   \n                     Gloomy outlook for Blair \n                   \n                     Carbon dioxide storage holds limited promise \n                   \n                     Emissions trading: The carbon game \n                   \n                     In focus climate change \n                   \n                     EU CO2sink project (Ketzin) \n                   \n                     International Energy Agency \n                   \n                     IPCC Special Report on Carbon Dioxide Capture and Storage \n                   \n                     US National Commission on Energy Policy \n                   \n                     National Resources Defense Council \n                   \n                     MIT Carbon Capture and Sequestration \n                   \n                     National Energy Technology Laboratory \n                   Reprints and Permissions"},
{"file_id": "443261a", "url": "https://www.nature.com/articles/443261a", "year": 2006, "authors": [{"name": "Helen Pearson"}], "parsed_as_year": "2006_or_before", "body": "We've been told to eat less and move more to battle the growing obesity epidemic. But could getting more shuteye also be a way to fight the fat? Helen Pearson investigates. For nutrition researcher Arne Astrup, it was a tired, overweight young girl who got him thinking. Twelve years old and 20 kilograms overweight, she was bright and more active than the average kid. Genetics and lifestyle seemed to be on her side: her slim parents packed her a nutritious lunch for school and did everything to ensure she ate healthily at home. But she was also something of a night owl. She loved to read and watch television into the night and did not sleep for more than seven hours. Astrup was aware of emerging research pointing to links between poor sleep patterns and appetite. After he discussed the possible connection with the family, the girl stayed in bed longer, for nine or ten hours. Within one month of starting to sleep longer, she started to lose weight and her cravings for junk food dropped. Scenarios such as this one, Astrup says, heightened his interest in the research. \u201cI think the sleep story is really fascinating,\u201d he says. \u201cIt seems to be something that might be quite fundamental.\u201d He is now planning studies on whether sleep-deprived people eat more from a buffet. Astrup is one of many medical researchers who have been struck by the tale of two growing epidemics and how they are intimately intertwined. Sleeping and eating are two of our most elementary drives. But in Western society both are veering out of control: we starve ourselves of sleep and gorge ourselves on food. Now many biologists are asking whether our cavalier attitude to sleep could be at least partly responsible for our expanding bodies. The idea is not as far fetched as it sounds, because there are plausible (if sketchy) biological mechanisms that could explain it. Overlaps are emerging between the brain mechanisms that control sleep and those that control appetite \u2014 mechanisms that may, in our evolutionary past, have been essential to fend off starvation. Many groups are now trying to figure out exactly how these circuits interconnect. The US National Institutes of Health (NIH) plans to commit as much as $2 million to such studies this year. \n               A diet of sleep \n             In one clinical trial at the NIH in Bethesda, Maryland, sleep is being doled out like a drug. The trial will test whether tired, obese people lose weight after increasing their sleep by just one and a half hours each night. \u201cIt's a possible way to control obesity that might even be pleasurable,\u201d says obesity researcher David Allison of the University of Alabama, Birmingham. \u201cMost people would say 'Sleep a little more? Yes I'd love to.'\u201d Some researchers question whether too much emphasis and money, in the obesity battle, has been placed on poor diet and lack of exercise. They say that good sleep \u2014 an activity that we ideally spend around one-third of our lives doing \u2014 is an integral part of the package of good health. If so, then many people could be sabotaging their own attempts to lose weight simply by depriving themselves of sleep. \u201cWe need to add in a third strand,\u201d says circadian-biology researcher Fred Turek at Northwestern University in Evanston, Illinois. It is clear that fresh ideas about obesity are needed. Two-thirds of Americans, and some 1 billion people worldwide, are overweight or clinically obese. The clinical definition of overweight is having a body mass index of 25 kg m \u22122  or more, but health risks can creep up at lower levels. The World Health Organization's 2002 World Health Report, for example, attributed about 58% of diabetes, 21% of ischaemic heart disease and 8\u201342% of certain cancers to a body mass index above 21 kg m \u22122 . Researchers have pinned most of the blame for obesity on genetic make-up, too many calories and too little activity. But genes cannot, for now, be changed and efforts to get people to switch their diets and exercise habits seem to have had little effect so far. Alongside the rising levels of obesity has been a parallel decline in the number of hours people spend sleeping \u2014 the first hint that sleep patterns could be connected to obesity. Americans' daily sleep has dropped from between eight and nine hours in 1960 to less than seven hours today. A similar trend is thought to have occurred in most industrialized nations. Most people blame televisions, computers, all-hour supermarkets and the attitude held in some countries that sleep is a superfluous pastime that could more usefully be filled by school, work or play. \n               Burning the midnight oil \n             The anecdotal correlation between poor sleep and obesity has been borne out by epidemiological studies. At least a dozen reports, from different parts of the globe, in both children and adults, have consistently found that people who sleep less are more likely to have a weight problem. In one well-publicized study 1 , James Gangwisch at Columbia University in New York and his colleagues matched up the self-reported sleep habits and body mass index of more than 9,500 people from across the United States, gathered between 1982 and 1992. They found that those between the ages of 32 and 49 who slept for five hours each night were 60% more likely to be obese than those who slept for seven or more. This was true after they controlled for other obvious factors connected with obesity, such as education, age, physical activity and smoking. And a study tracking British children as they grow up showed that poor sleep in three-year-olds is an important factor in predicting obesity at the age of seven, alongside well-established risk factors such as having overweight parents or watching television for long periods 2 . Of course, a host of mundane explanations could account for the observation that poor sleepers tend to be overweight. One possibility is that obese people tend to sleep badly because they are obese and unhealthy. Another possibility is that people who are awake for longer simply have more time on their hands to eat. A third is that tired, irritable people lack the motivation to eat healthily or go to the gym \u2014 as Gangwisch puts it, \u201cthey might say screw this, I'll eat a bag of chips\u201d. For now, researchers cannot rule out some of these explanations and it is likely that they play at least a part in the connection. But the ties between sleep and obesity are thought to run deeper than this because of another line of evidence from studies in humans and animals. This shows a biological mechanism by which sleep can promote appetite. \u201cSome of the biggest names in the field, who people trust, are saying there is a link,\u201d says neuroscientist Cliff Saper of Harvard Medical School in Boston, Massachusetts, who is starting his own studies into the association. One of those big names is Eve Van Cauter at the University of Chicago in Illinois, whose laboratory has produced a series of studies showing how sleep deprivation leaves its mark on metabolism and hormones. In one study 3 , 12 young men were allowed to sleep for only four hours a night on two consecutive nights. The researchers tested levels of hormones including leptin, which is released by fat cells and signals satiety, and ghrelin, which is produced by the stomach and signals hunger. They compared these hormone levels with those recorded after two nights of nine hours' sleep. \n               Out of synch \n             After two sleep-deprived nights, the men's blood levels of leptin had dropped by an average of 18% and their ghrelin levels had soared by 28%. At the same time, the bleary-eyed men said that they felt more hungry, particularly for carbohydrate-rich foods such as cakes, biscuits, crisps and bread, compared with proteins, fruit and vegetables. These findings in people have been mirrored by studies in tired mice. Mimicking human sleep deprivation in mice and rats is normally difficult because the methods used to keep the animals awake, such as plunging them in water, are also stressful or require extra physical activity from the animal. So although sleep-deprived rats tend to eat far more, they still lose weight. But studies on mice whose body clock is faulty underline the link between sleep patterns and obesity. Turek studied mice genetically engineered to lack a working version of a protein called Clock, which is active in a brain region called the suprachiasmatic nucleus that orchestrates the body's 24-hour rhythm. This region forms part of the brain called the hypothalamus, which controls many of our unconscious body functions including sleep and appetite. Mice without Clock lose their normal patterns of sleep, feeding and activity \u2014 and, as Turek and his colleagues found 4 , after several weeks, they grow obese and develop the hallmarks of metabolic syndrome in a few months. This has an array of symptoms including high blood sugar and cholesterol, and low insulin that, in humans, tend to be precursors to diabetes and heart disease. Together, Van Cauter and Turek's experiments suggest that poor sleep can rapidly wreck the body's normal systems for regulating appetite and predispose people and animals to obesity. What is not clear is the precise cellular and molecular chain of events that links one to the other \u2014 and it is these details that the NIH's $2-million injection is designed to reveal. \u201cWhat we're missing is the piece of evidence that would make it definitive,\u201d Allison says. Researchers do know that there are overlaps between the systems that regulate sleep and those that control appetite in the brain. Much attention has focused on a huddle of cells in the hypothalamus and the two proteins they produce. These proteins are known as either orexins or hypocretins \u2014 two groups simultaneously discovered and named them in 1998. At that time, researchers found that an injection of orexins into the brains of rats stimulated feeding, indicating that the proteins were involved in controlling appetite. Later on, other teams found that the system is defective during narcolepsy, a condition in which people fall swiftly and deeply asleep at inappropriate times. Intriguingly, narcoleptics are also often overweight. Researchers now think that orexin neurons promote wakefulness, feeding and exploration in animals. \n               Curbing the epidemic \n             It is not clear how orexins are involved in the sleep-deprivation and obesity story. One idea is that lack of sleep interferes with normal circadian activity in the hypothalamus and boosts the activity of orexin neurons. This may directly affect energy expenditure and feeding, and could also alter the production of leptin, ghrelin and perhaps other appetite-control hormones. But this is just one of several possible mechanisms. \u201cIt's a concert of disharmony,\u201d Van Cauter explains. \u201cSleep affects our entire physiology and sleep deprivation will have adverse effects at all different levels.\u201d In evolutionary terms, it is vital to hook up appetite and wakefulness. A mouse must eat to stay alive and be awake to forage \u2014 so it needs a mechanism that can recognize when fuel supplies are low and wake it up to find food. It is possible that humans have a similar control system that, when our ancestors were battling starvation, sent them searching for essential food. But now, anything that keeps us awake more than usual somehow taps into this circuit, sending us scavenging in the refrigerator for extra calories that we no longer burn off. A study by Tamas Horvath and Xiao-Bing Gao at Yale University 5  found that orexin neurons have a low threshold for activation. Overnight food deprivation boosts the formation of new synapses that excite them \u2014 which presumably encourages foraging and eating. Horvath suggests that these neurons are too easily stimulated, for example by stress or stray thoughts about work. Eating \u201cis an unfortunate by-product of arousal\u201d, he says. A pressing question for public health is whether the link with sleep patterns can be exploited to help battle obesity. Giovanni Cizza at the National Institute of Diabetes and Digestive and Kidney Diseases in Bethesda, Maryland, hopes to tackle this issue in a clinical trial funded by his institution. Cizza is recruiting 150 tired, obese people who sleep for six hours per night or less. Some of them will be taught to increase their sleep to seven-and-a-half hours (and offered a small fee as encouragement). The team will test whether, over a year, the extra sleep has an effect on weight, body fat, and leptin and ghrelin levels. If sleep can help shed even a fraction of excess weight, it could have a big impact on the health of the population, Cizza says. Following this logic, sleeping drugs could double as diet pills. Van Cauter says that she has been approached by one pharmaceutical company to discuss this possibility. Sanofi-Aventis, the Paris-based manufacturers of the widely used sleeping pill Ambien say that they do not have any planned trials. And drugs may not mimic the beneficial effect of a normal night's sleep if they do not fully recapitulate the pattern of sleep phases. The US Department of Defense has funded a four-year, $2-million study to examine whether the anti-fatigue drug called modafinil, which is taken by pilots to keep them awake, could be contributing to a rise in obesity in airforce veterans. Modafinil, also prescribed for narcolepsy and certain sleep disorders, is thought to exert some of its rousing effects on the orexin system. Many experts doubt that more sleep, be it natural or drug-induced, will be the simple answer to weight loss. Once a person is overweight, then poor sleep and uncontrolled appetite could become part of a vicious cycle: obesity might make it hard to sleep, and poor sleep might make weight harder to shed. \u201cIt's not like we can get everyone to sleep eight or nine hours a night and we'll solve America's obesity problem,\u201d Turek says. Instead, researchers are keen to identify those with 'high-risk' sleeping patterns in order to prevent problems before they arise. Van Cauter is interested in finding the seemingly lucky people who survive on a few hours of sleep without appearing to suffer. (One of the many mysteries of sleep is why some people naturally sleep nine hours and others only four.) Even though this group think they can manage fine on so little sleep, they may unwittingly be vulnerable to the long-term health effects of sleep deprivation. So an early warning sign, such as altered leptin, might alert doctors that the body is actually suffering more than its owner knows. It may, say some, be most useful to identify children who are not sleeping enough and encourage parents to change their children's sleep habits before they become ingrained. \u201cIf it's true that by sleeping half an hour extra a night our kids could be less obese it could have a huge public-health impact,\u201d Saper says. \n                     Sleep report opens eyes \n                   \n                     Neuroscience: While you were sleeping \n                   \n                     Restless nights, listless days \n                   \n                     Sleep insight \n                   \n                     WHO Obesity \n                   Reprints and Permissions"},
{"file_id": "442736a", "url": "https://www.nature.com/articles/442736a", "year": 2006, "authors": [{"name": "Laura Spinney"}], "parsed_as_year": "2006_or_before", "body": "The detection of cancer at an early stage in its development can be life-saving. With research efforts under way to find better methods to detect minuscule tumours, Laura Spinney finds out how near some of these cancer 'biomarkers' are to the clinic. When it comes to halting cancer, nobody disputes the value of early detection. Dr George Papanicolaou's smear test, which measures changes in the cervical lining, is a testament to the importance of screening. Since 1950, when it was introduced, the Pap smear has reduced mortality due to cervical cancer by 70% in developed countries. About half of lung-cancer patients whose cancer is caught while it is still restricted to the lung survive. Nearly all of those diagnosed after it has spread die. (See graphic, below.) Yet for all its promise, early detection remains the poor relation of cancer treatment. In 2005, the US National Cancer Institute (NCI) spent around 8% of its research budget on detection and diagnosis, compared with 22% on treatment. Britain's National Cancer Research Institute, a collaboration of the country's major cancer-research funders, spent 9% of its 2004 budget on early detection, diagnosis and prognosis, versus 19% on treatment. \n               Seeing the signs \n             Nevertheless, the technology needed to detect cancers at the very earliest stages is within sight. Advances in mass spectroscopy and in the detection of mutant DNA, for example, are now making it possible to pick up very low levels of tell-tale molecules. Meanwhile, medical-imaging technology is improving our ability to peer inside the body (see  'Powers of detection' , overleaf). Ultimately, scientists hope to devise straightforward, non-invasive tests, such as screening blood samples, to pick up on the earliest signs of problems \u2014 perhaps even before cells become fully cancerous. As a first step, scientists are hunting for effective 'biomarkers' \u2014 DNA or proteins that act as indicators of normal or pathological biological processes that could be used to screen for the presence of a cancer. But a big challenge facing biomarker hunters is that blood is a complex soup of molecules containing everything from the ubiquitous protein albumin to smaller molecules present only at trillionths of albumin's levels. As well as posing a challenge for molecule-detection technologies, this makes it even harder for biologists to pick out which proteins are relevant. \u201cThe important markers are likely to be in the low-abundance range, so the problem is searching for the needle in the haystack,\u201d says Lee Hartwell, Nobel laureate and director of the Fred Hutchinson Cancer Research Center in Seattle, Washington. Yet in the past few years, the technology for identifying potential biomarkers has advanced quickly, and many have been added to the list. For example, modern mass spectrometers provide high enough resolution that mass spectra of small proteins can now be obtained and matched against sequence databases for identification. Another method, called high-throughput proteomic profiling, relies on large arrays of different antibodies that specifically bind to proteins found in the blood. These can detect new proteins and changes in the levels of existing proteins that correlate with the development of cancer. Sam Hanash's group at Fred Hutchinson has been using proteomic profiling to identify markers for early-stage lung cancer, using two strategies 1 . The first is to look for proteins that have been shed by the tumour and are circulating in the bloodstream; the second is to search for an immune response to the tumour. The host immune system may not be able to destroy the tumour cells, but it probably recognizes them as problem cells, and Hanash wants to harness that raised antibody response as a diagnostic tool. \u201cWe're finding that some circulating proteins are inducing an immune response,\u201d he says. \u201cWe're also finding antibodies to circulating proteins.\u201d Hanash's biomarkers are just some of many that scientists have identified. Underpinning all this work is the understanding that using a single biomarker to diagnose a cancer is unlikely to suffice. \u201cWe're all agreed now that to rely on one marker to make a diagnosis is not going to work,\u201d says Hanash. No single biomarker can detect a given cancer with 100% sensitivity (meaning that all diseased subjects would test positive) and 100% specificity (with all healthy subjects testing negative). Panels of biomarkers with different individual sensitivities and specificities are therefore needed. A team led by David Sidransky at Johns Hopkins University School of Medicine in Baltimore, Maryland, for example, is trying to find ways of improving the accuracy of the blood tests used to detect a protein called prostate-specific antigen, or PSA. PSA is produced by prostate cancers, but can also result from benign prostrate enlargement or inflammation. Sidransky's team has identified an alteration to a gene called  GSTP1  that is unique to prostate cancer cells. The hope is that men with high levels of PSA in their blood could be referred for a biopsy to test for this genetic alteration, to reduce the number of false positives that result from relying on PSA levels alone 2 . To become fully cancerous, cells go through a complex, stepwise progression from normal cells to diseased ones. Scientists argue 3  that, for optimal early detection, researchers need to find biomarkers targeted to a stage in this progression where changes can be detected with the highest sensitivity and specificity. That may be a 'pre-malignant' phase, when cells begin to behave abnormally, but have not yet developed all the characteristics of tumour cells. One example of where this approach is being applied is in research on the common and sometimes crippling gynaecological condition endometriosis, where tissue that normally lines the womb appears on other pelvic organs. Women with endometriosis have a higher risk of developing a particular type of ovarian cancer and last year, Daniela Dinulescu, now at Brigham and Women's Hospital, Harvard Medical School, discovered genetic pathways underlying the progression from endometriosis to cancer in mice 4 . This raises the possibility that a test could one day pinpoint early genetic changes in women. In separate work, Dinulescu is collaborating with Hanash to validate around 20 biomarkers for early ovarian cancer. \n               Risk assessment \n             As researchers rush to assemble biomarker panels, the limiting factor to early detection is becoming the ability to detect these biomarkers in ever smaller quantities. Last year, Bert Vogelstein of Johns Hopkins University and the Howard Hughes Medical Institute (HHMI) in Chevy Chase, Maryland, showed for the first time that it is possible to detect trace amounts of mutated DNA against a noisy background of unmutated DNA, in a simple blood sample 5 . He adapted a technique called the polymerase chain reaction, or PCR, to detect fragments of a mutated gene called adenomatous polyposis coli (APC) in the blood of patients with advanced colorectal cancer. PCR allows researchers to selectively amplify a particular stretch of DNA using an enzyme, which can start work on very small quantities of DNA. Vogelstein's team also detected mutant APC molecules in more than 60% of patients with early, curable colorectal cancer; these were circulating in the patients' blood in extremely low quantities. Bioengineer Steve Quake of the HHMI and Stanford University thinks that this is just the beginning. \u201cThe fundamental limit of detecting single molecules has been reached in many cases of scientific interest, so in principle, this should be applicable to cancer detection as well,\u201d he says. Given that there is no such thing as a single magic marker for any cancer, Quake feels that the ideal way to detect early disease would be to sequence and so identify all the hundreds of thousands of molecules in a blood sample. \u201cAs crazy as it sounds, this is something that is coming within reach of the new sequencing technologies,\u201d he says, \u201cIf you have many markers of somewhat weak utility, the product of the probabilities of all those markers can be very powerful.\u201d But despite all the effort going into biomarker discovery, very few have so far been turned into diagnostic tests. Hartwell says this is because validating them is a slow process. A year from now, he expects that the consortium of seven US research institutions that he leads in the search for breast-cancer biomarkers will have assembled 100 candidates. But they will then have to develop sensitive tests for those markers and only if the tests perform well in distinguishing diseased from healthy individuals, can they apply them to larger populations to measure their sensitivity and specificity. Sudhir Srivastava, who heads the NCI's Cancer Biomarkers Research Group, says another factor delaying test development is that for the past 30 years, biomarker discovery has been driven largely by scientists doing basic research; such researchers may be less aware of which markers are likely to be most useful clinically. To try to change that, the NCI has brought together dozens of cancer-research institutions into an Early Detection Research Network (EDRN), which already funds a lot of research on detection, including Hanash's lung-cancer work. Bringing biomarkers to clinical application is one of the EDRN's main objectives. \u201cThis programme is attempting to strike a balance between discovery and validation,\u201d he says. \u201cIt brings clinical and basic scientists together, so that discovery is clinically driven.\u201d The speed with which early-detection tests reach the clinic will also depend partly on how much of their development is taken up by pharmaceutical companies. Some fear that, because single biomarkers will turn out to be of limited value, drug firms will steer clear of detection and stick to treatment. Hartwell disagrees. \u201cUniformly now, all the pharmaceutical companies want biomarkers for their particular drug, to know whether the drug is effective or not,\u201d he says. \u201cThe other place that this whole field could move into is drugs for prevention.\u201d As patients identified as high risk are likely to be taking preventive drugs for a lifetime, this could be a lucrative area for the drug industry. \n               From bench to bedside \n             Drug companies are acutely aware of the potential in this area, agrees Brian Holloway, director of preclinical studies on Iressa (the drug that's used to treat lung cancer), at AstraZeneca in Macclesfield, UK. They are also keen to find ways of targeting their treatments more effectively, he adds. \u201cThere is no cancer drug that works for everybody, so drug companies will look more and more for biomarkers to select the populations that will benefit from a given drug,\u201d he says. As promising as early detection sounds, it does have its problems. For one thing, simply being able to detect the presence of a tumour may not give doctors enough information to treat a patient appropriately. Michael Pollak, director of the Program of Cancer Prevention at McGill University in Montreal, Canada, points out that at least half the world's people older than 60 have cancerous growths, most of which are non-aggressive and will not cause disease. \u201cEarly detection of all the lesions would result in a huge industry of medical interventions, most of which would be unnecessary, but a few of which would be life-saving,\u201d he says. \u201cEarly detection must be able to distinguish incidental from dangerous cancers \u2014 a much more difficult job.\u201d And healthcare providers will still have to be convinced that early-detection tests are worthwhile at the population level, something that Richard Sullivan, head of clinical programmes at Cancer Research UK, says is notoriously hard to prove. It involves calculating the cost-effectiveness of the screening test, which in turn requires detailed information on the cost of screening, the long-term costs of cancer care, the sensitivity and specificity of the screening test and its impact on survival. But following Dr Papanicolaou's example, more screening programmes will almost certainly be established in years to come. \n                     Cancer: New fronts in an old war \n                   \n                     Cancer: Off by a whisker \n                   \n                     Cancer: The root of the problem \n                   \n                     Proteomics and cancer: Running before we can walk? \n                   \n                     Entrepreneur brings race cars and canaries to cancer research \n                   \n                     The Canary Foundation \n                   \n                     National Cancer Institute Early Detection Research Network \n                   Reprints and Permissions"},
{"file_id": "443268a", "url": "https://www.nature.com/articles/443268a", "year": 2006, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "Maurice Taieb laid the groundwork for the discovery of Lucy, the most famous fossil human ancestor. Rex Dalton meets the Tunisian-born geologist who prefers the desert to the limelight. It was the type of desert encounter where only someone like Maurice Taieb wouldn't flinch. In the shimmering afternoon heat of the Ethiopian desert, four tribesmen materialized across the river from Taieb's geology field camp. Along with their machine guns they carried millennia of tribal animosities, the sort of hatred that can erupt in deadly skirmishes over grazing rights or livestock. Taieb recognized the visitors as Issa tribesmen, prowling the lands of their rivals, the Afar. His Afar tribal guards and workers recognized it too, and melted away as quickly as the intruders arrived. Unarmed, Taieb turned to his few remaining colleagues and said: \u201cLet's invite them for tea.\u201d They sipped and bantered until the Issa left, melting back into the desert glare as silently as they had come. Taieb had manoeuvred his way through another dangerous interlude. \u201cFearless\u201d is how US palaeoanthropologist Tim White, of the University of California, Berkeley, describes Taieb, a geological legend for his exploration of the Afar region of eastern Ethiopia. It is such a dangerous and hard place that only a handful of early-twentieth-century researchers had attempted to explore there. When Taieb started in 1966, he had the rich sediments nearly to himself. Eventually, his work would help lead palaeoanthropologists to the best hunting grounds for fossils of our earliest relatives. Roaming the valley of the Awash River by Land Rover or donkey, Taieb picked his way through eroding gullies. Following a trail of snail fossils and encouraged by an elephant tooth, in 1968 he discovered the fossil-rich site now famous as Hadar. There, in 1974, colleagues would discover Lucy \u2014 the 3.3-million-year-old hominid that came to symbolize a new era of palaeoanthropology 1 . Lucy \u2014 named after the Beatles song \u201cLucy in the Sky with Diamonds\u201d but formally known as  Australopithecus afarensis  \u2014 walked upright like modern humans but still displayed primitive characteristics 2 . In this issue of  Nature , researchers continue Taieb's legacy with a report on a nearly complete skeleton of a three-year-old  A. afarensis 3 , 4 . From newly explored sediments just south of the Lucy site, Zeresenay Alemseged, an Ethiopian researcher at the Max Planck Institute for Evolutionary Anthropology in Leipzig, Germany, and his colleagues provide a stunning view of hominid evolution. With this three-year-old  A. afarensis  female, researchers can now see in a near-complete skeleton what a mosaic of evolution the species is \u2014 the upper limbs have characteristics of gorilla or chimp, whereas the lower limbs are made for walking upright like modern humans. \n               Mother lode \n             Thanks to the Hadar site, researchers arguably have more bones of Lucy's species than of any other early hominid. At Hadar, a sedimentary layer at least 200 metres thick preserves  A. afarensis  history between about 3 million and 3.5 million years ago. The annual rains have exposed hundreds of fossils, of dozens of individual hominids. This fossil library, with today's new additions, allows researchers to do crucial comparative analyses as with no other human ancestor. Beyond Hadar, Taieb has helped piece together the complex geological picture of the Afar region, which yields hominid specimens back to 6 million years old 5 . With such a pedigree, he should be a big name in science. But he's barely known outside his field. In the four decades since Taieb first went into the field, palaeoanthropologists have had their share of publicity. Some work the book and media circuit like most scientists do a library. Taieb disdains such hype-mongering; dismissing mention of headline seekers with the sweep of a hand clutching his ever-present cigarette. He seems to prefer the respect of the desert tribesmen; the headlines he wants to see are for the Africans he helped to train. \u201cMy passion was to do research in Ethiopia, working with young Ethiopian scientists so they could do the work,\u201d he says. Taieb also stands out in a field where squabbles over credit for discoveries and permits to work at key sites are common. \u201cHe is a role model for all palaeoanthropology \u2014 an unselfish individual who invited anyone to his rich Hadar site,\u201d says geochronologist Giday WoldeGabriel of the Los Alamos National Laboratory in New Mexico. \u201cHe shows how people can work together for the betterment of science.\u201d \n               Summit meeting \n             And he likes getting people together. In June, the 71-year-old Taieb threw a scientific party at the CEREGE Mediterranean European Laboratory outside the French city of Marseilles, where he has been a researcher for nearly 30 years. He invited about 40 scientists to a three-day conference to reflect on Lucy's place in evolution. It mixed backroom people with spotlight seekers, eminent Europeans with Africa's finest young rock hounds. And the Ethiopian scientists were front and centre, moderating key sessions. Only Taieb, skilled in scientific as well as desert diplomacy, could bring together such a crowd. Indeed, at a concluding dinner, Taieb was embraced simultaneously by two pillars of palaeoanthropology: White and Donald Johanson, now of Arizona State University in Tempe, who led the crew that found Lucy. Former collaborators, White and Johanson now barely speak to each other because of earlier bitter disagreements over research style and conduct. In his lecture at CEREGE, Johanson recalled how, as a 27-year-old novice, Taieb \u201cbubbling with enthusiasm\u201d told him: \u201cIf you want to find fossils, you should come to the Afar.\u201d It is a place where three tectonic plates of Earth's crust come together, exposing sediments of just the right age to contain human ancestors. At the time, new dating techniques had greatly improved scientists' ability to pin down the ages of sediments and their contents. In short, it was the perfect time and place for a desert aficionado such as Taieb to start working. Born in Tunisia, to a Tunisian father and a French mother, Taieb began his lifelong love of the African outback travelling with a merchant uncle selling goods to Bedouin on the outskirts of Tunis. He eventually made his way to France, receiving his doctorate from the University of Paris VI in 1974, with a thesis on the geology of the Awash River basin. Decades after the French and Italians cut roads from Djibouti to Addis Ababa, Taieb followed those trails through the Afar. Again and again he circled huge areas cut by gullies and ridges. The treks gave him a vast knowledge of the landscape and its peoples; around campfires today, Afar elders still ask with interest after Maurice. For many of Taieb's early years in the field, the biggest worries were maintaining fuel and water supplies. It wasn't until 1993 when, in what he recalls as his most dangerous moment, he faced down the Issa and asked them to tea. After that incident, he and his two companions \u2014 a student and an interpreter \u2014 lay away from their camp at night, ready to bolt at any sign of attack. Indeed, the area soon turned violent; a day later, a team sent by French officials to escort them out drove through a roadblock on an isolated path a few hours from Hadar. Panicked militia riddled the truck with bullets, killing an Ethiopian cultural officer and seriously injuring two others. For anyone, the Afar was a dangerous place. The Dergue government, backed by the Soviet Union, came to power in the mid-1970s; for most of the 1980s, all field research was shut down by the political climate. Yet some of the old hands of Ethiopian anthropology think that research may not have been as limited had certain people listened to Taieb's advice on what became known as \u201cthe grave-robbing incident\u201d. In the late 1970s, while preparing a book about Lucy's discovery 6 , Johanson wrote about digging the femur of a modern Afar tribesman out of a rock-covered cairn, in order to compare it with a fossilized hominid knee bone later determined to be  A. afarensis . When Taieb saw this vignette in an unpublished manuscript of the book, he says he objected vehemently, arguing that it would anger the Ethiopian authorities. His warnings were ignored. Today, Johanson will not discuss this episode. Becoming agitated when asked about these events, he denied hearing such advice. When the book was published in 1981, jealous competitors highlighted the passage for Ethiopian officials. As predicted, they were furious. Johanson's team, on which Taieb was the geologist, and others didn't get their excavation permits back for nearly a decade. When Johanson finally secured permits again in 1991, Taieb was dropped from the team. \u201cHe forgot to send the elevator back down,\u201d Taieb was to say years later. Some Ethiopian scientists tried to help Taieb win access to another key spot, but to no avail. He has since returned to the Afar only to help a student complete his thesis. \u201cI have no hard feelings,\u201d says Taieb. \u201cIt is life. I now have many letters of support from young Ethiopian scientists; this is my pleasure.\u201d And he invited Johanson to his June symposium, where the famous palaeoanthropologist spoke highly of his former geologist colleague. During cocktails before the final dinner at a historic French estate, the pair ventured off together to speak of old times. For the observing Ethiopian scientists, who had learned from Taieb in lab and field, it was another chance to mark his place in evolutionary history. \n                     Ethiopia: Awash with fossils \n                   \n                     Anthropologists walk tall after unearthing hominid \n                   \n                     Fossil hunters bristle over plans for US tour \n                   Reprints and Permissions"},
{"file_id": "442500a", "url": "https://www.nature.com/articles/442500a", "year": 2006, "authors": [{"name": "Philip Ball"}], "parsed_as_year": "2006_or_before", "body": "Chemistry is a key component in all the scientific disciplines. But does that mean it is nothing more than a handy tool \u2014 or are there still major chemical questions to crack? Philip Ball finds out. Physicists do not shy away from promoting the big questions that drive their field \u2014 how the Universe began, say, or what governs the behaviour of space, time and matter over scales from the atomic to the cosmic. Biologists, too, are happy to point to Erwin Schr\u00f6dinger's question 'What is life?', which they are attempting to answer by unravelling DNA and mapping out the structures and interactions of proteins. But what of the third basic science in the curriculum? To judge from the scant attention chemistry gets in the public media, you could be forgiven for thinking that it is a discipline whose time has passed, its grand puzzles all now answered. Does chemistry have any big questions left? Identifying such frontier questions seems all the more urgent because university chemistry departments are facing an uncertain future. The department at the University of Sussex in Britain, for many years home to Nobel laureate Harry Kroto, co-discoverer of buckminsterfullerene, was the latest in a long line threatened with closure. It has so far resisted an attempt to remodel it as a 'chemical biology' adjunct of the life-sciences division; but several other UK chemistry departments have failed to evade the axe. Following similar moves and concerns in the United States, a 2004 editorial in  Chemical and Engineering News , published by the American Chemical Society (ACS), proposed changing the organization's name, rebranding it as the Society for Molecular Sciences and Engineering. With departments closing and student numbers dwindling, can today's chemists be sure that their discipline will continue to be seen as a core science? Some of them complain that many of its most important questions are being framed in terms of the 'chemical' aspect of another discipline, rather than being seen as central to chemistry itself. In an attempt to gauge the prospects for academic chemistry,  Nature  asked many leading chemists what the field's big questions are, and whether in fact chemistry needs big questions to maintain a sense of coherence and identity. The strongly synthetic character of chemistry sets it apart from the 'discovery' sciences such as physics, biology, astronomy and the Earth sciences. \u201cChemistry creates its object,\u201d as the French chemist Marcelin Berthelot wrote in 1860. Many chemists still see this creativity as one of the field's strengths. \u201cIt makes chemistry able to set goals of a type most other sciences cannot hope to attain,\u201d says Ron Breslow, an organic chemist at Columbia University in New York and a past president of the ACS. \u201cWhere is synthetic astronomy \u2014 changing the gravitational constant to see what effect that has on the properties of the Universe, and thus perhaps improving it?\u201d And although synthetic biology is now emerging as a genuine discipline, to many chemists this is just another branch of applied chemistry, relying as it does on chemical techniques such as DNA synthesis and protein design. \u201cWe are the only science where things can be made that were never made before,\u201d says nucleic-acid chemist Jacqueline Barton at the California Institute of Technology in Pasadena. The downside of this focus on making stuff is that chemists can be portrayed as inveterate tinkerers \u2014 tweaking the molecular world to satisfy their curiosity, sometimes for fun and sometimes for profit. And it makes it especially hard to see where industrial chemistry ends and academic chemistry begins, because important practical challenges provide the motivation for much academic creativity. \u201cChemistry is the scientific enterprise that fuels industry,\u201d notes Barton. \u201cNot just petrochemicals, but pharmaceuticals, biotechnology and computer chips.\u201d Breslow agrees that chemistry faces not so much big questions as big practical challenges, such as \u201cto devise a practical method to derive our needed energy from sunlight; to create a room-temperature superconductor that can carry large currents; to learn now to perform the manufacturing we need without damaging the environment\u201d. \n               Be specific \n             No one would deny the importance of applied and industrial chemistry. But if chemistry's questions aren't so much about what we can know but about what we can do, does that make it a form of engineering \u2014 a quest for particular solutions to particular problems? According to inorganic chemist John Meurig Thomas of the Royal Institution in London, it is in the nature of chemistry to be a science of particulars. One can identify general principles of chemical bonding, for example, but what often matters is how these are enacted and modified in specific molecules. Similarly, he says, \u201cit would be ludicrous to look for a general theory of catalysis that applies to all enzymes, materials, surfaces and so on.\u201d With so many chemists happily focused on practical goals, and with other disciplines nibbling away at the edges of chemistry, are there any big questions left at the academic core of the subject? And if there are, do they have the intellectual excitement of those at the frontiers of physics and biology? Chemists certainly have the tools and concepts to help answer some of the frontier questions arising in these other disciplines. The clearest consensus among the chemists approached by  Nature  was that many of chemistry's most urgent questions are ones that address aspects of biology. \u201cTo me, the big unanswered questions concern the chemistry of life processes,\u201d says physical chemist Richard Zare of Stanford University. Barton agrees: \u201cA real understanding of biological processes always comes down to understanding the chemistry.\u201d \n               Essence of life \n             Harvard University chemist George Whitesides goes further. \u201cThe nature of the cell is an entirely molecular problem,\u201d he says. \u201cIt has nothing to do with biology, really.\u201d Whitesides suggests that the \u201creally intellectual\u201d parts of biology, such as its more quantitative and molecular aspects, are overlooked whenever biologists study whole organisms. These are strong claims, which biologists might contest. One can claim that the cell has nothing to do with biology, points out molecular biologist and Nobel laureate Sydney Brenner at the Salk Institute for Biological Studies in San Diego, California, but by the same token, one could say that all of chemistry is just quantum mechanics. Still, many of the gaps in scientific understanding of the fundamental processes of molecular biology, such as protein folding, genetic encoding of biomolecular function, and highly selective molecular recognition, are fundamentally chemical problems. And although molecular biologists may assume that these things are broadly understood, from a chemical viewpoint they get more puzzling the closer one looks. Scientific understanding is still not good enough to provide a rational and predictive basis for the kind of molecular-scale interventions needed in biomedicine and drug development. Meanwhile, the chemical nature of biomolecular processes such as signal transduction, identified as a key question by chemical engineer Matthew Tirrell at the University of California, Santa Barbara, is one of the issues that is positioning chemistry as an information science. The concept of a lock and key to explain biomolecular recognition, proposed by German chemist Emil Fischer in 1894, can now be seen as the start of what supramolecular chemist and Nobel laureate Jean-Marie Lehn of the University Louis Pasteur in Strasbourg, France, has called the science of informed matter. The concepts behind self-assembly have accustomed chemists to the idea that molecules can be programmed to interact and come together in very specific ways, and artificial replicating molecules have demonstrated the principles by which chemical information can be transmitted and amplified. \u201cFor me, chemistry has a most important contribution to make to the biggest question of all: how does self-organization arise and how does it lead the Universe to generate an entity that is able to reflect on its own origin?\u201d Lehn says. Lehn believes the next step is the design of chemical 'learning systems' that are not just programmed for assembly but can be trained. Indeed, another of the key questions in chemical biology raised by several chemists was the chemical basis of memory. \u201cOnce we know the answer, maybe we can design new thoughts and memories, or even just learn how to retain old ones,\u201d Barton suggests. Whitesides wants to know how to use chemistry to merge silicon electronics with grey matter. \u201cHow do I plug my computer into my brain?\u201d he asks. That might sound like a matter for neuroscientists and electrical engineers \u2014 but as the signals between neurons are chemically mediated, this kind of interfacing demands command of a chemical language. These are appealing directions for chemists to study, but do they qualify as truly chemical questions? To Whitesides, that couldn't be more the case. \u201cI take the view that most of what is interesting in science is now chemistry,\u201d he says. He argues that even some of the key questions in a field as apparently remote from chemistry as astronomy, such as 'How many Earth-like planets are there?' or 'What is on Saturn's moon Titan?' are fundamentally molecular ones. When addressing interdisciplinary questions, what truly distinguishes chemists from physicists and biologists is that they are not content to ignore molecular-scale mechanisms. Tackling these issues confronts chemists with what is perhaps the core challenge of their discipline: to understand and predict the relationship between molecular structure and function. Structure\u2013property relationships are crucial to drug design, for example \u2014 one of chemistry's major concerns. \u201cHow do we encode specificity for particular cells, organelles or tissues into molecules?\u201d asks Barton. \u201cHow do we make molecules go where we want them to go?\u201d It is also essential for designing catalysts for use in industrial synthesis. But at present, a full understanding of the link between structure and function is often possible only for relatively simple, small molecules \u2014 and even then there are many details of the problem that have yet to be clarified. \n               Action heroes \n             For example, Nobel laureate Ahmed Zewail, a physical chemist at the California Institute of Technology, points out that the dynamic behaviour of molecules can play as big a role in their reactivity as their molecular structure. It is now clear that the interactions between biomolecules aren't simply a matter of fitting a key into a lock \u2014 getting a good geometric match between the binding site and its target \u2014 but may depend on the dynamics of the interacting molecules and the solvent. Chemists now think of reactions as happening on a complex, multidimensional energy surface, or landscape, which can be as rugged as a mountain range. So understanding protein folding is a question of how the molecule's peptide chain negotiates a trajectory across this energy landscape so that it ends up in the 'valley' corresponding to the correctly folded conformation. \u201cIn biology, thinking about the relationship between structure and function is not enough,\u201d says Thomas. \u201cYou've got to think about movement around the energy landscape.\u201d In other words the dynamics are key. A big question for Zewail is how to control chemical function through dynamics. Compared with their ability to determine molecular structure, chemists have only just begun to understand what can be done to control reaction pathways in this energy landscape. In principle, this can be done by guiding molecules, perhaps using laser beams, into particular quantum states. So far this has been achieved for simple molecules, but looks extremely daunting for larger, 'floppier' ones. And even if they crack the principles of molecular design, how do chemists apply them? \u201cUntil we reach the stage when someone can go in the lab and synthesize an arbitrary molecule in 100% yield in pure form without having a graduate student spend a year working it out, we have not really mastered synthesis,\u201d says Barton. \u201cSo the big question concerns the nature and rules governing how to assemble atoms into new molecules in a predictable and effective way. Then we could make whatever substances we want.\u201d \n               Creative force \n             Only chemists know how truly difficult it is to engineer atoms and molecules \u2014 something that many other scientific disciplines rely on. If room-temperature superconductors or synthetic bacteria are ever created, it will not be physicists and biologists who make them. And if chemistry is chopped up and parcelled off to other disciplines, there will be no training ground for those who achieve such mastery over matter. It would be wrong, moreover, to suggest that the heart of chemistry \u2014 rational synthesis \u2014 lacks intellectual appeal. Some argue that, rather than trying to understand the world, chemists are attempting to understand all possible worlds. \u201cChemistry has a useful aspect, but that is not the basic science,\u201d says Breslow. \u201cThe basic science is clear once we realize that the limited examples of molecules and reactions that nature has supplied are a microdrop in an enormous bucket compared with the wonderful chemical world still to be created and examined.\u201d It has been estimated that there are about 10 40  possible molecules that could be made from common elements with a molecular size comparable to that of a typical drug. \u201cThe known chemical world, including the expansion of the natural world that chemists have achieved, is nowhere near 1% of that,\u201d says Breslow. It is this profusion that defies attempts to reduce chemistry to a handful of objectives. \u201cIts universe is defined not by reduction to a few elementary particles, or even the hundred or so elements,\u201d says theoretical chemist and Nobel laureate Roald Hoffmann of Cornell University in Ithaca, New York, \u201cbut by reaching out to the infinities of molecules that can be synthesized. There is no end to the range of structure and function that molecules exhibit.\u201d Most chemists seem content to work without big frontier questions to guide them. Such questions can be helpful to a discipline's sense of identity and direction, but they risk narrowing the possibilities of an inherently creative discipline. Some might argue that an excessive focus (at least in the public eye) on 'theories of everything' or decoding the human genome has not been terribly productive for physics or biology. Besides, in chemistry, as in any science, the biggest breakthroughs often come from unexpected directions. \u201cI do not think that I have ever identified in advance any of the major directions or key questions of chemical research that I witnessed in the past half century,\u201d confesses inorganic chemist Hubert Schmidbaur at the Technical University of Munich in Germany. \u201cAnd it seems to me that the situation will be not very different in the next five decades.\u201d \u201cThere is no Holy Grail in chemistry,\u201d Hoffmann admits happily. \u201cOccasionally some are held up for public view,\u201d he says, but they are just \u201cgimmicky candidates for the chalice\u201d. He adds that in a fundamentally creative field, the satisfaction comes from the chase, not the catch. \u201cMy natural philosophical disposition is not to work on big questions,\u201d says Hoffmann. \u201cI like working on many detailed small problems in this wonderful chemical garden, while keeping my eyes open for the connections.\u201d For more on this topic, see Editorial,  \n                     page 486 \n                   . \n                     How dangerous is chemistry? \n                   \n                     Chemistry department salvaged \n                   \n                     Chemists get out begging bowl to avert closure \n                   \n                     Chemistry society goes head to head with NIH in fight over public database \n                   \n                     Chemists in California \n                   \n                     The Sceptical Chymist blog \n                   \n                     National Academy of Sciences report on the future of chemistry \n                   \n                     American Chemical Society \n                   \n                     International Union of Pure and Applied Chemistry \n                   \n                     Should chemistry change its name? \n                   \n                     Editorial in  Chemical & Engineering News  on trends in chemistry \n                   \n                     The Public Images of Chemistry \u2013 special issue of  Hyle \n                   Reprints and Permissions"},
{"file_id": "441927a", "url": "https://www.nature.com/articles/441927a", "year": 2006, "authors": [{"name": "Erika Check"}], "parsed_as_year": "2006_or_before", "body": "Tigers are teetering on the verge of extinction and human contact in their habitat could be their greatest threat. Erika Check investigates whether local people can live alongside India's big cats. It is the dry season in southern India, but a thunderstorm has cooled the air and damped down the dust in the forest of Nagarhole National Park in Karnataka. As he drives slowly down a dirt road in the park, wildlife biologist Ullas Karanth keeps one eye on the horizon and the other on the wet ground, scanning for tiger tracks. Through a thicket of bamboo, we spot a young elephant ambling across a marsh. Karanth stops the jeep. The only sounds are barbets cooing from the trees and the frenzied cry of a cuckoo nicknamed the \u2018brain-fever bird\u2019. Karanth murmurs, \u201cThis used to be a bustling little village... 30, 40 houses, lots of cultivation....\u201d He doesn't finish his sentence, but his meaning is clear: now the people are gone, the wildlife is coming home to where it belongs. We drive on, scanning the underbrush for any movement. Suddenly, Karanth hits the brakes and exclaims in an excited whisper: \u201cThere!\u201d He points straight ahead, but at first I don't see it. A herd of spotted deer stands alerted by the road, about 20 metres ahead. I follow their eyes, and that's when I see him: the world's largest cat, pacing across the road. I yelp; Karanth shushes me. But the tiger continues at his leisurely pace through the grassy clearing. I catch only a glimpse of his massive head and striped flanks before he disappears into the dense lantana and the woods beyond. The deer advance nervously after him, checking to make sure he's gone. Karanth shakes my hand. \u201cYou are really lucky,\u201d he says. \u201cI have seen tigers so many times, but each time it is like a special event.\u201d For Karanth, each sighting is doubly satisfying, because his work in Nagarhole is part of the reason why tigers still exist there today. Karanth was the first Indian scientist to do rigorous tiger ecology in India. During his 20 years of work, he combined research with relentless activism to keep people and poachers away. And tigers have thrived there. In the rest of India, the big cat has not been so lucky. A century ago, 40,000 tigers roamed India's forests and grasslands. Now, the Indian government estimates that only 3,600 still exist \u2014 and most informed observers say the number is probably between 1,200 and 2,000 (see  \u2018Counting cats\u2019 ). India's tiger population is teetering on the brink of extinction. This is a tragedy, and not only for the tiger. The big cat is a source of national pride in India and a part of the country's identity. The tiger is also an umbrella species \u2014 a high-profile animal that earns protection for the other animals that live alongside it. This means that the same forces that threaten the tiger also bring bad news for many other vulnerable species in India, such as the rare Indian giant squirrel and the massive forest ox, the gaur. The reasons behind the tiger's decline are complex: rampant poaching; a lack of funding for the country's network of tiger reserves; a sclerotic and bureaucratic system of park management; the constant pressure of more people; and more development, such as industrial-scale mining in forests. Some scientists suggest that the government's distrust of conservation science is also behind the decline. \n               Tribal trouble \n             The conflict has led to emotional, seemingly intractable debates over the central question of tiger conservation in India: can tigers and people live together? Some, like environmentalist Sunita Narain, who led a government task force on tigers last year, say there is no choice. Today, 380,535 people live in India's 37,761 square kilometres of tiger reserves. Tigers and poor people live in the same spots, and both have rights to the land. But Karanth and other die-hard conservationists argue that tigers and people will never coexist peacefully. Faced with this bleak picture, some have given up on the tiger's future. But not Karanth, who has watched the animals recover from seemingly hopeless odds in some places. Karanth first visited Nagarhole in 1967, on a motorcycle trip during college. He hardly saw a deer, let alone a tiger. Logging and hunting were rampant. And India's tourism department was still luring visitors to the country by promising that they could shoot a tiger and take it home as an exotic souvenir. But the animal's fate was not sealed. In 1972, Prime Minister Indira Gandhi convened a task force to study India's disappearing tigers. The group's report expanded into the programme called Project Tiger. Today, that programme is responsible for the animal's survival in India, and controls a sprawling complex of 28 tiger reserves \u2014 from Namdhapa, on the eastern border with Burma, to Kalakad-Mundanthurai, at the southern tip in Tamil Nadu state. Wildlife continued to fascinate Karanth as he pursued an engineering degree. After just three years working as an engineer, he realized he hated his career and changed course, leaving to do a PhD in wildlife biology at the University of Florida. He returned to Nagarhole in 1986 \u2014 this time, as a young scientist gathering data for a research project. By that time, new park managers, led by forest officer K. M. Chinnappa, were working to stamp out poaching and enforce new wildlife protection laws. Nagarhole was starting to bounce back, and Karanth wanted to test its potential as tiger habitat. But undertaking a science project in the park was not easy. Chinnappa fought constant skirmishes with the \u2018tribals\u2019, groups of forest dwellers who lived by hunting, herding livestock and collecting firewood. Poachers, too, were angry that their prime hunting grounds were now off-limits. Early on, Karanth and Chinnappa realized they shared a mission. Karanth had started a project to count tiger prey, from the tiny spotted deer to the hulking gaur. By figuring out how much tiger food was roaming around Nagarhole, Karanth reasoned, he could predict how many tigers could live there. But the studies were interrupted in 1992, when simmering tensions erupted into violence. An angry mob of villagers stormed the park, burned Chinnappa's house down, and vandalized Karanth's research station. A local man was murdered in the aftermath of the riot, and officials arrested Chinnappa for the crime. Although he was later cleared, Chinnappa was done with the forest department. He began working instead as a wildlife activist in the villages in and around Nagarhole, as well as in other national parks in the larger state of Karnataka. Karanth, too, realized that he could not do his science in isolation. Since then, he has worked with Chinnappa and other activists to ensure that Nagarhole and other parks remain untouched. \n               Numbers game \n             Today, Nagarhole has one of the healthiest tiger populations in India \u2014 about 60, by Karanth's estimation. He attributes that to both a crackdown on poaching and an effort to relocate 12 villages outside the park. Karanth's belief that the tiger needs an inviolate habitat comes partly from his research. In the 1980s and early 1990s, Karanth perfected his method of counting prey to predict tiger populations, but absolute confirmation \u2014 by radio-collaring every tiger in the Nagarhole forests \u2014 wasn't feasible. So he decided to capture the animals another way: on film. Karanth adapted a device called the Trailmaster, designed to help hunters find deer. It had an infrared sensor that, if tripped, triggers a camera to capture the passing animal's image. Karanth installed a handful of Trailmasters at Nagarhole and spent the next decade working with statisticians to estimate the tiger population based on the number of tripped sensors 1 . This technique, called camera-trapping, is widely used in wildlife biology. Karanth's latest research, which combines ten years' worth of data, strengthens his conviction. In unpublished work, he estimates that over the past decade, an average of 23% of the park's tigers have died or left the park every year. And yet the population is stable, partly because females breed often. To Karanth, this underlines the importance of keeping people out and sustaining prey populations for the tiger. \u201cIt's very resilient,\u201d he says. \u201cBut you can't have people, and you can't have cows.\u201d Sweeping social and demographic shifts, however, are not in Karanth's favour. One billion people live in India, with a projected population of 1.6 billion by 2050. And the country's push towards social reform is empowering groups that have been traditionally disenfranchised. In the past, tribal groups living alongside tigers were simply pushed off their land by the government. The forced relocations often left whole villages stranded with no way to make a living; villagers came right back to the forest. Now some scientists are asking whether there should be a different solution \u2014 some way for tigers and people to coexist. \n               Peaceful coexistence \n             Not far from Nagarhole lies a test case for coexistence, in the Biligiri Rangaswamy Temple Wildlife Sanctuary \u2014 BRT for short \u2014 in Karnataka. Inside the sanctuary are 540 square kilometres of rolling hills, rainforest, coffee plantations, and thousands of members of the Soliga tribal group. In 1972, when the sanctuary was created, the government moved 25,000 Soligas into small settlements called  podus  in and around the park. Now, they support themselves by harvesting lichens, fruits, honey and other forest products. In one  podu  near the southern tip of the sanctuary, a group of Soliga women and children sits in a dirt yard circled by papaya, banana and mango trees. The women point out a shed with a dried-grass roof. Inside, stacks of burlap sacks bulge with lichens collected from the forests. It takes one week for a harvester to fill one sack, which earns the village 500 rupees \u2014 about US$10. The harvesters pass the lichens on to a central cooperative that sells them and distributes the profits within the community. Little evidence of profits can be seen in this  podu . The houses are constructed of mud and sticks, and villagers must rebuild the grass roofs each year. The women wear tattered saris and are afraid to send their children to the school a few kilometres away because elephants might trample them on the way. A deep trench encloses the village; it's supposed to keep the elephants from raiding the crops. But if elephants don't get to the crops, marauding packs of wild boars do. There is a new hospital an hour's drive away. But no one in the village owns a car, and they can't afford the 30-rupee bus fare. So what happens when someone gets sick? \u201cWe just die,\u201d says one Soliga woman, with a bitter laugh. And yet the Soliga do not want to leave this  podu . The state government has offered to resettle families on 300 hectares of land outside the sanctuary. But the men and women in this  podu  say they won't go. They are attached to their god, who lives in a hill that looms over the village. And they won't leave their temple, a spare, dirt-floored hut. \u201cThey have relationships with gods here, and they have burial sites within the sanctuary,\u201d says C. Madhegowda, an activist. Madhegowda, who is also a Soliga, explains: \u201cWe are children of the forest. We are not interested in leaving.\u201d \n               Meeting halfway \n             Madhegowda works with a group that is standing up to the government in ways that might have been unthinkable a few decades ago. In 2003, India banned the harvest of non-timber forest products from wildlife sanctuaries. In the past, the Soligas might have just accepted this removal of their main source of income. But not now. Madhegowda's group organized strikes in the Karnataka state capital, Bangalore, and met with officials in the forest department. For now, there is an informal d\u00e9tente that allows harvesting to continue in the sanctuary. \u201cTwenty-five thousand people depend on this harvest,\u201d Madhegowda says. \u201cYou can't ban that if you don't provide an alternative.\u201d The Soligas say they should be allowed to stay because they're not harming the park. Over the past decade, a Bangalore-based organization called the Ashoka Trust for Research in Ecology and the Environment (ATREE) has been working with the Soligas on sustainable harvesting practices. ATREE researchers have reported that the Soligas make accurate estimates of their harvest, and rarely use harmful methods, such as cutting branches off fruit trees. As a result, populations of harvested plants and bees have been relatively stable over the past decade 2 . ATREE researchers hope this will convince the state forest department to allow the Soliga to help manage the wildlife sanctuary where they live. \u201cWe're saying these people can take responsibility for managing this park,\u201d says Nitin Rai, an ecologist and sociologist with ATREE. Rai started out as a \u2018wildlifer\u2019 like Karanth, but now believes it is not realistic to manage tiger populations by moving people out of their way. He points to the antagonistic relationships that have sprung up around some of the tiger reserves, where villagers are so alienated from the park's mission that they happily serve as paid guides for poachers. Such antagonism notoriously led to the poaching of every tiger from a reserve called Sariska in Rajasthan. The revelation last year that all Sariska's tigers were gone led to a public outcry, and the government's appointment of a task force to examine India's tiger crisis. \u201cWe've got to convince ourselves that it's OK to use management systems that include people as part of the ecosystem,\u201d Rai says. A year-and-a-half ago, Sunita Narain would have taken Rai's side against Karanth. The outspoken activist has taken on some of the most powerful interests in India in her fight for environmental justice. Last year, her high profile and political connections earned her the job of chairing the prime minister's task force on Project Tiger. Narain's appointment was controversial, because she was an outsider to the tiger community. Narain's unenviable task was to find some middle ground between the two camps of the tiger debate. And she believes the group did find a good compromise. Its final report estimated it would cost between US$713 million and US$2.5 billion to move every person out of India's tiger reserves. The task force instead suggested the government pick the most crucial areas of habitat and concentrate on moving people away from those places first. In the rest of the areas, the task force suggested that the government find ways for people and tigers to coexist, as some say they do in the Biligiri Rangaswamy Hills. For Narain, this was a huge concession. \u201cComing into this as an environmentalist, I definitely thought that relocation was not the solution,\u201d she says. \u201cBut we tried to meet the conservationists halfway.\u201d And she stresses that the relocations, if they are done, should be done well. The villagers should be given good land, and enough money to make an honest go at a new life. This is a sore point in India, where government-run relocations are often botched, leaving people in worse circumstances than those they left behind. \n               Feasible futures \n             On that point, Karanth agrees with Narain. The day after our tiger sighting in Nagarhole, we are up before dawn, driving to the forest-department quarters to meet a park official. There Karanth lays out a stern directive: he is trying to work out a deal to move one of the last villages out of Nagarhole \u2014 and he wants the forest department on his side. \u201cYes, yes,\u201d the forest official agrees, nodding as Karanth spells out his plans. But this is only one step, and many months of negotiations will follow. Later, Karanth drives us past some of the villages that were relocated years ago to the outskirts of Nagarhole. There are neat rows of houses, and fields where men drive ploughs pulled by cows. Karanth sees this as inevitable progress. After all, he asks, should people live as the Soligas do forever \u2014 with no electricity, no hospital and no schools? Some of the voices in the tiger debate disagree with Karanth's methods. But most now agree with his premise: that saving the Indian tiger also means helping the people who live alongside it. \u201cThere is no doubt in my mind that we need to do a lot to save the tiger \u2014 as much as we need to do to save poor people who live with the tiger,\u201d Narain says. \u201cThe poverty of each is leading to the poverty of the other.\u201d Where possible, this will mean Karanth's strategy of relocating people to improve their welfare. Elsewhere, scientists, activists and officials must find other ways to end the antagonism between tigers and people. Otherwise, both species face a desperate future. \n                     Conservation in Myanmar: under the gun \n                   \n                     New subspecies of tiger is christened \n                   \n                     Siberian tigers fight for freedom \n                   \n                     Sperm donors courted for big cats \n                   \n                     WCS India (Ullas Karanth's programme) \n                   \n                     Biligiri Rangaswamy Temple Wildlife Sanctuary \n                   \n                     Tiger Task Force report \n                   Reprints and Permissions"},
{"file_id": "441922a", "url": "https://www.nature.com/articles/441922a", "year": 2006, "authors": [{"name": "John Whitfield"}], "parsed_as_year": "2006_or_before", "body": "There's more to science at the movies than Lex Luthor's attempts to synthesize kryptonite. In the first of two features on film, John Whitfield looks at how a cinematographic technique can provide insights into the perception of reality. In the second, Alison Abbott meets Ben Heisenberg, a director whose first film is a taut moral fable of laboratory life. What is reality? And does anybody care? These questions permeate the work of science-fiction writer Philip K. Dick, whose characters' memories and identities are frequently products of drugs and other technologies \u2014 implying that recollections, as well as appearances, should never be trusted. Dick's themes have been catnip to film-makers \u2014  Blade Runner ,  Total Recall  and  Minority Report  are all based on his stories. The latest example,  A Scanner Darkly , directed by Richard Linklater, will be released this July. Neuroscientists often ask the same questions. And one of the tools that they are using to address them is \u2018rotoscoping\u2019, a filming technique Linklater uses in  Scanner  and some of his earlier work to blur the distinction between reality and animation. Our notions of authenticity, brain scientists are finding, depend as much on emotional and psychological plausibility as they do on physical accuracy \u2014 and the brain will swallow almost anything, provided it comes in the form of a story. \u201cOur ability to think of others as having minds is very promiscuous, and applies itself across a wide range of entities,\u201d says neuroscientist Rebecca Saxe of the Massachusetts Institute of Technology. Watch a crude animation of a big square tracking a small one, for example, and the word \u2018chasing\u2019 springs to mind. Art exploits this promiscuity, creating emotional impact from strings of abstract symbols such as the lines of letters in novels, or from flickering images on screens. Studies of brain activity using functional magnetic resonance imaging (fMRI) show that the brain distinguishes movements that might seem to have intentions from other movements. Seeing mechanical motion, such as that of a pendulum, involves a different part of the brain from seeing biological motion, even if it's only that of a cartoon arm. But how do neuroscientists know whether our responses to such lab-based fictions reflect the real world? \u201cI was interested in whether people attributed intentions to cartoon people in the same way that they comprehend the intentions of real people, but finding or creating videos with cartoon and real people doing the exact same thing seemed very difficult,\u201d says neuroscientist and keen filmgoer Raymond Mar of the University of Toronto in Canada. Mar struck lucky; he discovered that Linklater's earlier rotoscope movie,  Waking Life  (2001) provided just such material. In rotoscoping, conventionally shot footage is transformed into cartoonish animation using a combination of human animators and computers;  Waking Life  uses the technique as a way of presenting a constantly shifting world of dreams, whereas  A Scanner Darkly  uses it to evoke the estranged world of drug use. In the extras on the first movie's DVD, Mar found clips of the video footage that had been transformed into animation, ranging from mellow scenes of pillow talk between Julie Delpy and Ethan Hawke to a man tipping petrol over himself. \n               Fact or fiction? \n             Mar and his colleagues showed both cartoon and video clips to subjects in an MRI scanner. They found that two areas of the cortex previously associated with detecting intention, the superior temporal sulcus and the temporal parietal junction (TPJ) fired more strongly in response to the video footage than to the cartoons 1 . Cartoon sequences, on the other hand, produced more activity in an area called the bilateral orbitofrontal cortex, which responds to rewarding stimuli. Mar's team speculate that this region may have been more tickled by the trippy cartoon footage than the more mundane video; it's certainly the case that watching the rotoscoped dreamscapes is a peculiarly rich experience. \u201cIt's an elegant but challenging study,\u201d says Saxe. \u201cIt's hard to imagine a more minimal contrast that more effectively manipulates this dimension. But the findings are mysterious.\u201d The mystery is what the patterns of brain activity in the two treatments mean. It may be that the lower activity in the TPJ stimulated by the cartoons reflects the fact that the animations are easier to process into ideas about intention than the \u2018real life\u2019 footage. Mar, however, plumps for an alternative, although not necessarily contradictory explanation \u2014 that the more detailed video footage contains more cues of intention, ticking more of the brain's boxes. His next plan is to show subjects both a real, live hand poking into the scanner and a video of the same event, and see how their brains respond. \u201cIt suggests a template-matching mechanism,\u201d adds neuroscientist Kevin Pelphrey of Duke University in Durham, North Carolina. \u201cIt may be that the more realistic stimuli portray more intentionality, which these brain regions prefer.\u201d But more realistic doesn't necessarily just equal more convincing. The best graphics and robots risk toppling into what, in 1970, the Japanese roboticist Masahiro Mori dubbed the uncanny valley, where their almost-but-not-quite realness becomes creepy and repellent. Earlier studies have indicated that the brain's mind-reading areas work harder if they believe they are perceiving a real person. Chris Frith of University College London and his colleagues found that when a person played the game known as paper, scissors, stone, another area associated with attributing intention, the anterior paracingulate cortex, fired more strongly if the subject thought they were playing a person rather than a computer \u2014 even if he or she was in fact playing a computer 2 . What we perceive can depend on what we believe; imagining a thing produces brain activity very similar to the genuine experience. As well as tapping into pre-existing biases, film and animation mould the brain, says Frith. \u201cThe brain responds to cultural effects, and the conventions of realism are constantly changing,\u201d he says. What counts as real, according to these conventions, depends on technologies of representation; as technology has changed, so has our perception of the hallmarks of reality. People once believed newsreels only if they were in black and white. Currently, the mark of authenticity \u2014 exploited, for example, in Paul Greengrass's  Bloody Sunday  and  United 93  \u2014 is wobbly handheld footage. The coarseness of video stock, as used in  The Blair Witch Project , is a similar signifier of reality. Our understanding of the social brain is still rudimentary, Frith adds. \u201cWe know quite a lot about which regions are involved, but almost nothing about what they're actually doing, and what neuronal computations are involved.\u201d What's needed, he says, are imaging studies than can reveal the timing of activity more precisely, showing how different brain regions interact, and theoretical analyses of how the brain solves social problems. \u201cUltimately, in a science-fictiony way, I'd like robots that can do theory of mind and attribute intention.\u201d This would be beyond what even Dick imagined \u2014in his novel  Do Androids Dream of Electric Sheep ?, filmed as  Blade Runner , the Voight\u2013Kampff empathy test is a way of spotting replicants' emotional deficiencies. \n               Easily deluded \n             The mental state that arises when we interact with unreality is complex. We get involved to the extent that, say, we cry when Bambi's mother dies, but not so involved that we walk out of the cinema and strike up a conversation with the nearest rabbit. Whatever the explanation is, says psychologist Richard Gerrig of the State University of New York, Stony Brook, it isn't the much-touted suspension of disbelief, because disbelief is not the default. \u201cPeople believe everything, and one must expend effort to disbelieve,\u201d says Gerrig. The brain, it seems, has a default setting of credulity, and a keen appetite for consuming and producing stories. Narrative is a crucial tool in our efforts to understand the world and some brain areas seem specialized for processing it 3 . Information presented in narrative form lowers our critical faculties, and experiments show that the more deeply people become immersed in a story, the easier it is to sway their attitudes towards those advocated in that story 4 . This resonates with  A Scanner Darkly , when an undercover cop becomes so engrossed in his \u2018fictional\u2019 identity as a drug dealer that his police persona begins to pursue his criminal one. Resisting our susceptibility to stories is a useful skill in a media- and advertising-saturated world, says Gerrig. \u201cWe need to get kids and adults to construct disbelief. Because people don't know about this tendency, it puts them at risk.\u201d \u201cIt's not important whether you label something as fiction or non-fiction,\u201d Mar agrees. \u201cThe true distinction is between narrative and non-narrative expository forms that don't draw you into their world.\u201d It also looks as if the ability to lose yourself in a fictional world might reflect your ability to navigate the genuine social world. Mar and his colleagues have found that the more time a person spends reading fiction the greater his or her empathy and social skills; for readers of expository non-fiction (such as, to pick an example at random, science journalism) the correlation is negative 5 . I thought it would be best to keep back that particular piece of reality until the end. \n                     The search for autism's roots \n                   \n                     A Scanner Darkly \n                   \n                     Waking Life \n                   \n                     Raymond Mar \n                   Reprints and Permissions"},
{"file_id": "441807a", "url": "https://www.nature.com/articles/441807a", "year": 2006, "authors": [{"name": "Laura Spinney"}], "parsed_as_year": "2006_or_before", "body": "Reducing your calorie intake makes you live longer \u2014 if you're a rat or a worm. Laura Spinney asks whether the same holds for humans \u2014 and if it does, whether the benefits could be put in a pill. It may be just a dot in the Pacific Ocean, but the Japanese island of Okinawa has long held a fascination for researchers into ageing. Its inhabitants include an unusually high number of centenarians, and fewer of its people die from diseases of old age such as cancer and stroke. Some have suggested the islanders\u2019 secret is their frugal diet. Now scientists are attempting to put this idea to the test \u2014 and are even developing drugs that they hope will produce the same effect. Could the path to a long and healthy life really be as easy as swallowing a tablet? The issue divides researchers. Calorie restriction, or cutting energy intake below energy expenditure, can slow ageing, reduce mortality, and extend maximum lifespan in rats, mice, fish, flies, worms and yeast. But to date, the only evidence that it works in humans is anecdotal. A 1978 study of inhabitants of Okinawa 1  showed that energy intake among adults was roughly 80% of the Japanese average. Some researchers have held this up as evidence that restricting diet reduces age-related diseases and extends longevity. But gerontologist Sataro Goto of Toho University in Chiba, Japan, points out that Japanese people have a daily energy intake nearly 20% less than the average of developed countries, and that the mean life-expectancy of Japanese women is 85 years \u2014 not significantly more than that of women in all developed countries. \n               Hungry for life \n             This hasn't stopped thousands of people all over the world intentionally going hungry in the hope of living longer and healthier. But no one yet knows if their punishing lifestyle will pay off. So an experiment designed to find out is being watched closely by researchers into ageing. The CALERIE (Comprehensive Assessment of Long-term Effects of Reducing Intake of Energy) study, sponsored by the US National Institute on Aging (NIA), published the outcome of its first phase in April 2 . Forty-eight men and women were randomly assigned to diets that would either maintain their weight, or reduce their calorie intake by up to 25% below the level required to maintain their weight. After six months, those on restricted diets showed lower body temperature and levels of insulin in their blood when they had not had a meal, both characteristics of long-lived people and animals. This slowing of the metabolism explains why people on calorie-restricted diets don't starve to death: their body's energy requirements drop to meet the number of calories in their diet. CALERIE is a departure in this hype-infested field, because the volunteers are of normal weight or slightly overweight, but not obese. Most studies so far have used obese individuals, muddying the waters with the health benefits of reducing obesity. The second phase of the study will begin this summer, observing 200 subjects over two years. But according to one of the present study's authors, physiologist Eric Ravussin of the Pennington Biomedical Research Center in Baton Rouge, Louisiana, there are already hints that calorie restriction affects ageing processes in cells. Participants on restricted diets lost weight exponentially, and were still losing weight after six months. They had reduced insulin resistance and reduced levels of low-density lipoprotein cholesterol, high levels of which are risk factors for type 2 diabetes and heart disease, respectively. The researchers also measured significantly reduced damage to volunteers' DNA. \u201cWe asked if there is a metabolic adaptation over and above what is expected due to the weight loss,\u201d says Ravussin, \u201cand the answer is yes, there is.\u201d He says the findings are consistent with the theory that calorie restriction reduces metabolic rate and lowers the production of harmful molecules called reactive oxygen species, also known as free radicals. These strip electrons from other molecules in the body; this is thought to harm cells and contribute to ageing. This metabolic change, Ravussin believes, could be what causes calorie-restricted rodents to live up to 40% longer than their unrestricted counterparts. Clearly it will be a long wait before the CALERIE study reveals a similar effect in humans, if it ever does. Meanwhile, the research community is divided over whether calorie restriction will extend lifespan in humans. The most recent online edition of the journal  Biogerontology  asked a number of experts to address this question 3 , in response to a new theory of longevity championed by mathematical biologist Lloyd Demetrius of Harvard University. Demetrius argues that the main factor determining lifespan is not the rate of free-radical production, but the cells' ability to resist short-term fluctuations in critical metabolites caused by environmental stresses, so protecting those cells from damage. \n               Long haul \n             Demetrius believes this is determined by a creature's evolutionary history. Rodents are opportunistic species that experience periods of plenty punctuated by periods of scarcity. When there is lots of food they reproduce like crazy; when there isn't, they sit tight. They reach sexual maturity early, have a narrow reproductive window and large litters. Humans, and larger-bodied animals in general, mature late, have fewer offspring and a wider reproductive window. They are more able to move from resource-poor regions to resource-rich ones, and are more stable in the face of environmental perturbations 4 . Demetrius's theory predicts that calorie restriction would extend lifespan in opportunistic species, giving them a chance to breed once conditions improve, but barely affect lifespan in more stable species. Some evidence supports this. As Linda Partridge of University College London's Centre for Research on Ageing notes: \u201cExtension of lifespan by dietary restriction has been seen particularly clearly in animals with high reproductive rates \u2014 rodents, worms, flies.\u201d In contrast, experiments in rhesus monkeys have yet to show that calorie restriction increases lifespan 5 . Believers in calorie restriction claim this is a methodological problem: monkeys live longer than rats, so the experiments take longer. Studies with rhesus monkeys begun in the United States in the late 1980s will not produce robust data for another 25 years, and human studies such as CALERIE will take even longer. In the meantime, they argue, the data from studies such as CALERIE support the idea that it could indeed work in people. Such optimism means that the development of drugs aiming to mimic calorie restriction is well advanced, with some compounds already in clinical trials 6 . These drugs are designed to slow ageing and possibly extend life by triggering metabolic adaptation without the need for semi-starvation. Whether or not calorie restriction increases longevity, it does seem to ward off certain age-related diseases. For example, epidemiological studies have shown that low-calorie diets are associated with lower incidences of age-related ailments such as cancer and neurodegenerative diseases 7 . The field grew out of a change in thinking about ageing. Until about 15 years ago, ageing was thought to be an unregulated process resulting from the accumulation of cellular damage. Then genetic studies, particularly in the nematode worm  Caenorhabditis elegans , showed that manipulating single genes could increase or decrease lifespan dramatically 8 . Later, lifespan-extending mutations were found in other organisms, including the fruitfly  Drosophila  and rodents. Many genes have been identified that are thought to play a part in determining lifespan, such as those involved in insulin signalling and stress resistance. One family of enzymes currently in the limelight is the sirtuins. The gene  SIR2 , after which the family is named, was one of the first longevity genes to be identified. Species from yeast to humans carry variants of it, and extra copies increase lifespan in yeast, worms and flies 9 . \n               Sirtuin something \n             Nobody knows how sirtuins might extend lifespan, but David Sinclair, who studies ageing at Harvard Medical School in Boston believes that they stabilize DNA and counteract the reduced fidelity of DNA-copying mechanisms in old cells. The Sir2 protein seems to work with a small molecule called NAD, which is involved in metabolism 9 , so the two together potentially explain the association between calorie restriction and ageing. In 2003, Sinclair's team described 19 plant-derived molecules that activate sirtuins in yeast 10 . One of these, resveratrol, found in grape skins, has been reported to have anticancer and neuroprotective effects. The chemical's presence in wine has been touted as the explanation of the \u2018French paradox\u2019 \u2014 the low incidence of heart disease in southern Europe, despite a fatty diet. Sinclair and others believe it may also extend life. Working with Rafael de Cabo, Donald Ingram and others at the NIA's Laboratory of Experimental Gerontology in Baltimore, Sinclair is studying the effects of resveratrol in non-obese mice. The study's results are not yet published, but de Cabo claims they look promising. There is already published evidence that resveratrol extends life in another vertebrate. In February this year, Alessandro Cellerino and his colleagues at the Scuola Normale Superiore in Pisa, Italy, revealed that the chemical not only extends the maximum lifespan of a short-lived fish called  Nothobranchius furzeri  by 60%, but also seems to protect the fish from neurodegeneration 11 . Cynthia Kenyon, a researcher in ageing at the University of California, San Francisco, is \u201cvery impressed\u201d with the resveratrol results, but, she points out, there is confusion over what it actually does. \u201cIn animals, at least in some cases, it looks like the effects you see are dependent on Sir2,\u201d she says. On the other hand, she says, resveratrol has so far shown no effect on Sir2 function in human cells in culture. This makes it unlikely that the chemical acts directly on the Sir2 protein, although it might target other biochemical processes that interact with Sir2. How it works may not matter in the end, she says. But in the absence of any conclusive evidence that sirtuins extend life in mammals, or that resveratrol activates sirtuins in human cells, Sirtris Pharmaceuticals, a company co-founded by Sinclair and based in Cambridge, Massachusetts, has screened half a million compounds and found some that activate human sirtuins in the test tube. \n               Natural experiments \n             According to Sinclair, Sirtris has unpublished animal data showing that some of these sirtuin activators affect markers of longevity in the same way that calorie restriction does in rodents and in the humans in the CALERIE study \u2014 for example, by reducing insulin levels. One of the molecules is already in clinical trials, although Sinclair warns it will be several years before Sirtris can show that this is an effective way to treat age-related disease, let alone extend life. Kenyon is co-founder of another Cambridge-based company pursuing anti-ageing drugs, Elixir Pharmaceuticals. Elixir is interested in sirtuins, but is taking a broader approach. Kenyon's group discovered that activity in biochemical processes controlled by insulin and another hormone called insulin-like growth factor 1 controls ageing. That work has led them to test molecules that affect those mechanisms, which control the body's use of glucose. The team is also investigating drugs that block the effects of ghrelin, another hormone that controls the body's glucose balance. \u201cThe way we are thinking about getting drugs approved is not of course for ageing, but instead for beneficial effects on age-related disease,\u201d says Kenyon. She envisages that people will start taking them once they have been approved for the latter \u2014 probably within the next five to ten years \u2014 and a huge natural experiment will be launched. \u201cIf they really are drugs against ageing, two things should happen,\u201d she says. \u201cOne, they should have efficacy in additional disease settings, and two, after a while you might start noticing that people taking the drugs seem younger.\u201d Just such an experiment is underway with the drug metformin, which enhances insulin action and was approved for treating type 2 diabetes in 1995. Since then, studies in rodents have shown that metformin can reduce tumour load, and reduce fasting insulin level and body temperature without significantly affecting body weight or food intake. But some researchers are doubtful about its usefulness as an anti-ageing drug, because it has side effects such as a small risk of a build-up of lactic acid in the body which, rarely, can be fatal. We may never be sure whether calorie restriction extends human lifespan, says Ingram. He argues that it is unlikely that a test of the effects of lifelong restriction in humans under controlled conditions will ever be done, for reasons of cost and practicality. In the meantime, some believe that researchers into ageing must follow any lead, because of the social and economic issues posed by a rapidly ageing population. A group of experts led by demographer Jay Olshansky of the University of Illinois, Chicago, is calling on the US government to increase its funding of basic ageing research, with the initial goal of delaying all age-related diseases by about seven years \u2014 a goal they regard as realizable for generations living today. \n                     Live, fast, die old \n                   \n                     Red wine ingredient makes yeast live longer \n                   \n                     Ageing: Growing old gracefully \n                   \n                     Calorie Restriction Society \n                   \n                     Sirtris Pharmaceuticals \n                   \n                     Elixir Pharmaceuticals \n                   Reprints and Permissions"},
{"file_id": "442351a", "url": "https://www.nature.com/articles/442351a", "year": 2006, "authors": [{"name": "Jenny Hogan"}], "parsed_as_year": "2006_or_before", "body": "Faster, safer and easier to control \u2014 chemical reactions in microreactors are taking off in the lab. Now industry is being seduced by the charms of the lab on a chip. Jenny Hogan investigates. A few years ago, a productive PhD student in Peter Seeberger's chemistry lab would run three or four experiments a day. Each would be a painstaking step towards optimized conditions for a new reaction \u2014 be it making a peptide or producing a sugar molecule for use in a possible vaccine. Since then, Seeberger's expectations have soared. Now his students have to work ten times as hard. The 120 reactions that formed the basis for one recent publication 1  were completed in three afternoons. It's not that Seeberger, at the Swiss Federal Institute of Technology in Zurich, has become a slave-driver. Rather, he has updated his lab equipment. He is working with a collection of microreactors \u2014 each one a miniature lab on a chip. The reagents are stirred up again, and again, in channels less than a millimetre in diameter, until the students get the results they need. \u201cPeople in my lab are very excited about this. It gives you time to do more chemistry,\u201d says Seeberger. \u201cI always say that microreactors will be chemists' round-bottomed flasks for the twenty-first century.\u201d The humble glass flask is a chemistry icon, used since alchemists tried to turn base metals into gold. But microreactors promise to make chemistry faster, cleaner and yield purer products. They might also open the door to syntheses not previously feasible on a large scale, and make dangerous \u2014 even explosive \u2014 reactions safer. The technology has grown over the past two decades \u2014 a convergence of the miniaturization of chemical and biological analysis techniques and the engineering of computer chips. Seeberger's chips (pictured above) are typical of what is possible. Just a couple of centimetres big, they feature tiny channels etched into silicon. Chemicals are injected into the device and they react where they merge. The bends in the channels help force the reagents to mix, and the length of the channels and the flow rate determine the reaction time. With reaction volumes measuring just microlitres, conditions such as pressure and temperature can be precisely controlled and quickly changed. \u201cYou only have to run the system until you have one drop of product coming out of the end. You'd spend the longest time walking downstairs to the spectrometer to analyse it,\u201d says Graham Sandford, a chemist at Durham University, UK, who has used microreactors in his lab 2 . Now the technology is also making the leap into industry. Microreactors performing cleaner and safer reactions could push the batch vessels used in the synthesis of some compounds \u2014 including drugs \u2014 into retirement. Ultimately, the devices could end up integrated into drug discovery 3 . \u201cFor me, the important message is that the technology can be applied on an industrial scale,\u201d says Volker Hessel, vice-director for research and development at the Institute of Microtechnology Mainz (IMM), which has collaborated with hundreds of chemical companies on microreactor projects. In industry, high-value products that are typically produced in small batches, such as pharmaceuticals and fine chemicals, have much to gain from the extra flexibility offered by microreactors. Scaling up a lab procedure to batch production can sometimes require redesigning a chemical reaction from scratch. \u201cIn flow world, you just run a reaction longer,\u201d says Tony Wood, head of discovery chemistry for Pfizer in Sandwich, UK. Wood says that Pfizer is only just beginning to explore the possibilities that the technology offers, but he hopes microreactors will change the rules for his chemists. Reactions that they now have to avoid because they are difficult to run in large volumes might become accessible. \u201cWhat's interesting to me is the opportunity to pursue fields such as electrochemistry or photochemistry,\u201d says Wood. \u201cThat would enable us to functionalize molecules in a quite different way from mainstream transformations.\u201d \n               Compound interest \n             But there are still some engineering troubles to be overcome. Precipitates are a problem in any reaction process, but in the tiny channels and chambers of a microreactor, clogging is an ever-present danger. \u201cSolids are problematic and if you can avoid them you will try,\u201d says Hessel. Researchers in the field cite the growing literature on systems that can handle solids as evidence that the problem of clogging will, with time, be conquered. The challenges of performing reactions in which different steps in a synthesis require different solvents are also being dealt with, say industry insiders. As a result, industrial interest in microreactors is spreading fast, says Hessel. \u201cIt's nearly all the big names in chemistry,\u201d he notes. One idea that industry has been quick to latch on to is safety. For example, reactions that release a lot of energy may be controlled in a small flask, but risk exploding in a larger batch vessel where excess heat is harder to dissipate. With a microreactor, scaling up the reaction safely simply requires running more devices in parallel. This encouraged Xi'an Huian Chemical in China to approach Hessel's team at the IMM to set up a microreactor plant to synthesize nitroglycerine. It took only five months for the team to get the plant running and, since September 2005, it has been producing nitroglycerine for use as a treatment for heart disease. Switching from batch to flow chemistry is not just about refitting a plant or lab, it often requires a change of mindset. \u201cYou are in a small, innovative team in an established company that has more than 100 years' experience in chemical production and you want to change things \u2014 there are some barriers beyond the technical,\u201d says Dominique Roberge, head of a project to evaluate microreactors at Lonza, a Swiss company that manufactures intermediates for the drug industry. Even Seeberger was not an immediate convert. After learning about microreactors in 2001, he decided to investigate further in his lab at the Massachusetts Institute of Technology (MIT), where he was then based. He was fortunate that Klavs Jensen in MIT's chemical-engineering department was already making microreactors. But Seeberger wasn't impressed at first. \u201cWhen Klavs showed me his devices, they looked like toys. I thought they would be useless, that you wouldn't be able to make enough material.\u201d But when he came to try it, he found that running a microreactor for a day produced 100 grams of material \u2014 far more than his students are ever likely to need for biological tests. Steven Ley, a chemist at the University of Cambridge, UK, is another who threw out many of his round-bottomed flasks after building a flow-chemistry lab. He welcomes the opportunity to do chemistry differently. For example, he says, some flask reactions have to be carried out at \u2212195 \u00b0C, the temperature of liquid nitrogen, to prevent 'overcooking' the reactants, but they can be performed at room temperature in microreactors. This would make them economical for industry. Even for existing batch reactions, microreactors can offer an attractive alternative. The flow inside their small channels is better behaved than in a large vessel, and so is more reproducible. In the drug industry, if a batch doesn't match the specifications approved by regulators, it has to be thrown away. \u201cMany batches are lost and very often it's the culmination of several months' work,\u201d says Brian Warrington, former vice-president of technology development for GlaxoSmithKline, UK. \u201cIt's a big commercial problem.\u201d That was one reason why Warrington pushed the idea of microreactors at GlaxoSmithKline in the late 1990s. The other was the idea of 'closing the loop'. Warrington says the ultimate goal is to have a microreactor pumping its product straight into a cell-based assay, which is hooked up to provide feedback to a computer controlling the synthesis of the next product to be tested. \n               Recipe for success \n             In his Cambridge lab, Ley is experiencing the drug industry's growing interest in flow chemistry directly. Companies are clamouring to send people to the lab for training, he says. One of his PhD students collaborates with Chris Selway, one of the drug-discovery chemists at Pfizer charged with evaluating the technology's potential for the firm. So far, Selway remains cautious about microreactors. \u201cWe are seeing lots of claims in the literature about how good flow chemistry is and, as a company, we want to be involved in that,\u201d says Selway, \u201cbut by no means is it a tool that's going to radically take over from batch chemistry.\u201d Industry still has to work out the economics of microreactors. Lonza began operating a pilot plant using microreactors in March and Roberge has analysed 83 reactions performed by the company 4 . He found that half could benefit from being carried out in microreactors, although solids in many of these reactions reduced that number to 16. His preliminary economic analysis suggests that the cost of building and commissioning the microreactor plant will be comparable to a batch system of similar throughput \u2014 around \u20ac250,000 (US$316,000). The main hope for future cost savings, he says, is if microreactors can deliver improved yields and so use lower amounts of raw material. \u201cThe question of whether microreactors are going to be used in the future, I think this is already answered 'yes',\u201d says Roberge. \u201cThe open question is what per cent of the market in fine chemicals they will take.\u201d For more on lab-on-a-chip technology see the Insight on  pages 367\u2013418  of this issue. \n                     Lab on a chip \n                   \n                     Chemical technology: All together now \n                   \n                     Micro lab makes cheap PET scan ingredient \n                   \n                     Science in culture \n                   \n                     Peter Seeberger's group \n                   \n                     Institute for Microtechnology, Mainz \n                   \n                     Steven Ley's group \n                   Reprints and Permissions"},
{"file_id": "4411040a", "url": "https://www.nature.com/articles/4411040a", "year": 2006, "authors": [], "parsed_as_year": "2006_or_before", "body": "Every day, all over the planet and beyond it, scientists try to make sense of the world in which they live. Here we present a composite picture of just one day \u2014 21 June 2006, the Northern summer solstice. See  news@nature.com  for a greatly expanded version of this feature. \n               Cerro Paranal, Chile \n               00:12    UT  Morten Andersen is at the control console for Yepun, the fourth telescope in the European Southern Observatory's Very Large Telescope array. He is taking a census of the smallest stars of the Galaxy's most massive young cluster, Westerlund 1. 20:12 (20 June) local time  24\u00b038\u2032 S 70\u00b024\u2032 W \n               Bintulu, Malaysia \n               00:23    UT  Diana James Junau, a project officer with the Planted Forests Project at Putra Malaysia University's Bintulu campus on Borneo, measures a molar tooth from one of a series of pig skulls. The skulls were collected as part of a five-year research project into the population biology of the bearded pig ( Sus barbatus ), to determine whether traditional food sources are sustainable when forests are turned into patchworks of cleared land. 08:23 local time  3\u00b010\u2032 N 113\u00b002\u2032 E \n               Low Earth orbit \n               00:26    UT  The Hubble Space Telescope begins a 1,344-second near-infrared exposure. Anton Koekemoer of the Space Telescope Science Institute in Maryland will combine these data with subsequent exposures of the same field, obtained throughout the day, as part of the Hubble Ultra Deep Field project. The aim is to find distant galaxies that were the first sources of light in the primordial Universe. \n               Oarai, Japan \n             01:00    UT  At a test facility that is part of the JOYO experimental fast reactor, Takafumi Aoyama, a nuclear engineer for the Japan Atomic Energy Agency, consults and calibrates a temperature monitor. The monitor, which works on the basis of thermal expansion, has been developed for use in the reactor's core subassembly. 10:00 local time  36\u00b019\u2032 N 140\u00b036\u2032 E \n               Hanoi, Vietnam \n               01:30    UT  Nguyen Van Hanh, a PhD student, shows Bui Xuan Nguyen some egg cells that have been matured  in vitro  for 22 hours. Nguyen needs the cells for his attempt to clone the saola ( Pseudoryx nghetinhensis ), a species of ox that lives only on the Laos\u2013Vietnam border and is one of the rarest mammals in the world. Some of the swamp-buffalo oocytes look good and have clear polar bodies. Nguyen starts to remove the nuclei from the buffalo egg cells while a colleague prepares some saola cells, readying them for fusion around noon. 08:30 local time  21\u00b001\u2032 N 105\u00b030\u2032 E \n               Bangalore, India \n             02:55    UT  This little girl, weighing 3.14 kg, is born just before 8 a.m. local time in Manipal Hospital \u2014 the first baby for proud mother and father Rhadhika and Rajesh Sinha, who were still thinking of a name for her when this photo was taken. Hers is one of the estimated 358,522 new human lives starting on 21 June 2006, more than double the number (155,000 or so) ending. The estimated world population stands at 6,523,642,761. Babies born in India can currently expect to live for 63.3 years, some years shy of the global life expectancy of 67. 07:55 local time  12\u00b058\u2032 N 77\u00b034\u2032 E \n               Newcastle, Australia \n             03:15    UT  Anne Imenes at Australia's National Solar Energy Centre is testing one of the 200 mirrors intended for the \u2018solar concentrator\u2019. The experimental array will concentrate 500 kW of solar thermal power on to a reactor 17 metres above the ground, heating it to more than 1,000 \u00b0C. The reactor will be installed in July. 13:15 local time  32\u00b053\u2032 S 151\u00b044\u2032 E \n               Strait of Johor \n             03:45    UT  Juan Walford and his colleague B. Sivaloganathan of the National University of Singapore are checking on some six-month-old seahorses, which they are rearing in a floating fish farm off the east coast of Singapore. Their study aims to use the seahorses as living indicators of the marine environment. This morning they find a lot of \u2018pregnant\u2019 males with embryos in their brood pouch, indicating that the juvenile males have successfully reached reproductive age, and that it would be feasible to restock the area. 11:45 local time  1\u00b024\u2032 N 103\u00b058\u2032 E \n               Florham Park, New Jersey \n             03:47    UT  Dan Silver, marketing director for a multinational manufacturer of medical devices, e-mails ProMED-mail with a report that he has translated from a Chinese newspaper. The Internet site ( http://www.promedmail.org ) provides information on disease outbreaks and depends on doctors, researchers and health professionals around the world sending in information in this way. The report describes 60 students and teachers who have come down with an undiagnosed febrile illness in Shaanxi, China, since 12 June. 23:47 (20 June) local time  40\u00b047\u2032 N 74\u00b028\u2032 W \n               Tokyo, Japan \n             04:00    UT  The hundreds of small mirrors, beam splitters and lenses in Akira Furusawa's quantum-computing lab at the University of Tokyo look like a mess; in fact they are meticulously organized. One of his students is trying to beat the world record for enhancing the quantum correlation between photons. 13:00 local time  35\u00b043\u2032 N 139\u00b046\u2032 E \n               Krasnoyarsk, Russia \n             04:00    UT  At the Sukachev Institute of Forest, Eugene Vaganov and Ernst-Detlef Schulze, visiting from the Max Planck Institute for Biogeochemistry in Jena, Germany, pore over a joint publication on carbon-isotope ratios in wood. They are trying to figure out how to disentangle the contributions of isotopes from photosynthesis, carbon storage, respiration and wood formation when interpreting the isotope patterns found in tree rings. 12:00 local time  56\u00b000\u2032 N 92\u00b047\u2032 E \n               New York, New York \n             04:33    UT  In a dark, cramped lab at Columbia University Medical Center a microscope automatically snaps images of live yeast cells as they divide. Joanne Bruno is trying to find out which fluorescently tagged proteins behave asymmetrically during cell division, segregating unevenly between daughter cells. 00:33 local time  40\u00b048\u2032 N 73\u00b058\u2032 W \n               Tidbinbilla, Australia \n             04:35    UT  At the Deep Space Network station outside Canberra, a 34-metre radio dish starts to communicate with the Mars Odyssey orbiter, 343.6 million kilometres away. Data will pass between the ground station and the satellite for the next 4 hours 35 minutes. 14:35 local time  35\u00b026\u2032 S 148\u00b056\u2032 E \n               W\u00fcrzburg, Germany \n             05:11    UT  At the wine laboratory of the Bavarian Health and Food Safety Authority, Norbert Christoph is switching on the sample changer of the 400-MHz NMR spectrometer. The device will determine the deuterium:hydrogen isotope ratio of ethanol and sugar in two Macedonian wines and three apple juices. In less than an hour, the staff of the institute will leave for their annual outing \u2014 an excursion by bus to the city and castle of Langenburg. After a visit to the old town, the castle and a vintage car museum they will enjoy some local food and wine. Meanwhile, back in the lab, the NMR wine spectrometer is pulsing at 61.4 MHz every 7 seconds in order to measure the deuterated ethanol molecules. 07:11 local time  49\u00b047\u2032 N 9\u00b056\u2032 E \n               Munich, Germany \n             06:24    UT  Martin Hrab\u00e9 de Angeles enters the German Mouse Clinic at the National Research Centre for Environment and Health. Today he and his colleagues are going to start characterizing new mouse models for bone-related diseases. By the end of the day they will have made a total of 1,636 measurements of characteristics such as bone density, strength and biochemical composition. While gathering these data, they will discuss ways in which models may shed light on the molecular basis of bone diseases. 08:24 local time  48\u00b007\u2032 N 11\u00b034\u2032 E \n               Bethlehem, Palestinian National Authority \n             06:30    UT  Moein Kanaan drops off some DNA sequencing reagents for his lab at Bethlehem University. Having an Israeli ID makes it easier for him than for many of his colleagues to move such reagents across the Israel\u2013Palestine border. The next job of the day is to order a new spectrophotometer called a Nanodrop, an instrument that measures minute amounts of DNA, RNA and protein. Kanaan is trying to squeeze the supplier of the Nanodrop for as big a discount as possible. 09:30 local time  31\u00b042\u2032 N 35\u00b012\u2032 E \n               Copenhagen, Denmark \n             06:37    UT  Erik Born of the Greenland Institute of Natural Resources is writing to the Danish Ministry of Justice for a permit to export and re-import two crossbows and a tranquillizer gun that he wants to take to Canada. He hopes to use the equipment to take skin biopsies from walruses on Baffin Island in August and to attach satellite transmitters to some of the animals. 08:37 local time  55\u00b041\u2032 N 12\u00b036\u2032 E \n               Low Earth orbit \n             06:46    UT  On board the International Space Station, Jeffrey Williams photographs a 19-km-diameter crater in the Australian outback that is more than 515 million years old. The snapshot, the first of three Williams will take today, is part of a series of Earth observations that the station does for a variety of different scientific projects. \n               Frederiksberg, Denmark \n             07:10    UT  At a supermarket run by the Danish Department of Human Nutrition, six overweight families collect food and drink for the next couple of days. This is part of Diogenes, a six-month dietary-intervention trial sponsored by the European Union. All the products are supplied to the volunteers free of charge, and the bar codes on the food items are used to declare nutritional information, which is registered by a computer linked to the barcode scanner at the checkout. Two families complain that the researchers are out of fresh chicken breasts. 09:10 local time  55\u00b025\u2032 N 11\u00b034\u2032 E \n               Tsukuba, Japan \n             07:24    UT  At the KEK B factory, birds are singing despite the overcast day. The electron and positron beams in the accelerator are colliding with a brightness that far outshines any other accelerator facility. The factory's Belle experiment has so far recorded more than 500 million pairs of B mesons. Like many at the facility, Jasna Dragic is analysing the large data sample and preparing a talk on the latest results, which she will present at the International Conference on High Energy Physics in Moscow. Her colleague Ruslan Chistov is impatiently waiting for final approval from Belle spokespersons to submit a manuscript to  Physical Review Letters  reporting the discovery of two new particles. 16:24 local time  36\u00b009\u2032 N 140\u00b004\u2032 E \n               Prague, Czech Republic \n             07:27    UT  Catherine Staessen of the Flemish-speaking Free University of Brussels has just made her presentation at the annual conference of the European Society of Human Reproduction and Embryology. She has analysed the limited trial data available so far on the value of screening embryos created by  in vitro  fertilization for chromosomal abnormalities before they are implanted. She concludes that there is no evidence that screening increases a woman's chance of having a healthy baby. \u201cYou have a beautiful approach but the wrong answers!\u201d complains Yury Verlinsky, director of the Reproductive Genetics Institute in Chicago and a pioneer of preimplantation genetics. \u201cAbnormal human embryos are an obvious fact, so if you remove those, how can there be no benefit?\u201d Staessen's response: do the trials and prove to me it works. 09:27 local time  50\u00b004\u2032 N 14\u00b026\u2032 E \n               The Alexander/Zomar stream, Israel \n             08:05    UT  Lior Asaf of the Arava Institute for Environmental Studies in Israel and his Palestinian partner Nader Al Khateeb of the Water and Environmental Development Organization in Bethlehem are measuring the flow of a stream using an electromagnetic velocity meter. The work is a part of a project to estimate the pollution from different sources that ends up in the Alexander/Zomar stream, which crosses the boundaries between Israeli and Palestinian lands. 11:05 local time  32\u00b021\u2032 N 34\u00b057\u2032 E \n               Kunming, China \n             08:35    UT  Weizhi Ji is preparing to view several rhesus-monkey embryos and cloned blastocysts obtained by  in vitro  fertilization under a confocal microscope at Kunming Primate Research Center. He is interested in comparing indicators of gene activation in the cells that go on to form the embryo itself and those that form the placenta and amniotic membranes. 16:35 local time  25\u00b004\u2032 N 102\u00b042\u2032 E \n               Peradeniya, Sri Lanka \n             08:40    UT  Kapila Dahanayake, a geologist at the University of Peradeniya, is analysing a sample of water and sediment from the tsunami of December 2004 that was preserved in an empty  arak  bottle found in a flooded house. The sediment contains microplankton typical of the deep ocean and has a particular distribution of grain sizes. They are very like the sediments found at a depth of 1.5 metres in a nearby well, which supports the idea that those sediments too were laid down by a tsunami. 14:10 local time  7\u00b015\u2032 N 80\u00b034\u2032 E \n               Bellville, South Africa \n             08:45    UT  At the South African National Bioinformatics Institute, Winston Hide analyses a set of mutations in HIV that have resulted from treatment with the antiretroviral drug neviparine, used to prevent transmission of the virus from mother to child. He then leaves for the airport to pick up his collaborator Harukazu Suzuki, who has spent 21 hours getting to South Africa from the RIKEN Genomic Sciences Center in Yokohoma, Japan. On the way, he drives through shacklands and past HIV-awareness posters, and jockeys for space with horse-drawn carts. 10:45 local time  33\u00b056\u2032 S 18\u00b038\u2032 E  \n               Robledo de Chavela, Spain \n             08:55    UT  As Mars sinks in Australia's western sky, it rises in the east for Spain. A Deep Space Network 34-metre dish near Madrid, almost identical to the one outside Canberra, takes up the job of bringing home data from Mars Odyssey. This includes data relayed through the satellite from the Spirit rover, currently encamped for the winter on a north-facing slope in the Columbia Hills of Gusev Crater. 10:55 local time  40\u00b030\u2032 S 4\u00b014\u2032 W \n               In transit, Frankfurt to Manila \n             09:00    UT  In a plane somewhere over the Indian Ocean, the chairman of the Intergovernmental Panel on Climate Change, R. K. Pachauri, is on his way to a meeting of the Asian Development Bank. He picks up an e-mail from the vice-president responsible for sustainable development at the largest retailing organization in the world. The two men met less than a week ago in Iceland. The message includes an invitation to a meeting next month where 150 senior corporate leaders will discuss climate change. \n               Mauna Loa, Hawaii \n             09:00    UT  The average concentration of carbon dioxide in Hawaii's air on 20 June has just been calculated: 383.63 parts per million. The data from Mauna Loa constitute the longest record of direct measurements of CO 2  in the atmosphere. 23:00 (20 June) local time  19\u00b032\u2032 N 155\u00b035\u2032 W \n               Ambohimiravavy, Madagascar \n             09:23    UT  At the summit of Ambohimiravavy (2,310 m), Martin Callmander and his team from the Missouri Botanical Garden in St Louis are collecting plants on the unexplored and hard-to-reach massif. 12:23 local time  14\u00b012\u2032 S 49\u00b06\u2032 E \n               Western Pacific Ocean \n             09:50    UT  A measuring device that has been moored to the bottom of the ocean bobs to the surface, ready to be picked up by Randolph Watts and Kathleen Donohue, from the University of Rhode Island, and their team on the Melville, a ship operated by the Scripps Institution of Oceanography. The surfacing instrument (a \u2018current and pressure recording inverted echo sounder\u2019, or CPIES, pronounced \u2018sea-pies\u2019) contains two years of data on the conditions in this cold mixed-water region. Picking up these devices is usually straightforward. This time, however, the surfacing CPIES has inadvertently turned off its relocation radio and strobe-light, and the sea is covered in pea-soup fog. It takes five hours to find it and haul it safely aboard. 23:50 ship's time  39\u00b031\u2032 N 148\u00b020\u2032 E \n               Alexandria, Egypt \n             10:02    UT  School students from Alexandria gather in the plaza of the new Bibliotheca Alexandrina to measure Earth's circumference using the method that Eratosthenes pioneered nearly 2,000 years ago. Eratosthenes \u2014 the third director of the original library of Alexandria \u2014 observed that, on the solstice, the Sun's reflection could be seen at the bottom of a well on Elephantine Island, near Aswan, and must thus be directly overhead. By measuring the length of a shadow on the same day near Alexandria, he was able to calculate Earth's radius geometrically. Today, children in Alexandria are making one measurement while children near Eratosthenes' well in Aswan make the other; they compare results in a videoconference. \u201cWe did it! We measured Earth's circumference!\u201d the Alexandrians shout out when the calculations come good. 13:02 local time . 31\u00b013\u2032 N 29\u00b057\u2032E \n               CERN, the border of Switzerland and France \n               10:05    UT  Having spent much of the morning finishing a paper for next week's European Particle Accelerator Conference in Edinburgh, Mike Lamont takes the lift down to the 27-km tunnel of the Large Hadron Collider. Pasted on its wall, the lift has a large schedule for football's World Cup; graffiti indicate, surprisingly, that it is about to be won by Morocco. In the evening, a beer outside a bar in St Genis beckons \u2014 but so does work on a second paper for the Edinburgh meeting. 12:05 local time  46\u00b014\u2032 N 6\u00b003\u2032 E \n               Amundsen\u2013Scott station, Antarctica \n             10:54    UT  After his daily 20-minute walk across the frozen polar plateau from the new elevated South Pole station to the Dark Sector Lab, Denis Barkats transfers 90 litres of liquid helium and 40 litres of liquid nitrogen into the cryostat for the BICEP telescope (for Background Imaging of Cosmic Extragalactic Polarization). The telescope makes use of the long, cold Antarctic nights to gather up as many photons as possible from the earliest days of the Universe. The liquids keep the array of 98 polarization-sensitive bolometers at the telescope's heart at a temperature of 250 millikelvin. The outside world isn't quite that cold \u2014 just \u221265 \u00b0C. And the atmosphere in the station is positively festive as the 64 \u2018winterovers\u2019 celebrate midwinter night. In three months exactly, the Sun will rise again. 22:54 local time  90\u00b0 S 0\u00b0 \n               Culham, UK \n             10.55    UT  Dragoslav Ciric coaxes the first pulses of deuterium through the Culham Science Centre's new neutral beam injector, which will eventually be used to heat plasmas in the Mega Amp Spherical Tokamak (MAST), a nuclear-fusion experiment run by the UK Atomic Energy Authority. Until now, MAST has relied on two injectors on loan from the Oak Ridge National Laboratory, but these use 1960s technology and deliver only short beam pulses of just a quarter of a second. By mid-July, researchers should be ready to investigate the effect of high-power, long-pulse injection on MAST plasmas. 11.55 local time  51\u00b039\u2032 N 1\u00b014\u2032 E \n               Ny\u2013\u00c5lesund, Svalbard \n             11:05    UT  Ruth M\u00fcller, Franciska Steinhoff and Max Schwanitz, researchers at the Franco-German AWIPEV base, head out to sea in a zodiac to measure light intensity in the water. AWIPEV's scientists work on all aspects of the environment, releasing balloons to measure ozone levels in the stratosphere and diving into the sea to pull up sediment samples. The light-intensity measurements are part of a project to assess the influence of ultraviolet radiation on algae in the Arctic Ocean. 13.05 local time  77\u00b034\u2032 N 23\u00b042\u2032 E \n               Bangalore, India \n             11:25    UT  Mylswamy Annadurai, project director for Chandrayaan-1, India's first mission to the Moon, sits down with two of his engineers for a final check of the spacecraft simulator systems they will be taking to the United States in two days. There they will be used to check the interfaces for two US instruments that will be included in the mission. The spacecraft is due for launch next year. 16:55 local time  12\u00b058\u2032 N 77\u00b034\u2032 E \n               Borneo, Indonesia \n             12:00    UT  Chairul Nidom, a virologist at Airlangga University's tropical-disease centre in Surabaya, Java, visits the Orangutan Rehabilitation Center in Palangkaraya to collect blood and trachea swab samples from orangutans. The trachea swabs will be tested for influenza viruses, including bird flu, and tuberculosis. The blood sera will be analysed for hepatitis viruses. It's the end of a busy day for Nidom, who began by giving an 8 a.m. lecture to veterinary students in Java on animal welfare, followed by an afternoon lecture to vets in Palangkaraya on the H5N1 bird-flu virus in chickens, pigs, wild birds and humans. 20.00 local time  2\u00b016\u2032 S 113\u00b055\u2032 E \n               Nouragues Station, French Guiana \n             12:10    UT  Following the station's temporary evacuation after two murders by illegal gold miners last month, work is beginning again under the permanent protection of the gendarmes. The tapir team is recovering photos from the 22 trap cameras set in 9 square kilometres of forest. The ONCFS, France's office for hunting and wild fauna, wants to compare the tapir densities in regions where they are hunted, and in the undisturbed region of Nouragues. 09:10 local time  4\u00b005\u2032 N 52\u00b040\u2032 W \n               Oxford, UK \n             12:25    UT  Overheard at lunch in a neuroscience lab: \u201cThey've only had 27 applications and they're supposed to fund ten of them. I really like those odds.\u201d 13:25 local time  51\u00b045\u2032 N 1\u00b015\u2032 W \n               Worldwide \n             12:26    UT  At the moment of the solstice, the Sun is as high as it gets above the plane of the Equator. \n               Ewa Beach, Hawaii \n             12:35    UT  Gerard Fryer is fast asleep in the trailer behind the Pacific Tsunami Warning Center when his pager starts bleating. Twelve thousand kilometres away an earthquake has been picked up by seismometers. He pulls on his clothes and runs to the operations building to discover that Dailin Wang, the scientist on watch, has already located and measured the earthquake: it was in the Nicobar Islands and of magnitude 5.9. This is not large enough to create a tsunami, so they send a message saying so. 02:35 local time  21\u00b019\u2032 N 158\u00b001\u2032 W \n               Rockville, Maryland \n             13:05    UT  Jeffery Taubenberger is packing up his office and lab at the Armed Forces Institute of Pathology ready for his move to the National Institutes of Health, where he will set up a research programme on flu pathogenesis. He is also in the process of analysing initial sequence fragments from pre-1918 human influenza cases obtained by screening autopsy lung tissue samples for influenza A with a DNA amplification protocol. This should allow him and his colleagues to determine in the long run whether any parts of the pre-1918 human flu virus were carried forward into the pandemic virus of 1918. 09:05 local time  38\u00b058\u2032 N 77\u00b002\u2032 W \n               Worldwide \n             14:00    UT  The Interacademy Panel on International Issues, representing the world's national science academies, releases a joint statement on the teaching of evolution. The statement urges parents and teachers to provide children with the facts about the origins and evolution of life on Earth. \n               Manhica, Mozambique \n             14:05    UT  After getting back from the World Health Organization office in Maputo where he was planning a meeting on the introduction of possible malaria vaccines in Africa next year, Betuel Siga\u00faque, a doctor and public-health worker, is catching up on e-mails when he's interrupted by bad news. The generator in T\u00e2ninga, one of the outlying areas involved in Siga\u00faque's malaria research, is misbehaving, and someone will have to go out there and bring research materials back to the main lab in Manhica. 16:05 local time  25\u00b024\u00d7 S 32\u00b048\u00d7 E \n               Atlantic Ocean \n             14:35    UT  A float that spends most of its life drifting well below the water pops to the surface off the northwest of Ireland and transmits a burst of data to a satellite. It is one of 2,447 floats in the Argo system, which travel the world mapping out currents and measuring water temperatures. The satellite sends the data down to a ground station outside Toulouse, France. An automatic program at the British Oceanographic Data Centre in Liverpool pulls the data across, checks the quality, reformats and passes them on to one of the three international Global Data Assembly Centres, where they are merged with data from all the other nations that contribute to the Argo programme. 15:35 local time  56\u00b012\u2032 N 11\u00b015\u2032 W \n               Pika Camp, Canada \n             15:22    UT  As the clouds lift from the mountain peaks of the Yukon, Sarah Trefry sets off to a nearby boulder field to record the vocalizations of collared pikas (Ochotona collaris). Her work is part of a study into the effects of rapid climate warming on the dynamics of alpine ecosystems. Although the past winter was one of the warmest on record, the late spring snowdrifts are melting slowly and the alpine meadows are only just beginning to show the first signs of summer life. 08:22 local time  61\u00b008\u2032 N 138\u00b010\u2032 W \n               South Atlantic Ocean \n               15:30    UT  The  Polarstern  is on her way south to the Southern Ocean to perform a study of overwintering mechanisms of krill and other organisms. The crew and scientists are using the few daylight hours to prepare the plankton nets. This is not so easy in a force 8.0 and with six-metre waves. 15:30 ship's time  46\u00b029\u2032 S 07\u00b031\u2032 E \n               Stockholm, Sweden \n             15:45    UT  Tomas Nyman of the Structural Genomics Consortium (SGC) lab at the Karolinska Institute is enjoying his afternoon coffee, eating fresh strawberries and cream on the sunny balcony outside the lab. The working day in Stockholm will soon be over, although the sun will continue to shine for a good five hours. The other two SGC laboratories, in Oxford and Toronto, will continue to operate into the Swedish night. By the time the Canadians end their day, the SGC will have deposited structures for ten human proteins in the Protein Data Bank. 17:45 local time  59\u00b017\u2032 N 18\u00b004\u2032 E \n               Atlanta, Georgia \n             16:20    UT  At the Centers for Disease Control and Prevention, Fred Tenover is dismayed by the outcome of his latest study. His team has been looking at the automated systems that identify antimicrobial-resistant bacteria. Tenover has just found that several of the systems fail to identify the strains of multidrug-resistant  Staphylococcus aureus  and  Enterococcus faecium  that are known to be resistant to linezolid. If labs can't detect these strains, they will tell doctors that a patient's infection should respond to the drug, when it might not. This is not the 20th wedding anniversary present that Tenover was anticipating. 12:20 local time  33\u00b043\u2032 N 84\u00b023\u2032 W \n               Sutherland, South Africa \n             16:42    UT  At dusk on the high plateau of the Great Karoo Desert, staff astronomers Encarni Romero Colmenero, Nicola Loaring and Fred Marang carefully align the 91 individual segments of the South African Large Telescope's 11-metre mirror. Tonight's scientific programme has been designed to test the instruments and subsystems of this fledgling observatory. They will be looking at a gravitationally lensed quasar, black-hole candidates in the Galactic bulge, dynamic winds from Wolf\u2013Rayet stars and giant radio galaxies. 18:42 local time  32\u00b023\u2032 S 20\u00b049\u2032 E \n               New York, New York \n               16:43    UT  The American Museum of Natural History has invited the press \u2018behind the scenes\u2019 to preview a snakes and lizards exhibition. Postdoc Jack Conrad is showing off a slab of orange sandstone collected from the Gobi Desert that contains the delicate, 84-million-year-old skeleton of a recently discovered lizard species. The grey-banded kingsnake vanishing up curator Darrel Frost's sleeve, meanwhile, is neither an exhibit nor a piece of research; it's a pet. 12:43 local time  40\u00b047\u2032 N 73\u00b058\u2032 W \n               Yellowstone National Park, Wyoming \n               16:59    UT  In Amphitheater Springs, Tim McDermott and graduate student Dana Skorupa remove samples of algae from water that can reach 72 \u00b0C. The researchers, based at the Thermal Biology Institute of Montana State University, will extract messenger RNA from the algae for use in microarrays. They are studying which genes get turned on or off in June, when levels of visible and ultraviolet light reach a peak, and the algae die off in a big way. 10:59 local time  44\u00b048\u2032 N 110\u00b044\u2032 W \n               Ambo, Peru \n             17:00    UT  An international team of scientists sponsored by the National Geographic Society is deep inside a cave, partway up a cliff that overlooks Ambo \u2014 some 2,000 metres up in the Andes. Bruce Shockey of the American Museum of Natural History is excavating a partial skeleton of a large sabre-toothed cat; 20 metres below him, Rodolfo Salas of Lima's Natural History Museum discovers the jaw of a new sloth. French colleagues map the labyrinth known locally as Jatun Uchco, the \u2018profound hole\u2019. Later, on his way to an Internet caf\u00e9, Shockey gets \u201ccaveman dust\u201d all over the people sharing his taxi. 12:00 local time  10\u00b008\u2032 S 76\u00b012\u2032 W \n               Muscat, Oman \n             17.03    UT  In her office at Sultan Qaboos University, botanist Annette Patzelt finishes formatting the draft manuscript of the Oman Plant Red Data List. The book contains detailed information about the conservation status of 262 Omani plants; it is the first such register in the whole of the Arabian peninsula. 21:03 local time  23\u00b036\u2032 N 58\u00b032\u2032 E \n               Waterloo, Canada \n             17:15    UT  \u201cHow do you make a clock out of light?\u201d asks Olaf Dreyer, a visitor having lunch at the Perimeter Institute for Theoretical Physics. He wants to figure out whether it is possible to use light beams to define time in a theory of quantum gravity. Fotini Markopoulou and Daniel Gottesman from the institute kick the idea around with him for a bit before more people arrive and the conversation inevitably turns to the World Cup. No practical plans for a light-clock emerge. 13:15 local time  43\u00b028\u2032 N 80\u00b032\u2032 W \n               Worcester, Massachusetts \n             17:54    UT  Daniel Rowe and his mentor Douglas Golenbock raise a glass to celebrate Rowe's successful defence of his PhD thesis. Rowe identified a protein that interacts with an immune-system molecule called toll-like receptor 4, or TLR4. The work took him five years; he has yet to decide his postdoc position. 13:54 local time  42\u00b016\u2032 N 71\u00b048\u2032 W \n               Moorea Island, French Polynesia \n               18:29    UT  Gustav Paulay and Sally Holbrook of the Gump Research Station collect coral symbionts from a reef just after dawn. Paulay is part of the Moorea Biocode Project, which aims to catalogue the entire flora and fauna of this volcanic island. Holbrook studies the complex system of coral reefs and lagoons that surround the island. The animals in the early-morning catch will be identified, photographed and genetically barcoded for use in studies of the reef's food webs and populations. 08:29 local time  17\u00b030\u2032 S 149\u00b050\u2032 W \n               Buenos Aires, Argentina \n             19.00    UT  Everyone in the organic-chemistry department of Buenos Aires University is gathered in front of the department lounge's television to watch the Argentina\u2013Netherlands football match. All research activity has stopped except for the NMR spectrometer next door, which is measuring a carbon-13 spectrum \u2014 something it will manage to do uninterrupted for the next 90 minutes. 16:00 local time  34\u00b012\u2032 S 58\u00b018\u2032 W \n               Gr\u00f6ningen, the Netherlands \n             19:00    UT  The staff, students and researchers from Ben Feringa's synthetic organic chemistry lab at Gr\u00f6ningen University, having enjoyed their annual barbecue at the boss's house, are gathered indoors in front of a big screen for the Netherlands\u2013Argentina match. Even those who come from neither country are pretty excited. 21:00 local time  53\u00b013\u2032 N 6\u00b034\u2032 E \n               Pasadena, California \n             19:04    UT  At the Jet Propulsion Laboratory, Jon Giorgini updates the orbit of asteroid 2004 XP14 and sends the tracking data to the Goldstone planetary radar facility in the Mojave Desert. 2004 XP14 will come almost as close to Earth as the Moon is on 3 July; it will be the closest asteroid approach ever observed by radar. 12:04 local time  34\u00b012\u2032 N 118\u00b011\u2032 W \n               Denver, Colorado \n             19:12    UT  Bruce Williams, who teaches 13-year olds at Joyce Kilmer School in Trenton, New Jersey, is registering a team to compete in the National Middle School Science Bowl in Denver. His students, Roger Barrett, Niyi Odumosu, Dyamond Ruffin and Dahlia Wesley, will compete in a hydrogen fuel-cell model-car race: the fastest over 10 metres wins. 13:12 local time  39\u00b041\u2032 N 104\u00b058\u2032 W \n               Norman, Oklahoma \n             20:00    UT  Samuel Mbainayel, a meteorologist in Chad's civil service, takes his wife and daughter to the local health centre for immunizations. When he gets back, he returns to the literature review he is doing for a paper on how variability in rainfall in the Soudano-Sahelian zone of west Africa might be affected by interactions between land surface and the atmosphere, which will form part of the master's thesis he's over in the United States to work on. 15:00 local time  35\u00b013\u2032 N 97\u00b026\u2032 W \n               Ond\u0159ejov, Czech Republic \n               20:45    UT  It's not a good night for the automated observatories in the Czech Republic as they scan the skies for fireballs \u2014 haze and clouds are affecting the view. The only trails observed are caused by planes and the International Space Station. Instead, Pavel Sporny spends the evening analysing the trajectory of a meteor spotted over the remote Nullarbor Plain in southwest Australia on 8 March, captured on disk by collaborators at the Western Australian Museum in Perth. 22:45 local time  49\u00b054\u2032 N 14\u00b046\u2032 E \n               Bar Harbor, Maine \n             21:35    UT  The Jackson Laboratory freezes its 3,052,986th mouse embryo. There are now 2,612 strains of mice cryopreserved in the labs' vaults. Also passing through the cryopreservation service today: 1 fresh sperm order, 1,597 two-cell embryos frozen in 38 straws, 40 embryo transfers and 3,084 oocytes. One order was shipped. 17:35 local time  44\u00b022\u2032 N 68\u00b012\u2032 W \n               Malarg\u00fce, Argentina \n             23:04    UT  At the Pierre Auger Observatory, after hours of careful calibration, researchers begin to take their data. Eighteen telescopes spread across the Pampa amarilla of western Argentina begin the hunt for the glow of cosmic rays from far beyond the Galaxy. The wind is a little high, but the skies are clear \u2014 and the night will be the longest that the team will have all year. 20:04 local time  35\u00b028\u2032 S 69\u00b035\u2032 W There is a greatly expanded version of this feature online\u00a0\u2192  http://www.nature.com/news/specials/solstice Reprints and Permissions"},
{"file_id": "441924a", "url": "https://www.nature.com/articles/441924a", "year": 2006, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "There's more to science at the movies than Lex Luthor's attempts to synthesize kryptonite. In the first of two features on film, John Whitfield looks at how a cinematographic technique can provide insights into the perception of reality. In the second, Alison Abbott meets Ben Heisenberg, a director whose first film is a taut moral fable of laboratory life. Johannes walks across the grass to a Munich research institute on his first day in the lab. A woman approaches him. She is from the secret service and she wants him to spy on a fellow postdoc, Farid, a French citizen of Algerian origin, vaguely suspected of being a sleeper terrorist. Disgusted, Johannes refuses. Johannes reports to his new boss, virologist Professor Behringer, who introduces him to Farid. Behringer, an authoritarian department head, wants the two young scientists to work on the same problem in molecular biology from different angles. Farid's approach is genomics, while Johannes is studying proteins. Through work, Farid and Johannes become friends \u2014 of sorts. And therein lies a tale of trust, ambition and betrayal. By the end of  Schl\u00e4fer  ( Sleeper ), a many-layered and earnest film selected for the 2005 Cannes film festival and now playing in European cinemas, Johannes will have informed on Farid. In doing so, he will have benefited his own career and may have won the girl. Critics have praised this award-winning first feature by a graduate of the Munich Film School. As an art-house film addressing big moral issues, it will not be to everyone's taste. But for an audience of scientists, three things will be remarkable. First, that the director chose to set a study of post-9/11 paranoia in a molecular-biology laboratory. Second, that the laboratory setting, and the interactions between the scientists, are unusually realistic \u2014 even though the plot itself has nothing to do with science. And third, that the director is the grandson of Werner Heisenberg, 1932 Nobel laureate and originator of the uncertainty principle in quantum mechanics. Benjamin Heisenberg was born in 1974, about a year before his grandfather died, but he is keenly aware of his forebear's legacy. The German physicist was criticized for working for the Nazi nuclear programme during the Second World War. But Benjamin suspends judgement on Werner's decision to stay in Germany, asking: \u201cHow can anyone ever be sure that things turn out the way they expect and want, and that other generations won't judge their decisions differently?\u201d Werner was not the only high-achiever in the family. Benjamin's father Martin, a professor at the University of W\u00fcrzburg, is one of the most highly cited behavioural scientists in Germany. His maternal grandmother was sister to Carl Friedrich von Weizs\u00e4cker, physicist turned philosopher, best known for his theories of the nuclear processes inside stars, and to Richard von Weizs\u00e4cker, the popular former German president. Two of his brothers studied sciences. But Benjamin's interests took him to art school, where his interest in film awakened. Film school was disappointingly superficial. \u201cThe philosophy of the school was to learn craft, not art,\u201d he says, recalling the scepticism some teachers had of his earlier short films. \u201cTheir film history started with  Terminator II  and they told me \u2018oh no, don't do art, no one will watch it; it's just a mindfuck\u2019.\u201d He laughs at his own audacity and earnestness. The idea for  Schl\u00e4fer  came to Heisenberg shortly after 11 September 2001. \u201cI saw that domestic security was being tightened and that no one was objecting,\u201d he says. \u201cThere would have been mass demos if politicians had tried to pass even a part of the new security laws ten years ago.\u201d Fear, he noted, was changing people's politics. It led him to wonder: how do fear and politics affect personal lives and relationships? How much does it take to weaken someone's moral convictions? Growing up among scientists, Benjamin knew that the scientific environment had the dramatic potential for examining these questions. Scientists are dedicated, driven and sometimes ruthlessly competitive. Scientific conflicts could illustrate how much pressure \u2014 or how little \u2014 it takes to break a person's ethical spine. The international nature of science was also important to the plot, because an Arabic researcher would not be so unusual. Benjamin created a potentially explosive, but plausible, mix of characters: a ruthlessly ambitious principal investigator; and two equally ambitious, inexperienced young researchers who collaborate, yet compete with each other. Two events test their friendship and collegiality. First, they fall in love with the same girl, Beate. When she chooses Farid, Johannes makes contact with the secret service agent he had previously spurned. But when Farid turns moody on Beate \u2014 he has learnt that he is being watched, but doesn't know by whom \u2014 she takes temporary comfort with Johannes. Uplifted by his romantic triumph, Johannes tells the secret service he will no longer speak with them. Then a lab drama sets things on their final, fateful course. One day Farid barges excitedly into Johannes' lab: \u201c Nature  has said \u2018yes\u2019!\u201d Suppressing his annoyance that he didn't know a paper had been submitted, Johannes is thrilled. He had, after all, helped Farid's project by reanalysing his sequence data and identifying two overlooked genes that were key to solving their biological puzzle. They celebrate. But then he learns that the manipulative Behringer has excluded him from the author list. His rage is ignited, with chilling consequences for Farid. After Farid is arrested for suspected involvement in a failed Munich bombing, Behringer replaces Farid's name with Johannes' on the  Nature  paper. We are never told the details of the research project. \u201cPeople would stop listening \u2014 they only need to know enough to understand why a conflict has developed, and glimpse its complexity,\u201d says Benjamin. But any milieu has to be convincing to make viewers believe in the film. This is why Benjamin took pains to ensure the science throughout was as real as possible. Enter Katarina zu Eulenburg, Benjamin's cousin, and an immunology PhD student at Berlin's Humboldt University. She read the script and hosted the actors in her lab, teaching them how to pipette liquids and handle animals. She was on set, ensuring that in each lab scene the actors and extras were working appropriately. She even provided her gloved hands for close-ups of detailed procedures. \u201cIt was such fun \u2014 and I realized how similar the procedures of film-making are to doing a research project,\u201d she says. \u201cIt is hard and disciplined work which you really have to believe in, because it takes such a long time to get the idea, write the script, find the financing and then make the film.\u201d Her and Benjamin's efforts were rewarded when  Schl\u00e4fer  won the 2005 Midas Prize for the best European drama featuring science. But after seeing the film, Martin Heisenberg told his son that no one \u2014 not even a scientist as ruthless as Behringer \u2014 can swap names on a paper already accepted by  Nature . But Benjamin didn't reshoot. \u201cThe dramatic moment was too important to the plot, and I was sure that one incorrect detail would not disturb the realism of the lab scenes.\u201d His father suspects he was the source of Benjamin's view of the importance of a  Nature  paper, something he finds a little embarrassing: \u201cI always tell my students that these things shouldn't matter so much.\u201d And Benjamin says he himself has only experienced labs with very positive atmospheres. \u201cMy portrayal of a pushy lab, whose competitive atmosphere became poisonous, comes from what other scientists tell me exists.\u201d In  Schl\u00e4fer , this poisonous atmosphere leads Johannes to betray Farid, a decision we condemn but understand. His betrayal is foreshadowed by a scene in which Johannes kills a lab mouse. \u201cWe see a parallel in Johannes' ability to coldly kill an animal, even though we know from other scenes that he is a caring and sensitive person,\u201d says Benjamin. \u201cKilling an animal for the greater good of science requires a decision to cross an ethical border \u2014 and then you just do it without thinking about the ethics every day.\u201d Benjamin imagines something similar must happen when someone decides to spy for the secret service. Would the German secret service really recruit scientists in a public research lab? It's not out of the question, says Martin Heisenberg, who witnessed an east German spy in his former lab at the institute for virology at the University of T\u00fcbingen before German reunification. \u201cHe regularly went through notebooks and transmitted information.\u201d Alexander Kekul\u00e9, a microbiologist at the University of Halle, Germany, says that spying on a suspected terrorist in a German lab today is entirely conceivable. \u201cThere have been recent scandals in Germany about the BND [secret service] getting journalists to spy on their colleagues \u2014 there would be even fewer scruples in spying in other professions, including science.\u201d With  Schl\u00e4fer , Benjamin says he has got the big moral issues out of his system, at least temporarily. His next film is about a bank robber in Vienna. But he knows he will come back to science in the future. \u201cMostly science in films is characterized by the classical mad professor, or someone running around saying \u2018Oh my God! The organ emulator is running at 400%!\u2019 \u2014 with no one having a clue what that could mean.\u201d But film-makers are becoming much more aware of the dramatic possibilities of science, he says: \u201cThe time of science in films is coming.\u201d The DVD of    Schl\u00e4fer    with English subtitles is available from September. It can be bought from  \n                     http://www.filmgalerie451.de \n                   . \n                     Science at the movies: The fabulous fish guy \n                   \n                     Science in the movies: Hollywood or bust \n                   \n                     A Scanner Darkly \n                   \n                     Waking Life \n                   \n                     Raymond Mar \n                   Reprints and Permissions"},
{"file_id": "442022a", "url": "https://www.nature.com/articles/442022a", "year": 2006, "authors": [{"name": "Sarah Tomlin"}], "parsed_as_year": "2006_or_before", "body": "An economist believes that a five-year aid effort in a dozen villages across Africa can teach the world how to defeat poverty. Sarah Tomlin reports on the project's progress in Rwanda. Celestin Ndahayo smiles broadly at me from below the corn (maize) that towers a metre or more above him, his daughter Annalita clutching his hand. This is the first corn harvest he has seen here in almost ten years. There was a smaller harvest of beans and sorghum in 2001; last year there was nothing. In the years without good rains, the people of Kagenge (sometimes called Mayange) in Rwanda survive the best they can. Some walk four nights and three days to reach a more productive region. Ndahayo sometimes takes construction jobs to support his wife and four children. Hyacinthe Mukaritaganda's husband is one of those currently working elsewhere, leaving her to manage their land and look after three children on her own. This season she planted corn on one-fifth of their land, a short walk from Ndahayo's homestead. Thanks to the rains, she is expecting a good harvest, which should provide enough seeds to plant all 2.5 hectares next year. Then, she hopes, her husband will stay at home to help. The rains, though, are not the only things bringing hope to Kagenge. In 2005, the village was chosen to take part in the Millennium Villages project. Led by the Earth Institute at Columbia University in New York, the project is applying a range of poverty-slashing interventions to 12 sites across Africa (see map). The idea is not just to show that interventions in a number of different areas, properly coordinated and financed, can make a sustainable change to the lives of the world's poorest communities. It is to show how that can be done quickly in a way that can be replicated easily. Donald Ndahiro, an agronomist trained in Uganda, is the project's agriculture coordinator for Rwanda. He says that when he arrived in Kagenge late last year conditions were desperate. \u201cThe villagers were emaciated.\u201d They wanted food aid more than they wanted the agricultural advice, drought-resistant seeds, fertilizer and new techniques that the project was offering. \u201cThey thought we were making fun of them,\u201d Ndahiro says. \u201cWe were telling them how to plant, how to harvest, but they were saying they were never getting any good rains. We told them to get organized.\u201d Five months later and the villagers are getting organized. Ndahayo is a member of the agriculture committee that will decide what to do with the surplus from this year's corn harvest. Mukaritaganda is helping to clear land for a tree nursery (villagers sometimes walk ten kilometres to gather firewood) and was part of the team that just built a communal tank to collect rainwater. She invites me with pride to a ceremony in which certificates are awarded to her and the 25 other villagers who worked on the tank. \n               Leading the way \n             The Millennium Villages project aims to provide improved resources and techniques not only in agriculture, but also in health, education, transport, energy and water provision, and financial management. The plan is to achieve the United Nations' Millennium Development Goals (see  box ) for the 5,000 or so people in Kagenge, and for the tens of thousands of people in the 11 villages elsewhere within 5 years \u2014 5 years ahead of the UN target date. The eight goals, committed to by 189 heads of state in 2000, include halving the number of people living on less than US$1 a day and controlling malaria by 2015. Progress so far has been limited, especially in Africa \u2014 far too slow for the impatient economist Jeffrey Sachs, head of the UN Millennium Project and the Earth Institute. Sachs wants the 'research villages' and the data that they provide to offer ways of picking up the pace: \u201cThe idea is to demonstrate a practical path and to mobilize governments.\u201d The man in charge of making such a demonstration is Josh Ruxin, a Columbia University public-health expert and the project's director in Rwanda. Ruxin, imbued with an impressive energy and passion, was initially sceptical of the village-by-village approach: he wanted to target a millennium country not an isolated village. But Ruxin is encouraged by the Rwandan government's own ambitious poverty-reduction strategy, known as Vision 2020. Ben Karenzi, the Rwandan health ministry's secretary general says \u201cWe believe it's possible, especially with the focused leadership we have and the commitment of our people, to make Rwanda a mid-level income country by 2020.\u201d In the context of that commitment, Ruxin is confident that with the help of the Millennium Villages project, Rwandans can succeed in not just turning round one village, but in transforming life for poor farmers across the country. \n               Money cares \n             Part of Ruxin's confidence comes from an assessment of the government. In the aftermath of the genocide of 1994 and the resettlement of some two million returnees from neighbouring countries, 64% of Rwanda's population was living in poverty (on less than US$1 a day) in 2000. But despite its internationally criticized role in the Congo war, the government of Paul Kagame is widely seen as committed to poverty reduction, and as embodying principles of good governance from the top down (for example, all ministers are required to declare their annual income). Ruxin believes that good governance will be an important factor in the long-term success of the millennium villages. Those running the project have deliberately avoided what they see as the worst African regimes. But they say that even in corruption-prone nations, such as Ethiopia and Kenya, the research villages so far remain free of corruption. Sachs points out that if you focus on supplying commodities, such as seeds, fertilizer and nets for protection against malaria, \u201cthere's very little money that changes hands\u201d. That said, Sachs is less worried than many about corruption; he knows people criticize this lack of concern, but doesn't care. \u201cCorruption is way down the list of practical issues,\u201d he argues, Africa's miserable roads, poor soil and endemic disease burden are at the top. Indeed, the road from Kigali, Rwanda's capital, to Kagenge in the Nyamata district, throws up choking red dust in the dry season and can be impassable in the rainy season. Although the land looks green from the air, the rains can be infrequent and Ndahiro confirms that the soils are poor. Some 70% of the patients at the village's clinic have malaria and the district has one of the country's highest levels of HIV, at about 13%. As well as suffering from Sachs's top three problems, Kagenge has its own particular sadnesses. The local mayor, Gaspard Musonera, lost three-quarters of his family in the 1994 genocide. \u201cNyamata district lost more than half of its population,\u201d he says. \u201cThe implications and consequences of that you can imagine for yourself.\u201d Kagenge itself is a community created since the genocide. Half the households live in settlement housing \u2014 or  umudugudu  \u2014 built by the government for survivors and returnees. Ndahiro is himself a returnee, living in Nyamata near the church where 10,000 people were murdered in 1994 \u2014 the blood stains on the walls and altar cloth remain as a memorial. He recalls how lifeless the town was when he arrived in 1997. People were bitter, he says; some didn't want to continue living. \n               Grand plan \n             Musonera sees the Millennium Villages project as a sign of hope for the most vulnerable people in his district, and a big test for poverty-reduction measures. \u201cIf it can be done here, it means it can be done elsewhere,\u201d he says \u2014 and that indeed is the point. The project is not just about breaking the cycle of poverty in 12 villages, but about learning how to do it in 1,200 or 12,000. Sachs's plan is to show that with a five-year investment of about US$550 per person \u2014 $50 a year from the project, $30 from government, $20 from other donors and $10 from the villagers \u2014 an integrated package of low-cost interventions can produce long-term financial sustainability in a way that not only can be repeated but can also be scaled up. The project plans to grow to 78 villages this year by creating clusters around the 12 original research villages. The expansion is being funded by the US Millennium Promise charity, which has so far raised $100 million to support Sachs's vision. Not everyone is convinced that the Millennium Villages project will succeed. Ecologist Ian Scoones at the Institute of Development Studies at the University of Sussex in Brighton, UK, is a member of the Future Agricultures Consortium, which was put together by the UK Department for International Development to focus on African agriculture and development. Scoones points to the Integrated Rural Development and 'villagization' schemes that tried to boost African agriculture in the 1970s and 1980s. \u201cThey created little islands of success but when donors pulled the plug they all collapsed.\u201d Scoones says he is very pleased that the millennium villages are putting African agriculture back on the map, but he is afraid of old mistakes being repeated, and worried about things moving too quickly. \u201cIndia launched its green revolution in the mid-1960s on the back of decades of solid investment and research,\u201d he points out. \u201cIt didn't happen overnight.\u201d For his part, Sachs sees patience, like a well-developed sensitivity to the issue of corruption, as an overrated virtue. He has no worries about moving too fast. \u201cIt happens to be an emergency,\u201d he says. And he has no illusions about the projects working as examples simply by word of mouth. \u201cThis is not viral. You can't do it without resources,\u201d Sachs notes \u2014 as ambitions grow, so must spending. The biggest risk, he says, is for official donors to sit on their hands. The goal is not to do without large transfers of money to Africa, it is to work out how to make those transfers more effective. After all, the individual interventions used in the Millennium Villages project are tried and tested methods, even if they haven't been applied all together in one location before. Asked about the target of reaching the millennium goals in five years, Celina Schocken, an international-affairs fellow at the US Council on Foreign Relations, says \u201cI absolutely believe they will succeed. I don't see how they can't.\u201d But she's less convinced about how scale up will be achieved. \u201cWhat good is an island of prosperity anyway?\u201d she asks. Scoones agrees that the big question is: \u201cHow, without that external support, do you replicate?\u201d That is the question Sachs, Ruxin and their colleagues are trying to answer. By documenting all the inputs and outputs for each research village they hope to tease out the synergies between overlapping interventions. Measuring 35 indicators for the 8 goals across several hundred households in 12 villages is time consuming and costly, but it is necessary to show not just that the investments work, but also how they work, and how they can work better. Only then can they be scaled up to the truly monumental level envisioned by Sachs, who wants to see development aid change the course of history. \u201cI think the biggest challenge is the defeatist attitude of the official donor community,\u201d says Sachs. Such rhetoric reinforces the suspicion that Sachs is unwilling to learn from lessons of the past. \u201cPeople in the development community see some benefit in the publicity Jeff Sachs gets,\u201d says Schocken, who used to work with Ruxin in Rwanda, \u201cbut they've seen these ideas before.\u201d \n               Skill shortage \n             In Kagenge, the villagers assembled for the water-tank certificate ceremony are briefly reminded of the international debate over their future. \u201cThis is an important day for the project,\u201d Ruxin tells them during a short address, \u201cYou are now the teachers for us and for the world.\u201d Some of the farmers I met in the fields yesterday have donned suits and ties for the occasion. Each villager who received training from visiting Kenyan water specialists receives a signed certificate \u2014 the expectation is that they will take the skills they have learned and pass them on to others. After many more speeches by village leaders, the villagers distribute soft drinks and, for those who can stomach it, fermented sorghum, the local brew. In the weeks before the corn is harvested, the contrast between Kagenge and the surrounding area is already striking. An emergency feeding centre supported by the UN Children's Fund UNICEF and the World Food Programme was set up in Kagenge in early March, in response to reports of serious malnutrition following last year's drought. Four hundred people from the wider local population are still receiving weekly rations, but not those of Kagenge. The bean crop and corn picked straight from the fields before the harvest mean that they have enough to eat. They also have a functioning health centre, which serves Kagenge and four neighbouring communities of similar size. The centre now has its own doctor, Angelique Kanyange, known to everyone as Dr Angelique, and its nursing staff has doubled in number. Dr Angelique is zealously improving the nurses' cleaning procedures with demonstrations of the use of a broom and disinfectant. Today, the clinic is seeing more than 35 patients a day, as well as some 50 mothers bringing children for immunization. One of the new patients is Musabyimana, an 8-year-old boy who is blind and in pain because of severe cataracts. His mother noticed his poor sight when he was three months old, but this is his first visit to the clinic. Dr Angelique is not sure what caused the cataracts, but there is hope, she says, because Musabyimana seems to be able to detect some light and colour. She will refer him to a specialist for treatment. For now, the project will fund it; the mother, a widow, could never afford it. Rural parts of Rwanda have community medical insurance schemes, but only 12% of families in Kagenge have cover. The goal of the Millennium Villages project is to get 100% coverage, with the hope that as the clinic becomes more useful to patients, more will join the scheme. \n               From village to province \n             It is here, though, that scaling up looks harder than it does in agriculture. Buying more fertilizers is easier than making more doctors. \u201cAngeliques are hard to come by,\u201d admits Ruxin. Indeed, Dr Angelique is the first government-appointed doctor in a rural health centre, demonstrating the government's commitment to the Millennium Project but also, perhaps, the project's weakness. \u201cThe president of Rwanda says 'I want this village to work', so they are going to get the best,\u201d says Schocken. There are currently about 200 doctors in the country. The medical schools may be able to produce 60 or 80 a year, but the country has a long way to go to reach the World Health Organization's minimum recommended level of one doctor per 5,000 people. Sachs claims recruitment problems can be overcome with decent salaries. Although the project is mostly about spending money on physical resources, he is in favour of top-up payments for doctors. But even with targeted salary increases, a country such as Rwanda suffers skill shortages in every sector. Kagenge currently has a team of ten dedicated people who work long hours to motivate the villagers and document their progress. Detailed accounts were not made available to  Nature , but in its first year the Kagenge project will spend as much on personnel as on materials. The budget for the cluster villages being set up in addition to the original 12 is smaller, and they will have much fewer support staff. \u201cResearch on top is extremely expensive,\u201d notes Ruxin, explaining that in future, and in villages that aren't research focused, costs should be much lower. But Sachs's claim that \u201cthe science behind this is broadly transferable without needing large teams\u201d has yet to be put to hard tests. The budgets matter to Theoneste Mutsindashyaka, a former mayor of Kigali and the governor of Rwanda's eastern province, which includes Nyamata and covers a quarter of Rwanda's population. He is a great fan of the Kagenge project, in part because it fits so well with the government's Vision 2020. He wants the figures so that he can roll out projects informed by the experience more widely. \u201cThe documentation is very important to me because I have to negotiate with partners,\u201d he says. He is impatient to get moving on the next stage of the project: \u201cWe want a millennium province, not just a millennium village,\u201d he says. Within the next year, Mutsindashyaka wants to set up a Kagenge-like village in each of his provinces' seven districts. \u201cWe are going to move village to sector, sector to district, but you have to have money,\u201d he says. And he is certain he can sell the idea to his friends all over the world, from Quincy Jones to Donald Kaberuka, the president of the African Development Bank. And although scaling up to 3,600 villages is daunting, the governor says he only needs the numbers from Kagenge to get started: \u201cI am total 100% confident that the project will succeed.\u201d From Sachs to the president to the governor to the mayor, the ambitions for transforming the country are vast. But in Kagenge, despite the good rains, the villagers themselves remain wary. They are not as confident that they will achieve rapid progress as the project leaders. Anxiety about what to do with the harvest surplus is high. Celestin Ndahayo and other farmers worry about whether they can really afford both to sell corn and store enough for food security; they are not sure they believe Ndahiro's forecasts for the yields of their smallholdings. And what if the rains don't come next year? In his experience, says one  umudugudu  farmer, when a project is here, then the rains come. Back in 2001, an organization helped them to plant cassava and sweet potato and the rains came. But when they left the rains stopped. So as long as the Millennium Villages project is here he believes it will rain again. He doesn't believe, yet, that his village can learn to flourish in the project's absence. \n                     Heavyweights act to tackle the 'big three' diseases \n                   \n                     Can we score the millennium goals? \n                   \n                     Science & Africa: A message to the G8 summit \n                   \n                     Africa 2005 \n                   \n                     MVP \n                   \n                     Millennium Promise page for MAYANGE \n                   \n                     MDG indicators \n                   \n                     Rwandan Ministry of Health \n                   \n                     Future Agricultures website \n                   Reprints and Permissions"},
{"file_id": "442128a", "url": "https://www.nature.com/articles/442128a", "year": 2006, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "Plate tectonics has created oceans and pushed up mountain ranges. But when did the process that shapes the planet get going? Alexandra Witze joins the geologists debating the issue. On a sunny, breezy day in the Wind River Mountains of Wyoming, a group of geologists are peering intently at a dark ridge of rock. Some 2.7 billion years ago, these rocks were alive with volcanic fire. Today, they jut out of a mountainside like the spiny tail of a sleeping dragon. This rock, says Kevin Chamberlain, a geologist from the University of Wyoming in Laramie, could be a special kind \u2014 a lava called a komatiite. Today, Earth's interior is too cool to produce this particular rock; 2.7 billion years ago, the hot lava would have run like water over the barren landscapes. But as the other geologists chip off fresh layers and scrutinize them through hand lenses, murmurs of dissent start to grow. Most geologists have never seen a komatiite; they are found almost exclusively among rocks of the Archaean era, which are more than 2.5 billion years old and thus very rare. But these men and women are experts in the truly old. And on the Wyoming hillside few of them are convinced that they are seeing the rock textures typically found in komatiites. The scene brings home the difficulties of trying to study the early Earth \u2014 there aren\u2019t many old rocks to look at, and those that are around are often so altered, chemically and physically, as to be nearly indecipherable. But as if to recompense those who study them, such ancient rocks, particularly of Archaean age, offer geologists great rewards. It is in the Archaean that the first earthly ecosystems are found, with their clues to life's earliest days on the planet. And it is in the Archaean that scientists can look for the beginnings of plate tectonics. Plate tectonics is the grand unified theory of geology. Everything we see today, from the abyssal plains of the oceans to the heights of the Himalayas, is shaped by plate tectonics. As far back as there has been complex life \u2014 and perhaps even before \u2014 continents have come together and moved apart in a dance that has altered climates and geographies, opening up new possibilities for life and sometimes closing down old ones. But it may not always have been so. Plate tectonics is driven by Earth's heat and constrained by the physical and chemical properties of the crust and mantle. The further back in time you go, the more different these things are likely to have been. It's been argued that on the early Earth, crustal plates, floating on a heat-softened layer of material beneath, would have simply been too thick and buoyant to get dragged beneath each other as they are today. And the greater temperature of the early Earth's innards would probably have made them move in very different patterns from those typical of today's tectonics. On other Earth-like planets there's no evidence for today's plate tectonics. Planets do not have to work this way, and there was probably a time when this one didn't. \u201cYou don't just make a silicate planet and plate tectonics starts,\u201d says Robert Stern, a geologist at the University of Texas, Dallas. \u201cSomething special has to happen.\u201d \n               Dynamic planet \n             The nature of that special something cuts to the discipline's philosophical heart. Since the early nineteenth century, geology has been ruled by the principle of uniformitarianism \u2014 that the planet operates on unchanging laws, and that the present can be used as a key to the past. But how can that approach hold up when a science from a world where plate tectonics explains more or less everything is applied to a world that may have lacked it? How can you understand ancient rocks when you do not know what processes put them there? The geologists clustered around the possible komatiites in the Wyoming hills had gathered to discuss these questions. Their visit to the mountain ridge had been organized as part of a June conference held in Lander, Wyoming. The specific aim of the meeting was to try to fix a date for the onset of plate tectonics: the earliest possibility is pretty much straight after the planet formed, about 4.5 billion years ago; the latest is just 1 billion years ago. To help them decide, the scientists brought to the table data from an array of disciplines. Geochemistry can help clarify the temperature and pressure at which Archaean rocks formed. Fragments of zircon crystals dated even earlier \u2014 to the Hadean eon, which stretches from about 3.8 billion years ago to the planet's birth \u2014 can provide hints about what Earth's surface environment was like back then. Palaeomagnetic studies can show how land masses moved across latitudes. And structural geology can identify features that, in today's world at least, seem to be indicative of plate tectonics. But in all these approaches, as with the komatiites, age makes the picture hard to discern. \n               Crashing continents \n             Scant and difficult-to-interpret evidence presents one set of problems; slippery definitions present another. Plate tectonics has lots of constituent parts. It's not just a theory of how things move, but of how they are made and from what. For example, explanations for different sorts of volcanism in different settings also explain why the mineral make-up of continental crust and the crust beneath the oceans is so different. Working out which attributes are essential to the theory, and which incidental, is not easy. The 65 attendees at the Wyoming conference came up with 18 different definitions of plate tectonics. Three components, most agreed, were key: there must be rigid plates at the surface of the Earth; those plates must move apart through ocean spreading, with new crust being made where the sea floor pulls apart; and the plates must on occasion dive beneath each other at subduction zones (see graphic  Fig1 ). The problem is that Earth could display one or even two of these properties without necessarily having a system like that described by modern plate tectonics (see  'A world without tectonics' ). Take rigid plates. Palaeomagnetic and other studies show that sections of Earth's crust moved relative to each other in the Archaean, just as modern crustal plates do. But ice floes on a polar sea move in the same way, points out geophysicist Don Anderson of the California Institute of Technology in Pasadena \u2014 and those ice floes aren't experiencing plate tectonics. Of the three, it seems subduction is closest to being diagnostic of plate tectonics. Subduction is the process by which one crustal plate slips beneath another, to be recycled into the mantle. Subduction requires rigid plates, and as it involves the destruction of crust, new crust must be created elsewhere, presumably at oceanic spreading ridges (see graphic  Fig1 ); otherwise, continental crust would eventually disappear. Some argue that this means plate tectonics should date further back than the earliest firm evidence for subduction. A dramatic use of this argument is that made by Stern. In a paper published last year, he took an extreme position, proposing that Earth has been free of plate tectonics for almost four-fifths of its life, with the system we see today starting up only a billion years ago 1 . He had two lines of argument. One was that plate tectonics could not begin until Earth's crust was cool enough, and that barrier was only passed about a billion years ago. The other was that the only reliable evidence for subduction on the early planet came from a period more recent than that. Stern points to the geological record of three types of rock. Ophiolites are distinctive sections of the ocean crust that gets mashed up, often through subduction, on the edges of continents. Stern argues that very few of these rocks are more than a billion years old. Metamorphic rocks called blueschists, produced by squeezing the basalt from which oceanic crust is made at high pressures but not very high temperatures, are being made in today's subduction zones; none, Stern says, has been found that is more than 800 million years old. And rocks from 'ultra-high-pressure terranes' of the sort produced where one plate rides over another are at most 630 million years old. He also makes a more general point. A dramatic shift, such as the introduction of plate tectonics, must have had huge planetary consequences. And between 780 million and 580 million years ago, Stern says, there was a series of glaciations, some very extreme \u2014 giving rise to the term 'snowball Earth'. \u201cIt was a wild time of change,\u201d says Stern. \u201cThe biosphere was out of control.\u201d On the basis that dramatic effects require dramatic causes, he argues that the introduction of plate tectonics, and with it an increase in planet-cooling volcanic eruptions, might have precipitated the great glaciations. \n               An age gone by \n             After reading Stern's arguments, Alfred Kr\u00f6ner of the University of Mainz in Germany fired off a rebuttal. He argues that there's plenty of evidence for plate tectonics stretching back at least 3.1 billion years 2  \u2014 including geochemical work, seismic images of the 'sutures' where colliding continents join and, indeed, a few ancient ophiolites. \u201cI believe we can see these features all the way back\u201d \u2014 possibly all through the Archaean, says Kr\u00f6ner. The exchange of papers led to the Wyoming conference. \u201cIt was overdue,\u201d says Kr\u00f6ner. \u201cNobody ever talks to one another.\u201d In Wyoming, they did: palaeomagnetists clustered around a white board with field geologists; geophysicists sat down for a beer with geochemists. Some of the newly shared data favour an early start for plate tectonics. Geoff Davies, a modeller at Australian National University in Canberra, presented work suggesting that one of the biggest stumbling blocks to an early start may have been removed. In the early 1990s, computer models created by Davies and others suggested that the crust on the early Earth would have been too thick and buoyant to get dragged down beneath another plate during subduction. But new simulations, using more sophisticated calculations, suggest that the crust may have been thinner than once thought 3  \u2014 as thin as 4 kilometres or less \u2014 which would be thinner than today's crust. \u201cMaybe plate tectonics on the early Earth was viable after all,\u201d says Davies. In other cases, recent findings overturned evidence for early plate tectonics. In 2001, a team reported that an ophiolite from Dongwanzi, China, was 2.5 billion years old \u2014 making it by far the oldest such subduction remnant yet discovered 4 . Now Guochun Zhao, of the University of Hong Kong, has re-dated those rocks, giving them an age of just 300 million years. Timothy Kusky of St Louis University in Missouri, who led the original study, says that Zhao took samples from a part of the rock already known to be much younger than the main part of the ophiolite. But several attendees at the meeting said they found Zhao's data convincing. If true, it would pull the earliest evidence for ophiolites at least half a billion years towards the present, leaving the Archaean an ophiolite-free zone. The Chinese ophiolite isn't the only evidence that is getting fresh scrutiny. For a while two independent groups have been quietly warring over the significance of a pile of ancient zircons from the Jack Hills region of Western Australia. The zircons are crystals that formed in the Hadean and later became incorporated into younger rocks. Last year in  Science 5 , geochemist Mark Harrison of the University of California, Los Angeles, and colleagues used the Jack Hills zircons to argue that continental crust was present 4.4 billion to 4.5 billion years ago. The evidence comes in the form of hafnium isotope ratios in the zircon crystals, which preserve signals of the lighter minerals typical of continental crust. The data also suggest, Harrison argues, that that crust was being recycled down into the mantle by 4.4 billion years ago \u2014 perhaps though a process similar to plate tectonics. Simon Wilde of the Curtin University of Technology in Perth, Australia, isn't so sure. \u201cYou have to be very careful with these rocks,\u201d he says. Measuring one spot on a crystal, as opposed to another, can yield very different hafnium values that lead to very different interpretations, he says. Wilde argues that the zircons should be interpreted more conservatively \u2014 that the evidence points to there being some continental crust, but not plate tectonics and its associated recycling, by 4.4 billion years ago 6 . \n               Ground forces \n             Such differences of interpretation make the problem of solving when plate tectonics began extremely difficult. In many cases, data can be interpreted in several completely different ways \u2014 all of which may seem valid. For instance, another Australian geologist presented seemingly convincing evidence that plate tectonics had begun by 3.3 billion years ago in Western Australia, based on the very different histories of two sections of an ancient rock formation called the Pilbara. Hugh Smithies of the Geological Survey of Western Australia says that the eastern part of the Pilbara, between 3.5 billion and 3.2 billion years old, \u201cshows no clear evidence for modern-style plate tectonics\u201d. It contains some geochemical markers that suggest subduction, but they could just as easily be explained by hot upwellings of rock known as mantle plumes or other non-tectonic phenomena. In contrast, looking at the western part of the Pilbara \u2014 which is 3.3 billion to 3.0 billion years old \u2014 Smithies sees plenty of evidence for plate tectonics. There are geochemical signatures that cannot be explained by other factors, and the rocks show features that hint that plates had interacted along their edges. Smithies thinks the western Pilbara contains the remains of an oceanic arc \u2014 the sort of line of islands, such as the Aleutians of Alaska, that are characteristic of some oceanic subduction zones 7 . But then along came Julian Pearce of Cardiff University in Wales, who argued that each of the geochemical markers in the western Pilbara can be explained by other phenomena, such as magmas with an unusual amount of water in them, or crustal material from different places getting mixed up. The various researchers are hoping to settle the matter with a field trip. An excursion is already planned for next year, to re-examine the evidence for plate tectonics in the western Pilbara. Field trips don't always resolve things. In the Wind River Mountains, the meeting attendees continued to argue about plate tectonics as they hiked from outcrop to outcrop. But a week of communing at the conference and under the high mountain sun brought them toward a consensus of sorts. Meeting organizers polled the attendees twice on when they thought plate tectonics began. At the beginning of the meeting, guesses were spread over more than 3 billion years of Earth history. At the end, a closing ballot showed that many had begun to push their thinking further back into the past; a majority of attendees voted for plate tectonics having started between 3 billion and 4 billion years ago. Kent Condie, one of the meeting organizers, calls that a success. \u201cWe've got a majority favouring a definition and approach,\u201d says Condie, of the New Mexico Institute of Mining and Technology in Socorro, New Mexico. \u201cSure, there will be a minority point of view.\u201d At the conference, that minority pretty much constituted Stern. By the end of the meeting, he remained the one person voting for a start to plate tectonics at 1 billion years ago. \u201cIt's not a simple question,\u201d he maintains. And on that, at least, others agree. Michael Brown, a geologist at the university of Maryland in College Park, doesn't endorse Stern's late start. But he does think that the nature of plate tectonics changed around that time. In a paper in press in  Geology 8 , Brown suggests that there have been two styles of plate tectonics: the modern kind that we see today, and an earlier version that lasted from about 2.7 billion to 700 million years ago. Evidence for the earlier style, he says, comes from minerals that are typical of higher-temperature, lower-pressure environments; these suggest a hotter Earth where plates did not subduct beneath each other to great depths and pressures. Minerals characteristic of high-pressure environments typify the later style. The properties of these minerals suggest to him that true plate tectonics, in which one plate subducts deeply beneath another, did not begin until about 700 million years ago. And there is a possible further complication. Geophysicist Paul Silver, of the Carnegie Institution of Washington, raised the notion that plate tectonics may have started and stopped several times during Earth's history. This is also an idea that Stern is comfortable with \u2014 he uses it to explain the presence of a small number of ophiolites about 2 billion years ago. An 'intermittent approach' would be a wonderful way to reconcile things \u2014 but it takes geology even further from the comforting realm of uniformitarianism, into a world where the most basic principles come and go in fits and starts. \n                     Southern California due major earthquake \n                   \n                     Tokyo's fault line just below the surface \n                   \n                     Triple slip of tectonic plates caused seafloor surge \n                   \n                     Penrose conference website \n                   Reprints and Permissions"},
{"file_id": "442125a", "url": "https://www.nature.com/articles/442125a", "year": 2006, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Implants in the brain could one day help paralysed people move robotic arms and legs. But first, scientists need to work out how our brains know where our limbs are, says Alison Abbott. Shut your eyes. Now, touch your nose. Chances are you can do this without even thinking about it. For this you can thank your sense of proprioception, which is so much a part of us that most of us are unaware that it exists. This 'sixth sense' lets our brain know the relative positions in space of different parts of our bodies. Without it, our brains are lost. Ian Waterman knows how that loss feels. More than 30 years ago he lost this sense almost overnight, when a flu-like virus damaged the required sensory nerves. His muscles worked perfectly, but he could not control them. \u201cI lost ownership of my body,\u201d he says. He could no longer stand, or even sit up by himself, and doctors said he would never be able to do so again. Waterman's condition arose from a disease called acute sensory neuropathy and is so rare that only a dozen or so similar cases are known to the medical literature. Some neuroscientists are taking a cue from Waterman's experiences and starting to investigate whether robotic devices controlled by thought alone could be integrated with an artificial sense of proprioception. If so, they reason, these 'neuroprosthetics' could be made to work in a much more life-like way. What's more, they hope to gain a deeper understanding of how proprioception works, and how they might be able to manipulate it. Some months after his virus attacked, Waterman, only 19 years old, was lying in bed applying all his mental energy to the fight for control of his body. He tensed his stomach muscles, lifted his head and stared down at the limbs that seemed no longer to belong to him. He willed himself to sit up. \n               Concentrated effort \n             Later, he realized that it was the visual feedback that allowed his body to unexpectedly obey the mental instruction. \u201cBut the euphoria of the moment made me lose concentration and I nearly fell out of bed,\u201d he remembers. From then on he learnt to compensate for his deficit in proprioception with other forms of sensory feedback to help him understand where his limbs are, and thus control them. It requires constant, intense concentration, but now, despite his profound impairments, he can manage fairly normal movements. Most of the input that he relies on is visual \u2014 standing up with his eyes closed is still nearly impossible \u2014 but he can also tune in to the tug of a jacket sleeve to work out the direction his arm is moving. Or to the cool air on his armpit when he raises his arm in a loose shirt. Neuroprosthetic engineers are realizing that many sensory feedback signals could be similarly harnessed. A neuroprosthetic is more accurately called a brain\u2013machine interface. Hundreds of electrodes, fixed into tiny arrays, are placed in or on the surface of the cortex, the thin, folded outer surface of the brain that controls complex functions including the organization of movement. The electrodes record the electrical signals from the cortex's neurons and these are translated by a computer algorithm and used to drive specific actions \u2014 the movement of a cursor on a computer screen, for example, or of an artificial limb. In this issue of  Nature , two papers 1 , 2 , 3  demonstrate dramatic progress in the area. A team consisting of John Donoghue's group, based at Brown University in Rhode Island and Cybernetics Neurotechnology Systems in Foxborough, Massachusetts, implanted 96 electrodes into Matt Nagle's motor cortex, the brain region that processes information about movement. Nagle is a quadriplegic patient and the first human volunteer to reach this advanced stage of testing (see picture, above). Hooked up to computers and attended by a team of technicians, Nagle could move a cursor to issue different instructions \u2014 for example, to open e-mails or turn down the television. Krishna Shenoy's group at Stanford University, California, has done similar work in a non-paralysed monkey's premotor cortex, the area of brain where the animal's movement-related 'intentions' are generated. Using a new algorithm, the team's brain\u2013computer interface produced results four times faster and more accurate than previously seen. \n               Closing the loop \n             The two papers show how closely neuroprosthetics are approaching medical reality. But although moving a computer cursor by thought alone may be dazzling, scientists have long-term ambitions to make neuroprosthetics reproduce more complex functions. Could patients direct a robotic arm to pick up a coffee cup, for example? \u201cFor this, the devices need to deliver feedback to the brain \u2014 we need to close the loop,\u201d says Daofen Chen, director of the neural prosthesis programme at the US National Institute of Neurological Disorders and Stroke in Bethesda, Maryland. The brain's sensory cortex receives signals \u2014 proprioception, touch, pain and so on \u2014 from the body (see graphic), and in response constantly modifies its movement-related commands. The current generation of output-only neuroprosthetics are open-loop systems \u2014 with more limitations than Ian Waterman, who can at least use visual, temperature and tactile feedback. \u201cBrain\u2013machine interfaces will have to become interactive,\u201d says Chen. \u201cBut now that we would like to exploit it, we realize we know next to nothing about sensory input.\u201d A handful of researchers is starting to try to work out where and how to stimulate the sensory nervous system to reproduce the sorts of information that a limb might send to the sensory cortex. It is early days: none of their work is published. And as so little is known about the system, there is no obvious place to start. Theoretically, the 'where' could be the nerves running from the limb into the spinal cord, or the spinal cord itself (see graphic). Or it could be higher \u2014 in the brain's thalamus, where incoming sensory signals are integrated and redirected to the appropriate part of the cortex, or the sensory cortex itself. The 'how' refers to the design of the electrical signals to be fed into the cortex. These could mimic the sensory system's natural nerve impulses, based on parameters such as frequency and amplitude. Or they could involve creating artificial signals that the sensory cortex is able to distinguish, in the hope that the brain can be trained to associate particular signals with particular parameters. Once scientists have worked out how best to encode the signals, the idea would be to place sensors on artificial limbs to generate signals representing proprioceptive information such as angle of joint, vibration, force of grip \u2014 and other sensory information that Waterman has found helpful, such as temperature. \n               Trained brain \n             Most in the field have a hunch that the signal will not have to mimic neural activity perfectly. The brain can, after all, cope with the very unphysiological signals generated by the most successful brain\u2013machine interface to date: the cochlear transplant. Already, some 110,000 profoundly deaf people have been implanted with the device, according to the US National Institutes of Health. The implant sits in the inner ear and interfaces with the auditory nerve. Its signals are totally artificial, and, at first, recipients can make nothing of the noise. But the auditory cortex, it turns out, is highly adaptable. With appropriate training, it can quickly learn to associate particular codes with particular sounds, so that transplant recipients can learn to follow conversations with ease. \u201cWhen the concept of stimulating the auditory nerve emerged in the 1970s, people said it would be impossible to generate the right electrical signal to the brain,\u201d says Shenoy. \u201cBut it turned out that you don't have to get it perfect, just close enough for the brain to do its own fine-tuning.\u201d On the other hand, one does not want to burden patients with having to learn too much, says John Chapin a physiologist at the State University of New York Health Science Center in Brooklyn, and a pioneer in using neural activity to control robots. \u201cIdeally we should aim to mimic the natural signal as closely as possible,\u201d he says (see  'Voyagers in the cortex' ). For now, whatever works will be good. \u201cWe don't know if it will turn out to be possible to incorporate sensory information but we are going to try,\u201d says neuroscientist Andrew Schwartz of the University of Pittsburgh, an expert in brain\u2013computer interface technology for the control of robotic arm movement. Schwartz is working with Douglas Weber, a bioengineer at Pittsburgh who is developing a model for studying sensory input. This involves using electrodes to stimulate the sensory nerves from the limbs of an anaesthetized cat at the point just before they enter the spinal cord, and simultaneously recording from neurons in the sensory cortex. Weber will then repeat the recording, only this time manually moving the cat's limbs, instead of stimulating their nerves with electrodes. He will then compare the pattern of neural activity in the cortex in the two situations and see whether he can mimic the patterns he sees in response to passive movements with artificial stimulation. \u201cNot everyone agrees, but my gut feeling is that we will be more successful if we stimulate outside of the central nervous system,\u201d says Weber. \u201cAt more central points there will be greater convergence of different inputs and I guess it would be hard to get clean signals\u201d. Lee Miller, a neurophysiologist from Northwestern University in Chicago, agrees that Weber could be right, but is nevertheless approaching the problem from the top. Although useful for study, stimulating nerves from peripheral areas of the body such as limbs will not work for a patient whose spinal cord is severed. Working in monkeys, Miller's group is electrically stimulating the part of the cortex that processes proprioception, and recording neuronal activity in the motor cortex at the same time. Miller hopes this will eventually let him design stimulation patterns that can imitate the brain's own processing of proprioceptive signals, much in the way that Weber is designing signals to imitate processing of movement. Monkeys will be trained to move a 'virtual' arm, created on a computer screen by computer algorithms fed by both recordings from the motor cortex and the simulated proprioceptive feedback. It's a complex experiment, he admits, which will probably take up to five years to get working optimally. But he draws hope from his group's finding, presented at the 2005 Society of Neuroscience meeting in Washington DC, that the monkeys can recognize and distinguish between high- and low-frequency stimulation. Chapin's set-up is equally ambitious. He also works on monkeys but his chosen target is the thalamus, the brain's junction box for sensory input. \u201cThe higher you go in the brain, the more complex and abstract things become,\u201d he says. \u201cIt is hard to know if you are stimulating something precise.\u201d In his experimental system, Chapin electrically stimulates the area of the thalamus that relays touch-related signals. Simultaneously he records in the areas of the sensory cortex that process tactile information. The monkeys meanwhile, have one arm strapped down and one free. They have been taught to point with their free hand to an area on their immobilized arm that they 'feel' is being touched. \u201cWe have found that we can produce a sort of 'natural response' in the cortex when we stimulate in the thalamus,\u201d he says. The response matches that produced normally when a specific part of the monkey's arm is touched. Chapin plans to extend his investigations to study proprioception in the same way. The papers on brain-machine interfaces by Donoghue and Shenoy 1 , 2  seem like science fiction becoming reality. The next step \u2014 trying to introduce sensory input into brain\u2013 machine interfaces \u2014 may appear at first glance to be as fanciful as the  Six Million Dollar Man . But few neuroscientists could seriously doubt their theoretical potential. As experience with the first generation of neuroprosthetics shows \u2014 it's a question of understanding how the system works. \n                     Sixth sense can come from within \n                   \n                     Nerve chip goes live \n                   \n                     See no evil, feel no evil \n                   Reprints and Permissions"},
{"file_id": "442242a", "url": "https://www.nature.com/articles/442242a", "year": 2006, "authors": [{"name": "Colin Macilwain"}], "parsed_as_year": "2006_or_before", "body": "The White House is trying to reform environmental and health regulation across the board. But it is doing so very quietly. Colin Macilwain takes a look behind the scenes. 'Bush declares war on environment,' read an unflattering CNN headline in March 2001. It was early in the administration of President George W. Bush, and the White House had just upwardly revised how much arsenic it would allow in drinking water. Never mind that they had stepped into a trap laid by former president Bill Clinton, who sharply lowered the acceptable level of arsenic just before leaving office. The direct approach to changing the standards didn't play well, and it would be the last time the administration openly sought to relax a pollution rule. After that, the usually direct administration took a discreet approach to reforming the way in which the US federal agencies regulate everything from drinking-water standards and air quality to airport security and work safety. The approach is so discreet that few outside Washington DC even realize it exists. Its headquarters are at the Office of Information and Regulatory Affairs, at the White House Office of Management and Budget (OMB) \u2014 the low-key powerhouse that oversees the $2.7-trillion annual spending of the federal government. Until this February, the effort's chief engineer was John Graham, a soft-spoken, bespectacled former director of the Center for Risk Analysis at Harvard. When Graham quit the government to run the Pardee RAND science-policy graduate school in Santa Monica, California, he left behind a proposal that could radically change how health, safety and environmental rules are drawn up. Its real effect would be to relax them, critics charge. The OMB Bulletin on Risk Assessment landed quietly on agency officials' desks in January. The 26-page document outlines proposed changes to the way the government conducts risk assessments \u2014 the scientific studies that quantify the risks to human health, safety and the environment caused by various activities. The bulletin is merely a proposal at this stage and, unusually, the OMB has sent it to the National Academies for review. An academies panel chaired by John Ahearne, an engineer and former head of the Nuclear Regulatory Commission now at Duke University in Durham, North Carolina, will report on it in November. The OMB should issue its final bulletin soon afterwards. And all federal agencies will be expected to adhere to it. \u201cThe quality of risk assessment in the federal government is uneven,\u201d explains Graham. \u201cWhat we're trying to attain is a standard across the government.\u201d But already the knives are out for the proposal. Environmental and consumer groups charge that its real aim is to weaken regulators, such as the Environmental Protection Agency (EPA) and the Food and Drug Administration. \u201cIf this is adopted in anything close to its current form,\u201d says Rena Steinzor, a regulatory-affairs specialist at the University of Maryland in Baltimore, \u201cit will have a devastating impact on public health and safety regulations.\u201d Congress is also getting involved. On 5 May, leading Democrats in the House of Representatives fired off a letter to the president of the National Academy of Sciences, Ralph Cicerone. In it, Tennessee's Bart Gordon, Michigan's John Dingell and other senior Democrats ask the National Academies to spell out the limitations of its own review. In effect, they warn it to refrain from filing a scientific endorsement of what they regard as a fundamentally political proposal. \u201cIt appears impossible to provide a comprehensive answer to the questions without reaching beyond the scope of a scientific review,\u201d says the letter. It goes on to charge that the \u201cOMB's proposed bulletin is in conflict with the approach taken in existing law\u201d \u2014 which, the authors say, already instructs individual agencies on how to assess particular risks. Both supporters and critics agree that the document is the crowning achievement of Graham's long march to regulatory reform. It is the fourth important element that he put in place from his powerful position at the OMB. Three broad-ranging edicts on how agencies should, and should not, go about the production of regulations have already been issued: they covered 'information quality', cost\u2013benefit analyses and scientific peer review. The last drew a sharp and uninvited riposte from the National Academies when it was released in 2003. The academies said it was far too prescriptive in telling scientists how to review the work of their peers; the document was subsequently watered down to accommodate the concerns. The main scope of January's risk-assessment bulletin is twofold: it offers guidance on how government agencies should go about conducting such assessments, and broadens the set of circumstances in which they need to be done. Scientific risk assessments are murky affairs at the best of times. The US environmental movement, in its 1970s heyday, strongly resisted the very concept; greens prefer the rival paradigm of the 'precautionary principle'. This principle holds that a regulator responsible for, say, clean water, should respond to uncertainty about the toxic effects of a given chemical by setting a limit that it holds to be safe, in advance of more precise information. The approach is anathema to regulated industries, but it has been embraced, at least in theory, by political leaders in the European Union. By contrast, in the United States, where water and air quality are more tightly regulated, risk assessment has been incorporated into many environmental laws. With the bulletin, it would be officially incorporated into all federal-agency responses to risk. \n               Triple bill \n             Graham has cited three main examples in support of the need for the bulletin. One takes issue with a 2002 EPA estimate of the 'safe' dose in drinking water of perchlorate \u2014 an ingredient in solid rocket fuel, often dumped on public land by the Department of Defense. A subsequent, 2005 National Academies study identified a much higher safe dose of the chemical than the EPA had done \u2014 leading the agency's critics to say that it should take more care before issuing such dose estimates. The second case also concerns the EPA, and an estimate that it made of deaths caused from cardiopulmonary disease as a result of diesel fuel emissions from off-road vehicles. The EPA made its estimate, Graham argues, even as it requested millions of dollars to research the topic. It turned out that the agency's estimate was merely a central value between risks that might be much higher or much lower, depending on the outcome of the research. \u201cWhen an agency produces a risk assessment with false precision,\u201d says Graham, \u201cit engenders negative reactions from stakeholders and from the scientific community.\u201d His third example addresses a rather different area. The Department of Agriculture decided to shut the Canadian border to cattle trading after mad cow disease was found in Canadian cattle in May 2003. Graham says that when he raised the issue of a risk assessment with department officials, as they were considering re-opening the border, he got \u201ca long silence and a blank stare, as if risk assessment was a term from a foreign language\u201d. Only last week, a National Academies panel took sharp issue with yet another EPA risk assessment \u2014 a 2003 estimate of the cancer risk posed by chemicals called dioxins. The panel, chaired by David Eaton of the University of Washington in Seattle, said that the agency had failed to quantify the risks in its assessment, or to justify the main assumptions behind it. Graham's bulletin would require government decisions to be subjected to formal risk assessment; in emergencies, this could be completed after the decision was implemented. Deciding what constitutes an emergency can be difficult, however, and critics claim the regime would stifle government action. \u201cThey won't be able to react to things that involve a rapid decision,\u201d says Jennifer Sass of the Natural Resources Defense Council, an environmental group based in New York. For instance, the EPA moved swiftly to monitor mould growth in New Orleans after Hurricane Katrina \u2014 a response that might not be possible under the new rules, she claims. The measures enacted by the OMB, critics say, will provide enough sawdust to clog up the wheels of government regulation for years to come. \u201cThe aim is to bog the process down, in the name of transparency,\u201d says Robert Shull, head of regulatory policy at the pressure group OMB Watch, based in Washington DC. Graham, a former economist, says its aim is quite the opposite. \u201cThere will be some cases where risk assessments will become more expensive\u201d as a result of the bulletin, he concedes. \u201cBut their improved quality will mean less controversy and less delay \u2014 so we can see opportunities for saving time and money.\u201d \n               Fans in commerce \n             The measure's strongest supporters are the US Chamber of Commerce and industry groups such as the American Chemistry Council, headquartered in Arlington, Virginia. \u201cIt crystallizes 15 to 20 years of research on how best to do risk assessment, on the basis of what has been said by the National Academies and others,\u201d says the chemistry council's senior toxicologist, Richard Becker. His group has endorsed the proposal and suggests that it be given more teeth by making it subject to judicial review, in which an aggrieved party can challenge the government's implementation of it. The National Academies is accustomed to being in the middle of this kind of fight, but it is rarely asked to review a policy proposal before it goes into force. Sceptics smell a trap: the academies' unsolicited intervention greatly diluted the previous White House proposal on standardizing peer review of scientific evidence. By asking for its advice first, the OMB can hope to implement a final rule \u2014 which it will write itself \u2014 that will be difficult for future administrations to overturn. The only time the unflappable Graham looks ruffled is when asked if his sending of the proposed bulletin to the academies for endorsement might be regarded as a ploy to muffle political criticism. \u201cAnyone who believes that the National Academies can be used in that way doesn't understand the process,\u201d he sniffs. \u201cPerhaps the OMB should be given credit for allowing its work to be criticized by the scientific community. There was no legal requirement for it to do so.\u201d Whatever the National Academies says in November, a rule is likely to be implemented that will embed risk assessment more deeply in the decision-making process. This will be John Graham's permanent legacy, subtly moving the regulatory goalposts in industry's favour, without catching the public's eye. \u201cIt all sounds very nice and sensible,\u201d says Shull. \u201cPolitically, it is much more viable than, say, trying to weaken the drinking-water standards.\u201d \n                     In search of sound science \n                   \n                     The rise of the bean counters \n                   \n                     Independence, but no Nobel winners for India since then \n                   \n                     Risk assessment proposal \n                   \n                     OMB Watch \n                   \n                     American Chemistry Council \n                   Reprints and Permissions"},
{"file_id": "442018a", "url": "https://www.nature.com/articles/442018a", "year": 2006, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "US nuclear weapons scientists are designing a warhead that is meant to be \u2018reliable\u2019 without ever having been tested. Geoff Brumfiel asks whether it could renew the United States' ageing stockpile. It's a hot spring day in the Nevada desert, and retired technician Ernie Williams is showing tourists how nuclear bombs used to be tested. Williams is short and gruff and a veteran of the US weapons complex. Since 1951, he has participated in around 80 nuclear detonations or \u2018shots\u2019. By his estimate, he has received 0.6 sieverts of radiation over his lifetime \u2014 about 40 times the dose of a modern nuclear-plant worker. But his only ailment is slight deafness, for which he blames genes, not nuclear blasts. From the tour bus, Williams points to a 15-storey tower rising above the pockmarked landscape of the Nevada test site. The tower was built for \u2018Icecap\u2019, one of the country's last three underground tests, which were cancelled in 1992 after the United States declared a testing moratorium. Workers simply left the tower baking in the desert sun, a monument to decades of nuclear explosions. Five hundred kilometres west, at California's Lawrence Livermore National Laboratory, Doug East is explaining to visitors the new face of nuclear testing. East is a computer scientist who spent his early career programming large networks at places such as IBM and the telephone giant Pacific Bell. He came to the Livermore laboratory in 1992 \u2014 the same year the United States stopped testing nuclear bombs. East leads the guests inside a cool, sterile vault that houses the lab's newest supercomputer, known as Purple. An array of black boxes, each stamped with \u2018IBM\u2019, Purple has the electricity needs of a small town and is one of the fastest computers on the planet. Inside the boxes, more than 12,000 high-speed processors crank through incredibly detailed simulations of nuclear-weapons tests. \u201cThis is the first machine\u201d, East says, \u201cthat really gives us button-to-bang capability.\u201d Harnessing the power of supercomputers such as Purple and data from past tests, US weaponeers are working feverishly on an ambitious programme to design a new nuclear warhead that they can certify will work \u2014 even without a test explosion. They claim that the new weapon will replace the ageing warheads in the US nuclear stockpile; that it will be safer and more reliable than existing designs; and that it will be easier to build and cheaper to maintain. Some designers informally call it the \u2018wooden bomb\u2019, because theoretically it will be able to sit on the shelf for years with little maintenance. Formally, the new weapon is known as the Reliable Replacement Warhead, or RRW. \n               Virtual explosion \n             For Livermore and its sister facility, Los Alamos National Laboratory in New Mexico, the RRW is the future. It will provide a new generation of weapons designers the chance to work on a nuclear warhead, and give the weapons labs a well-defined project in the post-cold-war era. But critics of the programme \u2014 including some who designed the current generation of US nuclear warheads \u2014 doubt that the RRW can be guaranteed to work without a test. \u201cI just can\u2019t believe anyone would prefer a new warhead that's designed by people who have never designed anything before and then made by people who have never built anything before,\u201d says Harold Agnew, former director of Los Alamos. \u201cTo me, that's ludicrous.\u201d Despite the criticism, the programme is quietly gaining political momentum. Congressional appropriators, who killed earlier design programmes, gave the RRW project a respectable $25 million last year. If the programme continues on target, the warhead could enter military service in the next decade. The debate over the RRW has its roots in the 1992 testing ban, instituted by the former President George Bush as the first step towards signing the Comprehensive Nuclear Test-Ban Treaty. The United States never ratified the treaty, but the government has maintained its voluntary moratorium on testing. \n               Historic stockpile \n             \u201cThe test ban symbolizes that the nuclear arms race is over,\u201d says Robert Nelson, a physicist and arms-control expert at Princeton University in New Jersey. As long as the United States doesn't test, he says, other nations \u2014including nuclear upstarts such as India and Pakistan \u2014 feel enormous pressure to follow suit. And the ban gives the United States a huge advantage over other established nuclear nations, because it already has data from 1,054 nuclear tests. China, by comparison, has conducted only 45. The end of testing has left the United States with an ageing stockpile of nearly 10,000 nuclear warheads. Most are between 17 and 30 years old, says Nelson, and are of roughly a dozen different designs 1 . All use ordinary explosives to compress nuclear material, often plutonium-239, which then triggers a series of runaway fission and fusion reactions (see graphic). The warheads aren't easy to maintain because, in addition to normal ageing processes such as rusting, the plutonium in a weapon' trigger, or \u2018pit\u2019 \u2014 the component needed to initiate the chain reaction \u2014 emits a small but steady stream of radiation. That radiation changes the properties of the plutonium alloy by altering its crystalline structure 2 , which in turn can cause the weapon to fail. Over the past 14 years, researchers have studied these warheads with a battery of computer simulations and experiments. In recent years, those studies have revealed some significant new details about the old weapons, says James Peery, who directs the explosive-testing programme at Los Alamos. \u201cThey're discovering things and seeing things that they did not expect,\u201d says Peery. Details, of course, are entirely classified. Nobody believes that there are serious problems with the existing warheads, but the unforeseen discoveries are contributing to anxiety about how long they can be maintained. \u201cI have great reservations that one could use pits that have aged for more than 50 years,\u201d says metallurgist Siegfried Hecker, a former director of Los Alamos who is now at Stanford University in California. At some point, Hecker says, the plutonium in the current pits will have to be melted down and remade into new ones. And that is where the RRW comes in. During the cold war, physicists focused on making the lightest, most explosive weapons possible, in order to fit many warheads on to a single missile, says Paul Robinson, a retired physicist who spent seven years directing the Los Alamos weapons programme. But in the post-cold-war era, where nuclear showdowns between superpowers seem less likely, missiles often carry fewer warheads, freeing up both space and weight for designers. \u201cToday, you could do a lot to make the things more robust,\u201d Robinson says. \n               Vision of the future \n             Weapons designers at Livermore and Los Alamos are now working on RRW designs as replacements for the United States' most abundant nuclear warhead, the W76, which is deployed on submarine-launched missiles. Eight W76s, each destined for a different target, can sit atop a single missile. But today, most missiles routinely carry four. At the centre of the Los Alamos campus, a gleaming glass-and-steel building houses the lab's alternative to the ageing W76. Past security turnstiles and fingerprint scanners, designers are using Los Alamos's supercomputers \u2014 only marginally less powerful than Livermore's Purple \u2014 to virtually test their RRW designs. Inside a secured room called \u2018the cave\u2019, 33 projectors display three-dimensional, stereoscopic simulations on the walls and ceiling. Using a joystick, designers can rotate, spin and zoom through each warhead design as it detonates, watching every stage of the explosion. The simulations are not pure abstractions; they are heavily based on years of experimental data, including those from non-nuclear explosive testing. Both Los Alamos's and Livermore's RRW prototype designs are based on earlier warhead designs that were tested underground, according to one weapons laboratory scientist who requested anonymity. \u201cThe designs will be so close that even sceptics will accept the simulations,\u201d he says. Few outside the weapons labs know what these simulated alternatives to the W76 look like. But congressional testimony, unclassified laboratory studies, and media reports all point to a number of likely changes. The most obvious alterations could be made to the weapon's plutonium pit. Adding more plutonium may ensure that the device detonates properly, even after years of sitting on a shelf. Pits could also be redesigned for ease of manufacture, says Hecker. During the cold war, pits were fashioned by shaping sheets of heated plutonium metal \u2014 a fast but imprecise technique. \u201cWe had very rough specs and then we went and conducted a nuclear test,\u201d Hecker says. \u201cAs we look to the future, I would definitely vote against doing it that way.\u201d He says that the pit for an RRW could instead be cast in a mould. \n               A difference of design \n             Toxic materials in the warhead may also be replaced with more benign substances. Currently, plutonium in the W76 warhead is surrounded by a shell of beryllium, which helps to amplify the initial nuclear explosion. But beryllium is also toxic and carcinogenic, so replacing it with heavier material such as stainless steel would reduce the environmental hazards associated with manufacturing the warhead. Finally, a report from the weapons labs indicates that designers are considering replacing the lightweight but volatile explosives on the W76's outer shell with a less powerful and more stable explosive called an \u2018insensitive high explosive\u2019. This would increase the weapon's size and weight but decrease the likelihood of an accidental detonation during storage. But will these changes really lead to cheaper warheads without the need for testing? A dozen current and former designers unanimously agree that changes might simplify the process of maintaining warheads. But there is far less accord on whether the new warheads would require testing, or whether they would be affordable when compared with remanufacturing the existing stockpile. Agnew, who oversaw Los Alamos during the design of the W76, is among the most vociferous critics of the RRW programme. Very little, he argues, is known about how alterations affect performance. \u201cThese nouveau designers don't know what the margin is. In fact, no one knows what the margin is,\u201d he says. For example, adding more plutonium to a warhead's pit doesn't necessarily make it more reliable, he argues. It could instead make the warhead more likely to accidentally explode, or it could overheat the \u2018secondary\u2019 fuel which produces most of the weapon's power. There's simply no way to tell without a test, he asserts. \u201cIf you really believe that the nuclear deterrent is important,\u201d he says, \u201cyou shouldn't put things in the stockpile that aren't tested.\u201d Sidney Drell, a Stanford University physicist, and others also question whether the changes are much of an improvement. A 1994 study by Drell and Robert Peurifoy, formerly at Sandia National Laboratories in Albuquerque, New Mexico, showed that rocket fuel, not high explosives, would be the most likely cause of an accidental explosion 3 . As a result, the Navy decided not to use insensitive high explosives on the W76. \n               Where doves meet hawks \n             And then there is the question of cost. The programme to look after the existing US nuclear stockpile is expensive \u2014 more than $1.4 billion a year. Members of Congress are interested in the RRW because of claims that it could save money, even though the labs have not released a total cost estimate. But Richard Garwin, a former bomb designer, argues that the reason the stockpile costs so much has more to do with its size than with its age. More money could be saved by cutting the number of existing weapons from 10,000 to 2,000, he says. With a diminished arsenal, says Garwin, \u201cwe will be able to maintain current weapons indefinitely.\u201d Not all former weapons designers are so critical of the RRW programme. Herbert York, Livermore's first director, believes that early warhead designs, which have already been tested, could provide a reliable basis for designing a much simpler but much heavier warhead. \u201cI'm not sure it would be practical, but I believe they could be designed to be stockpiled without testing,\u201d he says. Despite the criticism, Republicans and Democrats are looking favourably on the programme. Hawks like the programme because it will allow the United States to train a fresh generation of weapons designers. And doves, who have torpedoed earlier weapons programmes 4 , are wooed by the claim that the RRW will not need to be tested. Congress is likely to more than double the programme's budget for next year. And the military has lent its tentative support to the project (see  \u2018Convincing the generals\u2019 ). All this is good news for the country's ageing nuclear weapons complex, according to long-time observers. \u201cThe US nuclear programme has been in a cul-de-sac since the end of the cold war,\u201d says John Foster, a former director of Livermore who is chairing a panel on the complex for the Pentagon's Defense Science Board. Since the collapse of the Soviet Union, lab morale has sagged and efforts to refurbish warheads have fallen behind schedule, Foster says. The RRW programme provides the labs with a fresh challenge and clear vision. \u201cThe RRW\u201d, he says, \u201cwould catalyse the enterprise from design through production.\u201d And, indeed, the programme does seem to have a reinvigorating effect (see  \u2018British secret forces?\u2019 ). Weapons designers are thrilled to be working on their first new warhead in two decades. Earlier this spring, the Livermore team ran a huge simulation of its RRW design. At Los Alamos, scientists are about to conduct a non-nuclear explosive test to check some of their calculations. Both teams have submitted their designs to a review committee, where they are being peer-reviewed. One of the two designs will be selected as the basis for a development programme later this year. A second competition may even be held in 2007, for designs for an another RRW. If all goes well, pits for the first warhead could be manufactured as early as 2012 For now, the Reliable Replacement Warhead remains a series of zeros and ones, in the huge supercomputers at Los Alamos and Livermore. But back in the Nevada desert, the structure that once housed Icecap still looms above the Joshua trees. The rigging to hold a bomb \u2014 all 225,000 kilograms of it \u2014 remains in place, along with hundreds of metres of copper cable designed to carry data signals a few nanoseconds ahead of the blast. Icecap, in short, is ready for the next US underground test. \u201cShould we come back to nuclear testing,\u201d Ernie Williams cheerfully tells his visitors, \u201cit seems reasonable we'd start with this one.\u201d \n                     Bush buries US bunker-buster project \n                   \n                     Nuclear chiefs scotch story on frailty of ageing warheads \n                   \n                     Democrats slam Bush plan for fresh nuclear weapons \n                   \n                     Nuclear-weapons design plan raises fresh proliferation fears \n                   \n                     National Nuclear Security Administration \n                   \n                     Lawrence Livermore National Laboratory \n                   \n                     Los Alamos National Laboratory \n                   \n                     Sandia National Laboratory \n                   Reprints and Permissions"},
{"file_id": "442348a", "url": "https://www.nature.com/articles/442348a", "year": 2006, "authors": [{"name": "Erika Check"}], "parsed_as_year": "2006_or_before", "body": "The United States has embarked on a huge effort to try to track the H5N1 avian flu virus in birds migrating into the country. But is surveillance more urgently needed elsewhere? Erika Check reports. On a Sunday afternoon in June, graduate student Brad Comstock is standing on the flat, soggy tundra at the edge of the Bering Sea. He surveys the network of shallow ponds stretching out in front of him, and the vast delta reaching to the horizon beyond. Nestled down along the shores of the ponds are dozens of large, migratory sea ducks called eiders. Each bird is settled on a down-lined nest holding eggs that are just days away from hatching. The air is filled with the sound of eiders honking to each other over the constant, whistling wind. \u201cThis is eider heaven,\u201d Comstock says. But he is about to stir up trouble in paradise. Carrying a fishing net and a backpack full of test-tubes and cotton swabs, he creeps towards one of the nests. At the last moment, he hurls the net towards a mother eider. She jumps to evade him, but too late; her wings flap angrily as the net ensnares her. Comstock rushes in, extracts the offended bird and turns her upside down. He runs a cotton swab across her cloaca to take a faeces sample, then drops the swab in a test-tube. The sample will be held briefly at a nearby camp, then shipped to the National Wildlife Health Center in Madison, Wisconsin, where technicians will test it for the H5N1 avian influenza virus. The 7.7 million hectares of the Yukon Delta National Wildlife Refuge in Alaska may seem like heaven to eiders. But to the US government, it is the frontline in the battle to protect the country from the deadly bird flu virus. Comstock is part of a massive effort to track the possible entry of H5N1 into the United States. Since 2003, the virus, a more lethal strain than the flu viruses that normally infect birds, has rampaged through the rest of the world. More than 200 million poultry have died of H5N1 or have been culled to prevent its spread since 2004; 132 people have died after catching the virus from close relatives or directly from birds. So far, the virus hasn't learned how to jump efficiently from person to person. But public-health officials fear that if it does, a pandemic could follow, killing millions of people. The US government is worried about wild birds because the country is linked directly to Asia \u2014 where H5N1 first appeared \u2014 through two overlapping migratory bird flyways (see map). Every year, birds such as pintails, eiders, ducks, godwits and geese cross from Asia over the Bering Sea into Alaska. These birds mingle with other migrating groups at breeding and wintering grounds in Russia and western Alaska. No one knows which of them might be carrying H5N1, or whether the virus might leap from such a carrier into people or into the $29-billion poultry industry of the United States. This year, the US departments of agriculture and the interior will lead a $29 million effort to test wild birds for H5N1 and other avian flu viruses in Alaska and other parts of the United States. \u201cWe're at the nexus of bird migration from the Canadian Arctic to Russia and southeast Asia,\u201d says Robert Leedy, chief of the US Fish and Wildlife Service's office of migratory bird management in Anchorage, Alaska. \u201cIf H5N1 is going to be transferred in wild birds, the most likely avenue is Alaska.\u201d \n               Crossing continents \n             But some scientists have reservations about the testing programme. Many flu experts think poultry smuggling or imports, rather than migrating birds, are far more likely to bring in the virus. And others point out that it's still not clear how or whether wild birds contribute to H5N1 outbreaks in domestic poultry. Given these uncertainties, some question the decision to spend millions of dollars hunting for flu in Alaska, when the H5N1 virus is already racing across the rest of the globe. \u201cMore information is always better, so you can't complain about that,\u201d says William Karesh, director of the Wildlife Conservation Society's Field Veterinary Program based in New York City. \u201cBut it's much more important to go where the disease is in the developing countries, to see how this thing is spreading.\u201d Until last year, no one thought that migratory birds played any serious role in the spread of H5N1. But in July 2005, a team of virologists reported that some 6,000 migratory birds had died of an H5N1 outbreak at the Qinghai Lake nature reserve in China 1 . Many of the dead birds were bar-headed geese, which fly from China to India and Myanmar every year. Since that report, the H5N1 strain has been found in dead migratory birds in Asia, Russia, Europe, Africa and the Middle East. Four people even died from bird flu after collecting feathers from infected wild swans in Azerbaijan 2 . So what role do migratory birds play in spreading H5N1around the world? Genetic studies may help to answer this question. This May, Ian Brown of the United Kingdom's Veterinary Laboratories Agency in Weybridge revealed that H5N1 viruses taken from dead wild birds in Europe are very similar to H5N1 viruses found in Mongolia, Siberia and Qinghai Lake. Scientists have also reported that healthy birds in China were carrying the H5N1 strain just before their autumn migration last year 3 . That suggests the birds could have caused the outbreak of the virus in Europe last autumn, by carrying it to the continent from east Asia. Other studies have implied that wild birds shuttled the virus between Europe and Africa, where H5N1 first showed up in February. A team of researchers recently suggested that migrating birds may have transmitted H5N1 to Nigeria, the first African country to report the virus 4 . The scientists sequenced genes from bird flu viruses found in chickens on poultry farms. They discovered that many of the viruses, which seemed to cluster into three genetic groups, were similar to those found on other continents, including one strain that has been found only in wild birds in Europe. What's more, the virus outbreaks in poultry were found along major bird migration corridors. But none of these studies can conclusively show that migratory birds transmit the virus. In all cases, the wild birds themselves could have caught H5N1 from poultry or from some 'bridge' group, such as crows, jays or grackles. Officials agree that answering questions about the role of wild birds will require a lot more field work in live, migrating birds. And that's why James Sedinger and his team of young biologists, including Comstock, are spending their summer swabbing birds' rears on the Yukon Delta. Sedinger, a wildlife biologist at the University of Nevada at Reno, has been studying migratory birds in Alaska since 1977. His field camp sits at the edge of the tidal Tutakoke River, near where it runs into the Bering Sea. The camp rattles with the noise of birds at all hours in summer, when the sun sets for three hours every night, and if you don't watch your step you're likely to stumble into a mother bird sitting on a nest tucked into the grass. This is breeding central, not just for migrating eiders, but also for black brants \u2014 chunky sea geese that fly from Alaska to points south for the winter. Over decades of work, Sedinger has snapped identifying bands on thousands of brants at Tutakoke. Some of the birds have been found to spend the winter in China, Japan and Korea. The US government's plan to head off bird flu focuses on about 29 such species that spend time in Alaska and travel to Asia. About 15,000 birds will be tested in the state. Most will be trapped live by biologists. But the Department of Interior will also test birds shot by sport and subsistence hunters and \u2014 if and when they happen \u2014 dead ones found among mass bird die-offs. \n               Out for the count \n             So far, 3,772 samples from Alaska have been tested, and none has turned up positive. But that's not surprising. Most of the samples taken from live wild birds around the world are clear of H5N1, says Ward Hagemeijer of Wetlands International, a group that coordinates volunteer surveys of migratory birds. Last winter, Wetlands International tested almost 6,000 wild birds for H5N1 along migration paths in Africa, Europe and Asia. And scientists funded by the European Union have tested a total of 45,000 wild birds in Europe since last autumn. So far, none has turned up positive for H5N1, even though dead wild birds in all of these places have been found to be carrying the virus. Migrating birds are known to carry avian influenza strains other than H5N1. But, says Hagemeijer, \u201cFinding H5N1 in healthy wild birds is amazingly difficult.\u201d That perplexes wildlife-health specialists. Many of the dead or dying migratory birds found to be carrying H5N1 have been found near poultry farms, so it's not clear whether the wild birds infected the poultry or vice versa. And nobody knows how long the birds survive with H5N1, whether they are able to transmit the virus to other species and, if so, for how long they are contagious. Hagemeijer cautions that even the most incriminating data pointing to the role of wild birds in transmitting H5N1 \u2014 the genetic study of H5N1 in Nigeria \u2014 leave some gaps. Nigeria doesn't conduct rigorous safety checks on imported poultry. So it's possible that shipments of infected birds seeded the outbreak, a point the original research team concedes. The finding that one of the Nigerian strains matches a strain found only in wild European birds also isn't convincing evidence that migratory birds were the cause of the outbreak, says Hagemeijer. This is because many more H5N1 strains have been studied from wild birds than from poultry. \u201cIf you're looking at relations between strains, you're far more likely to find a close relative in the wild bird database than in the database for poultry, because more of them have been sequenced,\u201d he says. \n               Needle in a haystack \n             Hagemeijer adds that wild birds \u201cprobably play a role, but we still haven't found the smoking gun\u201d. Other animal-health officials agree. The real question is whether wild birds can serve as a permanent reservoir for the virus, rather than simply transport it from place to place. That is a worrying possibility because whenever the virus has a chance to mix in large groups of animals, as in the huge poultry farms of Asia, it might mutate so that it becomes lethal to people. \u201cThere are slight differences between the strains we find in wild birds, which suggests something is happening,\u201d says Scott Newman, a veterinary health specialist with the Wildlife Conservation Society who works at the headquarters of the United Nations Food and Agriculture Organization, in Rome. \u201cBut it's hard to tell whether it's happening in wild birds or in poultry.\u201d Given all these unknowns, some are sceptical about the United States investing so much money hunting for H5N1 in live birds. Newman points out that H5N1 has swept from Asia towards Europe, and not the other way around. Perhaps, he suggests, officials should be more concerned about migration from Europe, through Greenland to the US east coast, where limited testing is ongoing. Even the biologists doing the work say it's unlikely that they will intercept the virus. \u201cWe're looking for a needle in a haystack,\u201d Sedinger says. Others are frustrated that the United States is spending so much on testing birds within its own borders while the disease continues to spread elsewhere. \u201cWe should be doing more overseas,\u201d Karesh says. \u201cI'd like to see the United States do more outreach.\u201d The Food and Agriculture Organization is already working with groups such as Wetlands International, the Pasteur Institute, the French Agricultural Research Centre for International Development and the Centers for Disease Control and Prevention to test birds worldwide. This January, the United States pledged to spend $334 million in international aid for countries battling influenza. And last month, the US Agency for International Development agreed to provide $5 million to an effort called the Global Avian Influenza Network for Surveillance. Headed by the Wildlife Conservation Society, this will test birds for H5N1 in remote and poor places, such as Mongolia, that don't have their own surveillance system in place. This is the only good way to get a better handle on the dynamics of the virus in wild birds, Karesh says. Meanwhile, biologists who normally struggle for grants to study the basic biology of migrating birds are frustrated by the huge amounts of cash flowing for bird-flu studies. The field camp at Tutakoke, for instance, wouldn't even have been funded this summer without the bird-flu surveillance money. Sedinger and other bird biologists say the paucity of long-term studies on migratory birds makes it difficult to understand the spread of H5N1. It wasn't until 1995, for example, that scientists discovered that spectacled eiders spend their winters floating in holes in pack ice in the Bering Sea 5 . So while biologists are thankful for the bird-flu surveillance money, they also wonder whether officials will take the logical next step and invest more in monitoring studies as well. Still, biologists are happy for any chance to learn more about migratory birds \u2014 especially if it means spending long days chasing after ducks and geese on wind-blown tundra. And even if they don't track down the virus in Alaska, Sedinger, Comstock and the rest of the Tutakoke team will learn more about how viruses circulate in and between species \u2014 an important area of research given that at least two human flu pandemics in the past century began as bird viruses. \u201cWe're going to get our hands on some birds we wouldn't normally get to study,\u201d says Leedy. \u201cThat's going to lead to some good work.\u201d \n                     Avian flu and the New World \n                   \n                     Avian flu and the New World \n                   \n                     Blogger reveals China's migratory goose farms near site of flu outbreak \n                   \n                     Doubts hang over source of bird flu spread \n                   \n                     Migration threatens to send flu south \n                   \n                     Extinction risk from climate change \n                   \n                     Bird flu moves towards Europe \n                   \n                     In Focus: Bird Flu \n                   \n                     FAO: Avian influenza \n                   \n                     WHO: Avian influenza \n                   \n                     US government pandemic flu \n                   Reprints and Permissions"},
{"file_id": "442238a", "url": "https://www.nature.com/articles/442238a", "year": 2006, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "DNA extracted from bones could shed light on what happened when our ancestors crossed paths with Neanderthals. But not everyone can get the fossils out of the ground, as Rex Dalton learns. In the Moula-Guercy cave, nestled in a cliff overlooking France's verdant Rh\u00f4ne River valley, a bent Neanderthal finger bone pokes out of the ground, as if beckoning researchers. Not so long ago, the enticement might have been ignored. With palaeoanthropologists focusing instead on the most ancient branches of the human family tree, the study of Neanderthals \u2014 our closest relatives \u2014 was something of a backwater. But now researchers are eager to comply with the come-hither gesture. A modern-day rush is on: for Neanderthal DNA. Across Europe, researchers are scrambling to unearth fresh bones of Neanderthals that might yield intact genetic material. Spurred by recent discoveries of DNA segments in old bones, scientists are even laying plans to sequence an entire Neanderthal genome in the next two years. This week, scientists are gathering in Bonn, Germany, to mark the 150th anniversary of the discovery of  Homo neanderthalensis  \u2014 made in Germany's Neander Valley. During 21\u201326 July, experts will debate all aspects of Neanderthal life, from how they migrated across Europe to what effect climate may have had on their evolution  (Fig.1) . At centre stage will be the future of DNA research and what it could tell us about the Neanderthals' relationship to modern humans. \u201cNeanderthals split from the tree of human evolution about 400,000 years ago, just before the emergence of modern humans,\u201d says French archaeologist Jean-Jacques Hublin at the Max Planck Institute for Evolutionary Anthropology in Leipzig, Germany. \u201cIt is very important to understand in what respects genetically they are like us, and in what aspects they are different.\u201d So far, the only closely related species with a mapped genome is the chimpanzee ( Pan troglodytes ) 1 , whose ancestors diverged from those of humans about 7 million years ago. A Neanderthal genome could reveal much more about our recent evolutionary history. The main difficulty is getting hold of gen-etic material. Few museum specimens of Neanderthals have yielded DNA, and those that have could easily have been contaminated with modern human DNA. So fresh Neanderthal bones are prized possessions. The hunt for bones is focused on the ridgelines and rugged valleys of southern France, where Neanderthals may have held out in their last refuge before going extinct about 28,000 years ago. They would have overlapped with  Homo sapiens  in this area for thousands of years \u2014 probably eventually losing out in the competition for resources. \n               Out of bounds \n             Alban Defleur of the Institute of Human Palaeontology in Paris hopes to return to the Moula-Guercy cave to find many more Neanderthal bones; it was here he unearthed evidence, published in a 1999 paper 2 , that Neanderthals practised cannibalism at least 100,000 years ago. Today, the partially excavated site appears eerily frozen in time: tools are stacked about, strings marking quadrants sway from the ceiling, and a rubber bucket is placed over the finger bone that juts from the cave floor. Shortly after Defleur published his paper, the cultural ministry office in nearby Lyon halted digging at the cave. Officials baulked at renewing Defleur's excavation permit, ostensibly because his collaborators had not filed a required sedimentology report on earlier digs. They filed it recently, saying the delay was to allow graduate students to finish their theses before details of the site were released in the report. Some observers say in confidence that they think Defleur may have been snared in a web of nationalistic pride that overshadows French research. Co-authored with prominent US palaeoanthropologist Tim White, Defleur's article on cannibalism was published in the US journal  Science , rather than in a French journal \u2014 a move that could have irritated some. Other researchers have similarly fallen foul of government officials. In 1998, the same cultural ministry office that denied Defleur his permit was found by a French court to have forged documents. The forgery challenged the rights of the Chauvet cave's discoverer to data and photos; the nearby cave houses a world-famous display of rock art from this period. \n               Close encounters \n             In many respects, France has one of the richest histories in archaeology and palaeoanthropology. The University of Bordeaux I has been a worldwide magnet, with leading researchers coming to study the country's rich Neanderthal sites. But now a team based in Germany is generating anxiety. The Max Planck institute plans to dedicate millions of euros to the 'Neanderthal Genome Project', an international collaboration that aims to sequence ten Neanderthal genomes in the next decade 3 . The project is raising some eyebrows, however, among France-based archaeologists, including Catherine H\u00e4nni, of the Superior Normal University of Lyon. \u201cAre these true collaborations?\u201d she asks. \u201cOr do they just want the bone for DNA?\u201d Hublin, who used to work as an official at the main French research agency, the National Centre for Scientific Research (CNRS), understands the concerns. \u201cThe problem is a little like oil in an Arab country,\u201d he says. \u201cI don't want to be too critical. But France is a closed world \u2014 at times not caring about publishing in well-rated journals.\u201d Meanwhile, Hublin continues to push on. As part of a large team involving principal investigators from France and other countries, he is working on a hillside outside the village of Jonzac, an hour's drive inland from Bordeaux in southwestern France. A century ago near Jonzac, miners cutting a road to an underground limestone quarry unearthed Neanderthal artefacts. But these were left largely unexplored until about ten years ago, when French geologist Emile Marchais did some preliminary excavations. Later, in 2004, researchers from Bordeaux and the Max Planck institute began a more comprehensive excavation. When the team arrived in June to open this summer's digging season, fields of red poppies bloomed around them. Nearby, workmen prepared to erect a barn-like steel structure to shelter the expanding excavations \u2014 an indication of the project's permanence. As tarpaulin was pulled away, sending spiders scrambling, an array of artefacts was revealed \u2014 including spear points, hand axes, scrapers and animal bones. Neanderthal material is so thickly layered here that a customized computer program is needed to chart its recovery. The artefacts are from the Mousterian group of Neanderthals that lived from about 250,000 to 35,000 years ago. The Mousterian survived in Europe at least until the first group of  H. sapiens  \u2014 the Aurignacian \u2014appeared, some 40,000 years ago. In some Jonzac sediments dated to about 36,000 years ago, tools from the Mousterian and the Aurignacian lie close together. \n               Little to go on \n             Archaeologists Shannon McPherron and Marie Soressi, both at the Max Planck institute, take detailed notes about where artefacts from these cultures merge. They will examine the layers, to see whether they have been shifted by a flood or by the collapse of a wall of rock. If so, determining what happened at this cultural interface will be doubly hard. Archaeologists prefer ordered sediments, which are identified and dated by the type of chipped stone tool or by the fossils of extinct animals found in them. Shifting sediment layers also complicate the search for bone or teeth for DNA analysis.  Homo  remains found in jumbled sediments must be analysed with particular care to delineate Neanderthals from modern man. Ideally, McPherron notes, he would like to find Neanderthal and  H. sapiens  specimens embedded in well-ordered sediments, showing they lived together. Such a find could provide ideal opportunities for genetic analysis \u2014 possibly showing that the two species interbred, an idea that has been long debated but never proven. Just definitively showing that Neanderthal and modern humans coexisted at the same location has been extremely difficult. \u201cWe really, really need to understand the geology of the site to make assertions about specimens,\u201d says McPherron. Mysteriously, given the abundance of artefacts at Jonzac, Neanderthal fossils have been scarce there, and at the team's other French site \u2014 Roc de Marsal, 200 kilometres to the east. In two years, three teeth have been found at Roc de Marsal, and one tooth at Jonzac. The team was able to extract some collagen, or connective tissue, that could hold DNA. But McPherron says there is so little collagen that they are opting not to perform DNA analysis for fear of using it up. Instead, the team plans to test for stable isotopes such as carbon and nitrogen, which could offer glimpses into the diet and lifestyle of the Neanderthals. Svante P\u00e4\u00e4bo, a Swede who directs the Max Planck institute's research into palaeogenetics, is aching for more material for DNA analysis. In 1997, he published mitochond-rial DNA sequences from Neanderthal remains 4 . But although mitochondrial DNA is more abundant than DNA in the nucleus of a cell, nuclear DNA provides a far more complete picture. In May, P\u00e4\u00e4bo reported at a meeting at Cold Spring Harbor Laboratory in New York that his team had recovered and sequenced about 1 million base pairs of nuclear DNA from Neanderthal bones from Croatia. The work could provide clues about when certain disease-causing genes arose in evolution, and perhaps pin down the source of traits such as hair or skin colour. This week in Bonn, Max Planck officials will unveil the institute's plans to sequence an entire Neanderthal genome. The project will extract the nuclear DNA from bones or teeth from both the first Neanderthal specimen discovered, from Germany, and the Croatia material. \u201cAbsolutely, I believe we can do this now,\u201d says P\u00e4\u00e4bo. But his team wants to develop new technologies to meet its eventual goal of sequencing the genomes of ten separate Neanderthals. \n               Search of a lifetime \n             The DNA bug has also bitten Defleur, who is seeking to collaborate with one of the gene-hunting teams. His group has already found dozens of fragments of Neanderthal bones at Moula-Guercy, representing at least six individuals, both juvenile and adult. \u201cI think there are many more here,\u201d says Defleur. White, a palaeoanthropologist at the University of California at Berkeley, has excavated with Defleur from France to Ethiopia and says: \u201cDefleur's high-quality excavation and recovery at Moula-Guercy rivals that of modern forensics.\u201d A key advantage of the Moula-Guercy site is that the bones are well preserved \u2014 because of the cannibalism that removes the flesh and thus destructive bacteria. Long bones have been cracked open for the marrow, which may increase the likelihood that DNA will survive, says White, an authority on cannibalistic practices 5 . White suspects this may be why P\u00e4\u00e4bo was successful in sequencing DNA from the Croatia samples: because they were from a site, called Vindija, where cannibalism was practised. Almost monthly, palaeogeneticists are pushing back the clock on the age of DNA extracted from Neanderthals and associated mammals. The specimens from Vindija are roughly 38,000 years old, making them the oldest nuclear DNA yet extracted. Last month, H\u00e4nni reported the isolation of the oldest known Neanderthal mitochondrial DNA, at 100,000 years old, from the Scladina cave in Belgium 6 . And earlier this month, a Spanish-led team reported isolating 400,000-year-old mitochondrial DNA from a cave bear ( Ursus deningeri ) in the Atapuerca cavern complex in northern Spain; the same cave complex has produced nearly three dozen early Neanderthal specimens 7 . These older specimens produce only very short genetic segments. There were 123 base pairs isolated from the Belgium site. Only 41 base pairs were found in the cave bear studies, led by Cristina Valdiosera of the Complutense University of Madrid. This is a far cry from the 3 billion base pairs that make up a  H. sapiens  or, presumably, a Neanderthal genome. But the advances in DNA studies make Defleur think his time for finding more fossils has finally come. For more than 20 years, he has searched the French countryside for Neanderthal caves. In the early days, he worked from geological maps, checking certain limestone formations for caves. Elevations \u2014 that would have been high enough to provide protection, but not so high as to be hard to access when carrying game \u2014 were matched to the formations. Using these techniques, Defleur found new caves, a number of which held remains of Neanderthal life, and re-excavated shelters that had been left only partially examined. Near the Chauvet cave, a rock shelter containing many lithics, called Ranc de l'Arc, is one such place probed by Defleur. \u201cI would very much like a major excavation here,\u201d Defleur says. And there are other newly discovered caves, whose locations remain secret. These are likely to be rich sources of artefacts; some even have lithics and pottery on the cave floor. For each journey to these dwellings, Defleur observes a moment of silence, seeming to commune with France's earliest residents. \u201cThese are places of spirit,\u201d he says. By next summer, he hopes, the government bureaucracy will have the spirit to allow him more Neanderthal discoveries. \n                     Neanderthal DNA yields to genome foray \n                   \n                     Better bone dates reveal bad news for Neanderthals \n                   \n                     Cave bear DNA laid bare \n                   \n                     Fossil findings blur picture of art's birth \n                   \n                     Chimpanzee genome web focus \n                   \n                     Neanderthal meeting in Bonn, Germany \n                   \n                     Jean-Jacques Hublin \n                   \n                     Chauvet Cave \n                   Reprints and Permissions"},
{"file_id": "442344a", "url": "https://www.nature.com/articles/442344a", "year": 2006, "authors": [{"name": "Jim Giles"}], "parsed_as_year": "2006_or_before", "body": "The idea that readers should be able to replicate published scientific results is seen as the bedrock of modern science. But what if replication proves difficult or impossible? Jim Giles tracks the fate of one group of papers. There is nothing particularly unusual about the 6,893rd issue of  Nature . Published four years ago, it covers the usual mix of disciplines. One paper describes a development in quantum computing 1 , another contains the genome sequence of a slime mould 2 . The subsequent impact of the papers has also been within the normal range: a handful have been referenced hundreds of times each, but most have notched up only a few tens of citations. A third feature of the edition, though just as normal, is more surprising: at least two of the papers we published on 4 July 2002 contain results that may not be replicable. There is nothing suspicious about the papers, nor any suggestion that their authors are anything other than excellent scientists. Nor was that week particularly odd, and there is no reason to think that other journals publish fewer problematic papers. It is simply the case that the replication of results, a process absolutely central to science, is not always possible. \u201cIf you want to know whether a duck is crossing the street, you look twice,\u201d says Harry Collins, a social scientist at Cardiff University, UK, who cheerfully describes himself as the world expert on replication. Replication in science, he says, is the same: it is a way of being sure that something really exists, and the process by which tentative discoveries acquire textbook status. If, on the other hand, attempts to replicate a result meet failure again and again, that result will end up being discounted. But, as Collins would be the first to point out, the situation is more complicated than that. For a start, many papers, especially in minor journals, go unreplicated simply because of lack of interest (a third of all papers are never again cited in the scientific literature). Their replicability thus becomes moot. More concerning for scientific progress is what happens when attempts to replicate an interesting experiment fail. At what point does 'unreplicated' give way to 'unreplicable'? And what is the best way to find out where on this scale a particular result might sit \u2014 by gossiping in the bar? By reading another paper? Or by some semi-formal mechanism in between? To see replication and its absence in practice, and to ask whether journals and scientists are doing enough to monitor it, I tracked the fate of the 19 papers in issue 6893, asking their authors and others in the field whether the results had been reproduced. In a large majority of cases they had. For example, the procedure with which Ron McKay and his colleagues at the National Institute of Neurological Disorders and Stroke in Bethesda, Maryland, treated rats suffering from an analogue of Parkinson's disease with embryonic stem cells 3  has since been repeated in rats elsewhere and in other animals. Papers on the formation of silica films 4  and on yeast genetics 5  have quietly racked up citations as the results have been replicated and built upon. In other disciplines, results are corroborated rather then reproduced. If an identical description of the fossil found by palaeontologist Jennifer Clack 6 , based at the University of Cambridge, UK, was published elsewhere, for example, it would look more like plagiarism than replication. Her interpretation, though, has undergone something like replication; similar fossils that date from the same period have since been found and described in a way that conforms with her conclusions. In the case of the genome of  Dictyostelium discoideum 2 , an amoeba, few researchers would see the need to repeat the sequencing from scratch; in any case, the genome stored on the dictybase.org website can be updated should errors be identified. \n               Giant's signature \n             But for other papers from the 4 July issue, textbook status looks a long way off. One of those was authored by Sean Brittain and Terrance Rettig, both then based at the University of Notre Dame in Indiana. Their finding was an exciting one: they claimed, for the first time, to have seen H 3 +  ions in the disk of gas and dust surrounding a young star 7 . H 3 +  is seen in the atmospheres of Jupiter and Saturn, suggesting that the astronomers had spotted a gas giant in the act of formation. Yet right from the start, other researchers wondered whether Brittain and Rettig really had seen H 3 + . The evidence was in the form of distinct frequencies of infrared radiation: H 3 +  emits at three particular frequencies, and Brittain and Rettig reported detecting emissions in only two of those three. Takeshi Oka of the University of Chicago in Illinois wrote a cautiously optimistic News and Views commentary on the finding in the same issue 8  but had his doubts about the result. \u201cWe used our earliest observation time to check,\u201d he says now. \u201cWe couldn't see it.\u201d Over the next year, the two sets of authors exchanged their raw data in a bid to resolve their contradictory results. Such exchanges are never easy, given that the scientists are to some extent putting their reputations on the line. In this case, neither side seems to have completely accepted the other's conclusions. But Oka has since published a paper describing how he failed to find any of the three H 3 +  lines 9 . Rettig says that he and Brittain no longer \u201cpromote our very tentative interpretation that the unidentified lines might be H 3 + \u201d. But a second key result from the original paper \u2014 the detection of carbon monoxide in the same disk \u2014 has since been confirmed. Attempts to replicate another result from issue 6893 \u2014 one that, like the H 3 + , was considered worthy of mention on the cover \u2014 have created far more controversy. In July 2002, stem-cell biology was national news in the United States. Because some stem cells are pluripotent \u2014 they can develop into many different cell types \u2014 they offer a chance of replacing the tissues lost to old age or disease. But for opponents, most notably those on the influential religious right, the fact that cells for research are extracted from human embryos renders the process unethical, whatever its promise (see pages 486 and 491). As the two sides battled over plans to regulate the field, a team from the University of Minnesota pitched in with a paper that seemed to offer a peaceful solution. Catherine Verfaillie and her colleagues described how they had isolated fully functioning stem cells from adult human bone marrow 10 . If the results were correct, all the benefits of stem cells could be realized by taking samples from the patient involved \u2014 no embryos, no cloning. To those with moral objections this sounded vastly preferable; to others it simply sounded easier. It looked like a win\u2013win situation. But four years later, the implications of the paper are still far from clear. \u201cPeople found the paper amazing,\u201d says Stuart Orkin, a stem-cell biologist at Harvard University. \u201cBut there has been very little published literature since. There has been no clarification of what those cells are.\u201d The story will be a familiar one to many biologists. After publication, rival labs fell over themselves to reproduce the results. Many contacted Verfaillie with requests for the cells and the reagents used to create them, or to ask for more details of the experimental protocol involved. Yet progress was not smooth. Several high-profile groups sent researchers to Minnesota to learn how to extract and culture the cells, and then brought the result into doubt by stating that they could not repeat the process back in their own labs. Verfaillie counters that the procedure takes up to six weeks to master and that those who stayed for long enough have cracked it. Some, indeed, have published results 11 . \n               Shy journals \n             Verfaillie adds that her own team has since ironed out problems with the serum it used and will soon publish a comprehensive methods paper describing the new protocol. But researchers who think they have derived pluripotent stem cells from human bone marrow using related but not identical techniques to Verfaillie's, including Dominique Bonnet at Cancer Research UK's Lincoln's Inn Fields Laboratories in London, have still found it difficult to get their results into print. They say that referees for top journals, aware of the difficulties independent groups have had in replicating Verfaillie's results, are now so sceptical that it is hard to publish their findings. Verfaillie's protocol maybe indeed be unusually complicated, but it illustrates a common problem: it is often hard to tell whether an inability to replicate a result is due to a group's failings or a flaw in the original paper. The reason is often the countless tiny details of experimental method that are omitted from the methods sections of papers but can influence results. \u201cThings are different in different labs for very subtle reasons,\u201d says Gillian Murphy, a cell biologist at the University of Cambridge, UK. \u201cThe water can be different. We're about to move labs, and my group is very concerned that delicate cells might hate something in the new pipes.\u201d This issue is not confined to the biological sciences, as Collins's research reveals. In his 1985 book  Changing Order 12 , he quotes a physicist on the difficulties of recreating an exact copy of a piece of experimental kit: \u201cIt's very difficult to make a carbon copy. You can make a near one, but if it turns out that what is critical is the way he glued his transducers, and he forgets to tell you that the technician always puts a copy of  Physical Review  on top of them for weight, well, it could make all the difference.\u201d A paper can never be a foolproof recipe for the replication of its results because this sort of information, which the chemist and philosopher Michael Polanyi called \u201ctacit knowledge\u201d, can never be entirely captured in a scientific paper. It is thus not in principle possible to tell whether a failure to replicate is down to a lack of this tacit knowledge or a flaw in the original result. In practice, researchers compensate by exchanging tips by e-mail and at conferences. Replication is a social phenomenon, which accounts for the interest of sociologists such as Collins. But because the social interactions are not recorded anywhere, it is hard to consult or build on them. \n               Fraud and its fallout \n             This becomes a particularly vexed problem in cases of fraud. Just a few weeks before issue 6893 was published, a scandal hit nanotechnology. Jan Hendrik Sch\u00f6n of Bell Labs in Murray Hill, New Jersey, was one of the brightest prospects in the field, his string of high-profile papers in top journals a remarkable feat for a researcher in his early thirties. But in May 2002 Sch\u00f6n's world began to unravel: people noticed that data in some of his papers had been manipulated. Later that year he was fired, with many of his papers withdrawn as fraudulent. Allen Goldman's lab at the University of Minnesota in Minneapolis was one of many that wasted much time trying to replicate Sch\u00f6n's work \u2014 specifically his findings on the superconductivity of spherical carbon molecules known as buckyballs. A postdoc and two graduate students spent more than a year trying to make the things Sch\u00f6n had described in the papers happen anew in their own lab, convinced that they were failing because they could not replicate Sch\u00f6n's procedures. Others recount similar experiences: \u201cA postdoc of mine burned up a couple of years of his life,\u201d says Robert Dynes of the University of California, Berkeley. The experience was not all bad: Goldman notes that the work they did inadvertently led his group towards more discoveries. He also says he is now far more critical about the papers he reads in journals. But while his team was trying and failing, others were having similarly frustrating experiences \u2014 experiences that were only discussed at meetings and during the odd phone call between friends in rival labs. Had something appeared in the peer-reviewed literature, Goldman says he would probably have realized more quickly that something was wrong. One obvious solution is for journals to publish more papers that describe failures to replicate results. Most top publications have procedures for dealing with data that conflict with previously published results, although they demand that authors amass a comprehensive data set before allowing them to question a published result.  Nature  does this through Brief Communications, and like most journals publishes only a handful a month. Yet few scientists contacted by  Nature  suggested that the journals expand this activity, or lower the thresholds they require for questioning a result. \u201cMost failures to replicate exhibit incompetence,\u201d says Collins, whose feelings sum up those of many scientists. \u201cIt would be misleading to publish each one.\u201d At  Cell , editor Emilie Marcus says she would be reluctant to publish a statement saying that someone had simply failed to replicate a paper. \u201cThorough attempts to reproduce a result should be published,\u201d she says, \u201cIf you want to claim publicly that someone is wrong, that takes a certain degree of evidence.\u201d This leaves editors in a dilemma. The findings in papers are often hard to reproduce. Readers want to know if they should bother trying or instead dismiss the results as flawed. But the data that could help answer that question often lie unpublished in lab notebooks. \u201cRumours go round when part of a study doesn't work in other people's hands,\u201d says Chris Surridge, an editor at the open-access publisher the Public Library of Science in Cambridge, UK. \u201cBut it's very difficult to get the information into the public domain.\u201d \n               Electronic coffee \n             Scientific publishing's move online may be a big help here, opening up new possibilities for sharing and commenting on papers and methods. If these can be harnessed, coffee-break conversations about replication could be shared with the entire scientific community, allowing problems to be cleared up more easily and frauds discovered more quickly. The most obvious first step towards this is simply to allow comments to be posted on published papers. Several publishers, including the open-access journals hosted by BioMed Central, already have this sort of comment function but have found that it is not widely used. That could simply be because the facilities need promoting, as BioMed Central publishers say they are now trying to do. Another reason could be that only the most successful researchers are confident enough to criticize others in this public way, even if journals allow it. PLoS ONE , a new online-only journal from the Public Library of Science, will take the comment model further than anyone else when it launches later this year, with various functions for promoting discussion. Users will, for example, be able to annotate papers, including methods sections, with their own comments. In a bid to tackle information overload, comments \u2014 or perhaps the people making them \u2014 will be rated by visitors to the site. Users will then be able to see only comments above a certain rating \u2014 a system pioneered by Slashdot.com, a technology website. Authors will also be able to correct mistakes or misleading statements, and others can link to improved methods published elsewhere. The aim, says Surridge, who is managing  PLoS ONE  during its launch, is to recreate the kind of discussion that takes place in front of conference posters. To help keep things informal, the publication is considering allowing commenters to use nicknames \u2014 but only if they provide the site with academic bona fides. This is not the only way in which the new journal hopes to shake things up.  PLoS ONE  also has an unusual publication policy: it will not consider the novelty of a result when deciding whether to publish a paper. As long as a result is judged by referees to be methodologically sound, and the author can provide the open-access publication fee, it will be published. That will, says Surridge, make it easier to publish papers that cast doubt on previous results, as well as those that confirm them. Sceptical editors at other journals say it will be hard to attract submissions to a journal that sets such a low bar for acceptance. Referees might, for the same reason, also be reluctant to review papers for the journal. \n               Here's how \n             Another possibility is to pay more attention to the methods sections of papers. The fact that publishers see this as a potentially important market can be judged by the launch in June 2006 of two journals devoted exclusively to publishing experimental protocols. By giving methods sections the same editorial care as a full scientific paper, from peer reviewing to archiving in a dedicated and searchable website, the journals should allow authors to detail the often critical minutiae of their experimental methods. \u201cWhen I started in science I was told that I should be able to repeat an experiment by reading the paper, but that is almost never the case,\u201d says Michael Ronemus, editor of  Cold Spring Harbor Protocols , published by Cold Spring Harbor Laboratory Press. \u201cEditors tend to ignore supplementary information. It doesn't get the same scrutiny.\u201d At the other new journal \u2014  Nature Protocols  \u2014 editors say they will encourage authors to include a troubleshooting section. Both publications will include discussion facilities that will allow researchers to talk to the original authors and other users of the protocol. These could prove ideal places to resolve problems that crop up during attempts to reproduce a previous result. \u201cThe forums will go a long way to resolving conflicts,\u201d says Ronemus. Another possibility is to link to conversation happening elsewhere on the web. ArXiv, a store of online physics preprints now hosted by Cornell University, has from its inception been an online trendsetter. Last August it implemented a 'trackback' function: a system that allows online discussions about a web page, in this case a preprint, to be easily linked to the original page. Following trackbacks from arXiv papers typically leads to physicists' blog posts, in effect opening up a web of discussion that is something between open peer review and a coffee room at a conference. This might be disturbing for authors, who can now see their papers dissected in public, but it is a great way into the community's views on a paper and offers the benefits of informality and even anonymity. \u201cJunior people are often reluctant to have their name attached to negative comments,\u201d says Jacques Distler, a string theorist at the University of Texas, Austin, who helped to develop the trackback function at arXiv. \u201cThey don't know if it's someone they are going to be relying on later for a job.\u201d Many links from arXiv take users to CosmoCoffee, a forum for discussion of cosmology and high-energy physics in general, and arXiv papers in particular. \u201cOur motivation was to make papers more accessible, but it would be absolutely fantastic if it could resolve controversies as well,\u201d says Sarah Bridle, an astronomer at University College London and a co-founder of CosmoCoffee. In October 2004, for example, an intriguing paper appeared in which the authors claimed to have set limits on the mass of the neutrino using data on the cosmic microwave background (CMB) 13 , a faint glow of radiation left over from the Big Bang. The paper raised some eyebrows on CosmoCoffee, as users questioned whether the CMB data could be used in this way. But after a few exchanges, one of which included snippets of new data that had been generated to test out the conclusion of the preprint, users decided that the result made sense. For physicists wondering whether a particular result is robust enough to build upon, such discussion could prove a powerful resource. It is also, unavoidably, open to malicious attempts to undermine a particular researcher. Forty years ago, the Nobel-prizewinning immunologist Peter Medawar declared that all scientific papers were frauds, inasmuch as they describe research as a smooth transition from hypothesis through experiments to conclusions, when the truth is always messier than that. Comments, blogs and trackbacks, by expanding the published realm beyond the limits of the traditional paper, may make the scientific literature a little less fraudulent in Medawar's sense, and in the more general one. They could also help the many frustrated scientists struggling to reproduce claims when, perhaps, they should not be bothering. Replication, for all its conceptual importance, is a messy, social business; it may be that it needs a messy, social medium. \n                     Science in the web age: Joint efforts \n                   \n                     Misconduct finding at Bell Labs shakes physics community \n                   \n                     Stem cell hopes double \n                   \n                     Cloning agenda 'skewed' by media frenzy \n                   \n                     In Focus: Stem Cells \n                   \n                     Content page for 4 July 2002 \n                   \n                     PLoS ONE \n                   \n                     CosmoCoffee \n                   \n                     arXiv physics pre-print server \n                   \n                     http://www.cshprotocols.org/ \n                   Reprints and Permissions"},
{"file_id": "441683a", "url": "https://www.nature.com/articles/441683a", "year": 2006, "authors": [{"name": "Garry Hamilton"}], "parsed_as_year": "2006_or_before", "body": "Viruses are often thought of as simple creatures. But their staggering diversity and genetic promiscuity could make them the most creative force in evolution, says Garry Hamilton. When high-school students Joe Gross and Jake Falbo walked into Graham Hatfull's lab at the University of Pittsburgh five years ago, they had no idea what they were getting into. Hoping for a little hands-on scientific experience, the two were instead invited to participate in a mission that would have excited the likes of Linnaeus or Darwin. \u201cI said to them, \u2018Sure you can do a project\u2019,\u201d recalls Hatfull. \u201c\u2018Why don't you go out and discover some new viruses?\u2019\u201d It was no joke. Gross and Falbo are among the co-authors of a now heavily cited  Cell  paper for their separate identification of two previously unknown viruses called bacteriophages, \u2018phages\u2019 for short, which infect bacteria 1 . But unlike Linnaeus and Darwin, Hatfull's team doesn't have to embark on a far-flung expedition to collect and classify new creatures \u2014 they, and other virologists like them, come across them wherever they go. \u201cAll you need to do,\u201d says Hatfull, \u201cis go and look under a rosebush and you'll find a phage that's unlike anything anyone's ever seen before.\u201d In total, Hatfull and his squadron of phage-hunters have now isolated and sequenced more than 40 phages, from settings as varied as the grounds of a tuberculosis clinic in India, the monkey house at New York's Bronx Zoo and, yes, even beneath a rosebush in Latrobe, 55 kilometres southeast of their Pittsburgh base. They have discovered that every phage they come across is almost completely different from the next. Findings such as these have prompted a huge change in how biologists see all viruses: the spurt has fuelled the suggestion that viruses have a large part to play in the evolution of life on Earth. \n               In from the cold \n             Viruses have long been viewed as nature's outsiders. As parasites that depend on a host cell for survival, they don't seem to have fully earned the stripes of a living organism. But they are actually far more abundant, diverse and complex than once thought. Recent calculations suggest there may be more undiscovered genes in the viral world \u2014 most belonging to phages \u2014 than in all other life forms combined. And this vast presence and diversity has profound effects on the rest of life. By shuttling genes into and out of their hosts, viruses seem to be a major driving force in the evolution of higher organisms. Even within our own genome, genes that came from viruses are hard at work. This in turn has led to the realization that viruses probably play a major role in the ecological, biochemical and evolutionary processes that underlie the entire natural world. Rather than being the outsiders, viruses have emerged as perhaps life's most ubiquitous presence. \u201cI don't think you can look at any system now and leave viruses out of the equation,\u201d says Nicholas Mann, a microbiologist at the University of Warwick, UK. Hints of the vast, unexplored viral world first emerged during the 1980s. Researchers at the University of Bergen in Norway, with the help of a new electron-microscopy technique, showed that viral concentrations in some aquatic habitats were up to 10 million times greater than previous estimates 2 . Viral particles per millilitre ranged from 60,000, deep beneath the Barents Sea, to 254 million, from the surface waters of Germany's Lake Plussee. Since then researchers have discovered huge numbers of viruses wherever they have looked, from 2,000 metres below the surface of Earth to the sands of the Sahara Desert, from acidic hot springs to polar lakes. In total there are now thought to be some 10 31  viral particles on the planet \u2014 an astronomical figure that one researcher recently described as 250 million light years' worth of viral genes laid end to end. Then, in the mid-1990s, researchers started to grasp the dazzling variety that exists within this viral multitude. Biologists discovered that the known and cultured viruses represented just a fraction of their respective groups. From one cubic metre of seawater to the next, there was more genetic diversity in viruses than found in any other known group of organisms. \n               Constant reinvention \n             More recently, researchers led by microbiologist Forest Rohwer at San Diego State University, California, have developed a technique for extracting and sequencing unknown viral DNA en masse. In a series of studies that examined samples from seawater, marine sediment and human faeces, the researchers discovered that most viral groups had previously been completely undetected 3 . A kilogram of marine muck was found to contain up to a million different viral genotypes. In the human gut alone there may be as many as many as 1,200 distinct viruses. \u201cViral diversity is just way different from anything else,\u201d says Rohwer. \u201cEvery time we sequence it, most of everything we run into is unknown.\u201d Hatfull and his colleague Roger Hendrix have made similar discoveries, with help from a team of high-school students and undergraduates. In the 40 or so phages they have found, roughly half of each genome consists of genes that have never been seen before \u2014 not in any of the cellular organisms that have been sequenced to date, not even in any other virus 1 . And a similarly high percentage of unique genes has been found in most other sequenced viral genomes \u2014 there are some 450 in the largest virus, the mimivirus (see  \u2018Hidden talents\u2019 ). This has led to the conclusion that the bulk of nature's genetic information resides in the genomes of viruses. \u201cYou could argue that this is nature's greatest genetic experiment,\u201d says Hatfull. And viruses seem to be truly talented experimentalists. Although phages have long been known to cut and paste their genes together from different sources, Hatfull and Hendrix have shown just how rampant this shuffling is. By making a comparison of 14 different phages, the researchers detected spliced-in sequences that were no more than a single gene in length. Other segments were flanked by fragments of genes, as if they had been arbitrarily ripped from one virus and inserted into another. This suggests that, unlike cells, phages can combine bits of DNA even when there is no similarity between the sequences of the different DNA pieces. Hatfull and Hendrix now argue \u2014 and many others concur \u2014 that viruses are continually and randomly recombining with whatever DNA they might encounter while infecting a cell, be it phage or host DNA. Thus cells are stewing pots where viruses are continually reinventing themselves thanks to a blind but highly creative process that is not only spitting out novel combinations of genes, but is also generating brand new genes possibly never before seen in nature. Success relies on the huge number of new viruses being created in the world \u2014 estimated to be some 10 24  per second. \u201cAlmost all of this would be junk, useless monsters,\u201d says Hendrix. \u201cBut it's happening often enough that the few that survive are still a significant number. It's darwinian evolution on a grand scale.\u201d \n               DNA dispersal \n             Hendrix also points out that the common structure of most phages \u2014 a capsule full of DNA and a hollow tail used for injecting that DNA into a cell \u2014 would be ideal for facilitating the creative process by requiring excess DNA to act as stuffing that enables the head to maintain its shape. Free from the pressures of selection, such non-essential DNA would have an extended opportunity to evolve into something useful. The emerging picture suggests viral DNA can spread far and fast. Two years ago Rohwer and his colleagues reported that they had found the same sequence of viral DNA in 49 of 66 widely varied habitats \u2014 including the rumen of a cow from Idaho, a hot spring in California, the Antarctic Ocean and mucus from coral growing in the Caribbean Sea 4 . Analysis of 18 of these samples showed that the 533-base-pair sequence differed by no more than three nucleotides. Based on known phage mutation rates, this piece of viral DNA seems to have dispersed sometime in the past 2,000 years 5 . It is possible that the same host resides in all of these habitats, or that a single virus can infect a wide range of hosts. But Rohwer and others, who have now found similar results for several sequences, believe it is more likely that fragments of viral DNA are being passed between viruses that share the same host. \u201cWhen you look at a group of viruses, such as the algal viruses, there seems to be a very, very small core of conserved genes,\u201d says Curtis Suttle, a microbiologist at the University of British Columbia in Vancouver, Canada. \u201cThe rest is almost like a super-organism \u2014 a massive pool of genetic information that's being shared among all these different viruses.\u201d A big question now is the degree to which this super-organism extends its tentacles into host genomes. It has long been known that bacteria use genes acquired from prophages \u2014 phages that insert their DNA temporarily or even permanently into the DNA of their host \u2014 to gain competitive advantage and exploit new environments. Indeed, work reaching back decades has shown that prophage genes carried by bacteria are responsible for producing the primary toxins associated with diseases such as diphtheria, scarlet fever, food poisoning, botulism and cholera. \n               Behind bacteria \n             More recently, whole-genome sequencing has shown that most bacteria harbour an average of two or three prophages. Other studies have shown that phage genes make up the major differences between closely related strains of bacteria. In Japan, for example, researchers carried out whole-genome comparisons between a harmless laboratory strain of  Escherichia coli  and O157:H7, the strain of  E. coli  that has emerged in recent decades as a global health threat. They found that most of what separates the pathogenic strain from its close and harmless relative \u2014 almost a million base pairs of DNA \u2014 comes from 24 different prophage or prophage-like genetic segments 6 . A more recent genome comparison between pathogenic and non-pathogenic isolates of  Neisseria meningitidis , a normally harmless nasal-dwelling bacterium that is also known to cause meningitis, found only one noticeable difference between the two groups: a cluster of prophage genes that turned up in 29 of the disease-causing isolates but only 2 of the 20 benign bacteria 7 . This adds to a list of similar studies, such as one involving  Streptococcus pyogenes , a resident of the skin and mouth that can cause a range of ailments including scarlet fever, rheumatic fever and toxic shock syndrome. In the study, researchers demonstrated how strains associated with three different disease processes each have their own distinct cocktail of phage-encoded toxins 8 . But pathogenicity is just one trait that can affect bacterial evolution; scientists are beginning to find evidence that viral influence on the evolution of life may be a more general phenomenon. The bacterium  Pseudomonas aeruginosa , for example, kills its competitors with compounds produced by two modified phage genes 9 . Scientists don't actually have to go as far as deadly bacteria, or even the nearest rosebush, to get new viral DNA. Human beings are full of it. Retroviruses are a type of virus that specialize in attacking animal cells. And roughly 8% of our genome consists of DNA copies of these RNA-based viruses that have incorporated themselves into our genetic make-up. One example is a protein used by viruses when they attach themselves to a host cell at the start of infection: it has has recently been demonstrated to play an active role in binding together cells during development of the placenta 10 . \n               Evolutionary thoughts \n             Phage DNA has also made it into the human genome, despite the fact that phages target bacteria. The source was our mitochondria, the energy-producing capsules found in our cells and thought to be the descendants of a formerly free-living bacterium. During evolution, phage genes from the mitochondrial genome have transferred into our main genome, housed in the cell's nucleus. There, they help copy and express the few bacterial genes still present in the mitochondrial genome. Recently, this same phage DNA has been spotted in various modern bacteria belonging to the group from which mitochondria are thought to have descended 11 . That supports the idea that the gene made its way from virus to bacterium to cell nucleus, where it now plays a key role in the molecular circuitry that drives all multicelled organisms. Some researchers now believe that viruses have been instrumental in assembling the various molecular components that define the cell types associated with life's three domains \u2014 bacteria, archaea and eukaryotes. Luis Villarreal, director of the Center for Virus Research at the University of California, Irvine, even argues that a large portion of what distinguishes humans from chimps is viral DNA. \u201cI think it's become apparent that viruses are involved everywhere,\u201d says Villarreal. \u201cI would argue they are the most creative genetic entities that we know.\u201d \n                     Origins of DNA: Base invaders \n                   \n                     All at sea \n                   \n                     Ecology goes underground \n                   \n                     Human Genome Web Focus \n                   Reprints and Permissions"},
{"file_id": "441567a", "url": "https://www.nature.com/articles/441567a", "year": 2006, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "Costa Rica's flagship conservation institute needs help. Can a new deal with industry save it? Rex Dalton investigates. Along Costa Rica's Pacific coast, scientific explorers are trying to turn over a new leaf for a storied institute \u2014 the National Biodiversity Institute, or INBio. Near a crocodile-infested river in Guanacaste province, an international team searches for bacteria that one day may become a drug or industrial product. Taking advantage of the dry months, they are focusing on a microbe-rich forest zone \u2014 hoping to flip a leaf to find a fungus, either attacker or defender 1 . If they get lucky and isolate a fungus that produces a useful compound, any economic rewards will be shared with INBio, a once-model organization that is struggling to survive. Created in 1989, INBio, based in a suburb of the capital San Jose, became an early symbol for how developing nations might participate sustainably in the biotechnology revolution. World-class researchers joined with Costa Rica's well-trained academics, hoping to save the nation's biodiversity \u2014 4% of the world's total \u2014 by making money from it. In the first years, more than US$4 million in grants flowed from the drug giant Merck 2 . Foundations and other nations added more, with total donations eventually topping $63 million. But no major products emerged. There have been some modest successes, including a couple of industrial compounds and an over-the-counter hangover remedy. More significantly, INBio set itself up as a training ground for marrying traditional conservation values with modern high-tech methods. Today, other developing nations look to INBio as an example of how to achieve the goals of the 1992 Convention on Biological Diversity, which encourages sustainable development worldwide. But INBio's scanty returns, and Costa Rica's limited ability to fund research, has raised doubts about the institute's survival. Its future may depend on an ambitious new bioprospecting plan, funded by the United States, in which scientists will canvas the country for new drug candidates. Others are holding out hope for a far-reaching goal for a $500 million endowment to preserve a quarter of Costa Rica's biodiversity (see  \u2018A fund for the future\u2019 ). \u201cINBio is a high-quality machine with no gasoline,\u201d says Daniel Janzen, an entomologist at the University of Pennsylvania in Philadelphia who has worked in the country for decades and is trying to create the endowment. \u201cThe sooner we can get gas in the system, the sooner it can be cranked up.\u201d \n               Fuel source \n             Advocates hope the fuel for that engine will come from the Fogarty International Center at the US National Institutes of Health, which is giving the new bioprospecting team $3.5 million over four years. The effort is one of several International Cooperative Biodiversity Group (ICBG) projects, designed in part to put a value on preserving biodiversity 3 . Led by chemist Jon Clardy of Harvard Medical School, the five-year project includes researchers from the University of Michigan in Ann Arbor, the Broad Institute \u2014 a joint venture of Harvard and the Massachusetts Institute of Technology \u2014 and the Novartis Institutes for Biomedical Research in Cambridge, Massachusetts. Team members will be happy with whatever they find, but their assays and screens are designed to find compounds that can fight malignancies, infectious agents or neurodegenerative disease. For five years ending in 1997, INBio was involved with a different ICBG, led by researchers from Cornell University in Ithaca, New York. But that programme produced no money-making products, and, in hindsight, some scientists believe its goals \u2014 such as trying to extract useful compounds from insects \u2014 were unrealistic. The current project is designed to learn from the past. In particular, it is structured to be as open as possible about any potential new drug candidates. With the earlier grants, any compounds of interest left Costa Rica, disappearing behind the proprietary walls of corporate science. Clardy, who was part of the earlier ICBG when at Cornell, says he didn't want the new programme making the same mistakes. \u201cI specifically formed it this way,\u201d he says. To Rodrigo Gamez \u2014 a Costa Rican plant virologist who was key to forming INBio and remains president of its governing body \u2014 the experience was invaluable in securing the new pact. \u201cWe developed a capacity to negotiate,\u201d he says. But some question how hard the companies may or may not have tried to develop products in those earlier deals. Merck, for instance, is believed to have found that about 200 substances tested positively in preliminary screens against disease. But Clardy's group thinks the firm did not go far beyond initial screening. \u201cI think they bought good public relations with the grants,\u201d says one informed scientist, privately. Merck declined to respond. Similarly, in a separate deal, Bristol-Myers Squibb is believed not to have pursued promising signals from compounds it may have found in insects. Neither company has said much publicly about results from the tested compounds. \n               Bugged out \n             In the earlier ICBG, scientists from Cornell and INBio set out to learn from the country's insects, which use an array of chemicals for digestion and protection. Insects were a natural choice, given INBio's extensive documentation of the country's butterflies, moths and caterpillars. But the team found insect substances were present only in incredibly small quantities, making the material scarce for studies. And culturing them to produce enough material for lab experiments was onerous or worse. Clardy eventually came to believe that working with insect material was too difficult. \u201cWe are only going to work on compounds that can be easily cultured and duplicated for studies\u201d \u2014 from fungi, leaves, or other sources, he says. In addition, the data will be publicly accessible, in a database containing information such as where the compounds were collected and under what conditions. Clardy foresees an eventual library with some 5,000 to 10,000 compounds collected during the project. The database could even contain details on how compounds respond in various screening tests against pathogens, information that is usually considered proprietary. Clardy's group would get first shot at studying any promising disease-fighting compounds. But eventually the data would enter the publicly accessible ChemBank. The work is possible because academic groups, such as those at the Broad Institute and Harvard's Institute for Chemistry and Cell Biology, are getting hold of the sophisticated and expensive testing equipment that in the early 1990s, when the first projects got under way, was the preserve of drug companies. In Novartis, the project has a partner willing to adapt to the new environment \u2014 and one that apparently still has faith in drug development from natural products, a faith that many pharmaceutical companies have lost. As a team member, Novartis will get first opportunity to run its proprietary assays on promising compounds. But then the firm will have to negotiate agreements with INBio to advance any material to drug stage. \u201cI am very comfortable with this set up,\u201d says chemist Alexander Wood, a Novartis executive director for oncology research. \u201cWe want to have as many opportunities from new compounds as possible. When it comes to design, medicinal chemists can't match the natural process.\u201d \n               Revenue trickle \n             Such a deal might also address the lingering issue of a developing nation sharing in benefits from bioprospecting \u2014 which some activists call biopiracy. Historically, countries housing microbes that led to blockbuster drugs got virtually nothing. The diabetes drug acarbose, for instance, was derived from a bacterium in a Kenya lake; proceeds from its sales, about $380 million in 2004, go entirely to the drug company Bayer, which developed it. Under the diversity convention, a nation such as Costa Rica \u2014 which has adopted the pact \u2014 is to secure some reward from any pharmaceutical proceeds. In the past few years, Costa Rica has adopted and implemented laws specifically to address this access and benefit sharing. Costa Rica \u2014 and in turn INBio \u2014 is among the few developing nations currently receiving license fees from natural products. Diversa, a San Diego-based biotech company, is currently paying Costa Rica nearly $6,000 a year for two products developed from the country's resources. One is DiscoveryPoint, a fluorescent protein used for tagging material in experiments that comes from a marine organism found in the Caribbean Sea. The second is Cottonase, an enzyme for processing raw textile material to reduce the use of harsh chemicals. It was discovered in warm mud in a volcanic area just west of INBio's suburban campus. And INBio's knowledge of plants helped a Costa Rican firm, Lisanatura, develop a treatment for hangovers or indigestion \u2014 Hombre Grande \u2014 from a plant called amargo ( Quassia amara ). Diversa is also persisting with the insect world. Along with the California Institute of Technology in Pasadena and the US Joint Genome Institute in Walnut Creek, California, the company has contracted with INBio staff to probe the guts of Costa Rican termites for useful compounds. The project involves analysing how termites use microorganisms or enzymes to dissolve cellulose. Some termites have 100 species living in their gut 4 . The team isolates organisms or enzymes, sequences key genetic material, and then studies it. INBio's annual operating budget is around US$6 million. About 70% of that comes from grants and contracts, such as those from the ICBG and Diversa. In addition to bioprospecting, INBio has two other divisions \u2014 a nature park, where schoolchildren and others learn about the importance of preserving biodiversity, and its inventory work, largely involving Janzen. With such revenue sources, when grants run out it can mean staff layoffs; late last year, about 15 workers were let go. Those who remain are dedicated. In March, project leaders laid out an ambitious schedule for collecting, culturing and screening compounds. In Palo Verde National Park, they began searching river channels and an estuary on the Gulf of Nicoya. Scooping up bottom silt, they immediately found hair-thin, noodle-like filaments that are a type of cyanobacteria. So far, no drug has come from cyanobacteria. Their history traces back more than two billion years, and today there are hundreds of types that thrive in salt, fresh and brackish waters. Such bacteria are known to host protective molecules, which chemists hope to isolate 5 . Organic chemist David Sherman of the University of Michigan has joined with Jorge Cortes, a University of Costa Rica marine biologist who is an authority on sea fans. Since 2000, Sherman has been diving in oceans from Papua New Guinea in the South Pacific to waters off the Central American isthmus in search of new compounds 6 . Together, he and Cortes explore the environments cyanobacteria thrive in. \n               Local knowledge \n             In December, as part of an ICBG workshop, the pair found cyanobacteria while diving on reefs in the Pacific Gulf of Santa Elena, just south of Nicaragua. \u201cThe diving is very challenging \u2014 low visibility, a pronounced surge, and high surf,\u201d says Sherman. \u201cIt was really great to have a local expert involved.\u201d In the search for fungi on land, the process also involves unravelling the role of microbes. On a field trip this spring, INBio chemist Giselle Tamayo noted areas of interest in trees and shrubs. Two metres up from the ground, the vegetation is rich with fungi. Leaves will be collected and treated with antibiotics at INBio's labs. Then the fungi-bearing material will be cultured for weeks. Periodically, fungi will be removed from the sample vials. \u201cThe last ones to grow are what we are interested in,\u201d says Tamayo. \u201cThese slower growing ones probably have not been described before.\u201d Already, the team is seeing they will need luck. In the laboratory, technicians isolated DNA from water in a Costa Rican bromeliad plant. The DNA segments were then inserted into  Escherichia coli  bacteria, which produced a compound that has shown antibiotic activity in culture tests 7 . \u201cWe did get lucky with that,\u201d says Clardy. \u201cBut it could be nothing, and it could fall apart tomorrow.\u201d The same, they hope, won't happen to INBio. \n                     Diversa restructures, raising question over bioprospecting \n                   \n                     Stiffer rules required to stop commerce milking Antarctica \n                   \n                     Microbe hunt raises doubts over local benefits of bioprospecting \n                   \n                     The curtain falls \n                   \n                     INBio \n                   \n                     Daniel Janzen database \n                   \n                     Josh Rosenfeld article \n                   \n                     Convention on Biological Diversity \n                   Reprints and Permissions"},
{"file_id": "441570a", "url": "https://www.nature.com/articles/441570a", "year": 2006, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "Scientists say they gas mice and rats with carbon dioxide because it is humane. It's also simple, cheap and keeps their hands clean. Emma Marris analyses the final seconds of the lab rodents' life. \u201cOne uses CO 2  to knock the animal out, and it takes a bit longer than you would like \u2014 about a minute,\u201d explains Abigail Witherden, a post-doctoral kidney researcher at Imperial College London describing the death of a mouse. Then, to be sure: \u201cyou break its neck\u201d. This is Witherden's preferred method of execution. \u201cYou feel more brutal if you've got a wriggling, living creature and you break its neck,\u201d she says, \u201crather than something that is probably already dead. It is worthwhile considering that some of these things are done for the benefit of the people, not the mouse.\u201d Somewhere, right now, a lab mouse or rat is meeting its end, almost certainly by being gassed with carbon dioxide. The gas has become the preferred method because it is easy, cheap, and untroubling for the researchers. The rodent appears to drift off to a peaceful death. But some scientists say that CO 2  could cause pain and distress, and that it might be better to use a pricier anaesthetic, or a manual manoeuvre called cervical dislocation that breaks the animal's neck. Tens or perhaps hundreds of millions of laboratory mice and rats are killed each year. The exact number is unknown, as there is no reporting requirement in the United States, probably the heaviest user. In the United Kingdom, roughly 2.4 million rodents were used in experiments in 2004, according to the Home Office, which oversees lab animals 1 . \n               Big sleep \n             Their use is on the rise. \u201cWhile researchers' needs for animal models are declining, the use of rodents is increasing because of transgenic models \u2014 knockout mice and so forth,\u201d says George Goodno, spokesman for the Foundation for Biological Research, a Washington DC-based lobby group for animal research. Scientists can now buy transgenic rodents to order, and their commercial production involves culling those that have the wrong genes, are unhealthy or have become surplus inventory. In typical CO 2  gassing, one or more rodents are placed in a chamber and CO 2  is gradually added. As the CO 2  level rises, it acts as an anaesthetic and eventually knocks the rodent out. Then the lack of oxygen kills it. Adding CO 2  slowly is intended to avoid the pain caused by high concentrations, when the gas dissolves on the mucous membranes and forms carbonic acid. The hope is that the animals are out cold by this stage. In the United States, rodents are sometimes put into chambers that have been pre-filled with CO 2 , which kills them within 40 seconds, but might be more painful. The United Kingdom bans this method. None of these methods is foolproof. The Office of Lab Animal Welfare at the US National Institutes of Health (NIH) in Bethesda, Maryland, gets occasional reports of animals that wake up after being removed from the chamber. It is to avoid this that many labs, including where Witherden works, break the gassed animals' necks. There are rules about how to kill a mouse or rat, but many of the details are left up to the lab. The major guidance documents on killing methods for researchers in the United States, Europe and Japan are all being revised; the use of CO 2  is likely to be a point of dispute. In the United States, researchers using federal funds must follow the 2000 report of the American Veterinary Medical Association Panel on Euthanasia 2 . This report, though, is less of a how-to guide than a summary of the pros and cons of various methods. A few are deemed unacceptable; others, including gassing with CO 2  alone, are ruled as acceptable, because they consistently result in a humane death. Cervical dislocation is ruled as \u201cconditionally acceptable\u201d, partly because it takes some skill. The report also considers the effects of various methods on the researchers who kill the animals \u2014 but also says that \u201cthere are occasions, however, when what is perceived as aesthetic and what is most humane are in conflict\u201d. This February, a group of 34 scientists and those in the lab-animal business gathered at the University of Newcastle upon Tyne, UK, to discuss the possibility that CO 2  is not the most humane killing method. Huw Golledge, a neurophysiologist at Newcastle who helped to organize the conference, says that his work has convinced him that, at least with rats, loss of consciousness can be assured before the CO 2  reaches painful levels, if one adds the gas at less than 30% per minute. But before they pass out, are the rodents gasping for breath and panicking? \n               Breathless \n             \u201cThere is a strong suspicion that CO 2  levels that in no way cause them pain do cause them distress,\u201d says Golledge. Given the choice, rats will rapidly get out of a chamber containing CO 2  concentrations of about 15% or more 3  \u2014 far below the concentration needed to knock them senseless. What convinced Golledge was the work of Robert Banzett at Harvard University, who studies the distress caused by breathlessness, or dyspnoea, the sensation that one cannot breathe properly. \u201cHe has tried to make animal models for his human research,\u201d says Golledge, \u201cso we got him over. My opinion on CO 2  until this meeting was that if you did it right, it was okay. Having heard what Bob Banzett said about dyspnoea, I think we need to clear that up.\u201d Kathleen Conlee of the Humane Society, an animal-welfare organization based in Washington DC, doesn't need any more evidence. She thinks killing rodents with CO 2  alone is cruel, and prefers an anaesthetic gas \u2014 or, in cases where the anaesthetic would interfere with post-mortem measurements, the guillotine 4 . \u201cWe say, if there is a small number of animals, and the person is well trained, use decapitation. If you are doing a large number, use halothane or isoflurane, and then blast them with however much CO 2  you want.\u201d \u201cI feel strongly about it personally,\u201d Conlee adds. \u201cThe number of animals is huge; the evidence is here; the alternatives are here. It is a no-brainer.\u201d But moves away from CO 2  face two barriers: researchers' comfort levels and economics. \u201cIt is so widely used, and so convenient, and people have used it for so long, that it is going to be very difficult to change it,\u201d says David Morton, head of the centre for biomedical ethics at the University of Birmingham, UK, who helped to write a scientific report for the European Commission on the topic. The commission will use the report when revising its directive on lab-animal welfare 5 . The report argued for the use of anaesthesia before CO 2 , saying killing with CO 2  alone \u201cshould be phased out as soon as possible\u201d. But in large operations, setting up facilities for an extra anaesthetic would involve extra costs and regulations. Still, Morton believes it is worth pushing for: \u201cThe ethical mandate for me is that if you can show there is a welfare problem, then you have to do something about it.\u201d Other researchers, though, feel that the literature on CO 2  is too limited and contradictory to start moving away from it. \u201cThere are differences of opinion on the use of CO 2 , and much of this stems from the inconsistency in the literature. There is very little consistency of methodology in these papers,\u201d says B. Taylor Bennett, associate vice-chancellor for research resources at the University of Illinois, Chicago. \u201cWe need research done, but there are very few sources of funds for this type of research.\u201d \n               A good death? \n             Meanwhile, many researchers remain comfortable with using CO 2  alone. \u201cIf it is done properly, I don't have any problems at all using CO 2 ,\u201d says cell biologist Claire Pollock, who, when she began killing mice at the NIH's National Cancer Institute in Bethesda, allayed her fears by researching the science on the subject and throwing herself into her training courses. \u201cI can't stress enough that any technique is only as good as the researcher,\u201d she adds. \u201cI would much rather be put into a CO 2  chamber than be put into the hands of someone who doesn't know what they are doing for cervical dislocation.\u201d Indeed, many researchers use their own intuition to decide what makes a humane death, perhaps because lab rodents can't tell us what they are feeling. And even in aversion studies, rodent behaviour can be hard to interpret. Golledge, for example, says he would prefer slowly rising CO 2  over a dive into a prefilled chamber. Robert Banzett reportedly says he would make the opposite choice. Likewise, alternatives to CO 2 , such as carbon monoxide, are discussed with analogy to humans. Inhaling carbon monoxide is a common method of suicide, largely because it is said to be painless. \u201cAnd,\u201d points out Golledge, \u201cyou hear about people who have a malfunctioning gas boiler, and they just fall asleep [and die] in their armchairs.\u201d Another possibility is argon gas, to which lab rats seem less averse 3 . Argon is popular in the poultry industry and wins support even from animal-welfare campaigners. \u201cIt should be looked into,\u201d says Conlee. \u201cSometimes we hear about concerns with the anaesthetic gas [affecting] personnel, and that wouldn't be a concern with argon.\u201d In any case, all of these lab techniques are undoubtedly more humane than most domestic methods of killing unwanted rodents, from snap traps that sometimes merely paralyze, to sticky traps that capture the mouse, requiring some kind of blunt instrument or drowning. \u201cNo matter what you do, it is going to be more humane than at home,\u201d says Witherden. \n                     American College of Laboratory Animal Medicine \n                   \n                     US Policy on laboratory animals \n                   \n                     Association for Assessment And Accreditation of Laboratory Animal Care \n                   \n                     UK Home Office Code of Practice for the housing and care of animals \n                   Reprints and Permissions"},
{"file_id": "441686a", "url": "https://www.nature.com/articles/441686a", "year": 2006, "authors": [{"name": "Philip Ball"}], "parsed_as_year": "2006_or_before", "body": "Some economists had hoped that physicists might shake up the rigid theories typical of mainstream economics. But so far, they're unimpressed by physicists' handling of the markets. Philip Ball reports. For the past two decades, some physicists have been trying to apply their ideas and tools to an area that seems a long way from traditional physics. They are exploring the notion that there might be a kind of physics of the economy \u2014 an \u2018econophysics\u2019, as it has been dubbed 1 . Last year, some of these econophysicists even went as far as to suggest that economics might be \u201cthe next physical science\u201d 2 . But now this unlikely marriage is showing signs of turning sour. Even those economists who at first welcomed econophysics are starting to wonder whether it is ever going to deliver on its initial promise. Early successes in modelling financial markets have not led to insights elsewhere, some complain. Matters came to a head at the Econophysics Colloquium, held at the Australian National University in Canberra last November. A group of economists attending the meeting were so dismayed with what they saw many physicists doing that they penned a forthcoming paper entitled \u2018Worrying trends in econophysics\u2019 3 . In their critique, economist Paul Ormerod of the London-based consultancy Volterra and his co-authors accuse econophysicists of a litany of sins: applying inappropriate assumptions to economic systems, failing to do their homework properly, getting fixated on a small corner of the subject, and being sloppy with their statistics. At face value it is a damning indictment, and raises the question of whether econophysics will ever make a genuine contribution to economic theory, or whether it is doomed to remain a fringe interest. \n               Claim to blame \n             Some econophysicists admit that there are problems. \u201cEconophysics is a field with very uneven quality,\u201d says Doyne Farmer, a physicist at the Santa Fe Institute in New Mexico, who made pioneering contributions to the study of chaos before moving into economics. Yi-Cheng Zhang of the University of Fribourg in Switzerland is even more ready with a  mea culpa . \u201cMy economist friends are right. The literature is often littered with garbage. We can find gauge-field theories of finance, quantum options and so on. In short, anything goes.\u201d But others reject the accusations. In response to the Canberra critique, Joe McCauley, a physicist at the University of Houston, Texas, who now works mostly on economic problems, says, \u201cWould one write an essay called \u2018Worrying trends in physics\u2019 simply because a few minor researchers put out bad papers? Bad papers, even wrong papers, appear in every issue of every scientific journal.\u201d It is tempting to interpret this as a mere academic turf war. But Ormerod and colleagues are among the few people in economics who have taken econophysics seriously. Most economists don't know the discipline exists \u2014 and if they did, they would probably heap derision on it. The idea that physics might have something useful to contribute to economics arises because both fields are concerned with systems of many interacting components that obey specific rules. Statistical physics describes the behaviour of bulk matter based on the forces acting between atoms and molecules. Economics studies the interactions between economic \u2018agents\u2019 \u2014 market traders, say, or businesses. Arguably, deriving microeconomic principles from the behaviour of individual agents should pose similar problems to deriving thermodynamic laws from interatomic forces. The rules dictating how interactions play out between economic agents are admittedly more complex than the forces between atoms, but in conventional economics the rules have always been grossly simplified to make the models workable. For example, the core theory of mainstream economics, the neoclassical model, argues that agents always act with perfect rationality to maximize their \u2018utility\u2019 (for example, profit), based on complete information about the state of the market as a whole. In this picture, an economic market quickly reaches an equilibrium state, in which commodities find the price that perfectly balances supply and demand. \n               Model world \n             Economists recognize that real human agents do not always act in such a coldly rational way, and that they generally have to manage with incomplete information. But although Nobel prizes in economics were awarded in 2001 and 2002 for work that recognizes these limitations, neoclassical theories \u2014 and particularly the idea of equilibrium \u2014 remain central to mainstream economics. Ormerod and his colleagues, and other physics-friendly economists, had hoped that econophysics would help them create a new economics that is free from some of the dogmatic assumptions characterizing the mainstream discipline today. \u201cEconomics desperately needs econophysics,\u201d claims Ormerod's co-author Steve Keen, an economist at the University of Western Sydney in Australia. Keen had hoped in particular that econophysics might break his fellow economists' misconceived obsession with equilibrium. \u201cEquilibrium thinking still has them in its unshakeable thrall,\u201d he says. A glance at almost any plot of commodity prices over time belies the idea of market equilibrium: the values fluctuate wildly. But in neoclassical theory, these fluctuations are regarded as background \u2018noise\u2019 caused by unpredictable \u2018shocks\u2019 from outside the economic system, to which the market constantly and quickly adjusts. An attempt to explain such fluctuations using statistical physics was made as early as 1900 by Louis Bachelier; he proposed an explanation that introduced the theory of random walks, which was later developed independently by Einstein to explain brownian motion. Bachelier's theory was deemed too strange to be taken seriously by economists. But the issue was revisited in the early 1960s, when mathematician Benoit Mandelbrot showed that fluctuations in cotton prices have a statistical distribution that differs from that expected of a typical gaussian process \u2014 where each event happens randomly and independently of all others. There were more large fluctuations than a gaussian distribution predicts 4 . This has significant implications for economic theories that assume market \u2018noise\u2019 to be gaussian \u2014 but more importantly, it suggests that big fluctuations, perhaps even market crashes, are not rare anomalies but intrinsic to normal market behaviour. Mandelbrot's 1963 paper on price fluctuations is now regarded as one of the key precursors to modern econophysics. \n               Out of equilibrium \n             But it wasn't until the early 1980s, when an unusual mix of researchers got together at the Sante Fe Institute, that economists showed much interest in scientific ideas related to complex systems. They were helped by advances in computing. \u201cOnce we got desktop computers, we could model systems of many agents and allow them rules of behaviour and see how they evolved,\u201d says economist Brian Arthur, who worked at Santa Fe alongside physicists and evolutionary biologists to develop non-traditional approaches to economics 5 . Such computer simulations of the economy led to models of \u2018interacting agents\u2019 6  that were influenced as much by work on cognition and evolutionary biology as by physics. In these models, researchers could give the interacting agents any decision-making strategy they desired, and therefore study markets with different underlying behaviours. \u201cWhat we found was quite surprising,\u201d recalls Arthur. \u201cUnder some restrictive conditions, you get market equilibrium, but under other conditions you get much more complicated outcomes.\u201d It seemed there was no good reason to believe that microeconomics always operates at equilibrium. \u201cThe economy is out of kilter most of the time,\u201d says Arthur. That, he says, accounts for one of the virtues of econophysics. \u201cThe core of economic theory is still built around equilibrium models, but most models in econophysics are non-equilibrium ones.\u201d But those economists who have adopted new approaches such as agent-based modelling have become increasingly frustrated with the intransigence of main-stream economics. Some have even resorted to starting their own publication, the  Journal of Economic Interaction and Coordination , the first issue of which appeared online in May. Zhang says that three of the four authors of the Canberra critique are victims of the intellectual exclusion imposed by mainstream economists. \u201cThat's why they had such high hopes when physicists offered what seemed to be an alternative.\u201d So why have some of these physics-friendly economists become fed up? Although Ormerod and colleagues are highly critical of mainstream economic theory, they point out that \u201ceconomics is not at all an empty box.\u201d The Canberra critique accuses econophysicists of ignoring the existing literature \u2014 a charge also levelled at physicists when they began to dabble seriously in biology. Zhang agrees this is a fair complaint. \u201cMost econophysicists don't care about the underlying idea, history and cultural background to the problem. All they want is to get a lot of data, crunch the numbers better than anybody else, and publish a lot of papers.\u201d But he admits that doing the job properly is very daunting: \u201cCompared to anything I've done in physics, the requisite reading is enormous. I get a pile of books delivered every month, and I'm just overwhelmed.\u201d Another criticism raised by Ormerod and colleagues is that econophysics has too narrow a focus. Physicists have made important contributions to finance and industrial economics, Ormerod acknowledges, but he wants them to bring an open mind to other areas of the economy. \u201cI think they fail to appreciate that financial markets as such are not terribly important in economic theory,\u201d he says. \u201cThey are just a special case of a more general theory of markets.\u201d Why are physicists so finance-centred? Ormerod suspects that they feel more comfortable with the generally high-quality and long-term data from financial markets. Most other economic data exist in small, noisy data sets. Some econophysicists defend the focus on finance precisely for these reasons: \u201cFinance data are strong enough to falsify specific classes of models,\u201d says McCauley. Farmer adds that physics has often been successful by tackling problems that could be solved with the data and the tools to hand. Perhaps more damning, Ormerod and colleagues worry that some econophysicists have imported concepts from physics that just don't belong in economics. For one thing, they say, some have tried to apply conservation laws to quantities, such as money, that the economy doesn't actually conserve. \u201cConservation of money is too far from the truth in a credit economy like ours,\u201d McCauley agrees. But he says that such mistakes are rare. More broadly, claim the Canberra authors, physicists fail to appreciate how diverse and changeable the economy is in practice. \u201cPhysicists suffer from a belief that there must be universal rules,\u201d says Ormerod. \u201cThis is not a handicap in the physical world, but it is in economics, where the behaviour of agents may differ across time and place.\u201d But Farmer thinks that looking for generalities can still be useful: \u201cThe typical view among social scientists is that one should focus on documenting and explaining differences. Physicists have jumped in and said, \u2018Before we do that, let's spend some energy on first trying to understand if there is any way in which everything is the same.\u2019\u201d \n               Out on a limb \n             Economist John Sutton at the London School of Economics agrees that physicists can help to identify relationships within economics that are driven by, say, elementary statistics, irrespective of any assumptions about agent behaviour. He says he welcomes outsiders to economics \u201cwho may look in different places for interesting relationships\u201d. Sutton is, however, a rare example of a mainstream economist who is aware of the econophysics literature. Arthur believes that some shortcomings of econophysicists may be caused by their enforced isolation: \u201cThey tried to publish in mainstream economics journals, but were rebutted. It's a shame and a scandal that the journals haven't opened their pages to them.\u201d Consequently, econophysicists set up their own journals. This means they don't get the contact with economists that would have \u201crounded off their rough corners\u201d, Arthur says. Perhaps not surprisingly, some econophysicists reject the Canberra critique outright. \u201cThe problem is not with econophysics,\u201d McCauley says. \u201cThe problem is within the economics profession.\u201d He argues that the neoclassical model, which is still taught in all economics classes, was falsified in the 1970s by M. F. M. Osborne of the Naval Research Laboratory in Washington DC, who he regards as the first econophysicist (see  \u2018Is econophysics really new?\u2019 ). \u201cEconomists are overwhelmingly unaware of this contribution. Worse, the majority have no interest in anything that physicists write.\u201d Arthur counters that economics has a long history of assimilating ideas from outside the field. But he explains that while economics tolerates some \u2018unorthodoxy\u2019, it also isolates it from the core theory that is taught to students and practised by academics. There is an attitude of \u201cfine, but not in my journal,\u201d he says. Despite their concerns, the authors of the Canberra critique still hold out hope for econophysics. \u201cIt's just that we don't want it going off the rails,\u201d says Ormerod. Yet for McCauley, the goal of econophysics is not to batter its way into the citadel, but to raze and rebuild it. \u201cEconophysics will displace economics in both the universities and boardrooms,\u201d he says, \u201csimply because what is taught in economics classes doesn't work.\u201d Still, if econophysics is to survive and prosper, it needs support from somewhere, says Farmer. \u201cIt is clear that this is not going to come from economists.\u201d Turning to the physics community for help has not been the answer either. Rosario Mantegna, an econophysicist at the University of Palermo in Sicily, says that in academia physicists cannot pursue econophysics as their main research. \u201cThere is no one in the United States who has gotten tenure based on work in econophysics,\u201d Farmer agrees. \u201cThis is a real pity for physics.\u201d \n                     Simple sounds make for sound investments \n                   \n                     For good or evil \n                   \n                     Cards deal blow to market theory \n                   \n                     Economics Nobel 2001 \n                   \n                     Stock market shock explained \n                   \n                     Stockbrokers may act like sheep \n                   \n                     Physica A \n                   \n                     Journal of Economic Interaction and Coordination \n                   \n                     Econophysics database \n                   \n                     Volterra/Paul Ormerod \n                   \n                     Doyne Farmer's web site \n                   \n                     Joe McCauley's web site \n                   \n                     Brian Arthur's web site \n                   \n                     Santa Fe Institute economics program \n                   \n                     2005 Econophyics Colloquium \n                   \n                     Steve Keen's homepage \n                   Reprints and Permissions"},
{"file_id": "441802a", "url": "https://www.nature.com/articles/441802a", "year": 2006, "authors": [{"name": "Gabrielle Walker"}], "parsed_as_year": "2006_or_before", "body": "Could climate change run away with itself? Gabrielle Walker looks at the balance of evidence. \u201cBe worried.  Time  magazine advised those looking at its 3 April cover showing a forlorn polar bear surrounded by puddles of melted ice. \u201cBe very worried.\u201d Immediately below came the occasion for such alarming counsel: \u201cEarth at the tipping point.\u201d The idea that passing some hidden threshold will drastically worsen man-made climate change has been around for decades, normally couched in technical terms such as \u2018nonlinearity\u2019, \u2018positive feedback\u2019 and \u2018hysteresis\u2019. Now it has gained new prominence under a new name. In 2004, 45 newspaper articles mentioned a \u2018tipping point\u2019 in connection with climate change; in the first five months of this year, 234 such articles were published. \u201cWarming hits tipping point,\u201d one UK newspaper recently warned on its front page; \u201cClimate nears point of no return,\u201d asserted another. The idea is spreading like a contagion. The infectious analogy is appropriate. When the writer Malcolm Gladwell unleashed the idea of tipping points on the popular imagination in his book of the same name 1 , he was comparing the way aspects of life suddenly shift from obscurity to ubiquity to effects normally studied in epidemiology. Gladwell's tipping points were manifestations of the catchiness of behaviours and ideas. The notion that climate change is getting out of control is catchy, and it has caught on in academic papers and political debates as well as headlines. But is the climate really on the point of tipping over into a radically different state? And if so, what are the implications? A tipping point usually means the moment at which internal dynamics start to propel a change previously driven by external forces. The idea raises two questions. First, when will that moment be reached? Second, after it has been passed, is the system now destined to run its course regardless of what goes on elsewhere \u2014 is a tipping point a point of no return? Although there's no strong evidence that the climate as a whole has a point beyond which it switches neatly into a new pattern, individual parts of the system could be in danger of changing state quickly, and perhaps irretrievably. And perhaps the most striking of these vulnerable components are in the Arctic. Farthest north is the carapace of sea ice over the Arctic Ocean. South of that is the vast ice sheet that covers Greenland. And then there is the ocean conveyor belt, which originates in a small region of the Nordic seas and carries heat and salt around the world. \n               On thin ice \n             All three seem to have inbuilt danger zones that may deserve to be called tipping points. And the outside forces pushing them towards those points are gathering. \u201cThere is near-universal agreement that we are now seeing a greenhouse effect in the Arctic,\u201d says Mark Serreze from the US National Snow and Ice Data Center in Boulder, Colorado. Serreze studies sea ice, the member of the arctic triumvirate that has had most recent attention. In the winter, sea ice more or less covers the Arctic Ocean basin. Summer sun nibbles at the pack ice, shrinking it at the edges and creating patches of open water within. Open water reflects much less sunlight than ice \u2014 it has what is known as a lower albedo \u2014 so the greater the area of dark open water, the more summer warmth the ocean stores. More stored heat means thinner ice in the next winter, which is more vulnerable to melting the next summer \u2014 meaning yet more warmth being stored in the open water in the following year, a cycle known as the \u2018ice\u2013albedo feedback\u2019. \u201cOnce you start melting and receding, you can't go back,\u201d says Serreze. It seems that some of this process is under way. Serreze and his colleagues have found that the summer sea ice has shrunk by an average of 8% a decade over the past thirty years 2 . The past four years have seen record lows in the extent of September sea ice, and in 2005 there was 20% less ice cover than the 1979\u20132000 average, a loss of about 1.3 million square kilometres, which is more than the area of France, Germany and the United Kingdom combined. It was this finding that triggered a raft of alarming headlines. The ice's volume, rather than its extent, would be a more useful figure, but this is hard to estimate. Radar measurements showing how proud the ice sits with respect to nearby water would help, but the European Cryosat mission intended to provide these data was lost on launch in October 2005. A reflight is planned, but at present the only way to determine the pack thickness is from below. In 2003 Andrew Rothrock and Jinlun Zhang of the University of Washington in Seattle analysed results from a series of submarine cruises from 1987\u201397 and concluded that the ice thinned by about one metre during that period 3 . \n               Flaming January \n             A natural swing in wind and weather known as the Arctic Oscillation may have played a key role in the decline.In 1989, this index began to approach its positive mode,in which a ring of strong winds circles the pole. Zhangand his colleague Roger Lindsay, also at the University of Washington, believe these winds flushed large amountsof thick ice out of the Arctic through the Fram Strait, eastof Greenland. Last year, they published a model suggesting that because the replacement ice was thinner and morevulnerable to the ice\u2013albedo feedback, this extra loss pushed the Arctic over the edge. Their paper's title: \u201cThe thinning of Arctic Sea Ice, 1988\u20132003: Have We Passed a Tipping Point?\u201d 4 . But given that sea ice was disappearing even before the Arctic Oscillation lurched into its positive state, it is unlikely to have been the sole trigger. \u201cThe Arctic Oscillation was a strong kick in the pants,\u201d says Serreze, \u201cbut if we hadn't had it we would still have seen the ice loss.\u201d Whatever the precise mechanisms, the decrease in ice seems to be warming the atmosphere, as heat pours from the open water into the air above it. Springtime temperatures began rising throughout the Arctic basin in the 1990s 5 . This year, the Arctic archipelago of Svalbard experienced a remarkable heatwave. January was warmer than any previously recorded April, and April was more than 12\u00b0C warmer than the long-term average. Lindsay and Zhang suggest that the ice\u2013albedo effect has indeed passed a tipping point, with the internal dynamics more important than external factors. But neither observations nor models suggest that the effect will now run away without outside help. According to climate modeller Jason Lowe of the UK Met Office in Exeter, the relationship between sea ice and temperature is reassuringly linear. \u201cWhen you plot sea ice against temperature rise, whether from observations or models, it forms a remarkably straight line,\u201d he says. \u201cIt's not a runaway effect over the sorts of temperature ranges that we're predicting here.\u201d Lowe says that although the planet will almost certainly lose more ice, it does not have to lose it all. But if current trends in greenhouse-gas emissions and global warming continue, a planet that used to have two permanent polar caps will have only one. Losing the sea ice would be bad news not only for polar bears and other charismatic megafauna, but also for some of the Arctic's smaller inhabitants. Photosynthetic plankton that live in pores and channels within the ice are the foundation of the area's food supply, and are not well adapted to ice-free life. Open-ocean plankton might benefit, but the Arctic is so poor in nutrients that this would probably not be much compensation 6 . \n               Change of winds \n             Compared with the overall scale of human-induced climate change, the additional warming expected if the ice\u2013albedo feedback goes all the way would not be immense. The 4.5% of the Earth's surface above the Arctic Circle is simply too small to make a radical difference to the planet's energy balance. There are, however, some hints that the loss of sea ice may have more far-reaching effects beyond the simple number of watts absorbed per square metre. Tim Lenton, an Earth-systems scientist at the University of East Anglia in Norwich, UK, points out that our current, relatively stable pattern of winds, which is caused by three circulatory air systems in each hemisphere, depends in part on a white and cold North Pole. Sinking air in the Arctic is an integral part of an air system called a Hadley cell; there is another Hadley cell over the tropics. Between these two cells are the fierce westerlies and the high-altitude jet streams that drive storms around the middle latitudes. \u201cIf any part of the current structure broke down, that would be profound,\u201d says Lenton. \u201cIf the system starts to switch seasonally between three cells and a less stable structure, you change the position of the jet streams, you change everything.\u201d Models of this possibility are scarce, but Jacob Sewall and Lisa Sloan of the University of California, Santa Cruz, have shown that an ice-free Arctic could shift winter storm tracks over North America, drying the American west 7 . The local warming caused by less sea ice could also affect the second tipping point, the size of the Greenland ice sheet. Here the effects could be dramatic, although delayed by centuries; there is enough ice on Greenland to raise sea levels by seven metres. \u201cAfter hurricane Katrina, the deepest water in New Orleans was six metres,\u201d says glaciologist Richard Alley from Penn State University. \u201cGreenland is more than that for all the coasts of the world. Do you move cities, do you build seven-metre walls and hope they stay, or what?\u201d Until recently, nobody had painted a convincing portrait of how Greenland is responding to Arctic warming. A glacier here may recede while one over there grows; ice may be accumulating inland and eroding near the coast. But in the past couple of years, almost all of the indicators have started to point in the same direction. Greenland is melting. \n               Cracking up \n             Although satellite measurements of Greenland's interior suggest that snow has recently been accumulating there, the margins are receding 8 . Laser measurements taken from planes suggest that this coastal melting is probably enough to outweigh the build-up of snow inland 9 . Also, Greenland's glaciers seem to have been speeding up. A few months ago, Eric Rignot of NASA's Jet Propulsion Laboratory in Pasadena and Pannir Kanagaratnam of the University of Kansas, Lawrence, published satellite evidence that between 1996 and 2000, Greenland's more southerly glaciers had begun to accelerate, and that by 2005 the northerly ones had followed suit 10 . They estimate that over the past decade this lurching has more than doubled Greenland's annual loss of ice, from 90 to 220 cubic kilometres per year. \u201cIn the past decade there has been a lot of warming,\u201d says Alley. \u201cThere's plenty of room to argue whether that's a natural fluctuation or not, but there's a clear relation between Greenland getting warmer and Greenland getting smaller.\u201d Modelling by Jonathan Gregory from the University of Reading and his colleagues suggests that it would require an average warming worldwide of 3.1 \u00b0C to drive this shrinking to its ultimate conclusion of an ice-free Greenland 11 . This climatic point of no return is around the middle of the range foreseen by the Intergovernmental Panel on Climate Change, but is higher than a previous estimate made by the same group 12 . Their revision is a measure of how quickly the field is changing. \u201cIt's not just Greenland that is going fast,\u201d says Alley. \u201cThe rate of publications, the rate of new papers, and the rate of disagreement have multiplied amazingly.\u201d But these models do not take into account the dynamism of Greenland's glaciers. In 2002 Jay Zwally from NASA's Goddard Space Flight Center in Greenbelt, Maryland, found that as soon as summer meltwater appeared on the surface of west-central Greenland, the ice began to slip more quickly 13 . This is surprising, as slip rates should depend on processes at the base of the ice rather than at its surface. But Zwally points out that the great lakes of water produced by the melting could slip down conduits in the ice and be delivered directly to the bed. This result doesn't necessarily make a big difference to the fate of Greenland, as the increase in the ice's speed was relatively small. But it points to a new way in which the ice sheet could react to climate change quicker than anyone had realized. \u201cIn places inland where the ice is frozen to its bedrock, if you warm the surface and wait for heat to get conducted to the bottom it takes 10,000 years,\u201d says Alley. \u201cBut if you send water down through a crack it takes maybe 10 minutes, maybe 10 seconds.\u201d If this process started to move inland, even the interior of Greenland's ice sheet could be vulnerable to warmer air. That could point to the sort of self-sustaining feedback that tipping points are made of. \n               Getting fresh \n             The models don't incorporate this mechanism, because they can't. The cliff fronts of many Greenland glaciers are shot through with bright blue conduits, but nobody knows how widespread these veins are inside the ice. Still, the responsiveness of Greenland's glaciers makes that point-of-no-return figure of 3.1 \u00b0C even less comforting. What's more, a lot of damage can be done without losing all of the ice. The ice sheet did not vanish during the last interglacial, around 130,000 years ago, when temperatures in the north were a few degrees higher than they are today. And yet the latest analyses suggest that meltwater from Greenland increased the sea level by between two and three metres. The only good thing about such an increase is that it would take centuries. As with the melting of Arctic sea ice, the melting of the Greenland ice sheet has implications for its neighbours. The third tipping point is the origin of the great oceanic conveyor belt, or thermohaline circulation. Thanks to its cold temperatures and high salinity, water in the Nordic seas between Greenland and Scandinavia is unusually dense and sinks. Surface water is drawn northwards to replenish this. One result of this flow is that Britain is warmer than its latitude would seem to deserve. The sinking process sets a global mass of water in motion, transporting vast amounts of heat around the oceans. In the 1980s, models began to suggest that melting ice in the north could weaken this system, by putting a plug of fresh water over the sinkhole. This led to fears of abrupt climate change and snap ice ages in Europe and eastern America. These days most scientists think that the power of this flow to affect European temperatures under current conditions, or in a globally warmed future, has been overestimated 14 . But changes in the system could still have far-reaching implications. And models suggest that the thermohaline circulation has its own tipping point. Comparing the output from 11 different ocean and climate models, ocean modeller Stefan Rahmstorf from the Potsdam Institute for Climate Impact Research (PIK), Germany, has concluded that it would take between 100,000 and 200,000 cubic metres of fresh water per second to shut down the thermohaline circulation \u2014 similar to the outflow from the Amazon River 15 . And once the circulation is stopped, restarting it would take a lot more cooling than just reversing the system into the conditions in which it was previously working. \n               Dry Asia \n             The good news is that although the Arctic does seem to be getting fresher, it is nowhere near the danger point. Add together the increased output from disappearing sea ice (which moves fresh water from the point where sea water freezes to the point where the ice melts), the melting of Greenland and increased Arctic river flow and you still have barely a quarter of the lower bound of the model threshold. However, measurements of flow in the deep ocean suggest that the circulation might be fluctuating in ways not considered by the models 16 . And if the melting of Greenland were to gather pace, the thermohaline circulation would be vulnerable. If the lower bounds of the models turn out to be right, a rate of melting that would get through the ice in 1,000 years would trouble the ocean overturning in centuries. \u201cThe fate of the thermohaline circulation will be decided by Greenland,\u201d says Rahmstorf. \u201cIf that goes quickly it will be bad news for the deep-water formation. But if Greenland is stable, the risk of shutting down the circulation completely is very small.\u201d Any such shutdown would probably have only a small effect on European temperatures. But thanks to the Coriolis effect, says Rahmstorf, such a large shift in the ocean circulation would redistribute sea water so that the North Atlantic rose by up to a metre 17 . There are also suggestions that Atlantic fisheries could collapse. But the biggest danger would come farther south. In the past, similar changes in ocean circulation seem to have led to significant shifts in tropical rainfall. \u201cIf you switch off the thermohaline circulation, the tropical rainfall belts shift. All the models show this. It's quite simple robust physics,\u201d says Rahmstorf. General circulation models, which try to simulate the workings of the climate system as a whole, often including the ocean, predict at least some weakening of the thermohaline circulation by the end of the century, with a knock-on effect on tropical rainfall \u2014 the system that provides much of Asia with food. And as with Greenland, the change doesn't have to be complete to have consequences. \u201cJust weakening the system is by no means harmless,\u201d says Rahmstorf. \u201cYou'd get the same pattern of effects as for a total shutdown, but just a smaller amplitude.\u201d What insights do these three examples provide into tipping points more generally? One is that they are hard to predict, because they often depend on phenomena too subtle or small to be captured in climate models \u2014 the effect of wind patterns on sea ice, or the flow of water through ice-sheet cracks. And bear in mind that the members of the Arctic triumvirate are in principle pretty simple, in that they depend for the most part on physics alone. Possible tipping points in which biology too plays a role \u2014 for example, the potential die-back of the Amazon forests if their area diminishes beyond a certain threshold \u2014 are even harder to get a grip on. As a result, few researchers are prepared to put a number on how much warmer we could allow the climate to get without endangering humans. \n               Human factors \n             Another insight is that points of no return may not be particularly important. Large-scale melting in Greenland is a serious issue over centuries regardless of whether it goes all the way. And the question of whether sea ice would continue to shrink without global warming is academic. The current level of greenhouse gases ensures that the world will continue to warm over the next decades, and the current structure of the world economy ensures that, over that time, there will be further increases in the greenhouse-gas level. The question is what will be done about it, and how soon. As Alley says: \u201cThe human tipping points are probably more important than the natural ones. It's at what point the situation becomes intolerable to us that matters.\u201d The message from the Arctic is that there is still time to avert the worst potential consequences of the nonlinearities in our climate system. Although it is probably too late to stop a serious decline in sea ice, the other two more powerful members of the Arctic trinity look to be some way off their danger points. And unlike the thermohaline circulation, and perhaps the Greenland ice sheet, the change in sea ice does seem to be reversible. If disappearing ice and dying polar bears can tip public opinion over its \u2018be very worried\u2019 threshold into the realms of greater action, then further tipping points in the human world might have their own, positive, role to play. Ottmar Edenhofer, an economist at PIK, found that in some economic models of responses to climate change, increasing carbon prices encourages renewable energy. Above a specific threshold, even if the price of carbon drops, the advantages of renewables have become irrevocable and the move away from fossil fuels continues 18 . \u201cOur task is to find a way to kick the economic system into a new equilibrium and we can use the tipping points of the market to achieve that,\u201d says Edenhofer. \u201cTipping points are part of the problem, but they could also be part of the solution.\u201d \n                     Climate Change: The Arctic tells its story \n                   \n                     Science in culture: Artists on a mission \n                   \n                     Arctic ecology: On thin ice \n                   \n                     Climate change: A sea change \n                   \n                     Nature insight on climate and water \n                   \n                     Nature web focus on ice cores \n                   \n                     The Cryosphere Today \u2013 sea ice extents and more \n                   \n                     Potsdam Institute for Climate Impact Research \n                   \n                     Articles on the Arctic from the Real Climate blog \n                   \n                     BBC blog by Gabrielle Walker and colleagues \n                   Reprints and Permissions"},
{"file_id": "441564a", "url": "https://www.nature.com/articles/441564a", "year": 2006, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "Scientists and policymakers are battling over whether global warming is making hurricanes more destructive. Alexandra Witze ventures into the heart of the storm. It remains the worst tempest in written history. In November 1970, a tropical storm strengthened into a cyclone in the Bay of Bengal. It slammed into what is now Bangladesh, killing up to half a million people. But just try digging up specific data about it. You won't find it. In the database of Indian Ocean storms, the wind speeds for this particular cyclone are listed as zero. For storm researchers, this is a wearily familiar tale. The historical data on tropical cyclones are notoriously patchy. Some gaps are down to irregularities in the measurements collected during past storms. Others are caused by plain old human error. The Bangladesh cyclone, for example, happened during a period when databases from two government agencies were being merged and data were lost in the transition. Today, increasingly sophisticated satellite imagery makes gaps less likely. But researchers still wrestle with the imperfect historical record \u2014 especially if they are trying to understand whether global warming is making cyclones more intense. Some argue that such a claim is hard to prove if major storms of the past aren't on record. As the 2006 hurricane season gets under way in the Atlantic basin, few issues could be hotter than the relationship between global warming and tropical storms. Forecasters predict that things won't be as bad as 2005, which saw a record 28 named storms in the Atlantic and probably more than US$100 billion in damages (see  \u2018The season ahead\u2019 ). But authorities are looking to scientists to tell them whether 2005 is an example of a hurricane season that we will have to get used to. At first glance, a link between cyclones and global warming seems to makes sense. Tropical cyclones are born over the oceans, where masses of rotating air pick up ever more energy from warm surface water. Once the winds in the mass reach 33 metres per second, a tropical cyclone is born. In the northwest Pacific, it's called a typhoon; in the Atlantic and northeast Pacific, a hurricane; elsewhere, a cyclone. But only recently have scientists come up with the data that suggest global warming makes cyclones more intense. Two major studies laid the groundwork last year. In the first, published in August, atmospheric scientist Kerry Emanuel proposed that hurricanes had grown more intense over the past 30 years, most likely because of increasing sea surface temperatures 1 . Emanuel, of the Massachusetts Institute of Technology, developed an index to describe how destructive a storm could be, and found that the wrecking power of storms correlated strongly with sea surface temperature. \n               Tempests of doom \n             The second paper 2  came in September, soon after Hurricane Katrina had killed more than 1,800 people along the US Gulf Coast. A team led by Peter Webster, of the Georgia Institute of Technology in Atlanta, studied the occurrence of storms rated at the higher end of a strength-categorization scale called the Saffir\u2013Simpson scale. Hurricanes are ranked from 1 to 5 on the scale: storms with wind speeds reaching 33 metres per second are at the low end of category 1, and the threshold wind speed for a category 5 storm is 67 metres per second (or 241 kilometres per hour). Hurricane Katrina was category 5 when over the Gulf of Mexico, and had weakened to category 3 when it slammed into the Gulf Coast. Webster's team reported that there has been a rise in the number of category 4 and 5 storms in the past 35 years, in nearly all of the world's ocean basins 2 . Together, the Emanuel and Webster papers kick-started fresh efforts in a previously obscure corner of meteorology. A veritable flood of findings has emerged; some preliminary work was presented in April at a meteorology meeting in Monterey, California 3 . \u201cWe've moved forward immensely since last June,\u201d says Greg Holland, a co-author on the Webster paper and a meteorologist at the National Center for Atmospheric Research in Boulder, Colorado. Yet many research areas remain untapped. One major unknown, experts say, is how hurricanes interact with the ocean \u2014 not just form above it. \u201cRight now, almost everyone attacks the problem as hurricanes responding passively to climate change,\u201d says Emanuel. \u201cThey are active players.\u201d Hurricanes leave a trail of cold water in their wake \u2014 which is not currently accounted for in most climate models. Other researchers point to the need to better understand the factors affecting hurricane intensities. They hope that data from last year's \u2018hurricane-hunter\u2019 flights will help 4 ; these involved forecasters flying into the heart of Atlantic hurricanes to find out what drives changes to their intensity. And mysteries still surround the issue of how hurricanes form in the first place. Indeed, one speaker, David Nolan of the University of Miami, Florida, drew a crowd in Monterey with his provocatively titled talk, \u2018Could hurricanes form from random convection in a warmer world?\u2019 (His answer: \u2018no\u2019.) Given that more research is obviously needed, how should scientists best direct their efforts to get useful answers as soon as possible? Many echo the adage that the past is the key to the present, and argue that far more time and money need to be spent on paleotempestology \u2014 the study of past hurricanes, as recorded in geological deposits. Studies that look back as far as several thousand years ago could help resolve the frequency with which hurricanes form, or at least make landfall in certain regions of the world. These would provide an invaluable measure of \u2018normal\u2019 patterns against which to weigh modern trends. Perhaps most crucially, meteorologists say, the renewed interest in hurricanes could inspire researchers to work on improving the historical record of storms. It is possible to go back through the database and re-assess each storm with modern eyes, making sure its strength and trajectory are analysed by the same standard as more recent storms. That's what Christopher Landsea, a meteorologist at the National Hurricane Center in Miami, has been doing for the Atlantic hurricane database, which contains measurements on storms dating back to 1850. \n               Elusive evidence \n             Re-analysing past measurements is one thing. But the biggest problem, says Landsea, is in having to work with a lopsided data set. If you knew what was in sausages, you wouldn't want to eat them, he says; likewise the historical record shouldn't be trusted. For instance, Hurricane Wilma garnered headlines last summer when it was recorded to have the lowest central pressure \u2014 another measure of storm intensity \u2014 of any known hurricane in the Atlantic basin. Yet Wilma \u201cwas sampled just about every hour of its existence\u201d, says Landsea. Compare that, he says, to a tropical storm such as Carol, which moved up the US eastern seaboard for days in 1954 but was sampled only seven times over its lifetime. The picture gets even bleaker in the world's other ocean basins. In a recent informal study, Landsea looked through satellite images of storms in the northern Indian Ocean. From these pictures, he estimated that the storms should have been rated as category 4 or 5; they were recorded as being of lower intensities at the time. Landsea says that if these storms are missing from the records, how is it possible to conclude that hurricane intensities are increasing because of global warming? They could be, he says; it's just impossible to tell. Webster disagrees: \u201cChris has found several category 4s and 5s we missed in the early 1970s; he has to find 152 for us to be wrong.\u201d And Emanuel adds that, although one can argue about the particular number of hurricanes in a particular year, his measure of hurricane intensity still correlates strongly with sea surface temperature \u2014 no matter how many storms there are in a particular year. \n               Divided opinion \n             The disagreement echoes deep battle lines between several camps, many of which have been re-ignited by the recent studies. A flurry of critiques has appeared in  Science  and  Nature , as well as in the blogosphere. The debate has got personal at times, and few are happy about it. \u201cThe name-calling has got to stop,\u201d says Max Mayfield, director of the National Hurricane Center in Miami. In one recent paper, longtime climate-change sceptic Patrick Michaels and colleagues argue that rising sea surface temperature no longer affects the intensity of a hurricane once its winds have reached speeds of more than 50 metres per second 5 . In another, Philip Klotzbach of Colorado State University in Fort Collins writes that there is no strong correlation between hurricane energy and sea surface temperature in most of the world's ocean basins \u2014 and that Webster's and Emanuel's results are due mostly to the patchiness of data sets prior to the mid-1980s (ref.  6 ). And the  Bulletin of the American Meteorological Society  has hosted a feisty back-and-forth, spearheaded by policy expert Roger Pielke Jr of the University of Colorado. He calls links between hurricanes and global warming \u201cpremature\u201d. For many, the stakes could not be higher. Knowing where and how often storms might strike is crucial for shaping government policies. Exploding populations in coastal zones place ever-greater numbers of people at risk \u2014 a fact noted by some policy experts, who say that the apparent increase in hurricane destructiveness seen in the past few years is down to the fact that more people are living in at-risk areas 7 . Preliminary studies by other groups seem to bear out Webster's and Emanuel's conclusions. A new study of Indian Ocean hurricanes, presented at the Monterey meeting, suggests that there has indeed been an increase in category 4 and 5 storms in the region \u2014 and few Indian Ocean storms are missing from the database. And using a data set of global storms that occurred between 1958 to 2001, scientists from Purdue University in West Lafayette, Indiana, have found the same overall increase in storm destructiveness in recent years \u2014 particularly after 1985 (ref.  8 ). \n               Worse to come \n             Other scientists are turning to computer models for possible answers to questions, such as how much will sea surface temperature rise, and exactly how will that influence hurricane formation? Computer models suggest that sea surface temperatures in the Atlantic hurricane-forming region could warm by 2 \u00b0C by 2100. They also suggest that if this rise occurs, maximum wind speeds could increase by 6% (ref.  9 ). It may not sound like much, but damage from hurricanes rises in proportion to the cube of the wind speed. So far, the world's oceans haven't seen anything close to a 2 \u00b0C warming \u2014 just a 0.5 \u00b0C rise since 1970. \u201cThe warming we've seen to date is really just the tip of the iceberg,\u201d says Thomas Knutson, a climate modeller at the Geophysical Fluid Dynamics Laboratory in Princeton, New Jersey. Predicting future hurricane activity will also require greater understanding of how natural climate fluctuations interact with global warming. For example, the El Ni\u00f1o Southern Oscillation, a pattern of temperature fluctuations in the tropical Pacific ocean, can affect the formation of hurricanes in certain regions, as can volcanic eruptions. For people living in vulnerable coastal regions, answers to the debates can't come soon enough. In the United States, last year's destruction has prompted a new round of calls for hurricane preparedness and mitigation measures. New Orleans, the city devastated by Katrina, is slowly being rebuilt with higher levees to keep storm surges out. And while researchers argue over the details of databases and data analysis, residents are girding for another six months of uncertainty. \n                     Tempers flare at hurricane meeting \n                   \n                     Storms get fewer but fiercer \n                   \n                     Trouble brews over contested trend in hurricanes \n                   \n                     National Hurricane Center \n                   \n                     Joint Typhoon Warning Center \n                   \n                     Central Pacific Hurricane Center \n                   Reprints and Permissions"},
{"file_id": "440597a", "url": "https://www.nature.com/articles/440597a", "year": 2006, "authors": [{"name": "Kendall Powell"}], "parsed_as_year": "2006_or_before", "body": "The two Roger Pielkes can be obstructionist pains in the neck, say their colleagues. So why is this likeable father\u2013son pair such a welcome addition to the debate on global climate change? Kendall Powell clears the air. Roger Pielke Senior and Roger Pielke Junior share a name, a profession and a reputation. Both are mathematics-trained history buffs. Both ski and play golf as part of their active Colorado lifestyles. And both are prominent scholars in the highly polarized field of climate science, where their name can provoke much eye-rolling. The elder Pielke, 59, is professor of climatology at Colorado State University and the state's official climatologist. The younger Pielke, 37, is an expert in science policy at the University of Colorado, with a bumper sticker that declares \u2018Question Predictions\u2019 in his office. Father and son share a proclivity for contentious, if polite, debate, and they both antagonize their colleagues more often than their affable exteriors would suggest. Yet there are notable differences. Pielke Sr is a true climate hound, steeped in decades of research on atmospheric science. By contrast, Pielke Jr is a self-described policy wonk, who claims he simply hasn't inherited his father's obsession with the weather. Junior does, however, have the famous Pielke tenacity, and has put it to use in the world of science policy. He caught the bug after interning on Capitol Hill in 1991, when his adviser Rad Byerly became the chief of staff for the House Committee on Science. Pielke Jr then returned to the University of Colorado in Boulder to finish his master's degree, with a thesis that calculated the true cost of a space shuttle launch. He concluded that each launch cost just over $1 billion, contrasting with NASA's estimate of $400 million 1 . Shortly after his numbers appeared in a 1993 article in  The New York Times , Pielke Jr took a call from an official at NASA's Johnson Space Center, who asked him to retract his conclusions about the cost. He said he gladly would, if the official could only pinpoint what exactly was wrong. The person never called back. The incident, says Byerly, demonstrates the younger Pielke's coolness under fire. \u201cHe knows right where the jugular is,\u201d says Byerly. For his doctorate work, Pielke Jr turned to the stickiest problem he could think of. \u201cI asked myself: what's the hardest possible evaluation problem that I could do, that's messy and involves politics?\u201d In the early 1990s, the obvious choice was climate-change policy. And so he rigorously evaluated the US Global Climate Research Program, concluding that it was not meeting its mandate of providing useful information about climate science for decision-makers 2 . From that thesis arose an idea that Pielke Jr continues to push today, much to the discomfort of some climate scientists. He argues that the traditional relationship between science and policy, in which scientists do good science and hand the results to the policy-makers, is obsolete \u2014 particularly for complex modern issues such as stem-cell research and climate change. He advocates a two-way approach, in which policy-makers point scientists at the next set of questions to which answers would be useful. In the example of climate change, Pielke Jr says, many researchers have taken one of two sides: backing either mitigation policies to reduce greenhouse-gas emissions, or adaptation policies to deal with climate change as it occurs. \u201cOne of the most important roles science can play is to invent new options and introduce them to decision-makers,\u201d he says. \u201cWhen scientists take sides, they are giving up that role.\u201d He persistently challenges scientists who he thinks are acting as advocates for a particular position, including members of the Intergovernmental Panel on Climate Change and scientists who run a blog called RealClimate. \u201cTo be frank, that irritates the hell out of me,\u201d says Gavin Schmidt, co-founder of the RealClimate site and a climate researcher at NASA's Goddard Institute for Space Studies in New York. \u201cWhat he considers to be advocacy, to me, that's just interacting in the public realm.\u201d Schmidt and Pielke Jr have never met in person, but have had heated exchanges in the world of blogs (see  \u2018From the atmosphere to the blogosphere\u2019 ). \n                     Hurricane seasons warm up \n                   \n                     Scientist quits climate-change panel \n                   \n                     Science policy: Policy, politics and perspective \n                   \n                     Pielke Sr homepage \n                   \n                     Pielke Sr's Climate Science blog \n                   \n                     Pielke Jr's Science Policy homepage \n                   \n                     Prometheus \n                   \n                     RealClimate \n                   Reprints and Permissions"},
{"file_id": "440984a", "url": "https://www.nature.com/articles/440984a", "year": 2006, "authors": [{"name": "Jim Giles"}], "parsed_as_year": "2006_or_before", "body": "Once touted as too cheap to meter, nuclear power has become too costly to build. But the economics may be shifting, finds Jim Giles. On the east coast of Britain sits one of the most convincing arguments for ending the age of nuclear power. The Sizewell B reactor is not dangerous: since it opened in 1995 it has never been the subject of a serious security scare. Nor is it unreliable. Quite the reverse: at almost 1,200 megawatts, it is Britain's most powerful single nuclear reactor and is responsible for supplying 3% of the country's electricity needs. But Sizewell B is expensive. So expensive that no private investor would ever touch such a project. For nuclear experts, the story of Sizewell B is a familiar one. After the longest public inquiry into a construction project that Britain had ever seen, work began in 1987. It took eight years to come online. The budget was revised upwards three times over that period, eventually coming in at more than a third over the \u00a32 billion (US$3.3 billion) quoted in 1987. When the British government reviewed the project in 2002, it estimated that, when the costs of financing, building, running and decommissioning Sizewell B were fully accounted for, the average cost of every kilowatt hour (kWh) of electricity produced over the plant's 40-year life would be six pence \u2014 two to three times more expensive than power generated by modern gas-fired stations. \n               Running down \n             Nuclear power stations account for a fifth of the electricity generated in the United States and a third of that generated in Europe. But they are getting old. Since the 1979 accident at the Three Mile Island plant in Pennsylvania, orders for new reactors in the United States and Europe have reduced to a trickle. Decisions on how to replace the existing plants need to be made within the next ten years. And although renewable sources such as wind are looking increasingly attractive, large central power stations can only realistically be powered by nuclear fission, coal or gas. Almost all recent studies of nuclear energy have found that gas- or coal-fired replacements would be much cheaper. Given this underlying lack of competitiveness, why bother taking on board the associated risks of terrorism and weapons proliferation that come with the technology, not to mention the displeasure of many citizens? The answer is that when the downsides of fossil fuels \u2014 including, but not limited to, their carbon dioxide production \u2014 are totted up, nuclear power begins to look more attractive. Some economists are even starting to place bets on a nuclear renaissance. The current economic picture is persuasively summed up in the 2003 nuclear-economics report from the Massachusetts Institute of Technology (MIT) 1 . After considering the cost of building the plant, buying fuel and operating the reactor, and finally disposing of the waste and decommissioning the facility, the MIT team placed the cost of nuclear electricity at 6.7\u00a2 per kWh. Gas came in at 3.8\u20135.6\u00a2 per kWh, depending on wholesale gas price, with coal somewhere in the middle of that range. A 2005 report by the UK Royal Academy of Engineering 2  put nuclear costs on a par with coal and gas, but used some unreasonably favourable economic assumptions. Much of nuclear power's expense comes from construction costs, and the debts that must be incurred to pay them (see chart). A 1,000-megawatt gas-fired plant could, in favourable circumstances, be built in a year or so for $400 million, but a 1,000-megawatt nuclear reactor is likely to take five years to build and to cost between $1.5 billion and $2 billion, depending in part on where it is sited. The long construction time drives the price up by increasing the amount of interest that must be paid on the money borrowed for the project. And the length of the gestation isn't even predictable; it depends in part on \u201chow slick the lawyers are\u201d, notes Donald Jones, an energy economist at RCF Economic and Financial Consulting in Chicago, Illinois. If opponents of nuclear power raise legal challenges, costs mount up quickly. \u201cHalting construction for two years in the middle adds 15% to the final cost of electricity,\u201d Jones says. In 2005, hoping to encourage the industry by offsetting this risk, the US Energy Policy Act offered energy companies $500 million of coverage for losses due to construction delays. Yet US investors remain wary of nuclear projects. \n               Balancing act \n             Once a nuclear plant is running, operational costs are relatively low. But there are two important exceptions: storage of radioactive waste and, at the end of the reactor's life, decommissioning costs. In Britain, estimates of the funds required to clean up the country's 20 civil nuclear sites have frequently been revised upwards. Just last month, for example, officials increased the total predicted bill from \u00a356 billion to \u00a370 billion. Some schemes to deal with waste costs are in place \u2014 the United States has a levy on nuclear electricity that will fund the country's planned waste-storage facility at Yucca Mountain in Nevada (see page 987). But uncertainty about these costs continues to worry investors. The economic picture hasn't stopped all construction of nuclear power plants. India, China and Russia are building a handful of reactors, for example, but all are government-funded projects. It is in Europe and the United States, with their largely deregulated energy markets, that the economic arguments have bitten deepest. Finland is the only Western nation to have a new nuclear reactor under construction, but its Olkiluoto station, due for completion in 2009, makes economic sense only because of an unusual funding mechanism: local industries are paying for the plant in return for a contract from the operator that guarantees them low-cost electricity. This is not a model that can easily be exported. France may also decide to order a new reactor, possibly this year. But again the situation is different from that in most countries. France's experience with the technology means that investors will loan money for projects at a cheaper rate than elsewhere. A French 2003 study, for example, put the price of future nuclear electricity at 3.5\u00a2 per kWh \u2014 the lowest figure in any of the recent reviews, with the exception of estimates for Olkiluoto. When cost comparisons are extended beyond current prices and business practice, however, nuclear looks like a feasible option even beyond the borders of France. For a start, although the nuclear industry faces some unique challenges, the finances of its two major competitors are also looking a little troubled. Take gas: wholesale prices have increased fourfold over the past six years. That pushes the price of electricity from gas-fired stations to around 15% below nuclear. That gap will shrink further if industry forecasts about the efficiency of modern plants are accurate. Nuclear lobby groups claim new plants could probably be built in four years, not five. Independent experts are aware that the industry has a record of what is euphemistically known as \u2018appraisal optimism\u2019. But the MIT study, which worked with costs larger than those the industry usually uses, acknowledges that improvements are \u201cplausible\u201d. If this were so, the cost of nuclear electricity would come down to 4.2\u00a2 per kWh, making it competitive with gas and coal. \n               Fair exchange \n             Factor in the cost of greenhouse-gas emissions, and things look even better for nuclear. In Europe, where emissions from industry are already regulated and traded on a carbon market, prices are currently around $30 per tonne of carbon dioxide. At that price, says Paul Joskow, an economist and participant in the MIT study, nuclear is competitive with coal and gas, at least if the price of the latter remains high. This remains the case even when the cost of the carbon burned while the plants are built and their fuel mined and processed is taken into account. This broader analysis, which the nuclear industry is understandably keen to promote, boosts its standing. But where should the process of extending the cost comparisons end? Environmental groups say that going as far as carbon prices and no further means taking into account all the costs of other generators while leaving out costs specific to nuclear, such as lowering the barriers to nuclear proliferation. Assessing that argument takes the calculations on to less certain ground, although a smattering of studies have attempted to quantify some of the issues. Inasmuch as anything can be said for sure, however, it seems that the more inclusive approach may improve the case for nuclear. Take disaster liability. In Britain, the amount that reactor owners have to pay out in the event of an accident is limited to \u00a3140 million ($250 million). US industry contributes to a pool of money that ensures that up to $10 billion is available. Neither figure would be anything like sufficient should a disaster on the scale of Chernobyl occur (see page 982). The extra cost would have to be picked up by the taxpayer, so \u201cin essence this is government-subsidized insurance\u201d, says Matthew Bunn, a nuclear expert at Harvard University. If nuclear were forced to insure itself on the open market, it would find it impossible. \n               Government crutch? \n             We can't include the cost of terrorism issues. We don't have a handle on how to quantify that. Anil Markandya But how big a subsidy this actually is remains unclear, because the real risk of catastrophic accidents is unknown. Estimates can be generated by looking at the frequency of previous accidents and how the associated costs compare with events for which the insurance industry is prepared to provide cover. The MIT study suggests that the subsidy amounts to just $3 million per plant per year \u2014 a tiny figure when reactors produce $500 million of electricity annually. \u201cIn terms of the impact on the cost of electricity it's lost in the noise,\u201d says Richard Lester, an author on the MIT study. What's more, nuclear is not the only industry that benefits from subsidized insurance. A major explosion at a depot handling liquid natural gas could produce a bill well beyond the scope of the owner's cover. So would the wall of water let loose from a hydroelectric dam destroyed by an earthquake. \u201cThere is an implicit assumption that the government would step in,\u201d says Lester. \u201cEverything has an insurance limit.\u201d Some other costs are simply unquantifiable. The European Union's ExternE study 3 , which has been running since 1991, provides perhaps the fullest accounting of what economists call \u2018externalities\u2019 \u2014 costs that the people directly involved don't end up paying. ExterneE's audit assigns nuclear extra environmental costs of 0.2\u20130.8\u00a2 per kWh, mostly derived from air pollution attendant on the plants' construction, mining and transport of fuel, and decommissioning. The figures for fossil fuels, which include damage to the climate as well as air quality, are much higher \u2014 up to 18\u00a2 per kWh for coal. Yet even when ExternE's comprehensive analysis is considered, some things are still unaccounted for. \u201cWe can't include terrorism issues,\u201d says Anil Markandya, an economist at the University of Bath who works on ExternE. \u201cWe don't have a handle on how to quantify that.\u201d There is also the cost that would be incurred were an unstable nation to develop nuclear weapons by buying nuclear-reactor technology. \u201cThe contribution of the civil nuclear system to proliferation is impossible to monetize,\u201d says Bunn. \u201cBut that would be the biggest externality.\u201d \n               Unstable fuel \n             Such costs, even if they cannot be quantified, do not apply only to nuclear. \u201cIf you're concerned about nuclear safeguard costs you have to look at the costs of other sources,\u201d says William Nuttall, a nuclear expert at the University of Cambridge, UK. Putting a figure on the Western military spending associated with maintaining fossil-fuel supplies from the Middle East is a politically contentious task. But various estimates, from tens of billions of dollars a year to more than a hundred billion, suggest there is a hidden subsidy for oil prices that might top 10%. These arguments, although vital for policy-makers wondering what to encourage, will not on their own influence investors' decisions. But a final point in favour of nuclear comes from a source that the money men are used to listening to. Portfolio theory is an established way of generating a mix of investments that creates maximum return for a given level of risk. This, says Shimon Awerbuch, an economist at the University of Sussex, UK, is exactly how governments should approach energy decisions. \u201cTalking about generating cost without also talking about financial risk is like watching a movie with the sound turned off,\u201d he says. \u201cYou miss a big part of the story.\u201d In the case of electricity generation, \u2018risk\u2019 concerns the chance that fuel prices, be they uranium or gas, will go up. Hikes in oil prices have a similar knock-on effect to those of energy prices more generally \u2014 they reduce gross domestic product. The real cost of a fuel source, says Awerbuch, needs to take such risks into account. That is bad news for sources whose price fluctuates, such as gas, and good news for nuclear, as uranium costs are reasonably steady, and likely to remain so unless there is an unparalleled boom in plant building. Awerbuch's approach is to analyse the current mix of fuel sources in the economy to see what levels of risk governments are implicitly willing to accept. He then searches for other mixes that deliver the same risk at less cost. When trying out new combinations, something surprising can happen: adding an expensive non-fossil-fuel source such as nuclear or wind can actually decrease the overall cost. Nuclear lowers exposure to price hikes, and that lets planners simultaneously invest in riskier but cheaper sources such as gas. That additional gas more than compensates for the more expensive nuclear power, so overall prices fall. Although most of Awerbuch's work focuses on wind 4 , his analysis also suggests that the steady price of uranium means nuclear should be retained as part of a healthy mix of generation sources. When Awerbuch's way of looking at energy is combined with the recent rises in gas prices and, more significantly, the new carbon markets in Europe and the United States, another round of nuclear build seems a realistic possibility. A straw poll of nuclear experts shows they are starting to be convinced. Bunn used to offer straight bets against new nuclear construction starting in the coming decade. But put all these changes together, he says, and he might need to start offering odds. \u201cOver 15 years,\u201d he adds, \u201cI might switch my money to the other side.\u201d See Editorial on page 969. \n                     Nuclear deal riles India's researchers \n                   \n                     India's nuclear debate hots up after tsunami floods reactor \n                   \n                     Nuclear agreement paves way for fuel recycling in Japan \n                   \n                     Nuclear proliferation special: We have the technology \n                   \n                     International Atomic Energy Agency \n                   \n                     International Energy Agency \n                   \n                     Nuclear Energy Institute \n                   Reprints and Permissions"},
{"file_id": "440730a", "url": "https://www.nature.com/articles/440730a", "year": 2006, "authors": [{"name": "Claire Ainsworth"}], "parsed_as_year": "2006_or_before", "body": "Multicellular creatures can be battlegrounds for competing populations of cells. Claire Ainsworth learns how this way of looking at an individual is feeding into immunology and cancer biology. \u201cThere are colonies of pelagic tunicates which have taken shape like the finger of a glove. Each member of the colony is an individual animal, but the colony is another individual animal, not at all like the sum of its individuals... So a man of individualistic reason, if he must ask, \u2018Which is the animal, the colony or the individual?\u2019 must abandon his particular kind of reason and say, \u2018Why, it's two animals and they aren't alike in any more than the cells of my body are like me. I am much more than the sum of my cells, and, for all I know, they are much more than the division of me.\u2019\u201d 1 There is something wonderful, whether for a child or a great man of letters, in the sight of a rock pool brimming with strange and secret creatures. The pelagic tunicates that so enraptured John Steinbeck when he sailed the seas off California in 1940 with his friend, marine biologist Edward Ricketts, were particularly worthy of such wonder. Today, similar tunicates are providing scientific insight into the nature of the individual, in two seemingly separate realms. Both immunologists, interested in how the self is distinguished from non-self, and evolutionary biologists, trying to establish the level of organization on which natural selection acts, have turned to the tunicate  Botryllus schlosseri  for answers. Botryllus schlosseri  is a beautiful, cosmopolitan and enigmatic creature. Its life cycle starts with a tadpole-like animal that possesses a rod of elastic tissue called a notochord \u2014 the evolutionary precursor to the backbone. This tadpole picks a suitable patch of rock or seaweed frond on which to settle and, in a bizarre and seemingly atavistic transformation, metamorphoses into a polyp-like creature. The adult then begins to bud, producing genetically identical offspring called zooids. Once mature, the zooids produce their own buds. In a coordinated weekly orgy of death and regeneration, the old zooids perish and shrink back to nothing while the new take over. The entire colony of zooids is connected by a system of blood vessels and is sheathed in a jelly-like tunic. Underneath this mantle, the filter-feeding zooids cluster prettily like petals around communal siphons, through which they expel water. The question of where the individuality lies in such a creature is not easy to answer. Irving Weissman, of Stanford University, California, has been studying the  Botryllus  colonies of nearby Monterey Bay for 30 years. Now an eminent stem-cell biologist, he began his career with a keen interest in immunology. His interest drew him to the strange inter-actions that take place place when the  Botryllus  colonies infringe on each other's borders When the blood vessels of two colonies make contact, one of two things can happen. The two systems can fuse, producing a new colony that contains cells of two different genetic make ups \u2014 a chimaera. Or an inflammatory reaction can take place in which the interacting blood vessels are destroyed and a scar is formed that prevents fusion. Whether the colonies fuse or reject each other is determined by the genetic make up of each colony. Together these responses look, to an immunologist's eyes, uncannily like the mechanisms by which a body accepts or rejects an organ transplant. In the 1980s, Weissman's team found that the fusion\u2013rejection decision depended on a genetically encoded system that behaved very much like the major histocompatibilty complex (MHC) in humans 2 . The MHC, which displays specific protein fragments on the surface of cells, acts a bit like an identification tag and determines whether the tissues of a human organ donor are accepted or rejected by a recipient. But why do the colonies try to fuse in the first place? \n               Skill sharing \n             One suggestion is that a combination of two (or more) tunicates is better than one. For example, in the lab, if a colony that thrives best at 15 \u00b0C merges with one that thrives at 25 \u00b0C, then the fused colony will do pretty well at both temperatures. \u201cThe chimaera is very flexible,\u201d says Baruch Rinkevich, a biologist at the National Institute of Oceanography in Haifa, Israel. Pooling talents in this way would make the colonies better able to deal with environmental change. It also means that a colony can spread itself over a larger area \u2014 handy if you get nibbled by hungry fish. But how can colonies merge their characteristics? The answer lies in the tunicate's extraordinary ability to regenerate. The colonies can regrow themselves every seven days thanks to a particular sort of stem cell. In adult vertebrates, stem cells divide to produce both more stem cells and cells of various different types specific to a particular tissue; skin stem cells can produce various sorts of skin cell, for example. But take a tiny scrap of blood vessel from one of these tunicates, and it will regenerate the entire animal. In other words, some of the tunicate's adult stem cells behave like our embryonic stem cells; they have the power to make any cell type in the tunicate \u2018body\u2019. In creatures that develop once, such as humans, such powerful stem cells are needed only early on in development. But in a creature that regrows itself on a weekly basis, they must be on call all the time. \u201cIt's totally backwards,\u201d says Anthony De Tomaso, a former postdoc of Weissman's who has taken up the reins of the Stanford tunicate lab. \u201cBut it sets up this situation where you can have development occurring all the time.\u201d When two colonies fuse, the stem cells of each are free to build tissues in what used to be a separate colony. This mixes up the different cells and gives the \u2018new\u2019 tunicate the benefit of a greater range of gene variants without all the fuss and bother of sex. But the fusion raises questions about what level of organization natural selection now acts on \u2014 the genetically distinct cell lines originating from the two colonies, or the merged colony as a whole? It was another Weismann, differently spelled, who a century earlier argued that natural selection does not take place within the body. The nineteenth-century German biologist August Weismann divided the cells of multicellular creatures into two types: \u2018somatic cells\u2019, which make up almost all the parts of the body, and the cells of the \u2018germ line\u2019 \u2014 a small minority that produce just egg and sperm. Crucially, he said, there is a wall \u2014 the Weismann barrier \u2014 between the soma and the germ line. This barrier ensures that somatic cells can never contribute to the next generation, and prevents any of the creature's acquired characteristics from being passed on to its descendants. Weismann's work was fundamental to the rediscovery of mendelian genetics and to twentieth-century understanding of darwinian evolution. This viewed natural selection as acting on the individual organism, with the quality of the creature's germ line tested by the fitness of its genetically identical somatic cells. The idea of the individual as the only unit of selection has since been challenged by biologists advocating the role of higher and lower levels of organization. Group selectionists hold that evolution can select for attributes that benefit a group or species as a whole. Meanwhile, gene selectionists argue that evolution can be best understood in terms of choices between \u2018selfish\u2019 genes competing to copy themselves. The tale of the tunicates suggests another level, where selection acts directly on cell lines. One champion of this idea is Leo Buss, a biologist at Yale University in New Haven, Connecticut, whose decades of work on the topic have only recently begun to be addressed by molecular biologists. Buss argues that in protists, fungi, plants and 19 of the 33 different animal phyla around today, the Weismann barrier can leak 3 . In these organisms, somatic cells can become germ cells, and thus genetic changes acquired during development within the soma can become heritable. Natural selection can thus act on competing cell lineages within the body, favouring those that get into the germ line. \n               One-way street \n             This competition began more than half a billion years ago, when a group of single-celled organisms made the great transition to forming a multicellular body. Understanding how natural selection resolved the competitive conflicts between cells, and between cells and the individual creature, says Buss, is key to understanding how multicellular organisms evolved into the forms we see today. With Buss's work in mind, Weissman's team at Stanford traced the destiny of cells in fused tunicate colonies. The group found that, in some cases, cells from one colony seemed to completely replace the body tissues of another 4 . In others, cells from one colony sneaked into the gonads of the other, replacing the host's germ line 5 , 6 . In this fate worse than death, the tunicate is not just stopped from propagating its own genetic line, but effectively forced to churn out a competitor's offspring. It is this dreadful spectre that gives rise to the tunicate's rejection reaction. To avoid being parasitized,  Botryllus  colonies have acquired a well-developed \u2018sense of self\u2019 \u2014 the genetically encoded tissue-matching system, now called the FuHC system, that Weissman's team found in the 1980s. One colony will only permit another to fuse with it if its FuHC tissue-matching genes are sufficiently similar. The FuHC gene, like the MHC genes, comes in thousands of different versions; two colonies are only likely to be a match if they are closely related. In this case, the costs of germline takeover are much diminished. If incoming stem cells do hijack the germ line, they will be closely enough related for it not to matter too much; most of the host's genes will effectively still get through. De Tomaso and colleagues think that the FuHC operates in much the same way as the human MHC does in natural killer cells. Unlike T cells \u2014 white blood cells that actively look for foreign proteins associated with the MHC \u2014 natural killer cells look for \u2018missing self\u2019; that is, for cells that lack the normal MHC. Although the molecular players appear to be different in the tunicate, the same logic seems to apply when one colony merges with another. All creatures that can live as chimaeras, including fungi, flowering plants and primitive animals such as sponges, have some kind of ability to recognize foreign cells. Luis Cadavid, a biologist at the University of New Mexico in Albuquerque who has studied another colonial sea creature, a hydrozoan called  Hydractinia  finds that it, too, has a set of markers for selfhood. The commonality of this trait does not mean, however, that it has a shared origin. Although FuHC may function like the MHC, the sequence data of de Tomaso's team's shows that the two systems are not related 7 . \u201cMy impression is that there is a common theme to recognizing self versus non-self, but the mechanisms may have different origins,\u201d says Cadavid. Louis du Pasquier, a biologist at the University of Basel in Switzerland who works on the evolution of adaptive immunity, agrees. What the evidence points to, says Du Pasquier, is a common selection pressure \u2014 originally for a way to maintain the integrity of the self in the face of invasion, not by bacteria and viruses, but by members of your own species. \n               Self aware \n             This applies to humans too, argue De Tomaso and Weissman. Humans can play host to competing cell lines in a number of ways. For example, in an organ transplant or blood transfusion, blood stem cells from the donor can quickly colonize the recipient's body and may hang around indefinitely. But there are natural ways too. Although not an example of chimaerism, cancer involves a particular line of somatic cells declaring an independent identity from the body around it. In order to avoid alerting the adaptive immune system to their aberrant nature, some cancer cells stop expressing MHC genes. Rinkevich and his team have studied, as an analogous process, cells from  Botryllus  running riot in the bodies of a closely related species of tunicate called  Botrylloides . \u201cIt's like cancer,\u201d says Rinkevich. \u201cIt does have parallels, no doubt about it,\u201d agrees De Tomaso. \u201cCancer is somatic cell parasitism. It is selection inside a body for a variant.\u201d Weissman thinks that  Botryllus  has a lot to teach us. Cancer researchers now think that our bodies' stem cells play a key role in both the origin and development of cancerous tumours 8 , and Weissman sees intriguing parallels between such stem cells and the most highly successful of  Botryllus 's \u2018winner\u2019 stem cells. He is keen to uncover the genes that give these cells their super-predatory powers. \u201cI wouldn't be surprised if, when we finally isolate these genes, related genes are found in pathways that allow cancer cells to develop,\u201d he says. Cancer is not generally a route to true immortality, because it is normally limited to a single body with a finite lifespan (although not always, see  \u2018The biggest parasite in the world?\u2019 ). To persist beyond this, stem cells need to get out of the soma and into a germ line \u2014 any germ line. In theory, the easiest way for this to happen would be through pregnancy. Scientists have known for years that in many mammals, stem cells can cross from fetus to fetus in multiple pregnancies. Stem cells can also break out from fetuses into the mother's blood. Most are hunted down and destroyed by the mother's immune system, but usually some remain \u2014 although no one knows why or how. A woman who has had several children could be a mixture of many different cell lineages. \n                     Human genetics: Dual identities \n                   \n                     Stem Cells in focus \n                   Reprints and Permissions"},
{"file_id": "440987a", "url": "https://www.nature.com/articles/440987a", "year": 2006, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "The global future of nuclear power may rest in large part on local politics, reports Geoff Brumfiel. Like 83% of the state of Nevada, the land to your right as you head up from Las Vegas towards Reno is owned by the federal government. A military airstrip for unmanned aerial vehicles and a nuclear-weapons test range stretch into the distance; enormous concrete tubes that once held MX missiles lie baking in the sun. The only signs of civilian life are a state prison, a brothel and a few small clusters of air-conditioned trailer homes. It is here in Nye County \u2014 where 38,000 souls occupy 47,000 square kilometres \u2014 that the US Department of Energy would like to bury more than 70,000 tonnes of highly reactive nuclear waste. The federal government says that Yucca Mountain, a low peak on the western edge of its nuclear test site, some 100 kilometres northwest of Las Vegas, is one of the safest places in America for spent fuel to make its several-hundred-thousand-year journey to harmlessness. The State of Nevada is committed to stopping this thing by any legal means possible. Steve Frishman Local residents want jobs, so they are not wholly opposed to the repository. But other citizenry of Nevada, many of whom remember the consequences of above-ground nuclear-weapons testing in the 1950s and 60s, are deeply sceptical. And so are their elected representatives. \u201cThe state of Nevada is committed to stopping this thing by any legal means possible,\u201d says Steve Frishman, a geologist and the technical policy coordinator for the state government's Agency for Nuclear Projects. Virtually every democratic nation that has embarked on a programme for the disposal of \u2018high-level\u2019 nuclear waste has run into similar trouble, and few have found a way forward. Bitter fights with concerned citizens have derailed plans in Germany, Canada and the United Kingdom. \u201cAll of the world's programmes now recognize that the non-technical problems are bigger than the technical ones,\u201d says Charles McCombie, a nuclear-waste consultant who has worked extensively on the Swiss, Canadian and Japanese disposal programmes. Nations around the world with stocks of waste \u2014 including those anticipating rapid growth in such stocks (see  \u2018Booming nations go nuclear\u2019 ) \u2014 are realizing that to dispose of their nuclear waste, they need to build trust with local communities, and that it is on these efforts that much of the future of nuclear power depends. \u201cIf we don't solve the waste problem, we're going to have major trouble continuing the revitalization of nuclear power,\u201d says Tom Isaacs, director for policy, planning and special studies at Lawrence Livermore National Laboratory in California. \n               A lasting legacy \n             The decay of the isotope uranium-235 in nuclear fuel rods creates atomic nuclei with a wide range of radioactivities and chemical properties. Some, such as neptunium-237, are present only in small quantities but can remain radioactive for millions of years. Others, including caesium-137 and strontium-90, make the waste a health hazard for decades or centuries, and raise its temperature to above 500 \u00b0C. The most problematic is plutonium-239, generated from uranium-238, which when purified can be fashioned into a nuclear weapon. \u2018Reprocessing\u2019, in which plutonium-239 and unused uranium-235 are reused as fuels or for weapons, reduces the amount of high-level waste produced. But as yet, it has never proved an economic way of making nuclear fuel. And even if reprocessing were to improve dramatically, or if the use of reactors or beams of particles to transmute some of the isotopes into less noxious ones were to prove practical, generating energy from nuclear fission would still produce some waste posing a hazard for millennia. And that waste will need to be put somewhere. Since the late 1950s, the scientific community has more-or-less agreed that the best way to deal with this waste is to put it deep underground, says Malcolm Gray, a nuclear engineer with the waste-technology section of the International Atomic Energy Agency in Vienna, Austria. Providing safety and security for surface facilities, such as the spent-fuel ponds used at nuclear power plants, is costly, especially in the long term. Properly chosen and engineered sites can remain stable for millennia and would not require the modification of international treaties on dumping. \u201cGeological disposal is the only practical solution,\u201d Gray says. Sites for such disposal are under consideration around the world (see graphic). Yucca Mountain should serve as a cautionary tale, for such efforts, on two grounds. One is that despite years of research it is still not clear how well this particular site would meet the stringent regulations called for by the National Academies of Science (NAS). The NAS recommends a regulatory period of several hundred thousand years, which could mean keeping the waste safe all the way through the next two ice ages. The other is that the local people have never been more united in their rejection of the plans. All the world recognizes that the non-technical problems are bigger than the technical ones. Charles McCombie Yucca Mountain has been studied as a possible site for nuclear-waste disposal since 1978. A bevy of experiments in an eight-kilometre tunnel built in the side of the mountain have looked at how water, heat and stray radioactive material might move through the rock. The data are used to estimate exposure rates for humans who might stumble across the site centuries or millennia into the future. In recent years, some of those experiments have shown that waste could migrate to the water table more quickly than expected. And studies of young volcanoes around the mountain have raised concerns about a breach occurring at the site (see  Nature   412 , 850\u2013852; 2001). A big problem is that no models can be expected to precisely predict exposure over such vast time periods. As Michael Voegele, a geological engineer with more than 25 years of experience at the site, puts it: \u201cIt's impossible to know exactly how water is going to move through the mountain when Chicago's under 5,000 feet of ice.\u201d These uncertainties fuel the doubts of Las Vegans, says Peggy Maze Johnson, executive director of Citizen Alert, an environmental group that is trying to block the project. Local people think Yucca Mountain was picked because Nevada is thinly populated and has little political representation in Washington. \u201cI am not an expert,\u201d Johnson says. \u201cAll I know is that when something is based on politics, it doesn't make it sound science.\u201d \n               Looking to the leaders \n             Repositories cannot be built without addressing this blend of political and scientific concerns, says Tero Varjoranta, director of nuclear waste and materials regulation at S\u00e4teilyturvakeskus (STUK), a nuclear research centre and regulatory authority in Helsinki, Finland. Varjoranta says it is critical that government agencies be, and be seen to be, impartial and authoritative. \u201cWe're not selling the repository,\u201d Varjoranta says of his own office. \u201cWe try to convince people that if it is built, it will be safe.\u201d In neighbouring Sweden, such impartiality has been vital because of the strength of local municipalities, says Saida Engstr\u00f6m, a member of the board of directors at Svensk K\u00e4rnbr\u00e4nslehantering AB (SKB). Based in Stockholm, SKB is a company formed by the government that is charged with developing Sweden's waste repository. When Engstr\u00f6m was in charge of assessing the environmental impact of a repository in the village of Tierp, 67% of the population supported her work, she says. But the city council vetoed the project by one vote. \u201cI'd be lying if I told you I wasn't bothered by it \u2014 I was,\u201d she says. \u201cBut for the credibility of the project it is vital that you involve local people in the process.\u201d Ultimately, the veto at Tierp showed that SKB was serious about respecting local wishes and strengthened the programme's reputation in other parts of the country, she argues. The company is continuing investigations at Oskarshamn and \u00d6sthammar, where public support for the project continues to be strong. Attempts to gain public trust have put the two Scandinavian nations far ahead in their efforts to build a repository, and other countries are now looking to them for inspiration. In the United Kingdom, a government committee was established in 2003, to tackle radioactive-waste management. The committee, which includes environmentalists and social scientists as well as physicists and engineers, will determine whether a deeply buried repository is the best way to dispose of waste. Lessons have been learnt after plans to build a repository at Sellafield were thrown out by a public inquiry in 1997. \u201cThat was the end of an old tradition where scientists, industry and government got together behind closed doors, thought up the right option, thought up the right site and then announced it,\u201d says Gordon MacKerron, an economist from the University of Sussex who chairs the committee. Yet others worry that the pendulum has swung too far in the direction of public opinion. \u201cThere's a view out there, which basically says that science is not reliable anymore,\u201d says David Ball, a professor of risk management at Middlesex University in London. Ball resigned last spring from Britain's committee on radioactive-waste management, in protest of what he saw as an over-reliance on public opinion. \u201cThey didn't see science as holding any objective truth and replaced it with the good old common sense of the man on the Clapham omnibus,\u201d he says. \n               Winning trust \n             Keith Baverstock, a radiation health expert at the University of Kuopio in Finland (see  page 993 ), adds that the Finnish and Swedish success is not due to national public acceptance. \u201cThe Finns have never had more than about 40% of the general population agree that this form of disposal was the correct way to do it,\u201d he says. The key, he says, is to find a local community willing to accept the project. The views of local communities will not be determined only by local issues, such as jobs or worries about contamination. \u201cMany groups began opposing Yucca Mountain because they didn't want new nuclear power and they didn't want it seen as the solution that allows for the construction of new nuclear plants,\u201d says Judy Treichel, executive director of the Nevada Nuclear Waste Task Force, which opposes the Yucca Mountain project. And in Sweden, there might be more local resistance to the country's waste-repository project if the government wasn't already committed to phasing out nuclear power across the country. At least, that's the view of Johan Swahn, director of the Swedish Office for Nuclear Waste Review \u2014 an environmental group that uses government funding to monitor the repository project. \u201cPeople are more willing to participate in the process with the understanding that nuclear waste is a finite problem,\u201d he says. Regardless of what scientists and engineers believe is technically possible, they must be prepared to address these sorts of cultural concerns, says nuclear-waste consultant McCombie. More broadly, governments eager to see a nuclear repository built must work harder to win their citizens\u2019 respect. That's no mean feat and can only be achieved through time and transparency, McCombie says: \u201cYou can't just build it like you would a filling station; it has to be a long process with buy-in and the potential for reversal at each stage.\u201d \n                     Recycling the past \n                   \n                     Australia mooted as dump for world's nuclear waste \n                   \n                     Britain set to pull plug on nuclear-fuel reprocessing \n                   \n                     Out of sight, out of mind? \n                   \n                     Decision time at Yucca Mountain \n                   \n                     National Academies Radiation Studies Board \n                   \n                     Swedish SKB \n                   \n                     UK Committee on Radioactive Waste Management \n                   \n                     Finnish Radiation and Nuclear Safety Authority \n                   Reprints and Permissions"},
{"file_id": "440864a", "url": "https://www.nature.com/articles/440864a", "year": 2006, "authors": [{"name": "Naomi Lubick"}], "parsed_as_year": "2006_or_before", "body": "In 1906, a great earthquake destroyed San Francisco, and galvanized US seismologists. Naomi Lubick looks back at the event that changed the country's geological scene. The Great Earthquake and subsequent fire that destroyed San Francisco in 1906 began at 5:12 a.m. on 18 April. More than 3,000 people are thought to have died following the magnitude-7.9 tremor. The metropolis of San Francisco, built on gold-rush fortunes, was almost utterly destroyed in three days of fire, and officials spent years playing down the possibility of another \u2018big one\u2019. Yet the earthquake also jump-started seismology in the United States, inspiring it to catch up with countries such as Britain, Japan and Germany. The US scientific community had already encountered several major earthquakes. Three tremors of magnitude 8 or more racked the New Madrid region in the US Midwest in 1811 and 1812. And the city of Charleston, South Carolina, was seriously damaged during an 1886 earthquake. But the 1906 earthquake happened in the right time and place to act as a catalyst for science. Chance brought together several ingredients: the right people, the right technology, key ideas in need of testing \u2014 and a huge earthquake delivering the data. \u201cIt took that large an event to make seismology a national priority,\u201d says Jack Boatwright, a seismologist at the US Geological Survey (USGS) in Menlo Park, California. Around the world, the discipline of seismology began to coalesce in the late nineteenth century, as non-specialists interested in earthquakes began to work together. In India, Richard Dixon Oldham identified primary (P) and secondary (S) waves, the main components of seismic waves. In Germany, engineers developed precision techniques to make seismometers more accurate. British geologists teaching in Japanese universities became interested in the tremblings beneath their feet. And in 1891, when the great Nobi earthquake hit, Fusakichi Omori of Tokyo's Imperial University was primed to lead his colleagues in documenting the event. The Japanese view the magnitude-8 Nobi earthquake in much the way that Americans see the 1906 San Francisco event, says Thomas Jordan, director of the Southern California Earthquake Center in Los Angeles. Thousands of people died, and supposedly \u2018modern\u2019 structures were shaken apart. In tracking the subsequent tremors, Omori made critical observations that led to his eponymous theory on how aftershocks decay with time. And he brought his ideas with him to San Francisco, where he did field work after the 1906 earthquake. Omori's expertise was rare in the city. \u201cIn my estimation, there weren't any seismologists in the United States before 1906,\u201d says Boatwright, \u201cthere were geologists.\u201d That soon changed. Immediately after the tremor, Andrew Lawson, a professor of geology at the University of California, Berkeley, assembled a troupe of scientists, who fanned out across the affected area. The group became a kind of seismic SWAT team, collecting evidence north towards Oregon and south almost to Mexico. \n               Ground forces \n             For the first time, scientists began to recognize the extent of the San Andreas fault. This 1,300-kilometre gash marks the boundary between two sections of Earth's crust: the Pacific plate and the North American plate. California geologists knew of the fault's existence, but until 1906 they had little idea of the power it could unleash. They did, however, realize that such faults cause earthquakes \u2014 a theory that came in part from Omori's documentation of the Nobi disaster. Hours after the earthquake rattled San Francisco, geologists saddled up, got into automobiles \u2014 recently introduced to the region \u2014 or simply walked to sites along the San Andreas fault. \u201cIt was a really remarkable group of people,\u201d says Boatwright. Participants ranged from the respected geologist Fran\u00e7ois Matthes, who avoided talking to anyone, to the sociable Grove Karl Gilbert, a USGS legend who spent a lot of time chatting to people who lived along the fault and recording their personal experiences. Gilbert quickly picked up evidence of the fault's movement, noticing fences, houses and roads that had been offset by up to 6 metres in some places. \n               Stirring tale \n             In 1908, Gilbert, Lawson and their colleagues produced a large tome that included detailed descriptions of fault scarps, sag ponds and other evidence of the earthquake; it was accompanied by an atlas of maps and seismic profiles. The Lawson report was reprinted in 1969 1 , and seismologists and geophysicists still use it today for its physical descriptions, maps and timetables of the 1906 event. The scientists who created it didn't know what they were looking for, so they documented the earthquake observations very simply, in terms of factors such as building damage, fault movement and behaviour of the soil, says Dave Wald of the USGS in Golden, Colorado. In part, these extremely detailed descriptions are what makes the Lawson report so useful today. Putting the whole picture together was not easy. At the time there was no overarching theory of plate tectonics \u2014 how parts of Earth's crust grind against each other. So geologists couldn't quite figure out how the San Andreas fault worked. \u201cPeople didn't have a mechanism for large-scale deformation,\u201d says Carol Prentice of the USGS in Menlo Park. Gilbert and others had already noted that faults could move horizontally, not just vertically. And geologists working on the Lawson report could clearly see that the fault had ruptured for at least 430 kilometres along its length, without significant uplift. Eventually, the San Francisco event helped geologists to recognize that the San Andreas is a strike-slip fault, in which two plates slide past each other rather than moving under or over one another. \n               All in store \n             In 1910, a second volume of the Lawson report was published, written by Harry Fielding Reid.It contained the 1906 event's greatest and most lasting contribution to seismology: the theory of elastic rebound. A professor of physics and geology at Johns Hopkins University in Baltimore, Maryland, Reid proposed that faults store up stress until they can no longer hold it, at which point they snap like a rubber band stretched too far. His meticulous synthesis came from field observations and information about the timing of shaking along the San Andreas fault. Before the San Francisco earthquake, Reid had worked almost exclusively on glaciers, particularly on how they advance and retreat over time. Apparently, he saw a similar pattern in earthquakes. Elastic rebound implied that faults would gradually store stress over time, rupture in an earthquake and then begin the process all over again. And that, for the first time, suggested that earthquakes recur regularly on the same fault, in potentially predictable cycles. The USGS estimates that an earthquake similar to the 1906 event is not likely in the near future, although earthquake risk remains high in the region \u2014 particularly on the nearby Hayward fault. San Francisco residents, city planners and emergency officials are currently bracing themselves for an earthquake of magnitude 6.7 or greater, which has a 62% probability of occurring in the San Francisco Bay area 2 . This preparation is a lesson that California has taken decades to learn. In a scenario that may seem distressingly familiar to disaster planners today, Lawson struggled after the earthquake to get funding for follow-up studies and to spread knowledge about seismic risks. Members of his team helped to found the Seismological Society of America in August 1906, modelling themselves on Japan's Seismological Society. But, says Duncan Agnew, a historian of seismology at the University of California, San Diego, \u201cfor a long time 1906 was one of the few well-documented earthquakes.\u201d \n               Landscape view \n             Even now, scientists are coming up with more analyses of the long-past earthquake. At next week's centennial anniversary meeting of the seismological society, Boatwright and his USGS colleagues will unveil a \u2018shake map\u2019 for the 1906 event. To produce a map of the intensity of shaking, they combed through the Lawson report to recreate the rupture as it would have been documented by seismometers placed along the fault. Geologist Tina Niemi of the University of Missouri in Kansas City will present the meeting with a reconstruction of earthquakes along the section of the fault that jumped the farthest, in Marin County. She and her co-authors will report that large earthquakes recur on that part of the fault every 50 to 600 years, so some activity might be due soon. On the same day, the few survivors from 1906 will gather, as is customary, at 5:12 a.m. at Lotta's Fountain in downtown San Francisco, to lay a wreath of remembrance and tell stories. Thousands are expected to attend, as San Francisco embraces the legacy of its most devastating day. \n                     Earthquake prediction: A seismic shift in thinking \n                   \n                     Tardy earthquake excites California geophysicists \n                   \n                     Only pride hurt as predicted quake fails to strike California \n                   \n                     Earthquake forecasting \n                   \n                     USGS on the 1906 event \n                   \n                     Bancroft Library on the 1906 event \n                   \n                     1906 Earthquake Centennial Alliance \n                   \n                     100 th  anniversary earthquake conference \n                   Reprints and Permissions"},
{"file_id": "440734a", "url": "https://www.nature.com/articles/440734a", "year": 2006, "authors": [{"name": "Samir S. Patel"}], "parsed_as_year": "2006_or_before", "body": "The floods are getting worse in Tuvalu. As scientists argue over climate change and struggle to measure rising seas, Samir S. Patel meets the locals of this tiny island nation. On a Friday afternoon in late January, the phone rings in the Filamona Guest House, one of the few places to stay in the Pacific archipelago of Tuvalu. Hilia Vavae, the director of the Meteorological Office, sounds excited in her modest, laid-back way. \u201cThere is a...\u201d she struggles for the right word \u201c...flooding!\u201d The floods on Funafuti, the main atoll of Tuvalu, are caused by \u2018king tides\u2019 \u2014 the highest high tides of the year. They percolate up through the porous limestone, soaking the islets from the inside out. And they draw journalists, scientists and environmental advocates from around the world, all keen to see what is happening on the front line of climate change. If global sea levels rise as predicted, the 11,000 Tuvaluans will be among the first to see things go awry. Yet even here, attitudes and responses to the impending catastrophe vary in complex and sometimes surprising ways. Next to the guest house is the country's sole airstrip, built by the US military in the Second World War. Being the only open space on Funafuti, the airstrip provides a breezy place to sleep on muggy nights, not to mention a pitch for football and  te ano , a volleyball-like game. Following Vavae's directions, I walk across the airstrip and, rolling up my trousers, on into a clear tidal lake. Two kids run over to sail toy boats made of green Victoria Bitter beer cans pounded flat. The salt water creeps steadily along the dirt track next to the runway and surrounds the nearby buildings, including the diesel power station, a handful of pungent concrete pigpens, and the office where Vavae and her staff collect data on sea level and beam them to the Australian Bureau of Meteorology's National Tidal Centre (NTC), thousands of kilometres away in Adelaide. In January and February, Tuvalu experienced some of the highest tides ever recorded there, nearly 1.5 metres above mean sea level. They were caused in part by the convergence of natural short- and long-term tidal cycles, but were boosted, perhaps, by the effects of global warming. Although the tides are not especially high compared with Newfoundland's Bay of Fundy or even the English Channel, they are pretty alarming in a country where the highest ground is just 5 metres above sea level and most is much less than that. A mean sea-level rise in Tuvalu of just 20 to 40 cm in the next hundred years would significantly increase the frequency and depth of saltwater flooding and accelerate coastal erosion. It would threaten the Tuvaluans' food and housing, poisoning the pits where they grow giant swamp taro plants and undermining buildings. It could make the country simply uninhabitable. \n               Rising damp \n             There are two tide gauges in Tuvalu. One, operated by the University of Hawaii until 1999, sits on a small concrete wharf behind the three-storey Taiwanese-built government building. In 1993, the NTC installed a more modern and accurate gauge a few kilometres north at the country's only deepwater wharf. One of twelve in the South Pacific, this gauge should in theory provide quantitative confirmation that Tuvalu is being engulfed, as the king tides and the wet cuffs of my trousers suggest. But in 2000 an NTC analysis reported a negligible increase of 0.07 mm a year over the past two decades from the University of Hawaii gauge, and a drop in sea level from the seven years of NTC data 1 . It was clear that the El Ni\u00f1o/Southern Oscillation (ENSO), which drives down sea level in the western Pacific, affected both of these records. And the international environmental group Greenpeace asked John Hunter, a climatologist at the University of Tasmania, to have another look at the data. When he adjusted for ENSO and the vertical movement of the Hawaii gauge, which is thought to be sinking, Hunter found a sea-level rise of around 1.2 mm a year 2 . Hunter's figure is consistent with the global estimate of the Intergovernmental Panel on Climate Change (IPCC): 1 to 2 mm a year for the twentieth century 3 . But the Tuvalu estimates are based on a couple of gauges and a reasonably short record, points out John Church of the Commonwealth Scientific and Industrial Research Organization (CSIRO) in Hobart, Tasmania, who was one of the lead authors of the chapter on sea level in the IPCC's most recent assessment. Recently, Church and his CSIRO colleague Neil White have moved to a more regional approach. They have combined records from tide gauges around the world, some of which date back as far as 1870, with satellite altimeter data to assess regional variation in sea-level rise. Their results for the South Pacific are in line with the Hunter and IPCC estimates 4 , and they are now looking specifically at Tuvalu and other small island nations. \u201cThe thing that really interests me is how you reconcile the relatively low estimates of sea-level rise, which are the same order as what's happening in the rest of the world, with the anecdotal observations from Tuvalu,\u201d says Hunter. \u201cIt seems that the flooding reported there is bigger than 2 mm a year. The extremes of high tide could be getting bigger relative to the mean sea level, although that's disputed at the moment.\u201d Phil Woodworth of the Proudman Oceanographic Laboratory in Liverpool, UK, has studied high tides in the Pacific to see whether they have changed since the 1970s. He says the peaks seem to be increasing, but by no more than mean sea level 5 . \u201cThe extremes at Tuvalu seem determined primarily by the slower changes in the oceans due to ENSO and the long-term changes in mean level,\u201d he says. The disagreements are unlikely to be resolved soon. \u201cWe're going to be waiting around for a fair while before our estimates of sea-level rise become statistically meaningful,\u201d says Bill Mitchell, manager of the NTC. \u201cEveryone presses you to give a number. We put a vast amount of effort into telling people that you should not be using numbers yet.\u201d \n               Treading water \n             Vavae has a similar opinion of the numbers, but perhaps for different reasons. \u201cI always tell my people that it's not the data that you look at. You have to actually rely on your eyes,\u201d she says, glancing outside to where the salt water is once again creeping across the yard. \u201cA person who experiences it has got a much better feel or much better knowledge of what is happening than someone who is 100 miles away.\u201d When Vavae started working for the Meteorological Office in 1981, the saltwater flooding was no worse than that from heavy rain. Now it is extensive, regularly inundating large parts of the island. The floods have almost reached the  fusi , or supermarket; seaweed is starting to appear in the more commonly flooded areas, Vavae tells me. The Tuvaluans have mixed reactions to these changes, and the scrutiny that comes with them. A few days after witnessing the king-tide flood by the airstrip, I talked to Carol Farbotko on the verandah of the Filamona. Farbotko, a cultural geographer from the University of Tasmania, is working on a thesis on cultural responses to climate change on the islands. In her interviews with officials and community leaders, she has found that climate is a vague, long-term concern. People get much more worked up about problems such as waste disposal, a fetid and ubiquitous problem; overpopulation; and the accelerating erosion of traditional culture in the age of the Internet and DVDs. Even frequent workshops on climate and the dangers of accelerating sea-level rise fail to provoke a sense of urgency. \u201cOh, it's a very important concern,\u201d is the standard and slightly mechanical response to questions on climate change, Farbotko says. In her experience, and in mine, some Tuvaluans refuse even to talk about climate, or dismiss it with a weary wave of the hand. Tuvalu is a deeply Christian country, and some islanders put their faith in the promise God made to Noah in Genesis 9:11: \u201cAnd I will establish my covenant with you; neither shall all flesh be cut off any more by the waters of a flood; neither shall there any more be a flood to destroy the Earth.\u201d It would not take a very large breach in that covenant to wash Tuvalu away. Fogafale, the largest of the islets that make up the Funafuti atoll, is an elegant snake of land about 10 kilometres long. Even on a borrowed bicycle with a hard seat and half-flat tyres it is easy to get from one end to the other in a morning. The ride is comfortable, because the roads were resurfaced with a windfall from the sale of Tuvalu's appealing Internet country code, \u2018.tv\u2019, although the speeding motorcycles and cars no longer have to slow for potholes. During high tides, waves crash from the lagoon on to stretches of the new tarmac, spreading trash, coral rubble and other detritus across it. There was a time, I'm told, when the road behind the government building was set back a few metres from the lagoon, but now it is right on the sea. In itself this says nothing about sea level, because the islets are constantly changing shape: a spit forming here, an island tip disappearing there. Arthur Webb, a coastal ecologist in the Fiji offices of the South Pacific Applied Geoscience Commission, makes the point to me graphically by flipping through aerial and later satellite photographs from the 1940s on. \u201cThese really are quite natural processes,\u201d he says. \u201cIt's part of living on a soft-shored island.\u201d \n               Swept away \n             As Webb shows me images with Second World War seaplanes and torpedo boats sitting in the lagoon, he explains how, in 1943, US troops built a makeshift seawall and land-reclamation project along the length of the lagoon foreshore. After the troops left, locals built homes and roads on this 25- to 30-metre-wide stretch of rubble, which then began to erode. Jetties and channels further altered coastal erosion and deposition patterns. The move from portable, short-lived thatch houses to Western-style concrete-block homes has added to the difficulties. Despite regulations, the shores of Tuvalu have been ransacked for aggregate for construction \u2014 an activity that makes the prospect of flooding far worse. \u201cBeach mining is a disaster,\u201d says Webb. Those sceptical about Tuvalu's plight, including amateur scientist Willis Eschenbach, seize on local explanations such as mining to assert that fears about sea-level rise are created by hysterical journalists and environmental groups looking for a  cause c\u00e9l\u00e8bre . Eschenbach, who carried on a spirited debate with Hunter in the journal  Energy and Environment , has concluded that sea-level rise in Tuvalu is an illusion. He has used that conclusion to support an argument that there is no clear evidence for climate change. Webb, on the other hand, thinks that poor coastal management and climate change are acting in concert. Oddly, when he presented this sinister synergy to the  falekaupule , or council of elders, in Fogafale, their reaction was something like relief; a remote and difficult problem became a local and understandable one. They learned there were things they could do to mitigate erosion \u2014 they could further regulate beach mining and carefully dredge the lagoon for the island's aggregate needs, for example. Many scientists and people within the government insist that Tuvalu must step up the implementation of \u2018no-regrets\u2019 policies \u2014 activities that make sense whether the seas rise or not. These include introducing salt-tolerant crops and dealing with the island's highly visible trash problem. The process is painfully slow. Distressed that its capacity for action was limited to its own shores, in 2000 Tuvalu joined the United Nations for the express purpose of highlighting climate change. It was a considerable expense for a country that had a GDP at the time of just US$12.2 million. But membership allows Tuvalu to play a role as the most vocal and insistent of the small island developing nations, positioning itself as the conscience \u2014 or pest \u2014 of climate negotiations. \u201cThe current strategy is to continue making noises in the international forums,\u201d says Prime Minister Maatia Toafa. \n               Exit strategy \n             In addition to championing increased adoption of the Kyoto Protocol, which aims to curb greenhouse-gas emissions, Tuvalu also wants to discuss immigration policies with Australia and New Zealand. More open policies would provide both economic opportunities and the possibility of a new home if, or when, the islands become uninhabitable. Critics charge Tuvalu with using the sympathy generated by its position to increase the number of Tuvaluans living abroad and the remittances they send home. Toafa, a casual and jovial fellow who kicks off his sandals and props his feet on the table as we speak, is candid about their intentions. \u201cIt will work both ways,\u201d he says. \u201cOne, as an opportunity for people to go and develop their lives there, and secondly as a way of easing this resettlement problem.\u201d Ideally, Toafa would like to buy land in New Zealand or Fiji to resettle the entire nation. \u201cBecause we love the sea, we need a place close to the sea. And we know these are very expensive places,\u201d Toafa says. But relocation is more than a logistical and economic problem. It threatens their national and cultural identity \u2014 \u201cunless we can develop an underwater Tuvalu,\u201d he adds with a high-pitched laugh. So far, New Zealand has offered to accept more Tuvaluans, but that brings the total to just 75 a year, and even then it is as part of a labour programme, not resettlement. Despite reports in the press that the evacuation of Tuvalu is already under way, very few are going. \u201cI don't know if anyone wants to leave,\u201d says Pepetua Latasi, climate-change coordinator for the environment department. \u201cPeople are saying Tuvalu will be gone in 50 years' time, but I doubt it.\u201d Still, even Latasi, who is optimistic about both local efforts and international mitigation, concedes that if the expected rises in sea level coincide with increases in cyclone activity, the prospects are bleak. On another day of king tides, I walk down to the end of the airstrip, where floods overflow the edge of a trash-filled pit. Three boys are trying to turn over a half-submerged rowing boat. It capsizes, but they paddle back and have another go. \n                     Warnings rise over rising seas \n                   \n                     Sea-level rise is quickening pace \n                   \n                     Immigration could ease climate-change impact \n                   \n                     Oceans extend effects of climate change \n                   \n                     Climate Change in focus \n                   \n                     Climate and Water web focus \n                   \n                     Samir S. Patel's blog \n                   \n                     Tuvalu website \n                   \n                     IPCC report: Climate Change 2001 \n                   \n                     South Pacific Sea Level and Climate Monitoring Project \n                   Reprints and Permissions"},
{"file_id": "4401106a", "url": "https://www.nature.com/articles/4401106a", "year": 2006, "authors": [{"name": "Michael Bawaya"}], "parsed_as_year": "2006_or_before", "body": "Archaeologists are bringing past worlds vividly to life on the computer screen. But are the high-tech graphics helping science, or are they just pretty pictures? Michael Bawaya takes a look. There's more than one way to sink a ship, as Donald Sanders knows. President of the Institute for the Visualization of History in Williamstown, Massachusetts, Sanders spends a lot of his time repeatedly sinking a vessel off the coast of Cyprus. The ship isn't real \u2014 it's a computer model of a vessel that sank in the fourth century  BC . Sanders is trying to recreate what happened when the ship went down, leaving nearly 500 intact amphorae, or storage vessels, to be found centuries later on the sea floor. By loading his ship with a virtual crew and cargo, then sinking it in a number of different potential disasters, Sanders hopes to find a sequence of events that closely matches the archaeological evidence, and so work out might have happened centuries ago. His project, which should be completed later this year, is just one example in the growing field of virtual archaeology. It is a highly visual way to recreate lost worlds, helping researchers to imagine environments long past. Since the field emerged in the early 1990s, archaeologists have created a number of virtual sites, including Pompeii and the tomb of Nefertiti. Virtual archaeology can recreate individual objects, such as a hand axe, or entire sites and landscapes, such as Stonehenge \u2014 an ambitious project that took six months to complete. The possibilities of the technology are enormous, says Jeff Clark, director of the Archaeology Technologies Laboratory at North Dakota State University in Fargo. \u201cVirtual archaeology\u201d can allow us to explore ideas in ways that are otherwise not possible,\u201d he says. Sometimes, it may seem a bit too real. The technology is so impressive that it can eclipse the importance of the models, as users navigate their way through elaborately designed palaces or villages. Sarah Kenderdine, an archaeologist and curator at Museum Victoria in Melbourne, Australia, says some academics regard virtual heritage work as \u201ckitsch and consumerist\u201d rather than scientific and educational. But even if the models do resemble a video game, they can also offer an unparalleled entry point into a vanished world, providing new and unexpected insight. For instance, a three-dimensional representation of Rome's Colosseum, created several years ago by a team at the University of California, Los Angeles, has shown researchers just how cramped some of the upper hallways really were \u2014 confounding their expectations. Key to the new insights are technologies that let archaeologists peer into obscured artefacts and document landscapes in great detail. Computed-tomography scans of Egyptian mummies, for instance, can reveal internal structures nonintrusively. Laser and computed-tomography scans of rare and fragile human remains or artefacts produce three-dimensional models that can be measured and analysed in a way that real objects cannot. Some archaeologists are even taking the technology into the field, armed with laser scanners to create three-dimensional replicas of their findings as the excavation progresses. Such data are then fed into a modelling or drawing program, and displayed on the Internet. Researchers who live thousands of miles away, or theoretically years in the future, can then access these items. \n               Looking back \n             It sounds expensive, but it doesn't have to be, says Robert Stone, director of the Human Interface Technologies Team at the University of Birmingham, UK. Early virtual reconstructions were powered by supercomputers that cost hundreds of thousands of dollars. Today, he says, experts can achieve the same effects, and more, with a standard desktop computer. But software remains expensive; Sanders, for instance, spent nearly US$30,000 on the programs he uses to sink his virtual ship. The time spent working on such projects can also increase the cost. At Sanders' Massachusetts laboratory, projects run the gamut from individual objects, such as a small model of an ancient Egyptian ship that might take a day to complete, to entire building complexes, such as a ninth-century- BC  Assyrian palace with hundreds of rooms, which could take a year to finish. The former might cost a few thousand dollars, whereas the palace could run into hundreds of thousands. \u201cDoing virtual reality is becoming easier,\u201d he says, \u201cbut it still requires computer-modelling skills, data-interpretation skills and some programming to get it right.\u201d In many cases, it also takes an interdisciplinary team that includes archaeologists, architects, art historians, programmers and graphic modellers. And they must be trained to understand the cultural importance of their work, says Maurizio Forte, an archaeologist at the Institute of Technologies Applied to Cultural Heritage in Rome, Italy. \u201cTechnology is only a tool,\u201d he says. \u201cThe method is the core.\u201d Despite its capabilities, virtual archaeology has been slow to catch on. Sanders estimates that currently 80\u2013100 projects are under way at any one time. Western Europe is the most generous funder of such research. Kenderdine, in Australia, taps into European sources to fund worldwide projects such as her recreation of the ancient Cambodian city of Angkor. At the museum in Melbourne, visitors can experience Kenderdine's work inside a \u2018virtual room\u2019, an octagonal laboratory with panoramas displayed on the walls. The images aren't computer-generated, but are stereographic, high-resolution photographs. The idea is for visitors to be immersed in the ruins of Angkor, as if they were actually there. Such detailed recreation requires considerable research. A virtual model must be geometrically complete, says Elizabeth Riorden, a researcher at the University of Cincinnati, Ohio, who works on virtual reconstructions of the site believed to be the ancient city of Troy. \u201cBut we often have very incomplete knowledge of an archaeological site,\u201d she says. So researchers may be forced to resort to educated guesswork. Some archaeologists are sceptical because they don't understand the methods and assumptions used to create models, says Sanders. And there are currently no generally accepted guidelines. A February 2006 conference at King's College London aimed to address the issue of transparency. For the first time, virtual-heritage experts agreed to draft specific guidelines for documenting the creation of virtual models. These might, for instance, tell the viewer that 75% of a model was built using data from the archaeological record, and the rest was based on assumptions. \n               Image consultants \n             However it is created, a virtual model may end up looking so realistic that, as Clark notes, \u201cit imparts a sense of truth that in fact does not exist\u201d. Some virtual models could be partly accurate and partly not because one portion is based on abundant archaeological information whereas another portion is based on conjecture. \u201cIf you really try to get the visual simulation as accurate as possible, you must research the nitty-gritty details,\u201d he says. Most researchers agree that the technology is making archaeology more accessible to the public, as models are featured in museums and television documentaries. Displays can be posted on the Internet, in visitors' centres and in schools. Archaeologists may in fact be the hardest to bring around. \u201cMultidisciplinary communities embrace virtual archaeology as a key part of their research,\u201d says Forte. \u201cTraditional archaeologists are still very conservative and think virtual archaeology is a game.\u201d The cost and complexity of the work may keep some researchers away. One of virtual archaeology's grand ambitions, says Sanders, is \u201cto recreate the past in all its complexity, with people, animals, climate, vegetation and the ability of the scene to change and adapt to external conditions\u201d. And the more detail needed, the greater the technical and financial demands (see  \u2018Recreating ancient life\u2019 ). \u201cGeorge Lucas could probably do it if he wanted to,\u201d he says. But the evolution of video-game technology is helping virtual archaeology develop, says Stone. He recently gave his undergraduates the task of creating three-dimensional virtual models, complete with animation. Although none of the students is an experienced programmer, they used gaming software to make impressive models \u2014 ranging from an Afghan village with blowing sand and swaying trees, to an underwater wreck replete with marine life. It may be that virtual archaeology won't need someone like George Lucas after all. \n                     Is it small or just far away? \n                   \n                     Computer users move themselves with the mind \n                   \n                     Computers with every cadaver \n                   \n                     Virtual-reality mummy \n                   \n                     Troia Project \n                   \n                     Institute for the Visualization of History \n                   \n                     Archaeology Technologies Laboratory \n                   \n                     Virtual Heritage Lab \n                   \n                     The Virtual Room \n                   Reprints and Permissions"},
{"file_id": "440860a", "url": "https://www.nature.com/articles/440860a", "year": 2006, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "In a world of declining biodiversity, botanical gardens are coming into their own \u2014 both as storehouses of rare plants and skills, and increasingly as centres of molecular research. Emma Marris reports. At this moment there are a couple of ladies, one in her sixties, one in her eighties, walking through a greenhouse, their silver heads surrounded by hanging orchids in oranges, scarlets and lavender-tinged whites. These ladies, orchid-fancying mother and daughter, are bound to be there, because they were there in every garden visited during the writing of this feature. They are the embodiment of the botanical garden as popularly imagined: cosy, slightly old-fashioned, detail-oriented, perhaps a touch eccentric. But these greying dryads do not tell the whole story. Tucked behind the palmhouses of the world's larger gardens are buildings filled with industrial wheeled shelves for mounted plant specimens, scientists sitting round-shouldered over microscopes, and growth chambers housing rows and rows of seedlings. Botanical gardens have always been repositories of knowledge as well as cuttings. For centuries they were the heart of botanical scholarship. These days, most plant scientists work in academic settings, and increasingly study plants at a molecular level not obviously suited to the setting of greenhouses and flowerbeds. The challenge for botanical gardens is to maintain a place in the scientific world while remaining true to their hybrid heritage, a heritage that encompasses aesthetics, exploration and education as much as academic study. The two traditional scientific specialities of botanical gardens are plant taxonomy \u2014 the discovering, naming and sorting of species \u2014 and whole-organism biology \u2014 the study not of the ecosystem of the wild tulip, or the cellular functions that the tulip shares with a bunch of other plants, but of the tulip itself. Now, several of the world's larger gardens are broadening their focus and undertaking the sort of molecular investigation more typically found in research universities. Some botanists worry that this move stretches resources that would be better focused on the gardens' traditional strengths. But researchers within the gardens argue that a botanical garden, filled with so much knowledge of plant diversity and bristling with plant life, is the ideal place to do such work. As head of molecular systematics at the Royal Botanic Gardens in Kew, southwest London (see  \u2018Kew Gardens\u2019 ), Mark Chase pores over DNA and RNA to probe plant relationships. But he'll still pop out on a spring morning to look at the garden's many irises. He calls it \u201creciprocal illumination\u201d. Looking at the plant, then at the molecules, then back at the plant, \u201cyou really see things you hadn't noticed previously,\u201d he says. Michael Donoghue is a university plant systematist who, although firmly rooted at Yale University in New Haven, Connecticut, understands the lure of working in gardens. He argues that molecular systematics are \u201cpart of the gardens' mission to understand plants in all their glory\u201d. The New York Botanical Garden's grand foray into genomics \u2014 the $23-million Pfizer Research Center is opening there next month \u2014 may \u201craise some eyebrows\u201d, Donoghue says, \u201cbut why not endeavour to do all you can?\u201d He adds that his counterparts at gardens are able to tackle much meatier projects for longer periods: \u201cWe go from grant to grant; at the gardens, they have a longer view and really devote a lot of time and energy to one mission.\u201d \n               Keeping up traditions \n             But Richard Olmstead, a molecular systematist at the University of Washington in Seattle, echoes the worry in the botany world that such moves could diminish the gardens' focus on taxonomy and whole-organism biology. \u201cInsofar as any of their research is diverted towards more modern approaches away from those more traditional approaches, those traditional things are not going to get done, because no one is picking them up,\u201d he says. No one is picking them up because there is, at the moment, no money in it. \u201cMany people follow the money down to the molecular end,\u201d says Peter Crane, director of Kew, referring to the way that botany is often practised in academia these days. \u201cIt's a bit of a shame. We need a more integrated plant science.\u201d Edward Schneider, president of the Botanical Society of America, and a scientist at the Santa Barbara Botanic Garden agrees: \u201cWe need to preserve the understanding that a plant is more than just a bag of genes.\u201d Membership in the American Society of Plant Taxonomists has been steady over the past ten years. Nevertheless, there does seem to be a want of expertise on plant diversity in the average university department. \u201cThe number of people does not seem adequate to deal with the challenges that face the field,\u201d says Peter Raven, president of the Missouri Botanical Garden in Saint Louis (see  \u2018Missouri Botanical Garden\u2019 , overleaf). When universities want to compare species to deepen their understanding of a molecular finding, he argues, they increasingly turn to botanical gardens for help. Some think that universities have abandoned the whole-organism approach. Dennis Stevenson, vice president for botanical science at the New York Botanical Garden, has a guess as to why: \u201cTheir faculty can't go running off into the jungle.\u201d In contrast, many botanical gardens' researchers start their projects with a vigorous mosquito-slapping tour through the remote and fecund places of Earth in search of specimens. \u201cMost of the world's plant-diversity specialists are in the north, and most of the plant diversity is in the south,\u201d explains Crane. The most tangible fruits of these trips are large numbers of dried plants mounted on cards and bits of DNA in tubes filled with silica gel. As well as being an insurance policy in the event of rare plants going extinct, the DNA samples feed into molecular work. The mounted plants end up in herbaria \u2014 libraries of many millions of plants that are consulted by scientists. Although many university departments still have herbaria, they are increasingly becoming concentrated in botanical gardens. \u201cBotanical gardens are becoming depositories of herbaria, because universities are moving in a more molecular direction,\u201d says Schneider. Jim Solomon runs the Missouri herbarium. He says that when he feels anxious or stressed he likes to sit down and sort a box of uncatalogued specimens. The plants last a surprisingly long time, attached to paper backing with linen strips or Elmer's glue, and, although often rather brown, can retain a few lively characteristics. A maple specimen \u2014  Acer macrophyllum  \u2014 collected in 1892 by Emma Shumway in Seattle, Washington, looked as if it had been caught falling from the tree just seconds ago. \u201cAs habitats are modified, as plants go extinct, these collections will become increasingly important as a record of what these things looked like,\u201d says Solomon. \n               Plant protectors \n             Although preserving the vanished is important, preserving the all-but-vanished is even more so. Botanical gardens support conservation in two ways. They study plant diversity and establish what is rare where. And they grow rare plants and keep their seeds.  Cosmos atrosanguineus , the chocolate cosmos plant, can now be smelt and seen only in gardens such as Kew, because it is extinct in its native Mexico. Unfortunately, all the chocolate cosmos plants alive today seem to be cuttings of a single plant \u2014 so they are \u2018self-incompatible\u2019 and won't reproduce. Preserving genetic diversity is a large part of the gardens' agenda. \u201cThere is an increased focus on not just having one individual with a label on it, but creating  ex situ  populations of threatened plants,\u201d says Pete Hollingsworth, director of genetics and conservation at the Royal Botanic Garden Edinburgh. The London-based Botanic Gardens Conservation International (BCGI) keeps some 600 member gardens in touch with each other on conservation matters. \u201cI would say that the vast majority of our members have some sort of research programme,\u201d says Suzanne Sharrock, director of public awareness and understanding at BCGI. \u201cIn some cases it would be a very small programme, but the fact that they are botanical gardens rather than just public parks indicates that they are trying to be something more than a nice place to have a picnic.\u201d Botanical gardens in Australia, South Africa, Brazil, and, increasingly, China, are praised by botanists from Europe and North America for their garden science and conservation activities. Sharrock thinks that if they all spoke with one voice, that voice might be pretty loud. \u201cThere are botanical gardens in every country in the world, and in every major city,\u201d says Sharrock. \u201cThey have the potential to be a very powerful force.\u201d At the Second World Botanic Gardens Congress in 2004, the delegates adopted 20 goals for 2010 in support of the Convention on Biological Diversity's Global Strategy for Plant Conservation. Among these goals is doubling the number of \u201ctrained botanical-garden staff working in conservation, research and education\u201d. Another is compiling \u201ca working list of known plant species\u201d. Various bits and pieces of such a possible database already exist. The International Plant Name Index strives for comprehensiveness in cataloguing the seed plants of the New World and Australia, but it only lists names. Raven thinks that his garden's database of vascular plants (pretty much everything except mosses and liverworts), TROPICOS, might be \u201can important part of a final strategy\u201d, pointing users to published work on the species, and in some cases to maps of ranges. Plans for the ultimate database inevitably lead to talk of DNA barcoding. If species-specific differences in defined DNA sequences were matched with a species name in some kind of database, an untrained person could use a sequencer or a DNA-chip to read the barcode in a botanical sample, send it to the database, and get back a name and all other necessary taxonomic data. Apart from its undoubted geeky appeal, such a technology would in principle save a lot of time and drudgery. Carrying out identifications for colleagues at home and round the world is time consuming and uncompensated. The use of barcoding would free up people to do their own research. But Raven is cautious about such a scheme. He thinks that many lifetimes could be consumed setting the system up \u2014 time that would be spent making lists rather than learning anything about the organisms. \u201cThere are many millions of nematode species,\u201d he says, choosing an animal example. \u201cIf I had a slide of every nematode in the world, what would I do with it?\u201d Similarly, what would one do with barcodes for the 13,000 or so moss species? \n               Gardener's tips \n             There is, after all, so much else to look at, especially when one has the intellectual freedom that a large botanical garden can provide. Their widespread sources of funding and, in some ways, more forgiving range of stakeholders to please, mean that gardens can consider long projects and quirky studies that universities would be hard pushed to take on. In Missouri for example, 42% of the funds allocated to research comes from tickets, memberships and sales, and 58% comes from government grants, private donors and foundations. You can be \u201cmore creative\u201d, says Ken Cameron, an orchid specialist and head of molecular systematics at the New York Botanical Garden. \u201cAt a university, you don't really have a choice. You have to toe the line and study one of these model organisms.\u201d He prefers to study \u201cweirdo little orchids that nobody cares about\u201d. At the gardens, researchers also enjoy a great deal of public support. Amateur gardeners want to know how to keep their plants alive and blooming, and look to the professionals for help, as well as for inspiration. They are also often curious about the microscopic details and ecosystem-level stories behind plants. This makes botanical gardens ideal forums for fostering the public understanding of science. The Royal Botanic Garden Edinburgh is raising funds for a building designed specifically to mediate interactions between scientists and the public. This will open \u201cwith a bit of luck\u201d in 2009, says Stephen Blackmore, regius keeper of the garden. \u201cA lot of people want to hear about the research,\u201d says Blackmore, adding that scientists in botanical gardens seem to be regarded as \u201ca good source of reliable, factual information\u201d on everything from climate change to genetically modified organisms. The exchange of information is not all one way. Amateur gardeners can have valuable information to impart. \u201cFor many gardeners, the information is in their heads. The tricks they use to propagate plants \u2014 how to grow a pineapple in Cornwall \u2014 don't always get written down,\u201d says Crane. The value of this knowledge has only just been realized. Crane says he sometimes looks at the churchyard of Saint Anne's, opposite Kew's main offices, and thinks of the lost skills buried with generations of gardeners there. Raven feels that amateur gardeners could do more to keep rare and endangered plants alive. \u201cThere is probably more scope for scientists to get involved with gardening. I could see a lot of room for home gardeners maintaining genetic diversity in a world that's becoming more homogenous.\u201d And why not have them do research too? Some botanical gardens, including the one in Edinburgh, are already trying to track the subtleties of climate change by comparing various plant milestones year on year. Bringing home gardens into such networks would greatly increase the geographical reach of Edinburgh's researchers. Raven even imagines gardeners being issued with genetically identical indicator plants to make the data set really tidy. Gardens may thus have a functional role in the struggle to understand environmental change. But as important or more so, say Crane and Blackmore, is their inspirational role: the model they provide of how to relate to the flora of the Earth. Metaphorical thinking about the plant world has swung like a pendulum over the past decades. A hundred years ago, humans saw plants as resources to be deployed in ways that best served man \u2014 whether in amber fields of grain or in formal strolling gardens. With the rise of environmentalism, the view that humans should let nature run its course and the wild run wild has gained strength. But the wild, these days, is rather piebald. Roads and wires and concrete interrupt it almost everywhere. Some of the major constituents in various ecosystems are all but gone. So to keep what's there, it may be necessary to actively care for what is left, rather than to leave it be. \u201cMost ecosystems are not what they were, and they have to be managed,\u201d says Crane. And care is the hallmark of the gardener. Blackmore says that only \u201clong-term thoughtful intervention\u201d will protect plant diversity. And in the future, with climate change increasingly apparent and familiar cycles out of whack, only a competent, calm cadre of scientific gardeners may be able to tell the world how to keep the plants we rely on going, he says. \u201cBotanical gardens are really the only places at the moment that have the skills to adapt the landscape to those changing conditions. Maintaining biodiversity in the face of climate change is going to be a very active process.\u201d \n                     One place, one parent, two species \n                   \n                     Sex and the single flower \n                   \n                     Global Strategy for Plant Conservation \n                   \n                     International Plant Names Index \n                   \n                     TROPICOS \n                   \n                     Botanic Gardens Conservation International \n                   \n                     The Cycad pages from the Royal Botanic Garden Sydney \n                   Reprints and Permissions"},
{"file_id": "4401108a", "url": "https://www.nature.com/articles/4401108a", "year": 2006, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "Geophysicists are racing to understand a recently discovered phenomenon deep in the Earth. David Cyranoski joins them. It's a place where everything goes weird. Some 2,700 kilometres inside the Earth, just above the boundary between the planet's solid mantle and its liquid outer core, is a layer where seismic waves act strangely. Instead of carrying on through the Earth, some waves speed up when they enter the zone and, once inside, can vary in speed depending on their direction of travel. In some pockets, they can slow by as much as 30%. Such seismic waves are scientists' primary means of probing Earth's depths; waves bouncing through the planet's interior have revealed how Earth is layered into crust, mantle, outer core and inner core. But what could account for the seismic oddness of this region, known as the D\u2033 (or D-double-prime) layer? Until recently, nobody even knew what questions to ask. \u201cA couple of years ago, deep-Earth seismology seemed stagnant,\u201d says Christine Thomas, a seismologist at the University of Liverpool. But in 2004, a group led by Kei Hirose at the Tokyo Institute of Technology reported that they had reproduced the high pressures and temperatures of the lowermost mantle in the laboratory (see  \u201cA collaboration on the rocks\u201d ). And they found that the predominant mineral there \u2014 a magnesium silicate, MgSiO 3  \u2014 can take on a previously unknown structure 1 . The structure, known as the \u2018post-perovskite phase\u2019 of MgSiO 3 , has the same mineralogical composition as the more common perovskite form, but its atoms line up differently. \u201cThe denser post-perovskite is like a stack of two-dimensional sheets, compared with the three-dimensional bonding structure of perovskite,\u201d says Hirose (see graphic). And that shift in atomic arrangement, known as a phase transition, could help explain the seismic oddities of Earth's lower mantle. \n               Is that it? \n             Phase transitions are common in geology, and to some Hirose's explanation seems disappointingly mundane. \u201cWhat we were searching for all those years was nothing more than a phase transition,\u201d says John Brodholt, a mineral physicist at University College London. Nevertheless, post-perovskite has energized seismology, geodynamics and mineral physics. In a flurry of publications, researchers have debated the structure's properties, what other seismological mysteries it might explain, and its implications for heat flow within the mantle. Planetary experts are re-examining some quirks of Earth's rotation in light of the post-perovskite discovery, and are even thinking about how it could explain features of other planets. \u201cAs theorists we had been waiting for experimental clues,\u201d says Artem Oganov, a crystallographer at Swiss Federal Institute of Technology in Zurich. \u201cThis stimulated us.\u201d After the first publication by Hirose's group, mineral physicists confirmed, both experimentally and theoretically 2 , 3 , 4 , the existence and structure of post-perovskite MgSiO 3 . And once researchers started looking at other minerals, such as germanates, which have germanium in place of silicon, they found it there as well. \u201cIt's surprising no one found it earlier,\u201d remarks Thomas Duffy, a geophysicist at Princeton University in New Jersey. Post-perovskite seemed to explain many longstanding mysteries. For instance, simulations by Brodholt and his colleagues tackled the question of why a kind of seismic wave, called shear waves, suddenly accelerates at certain places within the D\u2033 region. Shear waves also behave differently there than do compressional waves, another type of seismic wave. If the region contains post-perovskite, rather than perovskite, post-perovskite's properties would explain both observations 5 . Similarly, post-perovskite can also explain why shear waves travel at different speeds within the D\u2033 region depending on their direction, a phenomenon known as seismic anisotropy. The atomic orientation of post-perovskite permits these waves to travel more quickly along certain crystallographic axes than others \u2014 giving rise to a pattern of anisotropy that can account for seismic observations in the D9 region 6 , 7 . \n               Deep questions \n             Such atomic-level studies illuminate bigger questions in Earth science \u2014 such as how heat and rock flow from deep in the Earth towards the surface, says Hirose. Figuring out how the lattice structure is deformed as perovskite becomes post-perovskite can tell scientists the rate and direction of flow within the lower mantle. Post-perovskite could help power the great engine of mantle convection in other ways. In some relatively cool regions of the mantle, the phase transition occurs at lower pressure, meaning at a shallower depth. Compared with adjacent warm regions, the newborn post-perovskite is not only cooler but also more dense \u2014 giving it another reason to sink. \u201cThe phase transition is an additional pump helping to drive convection,\u201d says Thorne Lay, a seismologist at the University of California, Santa Cruz. If post-perovskite is really so significant in inner-Earth dynamics, it would be a major coup for the newly discovered structure. But for now, post-perovskite often seems to provide more questions than answers. For example, the studies of post-perovskite's role in seismic anisotropy all give different predictions of how the deformed structure would look. Researchers know they must fit the correct deformation pattern into their models \u2014 but they don't know which one is right. Also missing is a precise understanding of the relationship between the pressure and temperature at which the phase transition occurs. The change is known to happen over a specific range of pressure and temperature, with higher temperatures requiring higher pressures. But pressure readings can vary by as much as 15 gigapascals depending on the laboratory equipment used in the experiments \u2014 translating to a difference in depth of 350 kilometres. \u201cThe pressure scale is the largest source of uncertainty,\u201d said Hirose last October, at a workshop on post-perovskite in Tokyo. \n               Spin doctors \n             Given that temperature varies greatly over the D\u2033 region, it is hard to know where post-perovskite might show up, argues Paul Tackley, a modeller at the Swiss Federal Institute of Technology in Zurich. His simulations 8  suggest that a post-perovskite phase would be absent in relatively hot regions, which could be some 3,200 \u00b0C, or 1,000 \u00b0C hotter than other regions of the mantle at a similar depth. These are the regions thought to produce mantle plumes, the massive conduits of heat and rock that many think rise from deep in Earth to emerge as \u2018hotspot\u2019 volcanoes on the surface. Large uncertainties also remain over one of the key questions in Earth dynamics \u2014 the extent to which Earth's core of liquid iron interacts with its mantle. Iron atoms can replace some of the magnesium in MgSiO 3 ; Wendy Mao, of the University of Chicago, Illinois, has measured just how much iron post-perovskite can take up. The answer, she found, is much more than any previously known lower-mantle material 9 . \u201cIron may have a dominant and very complicated role in post-perovskite,\u201d she says. Iron absorption may help account for the dramatic slowdown of seismic waves in certain pockets at the base of the mantle. But iron would also broaden the pressure range over which perovskite could turn into post-perovskite, making it harder to pinpoint the boundary where the phase transition occurs. Such problems may arise from trying to understand a complex region with simple models of how post-perovskite behaves, says Lay. \u201cThe big issue is that the characterization of post-perovskite is all based on numerical models,\u201d he says. And for regions where other elements are intermixed in complex ways, \u201cit is not clear that the numerical models are really relevant to Earth.\u201d The details of the D\u2033 zone promise to get knottier before they get simpler. \u201cAfter the initial euphoria that post-perovskite may explain everything, people are now taking a hard look at its properties,\u201d says Mao. Despite the uncertainties, researchers are pushing into new areas. Shigeaki Ono, of the Japan Marine Science and Technology Center in Yokosuka is investigating MgSiO 3  as a possible explanation for one of Earth's mysteries. \u201cEarth's rotation speeds up and slows down a few milliseconds over decade-long periods,\u201d says Ono. An electromagnetic coupling between the core and mantle might cause this, but if so the lower mantle must be highly electrically conductive. Perovskite does not have high conductivity, so if the lower mantle is made mostly of perovskite \u2014 as many scientists believed \u2014 that theory cannot be right. But Ono has found that a post-perovskite form of Al 2 O 3  \u2014 an aluminium oxide that is better characterized than post-perovskite MgSiO 3  \u2014 is highly conductive. He argues that post-perovskite MgSiO 3  probably has many of the same properties \u2014 including a conductivity high enough to explain Earth's rotation discrepancy. \u201cIt's a new contribution of the post-perovskite,\u201d he says. \n               Other worlds \n             Some researchers have even looked at how post-perovskite MgSiO 3  might affect other planets. Renata Wentzcovitch, of the University of Minnesota, Minneapolis, and her colleagues have simulated how post-perovskite would behave inside gas giants or in rocky planets in other solar systems. In such high-pressure, high-temperature fields, they find, the MgSiO 3  would start to behave almost like a metal 10 . Many scientists are excited by the possibility that on Earth there may be perovskite in some regions beneath the post-perovskite zone. This would explain seismic data that suggest shear waves change speed twice, first accelerating and then slowing. It could also constrain the temperature variations to a range of depths. \u201cIt would provide a kind of thermometer of the lower mantle and core,\u201d says Thomas. For his part, Hirose is glad to have provided this useful, if confusing, discovery. Post-perovskite may not account for a large portion of the D\u2033 layer, but its existence is what matters. \u201cThe proportions might not be huge,\u201d he says, \u201cbut with its location at the thermal and chemical boundary between the core and the mantle, it is going to help us understand crucial things about the Earth.\u201d \n                      Spring-8 accelerator \n                   \n                     American Geophysical Union: Study of the Earth's deep interior \n                   Reprints and Permissions"},
{"file_id": "441017a", "url": "https://www.nature.com/articles/441017a", "year": 2006, "authors": [{"name": "Erika Check"}], "parsed_as_year": "2006_or_before", "body": "Elias Zerhouni has one of the biggest jobs in biomedical research \u2014 running the massive US National Institutes of Health. But is he leading the agency up the right path? Erika Check examines his tenure. In a related  News Feature  this week, Jacqueline Ruttimann investigates whether the respected US National Institutes of Health meets the needs of young postdoc researchers. On 6 April, Elias Zerhouni spent the morning on Capitol Hill, telling Congress to invest more money in the National Institutes of Health (NIH) \u2014 for the good of the country and its research enterprise. But even as he was making his case, scientists across the continent were e-mailing each other an angry editorial that blames Zerhouni for leading the NIH in the wrong direction. Published in the  Journal of Clinical Investigation , and penned by the journal's editor-in-chief Andrew Marks (A. R. Marks  J. Clin. Invest.   116,  844; 2006), the piece hits Zerhouni in a sensitive spot. \u201cThe current state of the NIH,\u201d writes Marks, \u201cprompts me to say to its director, Dr Elias Zerhouni: \u2018Obviously you are not a scientist\u2019\u201d Marks says he meant no personal disrespect for Zerhouni, who is a radiologist by training. \u201cThere's a real gap between the people that run the NIH and the people that depend on it for their survival to do research,\u201d says Marks, a physiologist at Columbia University in New York. He says he has received a steady stream of e-mails from other researchers, praising him for writing the piece. \u201cThe tone of the editorial reflects a level of frustration that is not unique to the author,\u201d says one senior biomedical lobbyist in Washington DC, who asked not to be named. Zerhouni's many high-profile supporters say he is steering the NIH on the right course at a crucial time in the agency's history; after an unprecedented doubling of funds between 1998 and 2003, the NIH's budget has recently flattened and may even drop in the coming year (see graphic). But Zerhouni's vision of streamlining the agency's management and sharpening its focus on clinical, interdisciplinary and technology-driven research has upset others, who say this is distracting the agency from what it has always done well: fund basic research. Zerhouni cuts a sharp figure in Washington; he is charming and well-respected. But the former competitive swimmer is also unafraid to tussle with his critics. The changes he is making, he argues, are good for the NIH and for science. \u201cYou have to stand up for the integrity of the agency, no matter how popular or unpopular that is,\u201d he says. \n               Hard act to follow \n             The job of the NIH director is never easy, but Zerhouni inherited a particularly tough challenge in following Nobel laureate Harold Varmus, who headed the agency for six years. Previous directors were noted for either their administrative skills or their scientific achievements, but Varmus was a rare combination of world-class scientist and effective political operator. He left in 1999, and the Senate confirmed President George W. Bush's nomination of Zerhouni to replace him on 2 May 2002. Some researchers were sceptical from the start. When he took the job, Zerhouni looked more like a successful chief executive than a traditional bench scientist. A radiologist, who did research on advanced imaging techniques and patented eight inventions, his many roles at the Johns Hopkins School of Medicine in Baltimore, Maryland, included being an administrator for six years. Helping run one of the country's premier translational science institutions had, he felt, prepared him for modernizing the NIH and its mission. When Zerhouni took the helm of the NIH, agency veteran Ruth Kirschstein had been serving as interim director, but of the 27 institutes and centres that comprise the NIH, 6 were without a permanent leader. And there was no clear plan for how the NIH would take advantage of its unprecedented rise in funds. \u201cYou couldn't help but think that you needed to create a vision for the NIH,\u201d Zerhouni says. A year and a half after arriving at the agency, Zerhouni unveiled his signature \u2018Roadmap\u2019 initiative. This was designed to address a frequent criticism of the NIH: that it doesn't do a good job funding cross-disciplinary, risky and large-scale science. \n               More for the money \n             The Roadmap encourages translational research, or science that takes findings from the lab to the clinic (see  \u2018From bench to bedside\u2019 ). And it has created funding mechanisms to promote innovation, such as the lucrative Pioneer Award for high-risk research. The Roadmap also funds team resources, such as chemical databases that can be searched for drug candidates. At first, the Roadmap \u2014 projected to cost $2.1 billion over five years \u2014 drew widespread praise as an attempt to reform some of the NIH's old ways of doing business. But soon after it was unveiled, Congress began passing budget increases that were far below what the NIH had been used to. In his editorial, Marks taps into researchers' frustrations with the Roadmap. Some thought it should not have been started simultaneously with the flattening of the budget (see \u2018 Money worries \u2019), and others felt it diverted funds away from the individual-focused, investigator grants called R01s on which basic-science researchers depend. \u201cThe NIH Roadmap is not a roadmap at all, but a yellow-brick road,\u201d writes Marks. \u201cIt looks like it will lead us back to Kansas, but the way is really fraught with danger, the end of the road is not really where we want to go, and it is all just a fantasy.\u201d Marks told  Nature  that he feels a basic scientist would not have unveiled the Roadmap at a time when money was tight. \u201cIf the director of the NIH had a deep understanding of what it's like to be in the trenches supporting your labs with R01 grants, the timing would have been different,\u201d he suggests. \u201cI thought that was perhaps insensitive.\u201d Others think Marks himself was insensitive for penning the editorial. \u201cThe insulting nature of the whole piece was destructive to what we all need to do in the community,\u201d says Varmus, now president of the Memorial Sloan-Kettering Cancer Center in New York. An NIH advocate who did not want to be named, but who has worked on Capitol Hill on and off for years, put it more strongly: \u201cHe handed the people who want to hurt us a bat.\u201d \n               Poor connections \n             The editorial seems especially vitriolic because it echoes an unstated feeling in the community that Zerhouni is not \u2018one of the group\u2019. Some say there is little he could do to overcome this bias. \u201cZerhouni doesn't have the lab-rat appearance, and there have been elements in the community that expect the NIH director to be exactly in their image,\u201d says Varmus. Zerhouni served as a consultant to the White House when Ronald Reagan was president. That and his nomination by Bush \u2014 whose administration has been accused of ignoring science at the expense of ideology \u2014 has not endeared him to many scientists. \u201cI think that in a more open administration, Elias would have been a very good leader,\u201d says Jerry Keusch, associate dean for global health at Boston University in Massachusetts and former director of the Fogarty International Center at the NIH. For his part, Zerhouni says: \u201cThere is a little bit of a political overture in the comments I hear that suggests scientists are opposed to this administration. I'm pretty independent as a director, and that's unfair.\u201d But others say Zerhouni could have strengthened his position by reaching out to researchers, both inside and outside the agency. For instance, embarrassing revelations about financial conflicts of interest in newspaper reports in December 2003 and in subsequent congressional hearings (see  \u2018Congressional conflicts\u2019 ), caught Zerhouni off-guard. \u201cIf he was the kind of leader who could have understood or connected better with the institute directors and bench scientists, it would have served him much better in that storm,\u201d says one researcher, who asked to remain anonymous. \u201cHe got no backup, and he did contribute to that.\u201d Zerhouni attributes much of the discontent to the flattened budget, and notes that competition for NIH grants has accelerated dramatically in recent years. From 1999 to 2003, the agency attracted 5,300 new grant applicants. Then, the agency gained an additional 5,000 new applicants just in 2004 and 2005 combined. Zerhouni calls it \u201ca perfect storm that is hitting the NIH\u201d. Some say it was clear that the agency couldn't keep getting double-digit increases for ever, and that the NIH leadership should have devised a better plan earlier on for dealing with the end of the budget hike. And even though Zerhouni arrived towards the end of the budget doubling, his critics say he hasn't done enough to soften the blow. \u201cI think he's dealt with it through prayer,\u201d says Arthur Caplan, a bioethicist at the University of Pennsylvania in Philadelphia. \u201cIt hasn't been addressed head-on.\u201d \n               Ringing the changes \n             But an analysis by Howard Garrison, director of public affairs for the Federation of American Societies for Experimental Biology, suggests that the total proportion of funding for R01 grants, seen as the mainstay of traditional investigator-initiated research, is about the same as it was before the doubling began. \u201cWhat has happened is that the awards have gotton bigger and the competition has gotton fiercer, expectations have risen, and the number of new grants has not gone up,\u201d Garrison says. Zerhouni says this means that the Roadmap is not killing off investigator-initiated research, as critics have charged. Many scientists defend Zerhouni's decision to launch the Roadmap when he did. They argue that it was an astute political move at a time when the NIH needed a new vision to move forward from budgetary and conflict-of-interest woes. Some researchers think the Roadmap helped win congressional support by showing that the agency had a bold vision that would guide new spending. \u201cI share with Elias the feeling that the Roadmap stimulates Congress to reinvest, to get over their feeling that they've doubled the budget and nothing more is needed,\u201d says Gerald Fischbach, who headed the NIH's National Institute of Neurological Disorders and Stroke from 1998 to 2001, and who is now vice-president for health and biomedical sciences at Columbia University. Zerhouni argues that the research community has long called for the NIH to become a more flexible, forward-thinking enterprise that is in touch with the emerging need for technology-driven, interdisciplinary work. A 2003 report from the Institute of Medicine, \u2018Enhancing the vitality of the NIH\u2019, explicitly called for such an approach. \u201cI've not yet heard a scientific criticism of the Roadmap,\u201d says Zerhouni. \u201cI've only heard: \u2018It's taking away money from my programme or my institute.\u2019\u201d With no term limit, Zerhouni could potentially stay in his post for as long as it takes to fully implement the Roadmap. As stormy as his first four years have been, it's not likely to get much easier, say observers such as former Congressman Jim Greenwood, who investigated conflicts of interest at the NIH. Greenwood, now president of the Biotechnology Industry Organization, based in Washington DC, gives Zerhouni high marks for starting the NIH down a new path. But he says it will take time for the agency and its constituents to come along. \u201cThis is about turning around an aircraft carrier,\u201d he says. \u201cYou can't measure the radius of the turn at first. But you don't make the turn unless you take the wheel and start spinning it \u2014 and that's what he's done.\u201d \n                     Early success claimed for Zerhouni's NIH roadmap \n                   \n                     NIH 'roadmap' charts course to tackle big research issues \n                   \n                     New NIH project could be road to ruin for basic research \n                   \n                     NIH head looks to the 'biomedical century' \n                   \n                     Radiologist in the picture for top job at NIH \n                   \n                     NIH Roadmap \n                   \n                     The Journal of Clinical Investigation \n                   Reprints and Permissions"},
{"file_id": "4401104a", "url": "https://www.nature.com/articles/4401104a", "year": 2006, "authors": [{"name": "Helen Pilcher"}], "parsed_as_year": "2006_or_before", "body": "Facing a moral dilemma in the lab? No reason to panic. Helen Pilcher meets the academic troubleshooters who promise a quick answer to any ethical problem. Sometimes you're midway into a research project before an ethical dilemma reveals itself. This was Joachim Hallmayer's experience at the Stanford School of Medicine in California. About a year after he and his team had begun recruiting children for a genetic study of autism, they realized that they couldn't agree what results to share with the parents. \u201cIt's sometimes difficult to know what information will be useful and what will be dangerous,\u201d admits Hallmayer, a psychiatric geneticist. But the team was in luck. Across campus, at the Stanford Center for Biomedical Ethics, a pilot project offering biomedical researchers speedy practical advice on ethical concerns was under way. Hallmayer contacted the \u2018dial-an-ethicist\u2019 project, or bench-side consultation service as it is also known, and three weeks later got professional answers to his questions. Hallmayer's team is now pursuing its autism study with a renewed sense of harmony. But the Stanford service aims to go beyond ethical troubleshooting, say co-founders David Magnus and Mildred Cho. \u201cA lot of scientists don't really see ethics as a part of their job,\u201d says Cho. By making the usually academic field of bioethics more accessible, Magnus and Cho hope to promote a culture of ethical thinking within the laboratory. Often scientists don't think about ethics until it is too late \u2014 sometimes when their research has already hit the headlines. In South Korea, disgraced stem-cell pioneer Woo Suk Hwang not only fabricated results, he also obtained eggs from women donors under questionable circumstances. And he did it all while claiming to follow ethical guidelines. It's an extreme example of the harm that can be caused when ethics advice is ignored, but one that raises questions about the role of bioethicists in the laboratory. Some people doubt whether practical ethics advice can make a real difference. Can bioethicists retain credibility when their advice is sought but ignored? Are they liable when things go wrong? And what about claims that they rubber-stamp most research proposals? \u201cBench-side consultations are a way of integrating ethical thinking into a scientist's everyday life,\u201d says Magnus, director of the Stanford centre. Like most of his colleagues he doesn't think bioethicists should be expected to prevent misconduct. But he believes his bench-side service can foster integrity in trainee scientists and so indirectly prevent research going off the ethical rails. The Stanford service is designed to help researchers identify the ethical and social issues that arise in their work and aims to complement, not replace, the bodies that regulate human and animal studies. Institutional review boards (IRBs), for example, oversee all federally funded US biomedical human studies. They evaluate the risks and benefits to people who participate, from their recruitment through to the confidentiality of results. An IRB seal of approval must be in place before a study begins. Unlike IRB approval, the Stanford service is voluntary, not mandatory, and it yields confidential advice, rather than edict. The Stanford team will advise researchers at any point in a study, although they prefer to be involved at the start. The pilot seeks to address issues, such as the broader societal implications of a study, that go beyond the scope of IRBs. In fact, the project was set up, in part, to offer advice on human embryonic stem-cell research, which initially fell outside the IRB's purview. \n               Brisk business \n             Over the past six months, the pilot service has given consultations to seven different Stanford research groups. Topics ranged from oncology trials to microarray analysis, and the ethical issues from conflicts of interest to what to do with incidental findings. Six of the queries were easily resolved, most within 24 hours, and half of the responses involved alerting researchers to existing rules rather than developing new policies. Hallmayer's request, however, required deeper analysis. The California Autism Twin Study (CATS) plans to assess 300 twin pairs on a range of skills, including intelligence, language and planning ability. To date, more than 60 pairs of children have been recruited and around a dozen tested. But as the data trickled in, ethical concerns grew about how much to tell the parents about the test results. This is partly because the CATS tests can differ from those used by psychiatrists to diagnose autism and are administered by trained researchers rather than clinicians. Such non-clinical test results raise several problems. A presumed autistic child could, for example, test normal, or a normal sibling could display mild, sub-clinical problems. DNA tests designed to check whether twins are identical or not, and to exclude children with the inherited mental impairment, fragile X syndrome, create their own problems. A positive fragile X result would suggest one parent is carrying the causal mutation, for example. \n               Should we tell them? \n             Any such findings could have repercussions for family dynamics, insurance and state benefits, and so Hallmayer felt it wise to tell parents less rather than more. \u201cThey may have a right to know,\u201d he says, \u201cbut the results can give a child a label that is unnecessary and meaningless.\u201d Others on the team, however, felt it was wrong to withhold findings from parents. Hallmayer contacted the Stanford ethics service last year. He and his team discussed their problems with four consultants, including Cho and Magnus. Three weeks later, they received a written list of recommendations. The ethics team concluded that because the test results do not indicate a treatable or preventable life-threatening condition, researchers are not required to inform the parents. Non-clinical test results should not be disclosed, they advised, but other results can be shared if the parents want to know. Positive fragile X tests should always be replicated in a clinical laboratory, and parents should be made aware of the possible implications for insurance and employment. The CATS team tweaked its study design, received additional IRB approval, and pursued its study. Both Hallmayer and the Stanford ethicists are pleased with the outcome. But the bench-side experience is not always a happy one. Bioethicist Insoo Hyun, from Case Western Reserve University in Cleveland, Ohio, spent three months in summer 2005 in Hwang's laboratory studying the cultural, social and legal implications of the Korean stem-cell research. He also helped to fine-tune Hwang's informed-consent procedure for egg donation, publishing a paper reporting the procedure in  The American Journal of Bioethics  (K. W. Jung and I. Hyun  Am. J. Bioeth.   6,  W19\u2013W22; 2006). When he learned in December that Hwang had not followed the procedure he felt angry, saddened and betrayed. Hwang seems to have obtained independent advice from at least two ethicists in addition to IRB approval for his research. But he violated ethical guidelines by collecting eggs from laboratory juniors and other donors who were not warned of possible side effects. About 20 of the donors have developed side effects including abdominal pain, diarrhoea and vomiting. And their eggs contributed to research that now appears to have been faked. The kindness of the donors has been exploited, patient hopes shattered, and public confidence in stem-cell research is in tatters. The scandal underscores the perils of ignoring ethical advice. But it also highlights the limitations of ethical consultations in the laboratory. Hyun and his co-author had to retract their paper when it became clear that Hwang had ignored their recommendations (G. McGee,  Am. J. Bioeth.   6,  W33; 2006). \u201cMy experience with Hwang has changed my attitude to getting involved with other labs,\u201d says Hyun. In future, Hyun says he would be more sceptical. Magnus, who has also written about the ethics of egg donation, believes that the ethicists in Hwang's lab did little to find ethical flaws. \u201cI am concerned that too many of the bioethicists there were cheerleaders,\u201d he says. In his defence, Hyun says, bioethicists view themselves as pilots through treacherous waters. \u201cBeing an ethical consultant doesn't put you in a position to prevent misconduct,\u201d he says. Consultants presuppose that researchers have the integrity to take their advice seriously. \u201cIf people are out to deliberately deceive you, there's nothing you can do.\u201d Some view all ethical oversight with suspicion. Bioethicists merely provide the veneer of a job well done, says Vera Sharav, president of the Alliance for Human Research Protection, New York, a watchdog organization that seeks to expose unethical research practices. Most, she believes, issue ethical approval too easily and fail to offer truly independent advice. But Ronald Green, a bioethicist from the Dartmouth College Ethics Institute, Hanover, New Hampshire, finds generalized accusations of \u2018bioethics for hire\u2019 preposterous. Green chairs the ethics advisory board at Advanced Cell Technology, a US-based biotechnology company developing stem-cell therapies. Any payment to ethicists is a minimal daily stipend, and with ethical advice carrying no weight in law, \u2018buying it\u2019 simply doesn't make sense, he says. For his part, Hyun believes his relationship with Hwang was sufficiently independent, with funding from a Fulbright scholarship, to allay such concerns. \n               Blowing the whistle \n             What would the Stanford ethicists do if they discovered research that was illegal or breached public health and safety? Although they sign confidentiality agreements with Stanford researchers, Magnus and Cho say they have an obligation to report misconduct if they come across it. But what about less serious transgressions? \u201cI don't think bioethicists should be policing research,\u201d says Magnus. To avoid potential criticisms, the Stanford team is financially independent thanks to federal funding. The service is now being offered to all biomedical researchers at Stanford; other institutions, including Case Western Reserve University and Duke University in North Carolina, are setting up similar bench-side consultations. Hyun is not involved in the Case Western service at present, although he has not ruled that out. The hope is that such a service will promote ethical awareness in the lab. It's not a watchdog, but it must avoid looking like a show dog. It might not prevent deliberate misconduct, but it should help researchers who want to do the right thing. \n                     Ethics and fraud \n                   \n                     'Ethical' routes to stem cells highlight political divide \n                   \n                     Altered embryos offered as solution to stem-cell rift \n                   \n                     US experts draw up guidelines for stem-cell research \n                   \n                     Korea's stem-cell stars dogged by suspicion of ethical breach \n                   \n                     Brain-scan ethics come under the spotlight \n                   \n                     Biologists seek consensus on guidelines for stem-cell research \n                   \n                     Ethics of therapeutic cloning \n                   \n                     Stanford Center for Biomedical Ethics \n                   Reprints and Permissions"},
{"file_id": "441143a", "url": "https://www.nature.com/articles/441143a", "year": 2006, "authors": [{"name": "Jane Qiu"}], "parsed_as_year": "2006_or_before", "body": "To correctly \u2018play\u2019 the DNA score in our genome, cells must read another notation that overlays it \u2014 the epigenetic code. A global effort to decode it is now in the making, reports Jane Qiu. Manel Esteller's phone did not stop ringing for weeks. It was summer 2005, and he and his team at the Spanish National Cancer Centre in Madrid had just published a study comparing the activity of DNA in identical twins. The anxious callers were invariably twins whose sibling had developed a serious disease such as cancer or diabetes. Could the study help predict whether they too would succumb, they asked. Did the identical DNA sequence they shared with their afflicted twin mean they had the same genetic predisposition to illness? Surprisingly, the answer to the second question is \u2018not necessarily\u2019. Researchers have known for years that, despite their common genes, identical twins can have very different physical constitutions and develop different diseases. The traditional explanation for this is that our environment somehow interacts with our genes to produce our physical attributes, or phenotype, but no one knew exactly how. The study by Esteller and his team 1  showed that the missing link between nature and nurture could lie in a phenomenon known as epigenetics: a cryptic chemical and physical code written over our genome's DNA sequence. The term \u2018epigenetics\u2019 was first coined in the 1940s by British embryologist and geneticist Conrad Waddington, to describe \u201cthe interactions of genes with their environment, which bring the phenotype into being\u201d 2 . The term now refers to the extra layers of instructions that influence gene activity without altering the DNA sequence. By studying 80 pairs of identical twins, ranging in age between 3 and 74, Esteller's team found that epigenetic differences were hardly detectable in the youngest twins, but increased markedly with age. These changes had a striking effect on gene activity: the number of genes that differ in activity between 50-year-old twins was more than three times that in pairs aged 3. \u201cSo we are more than our genes,\u201d says Esteller. \u201cNot only is the DNA sequence important but also how gene activity is regulated in response to environment. This might explain why many identical twins have different susceptibility to disease.\u201d \n               Spot the difference \n             As well as offering answers to identical twins, deciphering this epigenetic code promises to dramatically alter our understanding of disease in the wider population (see  \u2018Tagged for disease\u2019 ). Many cancers might be triggered by epigenetic faults, for example. It should also fill some big gaps in our grasp of how the environment affects a creature's constitution \u2014 epigenetic changes explain how simply altering the diet of a pregnant mouse, for example, can completely change the coat colour of her pups 3 , or even alter their response to stress 4 . \u201cIt will illuminate some of the most profound questions in biology,\u201d says Stephan Beck, an immunologist at the Wellcome Trust Sanger Institute, Cambridge, UK, who worked on the Human Genome Project. How a given cell executes its unique genomic programme in time and space could shed fresh light not only on development and disease but also on what makes us human, he says. The complete epigenetic code of our genome, its \u2018epigenome\u2019 has increasingly been the focus of research over the past decade, and scientists are now embarking on an ambitious attempt to crack it. The International Human Epigenome Project, or IHEP, first suggested by Beck and colleagues in 1999, is the logical next step after the Human Genome Project, which published the draft sequence of the human genome's 3 billion DNA letters in 2001. But the IHEP faces daunting challenges. The sequence of the human genome is the same in all our cells, whereas the epigenome differs from tissue to tissue, and changes in response to the cell's environment. Can researchers really hope to pin down this vast, complex and ever-changing code in a meaningful way? \n               Clever packaging \n             If the DNA sequence of the genome is like the musical score of a symphony, then the epigenome is like the key signatures, phrasing and dynamics that show how the notes of the melody should be played. Epigenetic control of gene expression occurs in two main ways: either the DNA itself is chemically altered, or the proteins that package DNA into chromatin (the main component of chromosomes), are modified. These proteins, called histones, determine whether the chromatin is tightly packed, in which case gene expression is shut down (or silenced), or relaxed, in which case gene expression is active. The first kind of alteration takes the form of methyl groups added to the DNA \u2014 frequently to the base cytosine when it is immediately followed by guanine \u2014 by a process known as DNA methylation (see graphic). The methyl group can be sensed by proteins that turn gene expression on or off through regulating chromatin structure. The second, more complex kind of alteration involves changes to the histones around which chromosomal DNA is wrapped. Each histone has a protruding \u2018tail\u2019 to which more than 20 chemical tags can attach, like charms on a bracelet. Some of these tags, or certain combinations of them, dubbed the histone code, give rise to relaxed chromatin; others have the opposite effect. Epigenetic codes are much more subject to environmental influences than the DNA sequence. \u201cThis could explain how lifestyle and toxic chemicals affect susceptibility to diseases,\u201d says Vardhman Rakyan, a researcher at the Sanger Institute. \u201cUp to 70% of the contribution to a particular disease can be non-genetic.\u201d Indeed, one key finding of Esteller and his team's study was that epigenetic profiles of twins who had been raised apart or had noticeably distinct lifestyles differed more than those who had lived together for a while or shared similar environments and experiences 1 . Rakyan himself is studying a cohort of identical twins, where one twin has type 1 diabetes and the other does not, with the aim of finding epigenetic changes associated with the disease. Although different labs around the world, such as Rakyan's, are already working on their own individual studies, several researchers argue that it is time for a coordinated effort. A series of international workshops, and expert and government reports have emerged in recent months that address the value and scope of an international human epigenome project. The ultimate goal of such a project would be to identify all the chemical modifications of DNA and histone proteins for all chromosomes in all types of normal human tissue. As for the HGP, an international consortium would set priorities, coordinate research efforts, centralize materials and resources, create the necessary technologies and monitor research progress. \n               Piece by piece \n             \u201cEpigenomics is at a stage where genomics was 30 years ago, when everyone was working on their part of the puzzle,\u201d remarks cancer biol\u2013ogist Peter Jones at the University of Southern California, Los Angeles. Jones was formerly president of the American Association for Cancer Research (AACR), which is based in Philadelphia. \u201cWe need to see the bigger picture. It takes concerted efforts on an international scale. And this is how the IHEP would make a difference.\u201d Although a number of funding bodies \u2014 such as the Wellcome Trust, the AACR, the US National Cancer Institute (NCI), and the US National Human Genome Research Institute (NHGRI) \u2014 have shown interest by taking part in the discussion, funding agencies have yet to commit to financing and leading the project. \n               All aboard \n             Since the completion of the Human Genome Project, there have been many multi-centre schemes, each of which costs millions or even billions of dollars. Some of these initiatives, such as the US National Institutes of Health Human Cancer Genome Atlas (which aims to identify and catalogue genetic mutations in human cancers), have prompted arguments over scale and cost-effectiveness. A key question for funding bodies is whether the IHEP would be yet another multi-million-dollar project. Proponents say no. \u201cThe goal of the IHEP is not to create another big enterprise, but to make things as cost effective as possible, to interface with wonderful projects that are under way and to fund important pilot projects,\u201d says Andrew Feinberg, director of the Centre for Epigenetics of Common Human Disease at the Johns Hopkins University in Baltimore, Maryland. A number of smaller scale multi-centre epigenome projects are already under way or under discussion in Europe, the United States, India and Japan (see External links, below). Most prominent is that set up by the European Human Epigenome Project (HEP) Consortium in 2000. Following the publication of a pilot project in 2004 (ref.  6 ), the European HEP Consortium will soon make its data on the epigenetics of the entire chromosomes 6, 20 and 22 publicly available. Although few people doubt the importance of an international human epigenome project, how to go about it remains a subject of debate. A key challenge is defining what the epigenome entails and what cell types to study. Some researchers argue that the project should first tackle blood cells, because they are easy to collect and work with, and are our main \u2018window\u2019 into the epigenome of both healthy and diseased individuals. Once a high-resolution blood epigenome is determined, it will serve as a reference with which other epigenomes, including those of diseased or ageing tissues, could be compared. But the diversity of epigenomes in different cell types means that it may not make sense to restrict pilot projects to one single tissue, or to a particular time in a tissue's development. After intense discussion in three recent international workshops 5 , 7 , 8 , 9 , researchers in the epigenetic community now agree that initially eight to ten tissues, including the blood, should be studied simultaneously. Ultimately, the epigenome of all tissues, including embryonic stem cells, will be mapped out. Another question is whether to study cells grown in the lab or biopsies of tissues taken from people. Biopsies contain different cell types, which would muddy the picture, but lab-grown cells might contain abnormal epigenetic tags. At the moment, some biologists are leaning towards lab-grown cells as being the lesser of two evils, but exactly how different the epigenomes of cell lines are compared with normal tissues remains to be seen. The inclusion of cell lines in some pilot studies in the proposed IHEP should be able to resolve this issue. \n               Final frontier \n             Perhaps the greatest challenges facing the IHEP are technological: mass-production-style tools must be developed to decode the epigenome, and the morass of data will have to be stored and analysed. At the moment, the main method used to determine DNA methylation sites is reliable, but extremely expensive, and the technology used to study histone marks is prone to problems with accuracy and reproducibility. Scientists hope to tackle these problems by linking the IHEP to projects on the epigenomes of lab workhorses, such as the yeast, fruitfly and mouse, for which techniques are more advanced. Computational scientists are also developing the sophisticated bioinformatics tools needed to store and analyse multi-dimensional epigenome data. Given these technological challenges, it is only natural to question whether the research community is ready for such an enormous undertaking. Drawing on the experience of the early days of planning for the HGP, researchers working on epi\u2013genetics are unanimous in thinking they can do it. They have drawn up a plan of how to manage the international assets available for the IHEP 5 , 7 , 8 , 9  and say that, like the HGP, the IHEP will catalyse its own development. \u201cOne can never be 100% ready. We have 60% of the technology to go for the real thing,\u201d says Thomas Jenuwein, a molecular biologist at the Research Institute of Molecular Pathology at the Vienna Biocentre, Austria. \u201cThe rest will happen once the momentum is built up. We should have that vision to go in big.\u201d \n                     Plan matures for partner to genome quest \n                   \n                     G-Protein-coupled receptors: Better beta-blockers \n                   \n                     Mother's diet changes pups' colour \n                   \n                     Human Genomics Web Focus \n                   \n                     European Human Epigenome Project Consortium \n                   \n                     The Epigenome Network of Excellence (NoE) \n                   \n                     HEROIC (High-Throughput Epigenetic Regulatory Organization in Chromatin) \n                   \n                     ENCODE (Encyclopedia of DNA Elements) \n                   \n                     The Cancer Genome Atlas \n                   \n                     Centre of Cellular and Molecular Biology, Hyderabad \n                   Reprints and Permissions"},
{"file_id": "441146a", "url": "https://www.nature.com/articles/441146a", "year": 2006, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "The Arctic is the bellwether of climate change, which shows up there first and fastest. Quirin Schiermeier visits ecologists struggling to keep up. On a splendid, freezing spring morning in Sweden's far north, global warming seems far away. The pristine landscape around the Abisko Scientific Research Station, 200 kilometres inside the Arctic Circle, is glistening white. Thick ice covers nearby Lake Tornetr\u00e4sk. The spring equinox has passed, and the days are quickly getting longer. But it is \u221215 \u00b0C as Gareth Phoenix, a plant ecologist from the University of Sheffield, UK, who has wintered at the station, wades outside to check a \u2018snow melt\u2019 experiment installed between mountain birches. Thin heating cables and four 1,500-watt lamps hanging half a metre above the ground have melted the snow in small patches, exposing shrubs and lichens. Phoenix looks pleased. \u201cWarming the Arctic outdoors in winter isn't terribly easy,\u201d he notes. With Massachusetts-based engineer Frank Bowles, Phoenix has spent weeks tinkering with the array so that it melts snow without toasting the plants. He thinks he's got it right now: the heat has melted 45 centimetres of snow in three-and-a-half days, exactly what can happen during extreme warming events in the Arctic winter. Such warming events occur much more frequently at Abisko now than at any time since climate records began there in 1913, says the centre's scientific director, Terry Callaghan. Ecologists are worried that the short-lived melting episodes, and the sudden return to cold weather afterwards, could harm plants and soils. Such disturbances could resonate through the whole ecosystem in such areas, they suspect, with potentially devastating knock-on effects for nutrient supply, plant growth and animal populations. Change comes faster in the Arctic than elsewhere. As the snow-free season lengthens and sea ice becomes less abundant, albedo \u2014 the proportion of sunlight reflected by the ocean and the ground \u2014 decreases. The sunlight that's not being reflected by snow and ice is absorbed instead, and this amplifies climate warming at high northern latitudes. Computer models and on-the-ground observations both suggest that the most pronounced warming will occur in winter 1 . At the Abisko station, mean temperatures between December and February have risen by around 5.5 \u00b0C over the past century \u2014 eight times more than the average rise in the Northern Hemisphere 2 . Similar warming has been observed throughout the Arctic, causing glaciers in Greenland to flow faster, permafrost soils in Siberia to thaw, and boreal forests in Russia and Canada to move further north. The Abisko station has become a prime location for studying the effect of climate change on terrestrial Arctic ecosystems. Streams of very warm air masses have been observed here at least once every winter over the past seven years, causing temperatures near the surface to rise by 25 \u00b0C or more within a day. The warm air melts the snow cover, exposing and sometimes killing the plants beneath. If the melted snow then refreezes, the shrubs and lichens become encrusted with ice and are no longer accessible as food for lemmings and reindeer. Recent population crashes of wild reindeer on the Arctic island of Svalbard are thought to be linked with such \u2018icing\u2019 events, although a connection is not proven. \n               Unknown unknowns \n             Experiments such as the snow-melt study, which will run for at least three years, are meant to clarify the short- and long-term effects of the melting episodes. Mimicking nature is not nature itself, however: Phoenix says that much more study is needed before scientists can hope to understand the complexity of the changes going on. \u201cThe problem is to find out what the most interesting thing is of what you are measuring,\u201d he says. \u201cYou often don't know what you don't know.\u201d Each year, around 700 scientists and students come to Abisko to study Arctic climate and environment, carbon cycles, lake ecosystems and geomorphology. The station has plenty of rooms and lab space, as well as a new scenic lecture theatre and conference facility. And unlike remote Arctic research bases, Abisko is wired with electrical power. This luxury not only makes a stay at the station a comfortable experience; it is also a prerequisite for running long-term outdoor experiments that require a constant energy supply. \u201cIt would hardly be possible to run a winter snow-melt experiment at a place like Toolik Lake station in Alaska, where you have only diesel generators,\u201d says Jerry Melillo, co-director of the Ecosystems Centre at the Marine Biological Laboratory in Woods Hole, Massachusetts. For his part, Callaghan argues that many more long-term stations, like Abisko, are needed for the monitoring efforts (see  pages 127  and  133 ). \n               From sink to source \n             A key problem, he says, will be to determine whether the Arctic, which currently accounts for one-third of soil carbon storage on Earth, is likely to remain a carbon sink, or whether it will turn into a source of carbon. As soils grow warmer, many worry that greater microbial activity could increase the rate of decomposition and lead to increased releases of methane and carbon dioxide 3 . For the moment, computer models suggest that the Arctic is still a small carbon sink. But the trends are highly inconsistent, says Callaghan. Most Arctic lakes, for example, seem to be saturated with carbon dioxide and have turned into carbon sources. At Abisko, tall measurement towers and small chambers around individual plants monitor the carbon flowing from soils and vegetation to the atmosphere and back again. Such data are then fed into complex carbon balance and vegetation models. But adding carbon-flux data from just one additional site can have a huge impact on the overall picture. At Abisko, Callaghan has seen how small perturbations can affect the carbon balance of a whole region. From his office window, he points at a bare slope on the distant shore of Lake Tornetr\u00e4sk. During the exceptionally warm winters of 1950 and 2004, eggs of the autumn moth ( Epirrita autumnata ), a caterpillar feeding on mountain birch, survived there in vast numbers. Later in the year the insects destroyed large swaths of the forest, which has not recovered since. Insect outbreaks such as this can convert a whole area from a carbon sink into a source. Climate warming sometimes brings what may look like good news. In Lapland, for example, the tree line has risen 60 metres since 1900, and satellite images confirm that forests and shrublands have also increased in the northern parts of Siberia, Canada and Alaska. The extra biomass, some believe, could suck up more carbon dioxide. But once again, the picture isn't simple. Because trees decrease albedo in comparison with the tundra vegetation they replace, their spread might actually accelerate warming in Arctic regions. A recent study in Alaska suggested that terrestrial changes in summer albedo plus lengthening of the snow-free season already has an effect similar in magnitude to the warming expected from a doubling of carbon dioxide. And as shrubs and trees continue to proliferate, as some models predict they will in a carbon dioxide-rich world, they could further amplify Arctic warming by two to seven times 4 . \u201cDespite what governments may say, forest growth may do more harm than good to the climate,\u201d says Callaghan. The Abisko station has taken a lead role in the European Union-funded BALANCE project, which investigates present and future climate-change vulnerabilities in the region. In so doing, the researchers at Abisko have turned to some valuable allies: the local Sami population of reindeer herders, whose indigenous knowledge of the area could help scientists assess changes in terrestrial Arctic ecosystems. \u201cThe Sami are all around the landscape and they see many things we don't,\u201d says Callaghan. \u201cThey are really the missing link between our individual observations and satellite imagery. And including indigenous knowledge just makes our own experiments more relevant.\u201d For instance, this spring saw the launch of a joint research project, including linguists, anthropologists and Sami academics at the Nordic Sami Institute in Kautokeino, Norway. \u2018Snow and Ice\u2019 will assess the impact of environmental changes and extreme events on reindeer herding and the movement of reindeer and people through the region. Lessons from the winter snow-melt experiment are just one thing scientists at Abisko hope to share with the Sami. As night falls over Lake Tornetr\u00e4sk, the eerie green veils of the aurora borealis drift across the starry sky. Relaxing in the improvised \u2018tundra bar\u2019 in the station's cellar, Callaghan is scheming about supplying the Sami with small, portable weather stations. Then they too could become part of Abisko's science network. \n                     Alaska's climate: Too hot to handle \n                   \n                     Arctic research: Summer in Svalbard \n                   \n                     Climate change in focus \n                   \n                     Abisko Scientific Research Station \n                   \n                     Toolik Lake Station \n                   \n                     BALANCE project \n                   \n                     Nordic Sami Institute \n                   Reprints and Permissions"},
{"file_id": "441274a", "url": "https://www.nature.com/articles/441274a", "year": 2006, "authors": [{"name": "Nick Lane"}], "parsed_as_year": "2006_or_before", "body": "Among their many talents, bacteria are the world's best electrochemists, creating a life-powering flow of electrons in a startling range of conditions. In the first of two features, Nick Lane asks what limits, if any, constrain this ability. In the second, Charlotte Schubert meets the people trying to put this microbial ingenuity to practical use. In 1977 Engelbert Broda, a physical chemist, then at the University of Vienna in Austria, made a startling prediction 1 . Nitrate and ammonia can and will react together in the way that oxygen and organic carbon molecules do. And so, in theory, it would be possible for a creature to make a living by breathing nitrate and eating ammonia, just as we do by breathing oxygen and eating proteins, carbohydrates or fats. The creatures Broda had in mind for this bizarre diet were not some race of aliens floating in a soup of ammonia under red, nitrate-laden skies, but earthly bacteria of a sort never before imagined. The discovery of bacteria capable of this \u2018anaerobic ammonia oxidation\u2019 or anammox reaction, in the late 1990s 2 , was a high-water mark for the rising tide of what might be called microbial triumphalism. The anammox reaction, it transpires, is a dazzlingly hard trick to pull off \u2014 it produces hydrazine (rocket fuel) as an intermediate, which the bacteria have to tuck away in internal sacs made of lipids. If bacteria have found a way to take advantage of nitrates' propensity to oxidize ammonia, despite having to cope with a toxic by-product, is there anything they cannot do? Subsequent discoveries \u2014 of bacteria that \u2018breathe\u2019 metal oxides, or feed on bleach \u2014 have strengthened the case for bacterial omnipotence. Last month, one of the last gaps in the bacterial skill set was closed when some of them (in concert with helpful members of another class of single-celled organism, the archaea) were discovered using nitrates to get energy from methane 3 . And yet there are still some energy-producing pathways down which no bacteria have been seen to stroll. No known bacteria seem to live by oxidizing manganese; none derive their energy from molecular nitrogen; and no photosynthetic bacterium is known that uses ammonia, phosphine (phosphorus hydride), or even methane, although the laws of thermodyamics show that all those reactions could provide energy. Are such organisms really absent, or have we just not found them yet? And if they are absent, does this reflect a fundamental limitation on life's in\u2013genuity? Or does it reveal set truths about the Earth's environmental chemistry? \u201cI can imagine that evolution's random walk has failed to locate some reactions,\u201d argues Franklin Harold, an emeritus professor of biochemistry and molecular biology at Colorado State University in Fort Collins. \u201cIf no bacteria live by molybdenum reduction, no profound principle is indicated.\u201d But there is some reason to believe that the missing tricks could explain why bacteria specialize so narrowly, forming rich metabolic networks and ecosystems only through collaboration. Indeed, bacteria's inabilities may be fundamental to the question of why there are ecosystems at all, rather than one all-conquering bug. \n               Working in pairs \n             \u201cIt's a dangerous game to put up your hand and say \u2018bacteria can't do this or that\u2019, as we probably only know about 1 to 5% of bacteria on the planet\u201d says microbiologist Ken Nealson, at the University of Southern California, Los Angeles. \u201cBut I'd wager that there are constraints on what's possible, beyond whether a reaction yields energy; some of these are still an incredible, wonderful mystery.\u201d The common factor in all the forms of respiration known is the involvement of what chemists call a redox pair \u2014 an electron donor and an electron receptor. The process by which electrons are stripped from the donor is called oxidation, because oxygen is the most familiar stripper of electrons (and also a very powerful one). The electrons are transferred to an acceptor, which is said to be reduced; hence the term \u2018redox\u2019. Non-chemical ways of generating energy are conceivable (see  \u2018The first electrochemist\u2019 ) but don't seem to be used. Life's dependence on redox reactions led the great geneticist Hermann Muller to describe it as \u201ca fancy form of rust\u201d. But it is rust with a purpose. As the electrons go from willing donor to willing acceptor, they go down a thermodynamic gradient; this means they lose energy. In respiring cells, the energy from this flow of electrons is used to build up a gradient of protons across membranes. When the protons flow down this artificially created gradient back across the membrane, ATP is created \u2014 a molecule that stores energy in a form that can be given up wherever it is needed to drive the cell's molecular machinery. (Humans may prefer to use these electrons for other purposes; see  page 277 .) In the case of aerobic respiration, used by most animals and many bacteria, the electron acceptor is oxygen, but it can be anything that has a hunger for electrons: carbon dioxide, nitrate, perchlorate (used in bleaches) or plain protons will do. All that matters is that the recipient's redox potential \u2014 its tendency to acquire electrons \u2014 is higher than that of the donor. The greater the difference in potentials, the more energy is released (see graphic, overleaf). In photosynthesis, the energy of the Sun is used to tear electrons from unwilling donors, driving redox reactions thermodynamically uphill. The absorption of energy by a chlorophyll molecule turns it into a fierce oxidant. Like a dentist pulling teeth it then wrenches electrons from a bystander that would normally be loath to surrender them. The electrons are ultimately transferred to carbon dioxide (the electron acceptor) to form sugars. Whether the electrons come from willing donors, as in respiration, or unwilling ones, as in photosynthesis, the bacteria need a way to get rid of the reduced molecules where the electrons accumulate once the reactions have run their course. In anaerobic systems, the reduced molecule in question will often be hydrogen. \u201cUnless the hydrogen is removed, the whole ecosystem gums up; that's why hydrogen consumers such as methanogens [archaea that produce methane] are so important,\u201d says Nealson. Other organisms in the ecosystem benefit: \u201cBy removing hydrogen, shifting the equilibrium, they make other processes thermodynamically possible, such as using protons as electron acceptors, which generates more hydrogen.\u201d \n               Current thinking \n             A build up of end products is not the only obstacle. The need for specially lined rocket-fuel tanks that the anammox reaction forces on bacteria highlights the fact that intermediates can also be a problem. So do recent findings about bacteria that oxidize perchlorate, notes Stuart Ferguson, a biochemist of the University of Oxford, UK. So far, six bacterial species are known to use perchlorate as an electron acceptor 4 . At first, the key component was thought to be an enzyme shared by these species that normally passes electrons to nitrate or nitrite, and which serendipitously happens to be able to deal with perchlorate too (the receptor molecules are not dissimilar in size and shape). But more recent work has shown that the crucial common denominator is in fact an enzyme that can detoxify the most toxic intermediate, chlorite 5 . Bacteria with the enzyme chlorite dis\u2013mutase can cope; those lacking it could not. Being able to cope with toxicity, not the ability to carry out the appropriate redox chemistry, is the bottleneck. Is this a general truth? \u201cToxicity can be a big challenge for microbes,\u201d says geochemist Donald Canfield of the University of Southern Denmark. Canfield thinks that there may be molecules that are too toxic for bacteria to deal with. \u201cI don't know of any microbes that can oxidize phosphine, for example. Perhaps it's just too toxic \u2014 it ignites spontaneously in air.\u201d This is despite there being environments such as swamps and rice paddies where phosphine is plentiful. \u201cFrom a thermodynamic point of view it would make good sense,\u201d argues Canfield. In oxygen-free settings, phosphine's spontaneously combustible nature might be less of a problem. If there are phosphine-eaters, the most likely place for them to turn up would be in anaerobic sediments such as those of Lake Taihu in China \u2014 a site of ongoing research into the phosphate cycle. Another factor that might stump bacteria is kinetics \u2014 the speed at which a reaction takes place naturally. For example, a potential source of energy in many marine sediments is the reaction of iron oxides such as rust with hydrogen sulphide that can bubble up from below. \u201cBut this reaction happens so fast naturally that, apparently, no bacteria can tap into it for energy,\u201d says Canfield. \u201cThey would have to come up with an enzyme that could actually slow it down.\u201d On the other hand, some thermodynamically favourable reactions are hard to get started, and go slowly. Enzymes are good at speeding up such reactions through catalysis, but the degradation of certain compounds needs a huge pulse of energy to get going. For such compounds, with a very high activation energy, the reaction is too slow to be profitable for microbes despite their enzymatic cleverness. \u201cA good rule of thumb is to look for what accumulates\u201d, says biochemist Rolf Thauer, at the Max Planck Institute for Terrestrial Microbiology in Marburg, Germany. Lignin is a good example. When lignin, a key component of hard wood, first evolved around 375 million years ago, nothing could metabolize it; the great reserves of coal in the Carboniferous period attest to that failure. Even today, it remains a challenge, and one that is met for the most part by aerobic fungi, says Thauer. \u201cThe molecule is too large for most bacteria to handle, and its activation energy is too high.\u201d Conversely, a lack of build up can be evidence of microbial activity. Geochemists knew that methane was \u2018missing\u2019 from ocean sediments long before the anaerobic bacteria and archaea (another group of microbes) that oxidize it were discovered. It should have been accumulating, but wasn't, so something had to be oxidizing it. \u201cMicrobiologists said \u2018that's not possible,\u201d recalls Thauer, \u201cbut clearly it was going somewhere.\u201d Benzene is a similar case. Its disappearance from anaerobic environments means something must be consuming it. But the organisms and metabolic pathways involved have yet to be elucidated. \n               Accumulating evidence \n             Perhaps the most interesting accumulation is of the molecule currently filling 80% of your lungs. Atmospheric nitrogen is mostly made by bacteria that reduce nitrates. Reducing the nitrogen further, to make ammonia, could in principle be a way of getting energy out of various organic fuels; but this doesn't seem practical. \u201cBacteria do reduce nitrogen,\u201d says Friedrich Widdel, at the Max Planck Institute for Marine Microbiology in Bremen, Germany. \u201cBut none can glean energy from this process: the activation energy needed to break the triple bond of nitrogen is too high.\u201d Widdel points out that the opposite process, using nitrogen as an electron donor and thus forming nitrate, is also possible. He thinks that ecological constraints may be blocking the use of this reaction by bacteria. \u201cPerhaps the problem is the high energy \u2018hills\u2019 on the way to nitrate,\u201d he speculates. \u201cBut photo\u2013synthetic organisms should have unlimited energy: they \u2018ought\u2019 to be able to use nitrogen as an electron donor.\u201d Robert Blankenship, a biochemist at Arizona State University in Tempe, agrees that there are many potential forms of photosynthesis that bacteria don't seem to use, and also suggests an ecological explanation \u2014 scarce resources. \u201cBecause sulphur compounds are used as electron donors for photosynthesis, you might predict that phosphorus and nitrogen compounds would be used too, but to the best of our knowledge they're not,\u201d he says. \u201cI suspect they're too important for other purposes \u2014 they're in short supply, and limit growth because they're needed for proteins and nucleotides.\u201d \n               Creatures of habit \n             If there were no other electron donor available, this constraint might not be so strong. But there almost always is. Cyanobacteria, algae and plants benefit from a type of photosynthesis that uses water as its electron source; this process can take place anywhere where there is sunshine, water and carbon dioxide (the electron acceptor). What's more, with this kind of oxygen-generating photosynthesis, the reduced compounds that might otherwise be used as electron donors in photosynthesis are mopped up: the compounds react chemically with oxygen in the surroundings, or properly equipped bacteria burn them up in respiration. Few reduced compounds survive long where light is plentiful, because of the oxygenating effects of photosynthesis. This may well be why ammonia-based photosynthesis, another possible form of metabolism that Broda discussed in the 1977 paper 1 , has never been seen. These constraints may hamper the iron-oxidizing photosynthetic bacteria first discovered by Widdel and his colleagues in the early 1990s 6 . The purple bacteria use soluble ferrous iron as an electron donor for photosynthesis, precipitating insoluble ferric iron as rust. But they seem to be restricted to sediments rather than open waters. Part of the problem is that ferrous iron is rarely found high in the water column, where it is quickly oxidized; so iron-oxidizing bacteria thrive only in the sediments of shallow waters where mud low in oxygen is exposed to sunlight. The things bacteria don't do mostly seem to be things the environment doesn't easily let them do. But although, as a group, bacteria can make use of more or less every reaction within thermodynamic and ecological limits, any given bacterium can probably only manage a few of them. Given the apparent ease with which bacteria and archaea swap genes, this might seem surprising. Why don't photosynthetic sulphur bacteria, for example, ever reduce sulphate in the dark? Why are different bacteria needed to oxidize ammonia to nitrate and then reduce the nitrate back to nitrogen. In short, why is there not one super-bacterium that does everything? \u201cThat's a vexed question!\u201d says Blankenship. \u201cI'd say it was something to do with redox reactions running one way under some conditions, and another way if you change the conditions. So to make a certain reaction work, a bacterium has to occupy a particular niche.\u201d If a specific range of redox potentials are needed for some processes to work, but others are not compatible with that, then \u201cthe best solution is to cooperate with your neighbour\u201d, adds Blankenship. The ecosystem thus becomes a way of dealing with the build up of waste products, and getting as much energy as possible out of the environment. If Blankenship is right, the immense subtlety with which bacteria are able to get one thing done may be what stops them from getting everything done. If they weren't constrained by their own fine tuning of redox conditions, there wouldn't be any diverse microbial systems. And there's an obvious corollary. If you really want to understand how bacteria and archaea make their livings, you need to study communities, not individuals, whether they be tightly coupled partnerships such as those between archaea and bacteria that oxidize methane or the looser, more complex communities found wherever the redox potential of a sediment varies with depth. And that approach requires, among other things, a new culture of microbiology. As Nealson puts it: \u201cWhen I was a student, you were thrown out for working with mixed cultures, but today the interest is in how bacterial consortia operate. If you fish out two bacteria in a cling, they're doing something together, but we've hardly begun to look.\u201d \n                     Microbiology: Pipe dreams \n                   \n                     Palaeobiology: Sea change in sediments \n                   \n                     All at sea \n                   \n                     Microbiology web focus \n                   \n                     Nature Bio-oceanography Insight \n                   \n                     The anammox online resource \n                   \n                     Nick Lane's books \n                   \n                     Nasa astrobiology foxus on microbial mats \n                   Reprints and Permissions"},
{"file_id": "441271a", "url": "https://www.nature.com/articles/441271a", "year": 2006, "authors": [{"name": "Apoorva Mandavilli"}], "parsed_as_year": "2006_or_before", "body": "Prevailing wisdom says the adult brain cannot learn to see if it had no visual stimulation during childhood, but blind people in India seem to be breaking all the rules. Apoorva Mandavilli reports. Doctors gave SK his first pair of glasses in July 2004. He had been too poor to afford a pair before \u2014 but then he was a 29-year-old blind man, what use were glasses to him? Had he been given glasses as a child they might have helped him overcome his congenital aphakia \u2014 an extremely rare condition in which the eyeball develops without a lens. Yet his chances of being diagnosed, let alone treated, in the poor Indian village in which he was born were slim. As a result, SK was living in a \u2018hostel for the blind\u2019 with no running water when the doctors arrived from New Delhi. SK's doctors weren't sure how much sight he would gain, or if he would comprehend what he saw. For the first year, he had only the most basic visual skills. He could recognize simple two-dimensional objects but anything three-dimensional, even an everyday object such as a ball, was beyond him. All this was consistent with the idea of a \u2018critical period\u2019 in vision: that if you haven't learned to see by a certain age, you never will. But 18 months after getting his glasses, SK surprised everyone. He had begun to make sense of his world, building his visual vocabulary through experience and recognizing more complex objects with varying colours and brightness. In doing so, he turned one of the most fundamental concepts in neuroscience on its head. \u201cTwenty-nine years without any normal vision? I would have said that's a life sentence,\u201d says Ron Kalil, a visual neuroscientist at the University of Wisconsin in Madison. For Kalil and other experts, the impossible now seems possible. And while the scientists might be amazed by the brain's adaptability, the real winners are the countless blind people \u2014 both children and adults \u2014 who had been considered untreatable. \n               Light work \n             SK is the first success of Project Prakash (Sanskrit for \u2018light\u2019) launched in 2003 and run by Pawan Sinha, a neuroscientist at the Massachusetts Institute of Technology. Originally a humanitarian effort to help blind children in India, under Sinha's guidance Project Prakash has blossomed into a chance to investigate how we learn to see. The idea of a critical period for vision grew out of research in animals. In the 1970s, Tortsen Wiesel and David Hubel at Harvard Medical School famously shut one eye of a week-old kitten. They found that closing the eye for only a few weeks caused the kitten's visual cortex, the part of the brain that deals with sight, to develop abnormally. But similar experiments had little effect on adult cats. After further work in monkeys, Wiesel and Hubel suggested that there is a critical period during the first few months of life when normal vision must develop. In 1981, they shared the Nobel Prize in Physiology or Medicine for this and other work. The notion of a fixed critical period has been crumbling steadily for the past few decades, and the brain is now seen as much more flexible \u2014 able to grow new nerve cells and to adapt long after childhood. For instance, Uri Polat and his colleagues at Tel Aviv University in Israel showed in 2004 that adults with a lazy eye can be trained to improve their vision 1  \u2014 contrary to conventional expectations. In the strictest sense, SK's case does not contradict Wiesel and Hubel's finding. Without glasses, his visual acuity was 20/900, far worse than the standard 20/20 for normal vision or even the World Health Organization's definition of legal blindness at 20/400. Like others with aphakia, SK could sense light and movement, which arguably allowed his visual cortex to develop normally. With his glasses, SK's acuity jumped to 20/120, although he still saw objects as separate regions of colour and brightness, and struggled to put them together as a whole. He saw a cow, for example, as patches of black and white, each a separate object, until the cow moved. As soon as there was movement, SK was able to recognize that the set of objects made up a cow. Over the next 18 months, his acuity remained stuck at 20/120, but he was able to stop relying on motion to integrate objects and learned to recognize them even when they were still. \n               Critical point \n             Under conventional dogma, SK shouldn't be able to do this, but where scientists went wrong, says Sinha, is in applying the notion of critical period too broadly. They assumed, incorrectly, that if the eye's vision doesn't improve, neither does the ability to interpret what you see. \u201cWhat is so exciting about Sinha's work is that he is showing that's not the case,\u201d says Lynne Kiorpes, a neuroscientist at New York University. \u201cPeople can learn to use the vision they have.\u201d As well as being Sinha's native home, India is a natural choice for studying blind children. One-third of the world's 45 million blind people live in India, according to the non-profit organization Orbis International. Caught early, half of the cases of childhood blindness could be treated or prevented \u2014 but they aren't. Some parents see blindness as punishment for sins in a previous life. Most can't afford the treatment. Born poor, and neglected in the broader struggle for survival, more than 60% of blind children in India die before reaching adulthood, according to Orbis. Those who survive tend to be shipped off to special schools or hostels, where they resign themselves to making candles or weaving baskets. \u201cThe first thing that prompted me was seeing these numbers, the humanitarian goal was just so evident,\u201d says Sinha. After touring villages in the summer of 2002, Sinha joined forces with Dr Shroff's Charitable Eye Hospital on the outskirts of New Delhi. By Western standards the hospital is unassuming, but it is among the best equipped in the country. With funding from international organizations, volunteers from Dr Shroff's hospital head into under-served communities and screen hundreds of children for sight problems. In many cases, the children are beyond help. But if the condition seems reversible, the hospital offers to treat them, often for free. Many suffer from cataracts and, with only minor interventions, could well have normal vision. With Project Prakash, volunteers from Dr Shroff's hospital have begun visiting schools for the blind for the first time. The project aims to treat 15 \u2018hopeless\u2019 cases from these schools each year, and to enrol them in further experiments. SK is the oldest by far: most are aged 7 to 11. Sinha plans to measure the children's visual abilities before and after treatment to learn how much vision is needed for recovery to be possible. \n               Alternative view \n             SK's case has already provided new ideas about how vision develops. Studies on individuals who recover sight late in life are rare, and researchers who have measured acuity found that, as with SK, it did not improve over time 2 . Those scientists did not ask, as Sinha did, whether other visual skills might eventually improve. \u201cIf they'd looked at it that way, then they probably would have found all sorts of things they weren't looking for,\u201d says Nigel Daw, a neurobiologist at Yale University. \u201cThis will lead people to do more experiments along those lines. It's ground-breaking work in that sense.\u201d Sinha's work suggests that different visual abilities might have different critical periods. For example, detection of motion is one of the first abilities to develop, perhaps even hard-wired at birth; understanding of colour and full stereo vision develop later, and visual integration \u2014 the process that allows SK to resolve separate objects into a cow \u2014 might take even longer. \u201cThis project allows us to refine the very broad and vague idea of a critical period,\u201d Sinha says. And it challenges the clinical notion that treating a blind child beyond the age of around eight is hopeless. Most children with reversible causes of blindness can sense light, and so may be able to recover. In fact, says Sinha, the complete blindness created in animal experiments is rarely, if ever, found. Wiesel himself maintains that a critical period still exists, but that scientists should be more open-minded about how they interpret it for clinical use. \u201cThere is a critical period and we should try to help kids as soon as possible,\u201d Wiesel says. \u201cBut if for some reason that's not possible, and there's some vision left, certainly recovery is possible.\u201d SK's case has led Sinha to some interesting tangents. For example, children with autism also have trouble integrating different parts of an object into a whole \u2014 they literally can't see the forest for the trees. It turns out that these children also have trouble with motion perception 3 , 4 , which bolsters Sinha's hypothesis. He is now studying visual skills in autistic children. For Project Prakash, this is just the start. Soon after the project began visiting schools for the blind, Dr Shroff's hospital petitioned the Indian Supreme Court, demanding that at least those children who can be helped are given a chance. As a result, India's Supreme Court has ruled that before being admitted to a blind school, every child must be examined by an ophthalmologist. On the research front, Sinha plans to use imaging techniques \u2014 primarily functional magnetic resonance imaging \u2014 to study the brains of children and adults whose sight is restored later in life. He wants to see how much of the visual cortex in these people can be stimulated by light. Sinha also hopes to discover how much of the cortex has been taken over by other functions such as hearing or touch, and whether this change is reversible. \u201cI think that is a very, very fundamental question,\u201d he says. Presumably, the children might also recover different skills to varying degrees. Such information might help doctors design rehabilitation programmes and may offer clues to which visual abilities come prewired at birth and which develop over time. \u201cSlowly but surely, the evidence is coming forward to indicate that the brain is much more adaptable than we suspected,\u201d says Kalil. \u201cWork such as Pawan's should tell us to go looking for more.\u201d \n                     Motion perception is learned, not innate \n                   \n                     What blindness can tell us about seeing again: merging neuroplasticity and neuroprostheses \n                   \n                     Project Prakash \n                   \n                     Childhood blindness \n                   \n                     Dr Shroff's Charity Eye Hospital \n                   Reprints and Permissions"},
{"file_id": "441020a", "url": "https://www.nature.com/articles/441020a", "year": 2006, "authors": [{"name": "Jacqueline Ruttimann"}], "parsed_as_year": "2006_or_before", "body": "Does the respected US National Institutes of Health meet the needs of young postdoc researchers? Jacqueline Ruttimann investigates. Place yourself, for a moment, in the lab coat of a US postdoctoral fellow. You've just spent four to seven years working on your graduate-school dissertation, but the journey is not yet complete. At a time when many of your friends are making good money at real jobs, you have to work in another laboratory for almost the same time it took to complete grad school. Fail to receive funding and publish frequently, and your dream of one day heading your own lab will go up in smoke. The odds are stacked against you \u2014 and the roughly 45,000 other postdocs in the country. The most recent data from the National Science Foundation show that, of those who were postdocs in 2001, less than one-quarter had moved into a tenure-track university position two years later. Nevertheless, more than 20,000 postdocs work each year under the auspices of the US National Institutes of Health (NIH), the largest funding source for postdoc research in the country. Just over 10% of these young scientists work on \u2018intramural\u2019 research at the agency's main campus in Bethesda, Maryland. The rest rely on NIH funding for extramural research in labs across the country. In many ways, the NIH is an ideal place to be a postdoc. Although its budget is being cut back now, the agency had been relatively flush with cash in recent years, and its director Elias Zerhouni has made young investigators a priority during his tenure (see  page 17 ). Yet even at the NIH, postdocs struggle to acquire the skills needed to develop their own line of research and score a coveted tenure-track position.  Nature  spoke with four postdocs who are at key points along the journey to becoming a fledgling scientist. \n               Academia to industry \n             Sometimes the hardest thing about doing a postdoc is making the decision to become one. According to a survey by the research society Sigma Xi, the typical US postdoc makes an average salary of just $38,000. Financial concerns can often draw young scientists off into industry or other more lucrative careers. \u201cThose of us who decide to stick with academic science have to make a financial sacrifice that we shouldn't be making,\u201d says Tshaka Cunningham, who started a postdoc four months ago at the National Cancer Institute's vaccine branch on a project to develop an AIDS vaccine. Even so, Cunningham is infused with the energy of a young scientist. He has lots of ideas \u2014 so many that he keeps a notebook by his bed, thinking that he might one day use some of them at a university tenure-track position or as a consultant in the biotechnology industry. For now, he hopes that the NIH can help to steer him down either path. It's a big request for an agency reeling from a conflict-of-interest scandal, in which NIH investigators were chastized for regularly accepting large consulting fees from pharmaceutical companies and others whose products they evaluated. Cunningham is experiencing some of the backlash from this scandal, as everything down to his involvement with a non-profit organization he helped to found before his arrival, the National Association for Blacks in Bio, is being carefully scrutinized under new, more stringent NIH ethics rules. Still, he'd like to see the NIH adopt a more seamless relationship with industry so that postdocs could apply for industry grants while finishing their mentor's research project. As a graduate student at the Rockefeller Institute in New York, Cunningham combined his main project with a scheme funded by a $40,000 grant, part of which came from Merck, the New Jersey-based pharmaceutical giant. \u201cThe NIH should be an intermediary between academia and industry,\u201d he says. That's unlikely to happen any time soon, but the NIH is focusing on smoothing relationships between academic disciplines. The agency is moving towards integrating training programmes for different disciplines, so that postdocs can more readily move between different laboratories at the various institutes, says Michael Gottesman, the NIH's deputy director for intramural research. For now, that means having bench-bound postdocs attend a weekly course called Demystifying Medicine, among others, in which they learn more about the medical research that might spring from their lab findings. \n               Can postdocs manage? \n             Career training is what many postdocs crave most, according to the recent Sigma Xi survey. Many feel they have plenty of experience and training in the laboratory, but want more structured approaches to bigger career questions. Stephen Sperber, a second-year postdoc in a molecular biology lab at the National Institute of Child Health and Human Development, says that although he enjoys his work, the future is a bit worrisome. \u201cAs comfortable as I am now, this is not a job,\u201d he says. \u201cThat's the problem for postdocs: a lot of uncertainty. I don't know where I'm going to be in two years.\u201d Like most, he would like to run his own laboratory one day. But that will take some luck. Regardless of where they train, the average age at which a postdoc gets awarded his or her first independent research grant has been creeping up over time \u2014 from age 37 in 1980, to 42 in 2002. Only 6% of the much-desired \u2018R01\u2019 research grants go to new investigators. \u201cThat transition from postdoc to primary investigator is a very difficult one,\u201d admits Gottesman. One way the NIH is trying to help is through the new Pathway to Independence award, in which researchers in the middle of their postdocs can apply for a bridge award of up to $1 million \u2014 to fund the rest of their postdoc years and the beginning of their independent research. The award, says Gottesman, is \u201ca morale builder\u201d. Postdocs generally agree, but say launching a career in science takes more than just money. Many young postdocs often complain that they need management skills, training which usually takes a lower priority than getting a particular experiment done or a research paper written up. For them, the NIH has set up a course called Making the Right Moves: A Practical Guide to Scientific Management for Postdocs and New Faculty, says Norka Ruiz Bravo, deputy director for extramural research. The problem is, many postdocs are often not aware of such training opportunities, because they are too isolated in their particular institute or simply have not been able to find the information they need. \n               Alternative paths \n             There are also those who decide that bench life is not for them. Bernard McWatters has just finished a postdoc on a project linking polio vaccines to cancer. An affable young man with twinkling eyes and a mouth usually curled up in a smile, McWatters was extremely focused when it came to looking for a job. He knew he wanted to work at the US Food and Drug Administration (FDA) and achieved it by doing a hybrid postdoc between the FDA and the NIH, in which he spent half of his time at the bench and the other half doing regulatory work. He will soon start a regulatory-affairs job at the FDA's Centers for Biological Evaluation and Research. \u201cThere are all sorts of opportunities for the wayward scientist looking for a job,\u201d he says. McWatters ought to know. He has served as chairman of a career development subcommittee of a larger group set up to improve communication between postdocs and the NIH community. McWatters helped to sponsor monthly seminars on alternative careers, including such fields as patent law, science policy, medical communication and science consulting. He advises postdocs not to focus on just one career path, and not to wait until the end of their postdoc to start looking for jobs. The job hunt brings other complications for international postdocs. More than two-thirds of the NIH's intramural postdocs arrive from other countries, often on a visa that ties them to a particular mentor's lab. Alessandro Vichi, who came from Italy to work as a postdoc for five years at the NIH, says that the academic pressures are similar for US and international postdocs. But those visiting from abroad have to worry about visa issues, particularly since regulations were tightened after the terrorist attacks of 11 September 2001. \u201cYou're waiting without making a plan,\u201d says Vichi, who is now a research fellow at the National Heart, Lung, and Blood Institute. \u201cYou're not sure whether you want to go back home and you're not sure whether you want to stay.\u201d Regardless of whether foreigners decide to stay or leave the country after their postdoc, Gottesman argues that all receive sufficient training to get jobs. Those who stay, he says, are often successful in finding work \u2014 even if they are in the biotech industry or government rather than an elusive tenure-track position. \u201cUnemployment among people with postdoc experience is very low,\u201d he says. But even NIH officials admit that there is little evidence to support this reported success. The NIH has no formalized tracking system to follow where postdocs end up, which makes it difficult to monitor the success of their training. Bravo says that a tracking system is in the works, but is proving difficult to design as each of the agency's 27 separate institutes and centres have postdocs supported on different types of fellowships. Despite all the trials and tribulations that come with being a postdoc, it is often considered one of the most productive periods of a scientist's career. \u201cIt's a remarkable time of your life when you're unencumbered with the responsibilities that go with becoming an independent investigator,\u201d says Bravo, who fondly remembers her days as a postdoc. \u201cIt's a time when you focus on science.\u201d Reprints and Permissions"},
{"file_id": "441277a", "url": "https://www.nature.com/articles/441277a", "year": 2006, "authors": [{"name": "Charlotte Schubert"}], "parsed_as_year": "2006_or_before", "body": "The left-hand side of Peter Girguis's lab is a mucky place. Dried mud flecks benches, beakers and scales, and buckets of slime clutter the floor. Fresh ooze is flown in regularly from California. A skein of pipes fed by a 2,000-gallon tank of sea water in the basement keeps everything moist. On the floor, wires gush out of dismembered plastic cocoons, and electrodes poke out of various tanks. The messy workplace is a design shop for electrical generators. This summer, microbiologist Girguis and his colleagues at Harvard University in Cambridge, Massachusetts, will shove graphite electrodes, like those used in this lab, into the sea floor of Monterey Bay in California. The current that flows between the electrodes will power a suite of scientific instruments, including a wave and tide meter, and a fluorometer that measures levels of chlorophyll in sea water. Girguis's work depends on the fact that when bacteria respire they pull electrons off organic debris. Catch those electrons on an electrode that is hooked up to a second electrode in free water, say, or in air, or in another layer of sediment where the microbes use a different sort of electrochemistry (see  page 274 ) \u2014 and the electrons will flow from the first to the second electrode. That gives you a current with which you can power things. The currents produced by these microbial fuel cells are small, but their potential, in the non-voltage sense, may be surprisingly large. After all, microbes can produce electrons from sediment, sewage, food scraps or pig slop. Girguis, a self-confessed gearhead, takes some delight in a simple demonstration of their current-generating abilities using just a bucket, some hardware-store supplies and a bag of cow manure. People have been trying to harness the power of microbes in this way since the early 1900s, although with little practical success. Engineering advances and molecular biology's new tools, some of which have a home in the clean half of Giguis's lab, have allowed him and like-minded researchers to breathe life into the field. \u201cThis technology would transform the world if it ever became reliable and could be used on a large scale,\u201d says Bruce Rittman, director of the Center for Environmental Biotechnology at Arizona State University's Biodesign Institute in Tempe. But he admits that is still a dream. \u201cWe are quite some distance from capturing the benefits at an economic scale.\u201d To get there, he estimates that researchers will need to produce fuel cells with power-generating abilities that are a hundred times greater than those of today's cells. \u201cAll of microbial fuel-cell research invariably has to deal with the interface between microbes and the electrodes,\u201d says Girguis. One approach, favoured by engineers, is to try to improve the cell's design and the materials from which the electrodes are made. Biologists, however, aspire to better understand the microbes themselves, with the aim of informing more sophisticated designs \u2014 or designing better bacteria. \n               Natural lighting \n             Fuel-cell builders have been adding electron mediators \u2014 small molecules that help electrons move around \u2014 to their systems for years to increase the current. But another way to enhance performance may be to increase the physical contact between the bacteria and the electrodes. Last year, Derek Lovley at the University of Massachusetts, Amherst, revealed that members of a group of bacteria called  Geobacter  extrude protein spines, or pili, which conduct electricity 2 . And they are not the only ones, according to findings presented this February at a conference sponsored by the US Department of Energy, and held in Bethesda, Maryland. Yuri Gorby from the Pacific Northwest National Laboratory in Richland, Washington, reported that nanowires sprout from a range of bacteria, including the iron-breathing microbes  Shewanella  and the photosynthetic bacteria  Synechocystis . Using  Synechocystis , he and his colleagues have made a solar-powered form of microbial fuel cell. The key to the nanowires' conductivity, Gorby and his colleagues found, was the presence of certain cytochromes \u2014 electron-shuttling molecules found in bacterial membranes. Bacteria lacking the nanowires or cytochromes are poor conductors of electricity. Researchers are already thinking about how to harness the findings. Gorby's colleague, microbiologist Ken Nealson at the University of Southern California in Los Angeles, points out that  Geobacter  and  Shewanella  have evolved to transfer electrons to scraps of metal found in nature, not to an artificial electrode. Encouraging them to evolve electrode compatibility might yield a great deal of improvement. Nealson and Lovley say that \u2018electrode-adapted\u2019 bacteria could one day produce fuel cells with at least 100-fold greater power-generating abilities compared with current cells. Such \u2018improved\u2019 bacteria might be too specialized and delicate to thrive in marine sediments. But they could be put to use in purpose-built closed bioreactor systems, where they could generate electricity by chewing up biomass. \u201cIt's the best approach we have to developing an industrial application or producing a lot of power,\u201d says Girguis. While the biologists extol the potential of nanowires, Bruce Logan, an environmental engineer at the Pennsylvania State University in University Park, prefers to concentrate on the chemistry and physics of the fuel cells. Logan says he hasn't seen much practical advance from understanding the biology of the microbes. \u201cIt's not clear whether we are limited by the bacteria or the physics of the system,\u201d he says. And although he is not dismissive of the microbiologists \u2014 indeed he has just started collaborating with Nealson \u2014 he argues that \u201cmost of our improvements have been geared towards the physics and chemistry, suggesting that that is the major roadblock.\u201d \n               Energy on the cheap \n             Logan works with industrial runoff, animal waste and sewage. At least 1.5% of US electricity production goes into wastewater treatment, he says. He wants to change that equation, by developing systems that produce electricity as well as cleaning the water. The field of wastewater treatment by fuel cells was opened up by Byung-Hong Kim, a microbiologist from the Korea Institute of Science and Technology in Seoul; Kim developed a system that didn't need added electron mediators in the 1990s 3 . Since then engineers, such as Logan, have worked on various ways of improving the design and the power output of these cells. Logan's most recent table-top device yields the equivalent of 15.5 watts per cubic metre of household waste water flowing through it 4 . By the standards of a conventional fuel cell, such as one powered by hydrogen, that's a very poor yield. But with scale-up, Logan estimates that the waste water from 100,000 people, or a large industrial plant, could produce 0.8 megawatts, enough to power about 500 homes. Such a system would also scrub waste water of troublesome compounds such as ammonia, which is consumed by the microbes, and produce 50\u201390% less sludge than conventional methods. Unfortunately, the electron-gathering anodes in Logan's laboratory systems are made from moderately expensive graphite or carbon paper, and the cathodes need to be coated with very costly platinum for the best effects. Willy Verstraete, an environmental engineer at Ghent University in Belgium, says that, extrapolating linearly from the costs of the prototypes, the capital expenditure for a full-scale plant would be five to ten times higher than it is for today's treatment plants. So the hunt is on for cheaper materials. Logan points out that the technology does not have to be cheap, just cheaper than the current techniques. The United States already spends about $25 billion each year on domestic wastewater treatment, he says. He is working on a design of fuel cell that he says could be cost effective and scalable; he hopes this will yield ten times as much power per cubic metre of waste water as current designs do. \n               Power to the people \n             Verstraete is more cautious. \u201cIndustry is expressing a lot of interest,\u201d he says, \u201cbut they are not putting the money on the table.\u201d He says that companies want to see yields as high as 1,000 watts per cubic metre \u2014 and to see them in prototypes a thousand times larger than the milk-carton-sized devices now honing the technology. He sees a role for the technology in cleaning certain nitrogenous pollutants out of waste water, or chewing up effluents from the digesters that convert biomass to methane on some farms. But it's still a long road to the first microbial car (see  \u2018And what if it worked?\u2019 ). Meanwhile, the powering of marine instruments seems to be the application moving the fastest. Girguis's work on Monterey Bay sediments follows the lead of pioneers such as Lenny Tender at the US Naval Research Laboratory in Washington DC. Tender was one of the team that first successfully extracted current from marine sediment more than five years ago 5 . This summer, Tender plans to launch a microbial fuel cell that powers an acoustic device for measuring water velocity. Taking measurements every 4 hours it will consume an average of just 0.1 watts. Environmental monitoring on the land and the ocean surface is increasingly powered by solar cells that can work for years unsupervised. The ocean floors, being abysmally short of sunlight, need a different approach, and Tender, Girguis and their colleagues think that microbial-powered electricity is the solution. This would obviate the need for battery replacement that currently drives up the costs of such studies. \u201cThey have a lot more work to do before they can call this a real proven technology,\u201d says James Bellingham, chief technologist at the Monterey Bay Aquarium Research Institute, who is working with Girguis. \u201cBut these guys have come very far very fast.\u201d Which other niche areas might be successful is less clear. For his terrestrial applications, Girguis is collaborating with an architect who specializes in new materials. Together, they plan to design LED lighting systems that can be powered by a bucket of food scraps or animal waste. He says such devices could be mass-produced for about ten dollars each and could help generate light for many of the 1.6 billion people living outside the world's electrical grid. Although such people are obvious candidates for solar power, Giguis says the microbial option has the advantage of being much less expensive to manufacture and easier to operate and repair. And it can work in the dark. The technology need not always be cheap and cheerful. Rittman has a grant of $100,000 from NASA to design a compact microbial fuel cell that consumes human waste during a manned astronaut mission. But like many researchers in the area, he dreams of bigger opportunities on Earth. If microbial fuel cells could be adapted for the consumption of plant biomass, they could in theory produce twice the amount of electrical energy that combustion-driven generators can, says Rittman. Again, however, the best that has been done is not quite that good, and is stuck on the lab bench for now. But Nealson, for one, is confident things can improve, especially if links between different disciplines housed in different departments can be strengthened. \u201cOur job here is to understand how the bacteria work, and once we know that we can get together with the engineers. Then you might find out how to make the ideal fuel cell.\u201d He laughs, \u201cmaybe it will drive itself across campus.\u201d \n                     Sucking energy out of the drain \n                   \n                     Bugs boost Cold War clean-up \n                   \n                     Microbe fuel cell packs more power \n                   \n                     Microbiology web focus \n                   \n                     Nature insight: materials for clean energy \n                   \n                     Microbial fuel cells site \n                   \n                     The Shewanella Federation \n                   \n                     Bruce Logan's page \n                   \n                     The Geobacter project \n                   Reprints and Permissions"},
{"file_id": "441398a", "url": "https://www.nature.com/articles/441398a", "year": 2006, "authors": [{"name": "Helen Pearson"}], "parsed_as_year": "2006_or_before", "body": "The idea of genes as beads on a DNA string is fast fading. Protein-coding sequences have no clear beginning or end and RNA is a key part of the information package, reports Helen Pearson. \u2018Gene\u2019 is not a typical four-letter word. It is not offensive. It is never bleeped out of TV shows. And where the meaning of most four-letter words is all too clear, that of gene is not. The more expert scientists become in molecular genetics, the less easy it is to be sure about what, if anything, a gene actually is. Rick Young, a geneticist at the Whitehead Institute in Cambridge, Massachusetts, says that when he first started teaching as a young professor two decades ago, it took him about two hours to teach fresh-faced undergraduates what a gene was and the nuts and bolts of how it worked. Today, he and his colleagues need three months of lectures to convey the concept of the gene, and that's not because the students are any less bright. \u201cIt takes a whole semester to teach this stuff to talented graduates,\u201d Young says. \u201cIt used to be we could give a one-off definition and now it's much more complicated.\u201d In classical genetics, a gene was an abstract concept \u2014 a unit of inheritance that ferried a characteristic from parent to child. As biochemistry came into its own, those characteristics were associated with enzymes or proteins, one for each gene. And with the advent of molecular biology, genes became real, physical things \u2014 sequences of DNA which when converted into strands of so-called messenger RNA could be used as the basis for building their associated protein piece by piece. The great coiled DNA molecules of the chromosomes were seen as long strings on which gene sequences sat like discrete beads. This picture is still the working model for many scientists. But those at the forefront of genetic research see it as increasingly old-fashioned \u2014 a crude approximation that, at best, hides fascinating new complexities and, at worst, blinds its users to useful new paths of enquiry. Information, it seems, is parceled out along chromosomes in a much more complex way than was originally supposed. RNA molecules are not just passive conduits through which the gene's message flows into the world but active regulators of cellular processes. In some cases, RNA may even pass information across generations \u2014 normally the sole preserve of DNA. An eye-opening study last year raised the possibility that plants sometimes rewrite their DNA on the basis of RNA messages inherited from generations past 1 . A study on  page 469  of this issue suggests that a comparable phenomenon might occur in mice, and by implication in other mammals 2 . If this type of phenomenon is indeed widespread, it \u201cwould have huge implications,\u201d says evolutionary geneticist Laurence Hurst at the University of Bath, UK. \u201cAll of that information seriously challenges our conventional definition of a gene,\u201d says molecular biologist Bing Ren at the University of California, San Diego. And the information challenge is about to get even tougher. Later this year, a glut of data will be released from the international Encyclopedia of DNA Elements (ENCODE) project. The pilot phase of ENCODE involves scrutinizing roughly 1% of the human genome in unprecedented detail; the aim is to find all the sequences that serve a useful purpose and explain what that purpose is. \u201cWhen we started the ENCODE project I had a different view of what a gene was,\u201d says contributing researcher Roderic Guigo at the Center for Genomic Regulation in Barcelona. \u201cThe degree of complexity we've seen was not anticipated.\u201d \n               Under fire \n             The first of the complexities to challenge molecular biology's paradigm of a single DNA sequence encoding a single protein was alternative splicing, discovered in viruses in 1977 (see  \u2018Hard to track\u2019 ). Most of the DNA sequences describing proteins in humans have a modular arrangement in which exons, which carry the instructions for making proteins, are interspersed with non-coding introns. In alternative splicing, the cell snips out introns and sews together the exons in various different orders, creating messages that can code for different proteins. Over the years geneticists have also documented overlapping genes, genes within genes and countless other weird arrangements (see  \u2018Muddling over genes\u2019 ). Alternative splicing, however, did not in itself require a drastic reappraisal of the notion of a gene; it just showed that some DNA sequences could describe more than one protein. Today's assault on the gene concept is more far reaching, fuelled largely by studies that show the previously unimagined scope of RNA. The one gene, one protein idea is coming under particular assault from researchers who are comprehensively extracting and analysing the RNA messages, or transcripts, manufactured by genomes, including the human and mouse genome. Researchers led by Thomas Gingeras at the company Affymetrix in Santa Clara, California, for example, recently studied all the transcripts from ten chromosomes across eight human cell lines and worked out precisely where on the chromosomes each of the transcripts came from 3 . The picture these studies paint is one of mind-boggling complexity. Instead of discrete genes dutifully mass-producing identical RNA transcripts, a teeming mass of transcription converts many segments of the genome into multiple RNA ribbons of differing lengths. These ribbons can be generated from both strands of DNA, rather than from just one as was conventionally thought. Some of these transcripts come from regions of DNA previously identified as holding protein-coding genes. But many do not. \u201cIt's somewhat revolutionary,\u201d says Gingeras's colleague Phillip Kapranov. \u201cWe've come to the realization that the genome is full of overlapping transcripts.\u201d Other studies, one by Guigo's team 4 , and one by geneticist Rotem Sorek 5 , now at Tel Aviv University, Israel, and his colleagues, have hinted at the reasons behind the mass of transcription. The two teams investigated occasional reports that transcription can start at a DNA sequence associated with one protein and run straight through into the gene for a completely different protein, producing a fused transcript. By delving into databases of human RNA transcripts, Guigo's team estimate that 4\u20135% of the DNA in regions conventionally recognized as genes is transcribed in this way. Producing fused transcripts could be one way for a cell to generate a greater variety of proteins from a limited number of exons, the researchers say. Many scientists are now starting to think that the descriptions of proteins encoded in DNA know no borders \u2014 that each sequence reaches into the next and beyond. This idea will be one of the central points to emerge from the ENCODE project when its results are published later this year. Kapranov and others say that they have documented many examples of transcripts in which protein-coding exons from one part of the genome combine with exons from another part that can be hundreds of thousands of bases away, with several other \u2018genes\u2019 in between. This continuum of genes might even spill over the boundaries of chromosomes: last year, Richard Flavell at Yale University School of Medicine in New Haven, Connecticut, documented human immune-system genes that seem to be controlled by regulatory regions from another chromosome 6 . \u201cDiscrete genes are starting to vanish,\u201d Guigo says. \u201cWe have a continuum of transcripts.\u201d \n               Slippery concept \n             The large transcriptional surveys suggest that a vast amount of the RNA manufactured by the mouse and human genomes do not code for proteins. Last year a consortium of researchers in Japan, for example, estimated that a whopping 63% of the mouse genome is transcribed 7 , 8 ; only 1\u20132% of the genome is thought to be spanned by sequences that contain everyday exons. The discovery of RNA sequences that aren't just intermediates between the DNA and the protein-making machinery is not new in itself; the cell's protein-building apparatus requires a number of RNA molecules as well as proteins to operate. But the finding of \u2018microRNAs\u2019 and other RNA molecules now known to be vital in controlling many cellular processes in plants and animals, and the newly revealed ferment of RNA transcription, contributes to the view that RNA actively processes and carries out the instructions in the genome. Perhaps the regions that make non-coding RNA should also carry the status of genes, if not the name itself. \u201cI think it's time for people to take a deep breath and step back,\u201d says molecular biologist John Mattick of the University of Queensland in Brisbane, Australia. \u201cA lot of the information in the system is being transacted by RNA.\u201d Although functions have been identified for several RNA molecules, the crux of the debate now is the extent to which all the extra RNA plays a part. It is conceivable that it is easier to overtranscribe and ignore the rubbish than to invest in systems that produce only what is needed. A study from last year, however, hints that at least some of the mass of RNAs is doing something useful. Working at the Genomics Institute of the Novartis Research Foundation in San Diego, California, John Hogenesch and his co-workers systematically quenched the activity of more than 500 non-coding RNAs in human cells and found that eight were involved in cell signalling and growth 9 . But Hogenesh, and many other scientists, remain convinced that non-coding RNAs are much less important, functionally, than those that describe proteins; in the past, when scientists have searched for the genetic basis of a disease or other characteristic they have overwhelmingly found the underlying mutation to be in a protein-coding gene rather than in another region. \u201cThe preponderance of evidence suggests that protein-coding genes will hold their own when the day is over,\u201d Hogenesh says. Some of the recent discoveries \u2014 that the human genome makes a continuum of transcripts and that cells produce masses of non-coding RNA molecules \u2014 have not posed much of a problem to people outside the world of molecular biology. Population geneticists can examine how a trait is passed down and evolves regardless of the precise molecular mechanism that underlies it. For example, geneticists can build models showing how a mutation is inherited whether it affects a protein, a non-coding RNA or a regulatory region. \u201cI don't actually care if it's making a protein or not,\u201d says Hurst. \u201cThe equations are still the same.\u201d But the same can't be said for studies revealing so-called extragenomic modes of inheritance. In recent years, many investigators have focused on epigenetic inheritance, in which information is passed from parent to offspring independent of the DNA sequence. And this week in  Nature  (see  page 469 ), Minoo Rassoulzadegan's team at the French National Institute for Health and Medical Research (INSERM) in Nice, France, reports that RNA may sometimes be complicating traditional models of inheritance. In mice, mutations in the  Kit  gene cause white patches on the tail and feet; if a mouse has one normal  Kit  gene and one mutated one it will have the spots. The odd thing is that some of the offspring of such mice, who inherit two normal  Kit  genes, still have the white tail. The French group suggest that the mutant  Kit  gene manufactures abnormal RNA molecules, which accumulate in sperm and pass into the egg. These bits of RNA somehow silence the normal  Kit  gene in the next generation and subsequent ones, producing the spotted-tail effect. \u201cWe are convinced that it's a more general phenomenon,\u201d says co-author Fran\u00e7ois Cuzin. If this is strange, the work reported last year 1  on the cress plant  Arabidopsis  by Robert Pruitt and his colleagues at Purdue University in West Lafayette, Indiana, is even stranger. Here the gene involved is called  HOTHEAD . Pruitt and his co-workers' analysis shows that some plants do not carry the mutant version of  HOTHEAD  that their parents possessed. These plants had replaced the abnormal DNA sequence with the regular code possessed by earlier generations. \u201cIt's like, whoa, this changes everything,\u201d Pruitt says. \u201cIt definitely changes my view of inheritance.\u201d Pruitt is now working to explain how the plant could perform such a feat. One idea is that they carry a back-up copy of their grandparents' genetic information encoded in RNA that is passed into seeds along with the regular DNA and is then used as a template to \u2018correct\u2019 certain genes. Conceivably, Pruitt says, some of the mystery non-coding transcripts could be responsible. \u201cI think there's something being inherited outside what we think of as the conventional DNA genome.\u201d \n               Changing views \n             The implications of such findings for our understanding of evolution have yet to be figured out. But research into the role of RNA as a carrier of information across generations promises to enrich \u2014 and complicate \u2014 the notion of a gene yet further. Leaving aside the can of worms that studies on epigenetics are beginning to open up, does it matter that many scientists not directly concerned with molecular mechanisms continue to think of genetics in simpler terms? Some geneticists say yes. They worry that researchers working with an oversimplistic idea of the gene could discard important results that don't fit. A medical researcher, for example, might gloss over the many different transcripts generated by a sequence at one location. And the lack of a clear idea of what a gene is might also hinder collaboration. \u201cI find it sometimes very difficult to tell what someone means when they talk about genes because we don't share the same definition,\u201d says developmental geneticist William Gelbert of Harvard University in Cambridge, Massachusetts. Without a clear definition of a gene, life is also difficult for bioinformaticians who want to use computer programs to spot landmark sequences in DNA that signal where one gene ends and the next begins. But reaching a consensus over the definition is virtually impossible, as Karen Eilbeck can attest. Eilbeck, who works at the University of California in Berkeley, is a coordinator of the Sequence Ontology consortium. This defines labels for landmarks within genetic-sequence databases of organisms, such as the mouse and fly, so that the databases can be more easily compared. The consortium tries, for example, to decide whether a protein-coding sequence should always include the triplet of DNA bases that mark its end. Eilbeck says that it took 25 scientists the better part of two days to reach a definition of a gene that they could all work with. \u201cWe had several meetings that went on for hours and everyone screamed at each other,\u201d she says. The group finally settled on a loose definition that could accommodate everyone's demands. (Since you ask: \u201cA locatable region of genomic sequence, corresponding to a unit of inheritance, which is associated with regulatory regions, transcribed regions and/or other functional sequence regions.\u201d) Rather than striving to reach a single definition \u2014 and coming to blows in the process \u2014 most geneticists are instead incorporating less ambiguous words into their vocabulary such as transcripts and exons. When it is used, the word \u2018gene\u2019 is frequently preceded by \u2018protein-coding\u2019 or another descriptor. \u201cWe almost have to add an adjective every time we use that noun,\u201d says Francis Collins, director of the National Human Genome Research Institute at the National Institutes of Health in Bethesda, Maryland. But however much geneticists struggle to pin down the elusive gene, it is precisely its ambiguous nature that fuels their continued curiosity. \u201cIt's ever more fascinating,\u201d says Whitehead's Young. Some things, it seems, are not best portrayed by a crude four-letter word. \n                     Plan matures for partner to genome quest \n                   \n                     Cress overturns textbook genetics \n                   \n                     Gene regulation: The brave new world of RNA \n                   \n                     50 Years of DNA \n                   \n                     ENCODE \n                   Reprints and Permissions"},
{"file_id": "440274a", "url": "https://www.nature.com/articles/440274a", "year": 2006, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Alison Abbott talks to the man who wants theoretical chemistry to ease political strife in the Middle East. Can scientific ties counter political tensions? Roald Hoffmann hopes so. This Nobel laureate from Cornell University, New York, has organized a series of three chemistry workshops for young scientists from the Middle East, with the aim of forging trust and friendship between participants. The idea stemmed from a small international chemistry meeting in Malta in 2003, which was attended by many chemists from Arab countries. Hoffmann taught the first of his workshops, called \u2018Chemistry Bonds\u2019, in Jordan's ancient city of Petra in January. The 13 attendees hailed from Jordan, Palestine, Saudi Arabia and Syria, as well as from Israel and Iran. \n               Political relations in the Middle East are very tense. Can academic classes really help? \n             It is unrealistic to think a scientific gathering will solve the problems of the region. But perhaps many small actions that build trust and connections between people, especially young people, will help. This is my small contribution to peace, but I'm not romantic or do-gooding about it. The group I brought together could be future leaders of science. \n               So how could chemistry possibly build trust? \n             Pere Alemany, my co-teacher from the University of Barcelona, and I worked those young people hard. So there was a certain shared suffering, and victory over that, on their part. Camaraderie builds in such intense situations. But we also did things that bring people close. One day we all cooked a Jordanian meal together; on another, we all trekked through the pink sandstone monuments of Petra. I asked each participant to bring a favourite piece of music. For me, music is the next best thing to science for bringing people together. At 10 p.m. each night we listened to the tapes with drinks: wine, soft drinks. Alcohol wasn't an issue. Some drank it, some did not. I wasn't going to go without wine! \n               Was religion an issue? \n             No, not really, but it surprised me how little the participants knew about each others' customs. Most were from Muslim backgrounds, a few were Jewish and there was at least one Christian. Some were secular, some religious. One of the Jewish students ate kosher, which led to illuminating discussion of dietary rites. \n               Was gender an issue? \n             Only in that it showed the ignorance we have in the West. Half of the applicants were women. \n               Did the participants get on with each other? \n             Yes, although at the beginning there was much shyness. I learned in Malta how little communication there actually is between scientists from the various Arab countries. But I was taken aback when one Israeli confessed she had never had a social conversation with an Arab, even a Palestinian, before she came to the Petra workshop. Several of the participants said something changed in them that week. \n               Do you think they will stay in contact with each other? \n             I hope so. We will help them, but I think it will come naturally. The bonds that formed are strong; I feel that. I was touched that one of the Jordanians called the conference coordinator, Vanessa Buisson, to find out if the Israelis had got home alright despite a bombing in Tel Aviv on the day of departure. He said: \u201cThey grew part of me that week.\u201d \n               Did the participants risk attack or denunciation by taking part in the workshop? \n             Part of me says there was no risk; part says, be realistic. Let's be frank: the risks were personal, perhaps greatest to the three Israelis and the American organizers. But in a different way there were risks to students from the Arab countries. They spent time with Israelis; some people back home don't like that. \n               How much chemistry did they learn? \n             A lot. Everything that Pere and I had to teach them from a lifetime of molecular orbital lore. It will serve them well. Only one or two will become theoretical chemists. The rest will use their learning in whatever they do. I was touched when at the end of the course, one student thanked me for \u201cmaking chemistry come alive for me again\u201d. \n               Where will the next two workshops take place? \n             In the next 18 months, Harvard's George Whitesides will lead one in Egypt on nanochemistry and Harry Gray from the California Institute of Technology will teach another on bioinorganic chemistry in Qatar. We could teach more. My dream is to convince a Saudi prince to host a workshop for everyone in the region \u2014 for his young people, Israelis and others. \n               \u2018Chemistry bonds\u2019 \u2014 is a metaphor intended? \n             Atoms bond because they don't have a choice. That's not bad; good chemistry comes from that bonding. But people do have a choice \u2014 and we gave some of them the opportunity to do so. \n                     Building bridges \n                   \n                     Academies wrestle with issue of Islam's flagging science base \n                   \n                     Chemistry at Cornell University \n                   Reprints and Permissions"},
{"file_id": "440014a", "url": "https://www.nature.com/articles/440014a", "year": 2006, "authors": [{"name": "Jenny Hogan"}], "parsed_as_year": "2006_or_before", "body": "Atomic force microscopes have revolutionized the study of materials, but probing watery biological systems has proved more difficult. Jenny Hogan asks whether a fix is at hand. Bart Hoogenboom's window-less room is a cramped space, almost filled by the sturdy table at its centre. Cables dangle from piles of electrical devices that climb to the ceiling. In the midst of the tangle of equipment is a stack of three small metal cylinders. Hidden inside the top one is a sliver of silicon, its tip quivering up and down 200,000 times each second. \u201cIt is an experimental physicist's dream and a biologist's night-mare,\u201d says Hoogenboom. Happily, Hoogenboom is a physicist, a post-doc at the University of Basel in Switzerland, and for him the roomful of gadgetry to tinker with is a treat. But the outcome of his tinkering could enrich the lives of biologists The cylinders on the table are the working parts of an atomic force microscope, or AFM. Invented 20 years ago 1 , AFMs are based on a sharp tip at the end of a flexible beam, or cantilever; the topography of a surface is detected by the bending of the cantilever as the tip scans the specimen. Under the right conditions, AFMs can produce images that are accurate down to the last atom. The AFM has revolutionized the way mat-erial scientists study surfaces. But in general, it has been harder to apply AFMs to the study of delicate biological molecules and the soft tissues in which they are found. The team Hoogenboom works in, and others in labs around the world, are planning to change that, by finding out how to use AFMs in ways that don't damage the samples. Biologists already have pretty good methods for studying things down to molecular or even atomic scales, but these involve taking the objects of study out of their normal biological context. Electron microscopy requires samples to be fixed and exposed to a vacuum; X-ray crystallography requires the relevant proteins to be forced into crystalline arrays. \u201cThe advantage of the AFM is clear,\u201d says Peter Hinterdorfer, a biophysicist at the University of Linz in Austria, who has used the technique to study how antibodies bind to their targets: \u201cyou can image in physiological conditions.\u201d Some of the most impressive biological AFM images, including the image shown opposite, are the work of Andreas Engel, the head of Hoogenboom's team. But even this beautiful picture highlights the limitations of the technique. The golden rings are rotors that form part of an energy-conversion machine found in cell membranes, called an ATPase. Each protein ring has a diameter of around 5.4 nm \u2014 and although it is possible to see the molecules that make up each ring, you can't make out the amino acids of the protein, never mind see the thousands of atoms from which the rings are built. \n               Close encounters \n             This is poor resolution compared with that possible for hard surfaces, but is good for biological images taken with AFMs in \u2018contact mode\u2019. AFMs can work in a bewildering variety of ways, but contact mode is the most straightforward. The sample is brought into contact with the tip and moved back and forth and from side to side. But it is not the most precise way of doing things. The AFM tip tends to damage or dislodge the things it is scanning, which degrades the image resolution and limits the types of sample the technique can be used to study. The obvious solution is to make the tip behave more gently when scanning the samples, and it is to this end that various researchers have turned to the \u2018frequency modulation\u2019 or FM mode. In FM mode, the tip hovers just above the surface under study. A pulse of energy is used to make the cantilever tremble, and the topography of the surface below affects the frequency at which it does so. So long as the forces that the tip senses as it hovers above the surface can be inferred from changes to the cantilever's frequency, the tip doesn't actually have to touch the sample at all. AFMs used in FM mode (FM-AFM) can achieve stunning results \u2014 even resolving features below the scale of an individual atom, thought to be the signature of atomic orbitals, on a hard, flat silicon surface 2 . But that sort of resolution requires the system to be used in a vacuum. For a long time, getting the FM mode to work in a messier, wetter environment seemed out of the question. Having water around the cantilever was expected to deaden the vibrations. \u201cMost experts would not have expected FM-AFM to work in water,\u201d says Stuart Lindsay of Arizona State University in Tempe, whose group develops AFMs and other scanning-probe instruments for biological applications. \u201cI would have been included in that class. Otherwise, I would have tried it first.\u201d The first evidence that FM-AFM might work in water came from Takeshi Fukuma of Kyoto University's electrical-engineering department in Japan. When Fukuma was a postdoc in Hirofumi Yamada's lab, which does research into molecular electronics, he sought to reduce the noise in the AFM's output by optimizing the way the cantilever's deflection was being measured. He thought this might compensate for the water damping down the oscillations. In most commercial AFMs, a laser beam is bounced off the top surface of the cantilever to work out the tip's position. Fukuma and his co-workers set to improving each element of this sensor system, boosting the laser power and introducing a technique used in CD-ROM and DVD drives to stabilize the laser performance. The combined effect of the changes was to reduce the noise in the system to near its theor-etical limit 3 . Without even trying to minimize the noise from other sources, such as the buffeting of the cantilever by water, the team got striking results. \u201cI did just a few experiments to get atomic resolution,\u201d says Fukuma. In July last year, the research team reported their findings 4 . The paper's centrepiece is an image of mica taken in pure water, showing the atoms which are just half a nanometre wide. \n               Scaling down \n             The developments in Yamada's lab are attracting attention. \u201cThis is certainly an innovation we're interested in and something we're watching,\u201d says Craig Prater, director of technology development for Veeco Metrology in Santa Barbara, California; Veeco is one of the world's leading suppliers of AFMs. Yamada says that Seiko Instruments and Jeol, two other companies that build AFMs, have also expressed interest in his work. But the advance is not yet on the radar of the researchers whom it might benefit. \u201cBiologists will pay attention as soon as the technique is used to image biological samples,\u201d says Daniel M\u00fcller, who collaborated with Engel on the ATPase work, and is now at the University of Technology in Dresden, Germany. Yamada has recently presented images of biological samples, such as the membrane protein bacteriorhodopsin, at conferences. And looking for a closer collaboration with biologists, Yamada's student Fukuma has now moved to work with an interdisciplinary group at Trinity College Dublin in Ireland headed by Suzi Jarvis. Within three months of arriving in Jarvis's lab, Fukuma had optimized its AFM, as he had done in Kyoto, making many of the electrical components himself. Fukuma's results have spurred Hoogenboom's group to ever greater efforts. His team has now achieved true atomic resolution of mica in water too. It worked independently of the Kyoto group on optimizing the measurement of cantilever deflection, completely replacing the optical beam deflection system used in standard AFMs with a component designed by the team 5  \u2014 hence all the clutter. The Basel team has also produced images of bacteriorhodopsin (a paper on this is under review at  Applied Physics Letters ). The FM-AFM images of bacteriorhodopsin seem slightly fuzzy compared with the best made in contact mode. But that doesn't worry Hoogenboom. \u201cWe have ten years\u2019 experience doing contact mode; we haven't yet optimized the FM technique,\u201d says Hoogenboom. \u201cIf you drive a car for the first time, you don't know how fast you can take a corner.\u201d FM-AFM is unlikely to achieve atomic resolution for biological samples, however. AFM is in general a rather slow technique, and during the minutes that a scan takes, the mol-ecules under study will wriggle. Also, the sides of the tip will interact with any portion of the sample that rises up near it. Several groups are working on reducing these effects, for example by making the tip sharper or speeding up the process of scanning. But even a small improvement in resolution over the contact-mode images could help, according to M\u00fcller, who worked on the ATPase rotors. Just being able to see small molecules binding to proteins \u201cwould vibrate the whole community\u201d, he says. This could provide, for example, a quick way to see how drugs bind to their target receptors \u2014 providing a new tool in drug discovery. But until people start to use FM-AFM, no-one is really sure what the implications will be. \u201cThere are some interesting results coming out now,\u201d says Jason Cleveland of Asylum Research, an AFM supplier that is collaborating with Jarvis's group to explore how to introduce FM-AFM capability to their instruments. \u201cBut it's right at the beginning of the snowball, and you don't really know how big it will be.\u201d Back in Basel, Hoogenboom recalls a discussion at a conference where biologists explained how they wanted an AFM to work. Their request was for a black box where you press a button and it gets you nice images. The FM-AFM technique is more complex than that, but the instrument needn't be a nightmare, assures Hoogenboom. \u201cA bit more development and it will almost look as simple as your CD player.\u201d \n                     How AFM works \n                   \n                     The scanning probe microscope in biology \u2013 a review \n                   \n                     The first paper on AFM \n                   Reprints and Permissions"},
{"file_id": "441402a", "url": "https://www.nature.com/articles/441402a", "year": 2006, "authors": [{"name": "Stuart Clark"}], "parsed_as_year": "2006_or_before", "body": "The Sun occasionally hurls streams of particles towards Earth, where they can wreak havoc with satellites. Predicting these solar storms is hard, but some physicists believe we're about to face the biggest bout of solar flares in years. Stuart Clark reports. Halloween is supposed to be a time of weird phenomena and spooky events. But by any standards what happened in late October 2003 was unusual. Telecommunications around the world were disrupted, half of NASA's satellites malfunctioned, 50,000 people in Sweden were left without electricity, and the global airline industry lost millions of dollars. The link between these events was not supernatural, it was something far more familiar: the Sun. The chaos was caused as our star went through one of the more active moments in its 11-year cycle. And according to some predictions, what happened that October is nothing compared with what is going to occur in five or six years' time. \u201cSolar activity in the next cycle could be more of a problem than ever,\u201d warns Peter Gilman, a physicist at the National Center for Atmospheric Research (NCAR) in Boulder, Colorado. And it's not just satellites and telecommunications that face problems. Some researchers claim that the Sun's behaviour affects Earth's atmosphere \u2014 in particular influencing cloud formation. This claim has attracted global-warming sceptics, who argue that the Sun has greater influence than human activities on our changing climate. The Sun's 11-year cycle is driven by its magnetic field, and generates a flow of charged particles known as the solar wind. At the quieter parts of the cycle, activity is fairly low and the solar wind is reasonably uniform. But at the \u2018solar maximum\u2019, sunspots \u2014 dark patches caused by the magnetic field twisting at the surface \u2014 appear on the Sun's face. Huge solar flares explode above these spots causing turbulence in the solar wind and sending streams of charged particles hurtling through space. The most recent solar cycle was fairly moderate when measured by the number of sunspots (see graphic). Yet in late October 2003, three years after the cycle's peak, two monstrous sunspots appeared, each more than ten times the diameter of Earth. Both were in a state of almost constant eruption, spewing out billions of tonnes of electrically charged particles. These were the particles that caused such havoc when they hit Earth's atmosphere. The global maritime emergency call system blacked-out, contact was lost with expeditions on Mount Everest, and the accuracy of the global positioning system was impaired. As well as NASA's satellite malfunctions, the Japanese lost contact with one of their weather satellites altogether. The cost to the airline industry arose as planes were re-routed to lower altitudes, congesting the airways and burning more fuel. \n               Lucky escape \n             The sunspots bombarded Earth, on and off, for two weeks as the Sun's rotation carried them across its face. On 4 November, as the second sunspot was about to be lost from sight, it let loose another tremendous explosion. Solar physicists calculated that it was one of the largest solar flares in recorded history 1 . By sheer luck it exploded into deep space, catching Earth only in the side wash. Those who saw it breathed a sigh of relief and wondered what the damage might have been if such a flare had exploded facing Earth. If the latest prediction comes true for the next solar cycle, we may yet find out. Predicting the timing and strength of such solar eruptions is clearly important, but it is hampered by the fact that scientists know relatively little about the Sun's inner workings. So to coincide with the start of the next solar cycle, the largest coordinated study of the Sun will be launched next year. Known as the International Heliophysical Year (IHY), the initiative hopes to build awareness of the Sun's possible influence on Earth's climate and to bring researchers from different disciplines together to study solar activity. Currently, the Sun is at a solar minimum, and most predictions suggest that the next solar maximum in five or six years' time will be weak. But the most recent forecast, the first to be based on a completely physical model of the Sun, suggests otherwise. This forecast has been generated by Mausumi Dikpati and her team at the NCAR 2 . They have developed a computer simulation that mixes the Sun's internal magnetic dynamo with theories about how solar plasma circulates near the surface. And they have reached a sobering conclusion. \u201cWe expect between 30% and 50% more sunspots and solar activity than the cycle just ending,\u201d says Gilman, who is a member of Dikpati's team. The last time solar activity occurred on this scale was in 1958, when there was little technology in orbit. Now things are very different: Earth is surrounded by thousands of active satellites. Satellite operators rely on predictions of solar activity to estimate the lifetime of space missions. The solar wind heats Earth's thin upper atmosphere, increasing atmospheric density and causing more drag. Gilman estimates that a 30% increase in activity will almost double the atmospheric density at an altitude of 300 km, affecting low-altitude satellites. Mission planners looking ahead to 2012 may want to boost their spacecraft to higher orbits, or accept a shorter operational lifetime. Even above 800 km, where satellites are safe from atmospheric drag, other dangers remain. The solar wind can cause a build up in electrical charge, which then short-circuits and burns out sensitive equipment. This is the suspected fate of the Japanese Midori 2 satellite, lost during the 2003 flares. And as more satellites die in orbit, operators have to worry about dodging \u2018space junk\u2019. In the aftermath of a large solar storm, the change in atmospheric drag can shift the orbit of space debris, endangering active satellites. The Sun's influence over space hardware is only one aspect of the latest drive to understand the star. The possible effects of the solar cycle on our climate, especially cloud formation, are also receiving a lot of attention. A link between the two was suggested in 1997, when meteorologists Henrik Svensmark and Eigil Friis-Christensen, both at the Danish Meteorology Institute in Copenhagen, analysed weather satellite records for 1979 to 1992. They found that during solar minima, Earth was 3% cloudier than at solar maxima 3 . They also noticed that the influx of high-energy particles reaching Earth from deep space, phenomena known as cosmic rays, was up to 25% higher at solar minima, hinting that they might seed cloud formation. The pair called their finding a \u201cmissing link in solar\u2013climate relationships\u201d. Climate sceptics who argue that human activities are not responsible for global warming have seized on these results. They claim it shows that the Sun is largely responsible for variations in our climate. So convinced are they that last year two Russian sceptics placed a $10,000 bet that global temperatures will show an average fall for 2012\u201317 \u2014 on the assumption that the next solar cycle will be weak 4 . But most proponents of the solar\u2013climate link are proceeding more carefully. \u201cWe're not suggesting that all clouds are formed by solar activity, merely that the process might be modulated by solar activity,\u201d says Robert Bingham, a physicist at the Rutherford Appleton Laboratory in Didcot, UK. He is part of an international experiment known as CLOUD, or Cosmics Leaving Outdoor Droplets. This will use CERN's particle accelerator on the French\u2013Swiss border to fire charged particles through a chamber holding gases to simulate Earth's atmosphere and determine whether \u2018clouds\u2019 are created. \n               Global network \n             To take advantage of the next solar cycle more directly, the United Nations is heading an initiative to install radio receivers in all 191 of its member states. For the first time, the upper atmosphere's response to the continual collision of solar radiation would be monitored on a global basis. Although space officially starts at an altitude of about 100 km, scientists know little about this region because it is difficult to study. The UN project is one of the planned elements in the IHY. Although it has no dedicated research budget, the IHY has initiated a call-for-proposals aimed at making it easier for scientists from any discipline to gain access to solar instruments and data. \u201cWe are inviting ideas from the community,\u201d says Rutherford's Richard Harrison, the joint UK coordinator for the IHY. Certainly 2007 will put at scientists' disposal the largest-ever fleet of space missions for studying solar\u2013terrestrial interactions. A dozen spacecraft that track solar activity are already in orbit, and another three should launch this year, including the most sophisticated solar watchdog yet. NASA's Solar Terrestrial Relations Observatory (STEREO) consists of two nearly identical craft that will watch the Sun from different locations, one preceding Earth in its orbit and the other following behind. This will allow them to take stereoscopic images of the Sun and to track the three-dimensional structure of particle eruptions. In this way, STEREO might be able to supply advance notice of the speed and direction of eruptions as they head towards Earth. Such information should help satellite operators respond to imminent dangers, but for proper planning they will need long-term forecasts of solar activity. Some researchers, such as Harrison, believe that scientists don't yet understand the Sun enough to make meaningful long-term predictions. Certainly, past forecasts have relied on tracking signposts of future solar activity, without worrying too much about the mechanisms behind them. For example, in the 1970s, astronomers recognized that the build up of magnetism at the Sun's poles after the cycle has peaked has a bearing on the strength of the next cycle. Just last year, one of the pioneers of this method, Leif Svalgaard, used the Sun's polar magnetic field to predict that the next solar cycle will be the weakest for a century 5 . Other \u2018signpost\u2019 methods, such as those looking at the amount of 10.7-centimetre radio waves coming from the Sun or the number of bright patches near the Sun's poles, also forecast a weak cycle. The only signpost method to predict a strong cycle comes from solar physicists David Hathaway and Robert Wilson at NASA's Marshall Space Flight Center in Huntsville, Alabama. In 2004, they noticed that a solar cycle's strength correlates to the number of sunspots two cycles before. Applying that rule of thumb to the next cycle, they have predicted strong activity in 2012 (ref.  6 ). Dikpati's model agrees with this forecast and, crucially, puts the reason for it on a physical footing. In the past decade, physicists have discovered a vast conveyor belt of plasma on the Sun that seems to flow from the equator to the poles in each hemisphere at around 30\u201365 kilometres per hour. Sunspots are typically active for just a few weeks before fading from view, but their magnetic fields linger on. These weak fields are carried by the flow and accumulate at the poles before being submerged below the surface, where they presumably flow back towards the equator 7 . Dikpati's work combines sunspot observations dating back to the 1900s with a computer simulation of the Sun's magnetic dynamo and the conveyor belt (see graphic). In the simulation, the conveyor belt sweeps along old sunspots, submerging them at the poles. During the deep return flow, the Sun's rotation rejuvenates the old magnetic fields, creating new sunspots and fresh areas of solar activity. It is the only prediction in which every step uses a physics-based computer model, which is why it is being taken seriously by solar physicists. \u201cThe solid physics of Dikpati's model is a high hurdle for the other techniques to get over,\u201d says Hathaway. \n               Solar memory \n             The key to Dikpati's forecast is how fast the Sun's conveyor belt runs. The deep return flow is unmeasurable but the model suggests that it is slower than the surface flow, perhaps just 5 kilometres per hour. If so, the return leg of the journey would take a couple of decades. \u201cThis shows that the Sun retains a memory of its magnetic field for about 20 years,\u201d says Dikpati. So in her model, the Sun's activity is not based solely on the previous cycle's magnetic field but on the interplay with earlier cycles. In contrast, most \u2018signpost\u2019 prediction methods assume that the previous solar cycle immediately kicks off the activity of the next. \u201cIt is good for science that the predictions are now diverging,\u201d says Svalgaard, although he disagrees with Dikpati's conclusions. Solar physicists are now waiting to see if this physics-based forecast is right. And they may not have to wait for the peak of activity in six years' time to find out. All methods predict only the average number of sunspots, but records show that large cycles have always begun early and raced to their peak. That means that the telltale signs of a large solar cycle should be evident within just three or four years from now. \u201cWe must now let Mother Nature tell us who is right,\u201d says Svalgaard. But Dikpati and her team are refining their model to see whether it can predict features such as an early start. Either way, there will be plenty of sun watchers \u2014 from mission planners to climate sceptics \u2014 tracking the way the solar wind blows. \n                     STEREO homepage \n                   \n                     International Heliophysical Year (IHY) \n                   Reprints and Permissions"},
{"file_id": "440140a", "url": "https://www.nature.com/articles/440140a", "year": 2006, "authors": [{"name": "Tony Reichhardt"}], "parsed_as_year": "2006_or_before", "body": "The cost of the James Webb Space Telescope could cripple US astronomy. Tony Reichhardt takes a closer look. Every ten years, US astronomers get together to list the things they want most in all the world \u2014 and outside it. This may sound like a grown up version of writing to Santa Claus, but these Decadal Surveys are taken very seriously. Presenting a united front on what matters most to one's profession is a powerful bargaining tool when projects come up for political approval. On astronomers' most recent wish list, put together in 2000, pride of place was given to what was then known as the Next Generation Space Telescope, an observatory that would take up the mantle of the Hubble Space Telescope as Earth's orbiting eye on the cosmos. Half a decade on, that telescope, now named after former NASA administrator James Webb, is well under way. But, as always, there's a catch. At the beginning of the millennium, US astronomers thought that their most-wanted project would cost $1 billion. Its projected cost is now nearly five times that. Price tags that mimic the Big Bang's inflation are nothing new to astronomy. The problem for the James Webb Space Telescope (JWST) is that the budgetary space in which it's expanding is shrinking. Money once slated for science is being diverted to the space shuttle, the International Space Station and plans for future manned exploration (see  Nature   439 , 768\u2013769; 200610.1038/439768a ). So the costs of the Webb telescope are leading to cancellations \u2014 or \u2018indefinite deferrals\u2019, as NASA prefers to call them. For those whose dreams are crushed in this process, the Webb telescope is looking less like the future of their field and more like its foreclosure. One critic of the process is Shri Kulkarni, an astronomer at the California Institute of Technology in Pasadena. \u201cMy worry,\u201d he says, \u201cis that we are starting on a project whose cost we don't understand, and which is now devouring space-based astronomy. I doubt there's any project that is worth abandoning the rest of the field.\u201d Sterl Phinney is an astrophysicist, also at the California Institute of Technology, whose favoured project, the LISA gravitational wave telescope, has just been deferred indefinitely by NASA. He says that even before the recent cuts, the Webb telescope was \u201cbasically sucking up all the other money\u201d astronomers hoped to use. The community is now split between those who view the situation with growing alarm and those who, according to Phinney, \u201creally like the JWST and think it's OK that it eats everything else \u2014 although even some of those are worried about the balance and health of astronomy.\u201d Kulkarni thinks some in this second camp are still in \u201cstunned shock\u201d over the most recent shift of funds away from science at NASA, and just haven't reached a consensus on what to say, let alone do, about it. One thing on which everyone agrees is that, if it works as advertised, the Webb telescope will be one fantastic machine. The telescope's 25-square-metre mirror is not just much bigger than Hubble's; it is bigger than any you would have found at any observatory in the world when Hubble launched. \n               The long view \n             Hubble's design is optimized for visible and ultraviolet light, but the Webb telescope will see in the infrared. Sitting above the atmosphere, it will have an unfiltered view of a swathe of wavelengths from 0.6 \u00b5m (at the red end of the visible spectrum) all the way to the first fringes of the far infrared at 28 \u00b5m. At longer wavelengths, images of a given resolution require a larger mirror; the Webb telescope's honeycomb of burnished beryllium will give it a resolution in the infrared that is as sharp as Hubble's is in the visible. The mirror's size also makes the telescope particularly sensitive: its instruments should see objects 10 to 100 times fainter than Hubble can. Going into the infrared means the telescope has to have a big mirror and has to be stationed far from Earth (the heat from which would otherwise be a problem). It also has to be thoroughly shielded from the Sun, with a structure that somewhat resembles a multistorey trampoline. These requirements have all driven up the telescope's cost. But seeing in the infrared is not an optional extra; it's a necessity. If you want to look at the early Universe, the infrared is where the action is. Theory holds that after the glow of the Big Bang faded, the Universe entered a long, lightless \u2018dark age\u2019. Eventually, knots in the cold dark material condensed, collapsed and began to shine \u2014 the first stars. These earliest stars are receding from us at a great rate, which stretches out the light that reaches us and extends its wavelength towards the red end of the spectrum. The first stars are thought to have started shining less than a billion years after the Big Bang, giving them \u2018redshifts\u2019 in which the change in the light's wavelength relative to its original value is 20 or more \u2014 moving visible wavelengths well into the infrared. This is a large part of the reason why even far-sighted Hubble has never seen objects with a redshift of more than 7. The Webb telescope should solve this: young stars characteristically give off ultraviolet light that, after a redshift of 15, shines at 1.9 \u00b5m \u2014 \u201csmack in the middle of our best band\u201d, enthuses John Mather, the JWST senior project scientist. \n               First light \n             Recently, Mather was part of a team led by Alexander Kashlinsky, now of Goddard Space Flight Center near Washington DC, that used the much smaller Spitzer infrared space telescope to detect a diffuse glow from \u2018first light\u2019 stars (A. Kashlinsky  et al .  Nature   438 , 45\u201350; 2005). No current or planned telescope, not even the Webb telescope, can resolve individual first-light stars. But the Webb telescope should be able to see the supernovae that resulted when these massive but short-lived bodies exploded, providing the Universe with its first heavy elements. It should also see the first galaxies that formed. One of the key observations for the Webb telescope will be \u2018deep field\u2019 pictures similar to those taken by Hubble. In these, a telescope points at a small patch of sky, taking a long, deep exposure that is designed to reveal extremely faint, distant objects. Astronomers hope the Webb telescope's near-infrared deep field (where contrast and resolution are best) will provide them with images of the very first galaxies and proto-galaxies. For all this, advocates of the Webb telescope are eager to point out that it is more than just a \u2018first light\u2019 machine. They argue \u2014 especially to scientists whose projects are being sacrificed \u2014 that although the Webb telescope was inspired by cosmologists' interest in the earliest stars, it has much to offer other fields. Take, for example, the search for planets around other stars. One of the casualties of this year's NASA budget was the Terrestrial Planet Finder, a mission designed to look for objects the size of Earth; its budget fell to zero. The Webb telescope cannot do what the Terrestrial Planet Finder was meant to do. But planetary scientist Jonathan Lunine of the University of Arizona, Tucson, points out that it should still deliver relevant science that no current telescope can. An interdisciplinary investigator on the telescope's science working group, Lunine says it will return images and spectra for planets not all that much bigger than Jupiter, and may in special circumstances produce spectra for the atmospheres of planets as small as Uranus. Its high-resolution pictures of dusty circumstellar disks will be the sharpest ever, providing insight into planet formation. It even has applications within our own Solar System, for studying the thermal properties of the Kuiper-belt objects that orbit beyond Neptune. And these are just the planned observations. Heidi Hammel, a planetary astronomer at the Space Science Institute in Boulder, Colorado, and another member of the JWST science working group, says some of the telescope's most important results may well be unforeseen. Some of Hubble's best findings, including the deep-field observations, \u201ccame from things we hadn't even thought of, because it opened up new discovery space\u201d, she says. \n               Cash register \n             So no one is denying that the JWST will be a first-rate telescope, perhaps even a revolutionary one. Just last August an independent assessment team charged by the project to review the telescope's science potential reported that \u201cthe scientific case for the JWST mission has become even stronger\u201d since the Decadal Survey's endorsement in 2000. But what of its expense? NASA's latest budget puts the project's price tag, including $1 billion for a decade's worth of operations, at $4.5 billion. That's more than the entire annual research and development budget of the National Science Foundation; it represents more than $1 million for each full member of the American Astronomical Society. On top of that there are the contributions by Europe and Canada, junior partners on this telescope just as they were on Hubble. Europe will contribute one of the telescope's four instruments, the Near Infrared Spectrograph, and the launch on an Ariane 5 rocket. For an investment that approaches half a billion dollars, it will get 15% of the viewing time. Canada's $57 million will provide a fine guidance sensor and other hardware, for which it gets about 5% of the science use. The total cost is more than 30 times greater than that of the Keck telescopes on Hawaii, which boast two of the largest mirrors on Earth. One reason for this extraordinary expense is that the JWST is a challenging spacecraft to build. The segmented structure of the mirror, made from 18 hexagonal pieces of beryllium, is unlike anything built before; so is the multilayer sunshade and the system that will deploy them both. Robert O'Dell, who as project scientist for Hubble was in Mather's position 30 years ago, points out that Hubble was able to borrow much of its technology from spy satellites. The Webb telescope has no such heritage on which to draw. \n               Astronomical costs \n             NASA tried to head off difficulties by tackling some key technology issues early in the project's life. Although that helped to identify potential trouble spots, it didn't always reduce costs, says Eric Smith, programme scientist for the Webb telescope at NASA headquarters in Washington DC. For example, the engineers found they could build lightweight mirror segments, but not as fast as some had hoped \u2014 the job will end up taking six years instead of four. The early development work led to the mirror losing a third of its originally envisaged surface area in 2001. Other proposed cuts in capability \u2014 the dropping of the telescope's mid-infrared instrument, a possible further shrinkage to the mirror \u2014 were deemed scientifically unacceptable. Some savings have been found. In 2005, project managers decided to forgo the extra mirror polishing needed to make the telescope's images utterly crisp in wavelengths shorter than 1.7 \u00b5m, on the basis that future large ground-based telescopes equipped with adaptive optics would be able to deal with these wavelengths more-or-less as well. And switching from a vacuum test chamber in Ohio to one at the Johnson Space Center in Houston, Texas, should save more than $100 million. Despite this, the cost has continued to climb, alarmingly jumping almost $1 billion in 2005 alone (see graph). NASA's requirement that the programme beef up its contingency fund added a little over $200 million. A delay in the government's decision to move from a US launcher to the Ariane added an estimated $300 million as highly paid engineers were unable to move forward until they knew which rocket they were designing for. The situation is particularly embarrassing given that the cost of delaying the decision ended up being greater than the cost of the launch. That delay, and a NASA decision to rearrange the project's long-term budget yet again, saw the launch slip from 2011 to its current date of 2013. Every slip increases the total cost. By the standards of ground-based astronomy, just a year's worth of Webb telescope overrun looks vast. Even the most expensive proposed instruments, such as the Atacama Large Millimeter Array (see  Nature   439 , 526\u2013528; 2006) or the various Keck-dwarfing 30-metre telescopes that are under discussion, should leave ample change from $1 billion. But O'Dell offers some perspective. Space telescopes are more expensive not just because the technology is more challenging, but because every problem and every contingency has to be thought through and solved before launch. This typically requires a large team of engineers to remain in place for years. What seem to be additional costs have also come from NASA's long and painful switch to \u2018full-cost accounting\u2019. In this system, all of a mission's expenses \u2014 every paper clip and every guard at the front gate \u2014 are included in the total bill. This makes NASA overheads smaller, and the prices of individual missions greater. For all this, the growth in cost of the Webb telescope is not unprecedented. O'Dell recalls that in 1972, Hubble's total price including its first year of operation was projected to be $300 million ($1 billion in today's prices). According to Robert Smith, a historian at Canada's University of Alberta who wrote a political history of the telescope, Hubble ended up costing a lot more by the time it reached the launch pad in 1990 (several years late owing to the Challenger shuttle accident). He says that if the budgets were calculated according to NASA's current full-cost accounting standards, \u201cthe development cost of Hubble to date is certainly more than $4 billion in today's dollars\u201d. NASA's Eric Smith adds that when new instruments and operating expenses are added, that comes to $9 billion. This doesn't include the cost of four space-shuttle servicing missions to Hubble, and a fifth being planned \u2014 the cost of a shuttle launch can be put at about $500 million. All in all, building, launching, using and refurbishing Hubble has probably been the most expensive undertaking ever made in the name of pure science; the mission is still, remarkably, costing more than $300 million a year. In that context, you begin to understand how Lunine can claim with a straight face that the Webb telescope \u2014 which will outperform Hubble in almost every way \u2014 is in fact \u201ca bargain\u201d at $4.5 billion. Still, the discipline as a whole has to wonder whether it can afford a bargain quite this big in straitened times. Between them, Hubble and the Webb telescope will soon consume half of NASA's astrophysics budget (see chart). Some critics have concluded that the carefully crafted recommendations of the most recent Decadal Survey are no longer viable: if the costs had been clear, the priorities might well have been different. Kulkarni was on the review panel for ultraviolet, optical and infrared astronomy from space. He says, \u201cI now regret that we were not clear thinkers\u201d about what was affordable, and he believes that the plan was \u201cfiscally unrealistic\u201d even before NASA cut its science budget last month. If it was unrealistic, says David Black, president of the Universities Space Research Association, scientists should share in the blame. \u201cAstronomers pushed NASA to have all these missions. NASA bought into that. And all it takes is one hiccough like the JWST overrun. It's like rush hour traffic. One incident, and suddenly everybody's piled up, there are schedule delays, and it becomes unstable very quickly.\u201d The problems that come with sending mission after mission into this crowded traffic are exacerbated when the costs of the missions are set artificially low at the beginning. When NASA administrator Mike Griffin told a January meeting of the American Astronomical Society that the Webb telescope wasn't so much overbudget today as it was \u201cundercosted\u201d at its inception, he wasn't just putting a good spin on things. The Decadal Survey guessed the cost as $1 billion. Studies in the mid-1990s had pegged the price at between $500 million and $1 billion. These were based partly on the hope \u2014 unfulfilled, as it happened \u2014 that the Webb telescope might take advantage of advances in building low-cost spacecraft developed by the military. Oddly, earlier cost estimates for a large infrared space telescope were closer to the mark. A 1984 Space Science Board panel predicted the cost including operations to be $4 billion (roughly $7 billion today), and a subpanel of the 1990 Decadal Survey thought it would run to about $2 billion not counting operations, which, when adjusted for inflation, closely matches NASA's current projections. \n               Political plays \n             Garth Illingworth of the University of California, Santa Cruz, who chaired the 1990 panel, chalks the anomalously low estimates from the 1990s up to a \u201clack of reality\u201d inherent in the \u2018faster, better, cheaper\u2019 philosophy of Dan Goldin, NASA's administrator at the time. Goldin focused on accelerating the development of spacecraft, and increasing innovation, while accepting a moderate rise in the risk of failure. Some projects conceived under this tag, in particular two Mars missions lost in quick succession, brought it a certain disrepute. \u201cIt was a horrible, political circumstance framing all the discussion in that decade,\u201d says Illingworth. Reinhard Genzel of Germany's Max Planck Institute for Extraterrestrial Physics in Garching says it was clear at the time that a $500-million estimate for the Webb telescope was a \u201cpolitical price\u201d. Yet such was the climate of the 1990s that when estimates for the European Space Agency's smaller Herschel infrared observatory came in at $1 billion, he says, \u201cI was approached by many colleagues saying, \u2018You guys are so stupid. Why can't you do this for less?\u2019 That's now haunting NASA, of course.\u201d Today, Illingworth inveighs against the \u201cextraordinarily bad, artificial cost estimates\u201d of the Goldin era. But the 2000 Decadal Survey seems to have been happy to accept them. The world of big science is well used to projects being lowballed \u2014 a process that gets schemes started on the basis of a low cost estimate, with the implicit hope that by the time the true costs are known inertia and vested interests will make it impossible to pull out. Lowballing is not a practice anyone would defend on principle, but histories like the Hubble's show it can work (see  page 127 ). \n               Past the disquiet \n             Craig Wheeler, a University of Texas astrophysicist and president-elect of the American Astronomical Society, takes the lessons of Hubble to heart: \u201cI remember when we were building the Hubble Space Telescope, which has been spectacularly successful, there were an awful lot of eggs put in that basket. And other smaller, faster, university-based projects suffered. I think we got through it.\u201d Wheeler accepts the disquiet over the Webb telescope's costs, but he doesn't think astronomers have yet reached \u201cthe point we collectively would say \u2018enough\u2019\u201d. And he warns against revisiting the results of the Decadal Survey on the basis of the current crisis: \u201cYou alter those priorities at great risk.\u201d Kulkarni is more pessimistic. He thinks that NASA's \u201claserlike focus\u201d on the Webb telescope short-changes missions that would hunt for planets, probe the nature of dark matter, search for gravitational waves, and tackle other topics that might ultimately prove more popular with young scientists and with the public. His concern is shared by Charles Beichman of the Jet Propulsion Laboratory in Pasadena, a leading light of the cancelled Terrestrial Planet Finder mission. Beichman thinks the Webb telescope will be \u201ca fine machine. It will do fantastic science\u201d. In fact, he is on one of the instrument teams. But when he goes to professional meetings, he sees more young astronomers attending sessions on planet-finding than on Hubble or the Webb telescope. Lunine thinks the critics are fighting the wrong battle (and that anyone who doesn't realize that the Terrestrial Planet Finder would be costlier than the Webb telescope is dreaming). It is not the JWST that is to blame, he says. The real problem is that \u201cNASA's science budget is not adequate\u201d, and science is \u201ctaking the hit\u201d as the agency shifts its focus to returning astronauts to the Moon. That may be the case. But for now, the Webb telescope is left in the awkward position of being the only one eating in a room full of hungry people. \n                     Astronomers urge NASA not to cut corners on Hubble successor \n                   \n                     US space scientists rage over axed projects \n                   \n                     James Webb Space Telescope \n                   \n                     JWST Science Assessment Team Final Report \n                   \n                     2001 Decadal Survey for Astronomy \n                   \n                     First Light and Reionization Conference \n                   \n                     NASA 2007 budget request \n                   Reprints and Permissions"},
{"file_id": "440144a", "url": "https://www.nature.com/articles/440144a", "year": 2006, "authors": [{"name": "Michael Hopkin"}], "parsed_as_year": "2006_or_before", "body": "Ecologists paid by industry to assess the effects of businesses on the environment are often accused of selling their souls. But isn't scientific expertise exactly what is needed? Michael Hopkin investigates. Picture this: you are a talented research ecologist and you're evaluating whether a planned hydroelectric dam could damage the local ecosystem. Your findings lead you to believe that the fish in the river would not be significantly harmed by the dam. But when you publish your results, your colleagues refuse to believe them. Why? Because you work for the company that is building the dam. At first glance, big business seems to be only bad for the environment. After all, industry has cut down rainforests and opened up huge mining scars on the landscape. Surely, it might seem, any ecologist who takes money from an organization that so harms the natural world must be putting concerns about the environment second to salary. In fact, many ecologists take up industrial contracts to try to minimize the damage caused. But in doing so, they walk a delicate line between those who want to save the natural world and those who want to exploit its resources. Some face accusations from their peers that they've \u2018sold out\u2019. And conflicts often arise between their interests as researchers and those of the companies they work for. Faced with these challenges, many must question whether their decision to work for industry was worth it. For some, such as Tyrone Hayes, the answer is no. Hayes is an ecological toxicologist at the University of California, Berkeley, who has received funding from the agribiotech giant Syngenta in the past. \u201cMy view has changed a lot since working with Syngenta,\u201d he says. \u201cIt's made me a lot more sceptical of scientists who get involved with industry.\u201d Hayes's work touches on one of the most politically charged areas of applied research: the impact of pesticides on the environment. Specifically, he is studying the effects on frogs of atrazine, widely used on transgenic crops. At Berkeley, Hayes took up a contract with Syngenta, brokered by a consulting firm. In his research, he found that exposure to atrazine leads to male frogs becoming feminized, as measured, among other traits, by larynx size 1 . \n               Culture of mistrust \n             But Syngenta asked him to divide his data on larynx size by the frogs' body weight, a procedure that he says was designed to make the key finding disappear. A Syngenta representative said that processing the information in this way is a common method for handling data from such studies, designed simply to control for the presence of naturally stunted frogs. Hayes eventually gave up the lucrative contract, and no longer works with industry colleagues, who are forbidden from discussing their results with him. Hayes worries that many scientists in his field could come under similar pressure when working under industry contracts. \u201cIt's up to researchers to maintain the integrity to say: \u2018No, I produce data in my lab and I have got to stand by them\u2019,\u201d he says. But that can be hard to do, especially when research funding is at stake. In general, big businesses, often forced by environmental regulations to investigate the impact of their activities on the environment, have the financial muscle to fully support such projects. Last year the BP Conservation Project, funded by the UK-based oil giant BP, awarded US$600,000 to 28 groups of conservationists. Overall, the company spends around $100 million each year on community-investment efforts. That amount of money can seem vast to researchers used to relying on academic funding (see  \u2018The lure of industry\u2019 ). \n                     Industry lured by the gains of going green \n                   \n                     Green groups baulk at joining nanotechnology talks \n                   \n                     BP Conservation \n                   \n                     Flora and Fauna International \n                   \n                     Syngenta \n                   \n                     Rio Tinto \n                   Reprints and Permissions"},
{"file_id": "440270a", "url": "https://www.nature.com/articles/440270a", "year": 2006, "authors": [{"name": "Jim Giles"}], "parsed_as_year": "2006_or_before", "body": "Studies of medical literature are confirming what many suspected \u2014 reporters of clinical trials do not always play straight. Jim Giles talks to those pushing for a fairer deal. They answer only the questions they want to answer. They ignore evidence that does not fit with their story. They set up and knock down straw men. Levelled at politicians, such accusations would come as no surprise. But what if the target were the researchers who test drugs? And what if the allegations came not from the tabloid press, but from studies published in prestigious medical journals? The slurs may sound over the top, but each is based on hard data. Since 1990, a group of researchers has met every four years to lay bare the biases that permeate clinical research. The results make for uncomfortable reading. Although outright deception is rare, there is now ample evidence to show that our view of drugs' effectiveness is being subtly distorted. And the motivation, say the researchers, is financial gain and personal ambition. \u201cPatients volunteer for trials, but finances and career motives decide what gets published,\u201d says Peter G\u00f8tzsche, an expert in clinical trials and director of the Nordic Cochrane Centre in Copenhagen. \u201cThis is ethically indefensible. Change is not easy, but we must get there.\u201d It is a dramatic conclusion to come from a field of study with no proper name, staffed by part-time volunteers. Most are journal editors, medical statisticians or public-health experts, united by fears for the integrity of clinical trials. For the devotees of \u2018journalology\u2019 or \u2018research into research\u2019, the literature on clinical trials is their raw data and patterns of bias are their results. Some of these researchers are using their findings to change medical journals and make it harder for authors to misrepresent results. Others are working on what could become the biggest reform of clinical-trial reporting for decades: the creation of a comprehensive international registry of all clinical trials. It is a powerful idea, which could one day make all trial information public. It is also an idea that has pitched pharmaceutical companies against advocates for reform, in a tussle over whether transparency or commercial confidentiality best serves medical science. \n               Just say no \n             One of the biggest problems with clinical-trial reporting, the suppression of negative results, shows the importance of such debates. Because clinical researchers are not obliged to publish their findings, ambiguous or negative results can languish in filing cabinets, resulting in what Christine Laine, an editor at the  Annals of Internal Medicine  in Philadelphia, Pennsylvania, calls \u201cphantom papers\u201d. If that happens, the journal record will give an over-optimistic impression of the treatments studied, with consequences for peer reviewers, government regulators and patients. One alleged example hit the headlines in 2004. At that time, the antidepressant Paxil (paroxetine), made by London-based drug giant GlaxoSmithKline, was a popular treatment for adolescents in the United States. But doctors have now been warned off prescribing Paxil to youngsters, after evidence emerged that it increases the risk of suicidal behaviour. It was claimed in a court case brought in the United States that GlaxoSmithKline had suppressed data showing this since 1998. Rick Koenig, a spokesman for GlaxoSmithKline, says the company thought the charges unfounded, but agreed to pay $2.5 million to avoid the costs and time of litigation. Phantom papers can be tracked down through trial protocols \u2014 the document describing how a trial will be run and what outcomes will be measured \u2014 which have to be registered with local ethics committees. By matching papers with protocols, several groups have shown that many trials are completed but not published. And that, notes Laine, makes it impossible for journals and health agencies to assess potential drugs. \u201cYou never quite know if other data are out there that would influence your conclusions,\u201d she says. Last year, for example, a French team showed that only 40% of trials registered with its country's ethics committees in 1984 had been published by 2002, despite more than twice as many having been completed 1 . Crucially, papers with inconclusive results not only took longer to publish (see graph), they were less likely to see the light of day at all. Researchers in any field can sit on negative or inconclusive results. But critics say that clinical researchers carry a greater ethical burden, as their findings inform decisions about the licensing of drugs. \n               Don't believe the hype \n             Nor do the problems end when a trial hits an editor's desk. Results from a trial of the arthritis drug Celebrex (celecoxib) looked good when they were published in 2000, for example, but less so when physicians scrutinized the full data set. The original paper, which appeared in the  Journal of the American Medical Association  ( JAMA ), dismissed fears that Celebrex could cause ulcers. But that was based on data collected over six months. When other physicians analysed a full year's worth \u2014 which the authors already had at the time of their  JAMA  submission \u2014 they claimed that Celebrex seemed to cause ulcers just as often as other treatments 2 . The original study's authors say that the later data were too unreliable to be included, but acknowledge that they could have \u201cavoided confusion\u201d by explaining to editors why they had omitted them. This case is not a one-off. During his PhD at the University of Oxford, UK, epidemiologist An-Wen Chan looked at the protocols for 122 trials registered with two Danish ethics committees in 1994\u201395. More than half of the outcomes that the protocol said would be measured were missing from the published paper, he found 3 . Asked about the missing outcomes, most authors simply denied the data were ever recorded, despite evidence to the contrary. And when Chan looked at those missing data, he found that inconclusive results were significantly more likely to have been left out of the final publication. Medical journals can, however, request such missing data. One idea, adopted by  The Lancet  three years ago, is to insist that authors send in a trial protocol when they submit results. That should help identify whether researchers are reporting all the information they gathered. But even with all the data, journal editors face another challenge: hype. Researchers and sponsors tend to be interested in things that work rather than those that do not, so authors may subconsciously tweak results and talk up conclusions. \u201cResearchers are so worried about getting papers rejected that they put a lot of spin on results to make them seem as exciting as possible,\u201d says Doug Altman, a medical statistician who supervised Chan at Oxford. The hype shows up in a paper's conclusions. In 2003, epidemiologist Bodil Als-Nielsen and her colleagues at the University of Copenhagen looked at factors that might influence researchers' conclusions about a drug's efficacy or safety 4 . Their analysis of 370 trials showed that the strongest predictor of the authors' conclusions was not the nature of the data, but the type of sponsor. Trials funded by for-profit organizations were significantly more likely to reach a favourable verdict than those sponsored by charities or governments. Critically, the association was not explained by the papers having more positive results. In a study under review, G\u00f8tzsche and his colleagues show that industry-funded meta-analyses \u2014 studies that combine results from several clinical trials of a drug \u2014 are similarly prone to draw positive conclusions that are not supported by the data (see graph). For many clinical-trials experts, these funding biases explain all the others. For each act, be it the suppression of results or the omission of outcomes, there is a financial motive for the company whose drug is being tested. In many cases, the company funding the study also employs one or more of the authors. Given the combination of motive and opportunity, many see drug-company influence as an inevitably distorting factor. \u201cWhen we see an industry article we get our antennae up,\u201d says Steven Goodman, a medical statistician at Johns Hopkins University in Baltimore and an editor at the  Annals of Internal Medicine . \u201cIt's not that we assume the research is done badly. But we have to assume that the company has done all it can to make its product look as good as possible.\u201d \n               Editorial control \n             At  JAMA , editors began insisting last year that all research sponsored by for-profit organizations undergo independent statistical analysis before acceptance. Cathy DeAngelis, the journal's editor-in-chief, says  JAMA  had asked authors to do this for years, but began requiring it after editors started seeing papers that they thought dishonest. \u201cPeople said that for-profit companies would stop sending us trials,\u201d she notes. \u201cWell, guess what? If you look at what we're publishing you'll see that that's not true.\u201d Still, Goodman and others caution against blaming everything on industry. Government-sponsored trials also tend to report positive outcomes 5 , although the effect is weaker than with industry studies. And a publication in a big journal can boost authors' careers as well as company coffers. Others add that journals must also share the blame. Good peer reviewers and hands-on editors should, for example, weed out hype. But according to Richard Smith, a former editor of the  BMJ  (which was the  British Medical Journal ) and now head of European operations for the US insurer UnitedHealthcare, editors may be biased towards positive results. In an article published last May, titled \u201cMedical journals are an extension of the marketing arm of pharmaceutical companies\u201d, he pointed out that reprints of papers reporting positive results can generate millions of dollars, and that this might influence editorial decisions 6 . There is certainly evidence that drug companies attempt to use reprint income as a lever on journals.  The Lancet 's Richard Horton has said that authors sometimes contact him to say that sponsors are likely to buy large numbers of reprints if their study is published. Horton and other editors at top journals say they rebuff such threats, but some less well staffed journals lack policies for separating commercial and editorial decisions, suggesting that reprint income at least has the potential to distort decisions. Merrill Goozner, who tracks pharmaceutical issues at the Center for Science in the Public Interest, a lobby group in Washington DC, agrees. \u201cIt's a financial conflict of interest, plain and simple,\u201d he says. \n               Full disclosure \n             Such accusations make medical editors angry. They deny that commercial pressures influence peer review, adding that journals have introduced several measures that have helped to clean up clinical-trial reporting. One of the first initiatives, introduced in 1996 and revised in 2001, is the statement on Consolidated Standards of Reporting Trials (CONSORT). A set of guidelines on how to report a clinical trial, the statement is designed to ensure that authors present results transparently. It seems to be helping. Since top journals began insisting that authors follow the guidelines, researchers' descriptions of their methods, for example how they place subjects in treatment or control groups, have become more accurate 7 . Such information aids reviewers' decisions. Journals have also endorsed trial registries. By registering all trials when they begin, researchers will find it harder to suppress outcomes, editors believe. Several registries already exist, including one run by the US government. The World Health Organization (WHO) is working on an online portal that would bind these databases into a single source. And in 2004, the International Committee of Medical Journal Editors announced its members would not publish the results of trials that had not been placed in a public registry. Clinical-trials experts welcomed the move, but the industry response was patchy. Last June, the committee was forced to issue another statement after finding that some sponsors were being deliberately vague and entering terms such as \u2018investigational drug\u2019 in the field for the drug name. A follow-up study found the quality of information had improved considerably by last October 8 . Despite these successes, advocates of reform say bigger fights lie ahead. The experts working on the WHO registry want a list of mandatory entries for trial data, including the primary outcome. For drug companies this is a step too far, akin to asking an inventor to publish the description of an invention before it is patented. Instead, the companies propose depositing such information in a locked database, to be released when the drug obtains marketing approval. Going public too early, they say, would deter companies from taking risks on potential treatments and slow down the generation of new drugs. Yet for critics such as Smith, even the WHO portal does not go far enough. Along with other registry advocates, he would like to see all clinical results, not just protocols and outcomes, published in public databases. This proposal would seem to tackle problems with reporting data and bias. But it would not be simple. Aside from the commercial concerns of the drugs industry, the creation of a results database could lead to patients pressurizing doctors for access to experimental medicines. Health insurers and hospitals might also change the drugs they use after seeing the raw results, rather than waiting for peer-reviewed papers. The debate is in its infancy. Yet clinical-trials experts are more optimistic than they have been at many points in the past 15 years. Chan, now working for the WHO registry team in Geneva, says the first round of consultations with stakeholders should produce a policy statement in April. He and others add that although industry will probably continue to resist, the public attention generated by recent scandals, and the wealth of data available on the problems, mean that time is ripe for change. \u201cI'm not relying on hope,\u201d says G\u00f8tzsche. \u201cBut the results of all trials should be made public, not only those that the sponsor cares to tell the world about. This is incredibly important.\u201d \n                     Industry money skews drug overviews \n                   \n                     British drug company to put data online as criticism mounts \n                   \n                     Journal grows suspicious of Vioxx data \n                   \n                     Journals submit to scrutiny of their peer-review process \n                   \n                     Scientific publishing: Peer review, unmasked \n                   \n                     US government clinical trials database \n                   \n                     WHO clinical trials portal \n                   Reprints and Permissions"},
{"file_id": "440600a", "url": "https://www.nature.com/articles/440600a", "year": 2006, "authors": [{"name": "Nick Lane"}], "parsed_as_year": "2006_or_before", "body": "Many of the genes affecting mitochondria \u2014 tiny energy suppliers of cells \u2014 reside in the cell nucleus. Nick Lane joins the hunt for these sequences that may underpin diseases such as diabetes. Some of Gerald Shulman's patients at Yale University School of Medicine are young and slim. There's little wrong with them, and probably won't be for a decade or two. Yet tests raise an ominous spectre. All are the children of parents with type 2 diabetes, and, already, in their twenties, they are becoming resistant to insulin, the hormone that should be keeping their blood sugar levels under control. The problem seems to lie in their muscles, whose cells lack tiny lozenge-shaped structures called mitochondria. These normally function as powerhouses inside cells, burning up fuel with oxygen. Long regarded as the cell's menial coal-shovellers, mitochondria are emerging as key players in health and disease. The \u2018organelles\u2019 are unusual in having their own DNA, although many of the genes that once resided in the mitochondria have, over evolutionary time, decamped to the cell's nucleus. Shulman is one of a number of scientists who think that tracking down the hundreds of \u2018missing\u2019 genes that have shifted to the nucleus is going to change the way we think about common diseases such as diabetes and Parkinson's. Mitochondria store the energy released from food in the form of a molecule called ATP, which is used to power virtually all forms of work in the body, from muscle contraction to protein synthesis. Your body's mitochondria generate an impressive total of some 65 kg of ATP every day. The double-membraned organelles (see picture below) perform this feat thanks to a process called chemiosmosis, which pumps protons across one of their membranes. ATP is generated when the current of electrically charged protons, produced by this pump, passes through tiny protein motors embedded in the same membrane. \n               Ancient union \n             As well as looking like them and using chemiosmosis in the same way as bacteria, mitochondria contain a bacteria-like genome. Indeed, mitochondria were once free-living bacteria; they were engulfed by larger cells two billion years ago in a unique merger that gave rise to all complex, or eukaryotic, cells. The size of the genome housed within the mitochondrion varies between species. All mammals, for example, have retained just 37 genes, whereas yeasts have retained between 40 and 50, and some plants as many as 100. But mitochondrial genomes did not start out so small \u2014 they probably once contained at least a few thousand genes, inherited from the free-living ancestor of mitochondria 1 . Exactly what happened to most of these genes is a moot point, but the evolution of a stable symbiotic relationship within eukaryotic cells led to hundreds, perhaps even thousands, being simply transferred to the cell's main genome in its nucleus. These transfers meant that mitochondria became dependent on the host cell for virtually all their functions. Today, some 99% of human mitochondrial proteins are encoded in the nucleus; all the proteins and other molecules required to build mitochondria are synthesized in the main body of the cell, then imported into the organelle. Only a fraction of these genes has been identified; the rest lie hidden in the vast code of the nucleus's genome. This enigmatic 99% is now the focus of intense scrutiny. There are good reasons to believe that genes affecting the mitochondria could play a central role in human health and disease. Most of the genes that have remained in the mitochondrion have been linked to a series of devastating diseases, indicating the importance of fully functional mitochondria to human health. Genes residing in the mitochondria pose a particular problem, however \u2014 in part because they are unusually prone to damage. Unlike nuclear genes, which are wrapped in protective proteins and stored safely away in the nucleus, mitochondrial genes are vulnerable to attack from highly reactive molecules called free radicals; these are generated during energy production. In mammals, the mutation rate of mitochondrial genes is 10 to 20 times higher than that of the nuclear genes. The idea that mutations in mitochondrial DNA could cause metabolic diseases, or even ageing, has gained credence since Fred Sanger's group at the University of Cambridge, UK, sequenced the human mitochondrial genome 2  in 1981. According to David Thorburn, at the Murdoch Children's Research Institute in Melbourne, Australia, in the decades since, pathogenic mutations have been discovered in more than 30 of the 37 human mitochondrial genes. These alterations range from changes to single DNA bases to deletions of large sections of the genome. Their effects are a long list of rare disorders, best diagnosed and treated by specialists, who refer to themselves as mitochondriacs. The most common childhood condition is Leigh syndrome. This affects about 1 in 40,000 children and tends to develop within the first year of life, often after a viral infection. In most cases, degeneration of the central nervous system leads to loss of muscular coordination and death within a few years, although some children survive into their teens. Lethal infantile mitochondrial disease is much rarer but even more deadly. Children born after an uneventful pregnancy tend to have seizures soon after birth, make few or no spontaneous movements, and die of respiratory failure within weeks. Other conditions have relatively mild symptoms. A common feature of all these diseases is that they tend to worsen with age. Indeed, it is the cumulative effects of free-radical attacks, and the corresponding build up of mitochondrial mutations that may underpin aging. \n               Faulty engine \n             Mitochondria, along with their tiny genomes, are normally inherited only from the mother \u2014 they are present in huge numbers in the egg, whereas the handful in sperm is marked up for destruction in the fertilized egg. This gives at least some mitochondrial diseases a maternal-inheritance pattern. Even so, trying to spot mitochondrial diseases by looking to the mother can be grossly misleading, and has downplayed the importance of these organelles in disease. More than 80% of diseases known to be linked to faulty mitochondria don't follow a maternal-inheritance pattern at all. Why not? At least partly because some mitochondrial diseases may be caused by mutations in the nuclear genes encoding mitochondrial proteins. So far, mutations in more than 30 nuclear genes have been shown to give rise to mitochondrial disease. Thorburn, however, estimates that as much as a tenth of the population may be carrying genetic disorders that could affect mitochondrial function 3 . This is based on estimates of the number of mitochondrial genes in the nuclear genome and the incidence of recessive genetic disorders. He echoes a favourite catchphrase of mitochondriacs: \u201cMitochondrial deficiency can theoretically give rise to any symptom, in any organ or tissue, at any age, and with any mode of inheritance.\u201d The actual contribution of nuclear genes to mitochondrial diseases is highly uncertain for a simple reason \u2014 we are surprisingly ignorant of what the nuclear genes actually are, and how they interact with mitochondrial genes. In mammalian mitochondria, the best guess is that the nuclear genome encodes 1,500 distinct mitochondrial proteins. So far, barely half have been formally identified, and of these, the function of a sizeable proportion remains unknown. Nonetheless, the evidence that mitochondrial proteins are responsible for a lot more mischief than once thought is growing. A series of inherited conditions not thought of as \u2018mitochondrial\u2019 have turned out to be caused by mutations in genes encoding mitochondrial proteins 4 . For instance, Friedreich's ataxia (a progressive loss of coordination of voluntary movements) is caused by mutations in a gene encoding a small mitochondrial protein called frataxin. Hereditary spastic paraplegia (a progressive weakness and stiffness of the legs) can be caused by mutations in a mitochondrial enzyme, paraplegin. Other, more complex degenerative conditions, such as Parkinson's disease, progressive-blindness diseases and other nervous-system conditions also involve mutations in mitochondrial proteins 4 . Even cancer can be caused by mutations in nuclear genes encoding mitochondrial proteins 5 . Examples are now cropping up almost every year, and together they are beginning to focus attention on the central role of mitochondria in disease. These examples have all unexpectedly turned out to be \u2018mitochondrial\u2019, after years of tracking down candidate genes for the diseases. But new tools are letting scientists turn the old approach on its head. Rather than starting with an inherited condition and trying to track down the genes responsible, researchers are starting off with the mitochondria themselves, and attempting to hunt down the proteins needed to build them. Tracking down this array of proteins, or the mitochondrial \u2018proteome\u2019 is no easy task; researchers rely on a combination of methods to build an accurate picture, including mass spectrometry to identify proteins and molecular-biology techniques to measure RNA, the molecule used by cells as a template from which to build proteins. All the techniques based on this bottom-up approach have strengths and weaknesses, but by taking the best information from each, scientists are gradually piecing the mitochondrial proteome together. Once the normal proteins have been identified, any oddities in patients can be pinpointed. The abnormal protein can be mapped on to the candidate genes for disease, and any causal mutations involved identified. In 2003, Vamsi Mootha, a computational biologist at the Broad Institute in Cambridge, Massachusetts, and his colleagues published a list of several hundred new mammalian mitochondrial proteins 6 , raising the known mammalian total to around 600. Crucially, however, Mootha's group also examined tissue variations. In mice, they found that around half the mitochondrial proteins identified were present in four different tissues \u2014 brain, heart, liver and kidney. But the other half tended to be tissue-specific, with some degree of overlap (around 50%) between different tissues. \n               Building a powerhouse \n             Mitochondria are well known to carry out specific tasks in different tissues; for example, they make haem, part of the oxygen-carrying protein haemoglobin, in bone marrow cells. But the finding that hundreds of mitochondrial proteins varied in amounts from tissue to tissue came as a shock. If corroborated, this variation suggests that the control of mitochondrial gene activity is very sophisticated. And this has a corresponding impact on our susceptibility to disease; the more complicated the control system, the more likely it is to fail. Mootha's group reported the first two tissue-specific mitochondrial proteins, known as Err\u03b1 and Gabpa/b, in 2004 (ref.  7 ). Both control gene activity, which in turn affects how much mitochondria replicate themselves in particular tissues. If the expression of Err\u03b1 and Gabpa/b is high, then mitochondria replicate at a high rate, and become densely packed in the tissue. If their expression is lower, the number of mitochondria and their ability to burn fuel falls. Critically, Err\u03b1 and Gabpa/b influence mitochondrial function and density in particular tissues, notably the heart and muscle, and play a lesser role in tissues such as the liver. Mootha notes that this tissue specificity makes them valuable drug targets, because it restricts the potential for side effects in other tissues. The next question for Mootha and his team was what happens if the activity of Err\u03b1 and Gabpa/b falls? They predicted that a fall in the number and capabilities of mitochondria in particular tissues would result\u2014 a finding that Mootha and others had previously reported in the muscles of patients with diabetes. Sure enough, Mootha's lab found that the activity of these proteins was lower in the muscles of patients with type 2 diabetes 8 . But could such a change be a root cause of diabetes, or was this merely a consequence of some other metabolic problem, such as obesity? Type 2 diabetes has two cardinal features. The first is that cells become resistant to the effects of insulin, the hormone made by the pancreas that normally prompts them to take up and burn glucose. The second is high levels of glucose in the blood, or hyperglycaemia. Insulin resistance is typically one of the earliest signs of diabetes, often preceding hyperglycaemia by decades. Faulty mitochondria have already been linked to the second phase of the disease \u2014 namely the emergence of hyperglycaemia. Defective mitochondria in the pancreas fail to burn sufficient glucose, so the levels of ATP in pancreatic cells are abnormally low. But these cells rely on ATP levels to help them estimate the amount of glucose in the blood. As a result, the cells do not sense glucose properly, do not release appropriate amounts of insulin and the blood glucose level creeps up 9 . But what about insulin resistance? Shulman thinks that faulty muscle mitochondria could underlie insulin resistance in muscle tissue and was intrigued by Mootha's findings. \u201cWe've been working with volunteers who have a high genetic risk but a low \u2018lifestyle\u2019 risk of diabetes. We hope to eliminate confounding factors such as obesity, or indeed the early stages of diabetes itself, and focus on the earliest underlying genetic influences.\u201d \n               Complex pathways \n             Shulman's group has found three striking oddities in the muscle cells of the young volunteers: they are often very insulin resistant, taking up about 60% less glucose in response to insulin compared with the muscle cells of unaffected people; they have a low mitochondrial density, about 40% lower than normal; and they have a large accumulation of fat molecules, or lipids, around 60% above normal 10 . The key, says Shulman, is the high level of lipids. Lipids can cause insulin resistance by jamming the cellular machinery that helps receive the hormone's signal. But what causes their levels to rise in the cell? There are two main possibilities: a faster rate of lipid breakdown and delivery to muscles from fat tissues; or a defect in the muscle mitochondria themselves. If faulty mitochondria don't burn fats as fast as they should, then that could lead to a build-up of lipids inside the muscle cells. That would suggest the primary genetic cause of type 2 diabetes lies in the mitochondria. Faulty mitochondria also contribute to obesity, by not burning fats properly, and obesity in itself exacerbates diabetes. Shulman's group could find no evidence that abnormal fat breakdown and delivery from fat tissues was responsible, and so turned to look at possible faults in mitochondria. Following up on Mootha's findings, the team looked as whether a mutation in the genes controlling the tissue-specific mitochondrial proteins Err\u03b1 and Gabpa/b could underpin the low density of mitochondria in the volunteers. The result, published in December last year, was a surprise. They could find no such mutations, implying that the reduction in gene expression measured by Mootha was not the primary cause of diabetes. The primary fault must lie in another, as yet unknown pathway governing mitochondrial proliferation and activity. So faulty mitochondria may well be the cause of diabetes, but we still don't know what makes them faulty. Yet with hundreds of unknown mitochondrial proteins still to uncover, Shulman and Mootha have a long list of possible suspects to work through. Whether they will get results in time to help Shulman's young volunteers is an open question, but the answers seem set to revolutionize our understanding of disease. \n                     Inbred Israeli families aid research on rare diseases \n                   \n                     Gene study raises fears for three-parent babies \n                   \n                     News Feature: Deity of disease \n                   \n                     Broad Institute \n                   Reprints and Permissions"},
{"file_id": "439648a", "url": "https://www.nature.com/articles/439648a", "year": 2006, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "Fractal analysis has been used to assess the authenticity of paintings purporting to be the work of Jackson Pollock. Alison Abbott reports. Jackson Pollock, famed for his \u2018poured\u2019 paintings, was defiant in facing down the cynics who viewed them as random splatterings. \u201cI can control the flow of paint; there is no accident.\u201d And several decades after the abstract expressionist's death, science proved him right. In the late 1990s, physicist Richard Taylor analysed a selection of Pollock's poured paintings and found they were composed of distinct fractal patterns \u2014 made by dripping or pouring paint straight on to a canvas. Indeed, it seems that \u2018Jack the Dripper\u2019 was refining the fractal characteristics of his paintings long before the mathematics to analyse them was invented. Now, Taylor's evidence may prove critical in determining the authenticity of a group of recently discovered paintings that could be Pollocks. Given the financial implications, the physicist admits that he had to steel his nerves when writing his report. \u201cThis is a high-stakes game,\u201d says Taylor. \u201cA Pollock poured painting can be sold for millions of dollars.\u201d In 1998, for instance,  Blue Poles: Number 11, 1952  was valued at US$40 million. So Taylor knew that a negative result \u2014 if added to the doubts of Pollock experts \u2014 could strip many zeros from the value of the haul. When the discovery of 32 possible Pollocks was made public in May last year it caused an immediate sensation. Pollock, an alcoholic who had a chaotic lifestyle and eventually died in a car crash, bartered several of his works for groceries. So it is likely that some of his paintings remain to be discovered. But although many claims have been made, only a small number of major poured paintings were formally authenticated before 1995 \u2014 when the Pollock\u2013Krasner Foundation, set up under the will of Pollock's widow, Lee Krasner, disbanded its authentication board. The provenance of the 32 paintings seems convincing. Alex Matter, son of the photographer Herbert Matter and painter Mercedes Matter, who were close friends of Pollock, found the works among a jumble of his parents' belongings. Labels in his father's handwriting identified them as paintings done by Pollock in the 1940s that he had acquired as \u2018gift + purchase\u2019. Alex Matter showed the works to the art dealer Mark Borghi, who in turn contacted Ellen Landau, a Pollock expert who had served on the Pollock\u2013Krasner authentication board. Landau is now involved in preparing an exhibition called Pollock Matters 2006, which is being organized by Borghi and Matter to celebrate the 50th anniversary of Pollock's death. The new poured paintings will play a key role in the show, and Landau will outline their significance in Pollock's career. But other art historians have disagreed \u2014 some angrily \u2014 with the idea that the pictures are Pollocks. The doubters include Francis O'Connor, co-author of the definitive Pollock catalogue and another member of the authentication board, while it existed. Given the high-profile dispute, and the large numbers of paintings involved \u2014 Taylor says they could represent up to a tenth of the significant poured paintings Pollock is known to have produced \u2014 the Pollock\u2013Krasner Foundation decided it needed to get involved. It also decided that it required a more objective approach to authentication than the conflicting opinions of art historians could provide, especially if its judgement came to be challenged in the courts. Traditionally, the authentication of a painting relies heavily on experts' visual assessments, supported by analysis of materials used in the work and knowledge of where it came from. \u201cArt experts find it very stressful to make judgements based on visualization alone,\u201d says Taylor. \u201cThey feel a bit let off the hook when materials or provenance can help.\u201d But analysis of materials is of limited help in identifying true Pollocks, as the painter used common, off-the-shelf paints. And after  Life  magazine published a feature on Pollock and his sensational new approach to art in 1949, many readers tried their hand at his abstract expressionist style. Although the provenance of the 32 paintings looks compelling, sceptics argue that the works could have been painted by Mercedes Matter, imitating Pollock's style. The fact that the poured paintings are on the type of board that Matter typically used, rather than Pollock's usual canvas, supports this argument, they say. The counter position is that Pollock probably tried his method on Matter's boards because the two were so close. So the foundation discreetly approached Taylor to act as a more objective arbiter, sending him six of the paintings to analyse. \u201cFrom my point of view it was a good opportunity, as I was able to apply my research in the field,\u201d says Taylor. Back in the late 1990s, Taylor, who has a degree in art theory as well as physics, decided to pursue his suspicion that Pollock's pouring technique could be described using fractal geometry. Fractal patterns, which repeat themselves at different magnifications, are often associated with chaotic systems. During the 1970s, mathematicians used chaos theory to reveal fractal patterns in natural objects such as coastlines, trees and flames. There were two reasons to suspect that Pollock's paintings might obey fractal geometry. Moving around a large canvas laid on the ground, the artist let paint fly from all angles, using his whole body. Human motion is known to display fractal properties when people restore their balance, says Taylor, and films of Pollock seem to show him painting in a state of \u2018controlled off-balance\u2019. Second, the dripping and pouring itself could be a chaotic process. While continuing his research on nanoelectronic devices (which display fractal patterns in their electrical properties), Taylor set about looking for fractals in five Pollock poured paintings in his spare time. He placed computer-generated grids over photographs of the works, and found two distinct sets of fractal patterns. One was on a scale larger than 5 cm; the other showed up on scales between 1 mm and 5 cm (R. P. Taylor, A. P. Micolich and D. Jonas\u00a0 Nature   399 , 422; 1999). \u201cPollock was in control,\u201d says Taylor. The large-scale fractals are a fingerprint of the artist's body motion, he notes. \u201cBut the small-scale fractals are also to do with his choices \u2014 his height over the canvas, the fluidity of his paint, angle and force behind the trajectory, and so on.\u201d Taylor also found that the fractal dimension of Pollock's works \u2014 a value that describes the complexity of a fractal pattern \u2014 increased through the years as the artist refined his technique. It seems that Pollock was honing his ability to generate fractals a full quarter century before fractal geometry was formally described. When he moved to the University of Oregon in Eugene in 2000, Taylor began a more comprehensive analysis to determine whether these fractal patterns were unique to Pollock. He used every last bit of information about the artist he could find, studying movements in a 1950 film of Pollock at work, and even the splatters of paint that had missed the targeted canvas and landed on the floor, as recorded in old photographs. In total, he analysed 14 Pollock paintings, 37 imitations created by students at the University of Oregon and 46 poured paintings of unknown origin. The genuine Pollocks had been painted in several ways \u2014 sometimes the artist flicked paint from a brush or stick; on other occasions he let the paint run down a brush or stick to fall on the canvas; sometimes he poured directly from the paint tin, or punctured holes in tubes of paints and squeezed them directly on to the canvas. Yet all had the same group of fractal characteristics. \u201cThe only shared thing in Pollock's very different poured paintings is a fractal composition that was systematic through the years,\u201d says Taylor. Although the other poured paintings did include fractal patterns, none of them shared this particular group of fractal characteristics \u2014 and neither did Pollock's accidental floor splatterings. Dan Rockmore, a mathematician from Dartmouth College in Hanover, New Hampshire, has also searched for statistical signatures in art, in the works of painters such as Pieter Bruegel the Elder. He describes Taylor's findings as \u201cextraordinarily clever\u201d. The six paintings from the foundation arrived a year after Taylor submitted a paper describing these results to  Pattern Recognition Letters , which has since been accepted for publication. Applying the same statistical techniques to these works, he found that none of them obeyed the fractal geometry he had observed in Pollock's work. \u201cI found significant deviations from Pollock's characteristics,\u201d says Taylor. \u201cTaken in isolation, these results are not intended to be a technique for attributing a poured painting to Jackson Pollock,\u201d he wrote in a report to the Pollock\u2013Krasner Foundation last July. \u201cHowever, the results may be useful when coupled with other important information such as provenance, connoisseurship and materials analysis.\u201d There were months of silence after Taylor delivered his confidential report. Given the high stakes for the art world, the foundation wanted to continue research on the provenance of each of the 32 paintings before giving a definitive thumbs-up or thumbs-down. Inevitably, rumours began to circulate \u2014 in the past few weeks, they have prompted Borghi himself to request Taylor to analyse a selection of the paintings. Borghi says he sees a lot of merit in what Taylor does, although he doesn't believe authentication should rely on fractal analysis alone as painters often paint things out of style. The foundation has now decided to go public with the results of Taylor's study, while withholding a final, formal judgement. O'Connor, of the authentication board, pronounced his satisfaction that the findings \u201creinforce my own scepticism and reservations\u201d. The foundation is choosing to pursue a consensus among Pollock experts; a draft statement seen by  Nature  calls Taylor's results \u201ca valuable contribution to our investigation\u201d. The results may be enough to cast doubt on the value of Matter's finds, at least until there is a final ruling from the foundation. Confidence in pattern analysis in art authentication is on the increase \u2014 which is partly why the foundation commissioned Taylor in the first place. And in the world of finance, whether it's coffee, gold, or artworks, it is confidence that drives market prices. \n                     Computers confront the art experts \n                   \n                     Art conservation: Biology for art's sake \n                   \n                     Chemistry and art: St Luke's new coat \n                   \n                     Nature supplement, Artists on science: scientists on art \n                   \n                     Pollock Matters 2006 \n                   \n                     Pollock-Krasner Foundation \n                   \n                     Richard Taylor on 'Fractal Expressionism' \n                   Reprints and Permissions"},
{"file_id": "439652a", "url": "https://www.nature.com/articles/439652a", "year": 2006, "authors": [{"name": "Carina Dennis"}], "parsed_as_year": "2006_or_before", "body": "Less than a month ago, investigators at Seoul National University in South Korea announced that cloning researcher Woo Suk Hwang had lied when he claimed his team cloned human embryos with relative ease and produced stem cells from them. The news was a significant setback for cloning researchers. In this special section,  Nature  looks at how biologists are regrouping. Carina Dennis asks how they can get cloning to work given a very limited supply of eggs and Phyllida Brown looks at whether we will need therapeutic cloning at all, if immunologists can stop our bodies fighting transplants (see  page 655 ). And on  page 658 , one of Hwang's closest rivals admits it may not continue its cloning quest. Women occasionally offer Alan Trounson their eggs. They approach the stem-cell researcher from Monash University in Melbourne, Australia, after he gives talks to patient focus groups. Trounson wants to treat neurodegenerative disease by using eggs to create cells that match a patient's genetic make-up \u2014 a technique known as therapeutic cloning. \u201cThe technique is not legal in Australia, so it's a fairly brief conversation,\u201d he says. The discredited researcher Woo Suk Hwang owed his preeminence in cloning circles to his claims to have produced such patient-specific stem cells in an almost routine way. Now those claims have been exploded, researchers with aims like Trounson's are returning to the drawing board to see whether anyone can make patient-matched cells at all. The ultimate dream is to create specialized types of cell \u2014 such as insulin-producing cells or heart cells \u2014 to treat diseases such as diabetes or to repair damaged hearts or other organs. In the nearer future, scientists also hope to recreate embryonic cells from patients with diseases such as neurodegenerative conditions, to study an illness as it unfolds and to test new drugs. In regions where therapeutic cloning is permitted, a growing number of scientists have been licensed to start experiments. And even countries where the method is not currently allowed, such as Australia, are reviewing their laws. But human eggs are needed to make these cells, and a shortage of them could hold back the entire field. So researchers are investigating alternatives such as nurturing immature eggs, growing artificial eggs in the lab and using animal egg substitutes. Each strategy comes laden with its own technical \u2014 and ethical \u2014 challenges. To make therapeutic tissues such as heart cells, many researchers start with unspecialized, immature cells called embryonic stem cells. As their name suggests, such cells come from young human embryos, termed blastocysts, that are only a few days old. At the moment, researchers work on stem cells taken from surplus embryos created by clinics doing  in vitro  fertilization (IVF). But if these cells were simply transplanted into patients, the immune system would recognize them as foreign tissue and reject them (see \u2018Do we even need eggs?\u2019 on  page 655 ). Therapeutic cloning could, in theory, solve this problem. Working in animals, researchers have shown that if they transplant a nucleus from an adult body cell into an egg that has had its nucleus removed, the egg somehow \u2018reprogrammes\u2019 the adult nucleus back to an immature state, where it directs the development of an embryo. The resulting embryo is a genetic clone of the adult from whom the nucleus was taken. If this procedure works in humans, researchers could use cloned embryos to produce therapeutic or research cells that are essentially identical genetic copies of a patient's cells. Such cells should not be attacked by the patient's immune system. \n                 In excess \n               But cloning is a wildly inefficient process, often requiring hundreds of eggs to produce a single viable clone. Indeed, one shocking revelation of the Hwang affair, was the sheer number of eggs his lab had got through 1 . And obtaining human eggs is not easy. Donation is an unpleasant, invasive process that carries a small risk to a woman's fertility and can, in rare cases, cause life-threatening side effects. This may make it hard to recruit donors. \u201cWe'll have to wait and see how difficult human eggs are to acquire,\u201d says Arnold Kriegstein, director of the Institute of Stem Cell and Tissue Biology at the University of California, San Francisco. Most eggs currently donated to research are leftovers from IVF treatments \u2014 the ones that fail to fertilize and would otherwise be discarded. But these eggs typically fail to reprogramme 2 , \u201cprobably for the same reasons they failed to fertilize,\u201d says Alison Murdoch, of the University of Newcastle Upon Tyne, UK. Murdoch and her team have successfully cloned a single blastocyst using excess eggs from women having infertility treatment 3 . Ideally, researchers want healthy, competent eggs. Murdoch now asks women undergoing IVF treatment who produce plenty of eggs \u2014 more than 12 in a treatment \u2014 whether they would be willing to donate two eggs after the first dozen. \u201cWe have calculated that this does not significantly reduce their chances of a pregnancy,\u201d says Murdoch. \n                 Greater good \n               Some researchers expect altruistic donations will be sufficient for research purposes. \u201cMy view is that most eggs are likely to come from women who have family members with a disease and want to donate their eggs to advance research on that disease,\u201d says Trounson. But obtaining eggs for clinical use is likely to be a major obstacle, at least in the foreseeable future. \u201cI can't conceive there will be enough eggs to use on a wide scale. In the end, we have no choice but to develop other methods,\u201d says Robert Lanza of Advanced Cell Technology. His company, based in Worcester, Massachusetts, has done therapeutic-cloning research using eggs from altruistic donors 4  (see  page 658 ). And egg donations \u2014 especially those given altruistically \u2014 create an ethical quagmire. Is it appropriate to put healthy fertile women through such a procedure? Should they be paid for their eggs? These issues divide scientists. \u201cUntil we can get the efficiency to a reasonable level, we shouldn't be working in human eggs,\u201d says Stephen Minger, a stem-cell researcher at the Wolfson Centre for Age-Related Diseases in London, UK. The most obvious place to start looking for alternatives to conventional donation is to go direct to the ovary. Although women only ovulate around 500 eggs in a lifetime, their ovaries are packed with thousands of eggs at varying stages of development. What if researchers could somehow get hold of these \u2014 from ovarian biopsies, say \u2014 and grow them to maturity in the lab? Biologists are making some headway in culturing eggs that are in the last stages of development. Outi Hovatta from the Karolinska Institute in Stockholm, Sweden, is working on eggs that are on the verge of being ovulated. Researchers are able to coax such eggs, which are collected alongside mature eggs in normal IVF procedures, through the final stages of readiness for fertilization. Hovatta is optimistic that they could work in cloning experiments, which she is about to start. She estimates that altruistic donations of these almost-mature eggs would yield about 300 a year from a collaborating IVF clinic. But culturing really immature eggs has proved extremely difficult. Egg development in humans is long, extraordinarily complex and not well understood. It begins in the embryo when special embryonic cells make their way to the developing ovary (see graphic). Here, they divide many times to generate millions of egg precursors, called primary oocytes. At birth, the ovary contains about half a million follicles: these consist of a primary oocyte wrapped in one or more layers of cells that support the oocyte as it grows and accumulates nutrients needed for the early development of an embryo. Only after puberty do follicles fully develop, with one follicle growing to full size per menstrual cycle and releasing its enclosed oocyte. Just before ovulation, this oocyte ejects half its chromosomes, getting rid of half the remainder when a sperm makes contact with its own genetic cargo. In the hands of John Eppig, a researcher at the Jackson Laboratory in Bar Harbor, Maine, culturing eggs through these stages looks almost easy. He can create live mice pups by fertilizing eggs that have been cultured in the lab from ovaries extracted from newborn mice 5 . Although the first offspring of these experiments \u2014 dubbed Eggbert \u2014 was a sickly creature, subsequent mice look healthy. But it is a different story with larger animals. \u201cIt's tougher in species other than rodents because it takes so much longer for egg development to occur,\u201d says Eppig; it takes more than three months for a human egg to mature. Another factor is the relatively gargantuan size of a human egg, which swells to more than 100 micrometres. The challenge is to ensure that the voluminous egg receives adequate nourishment, as well as figuring out the right factors to coax it along the development pathway. That said, researchers have had some success with human eggs. Ronit Abir from the Rabin Medical Center at Beilinson Hospital, Israel, has nurtured isolated follicles for several weeks  in vitro . Controversially, she has also cultured immature eggs from aborted human fetuses to almost the same stage 6 . Aborted fetuses are not likely to be a good source of eggs, given the obvious ethical concerns, including the fact that a fetus cannot give its consent. But Abir says the work could unravel the mysteries of culturing eggs from early development and reveal ways to restore the fertility of cancer patients who have had their ovaries extracted and frozen. Hovatta's team has been able to grow human primary oocytes in intact ovary slices in culture and has nudged them along several developmental stages. But these efforts to culture primary oocytes have yet to yield eggs that can be fertilized. \u201cIn the beginning, I thought it would change everything, but now I see how slow the rate of progress is,\u201d says Abir. \n                 Make it up \n               Others are going right back to the earliest stages and trying to develop eggs from scratch using embryonic stem cells 7 , 8 . If such cells are left to grow very densely on a culture dish under the right conditions, they clump together and, amazingly, will form egg-like structures.  Researchers have not yet shown that these egg-like cells can be fertilized. But they might be good enough to reprogramme a nucleus, according to Hans Sch\u00f6ler of the Max Planck Institute for Molecular Biomedicine in M\u00fcnster, Germany. Sch\u00f6ler pioneered the growth of egg-like cells from the embryonic stem cells of mice. The research field is anxious to see whether reprogramming will be possible \u2014 Sch\u00f6ler claims to have made one unsuccessful attempt. \u201cWe are still working out the conditions,\u201d he says. \u201cIt's not as trivial as we'd thought.\u201d With human eggs presenting so many difficulties, some researchers are exploring the possibilities of animal eggs, at least for research purposes. \u201cHuman eggs are so precious \u2014 why waste them to practise on?\u201d asks Huizhen Sheng, from the Centre for Developmental Biology at Xinhua Hospital in Shanghai. Sheng's lab sparked an international storm when the media reported in 2002 that she was using rabbit eggs to clone human blastocysts. Some newspapers ran headlines about animal\u2013human monsters, fuelling public hysteria. The debate was reignited recently when Chris Shaw, a neurologist at King's College London and Ian Wilmut, the creator of cloned sheep Dolly, based at the University of Edinburgh, announced that they were seeking approval to do similar experiments. Sheng has published her data 9 , but the research community remains unconvinced that her method works. \u201cYou have to be uncertain of that work until it is repeated,\u201d says Trounson. Sheng attributes certain discrepancies to the lab's culturing methods, a problem that she says has now been rectified.  Creating hybrids of human cells and animal eggs is banned in many countries, under review in others such as Australia, and yet to be tested in the more permissive regulatory environment of Britain. But with scant supplies of fresh human oocytes, many researchers see animal eggs as the only practical alternative for refining therapeutic-cloning techniques. And they could be useful for generating patient-specific lines to study the genetic basis of human diseases. \u201cWe are trying to understand disease processes to identify new therapeutic targets. These cells are not for putting back into people,\u201d says Shaw. \n                 Mismatched \n               Still, many scientists are doubtful that animal eggs will yield useful human embryonic stem-cell lines. The main concern centres on mitochondria, the bacteria-like powerhouses of the cell. Mitochondria have their own genomes, which interact with the genome in the cell's nucleus. Mixing nuclei and mitochondria from different species simply may not work (see  \u2018Spanners in the works\u2019 ).\u201cIt's hard enough to keep our nuclear chromosomes in sync with our own mitochondrial DNA and we're the same species,\u201d says Irving Weissman of the Stanford School of Medicine, California. And yet scientists are undeterred. \u201cIt's an important question and difficult to answer until you have done the experiments,\u201d says Shaw. Given that eggs are so problematic, some teams are attacking the problem of reprogramming from a different angle. They are trying to see whether other kinds of cells share an egg's ability to reprogramme a nucleus. One such candidate is an embryonic stem cell itself. Recently, Harvard University researcher Kevin Eggan and his colleagues transformed adult body cells to an embryonic state by fusing them with embryonic stem cells 10 . And some experiments have even suggested that embryonic stem cells might be better at certain aspects of reprogramming than oocytes. But the major drawback of this method is that the chromosomes of the embryonic stem cell used to spark the process are retained. This limits a cell's therapeutic potential because a patient's immune system could recognize the leftover chromosomes and launch an attack. Researchers are working on fixes, however. For example, Paul Verma from Monash University has devised a way of getting rid of the unwanted chromosomes 11 , and now has unpublished evidence that mouse cells might be reprogrammed using this approach. Others are searching for those seemingly magical factors in eggs that allow them to wind an adult nucleus back to an embryonic form. Nobuaki Kikyo of the Stem Cell Institute at the University of Minnesota in Minneapolis, for example, has fished out factors from frog eggs that can repackage chromosomes, dismantle the nucleus's structure and switch on gene activity \u2014 all key aspects of reprogramming 12 . But this approach will take time. \u201cSomeone might get lucky, but I think it's a long way off,\u201d says Keith Latham from Temple University School of Medicine in Philadelphia, Pennsylvania. There is unlikely to be one single way to mimic the almost mystical reprogramming ability of a human egg. The answer, says Weissman, could be to combine methods \u2014 kickstarting the process with one approach, and finishing it with another. \n                     Verdict: Hwang's human stem cells were all fakes \n                   \n                     Timeline of a controversy \n                   \n                     Stem-cell brothers divide \n                   \n                     Nature  Stem Cells in focus \n                   \n                     Making Stem Cells Web Focus \n                   Reprints and Permissions"},
{"file_id": "439655a", "url": "https://www.nature.com/articles/439655a", "year": 2006, "authors": [{"name": "Phyllida Brown"}], "parsed_as_year": "2006_or_before", "body": "Less than a month ago, investigators at Seoul National University in South Korea announced that cloning researcher Woo Suk Hwang had lied when he claimed his team cloned human embryos with relative ease and produced stem cells from them. The news was a significant setback for cloning researchers. In this special section,  Nature  looks at how biologists are regrouping. Carina Dennis asks how they can get cloning to work given a very limited supply of eggs and Phyllida Brown looks at whether we will need therapeutic cloning at all, if immunologists can stop our bodies fighting transplants (see  page 655 ). And on  page 658 , one of Hwang's closest rivals admits it may not continue its cloning quest. Nobody likes rejection, but for a transplant patient it can be a death sentence. The risk that a patient's immune system will see a transplanted organ, or graft, as \u2018foreign\u2019 rather than \u2018self\u2019, forces transplant patients on to immunosuppressant drugs that can have severe side effects. Therapeutic cloning, its enthusiasts say, could solve the problem by allowing doctors to grow cells and tissues that are perfectly matched to individual patients. In this approach, a patient's DNA is transferred into an egg which is persuaded to develop into stem cells that in turn generate spare-part tissues. But many researchers now think therapeutic cloning is unrealistic, largely owing to the scarcity of human eggs. So the spotlight is turning on to different strategies, aimed at persuading the immune system to tolerate foreign tissue. \u201cThe field is moving very fast,\u201d says Harry Moore of the Centre for Stem Cell Biology at the University of Sheffield, UK. \u201cTen years ahead there may be no need for cloning, except in certain cases.\u201d There are different ways to increase the success of tissue transplants. One is to develop generic stem cells, cell lines and tissues, and then persuade the immune system to accept them. These could be therapeutic transplants of, say, insulin-producing cells to treat diseases such as type 1 diabetes. Those championing this idea admit it is years, or decades, from the clinic. So more pragmatic approaches are also under development. Instead of trying to make the immune system perfectly tolerant of a transplant, some researchers are aiming to increase its tolerance enough for patients to sharply reduce their dependence on powerful immunosuppressive drugs. At the moment, most people who have an organ transplant face a lifetime of treatment with drugs that affect the whole immune system, such as cyclosporine and steroids. Although these drugs increase the life of a grafted organ by several years, they often fail to prevent its eventual rejection, and they put patients at risk of infections, cancers and kidney failure. \n                 Drug problem \n               In studies on small numbers of patients who have had organ transplants, medical teams are discovering that they can manipulate the immune system so that drug treatment can be reduced. For example, Chris Watson and Roy Calne at Addenbrooke's Hospital, Cambridge 1 , are among those who gave patients an antibody, Campath-1 (alemtuzumab), at the time of an organ transplant. This antibody depletes lymphocytes, a large family of key immune cells. Watson and Calne then gave lower doses of immunosuppressive drugs to the patients than would normally be given following a transplant, and no steroids. For the five-year study period, the patients' grafts survived as well as did those in patients given conventional treatment. This treatment approach is not ready for widespread use, says Herman Waldmann of the University of Oxford, UK, who developed Campath-1. But, he adds, it demonstrates the principle that the use of immunosuppressive drugs can be minimized. Although researchers disagree about exactly how the antibody treatment works, some speculate that by holding off an immune attack, it somehow gives the immune system time to \u2018learn\u2019 to tolerate the graft. The aim now is to understand more about how this tolerance develops, so that more precise therapies can be designed. After all, says Waldmann, an antibody that blocks whole classes of lymphocytes is still a relatively crude instrument. The immune system distinguishes between \u2018self\u2019 and \u2018non self\u2019 proteins because of the actions of a subset of lymphocytes called T cells. Each T cell carries a receptor on its surface, uniquely shaped to fit a specific protein fragment or peptide. Other cells of the immune system patrolling the body pick up proteins, or antigens, and present fragments to the T cells. If a fragment fits into a specific T-cell receptor, it acts rather like a key in a lock, and, given certain conditions, will switch the T cell into an active state. The T cell then coordinates an immune response against that protein. Of course, if the T cells became activated in response to self proteins, disaster would ensue. So when new T cells maturing in the thymus (an organ just above the heart) encounter a range of protein fragments, T cells that specifically recognize self antigens are killed off or \u2018deleted\u2019. This editing, known as central tolerance, is what usually protects us against autoimmune disease. The process happens mainly in infancy and youth, with the thymus shrinking as an individual reaches maturity. But while this central tolerance is developing, the body can be fooled into accepting foreign cells as self \u2014 at least in newborn mice 2 . The young animals are given infusions of bone-marrow cells. These cells are the precursors of T cells and dendritic cells, another type of lymphocyte that plays a key role in presenting protein fragments to T cells. When the dendritic cells derived from this foreign bone marrow migrate to the thymus and present their protein fragments to the T cells maturing there, the fragments are seen as self and any T cells that recognize them are killed off. As a result, the animals later accept grafts of skin from the same donor source. \n                 Layers of command \n               But this approach is obviously not suitable for treatment in adult humans, because the thymus is producing so few new T cells. So researchers have tried to manipulate the mature immune system to achieve a sort of artificial central tolerance. In animal experiments, the host's bone marrow is partly inactivated and new donor bone marrow infused in its place. The presence of the foreign bone-marrow cells creates a state known as \u2018mixed chimaerism\u2019 3 , where T cells that react to either host- or donor-derived antigens are killed off, allowing the animal to accept other tissue from the same donor. Megan Sykes, David Sachs and colleagues have tried to harness this phenomenon of chimaerism to persuade the immune system to accept transplants of bone marrow and kidneys in a small number of adult patients. They treated the patients with drugs to deplete the host lymphocytes, then transplanted the new tissues. At first, the patients' blood appeared chimaeric \u2014 that is, the researchers found various types of blood cells in it bearing the donor's signature. If these findings reflect the animal experiments, they would suggest that T cells reactive to the donor tissues had been deleted as though they were targeting self. But there was a puzzle. The grafts survived for years, yet, strangely, the donor blood cells faded away after about 12 weeks, suggesting the patients were no longer chimaeric. This suggested that chimaerism could not fully explain the survival of the grafts, but that something else must be helping to protect them from rejection. Sykes suggests that the kidney grafts themselves must be doing something that helps the host immune system tolerate them. Instead of involving the central deletion of T cells in the thymus, this form of tolerance seems to be happening in the periphery, around the grafted tissue. One possibility, she suggests, is that a specialist set of T cells known as regulatory T cells is involved. Often found lurking in the vicinity of tolerated grafts, regulatory T cells are beginning to attract interest from many immunologists. Waldmann thinks that regulatory T cells may be key players in maintaining the long-term survival of a graft. He hopes that these cells could eventually be harnessed for preventive treatments that would dampen down the immune response to a graft, and has coined the concept \u2018negative vaccination\u2019. \n                 Clever cultures \n               Regulatory T cells seem to control and police the activity of other immune cells in their vicinity. Waldmann's idea that they protect grafts by suppressing aggressive immune responses to the donor antigen depends on the role of dendritic cells. If T cells are the generals commanding the immune army, then dendritic cells are their sentinels and scouts. They patrol sites, such as the skin and mucous membranes, picking up foreign proteins and presenting fragments of them to T cells. The dendritic cells can also act as arbitrators on the fate of these proteins, influencing whether the T cells attack or tolerate them 4 . If the dendritic cells encounter the foreign tissue in the presence of \u2018danger signals\u2019 \u2014 other molecules that trigger inflammation such as bacterial sugars or self proteins that are only released around wounds \u2014 they rapidly switch to a mature form. This form of dendritic cell produces signals that mobilize the T cells to attack. But in the absence of this signal, dendritic cells remain in an immature form. They still present the foreign protein to T cells, but will usually \u2018decommission\u2019 the T cells that specifically recognize the antigen, preventing them from responding aggressively to it. Waldmann holds that the interaction between dendritic cells and T cells in the vicinity of the transplant may be crucial to allowing the graft to be tolerated. At the time of transplantation, the tissue is inflamed through local trauma, but lymphocyte-depleting drugs stop an acute attack on it by the host immune system. This may create a window of time, argues Waldmann, during which the tissue can heal. Once it has healed, and provided there are no infections, then dendritic cells presenting foreign antigens to T cells around the graft will do so without a danger signal. As a result, the dendritic cells will not activate the T cells they encounter. Instead they will either \u2018decommission\u2019 T cells altogether, or push them to become regulatory T cells, in which they police other T cells and suppress any attackers. Together with Paul Fairchild and others, Waldmann is trying to exploit dendritic cells as a tool to develop tolerance to stem-cell transplants. Five years ago, Fairchild and his colleagues worked out how to make mouse embryonic stem cells differentiate into dendritic cells 5  in a dish in the lab. He and his team found that their characteristics were stable over relatively long periods. The dendritic cells were fully differentiated, so could not reverse their development to become stem cells, although they had the \u2018immature\u2019 dendritic cell features \u2014 the form most likely to make T cells tolerant. Fairchild reasoned that human dendritic cells, cultured in the same way, could play a role in inducing transplant tolerance. If the dendritic cells and the therapeutic stem cells required for a given transplant \u2014 say, heart-muscle cells \u2014 could both be cultured in lines from the same stem-cell source, the dendritic cells could be used to make the recipient tolerate the therapeutic cells 6  (see graphic). \n                 Early days \n               Fairchild and his colleagues have yet to test this idea fully in mice, although their early unpublished experiments suggest that mice injected with dendritic cells carrying a specific protein can be made to accept skin grafts carrying the same protein. Fairchild has begun culturing human dendritic cells from embryonic stem cells. There is, however, the risk that a dendritic cell could switch into its mature state, and so activate T cells. If this were the case, the cells could help destroy, rather than protect, the graft. Fairchild and Waldmann are exploring the idea of treating the cells with various compounds in culture to keep them in their \u2018tolerizing\u2019 form 7 . But, as Fairchild stresses repeatedly, these are early days. But not all researchers are confident that cells differentiated from embryonic stem cells will end up being suitable for transplant therapy. Mick Bhatia, an immunology researcher now at McMaster University in Ontario, Canada, worries that even under the most rigorous conditions, an undifferentiated stem cell could still slip in among the differentiated cells. Undifferentiated stem cells can trigger a form of cancerous growth. Waldmann and Fairchild agree that before the approach could be used for treatment, tests must be developed to identify any undifferentiated cells in a cultured line. No one pretends that tolerance is going to arrive tomorrow. \u201cBut the important thing is to take the field onwards in all the areas with potential,\u201d says Waldmann. As degenerative diseases such as heart disease become more common, Fairchild and other researchers are convinced that they should intensify their efforts. \u201cWe've got a lot further to go, but it's very exciting.\u201d (See  'Great expectations' ) \n                     Verdict: Hwang's human stem cells were all fakes \n                   \n                     Timeline of a controversy \n                   \n                     Stem-cell brothers divide \n                   \n                     Mother-to-daughter transplant reverses diabetes \n                   \n                     Counting the cost of cyclosporine \n                   \n                     Transplant dangers tackled \n                   \n                     Cancer vaccine field gets shot of optimism from positive results \n                   \n                     Nature  Stem Cells in focus \n                   \n                     Making Stem Cells Web Focus \n                   Reprints and Permissions"},
{"file_id": "439524a", "url": "https://www.nature.com/articles/439524a", "year": 2006, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "The first vaccine against Lyme disease was withdrawn because patients distrusted it. Should market forces be allowed to shape the next one, asks Alison Abbott. The idea that the customer is always right is gospel in most areas of business. Most, you might think, except the business of making drugs and vaccines. Not so. The cautionary tale of the Lyme-disease vaccine is a good example of how consumer power can override science. Lyme disease is a debilitating infection spread by bites from ticks. Found mainly in the United States and Europe, its prevalence is increasing as civilization encroaches on forested areas \u2014 home to the deer on which the ticks feed. A vaccine is badly needed to contain the disease, but the one jab available was pulled from the market after complaints from a group of patients damaged its reputation. Now, a similar vaccine is set to emerge, but some say it has been tweaked in a way that confers no obvious scientific benefit. Lyme disease made a meteoric entry into medical and public consciousness 30 years ago, when Allen Steere, a rheumatologist now at Massachusetts General Hospital in Boston, described an outbreak of a mysterious illness in Lyme, Connecticut. The disease he identified was caused by a spirochete, a spiral-shaped bacterium called  Borrelia burgdorferi . It is rarely fatal, but acute infection results in a range of unpleasant and debilitating symptoms, including flu-like effects, a bullseye rash and neurological problems such as facial palsy. After several weeks without treatment, some patients go on to develop joint pain or arthritis. If left untreated, it can last for years, but even then will usually respond to antibiotics. In a small subset of patients, however, the joint pain and swelling do not respond \u2014 the condition is known as treatment-resistant Lyme arthritis. It was arguments about the long-term persistence of Lyme disease that triggered the first \u2018Lyme scandal\u2019. In the 1990s, Steere was hounded by patients who claimed to be suffering from a chronic form of Lyme disease that left them persistently exhausted. Steere insisted that they had been misdiagnosed. Although the patients had antibodies to Lyme disease, indicating that they had at one time been exposed to  Borrelia , Steere maintained that their range of vague symptoms did not correlate with the true course of the disease. He says he wanted to save people from unnecessary antibiotic treatment, but instead found himself under protection of security guards, and dealing with hate mail and even death threats. \n               Ticked off \n             The dust had barely settled when another scandal broke, which eventually led SmithKline Beecham (now GlaxoSmithKline) to withdraw its vaccine LYMErix from the US market. Shortly after the jab was introduced, hundreds of vaccine recipients claimed that they had fallen victim to side effects \u2014 including autoimmunity, in which the immune system attacks the body. Their claims were based on a scientific hypothesis that is still unproven today: it predicts that people will raise antibodies to a stretch of a particular protein on the outer surface of  Borrelia , called OspA, that could destroy normal human protein as well as the bacterium. LYMErix happened to work by generating antibodies to this OspA protein \u2014 which is why patient advocacy groups latched on to the idea that the jab itself could cause autoimmune disease. One law firm filed a class action in December 1999, and individual suits began to flood in. The US Food and Drug Administration (FDA) and the Centers for Disease Control and Prevention published their own investigation into 900 or so adverse event reports and concluded that there was no evidence of a link between the autoimmunity complaints and the vaccine 1 . But the bad publicity torpedoed sales. In February 2002, GlaxoSmithKline withdrew the vaccine from the US market and abandoned plans to launch a similar one in Europe. Enthusiasm for the jab seemed to have been killed off, and various attempts to generate vaccines against different  Borrelia  proteins fizzled to nothing. But last year the company Baxter Vaccines in Vienna quietly announced that it was hoping to do clinical tests of a new vaccine candidate in Europe. To the astonishment of Lyme-disease researchers, the candidate turned out to be broadly based on the same  Borrelia  protein, OspA. The vaccine, which has been successfully tested in animals, differs from LYMErix in two ways. Europe is home to two more species of  Borrelia  than the United States, so Baxter's vaccine is designed to protect against infection from all three. And the short stretch of the OspA protein that had been associated with the hypothetical danger of autoimmunity has been spliced out. The news only deepened the frustration of Markus Simon of the Max Planck Institute for Immunobiology in Freiburg, Germany. Along with his colleagues, Simon was the one who had developed the concept of an OspA-based vaccine in the early 1990s, which was then taken to the clinic by GlaxoSmithKline. \u201cThis just shows how irrational the world can be,\u201d says Simon. \u201cThere was no scientific justification for the first OspA vaccine being pulled.\u201d \n               On the trail \n             The idea that Lyme infection \u2014 and the Lyme vaccine \u2014 could cause the immune system to attack the body has its roots in the 1970s, when Steere described treatment-resistant Lyme arthritis. Nothing to do with the \u2018chronic Lyme disease\u2019 that angry patients later claimed had been triggered by Lyme infection, this syndrome, which can last for years, is characterized solely by inflamed joints. The arthritis seems not to be associated with continuing bacterial infection. Indeed, in the decades that followed, bacterial DNA has only very rarely been found in fluid extracted from affected joints, even using modern, sensitive methods of amplifying DNA traces 2 . So scientists began to entertain the thought that this treatment-resistant Lyme arthritis could be an autoimmune response. The idea was supported by the discovery in 1990 that the immune systems of most patients developing treatment-resistant Lyme arthritis shared some very specific, and genetically determined, characteristics 3 . This subset of people happens also to be particularly susceptible to rheumatoid arthritis, an autoimmune condition afflicting the joints. If persistent Lyme arthritis were really an autoimmune condition, what might be the mechanism? Scientists first looked to the idea of \u2018molecular mimicry\u2019, which was developed in the 1980s to explain how microbes in general might cause autoimmunity. According to this hypothesis, part of one of a microbe's proteins might be structurally similar to part of a normal protein in the patient \u2014 but different enough to be recognized as foreign by the patient's immune system. On exposure, antibodies to the foreign protein would be produced, which would then also attack the patient's normal protein. To test whether the molecular-mimicry hypothesis might be applicable to treatment-resistant Lyme arthritis, scientists began to look for a host protein that the immune system might mistake for a  Borrelia  one. Looking for matches to OspA was an obvious starting point, and in 1998 Brigitte Huber, an immunologist at Tufts University in Boston, and a number of her colleagues including Steere, came up with a strong candidate. They found that a stretch of the  Borrelia  OspA protein shared a very similar amino-acid sequence with a human protein that helps move immune cells from blood vessels to inflamed tissue 4 . In the test-tube, this protein did the right thing \u2014 it prompted immune cells from treatment-resistant Lyme arthritis patients to trigger inflammation processes. The implications of this for a vaccine whose mechanism depended on OspA antibody production were clear. And patient advocacy groups, sensitized by the battle over \u2018chronic Lyme disease\u2019, were on the alert when LYMErix was approved for the US market by the FDA later in 1998. But although molecular mimicry has a sound and respectable foundation as a hypothesis, no one has yet shown that it happens in real life. And as time went by, Lyme-disease researchers showed that many other proteins also activated OspA-specific immune cells in the test-tube 5 , and some of them did not even share similar sequences. \u201cThis all dealt a big blow to the molecular mimicry theory,\u201d says Thomas Kamradt, an immunologist at the University of Jena in Germany. \n               No verdict \n             Huber says that the jury is still out because there is not yet a good animal model in which to test the hypothesis. But most Lyme researchers are now convinced that molecular mimicry is an extremely unlikely explanation for any autoimmune response that might underlie treatment-resistant Lyme arthritis. So, given that costly testing for safety and efficacy will have to start from scratch, does it make any sense to modify an earlier vaccine's structure in ways that may not, after all, be necessary? Simon thinks not. \u201cWe had years of clinical data with LYMErix \u2014 hundreds of thousands were vaccinated, and not one autoimmune response was ever confirmed,\u201d he storms. \u201cAnd who knows if the new vaccine with this sequence spliced out will still be protective in humans?\u201d Only clinical trials will tell. When contacted by  Nature , Baxter Vaccines declined to comment. Although no link has been found between LYMErix and an autoimmune response, some researchers are taking a precautionary stance. Steere, who has worked as a consultant for Baxter Vaccines, says: \u201cThere is no proof that autoimmunity ever developed in anyone, but it could be a very rare side effect.\u201d Splicing out the sequence in question \u201ctakes care of this theoretical concern\u201d, he adds. Huber agrees: \u201cIt is essential to err on the side of caution, and it is a simple matter to eliminate the potential problem.\u201d Steere, whose experience with the angry patients encourages him to keep his head well below the parapet, strongly believes that a vaccine is necessary. And he argues that this need will become clearer to the public as experience of Lyme disease expands: \u201cMost people who want a vaccine are those who have already had the disease.\u201d \n                     Media attack prompts editorial backlash against MMR study \n                   \n                     Review rejects MMR autism link \n                   \n                     Polio vaccine web focus \n                   \n                     Antibiotics web focus \n                   \n                     WHO page on Lyme disease \n                   \n                     CDC information on Lyme disease \n                   Reprints and Permissions"},
{"file_id": "439776a", "url": "https://www.nature.com/articles/439776a", "year": 2006, "authors": [{"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "Life happens in three dimensions, so why doesn't science? Declan Butler discovers that online tools, led by the Google Earth virtual globe, are changing the way we interact with spatial data. Next month, biologist Erik Born will be wielding a crossbow and firing satellite tags into the hides of walruses, having manoeuvred his rubber dinghy through the pack ice off western Greenland. By tagging the walruses, Born will be able to track the animals' movements and behaviour from afar over several years. He will keep an eye on them using the same free Internet tool that has opened the eyes of millions to the possibilities of digital geography (and the sight of their house from above) \u2014 the Google Earth virtual globe. When the walruses migrate in the spring, Born and anyone else with a copy of the Google Earth software and a decent Internet connection, will be able to follow their westerly path to Baffin Island or the Canadian coast, and their return. Born, who works at the Greenland Institute of Natural Resources in Nuuk, got the idea from his colleague Leif Toudal Pedersen, a remote-sensing researcher at the Technical University of Denmark in Lyngby. Last month, Pedersen began using Google Earth to visualize live data from satellites, recording the density and drift of Arctic ice, as well as the position of individual buoys and icebergs. Born's decision to follow suit means they can collaborate easily, with a tool that is free and convenient. Combining Born's tracking data with Pedersen's maps should reveal how changes in ice affect the walruses' movements and behaviour. With traditional Geographic Information Systems (GIS) software \u2014 which was previously the only way to deal with spatial data like these \u2014 combining the two data streams would have been a headache. With Google Earth it will be effortless, says Pedersen: \u201cIt provides a very easy interface to a lot of different data.\u201d \n               Home page \n             To the casual user, of which it has attracted millions since its launch last June, the appeal of Google Earth is the ease with which you can zoom from space right down to street level, with images that in some places are sharp enough to show individual people. Its popularity with a growing number of scientists lies in the almost-equal ease with which it lets them lay data with a spatial component on top of background imagery \u2014 a trick they can repeat with multiple data sets. By offering researchers an easy way into GIS software, Google Earth and other virtual globes are set to go beyond representing the world, and start changing it. Leaving aside its value as a tool for meshing data, Google Earth is simply an excellent visualization aid. It really comes into its own when you \u2018tilt\u2019 the browser away from the default vertical view, and fly along in three dimensions, swooping over mountains and through valleys, or along city streets with buildings looming up on all sides. David Whiteman, an atmospheric scientist at NASA's Goddard Space Flight Center, is using this fly-by feature to understand local weather systems. Meteorological phenomena that occur over distances between 2 and 200 km are poorly predicted by current weather models, and scientists badly need real-time observations to refine their predictions. So this summer, Whiteman will take part in a large field project, observing a three-dimensional column of the atmosphere centred on the Howard University campus in Beltsville, Maryland. Measurements will be made simultaneously by a battery of aircraft and satellite-borne instruments, including Whiteman's Raman Airborne Spectroscopic Lidar, which measures the properties of atmospheric water vapour, aerosol particles and cloud droplets. This exercise will generate masses of data on the formation and behaviour of clouds, which the three-dimensional capabilities of virtual globes are ideally suited to viewing. \u201cWe are going to be able to display curtains of data of different atmospheric quantities, looking into them from above and flying along them,\u201d explains Whiteman. \u201cWe will have the topology of the terrain; you will be able to see uplift and cloud formation, and its interaction with precipitation.\u201d And like Born's work, these easily viewed data can be shared instantly with colleagues. \u201cI think it's just a short matter of time before systems like Google Earth are an essential requirement for people in our field,\u201d says Whiteman. \u201cAs soon as one group shows that this is useful, everyone will adopt it.\u201d Indeed, increasing amounts of scientific data are becoming available, often in real time, in formats that can be displayed by virtual globes. Millions of species distributions from the Global Biodiversity Information Facility, headquartered in Copenhagen, Denmark, are available for Google Earth, for example. \n               Digital watch \n             The eventual impact will be nothing less than the realization of a \u2018digital Earth\u2019, as described by former US vice-president Al Gore in 1998. \u201cDigital Earth was always intended to allow us to \u2018fly\u2019 from space (virtually, of course) down through progressively higher resolution data sets to hover above any point on the Earth's surface \u2014 and then display information relevant to that location from an infinite number of sources,\u201d says Gore. \u201cIts highest purpose was to use the Earth itself as an organizing metaphor for digital information.\u201d But the project died a death in 2001 after Gore lost the 2000 US presidential election. Michael Goodchild, a GIS expert at the University of California, Santa Barbara, says that scientists' use of virtual globes is breathing fresh life into Gore's dream. There is renewed hope that every sort of information on the state of the planet, from levels of toxic chemicals to the incidence of diseases, will become available to all with a few moves of the mouse. The GIS software is already an important tool for understanding spatial and temporal factors in a wide range of disciplines. But many scientists that could use GIS do not, and it has remained largely the preserve of specialists. Goodchild is convinced that tools like Google Earth will increase awareness of GIS's potential and encourage researchers to explore more powerful GIS techniques. \u201cIt's like the effect of the personal computer in the 1970s, where previously there was quite an \u00e9lite population of computer users,\u201d Goodchild enthuses. \u201cJust as the PC democratized computing, so systems like Google Earth will democratize GIS.\u201d Goodchild illustrates the system's ease with an anecdote from his own teaching. \u201cTypically, I used to spend an entire year taking senior undergraduates through courses in GIS. And at the end of the year, as a treat, I might let them generate a three-dimensional fly-by over a landscape,\u201d he says. \u201cNow, using Google Earth, a ten-year old can do that.\u201d Goodchild thinks that once scientists experience this easy visualization, they will be drawn into deeper forms of analysis using the powerful techniques that GIS professionals have developed over decades. GIS techniques use statistics to predict or explain patterns, whether they be outbreaks of vector-borne disease or urban crime waves. Displaying such patterns on the current version of Google Earth requires gentle hacking. But Brian McClendon, director of engineering at Google Earth, says he wants as many people as possible to use the program \u2014 and will consider adding features that make it easier for them to get their data into it. McClendon is hoping to attract scientists who will generate interesting content for Google Earth's other customers. Whiteman emphasizes, however, that three-dimensional visualization is about more than just creating images. He says it is critical for generating scientific hypotheses and the questions they go on to test. \u201cWhat have I measured; what are the relationships I have here; what can I explore? It is part of getting an intuitive grasp of the problem, the measurements and the analytical challenge\u201d. If one can simultaneously visualize the data and the predictions from models, areas where the data agree or disagree with the model jump out, he says. \u201cThis could dramatically improve the efficiency of exploring relationships among quantitative data.\u201d \n               Popular movement \n             Google Earth has no analytic functions and is not designed to replace professional GIS software; in fact, it should be a boon to the software makers. \u201cGoogle Earth is just the most fantastic thing I have ever seen,\u201d says Jack Dangermond, founder and president of ESRI, the world's largest creator of GIS software. ESRI, which is based in Redlands, Virginia, and other GIS companies that were caught napping by Google Earth are now eager to capitalize on its success. They are creating a slew of products that combine its ease of use with their traditional analytic strengths. \u201cIt really is opening up our world,\u201d says Dangermond, \u201cand business is booming.\u201d Later this year, ESRI will release a huge upgrade of its flagship desktop GIS product, ArcGIS, which will let users publish virtual globes on the Internet and analyse their data in many ways. \u201cWe have taken several thousand functions and deployed them in an Internet environment,\u201d says Dangermond. As part of the package, ESRI will also release a free visualization tool, ArcGis Explorer, which some GIS professionals are calling a Google-Earth killer. Data in Google Earth need to be in a specific format; ESRI's tool will allow users to view not only data from ESRI's own products, but also information in formats that are being increasingly standardized through the Open Geospatial Consortium. This international body is working to ensure that computers can understand descriptions of the spatial features of anything from highways and postcodes to icebergs. Unlike Google Earth, the ESRI viewer comes equipped with a series of analytic tools. Scientists can run models on their servers, and simultaneously view them over the Internet in ArcGis Explorer by dragging and dropping data files. They can fuse multiple data sources on screen, and export them in whatever format they choose. Still, McClendon believes that both Google and ESRI will profit from a generally increased interest in GIS, and says he welcomes the competition. \u201cI think that we want many more people to understand and care about GIS, and ESRI's tools are the best in the business for that,\u201d says McClendon. Skyline Software Systems, based in Chantilly, Virginia, was one of the first companies to offer a virtual globe: TerraExplorer. Later this year it will release Skyline Online, a browser-based tool similar to ArcGis Explorer. \u201cIt will empower users at home as well as researchers with capabilities that have been available only to government agencies until now,\u201d says company president, Ronnie Yaron. \n               Globe trotters \n             Meanwhile, in the public sector, NASA has no intention of using its World Wind virtual globe to compete with Google, at least according to one scientist on the project, who did not wish to be identified. He points out that World Wind is explicitly designed for scientific information, and its code is open source so that scientists and software developers can tailor it to their needs. He admits that World Wind's performance lags behind that of Google Earth: it is slower and eats up much more memory and processor time. But NASA intends to remedy these performance issues \u2014 a major upgrade will be released in July, he says. Joshua Been, GIS librarian at the University of Texas, Arlington, sees firsthand how Google Earth is promoting interest in a range of professional GIS systems. Researchers and students increasingly come to him wanting to use Google Earth or World Wind, but most of their queries need analyses beyond straightforward visualization, he says. \u201cSo I try my hardest to show them the super-cool capabilities of full-blown GIS.\u201d Still, Been's library offers the same level of support for the two free systems as it does for professional software, and he encourages people to use them. \u201cIf they want GIS data to be viewable in a free and public three-dimensional viewer, Google Earth will continue to be the standard,\u201d he says. \u201cI disagree entirely that ArcGIS Explorer will be a \u2018Google-Earth killer\u2019\u201d For Ming-Hsiang Tsou, a geographer at San Diego State University in California, \u201cthe year 2005 was a watershed for GIS on the Internet\u201d. He notes in particular how natural disasters, such as the tsunami and Hurricane Katrina, etched the power and utility of GIS and Google Earth into people's minds (see  page 787 ). Before Google Earth appeared, Tsou himself created interactive two-dimensional Internet maps of the 2003 San Diego wildfires, featuring data on fire spread, hot spots and rescue operations. The huge public interest in such visualizations means scientific information is being made much more accessible to users, he says. Meteorological radar data and satellite images have long been available online, but Google Earth is allowing the agencies that provide them, such as the US National Oceanic and Atmospheric Administration (NOAA), to make the data more useful and user friendly. Real-time weather information can now be displayed in Google Earth alongside the landmarks and routes in which the general public is interested, says Valliappa Lakshmanan, a NOAA researcher at the University of Oklahoma, Norman. People can use Google Earth to ask: \u201cHow far is the rain core from the route that I will take this afternoon? Is it near grandma's house?\u201d Such detail is possible because the resolution of forecast data is now as good as 1 km, updated every 120 seconds. The value of scientific data increases the more you can link it to information that the user already considers important, says Lakshmanan. \u201cScientists should take this opportunity to use GIS to present their scientific results in a way that users can easily tie to other data sources.\u201d Nowhere is this more true than in the environmental sciences. One of the traditional roles of GIS has been to provide data to support decision-making. And environmental groups that have discovered GIS are starting to use it to change the balance of power in public debates. As more citizens become concerned about their local environment, easy-to-use virtual globes will facilitate the communication of spatial information between stakeholders and government agencies. Back in Greenland, Born and Pedersen hope to be able to record the effects of climate change on the spread of sea ice and on their tagged walruses. If the ice continues to recede in the Davis Strait, as it has for the past two decades, they will be able to monitor its immediate impact on walrus feeding grounds. Is it too much to hope that, in future, the same approach to sharing data will help us decide what to do about global climate change? For more on Google Earth, see page 763 and  \n                     http://www.nature.com/news \n                   . \n                     Mashups mix data into global service \n                   \n                     Mapping opportunities \n                   \n                     News Feature: Eye in the sky \n                   \n                     Studying the global distribution of infectious diseases using GIS and RS \n                   \n                     How does Google Earth work? \n                   \n                     The virtual world gets bigger \n                   \n                     Google Earth \n                   \n                     Nasa World Wind \n                   \n                     ESRI \n                   \n                     Web resources from Connotea \n                   Reprints and Permissions"},
{"file_id": "439779a", "url": "https://www.nature.com/articles/439779a", "year": 2006, "authors": [{"name": "Meredith Wadman"}], "parsed_as_year": "2006_or_before", "body": "Scientists and medical doctors view research through different lenses \u2014 but the gulf in outlook between the two tribes isn't what it used to be. Meredith Wadman reports. Genetics graduate student Rima Adler reached the limits of her collegiality one evening last May, when she needlessly missed a concert by the Baltimore Symphony Orchestra. Late that afternoon, she had stayed in the lab to help a physician colleague, and ended up spending three hours explaining the basics of the polymerase chain reaction and how it could help the young physician's analysis of their results. Adler ended up stuck in the lab until 9.30 at night, forgetting about her concert ticket. So she wasn't too thrilled the next day, when she heard the same colleague asking someone else in her lab at the National Institutes of Health (NIH) in Bethesda, Maryland, the same question. \u201cI thought: what the hell am I wasting my time for?\u201d Adler recalls. She thinks the colleague \u201cdidn't want to learn new things\u201d. It was a classic case, she says, of a physician \u201cwanting to just get an answer\u201d instead of taking time to properly understand a problem. Conflict between PhDs and MDs has been as perennial in biomedical research labs as head lice are in kindergarten \u2014 and can be just as unpleasant. \u201cThere is this stereotypical idea among medical students that those people are getting PhDs because they couldn't get into med school,\u201d says Javed Siddiqi, a California-based neurosurgeon who spent two years in a research lab before himself going to medical school. Meanwhile, he says, PhD students see themselves as intellectuals, \u201cbroadening the frontiers of science\u201d while medics, as he puts it, are \u201cmemorizing anatomy\u201d. The culture clash arises when freshly trained doctors are unleashed on laboratories full of bench scientists. Having been deferred to by patients and taught to treat and discharge people in three days \u2014 or even three hours \u2014 they encounter a strange environment where asking questions is as important as obtaining answers, quick fixes are rare, and jeans and trainers trump ties and stockings. And lurking always in the background is the fact that the MDs are paid higher salaries. \u201cThe clich\u00e9 \u2014 which must have some truth in it \u2014 is that the medics want to come into the lab and get everything to work very quickly, without any special struggle, and then write some fantastic paper and make their career in five minutes,\u201d says Paul Klenerman, a virologist with an MD and PhD at the University of Oxford, UK. \u201cThey generally get a bit of a shock when they realize quite how much effort is involved and how much can go wrong.\u201d But Peiman Hematti, a physician with extensive bench training who studies stem cells at the medical school of the University of Wisconsin-Madison, says the problem has diminished over the years. He recalls a conversation he had ten years ago when he was a young MD looking to get training as a postdoc: a friend cautioned him in the direst terms about the inhospitable environment he was sure to find if he joined a lab headed by a scientist rather than a physician. \n               Conflicts happen \n             In fact, Hematti's experience suggests that the stereotypes are losing their power. Conflicts \u201ccan happen in any lab\u201d, he says. He doesn't think they occur because physicians expect scientists to clean up their messy lab benches for them, or because PhDs look down their noses at MDs. Today \u201cit's more a personality thing; I don't think it matters whether there's an MD or a PhD at the end of your name.\u201d During a postdoctoral fellowship, for example, Hematti enjoyed what he terms a \u201cvery fruitful\u201d collaboration with Boris Calmels, a French scientist whose strengths in molecular biology combined with Hematti's clinical background to produce notable papers 1 , 2  on the integration of lentiviral and retroviral vectors into host genomes. \u201cThe result was much better than if we had worked separately,\u201d he says. Calmels, now at the Centre for Cell and Gene Therapy at the Paoli-Calmettes Institute in Marseille, France, agrees. \u201cWhen he had basic questions, usually I had the answer,\u201d he recalls. \u201cAnd when I was looking for specific clinical knowledge, he was the person to ask.\u201d A number of forces have combined to blunt the once-common tensions between scientists and physicians in the lab. For a start, clinical medicine is less hierarchical than it used to be, and produces doctors less likely to enter the lab with a \u201cScalpel, nurse!\u201d attitude. And in the United States, at least, sharp growth in biomedical-research funding, combined with growing demands for the translation of research into cures, is blurring the line between bench and bedside. \n               Birth of a divide \n             In this environment, cooperation has become imperative. Isolated efforts by physicians or bench scientists \u201cjust don't work\u201d, says Rick Morgan, a geneticist who oversees neurosurgeons' postdoc work at the National Cancer Institute in Bethesda, Maryland. Others argue that tensions have never been as great as laboratory gossip would suggest \u2014 and were always far outweighed by the achievements of MD\u2013PhD partnerships. These stretch back to Linus Pauling and Harvey Itano's classic 1949 discovery that sickle-cell anaemia is caused by a structural problem with the haemoglobin protein found in red blood cells. \u201cThe mix of MDs and PhDs in my lab enriched everybody's experience,\u201d says Sam Silverstein, a physician in the department of physiology and cellular biophysics at Columbia University in New York. Calmels, however, suspects that the problem is more acute in nations such as his own, where the two groups are educated separately from the moment they leave school. Moreover, in France, training rules may force physicians seeking academic appointments to complete a year of bench research. \u201cThey're not really interested in working at the bench,\u201d Calmels says. \u201cMost of the time, there are two different cultures.\u201d One hundred years ago, this division would have seemed odd. Biochemistry was then the nexus of medical research, but there was no such thing as a PhD in biochemistry: aspiring biochemists went to medical school on their way to the bench. This explains why the biomedical luminaries of the first half of the twentieth century, from Otto Meyerhof to Hans Krebs, started out as medical doctors. And although they spent the rest of their lives in research, their experience in clinical medicine provided an important bridge between the two. But after the end of the Second World War and a sudden influx of public funds, medical schools for the first time established academic departments with PhD-granting powers. Ironically, this meant that PhD students were suddenly being trained on the premises of major medical schools and centres \u2014 but with virtually no exposure to patients, doctors or pathology. Thus the great divide was born. \u201cFor many years, it was the prevailing view among scientists that somehow medicine was a bit dirty, because people made a lot of money and it wasn't based on hard science,\u201d notes Irwin Arias, a physician and cell biologist who oversees a popular NIH lecture series that teaches clinical medicine to scientists. \u201cThat created this tension. There was \u2018us\u2019 and there was \u2018them\u2019. The physician was the one who dealt with the patients.\u201d It wasn't until the early 1990s that the public push to bring research to the clinic gained momentum, and a slow rapprochement of the two sides began. \u201cThe wheel has turned,\u201d says Arias. \u201cMore and more there's the realization that you can't separate basic science from human health. Those who try to do it, even as scientists, often get butchered\u201d by grant reviewers, he says. Meanwhile, those paying for research are moving to address the issue directly. This week, the Howard Hughes Medical Institute (HHMI) is unveiling awardees in a new $10-million grant programme aimed at directing PhD students into clinically related thesis projects co-mentored by a physician and a scientist. Thirteen awards are being made following 82 applications. \u201cIt's an idea whose time may have come,\u201d says William Galey, director of graduate education programmes at the HHMI. And at the NIH main campus, a weekly lecture series schooling scientists in medical topics from tuberculosis to Parkinson's disease is drawing enthusiastic reviews. The \u2018Demystifying Medicine\u2019 series attracted 80 or so scientists to an NIH auditorium when it was launched five years ago; this year more than 900 have registered for it, and thousands all over the world download videos of the lectures. \u201cNowadays, PhDs are intensely interested in learning more about disease \u2014 what it looks like, what it feels like,\u201d says Arias, the course designer. \u201cAnd it turns out that it's a lot easier to demystify medicine for PhD scientists than it is to make last year's chief resident into a bench scientist.\u201d That transition may be the shape of the future, as the era of the physician\u2013scientist draws to a close. Today, \u201cyou have to be superhuman to do both well,\u201d says neurosurgeon Siddiqi. So getting the best science to the bedside is going to demand collaboration. \u201cNeither the MD nor the PhD is going to be doing the whole thing. More and more, people are realizing that their specialty is not an island; they are going to have to talk to somebody else.\u201d \n                     Training PhD researchers to translate science to clinical medicine: Closing the gap from the other side \n                   \n                     Distinct Genomic Integration of MLV and SIV Vectors in Primate Hematopoietic Stem and Progenitor Cells \n                   \n                     Recurrent retroviral vector integration at the  Mds1/Evi1  locus in nonhuman primate hematopoietic cells \n                   \n                     Demystifying Medicine 2006 course \n                   \n                     Howard Hughes Medical Institute awards programme \n                   \n                     (NB not live until wed 15 Feb) \n                   \n                     A bridge building between medicine and basic science \n                   Reprints and Permissions"},
{"file_id": "439905a", "url": "https://www.nature.com/articles/439905a", "year": 2006, "authors": [{"name": "Christina Reed"}], "parsed_as_year": "2006_or_before", "body": "Teams of researchers are finding vents in ocean floors around the globe. Christina Reed follows the hunt for these extreme ecosystems. Before Rachel Haymon sailed to the Galapagos Islands last December, a fellow oceanographer made her a bet on whether she would find a particular kind of hydrothermal vent. The Galapagos region is where scientists discovered these vents, underwater fissures where superheated water bubbles out of the sea floor. But one of the most dramatic vent phenomena \u2014 black smokers, chimneys that spew a flood of dark mineral particles \u2014 had never been seen in the area. According to some, this was because the geological setting of the Galapagos should not permit the formation of black smokers. But in retrospect, Haymon says, she should have upped the ante on her bet. On 14 December her team discovered the first Galapagos black smoker. And that isn't the only new find. In the past couple of years, marine scientists have discovered numerous vents. What was once a handful of isolated vents has expanded into a dizzying diversity of oceanic wonders. Researchers don't yet know what to make of all the finds, but for now they are sitting back and enjoying the feast. The recent finds include the most northerly active high-temperature chimneys in cold Arctic waters; chimneys in the South Atlantic; and the biggest plume ever recorded of hydrothermal chemicals released from an underwater eruption. With each new discovery, marine scientists are realizing that the unusual is to be expected. Variations in geology, chemistry, physics and biology make every vent field unique. Take, for instance, the warm water flowing from hydrothermal vents in the chilly waters of the Arctic Ocean. Here, 500 metres below the surface, iron-oxidizing bacteria, including the species  Gallionella ferruginea , cover the sea floor for kilometres. Within this region, known as Gallionella Garden, short mineralized chimneys sprout up, each around 15 centimetres high and often topped with delicate sea lilies ( Heliometra glacialis ) that make them look like one of Dr Seuss's cartoons. In July 2005, a team led by Rolf Pedersen of the University of Bergen, Norway, visited this site. It lies along the Mohns Ridge, where tectonic plates are pulling apart, or \u2018spreading\u2019 at a rate of about 16 millimetres per year; speedier ridges can spread by up to 200 millimetres per year. The researchers sent down a remotely operated vehicle called  Bathysaurus  to look at the region. Using temperature sensors, the group discovered that the chimneys bathed the sea lilies with a flow of fluid typically at 0.5 \u00b0C. That's warm for the Arctic Ocean; the background temperature at this site measured \u22120.3 \u00b0C. The team then sent  Bathysaurus  to the base of an escarpment, formed when an earthquake pushed up the sea floor 100 metres. Following the base of the cliff wall, the submersible found a series of hydrothermal mounds, each topped with several smoker vents. The fluid pouring out of the smoker chimneys here was far hotter. Because it was bubbling, the team estimated it must be at least 263 \u00b0C. And sure enough, when  Bathysaurus  extended its 260 \u00b0C-maximum sensor into the flow, the probe was knocked out of commission. \n               Smoking chimneys \n             While hovering over the site, the research team noticed something unusual just below their ship. \u201cThe echo sounder recorded something coming up,\u201d says Pedersen. He wondered whether the echo sounder had detected fish that had followed a drift of nutrients wafting up from vents on the sea floor. Using a similar signal farther down the ridge, the team discovered another vent field within hours, just 5 kilometres away from Gallionella Garden. After years of laboriously tracing chemical plumes to locate vent sites, the scientists were thrilled with the ease of their discovery. The echo sounder not only helped them locate the vents, but also imaged the plumes and what appeared to be an associated school of fish. \u201cThis had never been done before,\u201d says Pedersen. Next summer, the team will return to the vents to sample fish and other animals, microbes and fluids, and then use the new technique to search for vent sites in deeper waters farther north. As some researchers push north, others are going south \u2014 finding hydrothermal vents on the southern part of the Mid-Atlantic Ridge. This is the underwater mountain range that runs like a backbone down the middle of the Atlantic Ocean. As many as two dozen hydrothermal-vent fields have been found in the northern part of this mountain chain. But few expeditions had searched for vents in the south. So it was with an eager crew that the RSS  Charles Darwin  embarked on a month-long expedition in February 2005, to map the sea floor along a 250-kilometre stretch of the ridge. Near 5\u00b0 S, the team found two potential vent sites a kilometre apart, and mapped each with high-resolution sonar using an underwater robot named ABE (for autonomous benthic explorer vehicle) 1 . On one of ABE's trips, its mission was to photograph the field of potential black smokers with a digital camera. \u201cWhen we brought ABE back on deck, it was burnt through,\u201d says the expedition's leader, Chris German of the National Oceanography Centre in Southampton, UK. ABE's images showed that \u201cthe cooking place\u201d, as German calls it, contained fresh basaltic lava flows and what looked like shrimps, mussels and crabs. In April, the German research vessel  Meteor  visited three sites in the area. At one site, called Turtle Pits, a team led by Karsten Haase of the University of Kiel, Germany, sampled boiling fluids at temperatures close to 400\u00b0 C. \u201cThis temperature is the highest measured so far along the entire Mid-Atlantic Ridge,\u201d says team member Andrea Koschinsky, a geochemist at the International University Bremen in Germany. The high temperature indicates that fluids are being heated close to a magma source, most likely from a recent upwelling or volcanic eruption. \u201cThe effect of recent volcanic activity on submarine hydrothermal systems has so far been documented only along fast- and intermediate-spreading centres, such as the East Pacific Rise, but not from slow-spreading ridges where volcanic eruptions are rare,\u201d she says. The German team discovered another vent field at 9\u00b0 33\u2032 S, the most southerly vent field known so far on the ridge. Because very young and small mussels dominate the ecosystem, the group dubbed the field \u2018Liliput\u2019. In April, Koschinsky will lead the next  Meteor  cruise to the region, with a freshly mended ABE ready to track down any more hot vents. Oceanographers increasingly realize that when it comes to vents, they don't know what is out there. So they have taken to using plume sensors whenever possible \u2014 even when mapping a suspected plate boundary. Such was the approach of marine geologist Bramley Murton, when on board the  Charles Darwin  in the Indian Ocean in 2003. \n               Plume clues \n             Somewhat optimistically, Murton attached a series of sensors called miniature autono-mous plume recorders (MAPRs) to the towline of a dredge he was using to conduct a geochemical survey of the slow-spreading Carlsberg Ridge. Geologists only thought the ridge existed because Russian magnetic mapping of the sea floor in 1970 had predicted it should be there \u2014 not because anyone had actually seen it. Murton's expedition was the first to map the ridge with multibeam swath bathymetry, a kind of sonar that produces beautiful high-resolution images of the seafloor topography. The MAPRs were attached to the dredge, just in case, to pick up any hints of a plume from a sub-marine eruption. Murton's team managed to map the ridge, and excitedly pulled up glassy basalt rocks from it, but the biggest surprise was the MAPR results. The sensors had detected, in the chemistry of the water, evidence for a recent underwater explosion. Further lab tests, the results of which were announced at the American Geophysical Union meeting in San Francisco last December, showed that they hadn't come across just any hydrothermal plume. It was an \u2018event plume\u2019, meaning it came from a single underwater eruption. Most plumes that oceanographers use to track down vents come from a single chimney or vent field. If the chemical signal from a vent field is like the smoke from a candle, that from an event plume is like the smoke from a wildfire. Even more surprising, this one turned out to be the most energetic and biggest event plume ever seen \u2014 7 to 20 times larger than those previously recorded. It had risen 1,400 metres above the sea floor, measured one kilometre wide, and was drifting for 70 kilometres along the ridge. Twenty million cubic metres of lava would be needed to create the heat to drive such a plume, says Murton. Clearly it came from a major volcanic eruption. Because the Carlsberg Ridge is one of the slowest-spreading, and so supposedly less active oceanic ridges, many had thought it unlikely to be the location of a major volcanic eruption. At ridges such as this, heat is thought to be released more slowly from the underlying magma. Such oceanographic revelations show the importance of exploring new areas. But the search doesn't always have to involve month-long, deep-sea expeditions that typically cost millions of dollars. Scientists are increasingly seeing hydrothermal vents through a dive mask rather than through a submersible's viewport. Because the sea floor is on average about 4 kilometres from the surface, hydrothermal vents are frequently referred to as \u2018shallow\u2019 if they are anything less than 1 kilometre below the surface. But truly shallow hydrothermal vents, the kind a researcher can scuba dive to, are very different from their deep-water counterparts, as recent studies have shown. \n               Deadly cocktail \n             In an Icelandic northern fjord called Eyjafj\u00f6r\u2202ur, researchers discovered in 2004 a rich shallow-water vent ecosystem. \u201cThe area contains not a single vent, but a series of chimneys and fissures stretching for about 500 metres,\u201d says geologist Bjarni Gautason, who works for the company Iceland GeoSurvey. The tops of some of the chimneys are only a 14-metre dive from the surface. All the chimneys spout fresh water, contaminated with less than 1% sea water, and at temperatures reaching 77 \u00b0C. The chimneys \u201cform their own landscape providing a unique habitat for the animals and plants in the area\u201d, Gautason says. \u201cWe know about some other shallow-water hydrothermal-vent sites in the world, but none that makes chimneys like the ones here,\u201d says biologist Hrei\u2202ar Valt\u00fdsson of the University of Akureyri in Iceland, and a member of Gautason's team. \u201cSo we think ours are quite special.\u201d But shallow vents aren't always the kind you would want to visit. Near Ambitle Island, Papua New Guinea, some vents spew out a toxic cocktail of arsenic with a splash of sea water. As much as 1.5 kilograms of arsenic flows daily from these vents into the surrounding coral reefs 2 . For up to 200 metres around the vents, microbes coat the sediment and corals with a red and green biofilm. These organisms seem to be able to survive the contamination, and so may give researchers a chance to study arsenic in a marine environment, and see how microbes cycle it through the ecosystem. In the past few years, studies have suggested that arsenic from silt is getting into the water supply in Bangladesh and is potentially poisoning millions of people. The vents near Ambitle Island could provide clues on how to address such a problem. In Bangladesh, arsenic is being reduced, by microbial activity, from As(V) in the sediment to the more soluble As(III), which seeps into the groundwater and contaminates the drinking water. Jan Amend, a microbial geochemist at Washington University in St Louis, hopes that his team's work at the Ambitle vents will help researchers better understand how microbes might catalyse the reverse chemical reaction, oxidizing As(III) back into the less soluble As(V). \u201cThis part of the arsenic cycle has not received nearly as much attention,\u201d he says. Heat may also play a major role. Geochemists studying hydrothermal systems on volcanic islands off the coast of Italy have found that high water temperature can lead to higher concentrations of arsenic in the groundwater fed by those systems 3 . \n               Highway of the Pacific \n             Still, much is uncertain as to how the geology of a region influences the temperature and minerals that precipitate at different vent sites. The Galapagos spreading ridge, the site of Haymon's recent explorations, is one such example. A \u2018hot spot\u2019 of magma from deep in the Earth formed, and continues to form, the Galapagos Islands. The amount of magma and heat released from this hot spot has created a thick, possibly pliable crust in the region that could permit only low-temperature hydrothermal vents to emerge. And that's all previous expeditions had found. But Haymon thought that the spreading ridge north of the islands, where the hot magma lies close to the sea floor, would yield the focused flow of high-temperature fluid needed to form black-smoker chimneys. And nobody had yet explored this area for vents. A few hours after discovering the first Galapagos smoker chimney, named Plumeria after a tropical flower 4 , Haymon's team spotted a cluster of as many as six black-smoker chimneys, 12 to 14 metres high. In a nod to the Galapagos' most famous marine reptiles, the team dubbed the chimneys the Iguanas. Two weeks later, the group came upon two other sites featuring more black smokers. The discovery allows for the possibility that the underwater Galapagos chimneys, like the islands themselves, are home to unique animal species. Or it may be that the chimneys provide a highway for animals exploiting the special conditions the vents create; larvae could drift in the vent plume from chimney to chimney, up and down the mid-ocean ridge. \u201cWe saw animals, but I could not say from looking whether they were new species,\u201d she says. Only further exploration, collection of the organisms, and genetic testing will tell. It may be fitting that the Galapagos \u2014 the site that fuelled Charles Darwin's ruminations on evolution \u2014 are now the focus of the latest underwater mystery. But Haymon isn't surprised. \u201cWhen you consider the size of the ocean and how hidden everything is, we have only begun to really know what's there,\u201d she says. \u201cI'm convinced there are many surprises left for us.\u201d \n                     Deep-sea mission finds life in the Lost City \n                   \n                     Seafloor vents spawn spat \n                   \n                     Robots in the deep \n                   \n                     Lost City of Atlantis vents its secrets \n                   \n                     Biogeography of Deep-Water Chemosynthetic Ecosystems \n                   \n                     Discovery of \u2018black-smoker\u2019 chimneys \n                   \n                     Mid-ocean ridges \n                   \n                     The Carlsberg Ridge cruise \n                   Reprints and Permissions"},
{"file_id": "439908a", "url": "https://www.nature.com/articles/439908a", "year": 2006, "authors": [{"name": "John Whitfield"}], "parsed_as_year": "2006_or_before", "body": "Europe pumps large quantities of cash into schemes that encourage less-intensive farming. But, finds John Whitfield, some researchers are not sure what benefits they deliver. It's five-and-a-half hours on the train from London to Wageningen in the Netherlands. Through Kent and the Pas de Calais, the view is of rolling arable fields; north of Brussels, it changes to a flat checkerboard of pastures, edged by ditches and dotted with old windmills and modern wind turbines. These are some of the most intensively farmed landscapes on Earth, from which plants and animals unable to earn their agricultural keep have been excluded over many years. In the time it takes to make this journey, about \u20ac2 million (US$2.4 million) of public money will be spent, Europe-wide, on subsidies aimed at reversing the centuries-long trend towards the highest possible efficiency in farming. Together, governments and the European Union (EU) spend roughly \u20ac3.5 billion a year on schemes aimed at encouraging less-intensive farming in order to see gains in biodiversity, landscape preservation, and water and soil quality. These schemes cover a quarter of the farmland in the 15 countries that made up the EU before enlargement \u2014 an area about the size of Germany. But their continuation is not guaranteed. The latest EU budget deal could see funding for such \u2018agri-environment\u2019 schemes in these countries fall by a fifth compared with 2006. As well as being a daunting spending commitment, Europe's agri-environment schemes also represent one of the world's biggest ecological experiments \u2014 or they would do, had anyone bothered to formulate hypotheses or collect data. Unfortunately, from this point of view, most of Europe's agri-environment schemes have very vague goals, such as to \u201cprevent damage to the environment\u201d or \u201cprovide wildlife habitats\u201d. Specific targets are not set; progress is rarely monitored; the baselines from which they start are not defined. The good that they do is thus hard to measure, which in some eyes makes the schemes hard to justify. As the European Court of Auditors wrote in a critical report 1  on the subject last year, \u201cIf a measure cannot be adequately checked, it should not be the subject of public payment.\u201d \n               Going Dutch \n             At the end of last month, 75 researchers, as well as conservationists and officials from Spain, Estonia and most places in between, met in Wageningen to discuss measurements of the schemes. They looked at the largest international study of agri-environment strategies so far, and talked more generally about whether such schemes work, how to tell if they do, why some are successful, and how the rest could be improved. There is definitely a lot of room for such improvement. If judged in terms of biodiversity, research suggests that almost half the schemes have no effect. A handful have an effect opposite to that intended. The meeting was convened by ecologist David Kleijn and his colleagues at Wageningen University, who have pursued the subject with vigour. In 2001, in one of the first scientific audits of an agri-environment scheme, they showed that a Dutch project intended to help ground-nesting meadow birds by delaying the mowing of fields was having no effect \u2014 birds actually seemed to prefer intensively farmed fields 2 . The paper caused a storm. The Dutch agriculture ministry, which did not take kindly to being told it was wasting its money, summoned the researchers to explain themselves. \u201cThey really didn't like it,\u201d recalls Frank Berendse, one of the team. \u201cI was quite amazed by the impact,\u201d adds Kleijn. Sue Armstrong-Brown, head of agricultural policy at Britain's Royal Society for the Protection of Birds, thinks that the Wageningen research contributed to the current cuts in agri-environment spending: \u201cYou can trace a lot back to the 2001 Kleijn paper, and the debate around it \u2014 that work caused a lot of concern among policy-makers.\u201d Kleijn went on to win EU funding for an evaluation of agri-environment schemes across Europe, a project called EASY. He began by publishing a meta-analysis in 2003 of the evaluations up to then, working with William Sutherland of the University of East Anglia in Norwich, UK. They unearthed 62 studies, of which three-quarters had been conducted in Britain or the Netherlands; a third of the delegates at the Wageningen meeting were from Britain, reflecting the higher profile of nature conservation in the north of the continent relative to the south. Many studies were rejected owing to their poor design; of the rest, 54% showed some positive effect of schemes on biodiversity, 6% showed a negative effect, and the rest no effect 3 . \u201cIt was rather depressing, wading through report after report that showed minimal benefits,\u201d says Sutherland. That same year, Kleijn and an international team of researchers began a study of schemes in the Netherlands, Spain, Germany, Britain and Switzerland, the results of which were unveiled at January's meeting in Wageningen. This team chose pairs of nearby fields, in which one was part of a scheme and the other was not. They then surveyed the fields for five groups of creature: plants, bees, grasshoppers and crickets, spiders and birds. Out of 25 categories (five species groups in five countries), 12 showed a positive response to agri-environment schemes 4 . Plants showed the most widespread benefits, with higher diversity on scheme fields in every country except the Netherlands. Bees benefited in Germany and Switzerland, grasshoppers and crickets in Britain, and spiders in Spain. In cases where the biodiversity went up, nearly all the beneficiaries were common species; only one scheme \u2014 a Spanish programme aimed at making arable fields bird-friendly by leaving winter stubble \u2014 showed a positive effect on endangered species, one of which was the thekla lark ( Galerida theklae ). \u201cOne could argue that this lack of benefits is the most important result, because endangered species are the ones that suffer most from the intensification of agriculture,\u201d says Kleijn. \n               Green sleeves \n             His interpretation was not popular with everyone at the Wageningen meeting. British conservationists, for example, questioned the EASY team's decision to study a British scheme that pays farmers to leave strips of grass at the edges of their arable fields, saying that it was never designed to protect rare species. \u201cIt would be a bit nuts to criticize a scheme for not delivering something it wasn't designed to do,\u201d says Juliet Vickery, head of terrestrial ecology at the British Trust for Ornithology in Norfolk. Britain's Environmental Stewardship programme of EU-approved agri-environment schemes has a small upper tier of focused, complex schemes aimed at preserving habitat for particular species, and a far broader lower tier of things such as grassy verges, which UK conservationists expect nearly all farmers to adopt. The programme has a strong research base, with monitoring planned for 2,000 sites. \u201cIt's the best model for these schemes in the world,\u201d says Armstrong-Brown. \u201cThere's a tremendous amount of expertise in Britain,\u201d agrees John Finn, an ecologist with Teagasc, the Irish Agriculture and Food Development Authority, \u201cbut it isn't yet being shared effectively with other countries.\u201d The model for the upper tier of these efforts is the campaign to improve the lot of the cirl bunting ( Emberiza cirlus ). This English farmland bird was once widespread, but by the 1980s it had been restricted to a small part of the southwest \u2014 although it was still common in other countries. A sustained conservation effort has paid farmers to leave stubble for winter food and to preserve grassland as a source of summer insects. Together with intensive support from the Royal Society for the Protection of Birds, this has seen the population expand sixfold in less than two decades 5 . \n               To cap it all \n             Kleijn accepts that intensive conservation efforts can work, but by the end of the Wageningen conference he still feared that, without focus, few schemes would achieve tangible goals. Others, however, left the meeting worried that such an emphasis on measurable results could be counterproductive. \u201cThe EASY studies are very sound, but the take-home message tends to overstate the case,\u201d says Simon Mortimer of the University of Reading, UK. \u201cIt could potentially be quite damaging to a policy that has had benefits.\u201d Just because the benefits so far have been small, he adds, does not mean the schemes aren't working. And the failures can be understood by analysing what farmers do, and how they respond to incentives, says J\u00f8rgen Primdahl, an environmental scientist at the Royal Veterinary and Agricultural University in Frederiksberg, Denmark. Kleijn's study would have been stronger had it done this, Primdahl maintains. \u201cJust looking at the outcomes of schemes, without any reference to agricultural practices, is not very helpful,\u201d he says. \u201cIf you have good models, you can explain why a scheme doesn't work.\u201d In the long term, the schemes may fare well despite a lack of clear data on their results. The public is more likely to approve of paying farmers to manage the landscape than to produce food surpluses, says Hans-J\u00f6rg Lutzeyer of the European Commission, one of only two commission officials to make it to Wageningen. While centralized EU funding for agri-environment schemes is being cut, member states have increased their freedom to switch money into environmental measures from other farm subsidies. World Trade Organization rules, which prohibit subsidies for food production but allow them for the environment, add to the attractions of agri-environment projects for Europe. Such schemes may not be the best way to promote the preservation of endangered species. Sutherland, for one, argues that Europe might do better to allow some areas to revert to a state close to wilderness while others are intensively farmed, and then to manage the whole system so as to maximize leisure, flood protection, and water quality. He says biodiversity benefits would accrue even if not particularly targeted 6 . But Europeans like farmland landscapes, and will probably continue to try and convince themselves that there are practical ways to keep areas that are rich in wildlife and pleasing to the eye, which also produce cheap food and don't pollute streams and rivers. \u201cIt's worth remembering how weird Western European conservation is,\u201d Sutherland says. \u201cIf you tell American, Asian or African conservationists that we farm in national parks, they look at you in disbelief.\u201d \n                     Birds fly in the face of 'green' farming incentive scheme \n                   \n                     Agri-environment schemes do not effectively protect biodiversity in Dutch agricultural landscapes \n                   \n                     Conservation: Dollars and sense \n                   \n                     Organic farming in focus \n                   \n                     EASY \n                   \n                     The agri-environmental footprint project \n                   \n                     UK environmental stewardship \n                   Reprints and Permissions"},
{"file_id": "440020a", "url": "https://www.nature.com/articles/440020a", "year": 2006, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "Some say that life began in fire. Hauke Trinks thinks it began in ice, and is bent on taking the hard route to prove it. Quirin Schiermeier tells the Arctic adventurer's tale. Hauke Trinks is obsessed with ice, specifically sea ice \u2014 ordinary frozen sea water. What others see as a frozen wasteland, this 63-year-old German physicist sees as the kind of place where the first chemical steps that led to life may have occurred. Trinks has undertaken daring Arctic expeditions to investigate whether, in the freezing conditions, the right mix of chemicals produces RNA molecules; many think these could have been the basis of early life. Recent lab experiments seem to lend support to Trinks's hypothesis that RNA molecules could have got their start in the frigid, rigid molecules of sea ice. Trinks's approach to the problem is unusual. In 1999, the former president of the Hamburg University of Technology sailed alone for a year in a fjord in the Norwegian archipelago Svalbard, and set up a floating research laboratory on the boat. His adventures there sound like a Jack London novel. A polar bear tore apart his rubber dinghy. When he ran out of food, he switched to dog food and hunted seals. When he got toothache, he pulled out the culprit with pliers \u2014 with plenty of rum serving as anaesthetic. Despite the hardships, two years later, Trinks wanted to go back. The governor of the archipelago forbade him to go alone, so this time he took along former librarian Marie Ti\u00e8che, whom he had met one night in a Svalbard pub. For several months in complete darkness, Ti\u00e8che held the fort while Trinks studied the physical and chemical properties of sea ice. \u201cIt's a rather tough job out there \u2014 you spend most of your time and energy just trying to survive,\u201d he says. The sea ice is a far cry from the toasty environments, such as hydrothermal vents or warm ponds, that others have proposed as possible locations for the origin of life. But some scientists agree with Trinks: ice could be just right. Complex RNA molecules often degrade at high temperature, and liquid water is thought to be too reactive to support the chemical evolution of RNA. A frigid environment, possibly on an ice-encrusted \u2018snowball Earth\u2019, could therefore be more hospitable. Cold could even facilitate the chemical reactions needed for nucleotides \u2014 the building-blocks of RNA \u2014 to link together into RNA molecules. Perhaps minerals trapped in the sea ice, or surface effects of ice crystals, assist the reactions, says Gerald Joyce, a molecular biologist at the Scripps Research Institute in La Jolla, California. \n               Deep frozen \n             By measuring the physical features of sea ice, such as its electrical properties, Trinks hopes to better understand the kind of environment in which he thinks early chemical reactions may have taken place. But because other biological molecules permeate the ice, testing whether sea ice provides ideal conditions in which the nucleotides of RNA link up is next to impossible out in the field. To do this, Trinks joined forces with Christof Biebricher, a biochemist at the Max Planck Institute for Biophysical Chemistry in G\u00f6ttingen, Germany. Biebricher created artificial sea ice in a freezer, turning the freezer off and on every few hours to simulate the temperature fluctuations of the Arctic. He added a group of nucleotides, and waited. A year later and a half later, Biebricher thawed and analysed the samples. He found a rich harvest of RNA, with nucleotides linked together in chains more than an order of magnitude longer than those seen in other origin-of-life studies \u2014 including the famous experiments done at higher temperatures in the 1960s and 1970s by chemist Leslie Orgel. Orgel himself encouraged Trinks and Biebricher to submit their paper, and its publication in October 1  caused a ripple of excitement in the chemical-evolution community. \u201cThis is fascinating,\u201d says Donna Blackmond, a physical chemist at Imperial College London. \u201cStudies like this really help us think about where to go next experimentally. Maybe we should look at lower temperatures.\u201d Trinks's paper takes a long run-up, first describing the complex properties of sea ice. Sea ice is quite different from freshwater ice, he explains.It's made of salt-free ice crystals enveloped by membrane-like layers of water, highly concentrated brine, carbon dioxide bubbles and salt crystals. The network of countless channels and compartments could provide surfaces on which RNA molecules could assemble and grow. Many scientists working on the problem think that life's last common ancestor, which arose more than 3.5 billion years ago, was heat-loving. But this does not disprove an icy beginning, says Alexander Vlassov, a biochemist with the biotech firm SomaGenics, based in Santa Cruz, California, and author of a recent review on the subject 2 . The earliest RNA molecules could have started out in cold environments, before moving to warmer ones. Still, every hypothesis about the origin of life is impossible to prove conclusively. And even if Trinks's idea stands up, it would solve only a part of the problem, says Joyce. The question of where the initial building-blocks came from remains unanswered, for example. In the meantime, Biebricher is busy replicating and improving his experiment. And Trinks left Germany on 11 February for Spitsbergen, where he is once again taking measurements of his beloved sea ice. \n                     Origins of life: Born in a watery commune \n                   \n                     Stirring the primordial soup \n                   \n                     Arctic research: Summer in Svalbard \n                   \n                     Drowning polar bears worry researchers \n                   \n                     The Arctic is on thin ice \n                   \n                     Origin of Life, by Leslie Orgel \n                   Reprints and Permissions"},
{"file_id": "440016a", "url": "https://www.nature.com/articles/440016a", "year": 2006, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "Last autumn's deadly earthquake caught Pakistan's government and scientific community off guard. Now a handful of officials and academics are struggling to bring the country up to code. Geoff Brumfiel reports from the scene. On the morning of Sunday 8 October, residents of Pakistan's capital, Islamabad, were shaken from their beds. The city was in the grip of an earthquake so violent that people could barely stand to flee. On the western side of town, a poorly constructed apartment building collapsed, killing dozens. Closer to the quake's epicentre, the story was worse. In the northern city of Muzaffarabad, concrete buildings pancaked, killing their residents instantly. Massive landslides wiped out villages perched on steep mountain slopes, and falling rock severed the narrow highway that connects the mountainous region to the rest of the country. Phone lines were down, but rumours were flying. Some said nuclear testing had caused the event. The earthquake caught most Pakistanis off guard, says Khawaja Azam Ali, dean of the faculty of natural sciences at Quaid-i-Azam University in Islamabad. The last time Pakistan experienced a massively destructive earthquake was in 1935, when the western city of Quetta was flattened, killing more than 30,000 people. This time, says Ali, \u201cknowledge about the earthquake risk was zero. There were simply not many people thinking about it.\u201d That's surprising, because winding through the foothills just beyond Ali's office window is a geological fault hundreds of kilometres long. It is one of about half a dozen similar faults running more or less through the heart of Islamabad. They are a silent reminder that this troubled nation \u2014 home to grinding poverty, tribal insurgencies, radical Islam, and more than two dozen nuclear warheads \u2014 lies in one of the world's most seismically active regions. In the wake of last October's disaster, which killed at least 73,000 people, a fledgling movement has emerged to prepare the nation for future earthquakes. The government has pledged millions of dollars to build a new seismic network, and local universities are ramping up programmes in seismology and earthquake engineering. But success will require a sea change in Pakistan's attitude towards seismic risk. The nation's military government will have to release sensitive data that have remained hidden. Universities must train a new community of seismologists. And ultimately, ordinary people \u2014 many of them illiterate \u2014 will have to be educated about how to survive quakes. This effort will take collaboration between the military, civilian government and educational institutions on a scale never before seen in the country. But it can be done, says Ali: \u201cIf there is such cooperation, then yes, there is a chance of progress.\u201d The problem is formidable. Pakistan lies at the junction of three tectonic plates (see graphic). From the southeast, the Indian plate is sliding towards Afghanistan at some 40 millimetres each year, while from the southwest the Arabian plate moves northwards at a nearly identical clip. Caught in this geological pincer movement is a small promontory of the massive Eurasian plate. As the Indian and Arabian plates plough into Eurasia, they push up the Himalayan and Hindu Kush ranges in northern Pakistan, and a series of smaller ranges along its borders with Afghanistan and Iran. The enormous compression warps and tears the plates, creating hundreds of active faults throughout the country. Nearly all of the major faults have been catalogued by the Geological Survey of Pakistan. But the survey did little beyond mapping each fault. To assess the danger, seismologists need a record of the tiny tremors that rattle the country almost daily. Collecting data from multiple seismic stations on those smaller quakes shows which faults are currently under stress. Such \u2018microseismic\u2019 data have been collected for years by three public agencies: the Water and Power Development Authority, which oversees the nation's dams; the Atomic Energy Commission; and the Meteorological Department, which is formally responsible for seismic monitoring. But historically, that information has not been made public, according to Qamar-Uz-Zaman Chaudhry, director-general of the meteorological department. One main reason, he says, was the army's concern that the data could be used to monitor explosive tests related to the nation's nuclear programme in the lead-up to its 1998 nuclear tests. \u201cPreviously there were fears about nuclear monitoring, and that has hampered our cooperation with other countries,\u201d Chaudhry says. Pakistan's army has another reason for secrecy: the country's most active seismic areas are in the contested region of Kashmir. Since the partition of India and Pakistan in 1947, both countries have laid claim to the region. To this day, roughly 100,000 Indian and Pakistani troops stare at each other across a delicate line of control. Fearing that either side could gain even a tiny edge from geospatial measurements, \u201cboth India and Pakistan have said that nobody can do any kind of GPS measurements\u201d, says Jack Schroder, a seismologist at the University of Nebraska in Omaha who has worked in Pakistan. \u201cThis paranoia is completely unfounded,\u201d contends Roger Bilham, a geologist at the University of Colorado, Boulder. \u201cAerial photographs are available to everybody, maps were left by the British, and if you want to know exactly where something is, you can get a GPS receiver from Wal-Mart. If the government wants to save people, they've got to protect them from earthquakes, not India.\u201d In fact, there are signs that Pakistan's military government is opening up. Immediately after the October quake, government officials asked non-profit groups and relief agencies to remove satellite photos of the affected areas from their websites (see  Nature   437 , 1072\u20131073; 2005). But after a brief public outcry, the authorities quickly relented. \u201cThe government and whole country have been awakened,\u201d says Umar Farooq, director of the Institute of Geology at the University of the Punjab in Lahore, who has criticized the government for its secrecy. Chaudhry adds that the government has also begun establishing cooperative agreements with the United States, China, Iran and Japan. These would allow for the exchange of data and equipment, among other things. But Pakistan has yet to establish any working relation with India, which shares many of the same geological faults (see page 1). Such openness is a positive step, says Julian Bommer, a seismologist at Imperial College in London. But to build an effective model of seismic risk, he says, \u201cyou need to supplement microseismic information with the earthquake history\u201d. Knowing the size and frequency of quakes along individual faults gives seismologists a sense of what those faults can produce. Such information, in turn, is a major ingredient of the complex formulae that determine earthquake risks and building codes. Historical records of earthquakes in the region are spotty at best, Bommer says. To compensate, geologists must travel to individual faults and conduct trenching surveys, which analyse soil for signs of past earthquakes. Trenching is a rough way for geologists to get a sense of the timing and size of earthquakes along a particular fault. Outsiders alone cannot survey Pakistan's hundreds of faults, says Bommer, who has conducted earthquake-hazard assessments in Greece, Turkey and Iran. \u201cMy experience,\u201d he says, \u201cis that stuff that's done by flying in and out is often insufficient.\u201d In short, a comprehensive assessment of Pakistan will require homegrown talent. Unfortunately, that talent pool is currently small. The nation of 160 million is home to perhaps half a dozen researchers with PhDs in seismology and, to date, Pakistan's many universities have produced just a single seismology PhD on their own, awarded in January (see  \u2018Portrait of the seismologist as a young woman\u2019 ). The meteorological department has only 15 employees with training in the field, and none have PhDs, says Chaudhry. One employee is now working on a PhD in China. \n                     Quake aid hampered by ban on web shots \n                   \n                     UN opens access to earthquake shots \n                   \n                     Seismologist keen to get into Pakistan faces delays \n                   \n                     Pakistan Meteorology Department \n                   \n                     Geological Survey of Pakistan \n                   Reprints and Permissions"},
{"file_id": "440398a", "url": "https://www.nature.com/articles/440398a", "year": 2006, "authors": [{"name": "Philip Ball"}], "parsed_as_year": "2006_or_before", "body": "Despite some remaining hurdles, the mind-bending and frankly weird world of quantum computers is surprisingly close.  Philip Ball  finds out how these unusual machines will earn their keep. Five years ago, if you'd have asked anyone working in quantum computing how long it would take to make a genuinely useful machine, they'd probably have said it was too far off even to guess. But not any longer. \u201cA useful computer by 2020 is realistic,\u201d says Andrew Steane of the quantum-computing group at the University of Oxford, UK. David Deutsch, the Oxford physicist who more or less came up with the idea of quantum computation, agrees. Given recent theoretical advances, he is optimistic that a practical quantum computer \u201cmay well be achieved within the next decade\u201d. This excitement is, however, tempered by the hurdles that have yet to be overcome. Building a quantum computer is still very, very hard to do. This is partly because it involves making quantum systems do things that don't come naturally to them. \u201cThere is progress, but it's still very slow,\u201d says physicist Chris Monroe of the University of Michigan in Ann Arbor. And even if we did have a working quantum computer today, there are hardly any programs that could run on it. In fact, it is likely that even once the machines are available, quantum computers are destined to remain niche products \u2014 excellent for certain tasks but not versatile devices like conventional personal computers. \u201cQuantum computers will almost certainly never become general-purpose desktop machines,\u201d concedes Isaac Chuang, a quantum physicist at the Massachusetts Institute of Technology (MIT) in Cambridge. Nevertheless, as a scientific research tool the quantum computer could be revolutionary because of its ability to simulate other quantum systems. In conventional, or classical, computers, information is stored as strings of bits: binary digits each of which can take the value of 0 or 1. The same is true for quantum computers, except that this time the binary digits \u2014 \u2018qubits\u2019 \u2014 are stored in the quantum states of microscopic systems, such as the electronic state of an atom or ion. So by its very nature, a quantum machine should be much better suited to simulating quantum systems than a classical computer. A quantum simulator would describe and predict the structure and reactivity of molecules and materials by accurately capturing their fundamental quantum nature. This is the sort of employment the early machines are likely to find: doing calculations of interest to chemists, materials scientists and possibly molecular biologists, says Steane. \u201cJust a few dozen qubits may shed light on other physics problems that are intractable with conventional computers,\u201d notes Monroe. \u201cThere are models of high-temperature superconductivity and other condensed-matter systems that might be approached in such a quantum simulator.\u201d \n               In a spin \n             In fact, quantum simulations can already be done using atoms and molecules that store qubits in their nuclear spin and can be probed and manipulated using nuclear magnetic resonance (NMR) techniques. In their own terms, these \u2018computers\u2019 \u201crun rings around any classical supercomputer\u201d, says Seth Lloyd, a theorist at MIT. He and his MIT colleague David Cory have been using this technique to simulate a variety of quantum systems in crystals of calcium fluoride and other materials. \u201cAs the crystal contains a billion billion spins, these simulations remain out of the reach of the most powerful classical computers,\u201d says Lloyd. The approach remains limited in terms of the different systems it can simulate, although Lloyd anticipates that fully functioning simulators will be readily available by 2020. The key to the potential success of quantum computers is also the cause of the problems within the field: the quantum nature of data storage and manipulation. In classical computers, bits have clearly defined values of 1 or 0, but the laws of quantum mechanics allow qubits to exist in a \u2018superposition\u2019 of states \u2014 a mixture of both 1 and 0 that would be impossible in an everyday computer. This means that a quantum computer has much greater capacity for storing information. A quantum processor can also compute with more than one qubit at once by exploiting another quantum property called entanglement, which makes qubits interdependent. The weird nature of the entangled state means that a measurement on one qubit instantly affects another, even though their previous individual states were undefined until that moment. Entangled states don't readily exist in nature: quantum engineers have to make them by allowing qubits to interact with one another. By exploiting superpositions, a single quantum computer in effect mimics a whole suite of classical computers running at once, and by using entanglement these \u2018parallel computers\u2019 can be linked together. Unfortunately, this powerful parallel processor has an Achilles' heel. A quantum superposition has to remain stable for at least as long as it takes to do the computation. But as soon as qubits interact with their environment, the delicate superposition becomes unstable, a process known as decoherence, which causes information to leak from the quantum computer. Decoherence is especially problematic for entangled states, because then the decoherence of one qubit can affect the others too. Preventing decoherence means reducing uncontrolled interactions with the environment. Cooling the quantum system to very low temperatures helps \u2014 but it may also be necessary to shield the qubits from stray electromagnetic fields. In practice, researchers have found it difficult to avoid decoherence of specific qubits for longer than a few seconds. But in principle it should be possible. \u201cFor qubits encoded in trapped ions, nobody really believes that we will ever be limited by coherence time,\u201d says Monroe. Despite the fact that qubits need to be isolated from their environment to avoid decoherence, they must interact strongly with one another, to perform computations. And it must be possible for qubits in superposition to interact strongly with the environment when needed, so that the information can be read out. It is an extraordinarily delicate balancing act, which involves rules that defy intuition and aren't even completely understood. \n               An easy mistake to make \n             Decoherence also means that, as they process qubits using logic gates, quantum computers will inevitably incur errors at a much higher rate than classical computers. \u201cThe modern transistor has an error rate of less than 1 in 10 14  or more switching events. In comparison, the best quantum gates we currently imagine will optimistically have an error rate of something like 1 in 10 7 ,\u201dsays Chuang. Some researchers thought at first that this would make quantum computers too error-prone to be useful. But thanks to quantum error-correcting codes devised in the 1990s 1 , 2 , it is now possible to correct error rates as high as 1 in 10 5 . By 2002 the key principles behind a quantum computer had been sketched out by theorists (see  \u2018How to build a quantum computer\u2019 ), but how best to implement them in a real device remains a wide-open question. Much of the current effort is focused on making quantum computers using atoms or ions that are held in a trap. In an ion-trap computer, the qubits are encoded in the electronic states of ions that are confined by an electromagnetic field. The ions interact with each other through electrostatic repulsion, and can be entangled by using laser beams to make them jiggle in unison. The quantum states of the ions can be read out by using other lasers to excite fluorescence, the wavelength of which depends on the ion's electronic state. \n                     Ion trap in a semiconductor chip \n                   \n                     Quantum computing gets a step closer \n                   \n                     Quantum computers go for a spin \n                   \n                     Quantum lab fits on a chip \n                   \n                     Quantum information: Flight of the qubit \n                   \n                     Computing: Quantum bits and silicon chips \n                   \n                     Quantum computing: The qubit duet \n                   \n                     Physicists flip a qubit \n                   \n                     Blame it on the bugs \n                   \n                     European Union strategic report on QC \n                   \n                     US roadmap on QC \n                   \n                     Tutorials and articles on QC \n                   \n                     Monroe web site \n                   \n                     Wineland web site \n                   \n                     QC group at Univ. Innsbruck \n                   \n                     QC group at Oxford \n                   \n                     Chuang web site \n                   \n                     Nielsen/Univ. Queensland web site \n                   Reprints and Permissions"},
{"file_id": "440402a", "url": "https://www.nature.com/articles/440402a", "year": 2006, "authors": [{"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "Tiny computers that constantly monitor ecosystems, buildings and even human bodies could turn science on its head. Declan Butler investigates.  \u201cWhat are you doing in the lab? Why aren't you out working in the field?\u201d These are not the sorts of question you usually put to your computer. But they should be, according to the proponents of a new type of information technology sometimes known as \u2018smart dust\u2019.  In their current, mostly desktop, incarnation, computers used for science usually come into their own quite late in the process of inquiry. Questions are asked, the data that might answer them identified, that data gathered \u2014 and only then does the computer start to play a role. In the future, this set up could be reversed. Computers could go from being back-office number-crunchers to field operatives. Twenty-four hours a day, year-in, year-out, they could measure every conceivable variable of an ecosystem or a human body, at whatever scale might be appropriate, from the nanometric to the continental. These new computers would take the form of networks of sensors with data-processing and transmission facilities built in. Millions or billions of tiny computers \u2014 called \u2018motes\u2019, \u2018nodes\u2019 or \u2018pods\u2019 \u2014 would be embedded into the fabric of the real world. They would act in concert, sharing the data that each of them gathers so as to process them into meaningful digital representations of the world. Researchers could tap into these \u2018sensor webs\u2019 to ask new questions or test hypotheses. Even when the scientists were busy elsewhere, the webs would go on analysing events autonomously, modifying their behaviour to suit their changing experience of the world. If this scenario sounds far fetched, imagine the owner of a mainframe in the 1970s asking why it wasn't sitting on millions of desks and laps worldwide. An absurd question \u2014 to which the answer was \u201cit's just a matter of time\u201d. The world's stock of computing power, and the number of devices over which it is distributed, has increased exponentially since then, as has the capacity of networking technology. These trends show no sign of slowing down, and that makes pervasive sensor nets not so much possible as inevitable. One does not need to be a visionary to see that soon, tiny devices with the power of today's desktops will be cheap enough to put everywhere.  Gaetano Borriello, a computer scientist at the University of Washington in Seattle, argues that such widely distributed computing power will trigger a paradigm shift as great as that brought about by the development of experimental science itself. \u201cWe will be getting real-time data from the physical world for the first time on a large scale.\u201d Instead of painstakingly collecting their own data, researchers will be able to mine up-to-the-minute databases on every aspect of the environment \u2014 the understanding of diseases, and the efficacy of treatments will be dissected by ceaselessly monitoring huge clinical populations. \u201cIt will be a very different way of thinking, sifting through the data to find patterns,\u201d says Borriello, who works on integrating medical sensors \u2014 such as continuous monitors of heart rate and blood oxygen \u2014 with their surroundings. \u201cThere will be a much more rapid cycle of hypothesis generation and testing than we have now.\u201d  Mallikarjun Shankar, who works on sensor webs for military and homeland security at Oak Ridge National Laboratory in Tennessee, agrees. \u201cIf one looks at the landscape of computing, this is where it will link with the physical world \u2014 where computing science hits the tangible, the palpable world. It is the next frontier.\u201d \n               From virtual to actual \n             Things don't get much more palpable than the experience of having an offshoot of Europe's biggest ice sheet grinding you into the granite below. That is the lot in life of the sensor web that Kirk Martinez, of the University of Southampton, UK, has been running for the past few years. He is helping glaciologists to study the dynamics of the Briksdalsbreen glacier in northwest Norway, part of the Jostedalsbreen ice field, in the hope of better understanding the impact of climate change and weather patterns on the ice field 1 .  Martinez's team uses a hot-water \u2018drill\u2019 to place pods \u2014 a dozen at any one time \u2014 at different locations deep under the ice. Each pod is equipped with sensors that measure variables such as temperature, pressure, and movement; the data collected are used to work out the rate of flow of the glacier and to model subglacial dynamics. The sensor web can be programmed in such a way that the individual pods cooperate. \u201cYou can get the pods talking to each other, and deciding that nothing much has happened recently as most of our readings have been the same, so lets the rest of us go to sleep and save batteries, with one waking us up if something starts happening,\u201d says Martinez. Martinez himself is a computer scientist, not a glaciologist. He was drawn to the task of remotely monitoring a hostile environment around the clock because he wanted the challenge of trying to bring together the various different technologies such sensor webs need. \u201cThis is very, very technologically tricky stuff,\u201d he says. For non-computer scientists, it is even trickier. Researchers can already buy pods the size of cigarette packets or credit cards off the shelf from a slew of new companies such as CrossBow and Dust Networks (both based in the Bay Area of California). But that's only the start. At present, creating a sensor web for a specific scientific application requires extensive customization, particularly in the programming. Katalin Szlavecz, an ecologist at Johns Hopkins University, in Baltimore, Maryland, works on soil biodiversity and nutrient cycling. She was driven to sensor webs by frustration caused by trying to solve complex problems with limited data. Soil is the most complex layer in land ecosystems, but it is poorly understood, because sampling is limited by the fact that technicians must collect soil samples by hand, and analyse them back in the laboratory. \u201cWireless sensor networks could revolutionize soil ecology by providing measurements at temporal and spatial granularities previously impossible,\u201d she says. \n               Data stream \n             Last September, Szlavecz deployed ten motes along the edge of a little stream just outside the university campus. Each mote measures soil moisture and temperature every minute, and the network transmits its data periodically to a computer in her office.  It sounds simple, but just to get the pods up-and-running she had to create a multidisplinary team, involving computer scientists, physicists and a small army of student programmers. Szlavecz says that she has \u201cno doubt\u201d that pod networks will take off widely, but that it won't happen until they are easier to deploy. And cheaper: \u201cEach unit costs around US$300, but if you include all the hours of student time, each works out closer to $1,000.\u201d Despite this, Szlavecz says, the Microsoft-funded pilot was a success, revealing previously unobserved variations in soil microclimate, and showing how rain affects wetting and drying cycles 2 . (Szlavecz's colleague Alexander Szalay is an author of the commentary on  page 413 ). (See  Milestones in scientific computing: Interactive timeline ) \n               Handful of dust \n              Difficulties like Szlavecz's are all too common, says Kris Pister, founder and chief technology officer of Dust Networks. It was Pister who, at the University of California, Berkeley, coined the term smart dust to describe his vision of sensors smaller than the eye could see joined into networks larger than the mind could comprehend. Pister built prototype sensor webs with funding from the Defense Advanced Projects Research Agency, which is interested in the technology's military applications. But he says it was the desire to create more usable systems that led him to get the commercial backing to create Dust Networks. \u201cIt was very frustrating that while we could do spectacular demos, we couldn't give scientists something off-the-shelf, to put in a tree or a river. What people want is the ability to just put sensors out in the environment and get data back.\u201d He likens the current stage of sensor-web development to the early days of computing. \u201cThere are a group of experts at the cutting edge of sensor webs, who have the time and expertise to go in and learn how to use the tools and do all the neat stuff,\u201d he says. \u201cBut for people who are not experts it has been difficult to get in and use it.\u201d He predicts, however, that just as the first web browser, Mosaic, and its successor, Netscape, sparked mass take-up of the World Wide Web, so future, more user-friendly sensor-web tools will generate interest. Although Pister is interested in scientific applications, his key target is the lucrative industrial market for control systems. He has been contracted by the US Department of Energy to help build \u2018intelligent\u2019 self-monitoring lighting systems for factories, offices and homes; the 600 billion kilowatt-hours of lighting used for this purpose account for 30% of total building electricity use. \u201cThe next step is about really getting some standards and commercial adoption,\u201d he says. \u201cThat will drive cost down and performance up, and then scientific uses will take off.\u201d Even with today's sensor-web technology, applications for research are proliferating. The Jet Propulsion Laboratory (JPL) in Pasadena, California, which is responsible for most of NASA's planetary science, has been running nine large sensor webs to study among other things, flooding, pollution and microclimates, in settings ranging from botanical gardens to the Sevilleta National Wildlife Refuge in central New Mexico. This month, Kevin Dellin, the head of the JPL sensor-web programme, spun it off into a company, Sensorware Systems. \n               They mote be giants \n             But sensor webs currently have major limitations for people doing science in the field, says Deborah Estrin, director of the Center for Embedded Networked Sensing in Los Angeles, California, which operates a suite of land- and sea-based monitoring projects in collaboration with university groups. Estrin says that sensor webs alone are often not sufficient for all monitoring needs, and that the cost of sensors prohibits researchers from obtaining the pod densities often needed for detailed field experiments. Estrin sees the sensor-web revolution as an important thread in a grander tapestry of global monitoring, which involves billions of dollars being poured into projects to monitor the continents and oceans. The US National Science Foundation's Ocean Observatories Initiative (OOI), for example, plans to spend $300 million over the next six years on gigabit \u2018backbones\u2019 \u2014 fibre-optic carriers of data \u2014 across the floor of the Pacific Ocean. On land, the planned National Ecological Observatory Network would enable research on terrestrial ecosystems at regional to continental scales in real time. And underneath the surface, the EarthScope project would explore the four-dimensional structure of the North American continent from crust to core. Integrating local sensor webs and all these other networks is one of the biggest challenges facing the development of observational science, Estrin says. Such mega-observatories may seem very different from Szlavecz's handful of little sensors alongside a stream in Baltimore. But the principles behind them are strikingly similar: to suck the best real-time data out of the environment as possible. \u201cInstead of handling individual data files you will be handling continual streams of data\u201d, says Robert Detrick, chair of the OOI's executive steering committee. \u201cYou will be combining inputs from different sensors interactively to construct virtual observatories: sensors will be in everything.\u201d  The OOI's aim is to get around the fact that oceanographers tend to see only what their research vessels happen to be traversing at any given time. Checking in on the ocean fibre-optic backbone will be swarms of tiny autonomous submarines. These will carry sensors, go off on sampling missions and return to the backbone to recharge their batteries and upload data. \u201cThey can all communicate with one another acoustically,\u201d Detrick enthuses. \u201cOne can say, \u2018Hey, I've found an anomaly over here, so come on over\u2019\u201d. Static sensors will monitor tectonic plates continuously along their entire length. Episodic events such as earthquakes, volcanic eruptions, and instabilities in ocean currents will be captured in real time, something that is impossible to do using ships. The existence of such large networks points to some major challenges down the line, says Estrin. Sensor webs will frequently be just single layers in a stack of data-collecting systems. These will extract information at different temporal and spatial scales, from satellite remote-sensing data down to  in situ  measurements. Managing these stacks will require massive amounts of machine-to-machine communication, so a major challenge is to develop new standards and operating systems that will allow the various networks to understand each other. Sensors and networks of sensors will need to be able to communicate what their data are about, how they captured and calibrated them, who is allowed to see them, and how they should be presented differently to users with different needs. The lack of standards is not an insoluble problem for sensor webs, says Shankar \u201cbut it is slowing the field down by several years\u201d. \n               Catching the moment \n             Despite the difficulties, the use of sensor webs continues to grow. For all the trouble her first ten pods caused her, Szlavecz is upgrading to a network of 200. And experts see no fundamental obstacles to their eventual ubiquity. \u201cWe are well on the road to getting there, and I would argue that on many points we are already there,\u201d says Borriello. By 2020, says Estrin, researchers using visualization tools like Google Earth will be able to zoom in, not just on an old satellite image, but on \u201cmultiple  in situ  observations of the Earth in real time\u201d. Data networks will have gone from being the repositories of science to its starting point. When researchers look back on the days when computers were thought of only as desktops and portables, our world may look as strange to them as their envisaged one does to us. Although we might imagine a science based so much on computing as being distanced from life's nitty gritty, future researchers may look back on today's world as the one that is more abstracted. To them the science practised now may, ironically, look like a sort of virtual reality, constrained by the artificialities of data selection and lab analysis: a science not yet ready to capture the essence of the real world. \n                     Robot wars \n                   \n                     Cash shortall threatens to rock US geophysics project \n                   \n                     Earth observing: Something to watch over us \n                   \n                     Oceanography: All wired up \n                   \n                     Report raises hopes for grand network of US ecology centres \n                   \n                     Web resources on sensor webs \n                   \n                     Glacsweb, glacier project \n                   \n                     ORNL SensorNet \n                   \n                     Life under your feet \n                   \n                     Dust Networks \n                   \n                     Center for Embedded Networked Sensing \n                   \n                     Sensorware Systems \n                   Reprints and Permissions"},
{"file_id": "440399a", "url": "https://www.nature.com/articles/440399a", "year": 2006, "authors": [{"name": "Jacqueline Ruttimann"}], "parsed_as_year": "2006_or_before", "body": "\n               View an interactive version of the timeline \n             \n               View an interactive version of the timeline \n             \n               PRE 1960s: PROTOTYPES \n             1946  ENIAC, widely thought of as the first electronic digital computer, is formally unveiled. Designed to compute ballistics during the Second World War, it performs calculations in a variety of scientific fields including randomnumber studies, wind-tunnel design and weather prediction. Its first 24-hour forecast takes about 24 hours to do. 1951  Marvin Minsky, later of the Massachusetts Institute of Technology (MIT), builds SNARC, the first machine to mimic a network of neurons. 1954  John Backus and his team at IBM begin developing the scientific programming language Fortran. 1956  Building on earlier experiments at the University of Manchester, UK, and elsewhere, MANIAC at the Los Alamos National Laboratory in New Mexico becomes the first computer to play a full game of chess. In 1996, IBM's Deep Blue computer will defeat world chess champion Garry Kasparov. 1959  John Kendrew of the University of Cambridge, UK, uses computers to build an atomic model of myoglobin using crystallography data. \n               1960s: MAINFRAMES \n             1962  Charles Molnar and Wesley Clark at MIT's Lincoln Laboratory design the Laboratory Instrument Computer (LINC) for researchers at the National Institutes of Health. It is the first lab-based computer to process data in real time. 1963  In California, the Rancho Arm becomes the first artificial robot arm to be controlled by a computer. 1966  Cyrus Levinthal at MIT designs the first program to represent and interpret protein structures. 1967  ARPANET \u2014 the predecessor of the Internet \u2014 is proposed by the US Department of Defense for research networking. 1969  Results of the first coupled ocean\u2013atmosphere general circulation model are published by Syukuro Manabe and Kirk Bryan, paving the way for later climate simulations that become a powerful tool in research on global warming. \n               1970s: WORKSTATIONS \n             1971  Computing power shows its potential in medical imagery with a prototype of the first computerized tomography (CT) scanner. 1971  The Protein Data Bank is established at Brookhaven National Laboratory in Upton, New York. 1972  Hewlett Packard releases the HP-35, the first hand-held scientific calculator, rendering the engineer's slide rule obsolete. 1976  At Los Alamos, Seymour Cray installs the first Cray supercomputer, which can process large amounts of data at fast speeds. \n               1980s: PERSONAL COMPUTERS... \n             1983  Danny Hillis develops the Connection Machine, the first supercomputer to feature parallel processing. It is used for artificial intelligence and fluid-flow simulations. 1985  After receiving reports of a lack of high-end computing resources for academics, the US National Science Foundation establishes five national supercomputing centres. 1989  Tim Berners-Lee of the particle-physics laboratory CERN in Geneva develops the World Wide Web \u2014 to help physicists around the globe to collaborate on research. \n               1990s: INTERNET... \n             1990  The widely used bioinformatics program Basic Local Alignment Search Tool (BLAST) is developed, enabling quick database searches for specific sequences of amino acids or base pairs. 1996  George Woltman combines disparate databases and launches the Great Internet Mersenne Prime Search. It has found nine of the largest known Mersenne prime numbers (of the form 2 n \u22121), including one that is 9,152,052 digits long. 1996  Craig Venter develops the shotgun technique, which uses computers to piece together large fragments of DNA code and hastens the sequencing of the entire human genome. 1998  The first working quantum computers based on nuclear magnetic resonance are developed. \n               21st CENTURY: IMPLICIT COMPUTING... \n             2001  The National Virtual Observatory project gets under way in the United States, developing methods for mining huge astronomical data sets. 2001  The US National Institutes of Health launches the Biomedical Informatics Research Network (BIRN), a grid of supercomputers designed to let multiple institutions share data. 2002  The Earth Simulator supercomputer comes online in Japan, performing more than 35 trillion calculations each second in its quest to model planetary processes. 2005  The IBM Blue Gene family of computers is expanded to include Blue Brain, an effort to model neural behaviour in the neocortex \u2014 the most complex part of the brain. 2007  CERN's Large Hadron Collider in Switzerland, the world's largest particle accelerator, is slated to come online. The flood of data it delivers will demand more processing power than ever before. Reprints and Permissions"},
{"file_id": "440268a", "url": "https://www.nature.com/articles/440268a", "year": 2006, "authors": [{"name": "Jenny Hogan"}], "parsed_as_year": "2006_or_before", "body": "A series of mental challenges is helping physicists to prepare for the strange data they may get when the next particle accelerator goes live. Jenny Hogan joins the work-out. Normally, the trick to learning something at a scientific meeting is to listen to the key lectures. But one afternoon last month, in a conference room at CERN, the European particle-physics lab near Geneva, physicist Matt Strassler managed to convince several researchers that they might learn more if they left the lecture room. He wanted them to avoid hearing the solution to a puzzle they had been working on for months. Welcome to the strange world of the Large Hadron Collider (LHC) Olympics, a workshop held at CERN in which teams of theorists studied fake data in order to explore unproven theories. Strassler, a theorist from the University of Washington in Seattle, was one of the organizers of the event, which brought together more than 50 theoretical physicists from across Europe and the United States. These \u2018olympians\u2019 have devoted their careers to building mathematical models of the Universe and matter. The LHC Olympics was designed to put their ideas to the test; their challenge was to prepare for the real data expected to emerge from one of the biggest experiments in physics. Strassler and his colleagues had accepted that challenge \u2014and they weren't going to give up on their first attempt. That experiment is the LHC, a machine under construction in the tunnels beneath CERN. When it is switched on in 2007, the collider will be the world's most powerful particle accelerator; it is expected to churn out data that will test some of the most cherished theories of physics. But if something turns up in the LHC that no one has predicted, theorists will have to work backwards from a set of observations to try to find a model that fits. \u201cEveryone in this room wants to be prepared so that when the first slide of the first data from the LHC is presented, we will understand what we are seeing,\u201d says Jesse Thaler, a physics PhD student from Harvard University in Cambridge, Massachusetts. Studying the debris from collisions in particle accelerators has already helped physicists piece together many details of the subatomic world. But the LHC is pushing particle physics to a new frontier. When high-speed protons smash head-on in the LHC, obliterating each other, seven times more energy will be released than from similar collisions in today's most powerful machine, the Tevatron at Fermilab in Batavia, Illinois. This raises the possibility that in the explosion of particles that sprays out, some new, exotic speck of matter might be found. \n               Full speed ahead \n             The experimentalists building the mammoth detectors that collect and characterize the debris will comb through the data, searching for new particles. They have many ideas of what they might find at these energies, but it is possible that there may also be some complete surprises. \u201cTo go from seeing particles to understanding theories is a big step. This is where the LHC olympians will come in,\u201d says Albert de Roeck, physics analysis coordinator for the LHC detector known as the Compact Muon Solenoid (CMS). Physicists have not had to contend with unexpected physics since the \u2018standard model\u2019, which divides matter into families of particles, was hammered out in the 1970s. \u201cThe 1960s and 1970s were the golden years for particle physics,\u201d says de Roeck. \u201cThe energy that was there then hasn't really been there since. The hope of the whole particle-physics world is that the LHC will bring that era back.\u201d The LHC Olympics was also designed to attract new people into the fray. Herman Verlinde, a string theorist from Princeton University, New Jersey, is one of the newcomers who helped organize the meeting. \u201cString theorists, like any physicists, are attracted to exciting areas of physics,\u201d he explains. String theory is one contender for a deeper theory of nature, which goes beyond standard-model physics. Despite its unqualified success, the standard model contains at least 19 arbitrary parameters such as particles' masses that have to be measured from experiments. Physicists seek some greater underlying theory to explain these numbers, and have dreamed up numerous formulations that might provide it. \n               Going for gold \n             Some of these theories predict that the standard model will be supplemented by families of heavier particles, which the LHC may find. And the LHC could identify dark matter, uncover evidence for extra dimensions, or create miniature black holes. At the very least, physicists hope it will find the Higgs boson \u2014 a hypothetical particle, predicted to exist by the standard model, which is thought to endow other particles with mass. Or it might find nothing at all. \u201cI can't emphasize enough how tense and exciting this time is,\u201d says Nima Arkani-Hamed, a theorist from Harvard University, and one of the driving forces behind the LHC Olympics. Last time CERN made a big discovery, Arkani-Hamed explains, the theorists had known exactly what the experimentalists should look for. \u201cThis time,\u201d he says, \u201cthe theorists are saying \u2018maybe this will happen, maybe that will happen.\u2019\u201d Arkani-Hamed first became worried in 2004 that many theorists were not prepared for the flood of data that will soon issue from CERN. In his original vision for the LHC Olympics, experimentalists would create a simulated data set that obeyed some hidden physics from beyond the standard model, analyse it as they would the real data, and then present their plots to an audience of theorists. \u201cThe idea was that we would have a massive data challenge and then a mini conference,\u201d says Maria Spiropulu, who will work on the CMS data analysis at CERN. At the conference, the theorists would be challenged to explain the experimentalists' plot, \u201cThen they would say, \u2018Eureka! This is intersecting D-brane string-inspired supersymmetry.\u2019\u201d But right now, as the start date for the LHC looms, the experimental teams are too busy to get involved with games. \u201cWe're working 16-hour days as it is,\u201d says Spiropulu. \u201cBut these people don't give up. The theorists said, \u2018If you give us a hint, we'll do it ourselves.\u2019\u201d So, a few months before the February workshop, several theorists put together three simulated data sets. These \u2018black boxes\u2019 were then posted on a website for other meeting participants to download. Could the conference-goers work out, from the plots and tables, what physics the data described? As you might expect, says Spiropulu, the theorists' data analysis was very basic. \u201cBut for them, this was training so that they understand the language that we are using.\u201d With real LHC data, one of the big problems will be distilling the bumps that signify new particles from the \u2018background\u2019 events produced by standard-model physics. No background was included in the theorists' black boxes. Ian Hinchliffe at the Lawrence Berkeley National Laboratory in California, a physicist on the ATLAS detector team, worries about this omission. \u201cIt might lead some theorists to wrong conclusions about what is possible,\u201d says Hinchliffe, who ran a data challenge for the people working on ATLAS, which included the necessary background events. That experience, says Hinchliffe, taught them how hard it might be to recognize new events. \n               Runners up \n             But the LHC Olympics is not a bad starting point, says Strassler. He describes the black-box exercise as \u201ca sort of flight simulator for someone who wants to understand clearly what the pilots are doing, rather than something for the pilot\u201d. And for the next meeting, planned for August, the theorists hope to develop at least one black box with background. First to present his black-box solution this time was Verlinde. \u201cClearly we are not going to impress you with our skills in analysing data,\u201d he began. \u201cYou can see how far one can get, starting basically from zero, with minimal students and minimum sleep over the past few days.\u201d Another presentation came from Harvard University students, who had tested 2,500 models, using an algorithm to optimize their first guess of what particles in the black box might be. \u201cCheaters,\u201d someone called out when the team thanked the Harvard-Smithsonian Center for Astrophysics for allowing them to use its powerful computers; the heckler's own talk was titled \u201cWhat you can do with a black box, an undergraduate and two weeks\u201d. Both Verlinde and his co-workers, and the Harvard students had figured out that the box they had studied was based on a version of supersymmetry \u2014 among the leading theories for a physics beyond the standard model. And they identified the masses of most of the particles that this theory predicts \u2014 from squarks, to winos and zinos. There were some differences between the two groups, but their answers were almost the same. \u201cWe're quite proud of our guess,\u201d says Princeton University theorist Leonardo Rastelli who collaborated with Verlinde. But Strassler hadn't figured out what was going in the black box he had tackled. That's why, when its solution was to be announced, he persuaded several others who had been working on it to leave the room with him. He wanted to keep going until he cracked it. \u201cIf the black box hasn't been decoded, then the learning process isn't over.\u201d And of course, when the real results come in, there won't be anyone who can reveal the answer. It will have to be obtained through hard work. \n                     CERN: the show goes on \n                   \n                     The Grid: tomorrow's computing today \n                   \n                     Outrageous fortune \n                   \n                     LHC Olympics homepage \n                   \n                     About the LHC \n                   \n                     Hands on CERN \u2013 an interactive guide to particle physics \n                   Reprints and Permissions"},
{"file_id": "439014a", "url": "https://www.nature.com/articles/439014a", "year": 2006, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "The Afar region of Ethiopia is littered with traces of the earliest humans. Rex Dalton gets on the trail with a team of devoted experts who just live for the next find. Ethiopians have a peculiarly fitting saying about the remote province of Afar \u2014 it is \u2018where it all began\u2019. Isolated in the harsh region is a low ridge that has brought to light humanity's beginnings, by providing a wealth of fossils. In early December, under the glare of a near-equatorial sun, an international team of palaeoanthropologists scours sediments on the ridge for fossil bones, teeth or artefacts; the researchers have worked here for more than 20 years. Five years ago they identified a site called Asa Issie, or \u2018red hill\u2019, as promising ground after unseasonable torrents trapped them in the area. This year they have returned to see what has surfaced after summer rains swelled the nearby Awash river. At times stooped, sometimes crawling, but always focusing on the sea of stones that cover the ground, the crew spreads slowly through ravines. Alongside the scientists are locals, whose sharp eyes and self-taught knowledge make them integral to success. First come fossils of rhinoceros ( Ceratotherium ) and kudu (Tragelaphini), bones that provide insight into the region's former environment. Then a shout pierces the afternoon wind: \u201cCanine!\u201d A cream-coloured hominid tooth, about 4 million years old, has been spotted among the white pebbles. It is only the third day of a month-long field season, and this discovery marks the 13th consecutive year that the team has found hominid specimens. Later, as the sun sets over the mountains to the west, team co-leader Tim White says to me: \u201cThere is no other spot on the planet like this. It is a special place.\u201d There is also no other team like White's. Year after year, the Middle Awash project has identified some of the most important hominid specimens, including one of the oldest  Homo sapiens  and the 5.8-million-year-old  Ardipithicus kadabba 1 , 2 , 3 . As other palaeoanthropologists have sought older and older hominids \u2014 and the fame that comes with such discoveries \u2014 the Middle Awash team has shone by identifying new hominid characteristics that flesh out the evolutionary track from ape to man. Much of this success can be traced to the project's multinational roots. It represents the best of scientific capacity-building: African scientists receive doctorates at top universities overseas, and then return to work and nurture projects at home. Scientists from abroad, such as White \u2014 at the University of California, Berkeley \u2014 are in the minority. The team's other leaders are Ethiopians: there is Giday WoldeGabriel, a geologist at the Los Alamos National Laboratory in New Mexico; the palaeoanthropologist Berhane Asfaw, director of Ethiopia's National Museum in Addis Ababa; and Yonas Beyene, a government archaeologist. Yohannes Haile-Selassie, a key team member who received his doctorate at Berkeley, like Asfaw, is curator of physical anthropology at the Cleveland Museum of Natural History in Ohio, a bastion of hominid research. Postdocs and students come from France, Lebanon, Turkey and the United States. Everyone eats at the same table; team leaders shovel dirt just like the others. White drives the team hard, but junior scientists thirst for his feedback. One night, postdoc Michael Black \u2014 a specialist in hominid biomechanics who doubles as the camp solar electrician \u2014 waits so long for White's last instructions that he falls asleep at the dinner table. The hominid tooth was found by Ferhat Kaya, a graduate of Turkey's Ankara University who studies minuscule mammals. Desperately working on his English skills to get on a US doctoral programme, Kaya beams when White gives him a complimentary high-five and tells him: \u201cGood job, Ferhat.\u201d And everyone is pleased that the trip's first hominid discovery is made by the only trained scientist in the crew who is Muslim. \u201cFantastic,\u201d says Asfaw. \u201cIt shows the international aspect of the team.\u201d Every year, the Middle Awash team ventures into the field in November or December, when the climate is most suitable for exploration. But rains still come. Once, a French researcher desperate to leave escaped by hiring Afar tribesmen to pull him on a raft to the asphalt highway. Political upheavals have also threatened the project. Fieldwork stopped during most of the 1980s because of domestic turmoil and the writing of new antiquities laws. This year, David Perlman of the  San Francisco Chronicle  and I went to join the team, the first time journalists have been allowed to come along since its explorations began in 1981. The Afar fossil ground is only about 250 kilometres northeast of Addis Ababa, which is located in the highlands near the centre of the country (see map). But the journey down to the Middle Awash site is a gruelling and dangerous three-day drive, and the crew has to make its own way along the final stretch on to a ridge known as Bouri. \n               Scene from Afar \n             The arid land is dotted by small villages of crude huts, often abandoned when the herders seek fresh grass. To me, the hamlets seem little changed from earlier eras, but today's Afar lifestyle might be better described as \u2018Neolithic, with Kalashnikovs\u2019. Virtually every herder carries one of the Russian rifles, and some are equipped with rocket-propelled grenades. The Afar are fighting another tribe, the Issa, which pushed north from Somalia seeking grazing lands. Daily, the palaeoanthropologists seek intelligence to avoid potential firefights. Armed policemen accompany the team at all times, as do Afar sheikhs and leaders, such as tribal chief Hamed Elema \u2014 who has himself become a skilled fossil finder. A year ago, the team had to abort a trip to date new hominid specimens because a gun battle threatened. Last month, nine heavily loaded, all-wheel-drive trucks moved northeast out of Addis Ababa, through the Awash river valley and to the Bouri \u2018peninsula\u2019 \u2014 so named for its shape on the satellite images used to find sites. The drive features spectacular views of the mountains created by tectonic action along the African rift zone. The rift runs along the east of Africa; in the Afar region, the Arabian and Somalian plates pull east and away from the Nubian plate, causing giant blocks of crust to shift. Standing on the Bouri ridge, looking west toward the rift margin, the mountains resemble giant stairs: whole sections have dropped down after the tectonic expansion. To the north, the blocks have shifted and twisted, creating a complex geological terrain of ridges. To the south is Yardi Lake, a water-filled depression full of crocodiles and hippos. Rain washes the edge of the mountainous blocks, exposing fossils of various ages. But dating the specimens is an onerous chore. It can take weeks of arduous hiking to collect samples for geochemical testing. The easiest and best way to find a fossil's age is to date a volcanic tuff layer above or below it. However, when the tuffs have been altered by weather and time, they produce no datable signature. Then geologists must correlate fossil-bearing layers with distant sediments whose ages are already known. It took several years, for instance, for WoldeGabriel and Haile-Selassie to determine the age of the  A. kadabba  specimen 4 . \u201cThe geology is still ongoing,\u201d says WoldeGabriel. \u201cI need to go and check some of the more complex faulting.\u201d The remote treks put even Ethiopians in personal peril. In the late 1990s, when Haile-Selassie and WoldeGabriel were exploring the region near a sacred village, an offended Afar tribesman chased them away. \u201cHe kept his Kalashnikov on us,\u201d recalls WoldeGabriel. \u201cIt didn't make any difference that we were Ethiopians.\u201d The work was worth it, though. Their 2001 article showed that the hominids \u2014 then the earliest known \u2014 lived in a wooded environment, not a savannah as previously thought. On the Bouri peninsula, just a short drive is needed to reach interesting sediments. The early  H. sapiens  was found on the ridge, and a 1-million-year-old  H. erectus  was uncovered near the Bouri hamlet 5 , also called the \u2018hyena condominiums\u2019 because the beasts occupy the caves, bursting out when the team starts exploring. A 2.5-million-year-old  Australopithecus garhi  was discovered 6  just where the ridge descends to the Awash river \u2014 a site that is known for extremely early evidence of stone-tool cuts on antelope bones. And just across the river is the Maka site, where the team discovered the 3.4-million-year-old  A. afarensis  that sparked one of its first major publications 7 . Bouri is starkly different from the famous Hadar location, home to the 1974 discovery of the  A. afarensis  skeleton known as Lucy 8 . Just 70 kilometres to the north, Hadar is rich in hominid fossils \u2014 but they all come from the same time period, 3 million to 3.5 million years ago. Bouri, in contrast, contains varied ages and species, which help palaeoanthropologists to understand the evolving features of man. To sort through all this material, the Middle Awash team has devised sophisticated collection methods 9 . In one, \u2018the crawl\u2019, the researchers mark a promising area with surveyors' lines, then crawl across it shoulder to shoulder, removing every fossil found. Later comes \u2018maintenance\u2019, in which they return at various intervals to see what else has eroded from the plot \u2014 if fossils aren't found shortly after being exposed, they disintegrate. With field time valuable, White is trying an experiment to determine the most efficient way to monitor sites. He planted 200 casts of fossils at an Awash site. The crew will return periodically for maintenance; by seeing how many casts are found, White hopes to determine how often return visits are needed if nothing is to be missed. Such care underscores the long-term views of the team. White dislikes what he calls \u201chominid treasure hunts\u201d, where researchers move in for short field visits to grab hominids and then headlines. For the team, the cataloguing of animal fossils deserves the same care as preserving hominid fossils. During last year's field season, the team collected about 1,400 vertebrate fossils \u2014 from elephant bones to the teeth of tiny mammals, which they preserve on waxed pinheads. All are cleaned and stored at the National Museum in Addis Ababa, which holds some 15,600 vertebrate fossils. The Ethiopian government is building a major new $3.5-million research facility for the museum, and a whole wing will be set aside for the palaeoanthropology collection. Such repositories help the study of hominids, whichever team discovers them. For instance, French palaeoanthropologist Michel Brunet helped date the oldest hominid,  Sahelanthropus tchadensis 10 , in Chad using a coexistent pig ( Nyanzachoerus syrticus ) from Ethiopia's collection. The pig species is known to have disappeared about 5.7 million years ago, so Brunet knew his specimen, called Touma\u00ef, had to be at least that old. Now, a former member of Brunet's team, Jean-Renaud Boisserie of Berkeley, is in the Middle Awash seeking to augment the animal fossil record \u2014 particularly that of hippopotamuses. These creatures can provide exquisite detail on dates and palaeobiogeography. They live in water, but species differ among river basins. Boisserie seeks hippo skulls and teeth, from which he can extract carbon-isotope samples and learn about the animals' diet and environment. \n               Tuff choices \n             \u201cSo what period do you want to examine?\u201d White asks Boisserie on the drive to Bouri, offering him several options. Boisserie selects 2.5 million years. That means the team will head first to the Lubaka site near the Awash river, keeping eyes open for roaming lions. Largely barren of vegetation, the hillocks at Lubaka reflect the volcanic tuffs used to date specimens. There is also another target today: stone tools. Although the  A. garhi  specimen was found with antelope bones that had cut marks, there were no artefacts. White and Asfaw want to link tools to the hominids. After an hour and a half of hot walking, Boisserie gets his wish. Team member Kampiro Krantu finds a good hippo jaw, with teeth, that is just surfacing in a ravine. Krantu is one of the team's best searchers. He has no scientific training, and no one on the team can speak his Konso language. But his talent for spotting fossils is legendary. The team leaders are not so lucky. White spots a black basalt chopper in a hillside. But he rejects it when he notices evidence in the sediments encasing it that suggest it has been washed there from its original location. Finding a tool that is truly  in situ  must wait for another day. After a week in the field, the team cuts a new road out of the bush, making it easier for supply trucks to come and go. They begin to explore intriguing foothills south toward the rift margin. Hominid fever, however, dwindles when the only find is a giant tortoise fossil. Back at the trucks, an Afar girl waits for us with a brick-shaped elephant tooth. White jokingly suggests our police guards arrest her, and she is asked to return the fossil to its location. He worries that keeping such items encourages locals to remove fossils from their surroundings, destroying vital geological information. A week later, the team returns to ensure the fossil was replaced. And there, near the elephant tooth site, they find fresh fuel for their fever \u2014 a hominid tooth shard. Bushels of earth around it will be sieved for any remaining pieces. Tuff dating will follow, then site maintenance. A published article may be years off, but once again Afar shows where it all began. \n                     Early Pliocene hominids from Gona, Ethiopia \n                   \n                     2.5-million-year-old stone tools from Gona, Ethiopia \n                   \n                     New four-million-year-old hominid species from Kanapoi and Allia Bay, Kenya \n                   \n                     Stratigraphic placement and age of modern humans from Kibish, Ethiopia \n                   \n                     Plio\u2014Pleistocene hominid discoveries in Hadar, Ethiopia \n                   \n                     Revealing hominid origins initiative \n                   \n                     Cleveland Museum of Natural History \n                   \n                     Paper by Boisserie on Hippopotamidae \n                   \n                     \n                         Australopithecus afarensis \n                       \n                   \n                     Lucy website \n                   \n                     National Museum, Addis Ababa \n                   Reprints and Permissions"},
{"file_id": "439010a", "url": "https://www.nature.com/articles/439010a", "year": 2006, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "A growing number of cosmologists and string theorists suspect the form of our Universe is little more than a coincidence. Are these harmless thought experiments, or a challenge to science itself? Geoff Brumfiel investigates. Why are we here? It's a question that has troubled philosophers, theologians and those who've had one drink too many. But theoretical physicists have a more essentialist way of asking the question: why is there anything here at all? For two decades now, theorists in the think-big field of cosmology have been stymied by a mathematical quirk in their equations. If the number controlling the growth of the Universe since the Big Bang is just slightly too high, the Universe expands so rapidly that protons and neutrons never come close enough to bond into atoms. If it is just ever-so-slightly too small, it never expands enough, and everything remains too hot for even a single nucleus to form. Similar problems afflict the observed masses of elementary particles and the strengths of fundamental forces. In other words, if you believe the equations of the world's leading cosmologists, the probability that the Universe would turn out this way by chance are infinitesimal \u2014 one in a very large number. \u201cIt's like you're throwing darts, and the bullseye is just one part in 10 120  of the dart board,\u201d says Leonard Susskind, a string theorist based at Stanford University in California. \u201cIt's just stupid.\u201d \n               One in a zillion \n             Physicists have historically approached this predicament with the attitude that it's not just dumb luck. In their view, there must be something underlying and yet-to-be-discovered setting the value of these variables. \u201cThe idea is that we have got to work harder because some principle is missing,\u201d says David Gross, a Nobel-prizewinning theorist and director of the Kavli Institute for Theoretical Physics in Santa Barbara, California. But things have changed in the past few years, says astronomer Bernard Carr of Queen Mary, University of London, UK. String theorists and cosmologists are increasingly turning to dumb luck as an explanation. If their ideas stand up, it would mean the constants of nature are meaningless. \u201cIn the past, many people were almost violently opposed to that idea because it wasn't seen as proper science,\u201d Carr says. \u201cBut there's been a change of attitude.\u201d Much of that change stems from work showing that our Universe may not be unique. Since the early 1980s, some cosmologists have argued that multiple universes could have formed during a period of cosmic inflation that preceded the Big Bang. More recently, string theorists have calculated that there could be 10 500  universes, which is more than the number of atoms in our observable Universe. Under these circumstances, it becomes more reasonable to assume that several would turn out like ours. It's like getting zillions and zillions of darts to throw at the dart board, Susskind says. \u201cSurely, a large number of them are going to wind up in the target zone.\u201d And of course, we exist in our particular Universe because we couldn't exist anywhere else. It's an intriguing idea with just one problem, says Gross: \u201cIt's impossible to disprove.\u201d Because our Universe is, almost by definition, everything we can observe, there are no apparent measurements that would confirm whether we exist within a cosmic landscape of multiple universes, or if ours is the only one. And because we can't falsify the idea, Gross says, it isn't science. Or at least, it isn't science in any conventional sense of the word. \u201cI think Gross sees this as science taking on some of the traits of religion,\u201d says Carr. \u201cIn a sense he's correct, because things like faith and beauty are becoming a component of the discussion.\u201d And yet in the overlapping circles of cosmology and string theory, the concept of a landscape of universes is becoming the dominant view. \u201cI really hope we have a better idea in the future,\u201d says Juan Maldacena, a string theorist at the Institute for Advanced Study in Princeton, New Jersey, summing up the views of many in the field. \u201cBut this idea of a landscape is the best we have today.\u201d The stakes are high: string theorists know that pursuing an unverifiable theory could look like desperation, but they fear that looking for meaning in a meaningless set of numbers may be equally fruitless. \n               Kepler's error \n             At the core of this dilemma is a concept known as the anthropic principle: the idea that things appear the way they do because we live at a certain spot in the Universe. It's not a new concept, and has previously been regarded more as philosophy than science. But some scientists say that it offers a useful change of perspective. \u201cIt's very important to take into account stuff like this, or you can come to completely incorrect conclusions about the Universe,\u201d argues Max Tegmark, a cosmologist at the Massachusetts Institute of Technology, Cambridge. \u201cFor example, you might assume our Solar System is typical, but a typical point in space is some intergalactic void where you can't see a single star.\u201d Failing to consider our observational location has burned scientists in the past. The sixteenth-century German astronomer Johannes Kepler spent years trying to understand what seemed to be the even, geometrical spacing of our planets from the Sun. Kepler searched for meaning in the planets because he thought our Solar System was unique; today's scientists understand that our Solar System is but one of probably billions in the Galaxy. Under such circumstances it seems reasonable to assume the planets are spaced according to little more than random chance. In much the same way as Kepler worried about planetary orbits, cosmologists now puzzle over numbers such as the cosmological constant, which describes how quickly the Universe expands. The observed value is so much smaller than existing theories suggest, and yet so precisely constrained by observations, that theorists are left trying to figure out a deeper meaning for why the cosmological constant has the value it does. Many are still searching for some great unifying theory that would explain these variables. But others have started to believe that, like Kepler, today's physicists are looking for meaning where there is none. \u201cIn recent years, it was looking more and more to me like the laws of nature were environmental,\u201d says Susskind, who has just written a book making this argument (L. Susskind  The Cosmic Landscape: String Theory and the Illusion of Intelligent Design . Little Brown, 2005). He suspects that there are many universes, all with different values for these variables. Just as human life had to evolve on a planet with water, he says, perhaps we also had to evolve in a Universe where atoms could form. Until recently, Susskind was in the minority. Hints of multiple universes, however, were given by a cosmological theory known as inflation. Inflation is the leading theory of the early Universe; it postulates that a period of rapid early expansion created the flat and uniform cosmos we see today. One version of inflation theory, devised in the early 1980s, suggests that inflation occurred even before the Big Bang. In this version, the expanding cosmos was foamy and energetic, says Steven Weinberg, a researcher at the University of Texas, Austin. \u201cEvery once in a while, one part of the Universe would expand and become a Big Bang,\u201d he says. \u201cAnd these Big Bangs would all have different values for their fundamental constants.\u201d \n               Strings attached \n             In 1987, Weinberg made a prediction that turned out to support the idea of an anthropic Universe. Preliminary observations indicated that the cosmological constant was zero, but Weinberg reasoned that if the constant was constrained by our anthropic perspective then it would be small, so as not to interfere with the formation of galaxies, stars and planets, but non-zero, because it would be essentially random. \u201cThat prediction has since been confirmed by observations of supernovae and the microwave background,\u201d says Weinberg, who admits he was a reluctant convert to the idea. The latest circumstantial arguments for multiple universes come from string theory. String theory posits that tiny strings vibrating in the fabric of space-time give rise to the multitude of particles and forces in the macroscopic Universe. Although string theory lacks experimental support, it attracts broad interest because it seems to offer a route to a grand theory of everything \u2014 a way to unify relativity with quantum mechanics. But as theorists developed string theory, they discovered that the equations gave rise to multiple solutions, each of which represented a universe with different physical properties. \u201cThe hope always was that we would understand why one solution was picked out,\u201d says Joe Polchinski, a string theorist at the Kavli Institute. But despite their best efforts, after two decades theorists are still stuck with a million different solutions for the equations, and therefore a million potential universes. This landscape of solutions, as it became known in the community, was both troubling and intriguing. On the one hand, the theory stubbornly refused to yield a single solution resembling our own cosmos, but then, some argued, that might also explain the cosmological constant's apparent randomness. If these many solutions actually represent millions of universes, then the idea that one had worked out just right for us wasn't so far-fetched. \n               Ignorance is bliss \n             The snag was that one million universes wasn't enough. To explain the perfectly adjusted cosmological constant one would need at least 10 60  universes, says Polchinski. Then, in 2000, he and Raphael Bousso at the Lawrence Berkeley National Laboratory in Berkeley, California, calculated that there could be a lot more than a million solutions. \u201cThe calculation had such topological complexity that you could potentially get 10 500  universes,\u201d Polchinski says. With so many solutions, says Weinberg, it becomes easier to imagine that we happen to live in a Universe that seems tailored for our existence. Easy to imagine, hard to prove. Because other universes would be causally separated from our own, it seems impossible to tell whether our cosmos is the only one, or one of many. Most scientists find this disturbing. Talk of a Universe fine-tuned for life has already attracted supporters of intelligent design, who claim that an intelligent force shaped evolution. If there's no way to tell whether the values of scientific constants are a coincidence, the movement's followers argue, then why not also consider them evidence of God's handiwork? The anthropic reasoning behind the landscape of universes is disturbing on another level, says Gross. Most theories grow stronger with each observation that matches their predictions. However, for the anthropic principle, random chance is the main factor. Patterns and correlations, the stones from which scientific theories are built, weaken it. In other words, he says: \u201cThe power of the principle is strongest where you have ignorance.\u201d That may be, but measurements that could support anthropic reasoning are in the works. In 2007, researchers at Europe's CERN particle physics centre in Geneva, Switzerland, will turn on the Large Hadron Collider, a massive accelerator that will probe particle energies never before seen by researchers. The accelerator might detect so-called supersymmetric particles, predicted by some as a way of unifying the strong and weak nuclear forces with the electromagnetic force, an important step in uniting all the forces of physics within a single theory. These particles could also hint at whether we live in one of many universes, says Nima Arkani-Hamed, a string theorist at Harvard University in Cambridge, Massachusetts. If the collider detects certain types of super-symmetric particles, he says, it will indicate another fine-tuning in the cosmos \u2014 the ratio of the weak nuclear force to the strength of gravity. The anthropic argument is the same: if the number was off by as little as one part in 10 30 , then we would not be here to discuss it. It might seem that the detection of a second, perfectly tuned number would only exacerbate the debate, but Arkani-Hamed argues that it will have the opposite effect. Unlike the cosmological constant, which has had a controversial history even in cosmology, this fine-tuning would appear in the standard model, which most physicists consider to be the most complete physical theory ever developed and tested. It would strengthen the case for the arbitrary nature of certain fundamental constants, Arkani-Hamed contends: \u201cThese measurements wouldn't directly prove or disprove the landscape, but they would be a very big push in that direction.\u201d \n               Leap of faith \n             Still, many scientists distrust the concept and continue to seek alternative explanations. Among them is Lisa Randall, also at Harvard. Randall suspects that multiple universes are a mirage resulting from the unrefined equations of string theory. \u201cYou really need to explore alternatives before taking such radical leaps of faith,\u201d she contends. And with no foreseeable way to detect other universes, Gross feels that such leaps of faith should not be taken. \u201cI feel that it's a rather extreme conclusion to reach at this point,\u201d he says. Susskind, too, finds it \u201cdeeply, deeply troubling\u201d that there's no way to test the principle. But he is not yet ready to rule it out completely. \u201cIt would be very foolish to throw away the right answer on the basis that it doesn't conform to some criteria for what is or isn't science,\u201d he says. Gross believes that the emergence of multiple universes in science has its origins in theorists' 20-year struggle to explain the finely tuned numbers of the cosmos. \u201cPeople in string theory are very frustrated, as am I, by our inability to be more predictive after all these years,\u201d he says. But that's no excuse for using such \u201cbizarre science\u201d, he warns. \u201cIt is a dangerous business.\u201d \n                     Mysterious cosmos \n                   \n                     New model of expanding Universe \n                   \n                     Is physics watching over us? \n                   \n                     Physics ain't what it used to be \n                   \n                     Anthropic principle site \n                   \n                     Cosmologic constant site \n                   Reprints and Permissions"},
{"file_id": "439130a", "url": "https://www.nature.com/articles/439130a", "year": 2006, "authors": [{"name": "John Whitfield"}], "parsed_as_year": "2006_or_before", "body": "Could viruses have invented DNA as a way to sneak into cells? John Whitfield investigates. What with the threat of bird flu, the reality of HIV, and the general unseemliness of having one's cells pressed into labour on behalf of something alien and microscopic, it is small wonder that people don't much like viruses. But it's possible that we may actually have something to thank the little parasites for. They may have been the first creatures to find a use for DNA, a discovery that set life on the road to its current rich complexity. The origin of the double helix is a more complicated issue than it might at first seem. DNA's ubiquity \u2014 all cells use it to store their genomes \u2014 suggests it has been around since the earliest days of life, but when exactly did the double spiral of bases first appear? Some think it was after cells and proteins had been around for a while. Others say DNA showed up before cell membranes had even been invented 1 . The fact that different sorts of cell make and copy the molecule in very different ways has led others to suggest that the charms of the double helix might have been discovered more than once 2 . And all these ideas have drawbacks. \u201cTo my knowledge, up to now there has been no convincing story of how DNA originated,\u201d says evolutionary biologist Patrick Forterre of the University of Paris-Sud, Orsay. Forterre claims to have a solution. Viruses, he thinks, invented DNA as a way around the defences of the cells they infected 3 . Little more than packets of genetic material, viruses are notoriously adept at avoiding detection, as influenza's annual self-reinvention attests. Forterre argues that viruses were up to similar tricks when life was young, and that DNA was one of their innovations. To some researchers the idea is an appealing way to fill in a chunk of the DNA puzzle. Furthermore, the hypothesis should be testable through genomic studies and even lab experiments. But whether or not it pans out, Forterre's idea reflects an emerging consensus that viruses' diversity, mobility and capacity for rapid change has made them major players in some of the most important moments in life's evolution. \n               Small world \n             Most researchers think that before there was DNA, life stored its information in RNA, the double helix's more versatile chemical cousin. RNA is a slender, flexible molecule, usually made as a single strand. RNA molecules can contort in ways that allow them to catalyse chemical reactions, including in some cases their own replication. It is possible to imagine an \u2018RNA world\u2019 where the molecule does almost everything \u2014 catalysing reactions and replications that would today be catalysed by proteins, and storing information that would now be stored in DNA. It is not possible to imagine DNA, a rigid, double-stranded rod that can be replicated only with the help of a protein, operating as a one-molecule band in the same way. That's one reason for thinking it came along later, after complex proteins had been added to the RNA world. Once DNA did arise, it would have had several advantages as genetic material. DNA's skeleton is more chemically stable than RNA. This stability allows DNA genomes to be longer, and so allows organisms to become more complex. But, as Forterre points out, the beneficial chemical property cannot explain why DNA appeared in the first place. Natural selection has no foresight; no innovator could acquire DNA on the basis that it would later be helpful in the expansion of its genome. \u201cThat would be like saying that dinosaurs evolved feathers because they knew they were going to turn into birds,\u201d says Forterre. Instead, he thinks that DNA's original selective edge was that it allowed viruses to avoid their host's defences. Many cells repel invaders by degrading their genetic material. But enzymes that had evolved to break down RNA would have ignored DNA. \u201cA virus that invented DNA would have a tremendous advantage in overcoming cellular defences,\u201d agrees Malcolm White, a proteomics researcher at the University of St Andrews, UK. Forterre's hypothesis \u201cfits with a lot of the clues that are scattered through genomes and phylogenies\u201d, he adds. \n               Insider traders \n             Two main lines of evidence point to viruses as likely genetic innovators, says Forterre. One is the diversity of genetic systems in contemporary viruses, which suggests an evolutionary tendency toward reinvention. Viruses have genomes made from double- and single-stranded DNA, double- and single-stranded RNA, and even DNA in which the chemical base uracil \u2014 also used in RNA \u2014 replaces the usual thymine. The genome can be carried on a single string, on a closed loop, or as a set of fragments. Many of these changes are thought to have occurred to help viruses avoid their host's defences. In fact, biologists believe they are only just beginning to fathom the extent of this diversity. \u201cWe don't know much about the viruses of the world,\u201d says Philip Bell, a molecular biologist at Macqaurie University in Sydney, Australia, who has argued that the nuclei found in complex cells are also descended from viruses 4 . Many of the viruses now being found in hot springs, for instance, feature unusual shapes \u2014 including spindles, rods, filaments and droplets \u2014 as well as genes with no known counterparts in other organisms 5 . The other line of evidence rests on relations among the genes used for DNA processing. There are three domains of cellular life: archaea, bacteria, and eukaryotes (the group containing plants and animals). All three share similar genes for turning genetic information into protein, and a similar enzyme for converting the components of RNA into those of DNA. This strongly suggests that these genes arose before the domains went their separate ways, probably in the RNA world. But the similarities break down for DNA helicases and polymerases \u2014 the enzymes that unwind the DNA double helix and copy each strand. Although the archaeal and eukaryotic versions of these genes are similar, the bacterial versions are radically different from both, suggesting that perhaps these DNA replication systems evolved independently. What's more, the DNA polymerases of eukaryotes and bacteria are more closely related to similar enzymes found in viruses than they are to each other. This all implies to Forterre that the ability to copy DNA molecules did not originate with cells, but with their parasites. In Forterre's scenario, the RNA genes in a cell infected with a DNA virus migrated to the new, more stable format over time with the help of a reverse transcriptase \u2014 an enzyme that makes DNA copies of RNA genes. Viruses could be expected to contain such enzymes, as they are so helpful for replication or for pinching useful genes from a host. Once the DNA genome became a complete warehouse of cellular genes, the original RNA chromosomes would be redundant. The process happened more than once, which explains the different DNA handling systems. Anthony Poole, an evolutionary biologist at Stockholm University, Sweden, finds the idea intriguing. The hypothesis fits with much of the evidence from viral biology and gene trees, he says. \u201cIt's very well thought out. I like it a lot.\u201d At the same time, Poole warns, the evidence is ambiguous. For instance, although viruses and cells swap genes with alacrity, it can be difficult to work out which genome gave birth to an innovation, and which imported it. \u201cPhylogenies suggest a relationship between viral and cellular sequences. The problem is we don't know the order things happened in \u2014 viruses could derive from cellular lineages,\u201d he says. Nor is everyone persuaded by Forterre's idea. Bill Martin, an evolutionary biologist at the University of D\u00fcsseldorf in Germany, says he agrees that the evidence points to DNA arising more than once, and that reverse transcriptase was probably involved in the transition from RNA to DNA. But he doubts that DNA's original selective advantage lay in infection. \u201cIt's completely off-target,\u201d says Martin. \u201cThe simple chemical stability of DNA is the main point.\u201d \n               Viral marketing \n             Forterre advocates gathering gene sequences from a greater diversity of viruses to seek descendants of the lineage that might have first infected cells with DNA. A good place to look, he says, would be a recently discovered group that infects amoebae, the mimiviruses 6 . These have huge, double-stranded DNA genomes \u2014 longer than those of some bacteria \u2014 and some of their genetic enzymes are similar to those in eukaryotes. Forterre also thinks that lab experiments with viruses and bacteria might recreate some aspects of the evolutionary process. It might be possible, for example, to replace a cell's DNA replication enzymes with their viral counterparts. But can we ever really be sure about anything that happened so close to the origin of life? \u201cIt's an area of discourse rich in conjecture and poor in proofs, but I tend to be optimistic,\u201d says Eugene Koonin. A genomics researcher at the National Center for Biotechnology Information, based in Bethesda, Maryland, Koonin was the first proponent of the idea that DNA evolved twice. He says the idea of an RNA world shows that consensus \u2014 if not certainty \u2014 is possible, he says, and we can hope for the same regarding the origin of DNA. \u201cThe study of viral genomics is not going to come to fruition in the next five years, but it's not hopeless.\u201d \n                     Origins of life: Born in a watery commune \n                   \n                     Special feature Part one; Origins of life \n                   \n                     The antiquity of RNA-based evolution \n                   \n                     Patrick Forterre \n                   Reprints and Permissions"},
{"file_id": "439132a", "url": "https://www.nature.com/articles/439132a", "year": 2006, "authors": [{"name": "Ichiko Fuyuno"}], "parsed_as_year": "2006_or_before", "body": "Japan's mission to collect a sample from a distant asteroid looks to have ended in failure. Ichiko Fuyuno investigates how the setback will affect Japan's struggling space programme. It was always a high-risk mission. No spacecraft has safely brought back a sample from the Solar System since the Soviet probe returned with lunar soil in the 1970s. So when, on 25 November 2005, a team from the Japanese space agency monitored the descent of the Hayabusa spacecraft towards the bumpy surface of the asteroid Itokawa, everyone in the control room was tense. Once Hayabusa was 360 metres above the asteroid, the touchdown command was issued. \u201cI felt as if all the people in the room were riding on it and descending together,\u201d recalls Junya Terazono, the agency's publications officer, who was busy posting photos and live updates to a website as the spacecraft descended. Despite the risks, after it had travelled 2 billion kilometres, and spent three months imaging the 540-metre-long rock, hopes that Hayabusa would bring back a souvenir from its trip were high. And on the morning of 26 November, a signal from the craft suggesting that it had fired pellets, designed to throw up rock fragments from the asteroid's surface, caused an eruption of noise in Hayabusa's control room. But the joy didn't last long. Just days later, the Japanese space agency, known as the Japan Aerospace Exploration Agency or JAXA, announced that it was highly unlikely that any pellets had been released or any sample collected. Mechanical problems had been detected in the probe back in July, but these troubles became catastrophic soon after the spacecraft landed on Itokawa. After the team lost communication with the spacecraft in early December, project manager Jun'ichiro Kawaguchi decided to delay Hayabusa's return by three years to 2010 to give them more time to revive it. The chances of a safe return look gloomy. Hayabusa would have capped a breakthrough year for Japan's space programme \u2014 had everything gone well. After a difficult decade, marked by a string of expensive satellite and rocket failures and a tough budget environment, Japan merged its existing space agencies in October 2003. The three agencies were the National Space Development Agency (NASDA) \u2014 Japan's main rocket and satellite developer; the Institute of Space and Astronautical Science (ISAS), responsible for scientific missions; and the smaller National Aerospace Laboratory. The merger was intended to cut costs and revitalize a space programme that had lost its way after a strong start in the 1970s and 1980s. \n               Lost in space \n             Today, the two-year-old JAXA has an ambitious wishlist for exploration over the next two decades, and a 2% budget increase for 2006 \u2014 the first budget increase for Japan's space programme in many years. But turning round Japan's fortunes in space exploration will depend on whether it can find ways to improve its track-record without killing its ambitious spirit. Critics say Japan tries to do too much with too little. JAXA's budget (\u00a5180 billion for 2006) is a tenth of NASA's, and less than half that of the European Space Agency or ESA (see graph below). And, at \u00a512-billion (US$100 million), Hayabusa cost only about half that of NASA's Stardust mission, which is set to return to Earth with captured cometary dust on 15 January. Japan can afford fewer missions, and so has fewer opportunities to launch new technologies. The result is to stuff as many ideas as possible into one launch. Hayabusa certainly carried a lot of hardware \u2018firsts\u2019. Some of these, such as the Japanese ion-drive engine used to propel the spacecraft out to the asteroid, worked fine. Others, such as the small surface probe Minerva, failed to deliver. \n               Tales of woe \n             \u201cMaybe sometimes Japan tries to do too much for its resources,\u201d says Andrew Cheng, a planetary scientist at Johns Hopkins University in Baltimore, Maryland, and a member of Hayabusa's science team. \u201cI'm happy to see very brave decisions and to launch very complicated missions. All that is good,\u201d adds Cheng. \u201cBut they cannot fail every time either.\u201d The year Hayabusa was launched was a particularly troubled time for Japan's space programme. In October 2003, the Midori-II Earth observation satellite failed. The following month, one of the Japanese flagship rockets, an H-IIA, had to be destroyed in mid-flight. Then the Mars probe Nozomi, in trouble since 1998, was finally lost in December. And last summer, the main X-ray instrument on the joint US\u2013Japan Suzaku telescope shut down, reducing scientists' ability to study black holes. Despite these troubles, many Japanese space experts believe that Japan should not just try to catch up with Europe and the United States, but should blaze its own trail. \u201cHaving ambitious dreams is good,\u201d says Masakazu Iguchi, head of the space activities commission that reviews Japan's space activities for the education ministry, which oversees JAXA's budget. But, he warns, \u201cJapan should move steadily towards its goals. If you want to climb Mount Everest or a small mountain, either way you have to move up step by step.\u201d Iguchi argues that the important thing is to learn from failure. \u201cI think JAXA understands that,\u201d he says. Under pressure to improve the performance of Japan's space programme's after the 2003 disasters, politicians sought the help of outside experts, including top US and European space-agency chiefs. And despite resistance from JAXA officials, the agency formed an advisory commission for mission success in 2004. Headed by former NASA chief Daniel Goldin, the commission released a report in March 2005, listing 21 ways the agency could improve. The Goldin commission suggested that JAXA strengthen ties with industry by shifting technical responsibilities to its prime manufacturers. In the past, Japan's space programme retained control over most design decisions, and interactions between agencies and the manufacturers were limited. It is hoped that with more responsibility, Japanese firms will gain the expertise needed to allow the country to compete in the global satellite market. Another key recommendation was to boost the efforts of systems engineers. Toshifumi Mukai, who heads a chief engineer's office established in October 2005, says systems engineers do important work at the start of a project by defining mission requirements and identifying potential risks. Under the new system, chief engineers operate independently of the project managers, who are now required to share development data with others more openly. But some JAXA officials are concerned that too much focus on risks, as well as constant reviews, will further weaken morale. \u201cJust how to get prepared in the event of failures is becoming daily work. I think that's wrong,\u201d says Kawaguchi, who believes Japan must keep being adventurous. Many Japanese space experts are wary of adopting the approach taken by China's space programme. Although China has had two successful astronaut missions, it uses off-the-shelf technology, which many Japanese space experts dismiss as lacking innovation. Others worry that JAXA will become as cautious as NASA or ESA. \u201cI think ESA is more conservative than JAXA, at least as reflected in design philosophy for spacecraft and in mission operations,\u201d Cheng says. He hopes Japan does not become too risk averse. There is no sign of that in JAXA's 20-year vision for space exploration, released in April last year. Calling for lunar exploration and perhaps eventually manned spaceflight, the ambitious scope of the 20-year plan seems at odds with current funding levels. Since a peak in 1999, the Japanese space budget has shrunk by 20%. \n               Risk taker \n             Critics, including the Goldin commission, have long argued for a strategic vision for Japan's space programme \u2014 one that will help it set priorities, and that will encourage better integration of the agencies that make up JAXA. Since the merger, the three agencies have largely retained their separate cultures and resisted being unified further. The vision document is a first important step, says John Logsdon, professor of space policy at George Washington University in Washington DC. \u201cJAXA is right now going through the process to deal with bureaucratic reorganization,\u201d he says. \u201cIt takes time.\u201d Decisions about human spaceflight and Moon bases won't be made anytime soon, so JAXA can focus on immediate priorities, such as improving rocket reliability, says Kimikazu Iwase, director of the space development and utilization division at the education ministry. Iwase attributes a successful H-IIA rocket launch in February 2005, the first for 15 months, to better pre-launch testing. Whatever Hayabusa does next, Kawaguchi's team has many busy months ahead analysing the data and images sent back by the craft before its descent. More than 1,500 high-resolution pictures have revealed a rocky surface devoid of debris. This is in striking contrast to the highly weathered surface of the asteroid Eros, which NASA's Shoemaker spacecraft visited in 2001. Hayabusa did not achieve everything JAXA hoped for, but few question its engineering and scientific achievements. \u201cWhether or not we ultimately get a sample returned to Earth, the mission still is a success from a science point-of-view,\u201d says Donald Yeomans of NASA's Jet Propulsion Laboratory in Pasadena, California, and US project scientist for Hayabusa. \u201cThe Japanese flight team performed well dealing with unexpected spacecraft anomalies and a bizarre and rocky asteroid surface.\u201d What JAXA learns from such experiences will shape its fortunes over the next decade. \u201cOverall, things are getting better, but we haven't fully gotten out of the doldrums,\u201d says Yasunori Matogawa, associate executive director at JAXA. \u201cHayabusa was the mission that could have opened the door. Now we will have to see whether it has really done so.\u201d \n                     News in brief \n                   \n                     Hayabusa ready to head home with asteroid sample \n                   \n                     Spacecraft on course to score a first with asteroid samples \n                   \n                     Ideas abound as Japan aims to boost its space image \n                   \n                     Satellite loss throws Japan's space programme into disarray \n                   \n                     News in brief \n                   \n                     JAXA missions \n                   \n                     JAXA projects \n                   Reprints and Permissions"},
{"file_id": "439134a", "url": "https://www.nature.com/articles/439134a", "year": 2006, "authors": [{"name": "Roxanne Khamsi"}], "parsed_as_year": "2006_or_before", "body": "A number of fatal brain diseases are linked to misfolded proteins, an effect researchers are mimicking in the lab. But as they generate new versions of these malformed molecules, could they be creating a monster? Roxanne Khamsi finds out. In a secure lab in Texas, five machines are purring away quietly. Working through the night, these boxes churn out billions of malformed proteins. A seemingly odd thing to mass-produce, these distorted molecules are at the heart of research into a family of diseases that destroy the brain. The equipment was devised by Claudio Soto 1 , a biologist at the University of Texas Medical Branch in Galveston, who works on prion protein, a naturally occurring molecule. Misfolded versions of this protein are thought to cause conditions such as mad cow disease and its human equivalent, Creutzfeldt\u2013Jakob disease (CJD). Such abnormal prions are infectious and, when ingested, make their way to the brain, where they slowly distort the natural prion proteins, causing disease and, ultimately, death. But this conversion process takes a long time \u2014 in humans it can be more than a decade before enough abnormal prion has been made to cause disease, making it hard for scientists to investigate the diseases. Soto's machines have changed that: they can quickly turn a mass of normal proteins into twisted imitations of their former selves. \u201cWe can mimic the process of prion replication that normally takes a year in the brain within a matter of hours in the test tube,\u201d says Soto. And that acceleration is allowing researchers to experiment with the abnormal prions, gaining fresh insight into how they cause disease. But it begs one important question: is it safe? Collectively, the diseases linked to infectious prions are called transmissible spongiform encephalopathies, or TSEs, because of the spongy appearance they give the brain. Most TSEs are species-specific. Scrapie, for example, occurs in sheep; CJD in humans; and chronic wasting disease (CWD) in deer and elk. But some TSEs have hopped from one species to another \u2014 mad cow disease, for example, is believed to have jumped the \u2018species barrier\u2019 to cause disease in both cats and humans. Given that the abnormal prions are notoriously hard to destroy \u2014 they resist temperatures as high as 120 \u00b0C and are not broken down by the enzymes that usually degrade proteins \u2014 is it wise to be creating new types of them in the lab? Like all prion research, Soto's work is carried out under strict biosafety conditions. He argues that the risk of any prions escaping is very low. \u201cThe potential benefits are much greater than the risk,\u201d he argues, adding that the work his team is doing could help predict whether a TSE is likely to jump between species. Other researchers agree. They say that making prions in the test tube will help them tackle key problems such as why one type of abnormal prion behaves differently from another. \u201cIt's one of the most burning questions in the entire field,\u201d says Adriano Aguzzi, a prion researcher at the University Hospital of Zurich in Switzerland. Scientists have long explored the question of whether TSEs can jump between species. Although a particular TSE spreads efficiently within a species, researchers have found it much harder to infect one species with the abnormal prion from another. This led to the idea of a species barrier that limited such infections in the wild. \n               Broken barriers \n             But that view was challenged in the late 1980s, when Britain's cow herds were struck by a new TSE: mad cow disease, or bovine spongiform encephalopathy. Scientists suspect that the species jump was caused by the farm animals being given feed created from the rendered carcasses of scrapie-infected sheep. And the disease didn't stop there \u2014 it also seemed to jump from cows to humans, causing a condition called variant CJD. Experts generally agree that abnormal prions from cows can convert normal ones in humans, and cite this as an example of a prion disease crossing the species barrier. So just how easily do infectious prions jump from one animal species to another? It's no idle question. Although governments have banned certain animal by-products from feed to restrict the transmission of prion disease among livestock, some experts say that there may be a need for further restrictions to prevent the possible spread of TSEs to species such as pigs, which are currently thought to be much less susceptible to prion disease under experimental conditions. But in the 1990s, a new TSE emerged in UK cats, claiming the lives of more than 80 domestic animals. It even spread as far as zoo animals, reaching cheetahs, ocelots, pumas, lions and tigers. Again it was linked to the BSE prion, suggesting that the species barrier might be easier to breach than was once thought. To get a clearer picture of infectious prions and their ability to jump species, researchers are turning to Soto's mass-production technique. In the brain, the abnormal prions tend to link together in chains, eventually forming visible tangles. But this limits the number of prions free to convert normal proteins \u2014 only those at the end of the chains can cause the switch, making it a very slow process. In his protein-misfolding cyclic amplification (PMCA), Soto found a neat way to sidestep this problem. The technique initially incubates a small amount of abnormal prion with an excess of normal protein, so that some conversion takes place. The growing chain of misfolded protein is then blasted with ultrasound, breaking it down into smaller chains and so rapidly increasing the amount of abnormal protein available to cause conversions. By repeating the cycle, the mass of normal protein is rapidly changed into misfolded prion. \n               Tangled web \n             Soto and his team have used the technique to probe the limits of the species barrier. In unpublished work, they incubated normal protein from lab animals such as mice with a range of infectious prions, including those that cause mad cow disease, scrapie and CWD. The resulting prions caused disease when injected into the brains of rodents. \u201cThe data from our experiments suggest that there is no absolute species barrier. You can always break it,\u201d says Soto. \u201cIt's just a question of how far you push the system.\u201d Soto's lab is now extending these experiments to look at other possible species jumps, including that from infected deer to humans. Although eating meat from scrapie-infected sheep does not seem to cause prion disease in humans, the effects of eating venison contaminated with CWD remain unclear. It has not yet been proved that CWD can jump to humans, but studies have shown that injecting CWD prions into monkeys can cause disease 2 , says Paul Brown, former medical director of the Laboratory of Central Nervous System Studies in Bethesda, Maryland. He stresses that CWD does not pose a public health risk as deer remains do not typically get cycled into the food chain. \u201cThe concept of an absolute species barrier is probably a myth,\u201d says Brown, adding that there are simply \u201cvarying degrees of ease\u201d in terms of how readily the prions can be transmitted. Under experimental conditions, skunks, pigs and even gerbils seem susceptible to TSEs from other species 3 . As yet, researchers have not succeeded in infecting dogs or rabbits with a TSE. \n               Taking the strain \n             Another big question is what makes a particular prion \u2018strain\u2019 act in the way it does. Strains are defined by the characteristics of the illness they cause in a given animal, including incubation time and the type of damage done to the brain. Often, different strains will have distinctive biochemical properties \u2014 although two strains within the same species can have the same amino-acid sequence and behave differently. Byron Caughey and his team at the Rocky Mountain Laboratories in Hamilton, Montana, have been working on making prions in the test tube for more than a decade. In 1995, they showed that infectious prions made in the absence of cells retain strain-specific qualities, such as tending to convert only prions from the same species 4 , 5 , which suggests that no other cellular components are needed for the prion's activity. Some studies suggest that the species barrier is dictated largely by the three-dimensional structure of the abnormal prion species 6 . In other words, the given prion's structure has to be able to work with the native normal protein. By changing just two key amino acids in the proteins, Witold Surewicz of Case Western Reserve University in Cleveland, Ohio, and his team have created new prion fragments that defy the species barrier 7 , 8 . The result was new structures that could adopt the shape of prion fragments from a different species. Using Soto's technique, such investigations into the species barrier will only accelerate. But as biologists mix normal and infectious prions from different species in the lab, they will create novel prions with unpredictable abilities. The risks of creating a \u2018super-strain\u2019 are not new \u2014 scientists recently debated the merits of recreating and tinkering with the virus strain that caused the 1918 flu pandemic. Proponents of the prion work say that the threat of creating an accidental monster is minimal. They stress that unlike the genetic manipulation of flu viruses, which could produce an inhaled superbug, similar work in prions is unlikely to produce infectious material that can be transmitted through the air. Others point out that scientists have been creating new prion strains for many years \u2014 albeit in animals, rather than in the test tube. \u201cI do not see the generation of new strains in cell-free systems as a fundamentally new type of biohazard,\u201d says Caughey. And understanding how easily particular infectious prions can pass from one species to another will help officials design adequate safety measures to protect us and our food chain, says Soto. \u201cIt's very important to evaluate the danger and be prepared.\u201d \n                     Blood test detects deadly prions \n                   \n                     The 1918 flu virus is resurrected \n                   \n                     Lab-made prions trigger mad cow symptoms \n                   \n                     Nature Prion Disease web focus \n                   \n                     University of Texas Medical Branch \n                   Reprints and Permissions"},
{"file_id": "439388a", "url": "https://www.nature.com/articles/439388a", "year": 2006, "authors": [{"name": "John Whitfield"}], "parsed_as_year": "2006_or_before", "body": "Can reading the classics through Charles Darwin's spectacles reawaken literary study? John Whitfield reports. When, at the beginning of  The Iliad  \u2014 and Western literature \u2014 King Agamemnon steals Achilles' slave-girl, Briseis, the king tells the world's greatest warrior that he is doing so \u201cto let you know that I am more powerful than you, and to teach others not to bandy words with me and openly defy their king\u201d 1 . But literary scholar Jonathan Gottschall believes that the true focus of Homer's epic is not royal authority, but royal genes. Gottschall is one of a group of researchers, calling themselves literary darwinists, devoted to studying literature using the concepts of evolutionary biology and the empirical, quantitative methods of the sciences. \u201cWomen in Homer are not a proxy for status and honour,\u201d says Gottschall. \u201cAt bottom, the men in the stories are motivated by reproductive concerns. Every homeric raid involves killing the men and abducting the women.\u201d The violent world of the epics, he says, reflects a society where men fought for scarce mates and chieftains had access to as many women as slaves and concubines 2 . And he thinks that everything written since Homer is open to similar analysis. Literary darwinism is a mode of analysis; it's also a bit of a crusade, an attempt to shake up literary criticism. \u201cLiterary theory requires a theory of human nature, because literature is shaped by human motives and cognitive biases,\u201d says Joseph Carroll of the University of Missouri, St Louis. The problem, say the literary darwinists, is that for the past few decades the humanities have, in the case of critics deconstructing texts, denied the need for a theory of human nature, asserting that the study of texts can be concerned with nothing outside those texts. Or else they have been stuck on theories of human nature that are rooted in the subjective and the social. Those influenced by freudianism, for example, might read a novel looking for hints of a child's sexual desire for its parent. A marxist would seek out economic and class conflicts. Carroll has no truck with this: \u201cThe theories up to this point have all had a little bit of the truth, but have also all been fundamentally flawed,\u201d he says. \u201cNone comes to terms with the fundamental facts of human evolution.\u201d Literary darwinists believe that literature reflects a universal human nature shaped by natural selection, and as a result, read texts in terms of animal concerns such as mate choice, relations between kin, and social hierarchies. Such a scientistic approach can meet with hostility. \u201cAt one meeting of the Modern Languages Association, someone stood up and called me a proto-fascist,\u201d says Nancy Easterlin, an expert in Romantic literature at the University of New Orleans, Louisiana, who uses ideas from cognitive science in her analysis of the mother\u2013child bond in William Wordsworth's  Prelude . The tide may be turning, however. \u201cThe ideological resistance is crumbling pretty fast,\u201d says the British author Ian McEwan, who has used scientific ideas in several of his novels. \u201cNow things are spoken of that would have routinely got you called a Nazi a few years ago.\u201d The English department at Texas A&M University, in College Station, has recently approved a seminar on literary darwinism \u2014 the first university course on the subject, says Brett Cooke, the course leader and an expert in Russian literature. \n               Man to beast \n             So what does it mean to read literature through a darwinian lens? At one level, it can seem remarkably obvious. In their recent book  Madame Bovary's Ovaries 3 , evolutionary psychologists David and Nanelle Barash argue that a darwinian understanding of female mate choice shows why the eponymous adulteress takes lovers who are more attractive and accomplished than her mediocre husband. This may sound crass, but Carroll argues that the approach is capable of subtlety. A darwinian analysis of Jane Austen's  Pride and Prejudice , he says, goes beyond the simple idea that women look for fortune in men, to show how such animal concerns are filtered through the vast flexibility of human behaviour, cultural conditions and individual variation. \u201cI don't look at  Pride and Prejudice  and try to sort out what is biological and what is cultural,\u201d says Carroll. \u201cI look at it and examine the way underlying biological dispositions are organized in a specific cultural ecology. Nobody in the novel escapes the problems of mate selection, status and forming alliances. But the characters also integrate these concerns with human qualities, such as intelligence, character, morals and cultivation.\u201d The noble, romantic characters, such as Elizabeth Bennett and Darcy, integrate successfully, hiding their reproductive issues beneath their social graces. The more comic characters, such as Elizabeth Bennett's mother, do not (although in marrying off her daughters, she is quite the evolutionary success). Romantic comedies play upon the audience's pleasure at seeing reproductive strategies rewarded; tragedies appeal by invoking recoil from maladaptive acts. \u201cStories that focus on non-normative behaviour, such as when Medea kills her children, take their punch from the audience's understanding that this is not how humans behave,\u201d says Gottschall. Not everyone in the movement is equally keen on reductions to a purportedly universal human nature. \u201cI've always had a love-hate relationship with evolutionary psychology. It's very interesting as far as it goes, but it marginalizes culture and other open-ended processes,\u201d comments David Sloan Wilson, a biologist at Binghamton University, New York. Wilson is also the editor, with Gottschall, of  The Literary Animal 4 , a recent collection of essays on literary darwinism. But, Wilson adds, literature is an immense source of data on human behaviour: \u201cIt's the natural history of our species.\u201d Anything that wakes literary study up to the idea of a shared human nature, reflected throughout literature, is to be welcomed, says McEwan, one of whose lectures is reprinted in  The Literary Animal . \u201cTo think in evolutionary terms about human nature has helped me as a novelist, and to some extent as a reader,\u201d he says. An evolutionary emphasis might also help the study of literature to reverse its journey into obscurantism and irrelevance. \u201cIt's a tragedy, the way that literary criticism has lost its place,\u201d he says. \u201cI don't read much literary theory, especially of the kind that has dominated the academy for the past few decades: there's been a great flatus of nonsense and pseudoscience.\u201d But even if literary theory is starting to get over its objection to evolution, it may be more resistant to the other main item on the darwinist's agenda \u2014 quantitative research methods. \u201cAmong literary folk, the fear of quantification is greater than the fear of evolution,\u201d says Wilson. Gottschall agrees: \u201cAll literary scholars think they're mathematically disabled.\u201d \n               A common tale \n             Gottschall has analysed a database of folk tales from around the world to test the idea that a focus on beautiful princesses in need of rescuing and dashing hero is not just a product of patriarchal attitudes in European societies, as some feminist critics have claimed. He found that all around the world, the majority of folk tales feature brave heroes marrying beautiful heroines, with the two living happily ever after 5 . Gottschall and Carroll, collaborating with psychologists, are also currently analysing the data from an online questionnaire that gathers people's responses to characters in nineteenth-century fiction; they aim to see how these compare to the personality categories and goals defined in evolutionary psychology 6 . By borrowing the scientific method, says Gottschall, literary scholars can work out what a story is \u2018really\u2019 about, not in some ultimate, metaphysical sense, but in the sense of whether a wide range of people interpret a work in the same way. Such an approach, he says, is needed if literary scholarship is to create testable, durable knowledge \u2014 and to prevent arguments being settled solely by who deploys the sharpest rhetoric and the best memory. David Amigoni, a specialist in Victorian prose at Keele University, UK, agrees that there can be value in darwinian interpretations, as well as in reading Darwin. But he insists that attitudes and readings are mutable. \u201cProving claims of truth is not necessarily what literary critics are looking to do. They're looking for patterns of meaning, rather than trying to produce an overarching theory of life.\u201d He doubts that graphs and statistics can say much about literature: \u201cThe emphasis on hard data will probably be a bit strange to a lot of literary critics. I have a concern that something that ends up in numbers hasn't really taken account of literary value.\u201d Gottschall, though, wants to move beyond literary value \u2014 or for that matter, traditional literary criticism. Literary scholars may adopt their theories from other branches of knowledge, but they also push them outwards, using their theoretical frameworks to analyse philo-sophy, science, history and gender politics, for example. Ultimately, the theories of human nature that become widely held in a society will influence how that society believes people respond to their environments, and how they should be treated. \u201cLiterary scholars aren't harmless,\u201d Gottschall says. \u201cWhen we get it wrong it matters.\u201d \n                     A change of mind? \n                   \n                     Literature red in tooth and claw \n                   \n                     Geologists show Homer got it right \n                   \n                     Sense and sensibility \n                   \n                     The literary animal \n                   \n                     London Review of Books \n                   \n                     David Sloan Wilson \n                   \n                     The Center for Evolutionary Psychology \n                   Reprints and Permissions"},
{"file_id": "439384a", "url": "https://www.nature.com/articles/439384a", "year": 2006, "authors": [{"name": "Virginia Gewin"}], "parsed_as_year": "2006_or_before", "body": "Soil microbes are notoriously hard to culture, so how can we make the ground yield its secrets? Virginia Gewin finds that genetic sequencing \u2014 of samples not species \u2014 may be the answer. Leonardo da Vinci once remarked that \u201cWe know more about the movement of celestial bodies than about the soil underfoot.\u201d You could argue that his insight still holds true the best part of 500 years later. But new genomic technologies mean that the microscopic bodies that enliven soil may be about to get the attention they deserve \u2014 if not as individuals, then as communities. Twice as much carbon is stored in Earth's soil as exists in the plants that grow from it and the animals that depend on them. It is the soil's microbes that are responsible for recycling this carbon, and other nutrients. Living in the fractal jumble of weathered rock, mineral particles and decaying organic matter are a cast of thousands, some say millions, of species. These soil organisms occupy an endless foam of tiny niches; they purify water, detoxify harmful substances and recycle waste products. They restore carbon dioxide to the air and make the atmosphere's nitrogen available to plants. Without them, continents would be deserts \u2014 home to little more than lichen, and not much of that. Knowing which processes soil microbes are responsible for, and how, is increasingly important to everyone from farmers to climate planners. Microbiologists want to see how the organisms communicate with each other and refine the niches in which they live. Drug developers want to know how the soil microbes poison each other with antibiotics, and other commercially minded researchers think they could discover useful industrial enzymes or additives for biofuels. But beyond a gross understanding of inputs and outputs, the specific ecological roles of microbial species or communities in the dirt remain elusive. The main stumbling block has been that up to 99% of soil microbes cannot be grown in laboratory cultures \u2014 the traditional way to study microorganisms (see \u2018 Cultural renaissance \u2019). But as genetic technology has improved, it has provided ways around this. First, it managed to reveal the general level of biodiversity through the sheer quantity of different sequences, then it allowed scientists to trawl for individual genes. Now technology offers the promise of extracting not just the genomes of the creatures in the soil, but \u2014 in a sense \u2014 the genome of the soil itself. The physical, chemical and biological complexity of the soil make this sort of study a daunting prospect, even for a hugely ambitious gene-sequencer such as Craig Venter. Following his work on the human genome, Venter has been using the sequencing muscle of his institutes to look at samples from seas around the world. But soil's greater complexity makes it the ultimate challenge for such studies, and arguably the biggest open frontier in biology today. Julian Davies, a pioneer of soil genomics at the University of British Columbia in Canada, thinks that finding out what is really going on in the soil will turn our world upside down \u2014 making that below us more wonderful than that above. \u201cOnce the diversity of the microbial world is catalogued,\u201d says Davies, \u201cit will make astronomy look like a pitiful science.\u201d The full extent of this diversity is still up for debate. But its magnitude was suggested 20 years ago by Norman Pace, a microbiologist currently at the University of Colorado, Boulder. Pace adapted techniques that Carl Woese, a microbiologist at the University of Illinois at Urbana-Champaign, had used to probe the genetic relationships between all living things. Woese's work reclassified the microbial world using variations in the sequence of a highly conserved RNA molecule; Pace applied his technique to DNA taken from the environment. Although Pace's studies excelled at revealing the diversity in soil, they were less good at suggesting what that diversity did. \u201cWe knew enough to know that we were ignorant about most of the organisms, but we still couldn't get our hands on them,\u201d says Jo Handelsman, a plant pathologist at the University of Wisconsin, Madison. The problem was partly solved as researchers refined methods for taking genes from the vast number of organisms they couldn't culture and inserting them into lab workhorses such as  Escherichia coli  \u2014 allowing analysis of particular genes and functions. Handelsman dubbed these studies of DNA from uncultured organisms \u2018metagenomics\u2019, and was one of the first to take advantage of it. Because many antibiotics have been derived from soil microbes that do grow in cultures, such drugs were an obvious thing to look for in those that do not. So far, Handelsman and her collaborators have turned up antibiotics called turbomycin A and B (ref.  1 ), and Davies has discovered one he named terragine 2 . Companies were swiftly set up to capitalize on the promise of \u2018functional screens\u2019 for possible antibiotics or industrial compounds; they included Diversa, based in San Diego, California, and TerraGen Discovery \u2014 now part of Cubist Pharmaceuticals in Lexington, Massachusetts. But none of the dozen or so compounds found through metagenomic studies of soil seems set to be useful. Davies, who founded TerraGen in 1996, resignedly describes the situation as \u201crather disappointing\u201d. One of the problems is that the screens ignore a lot of the soil's natural diversity; genes from many of the microbes may simply not be expressed when cloned into  E. coli . In fact, terragine is one of them: it was isolated from genes inserted into recombinant streptomycetes. \n               The utility of resistance \n             Although the search continues for new antibiotics, using metagenomics to understand soil's role in antibiotic resistance holds perhaps greater promise. Indeed, Davies has a theory that antibiotics evolved as signalling molecules that drove the development of sensing and evasion strategies in microbes. The idea is gaining popularity and leading to exciting links between environmental and medical microbiology. Recent work by Gerard Wright, a microbiologist at McMaster University in Ontario, demonstrates that soil microbes have amassed a bevy of tactics to elude the onslaught of antibiotics. On average, every one of 480 strains of  Streptomyces  that Wright harvested from soils across Canada was able to survive 7 or 8 of the 21 antibiotics presented to it \u2014 to many of which it had probably never been exposed 3 . Tracing genes for antibiotic resistance is an area that is ripe for study by metagenomics, enthuses Handelsman. The search for enzymes to improve the specificity, efficiency and sustainability of industrial reactions has already had some success. In one desert soil sample, researchers at Diversa unearthed more than 100 enzymes that cut up esters and lipids for use in such reactions. With only 200 esterase enzymes known before this work, discovering half as many again is no small accomplishment. For the past five years, Diversa has turned its attention to developing products, but many sit stuck in a bottleneck of government regulations, as approval processes can take several years to navigate. Of course, the functions of genes found in soil do not have to be commercially useful in order to be ecologically enlightening. In simpler environments, such as the sea, metagenomic studies have yielded remarkable discoveries \u2014 for example, microbes that make use of sunlight without the benefit of chlorophyll. These organisms are no marginal oddity, having recently been shown to account for 13% of the microbes in the shallow parts of the ocean 4 . Soil ecologists may be envious of such a breakthrough in marine systems, but recent findings suggest that soil's secrets may not remain hidden much longer. Christa Schleper is a microbial ecologist at the University of Bergen, Norway, with a laboratory that produces libraries of large DNA fragments. She recently discovered that some of the oxidation of ammonia that goes on in the soil is being carried out by genes that come not from bacteria, but from archaea. The archaea were established as a separate kingdom of life by Woese, and until recently they were seen as being of ecological importance only in marginal and extreme environments.Now they are being discovered playing mainstream roles. Schleper's archaea findings 5 , 6  are already causing scientists to reexamine the nitrogen cycle \u2014 the conversion of nitrogen to and from forms needed by plants and animals. The success of studies such as Schleper's will not only spur work on as-yet-uncultured organisms, but also generate hypotheses about community dynamics and functions. Schleper notes that soil contains a lot of archaea, but that in moderate environments they are much less diverse than the bacteria they live alongside. So, did they always inhabit moderate environments, being overrun at a later point by more diverse bacteria? Or did they invade, and if so, how did they acquire their niche? \n               Nitty gritty \n             \u201cWe need more studies of the genes discovered by functional screens\u201d in order to take such work further, says Rolf Daniel, a microbiologist at the Georg-August University in G\u00f6ttingen, Germany. \u201cMassive sequencing efforts are the starting point.\u201d Instead of cloning genes into bacteria and then studying the proteins that are expressed, large-scale sequencing allows scientists to analyse the genes directly, comparing their sequences with those of genes of known function. Ecology groups in the United States and Europe are trying to piece together the funding necessary for such sequencing capacity. This is where Venter comes in. He plans to use the \u2018shotgun\u2019 sequencing technique \u2014 in which all the DNA in a sample is cut into small fragments, sequenced and then pieced together \u2014 to create a genetic inventory of the planet. In 2004, he produced a paper that sequenced the DNA in a sample of the Sargasso Sea: it yielded 1.2 million new genes and evidence for 148 new species 7 . In addition to the marine samples taken as part of his  Beagle -emulating voyages aboard the  Sorcerer II , Venter is also taking soil samples. Soil searchers have greeted this sally on to \u2014 indeed, into \u2014 their turf with a degree of scepticism and even jealousy. Only a handful of soil labs are funded well enough to construct metagenomic libraries like Schleper's, whereas the various institutes he has set up make Venter a one-man superpower in sequencing terms. Eugene Madsen, a microbial ecologist at Cornell University in Ithaca, New York, grudgingly agrees that Venter's enormous sequencing ability will probably be good for the research community. But he wonders whether Venter will end up with anything more than a hodgepodge of random sequences. Venter freely admits that, compared with the clear waters of the Sargasso Sea, soils are proving a challenge. Sea water is well mixed and unstructured. You can take as much of it as you want \u2014 the Sargasso work took 200 litres \u2014 remove the DNA, sequence it, and come up with a pretty good picture of what was there and in what proportions. The diversity in soils means that sequencing a sample of more than a gram is overly ambitious, and this in turn means that the DNA from that gram needs to be amplified before it can be shotgunned. Venter concedes that this can create artefacts unless care is taken, and his Sargasso Sea work has been faulted for sequencing sample contaminants. \n               The whole truth \n             The focus of research need not be on single genes. Putting together individual microbial genomes from the jumble of library sequences is an obvious goal \u2014 albeit one that is as yet out of reach for samples that contain thousands of species. The most complicated system attempted so far was a community consisting of a handful of microbe species that lived in the extremely acidic waters draining from an old iron mine 8 . Even in this case, the genomes of just two species have been mostly completed, but the work is seen as a major step towards the ultimate goal of determining the ecological role of community members. \u201cAssembling genomes from the environment is a far more complicated task than the human genome,\u201d Venter says. Producing the sequence of a human's 3 billion base pairs required enough shotgunned snippets of DNA to cover the whole genome five to eight times over. \u201cThe soil work is like trying to assemble 40,000 human genomes simultaneously with only one-half to one-quarter of the sequencing coverage.\u201d Handelsman estimates that getting enough coverage to assemble the genomes of the species in 1 gram of soil would mean sequencing 250 billion base pairs; Venter shotgunned just 1 billion base pairs for the Sargasso Sea paper. Schleper has constructed three libraries in total, covering 3 billion base pairs of DNA from just two different soil samples, a sandy ecosystem and a mixed forest soil, but hasn't sequenced them. She welcomes help doing the high-throughput work. \u201cThe interesting biology is interpreting and looking through it,\u201d she notes. Even with all the sequencing power that money can buy, stitching together genomes of thousands of species is difficult owing to the plentiful horizontal transfer of genes. According to Jonathan Eisen, a genomicist at The Institute for Genome Research in Rockville, Maryland, the largest number of complete genomes so far assembled from a mixed DNA sample is just two: a study he was involved in managed to disentangle the genomes of a  Wolbachia  symbiont and its  Drosophila melanogaster  host. Eisen stresses that even that was not easy. To Schleper and others, however, the assembly of whole genomes is not necessarily an appropriate goal. Given the enormous amount of genetic exchange among microbes, treating the soil itself as an organism may be more valid. This is the approach used in a recent collaboration between Diversa and the US Department of Energy's Joint Genome Institute at Walnut Creek, California. The researchers put fragmentary sequences from their data into categories or \u2018bins\u2019 based on the metabolism they represent, rather than the gene or organism 9 . They compared the microbial communities in an agricultural soil with samples from sea water and with microbes from ocean-floor sediments recovered from sites where dead whales have decomposed. The comparisons showed that even incomplete DNA could characterize an environment and suggest ecological roles within it. Whether or not individual genomes are pieced together, there is no doubt that the sequencing of soil genomes will be fertile ground for hypotheses about how microbial communities drive the ecological processes of a region. At the moment, says George Kowalchuk, a microbial ecologist at the Netherlands Institute of Ecology in Heteren, \u201ceven if you have the entire community genome, you're still far from predicting how it works.\u201d But at least we are heading in the right direction. \u201cI think this is an area that will have far greater impact on science than sequencing the human genome,\u201d says Venter. Not everyone might want to go quite that far, but Schleper is happy to insist: \u201cThis is really a new era in microbial ecology.\u201d \n                     Metagenomics: DNA sequencing of environmental samples \n                   \n                     The metagenomics of soil \n                   \n                     Metagenomics and industrial applications \n                   \n                     Genomic studies of uncultivated archaea \n                   \n                     Craig Venter's Sorcerer II expedition \n                   \n                     Jo Handelsman's website \n                   Reprints and Permissions"},
{"file_id": "439262a", "url": "https://www.nature.com/articles/439262a", "year": 2006, "authors": [{"name": "Rex Dalton"}], "parsed_as_year": "2006_or_before", "body": "For decades, much of the early history of fish evolution was locked away in rocks in China. Rex Dalton tracks down the scientist who brought many of the remains to the surface. The dry desert setting of Mesa, Arizona, may not seem the most appropriate place to talk about the watery world of fish. But palaeontologists gathered in a lecture hall there last autumn to celebrate a life spent studying ancient oceans and the fish that swam in them. The symposium, held by the Society of Vertebrate Paleontology, celebrated one of China's most prominent palaeontologists \u2014 Meemann Chang, former head of the Institute of Vertebrate Paleontology and Paleoanthropology (IVPP) in Beijing. Her work has helped clarify the links between the fish that swam in Earth's oceans 400 million years ago and the air-breathing, land-walking creatures that evolved from them. Now 69, she has played a key role for years in bringing little-known Chinese fish fossils to the attention of the scientific world. Chang's career has been far from straight-forward. Along with her successes, she has also faced significant difficulties, thanks to the shifting political landscape of her homeland. Despite such problems, her enthusiasm for her subject remains undimmed. \u201cI am still digging and collecting fossil fishes,\u201d she smiles during an interview at the landlocked Arizona hotel. Chang's father, a gifted pathologist from Nanjing, wanted her to become a physician, but love of her country led her to choose geology instead. In 1958, during the Great Leap Forward, she was among those who heeded the call of vice-president Liu Shaoqi to study the Earth so that China might exploit its natural resources, such as oil. For Chang, that introduction to China's rocks set her on the path to study fish fossils, a quest that has taken her to all the continents of the world. In 1965, Chang was chosen to do graduate research at the Swedish Museum of Natural History in Stockholm, one of the leading research centres in palaeontology. But her time there was to prove short-lived. When the Cultural Revolution swept China in 1966, Chang, ever the patriot, halted her studies and returned home. In Beijing, Chang was confronted by the new phenomenon of the Red Guard who, on the orders of Mao Zedong, \u2018purified\u2019 China by isolating and punishing the academic classes. \n               Bad dream \n             For more than a decade, Chang lived what is now called the \u2018time of nightmares\u2019 \u2014 public humiliations to challenge the intellectual spirit, and hard labour in the countryside to break the body. \u201cI wasn't allowed to do research,\u201d she recalls, \u201conly to read Mao.\u201d It would be many years before Chang was able to return to Sweden to complete her doctorate. \u201cShe is a wonderful person who has been through a lot,\u201d affirms John Maisey, curator of fossil fish at the American Museum of Natural History in New York and one of the organizers of the Mesa meeting. \u201cBut she still smiles and is charming.\u201d Chang's career has taken her through many countries, and allowed her to pick up numerous languages. She earned her undergraduate degree in 1960 at the Lomonosov Moscow State University, where she became fluent in Russian. She learned modest Swedish while in Stockholm, is fluent in English and reads German and French. But she is also adept at deciphering another language: that of fossilized remains 1 . She can readily navigate a path from the \u2018age of fish\u2019 400 million years ago in the Devonian period, through to the end of the dinosaur age and the Cretaceous period 65 million years ago. In her current studies, Chang is working to understand the species distribution pattern of fish across the Pacific Ocean \u2014 a distribution that reached its maximum during the Eocene epoch, between 34 million and 56 million years ago. Most of these fish became extinct in the western Pacific, she notes, but a few, such as the coelacanth, still survive in the eastern Pacific. \u201cTracing the origins and distribution of these fish is a very exciting endeavour,\u201d she says. Chang's contribution to Chinese palaeontology was recognized in 1983 when she became the first woman to head the IVPP. This was significant not only because of her gender but because it marked the IVPP's move away from political appointments to those based on merit. Chang served two terms as director, ending her tenure in 1990, and helped shepherd the institute from the days when whole families were living on an upper floor of the research building, to a new facility that included modern laboratories. Unlike some of her more rigid compatriots, Chang was very flexible and open when it came to guiding her students' careers, says palaeontologist Desui Miao, who helped to organize the Mesa symposium. He cites the case of Zhonghe Zhou as an example. One of Chang's promising students, Zhou had begun a doctoral programme in the early 1990s to examine fossil fish. But then quarries in the northeastern province of Liaoning started to yield an intriguing assortment of fossilized birds dating back to the early Cretaceous. Zhou saw this as an opportunity to switch from fish to avian fossils. Chang agreed, allowing him to change the direction of his research. \u201cThis was a major break with Chinese tradition,\u201d says Miao, of the University of Kansas in Lawrence. \u201cBut it showed how she treated every student,\u201d he adds \u2014 working first and foremost to develop them professionally. The change more than paid off. What began as a seemingly minor academic move helped pave the way for China to become a leading force in palaeontology. Liaoning's avian-like fossils of feathered dinosaurs with rapacious teeth redefined how birds evolved 2 . Soon, the world's top palaeontologists were clamouring to come to China, which in turn generated collaborations and opportunities abroad for young Chinese researchers. Miao himself was among those who benefited from these new links \u2014 in the late 1980s, he found himself studying at the University of Chicago. Once more, Chang was to show her willingness to put her students' interests first. In 1988, Miao knew he wanted to continue his postdoctoral studies in Chicago, but at that time China's leaders, worried about a countrywide \u2018brain drain\u2019, were pushing for foreign-trained scientists to return home. \n               Taking a gamble \n             Miao decided to write to Chang, asking her permission to stay in the United States. A Chinese colleague thought this was a rash move, calling him a \u201cbloody fool\u201d, Miao remembers. But soon after, Miao received a letter from Chang granting her permission. \u201cI was stunned,\u201d he recalls. \u201cFor the first time in a long time, I wept.\u201d Chang's experiences in Stockholm, of course, meant that she knew only too well the difficulties of studying abroad. But she also understands the rewards. Despite the interruption by the Cultural Revolution, her research in Sweden did much to rework the evolutionary tree for fish \u2014 and sparked some very lively debate. Hans-Peter Schultze, a palaeontologist who was doing a postdoc at the Stockholm museum in the early 1960s, remembers the rumours of fabulous specimens from the early Devonian that Chang had brought from quarries in Yunnan province. At the time, palaeontologists regularly argued about the evolutionary tree of fish before species evolved to move ashore. Such trees, or cladograms, are important in understanding historical biodiversity and specialized characteristics of current species. Swedish palaeontological icons Erik Stensi\u00f6 and Erik Jarvik \u2014 both now deceased \u2014 held strongly to a view about the split between two lineages of Devonian fish: lungfish (dipnoans) and lobe-finned fish (porolepiformes). Before Chang's work, there was no known species that shared characteristics from both these types of fish, which were the predecessors of creatures that later walked on land and breathed air. But Chang had a fossilized fish that did:  Youngolepis , a specimen dating from around 415 million years ago in the Devonian 3 . \u201cIt was very disturbing for them when Chang brought the new form,\u201d says Schultze, who is now at the University of Kansas. \u201cJarvik called  Youngolepis  the \u2018devil's fish\u2019.\u201d In jest, Chang later used that epithet to name another Chinese specimen,  Diabolepis , which furthered her theories of the link between lungfish and lobe-finned fish. Her specimens \u201cbecame pivotal in strengthening the connection\u201d between these species, says Schultze, and helped to lay the groundwork for Chang to propose an evolutionary history for the fishes. Debate over this history continues today, with some authors using cladograms to challenge her conclusions about how closely  Youngolepis  and  Diabolepis  are related to the dipnoans. Lars Werdelin, a graduate student in Stockholm when Chang returned to complete her graduate degree, says her understated manner made her data even more convincing. \u201cShe doesn't stretch the evidence,\u201d says Werdelin, now senior vertebrate curator at the Stockholm museum. \u201cShe is not prone to hyperbole. When she says something, you believe it.\u201d Although colleagues often tell of Chang's personal warmth, they acknowledge that she also has a steely, determined side. In the late 1950s, she was a student leader charting a future field trip in a dangerous area of Kazakhstan. Outsiders were loathed then, and hotels used to deny foreigners a room. \u201cShe demanded a room \u2014 arguing, patting her side and saying: \u2018I have money. I have money\u2019,\u201d says Ke-Qin Gao, a palaeontologist at Peking University in Beijing, who heard the story later. \u201cShe was fearless.\u201d And she got the room. Today, Chang never tells such stories. Asked about her successes, she brushes aside the questions, seeking credit for her students and colleagues. Fortunately, her students and colleagues have found a way to honour her record. Xiaobo Yu, a palaeontologist at Kean University in Union, New Jersey, is preparing a book based on the Mesa symposium. Yu couldn't go to college during the Cultural Revolution. But afterwards, Chang took him on as her first graduate student. It set him on a course to receive his doctorate from Yale University. \n                     Institute of Vertebrate Paleontology and Paleoanthropology \n                   Reprints and Permissions"},
{"file_id": "439256a", "url": "https://www.nature.com/articles/439256a", "year": 2006, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "A collapse in ocean currents triggered by global warming could be catastrophic, but only now is the Atlantic circulation being properly monitored. Quirin Schiermeier investigates. Henry Ellis, captain of the British slave-trader  Earl of Halifax , had a scientific bent. While sailing the subtropical Atlantic in 1751, he measured water temperatures at different depths, using a thermometer, a long rope and a bucket fitted with flaps that sealed water inside the vessel when it was raised. Ellis was surprised to find the coldest water in a mid-ocean layer around 1,200 metres below the surface. The Sun, he concluded, did not warm the ocean in proportion to depth. The discovery proved useful for Ellis's crew: \u201cBy its means we supplied our cold bath, and cooled our wines or water at pleasure,\u201d he wrote in his notes. But the global significance of the Atlantic's cold depths escaped Ellis and pretty much everyone else for the next two centuries. He had stumbled upon the generator of a world-girdling system of currents \u2014 an enormous flow of water known as the \u2018global conveyor belt\u2019 1 , which transports warm surface water towards the poles and cold deep water back to the tropics. Driven by differences in temperature and salinity, this \u2018thermohaline\u2019 circulation has in recent years become infamous as the possible cause of major climatic upheaval. But only in the past year have much-needed automated systems been installed to monitor this circulation almost constantly. \u201cThere's a crying need for these data,\u201d says Gavin Schmidt, a leading climate modeller at the NASA Goddard Institute for Space Studies in New York. \u201cFor the first time we'll be able to observe the ocean \u2018weather\u2019 in all its complexity.\u201d The cold water Ellis had found in the Atlantic's depths comes from two regions at the ocean's north end, in the Greenland and Labrador seas. Here, saltier water coming northwards cools and sinks, before reversing south. This great submarine U-turn is peculiar to the waters of the North Atlantic, whose extreme cold temperatures and saltiness give it a higher density than is found in the Indian and Pacific oceans. Evidence from the ice ages suggests that shifts in the thermohaline circulation have dramatic effects on the temperature in western Europe and beyond; past shutdowns of the conveyor drastically cooled the climate all around the North Atlantic in a matter of years by stalling the currents that bring warm water northwards 2 . And computer models suggest that, in a seeming paradox, intense regional cooling could be triggered by global warming 3 . By the beginning of this century, the apparent fragility of the thermohaline circulation had made it by far the best-known exemplar of the surprising, non-linear and potentially catastrophic shifts in climate that makes the prospect of a greenhouse world so scary. \n               Current affairs \n             But the flows themselves remain surprisingly unmeasured. Until this year, almost all attempts to monitor what is happening in the Atlantic's depths have relied on some form of Captain Ellis's method \u2014 roaming along the surface and dredging up water from various depths as one goes. This year, scientists will have access to continuous measurements collected by 22 moored \u2018profilers\u2019 \u2014 sensors that travel up and down wires from buoys to moorings on the sea floor taking measurements as they go. The profilers were set up last year by a UK programme called Rapid Climate Change (RAPID), a \u00a320-million (US$35 million), six-year programme of the Natural Environment Research Council which has installed these profilers as part of a wider scheme to quantify the likelihood and magnitude of rapid climate change in the future. Climatologists worldwide are anxious to get hold of these data. The most recent shipboard study, published in  Nature  last year, suggested that the circulation might be yet more fragile than had been thought 4 . But at the same time, other research suggests that its potential to do harm may be much subtler than images of a Europe thrown into a mini ice age suppose. The idea that changes in ocean circulation might be a key determinant of climate change dates back to the early twentieth century and to the great American geologist Thomas Chamberlin. In the 1950s, the oceanographer Henry Stommel pioneered scientific understanding of the three-dimensional structure of the Earth's oceans, and of the currents that flow one way on the surface and another way at depth. But the theorizing that brought the North Atlantic branch of the great conveyor to its present fame dates back only to 1984, when Wallace Broecker, a geochemist at Lamont Doherty Geophysical Observatory at Columbia University in New York, attended a talk in Bern by Hans Oeschger, a Swiss climatologist. While Oeschger outlined his latest findings about climate instabilities and large oscillations of atmospheric carbon dioxide during the most recent ice age, it occurred to Broecker that a switching on and off of the thermohaline circulation in the North Atlantic could be the missing link. Temporary failure of the Atlantic conveyor could have wreaked havoc on climate, he thought. Although the carbon dioxide fluctuations Oeschger wanted to explain later proved to be artefacts, the idea that the conveyor could stop and start with planet-juddering effects took off. In 1985, Broecker and his colleagues published a landmark paper 5  drawing on early computer models of the ocean's flow. They proposed that the Atlantic circulation had two distinct stable modes \u2014 one with the conveyor on and one with it off \u2014 and that it was relatively easy for it to move from one mode to the other. The distinction between the two modes, they suggested, might explain the difference in climate between ice ages and warmer interglacials. Soon thereafter, computer models began to show that an increase of carbon dioxide in the atmosphere might, by increasing the temperature and thus the supply of fresh water to the North Atlantic, cause just such a shutdown. The idea of a threshold that, if passed, could cause calamity, or as Broecker termed it, \u201ca nasty surprise in the greenhouse\u201d, has played an increasingly important role in predicting the consequences of a greenhouse effect. In the late 1990s, William Calvin brought the idea to a wider audience with his article entitled \u2018The great climate flip flop,\u2019 which graced the cover of  The Atlantic  \u2014 as a neurophysiologist, Calvin had been interested in whether rapid climate change had been a decisive factor in human evolution. A few years later, a 2003 report for the Pentagon, \u2018Imagining the unthinkable\u2019, described how rapid climate change caused by such a shutdown could pose threats to whole societies and the peaceful coexistence of nations. Shortly thereafter, a film called  The Day after Tomorrow  pictured the citizenry of the United states chased over the Mexican border by an instant ice age; again, the North Atlantic was to blame. Given the thermohaline circulation's pivotal role in discussions of climate change, there was much excitement when, last November,  Nature  published evidence suggesting that the system could have slowed down dramatically 4 . The evidence had been gathered by Harry Bryden, an oceanographer at the University of Southampton, UK, and his team, while on a research cruise that also put the finishing touches to the deployment of RAPID. Comparing their 2004 measurements with data from 1957, 1981, 1992 and 1998, Bryden and his colleagues found that some of the warm surface water that used to flow northwards now seemed to remain trapped in the subtropical Atlantic, looping east and then returning south rather than heading north. Altogether, the \u2018overturning\u2019 circulation at 25\u00b0 N \u2014 the latitude where Ellis had first probed the ocean 250 years before \u2014 seemed to have decreased by about 30%. \n               In too deep \n             The result came as a surprise to those in the field. Few scientists had thought that such dramatic slowing of the thermohaline circulation could happen so soon. Models suggest 6  that the increase in fresh water needed for a conveyor shutdown would not be expected without a global warming of 4\u20135\u00b0C; warming in the twentieth century is currently put at 0.6\u00b0C (ref.  3 ). The most complex computer models of the climate and oceans, the sort used to make climate predictions for the Intergovernmental Panel on Climate Change (IPCC), suggest that the flow might be expected to slow by an average of 25% by the end of the twenty-first century, but not to shut down completely 3 . Running complex models long enough to simulate some sorts of change, however, uses an unfeasible amount of computing power. So for some purposes \u2018intermediate\u2019 models can capture things better. Stefan Rahmstorf, an oceanographer at the Potsdam Institute for Climate Impact Research (PIK) in Germany recently compared the circulation's response to an influx of fresh water in 11 simpler models; all showed a threshold, called the bifurcation point, beyond which the thermohaline circulation cannot be sustained 7 . The size of the threshold suggests that the possibility of shutdown is real, but not immediate. Rahmstorf says, \u201cIt is very unlikely that it will become really critical for the thermohaline circulation within the next 100 years.\u201d This is not to say that freshwater flows are not increasing; they are. The annual runoff of the six mightiest rivers draining into the Arctic Ocean, including Russia's Ob, Lena and Yenisey, is now 128 cubic kilometres greater than it was when routine measurements began 70 years ago 8 , an increase of about 7%. In addition, rising temperatures are making sea ice melt more rapidly. Perhaps most important, the huge Greenland ice sheet is showing worrying signs of disintegration; it is currently thought to be shrinking by 50 cubic kilometres per year 9 . Ruth Curry, an oceanographer at Woods Hole Oceanographic Institution in Massachusetts, has investigated how much of this extra fresh water lingers in the parts of the Greenland and Labrador seas that are critical for the functioning of the thermohaline circulation. Her recent analysis 10  of 1950 to 2005 salinity data suggests that 4,000 cubic kilometres \u2014 eight times the annual outflow of the Mississippi river \u2014 of fresh water have accumulated in the upper ocean layers since the 1960s. \u201cThe extra freshwater input is beginning to affect density,\u201d she says. But the amount of fresh water needed to shut down the thermohaline circulation in Rahmstorf's comparisons is an order of magnitude greater than the flux reported by Curry, and she agrees that the circulation will not be unduly affected this century. Peter Wadhams, an oceanographer at the University of Cambridge, UK, last year reported a substantial weakening of convection \u2018chimneys\u2019 down which surface water flows in the Greenland sea, but it is unknown how much of the observed effect is due to natural variability. This is all hard to reconcile with Bryden's findings, which suggest that a strong slowdown is already under way. \u201cSomething strange is going on here,\u201d says Michael Schlesinger, a climate modeller at the University of Illinois at Urbana-Champaign who views the possibility of a thermohaline circulation shutdown as more likely and more worrying than many of his peers. \u201cIf Bryden's findings are real it means that the circulation is much more sensitive to fresh water than any model has ever predicted.\u201d It is not just that the results are unexpected \u2014 they also seem hard to reconcile with other data. If the circulation were slowing down as Bryden suggests, one might expect that Europe would already be getting colder. The North Atlantic transports around a petawatt of heat \u2014 equivalent to the thermal output of about 500,000 large power stations \u2014 towards Europe. Interrupting that flow should have a cooling effect on the climate, but no such change has been seen. \n               A fragile balance \n             It may be that the system has a previously unexpected level of natural variation. Or it could be that Bryden recorded noise, rather than a signal \u2014 did a set of readings, through coincidence, the presence of ocean eddies and other natural disturbances, make it seem that the circulation was slowing when it wasn't? A statistical artefact cannot be excluded. \u201cThe results are based, after all, on just five snapshots of an extremely noisy and under-sampled system,\u201d says Carl Wunsch, a physical oceanographer at the Massachusetts Institute of Technology, who harbours long-standing doubts about the significance of the thermohaline circulation and its possible shutdown. \u201cThe story is appealing, but it is a very extreme interpretation of the data. It's like measuring temperatures in Hamburg on five random days and then concluding that the climate is getting warmer or colder.\u201d In response to his critics, Bryden points to data on the density of the ocean at various depths gathered at the same time as the flow readings, which provide independent support for the idea that the circulation is slowing. But although other scientists are less harsh than Wunsch, many remain cautious. \u201cBryden's results are extraordinary,\u201d says Schmidt, \u201cbut this is exactly why they also require extraordinary evidence.\u201d If Bryden's results are correct, there is another explanation of the lack of cooling in Europe: that a slowdown of the thermohaline circulation will not have the dire effects that have been suggested. It may be that, in today's climate, the role of the thermohaline circulation in warming Europe has been overestimated. A paper published in 2002 suggested that the westerlies, the dominant winds in mid-latitudes that blow from west to east play a much larger role than was long thought 11 . But much of the heat transported in the atmosphere ultimately comes from the ocean. \u201cIt is true that the atmosphere does the heavy lifting,\u201d says Jeff Severinghaus, an oceanographer at the Scripps Institution of Oceanography in La Jolla, California, who was once a student of Broecker's. \u201cBut the ocean exerts the control, just like the driver of a car.\u201d Evidence for the huge effects of past thermohaline shutdowns is near indisputable. The best case is that of a 1,300-year cold period that occurred around 12,000 years ago, known as the Younger Dryas. The carbon isotope ratios in fossilized plankton from the period suggest that the thermohaline circulation was much slower than it is today (slow circulation allows light carbon isotopes to build up near the ocean's surface). This slowdown coincided with a vast surge of fresh water into the North Atlantic. The melting of the ice-caps as the ice age ended created a vast reservoir of fresh water known as Lake Agassiz. It was far larger than any of today's Great Lakes, over parts of Minnesota, Dakota and Manitoba \u2014 Lake Agassiz. To the east, the lake was bordered by a tongue of the Laurentide ice sheet. When the tongue collapsed, a huge amount of water flooded down the St Lawrence River and into the Atlantic. According to ice cores drilled in Greenland, similarly large temperature oscillations \u2014 the Daansgard-Oeschger events that first piqued Broecker's interest in the 1980s \u2014 took place throughout the 90,000 years of the most recent ice age. It is likely that they were also caused by the thermohaline circulation stalling. But in this respect, as in others, the past may not be a straightforward guide to the present. The consequences of a shutdown could depend on the climate at the time the current stalls. Broecker now believes that the cooling in the Younger Dryas and the Daansgard-Oeschger events came about because the shutdown of the thermohaline circulation was exacerbated by a positive feedback, in the form of enhanced winter sea-ice formation. An influx of fresh water at high latitudes encourages the formation of sea ice, because fresh water freezes more easily. Because ice reflects sunlight, and stops heat from the ocean below reaching the atmosphere, spreading sea ice would strongly amplify cooling due to thermohaline slowdown, especially in winter. Studies of moraines in Greenland and Scandinavia show that during the Younger Dryas the cooling in summers was relatively moderate, whereas in wintertime temperatures must have been more than 30\u00b0C lower than now. It is hard to evaluate the amplifying role of sea ice very precisely. Most coupled ocean\u2013atmosphere models include a sea-ice component, but the representation is crude and leads to an unrealistic simulation of sea-ice distributions. If this feedback is as important as Broecker thinks, then the effects of a thermohaline circulation shutdown in a warmed world may be very different from those seen during the ice ages and their immediate aftermath. Today, satellite images show sea-ice cover at a historic low. In a world that had undergone the degree of warming needed to trigger a thermohaline shutdown in most models there would be almost none. Rahmstorf speaks for many climate researchers when he rejects the idea that a thermohaline shutdown in today's climate would lead to a rerun of the Younger Dryas, in which large parts of Europe were frozen. \u201cYou can't just assume a linear relationship and say that everything will happen on a 5\u00b0 higher level,\u201d says Rahmstorf. Broecker still believes that global warming may have surprises in store, possibly including a collapse of the thermohaline circulation, but he agrees that \u201cthe notion that it may trigger a mini ice age is a myth\u201d. \n               Earth watch \n             The fact that a future shutdown might not have the predicted effects on climate might go some way to explaining how Bryden could observe the circulation slowing \u2014 or at least fluctuating \u2014 without major climatic consequences, at least so far. Although Severinghaus agrees that this may be part of the story, he and many of his peers would rather believe that there was a randomly wrong signal in the data. \u201cIt just doesn't quite fit,\u201d says Schmidt. \u201cIf the circulation has been 30% down for a decade, it should at least have produced a 1\u20132\u00b0 drop in sea surface temperature even if it didn't cool Europe. But no such thing has been observed.\u201d Bryden says that the new RAPID system for monitoring flow in the Atlantic should allow them to know within a decade whether they found a long-term slowdown or a natural fluctuation. Other new approaches may also help. The ARGO system, part of the international Global Climate Observing System, is a fleet of robotic floats that monitors temperature, salinity and current in the upper 2,000 metres of the Indian, Atlantic and Pacific oceans. The free-drifting floats sink to pre-established depths and then surface to transmit their data to satellites. ARGO data are invaluable for monitoring changes in remote ocean regions, according to Lynn Talley, a physical oceanographer of the Scripps Institution of Oceanography in San Diego, California. For example, they have already revealed a spectacular warming of the southern ocean surrounding Antarctica, she says. And  in situ  monitors are not the only way of keeping an eye on the deep ocean. A weakening of the thermohaline circulation would change the entire topography of the sea surface, says Rahmstorf. Such large-scale changes could be picked up by satellites. A recent simulation 12  suggests that the sea level of the North Atlantic could rise locally by up to a metre as a result of adjustments to the density flows below the surface; in some regions the rate of change could be up to 2.5 centimetres per year. Scientists have begun using satellite altimetry to see if such changes are already observable; again, they expect robust results within a decade. Modellers also have much to do. Most model studies, such as those used by the IPCC, look at how a freshwater-induced shutdown of the thermohaline circulation might change temperatures if everything else remained the same. A harder question is what a shutdown might mean in a world that is, on average, getting warmer. Bryden's findings have caused a stir throughout the climate research community; lead authors of the chapters on ocean physics and circulation in the IPCC's fourth assessment, due in 2007, are reworking their submissions. \n               Future unknown \n             Wolfgang Cramer, an ecologist at PIK, predicts complex changes in the climate, with some effects exacerbating each other and some that cancel each other out. For example, Cramer says, meteorological perturbations caused by a thermohaline shutdown could lead to a dramatic increase in the frequency of major floods and storms in large parts of Europe even if overall temperatures do not drop. \u201cIt's not the mean, it's the extremes that are most worrying,\u201d he says. One aspect of the problem is that the thermohaline circulation is not just a climatic affair. Its effect on ocean circulations means it influences the rates at which nutrient-rich bottom water rises to the surface all around the world. A recent simulation suggests a shutdown might lead North Atlantic plankton stocks to collapse to less than half their current biomass 13 . Globally, a decline of more than 20% might be expected thanks to reduced upwelling of nutrient-rich deep water and gradual depletion of upper-ocean nutrient concentrations. \u201cPlankton builds the base of the marine food web. So a decline in global plankton biomass and productivity can be expected to have consequences for fish, squid and whales as well,\u201d says Andreas Schmittner, a climate researcher at Oregon State University in Corvallis. \u201cA weaker Atlantic overturning circulation could result in a reduced fish supply to people living along the shore lines of the Pacific and Indian Oceans.\u201d Other possible effects of a shutdown predicted by models include warming in the tropics, or, rather surprisingly, over Alaska and Antarctica. Rainfall patterns might change, too. A southern shift of the thermal equator \u2014 which has accompanied thermohaline circulation shutdowns during ice ages \u2014 could lead to monsoon failures, and droughts in Asia and the Sahel region, says Severinghaus, and these effects seem to be independent of sea ice. Such shifts could have severe consequences for poor farmers in many parts of the world, consequences that may be considerably more disruptive than colder winters in affluent northern Europe, says Severinghaus. And, as Schlesinger points out, a weakening or stopping of the thermohaline circulation would reduce the carbon dioxide uptake of the ocean, which would mean a positive feedback on global warming. The oceans currently absorb about a third of the carbon dioxide released from fossil fuels, although the proportion is set to decrease as emissions climb. Some 250 years after Captain Ellis first probed the Atlantic, its depths still hold secrets and threats. Even in a new age of constant monitoring and improved modelling, it will be some time before the likelihood, and the probable effects, of a thermohaline circulation slowdown can be predicted with accuracy. The intricacies of a system that depends on delicate balances between fresh and salt water over vast ocean basins, on the details of atmospheric circulation, wind-driven currents and the topography of deep sea floors will not yield answers quickly. \u201cIf you would like to learn how a planet operates you would probably not choose the Earth,\u201d remarks Schlesinger. We greenhouse dwellers, alas, do not have a choice. \n                     Slowing of the Atlantic meridional overturning circulation at 25\u00b0 N \n                   \n                     Oceanography: The Atlantic heat conveyor slows \n                   \n                     A change in the freshwater balance of the Atlantic Ocean over the past four decades \n                   \n                     Rapid changes of glacial climate simulated in a coupled climate model \n                   \n                     NERC's rapid climate change programme \n                   \n                     The IPCC \n                   \n                     Spencer Weart's The Discovery of Global Warming \n                   \n                     Realclimate: commentary on climate issues \n                   Reprints and Permissions"},
{"file_id": "439382a", "url": "https://www.nature.com/articles/439382a", "year": 2006, "authors": [{"name": "Apoorva Mandavilli"}], "parsed_as_year": "2006_or_before", "body": "SARS caught China unawares. But the ensuing struggle to characterize and contain the virus has put the country's work on infectious diseases back on target. Apoorva Mandavilli reports. Like anyone who was in Beijing in the spring of 2003, Hongkui Deng remembers it vividly. The Chinese government could no longer deny that the country was in the grip of a new and potentially fatal disease: severe acute respiratory syndrome (SARS). By July, the epidemic would have spread, affecting more than 8,000 people worldwide and claiming 813 lives; but in April, the panic was already palpable. Normally bustling, the streets of Beijing were virtually deserted. The few people who ventured out wore masks and gloves, and avoided even eye contact with others. Cinemas, schools and shops were closed. It was, as many describe it, frightening and eerie \u2014 even apocalyptic. \u201cEveryone was scared,\u201d Deng recalls. Deng, a cell biologist, had returned home in 2001 after more than a decade in the United States. Now based at Peking University, he was pursuing his research on embryonic stem cells. Returning from a conference in April 2003, he learnt that the mother of one of his students had SARS. Once officials had sprayed the lab, Deng's students began asking if they could work on the disease that was paralysing the nation. \u201cEverybody wanted to do something,\u201d he says. Deng had limited experience in virology, apart from a short stint working on HIV, and his students had even less. But like many other scientists in China, the team saw research on SARS as both an opportunity and a duty, and set about mastering the basics \u2014 fast. \n               Feverish activity \n             For at least six months, Deng's lab stopped working on stem cells and focused entirely on SARS. It wasn't alone. Across the country, scientists trained in protein science, anatomy, immunology and biochemistry \u2014 almost anybody who could contribute in any way \u2014 were shelving their normal projects. \u201cEveryone was working on SARS,\u201d says Deng. \u201cYou just had to.\u201d That commitment has paid off. Although China still faces a great many hurdles, its government and scientific community are becoming better prepared to combat epidemics, say some US scientists. Long after global interest in SARS has waned, Chinese scientists are still publishing important work on the disease. In September 2005, for instance, one team identified bats as a natural reservoir of the SARS virus 1 . And in October, another group reported a molecule that can inhibit the replication of a wide range of SARS-like viruses 2 . In part, the boom in China's infectious-disease research reflects the country's growing activity in science. Government spending on the sector in 2004 was up 16% on the previous year, outpacing the country's economic growth of about 10% per year. Since the late 1990s, the number of Chinese papers published in international journals has risen dramatically 3  and the number of domestic patent applications has also gone up. But SARS was in many ways a wake-up call. It served as a grim preview of the disaster \u2014 and public humiliation \u2014 that awaited China if it did not take infectious diseases seriously 4 . Since 2003, two new infectious-disease institutes \u2014 one set up by the Chinese Academy of Sciences, the other a joint effort with the Pasteur Institute in Paris \u2014 have been set up in China. Plans for another, a collaboration with Columbia University in New York, are expected to be announced next month. Chinese scientists say that an increasing number of students are choosing to work in infectious disease, and many scientific collaborations born during the SARS epidemic are still thriving. \u201cBefore SARS, scientists routinely didn't work together,\u201d says Deng. \u201cChina is doing much better now.\u201d China is a hotbed of infectious diseases: according to agencies such as the World Health Organization (WHO), hepatitis, tuberculosis, diarrhoea, encephalitis and HIV are all prevalent at epidemic rates. And with people, poultry and animals of all kinds living in close quarters, there are plenty of opportunities for pathogens to jump between species. SARS was an unknown entity when it struck, and was probably spread to people by infected palm civets in China's wild-animal markets. In 2005, the country also saw an unusual cluster of infections from the pig bacterium  Streptococcus suis , which jumped from swine to infect more than 200 people. These days, all eyes are on avian influenza, and the government has already banned poultry from markets and culled millions of birds in an attempt to prevent the virus from jumping to humans. \n               Slow start \n             Given such dangers, infectious-disease experts are delighted at the changes in China. \u201cI think SARS prepared the ground for being willing to work with emerging infectious diseases,\u201d says Robert Webster, a leading flu expert at St Jude Children's Research Hospital in Memphis, Tennessee. Looking at the way China has handled outbreaks of bird flu, Webster says, the government seems to be much more transparent and willing to share information than it was during the SARS crisis, when it denied the existence of the disease for several months. \u201cThere are still problems out there but the situation is much improved,\u201d he says. During the initial months of the SARS epidemic, scientists on China's mainland contributed little to the understanding of the disease. Even in May 2003, weeks after virologist Albert Osterhaus of Erasmus University in Rotterdam, the Netherlands, and his colleagues had identified a novel form of coronavirus as the cause of SARS 5 , some senior Chinese scientists still refused to acknowledge that it was the culprit, backing  Chlamydia  bacteria instead. In fact, researchers at a Chinese military institute in Beijing had come to a conclusion similar to that of Osterhaus as early as March, but internal politics reportedly stifled their finding. At a May meeting in Beijing organized by the Chinese Academy of Sciences, Ian Lipkin witnessed the disagreements firsthand. \u201cEven at this point, there was this argument back and forth about who was right and who was wrong,\u201d recalls Lipkin, director of Columbia University's Jerome L. and Dawn Greene Infectious Disease Laboratory. Ultimately, several government officials were sacked over the handling of the crisis. But after its initial failures \u2014 for which China was openly chastised by the international community \u2014 things changed dramatically. Lipkin says that an enormous medical facility, intended to quarantine and treat those infected, was built about 50 kilometres outside Shanghai in just a few months. At the May meeting, Lipkin had presented a roadmap for the government to help it combat SARS, covering drugs, vaccines and research on the basic biology of the virus. By the time he returned to China in July, \u201cthey had virtually finished everything there was on this roadmap\u201d, he says. \u201cI can't tell you how impressed I was. It was extraordinary.\u201d The government also realized the need for cooperating both with the international health agencies and with its own scientists. The ministries made special concessions. They created funding pools, extended grants that were about to expire and forgave deadlines so that researchers could focus on SARS. The incentives worked, drawing some of China's most successful scientists to the field. Among them was Zihe Rao, a structural biologist who heads the Institute of Biophysics in Beijing. By October 2003, Rao's team had published the structure of a SARS protein 6  and has since designed a molecule that inhibits the replication of SARS and several related coronaviruses 2 . These days, more than half of Rao's lab still works on infectious diseases. \u201cThese are emerging and re-emerging diseases. This is research that will be very useful,\u201d says Rao. \n               Learning curve \n             Will SARS come back? Quite possibly. Zhihong Hu, director of the Wuhan Institute of Virology, is tracking the SARS coronavirus in palm civets, looking in particular for a strain with the two mutations required for the virus to jump to humans 7 . She says there is some evidence that the strain is already circulating in farm civets, but is waiting for confirmation before publishing her findings. Hu says her work is progressing slowly, in part because research on SARS now faces more obstacles than it did initially, which to some degree results from a shift in government attitudes. At the height of the epidemic, the government had few restrictions on who could work with the coronavirus. But that stopped abruptly in April 2004, after it was discovered that the virus had escaped the previous month from the Chinese National Institute of Virology in Beijing. Although the lab was among the few in China equipped to work with dangerous pathogens, the incident prompted the government to introduce strict regulations. During the SARS epidemic, the Chinese Center for Disease Control and Prevention (CDC) freely shared samples of the SARS virus with various scientists, including Guo-Ping Zhao, executive director of the Chinese National Human Genome Center in Shanghai. Zhao had never worked with viruses before but he and his team quickly learned enough to track the evolution of the virus through sequence analysis. In just four months, they had completed their analysis and soon after, published a paper showing the molecular evolution of the virus during the epidemic 8 . Apart from a crash course in epidemiology, Zhao says the group learned to sequence viruses from tissue samples. This expertise, he adds, could prove useful with bird flu. But that is if China can straighten out the remaining kinks in its research structure. Not surprisingly, the Chinese CDC and the Ministry of Agriculture keep a tight rein on SARS and avian flu viruses and allow only the few labs that have biosafety level 3 facilities to work with them. But tissue samples, such those used by Zhao, are similarly controlled. \u201cI think the key issue in China now is all the government agencies,\u201d says Zhao. The government may have learned to share more openly with the WHO and with international experts, but it is not open enough with Chinese scientists, he says. \u201cThe government agencies should learn how to work together with research institutes,\u201d he says. \u201cI think Dr Webster in the United States knows much more information than I do.\u201d That may be so, but the international lines of communication are not trouble-free: the WHO, for example, has complained on several occasions that it has yet to be granted access to virus samples from last year's outbreak of bird flu. Still, Zhao and others say that they are optimistic about China's growing expertise in infectious diseases. \u201cI think we are learning, but it's not so easy to change in one night,\u201d Zhao says. \u201cThings have improved a lot; I hope they improve more.\u201d \n                     Bird flu 2005: the ongoing story \n                   \n                     SARS threatens reappearance in China \n                   \n                     Antibodies to SARS-like virus hint at repeated infections \n                   \n                     Bird flu web focus \n                   \n                     Avian Flu web focus \n                   \n                     WHO info on China \n                   \n                     Chinese Center for Disease Control and Prevention \n                   Reprints and Permissions"},
{"file_id": "439526a", "url": "https://www.nature.com/articles/439526a", "year": 2006, "authors": [{"name": "Jeff Kanipe"}], "parsed_as_year": "2006_or_before", "body": "Two decades after plans were set in motion for the world's most powerful ground-based telescope, astronomers are bracing themselves for a downgrade to curb escalating costs. Jeff Kanipe reports. The view from the sprawling Llano de Chajnantor plateau perched in the Andes Mountains of northern Chile is spectacular all year round. The plateau lies some 5,000 metres above sea level, with the skies above forming a deep blue backdrop to the tawny, windswept desert floor and the distant badlands of low hills and volcanoes. Inhospitable to life, the plateau has become destination number one for radio astronomers, who probe the mysteries of the Universe by measuring radio waves emitted by celestial objects (see  \u2018Transformational telescopes\u2019  on page 528). The high altitude and dry atmosphere are perfect for observing objects that emit submillimetre wavelengths; these wavelengths are in the spectral window that lies between the far infrared and high-frequency radio bands. Some of these wavelengths are absorbed by water vapour, ruling out observations at lower altitudes. Radio astronomers have been hoping for decades to be able to explore this window with the right telescope at the right location. They will eventually get their wish when the international Atacama Large Millimeter Array (ALMA) is built on the Chajnantor plateau. By combining 64 radio antennas, each 12 metres in diameter, ALMA was originally projected to be 10 to 100 times more sensitive than current high-altitude telescopes. During a meeting with astronomers in Washington DC last month, Adrian Russell, ALMA's US project director, described recent progress on the project, almost as if he could not believe it himself. \u201cALMA is actually happening,\u201d Russell announced. \u201cIt's real.\u201d Attendees at the American Astronomical Society meeting had come to hear Russell and Fred Lo, director of the National Radio Astronomy Observatory (NRAO), which is responsible for US construction and operations, update them on the status and scientific prospects of ALMA. After their talks, Lo and Russell fielded technical questions from the audience, but one caught them flat-footed: how much was the project going to cost? Lo replied that because the National Science Foundation (NSF) was reviewing ALMA's US funding, it would be \u201cdamaging to the review process\u201d to say more at this stage. What went unsaid is the fact that ballooning costs for commodities such as petroleum and steel are forcing ALMA scientists to consider the unthinkable: whether to reduce the size and sensitivity of their dream telescope. Instead of the hoped-for 64 antennas, they may have to do with 50 or even fewer. The seeds for ALMA were sown in 1982, when an NSF committee proposed building an array of antennas up to two kilometres across, which would operate at millimetre wavelengths. A decade later, the National Research Council gave the project \u2014 then called the Millimeter Array \u2014 its blessing. In that incarnation, the array would have consisted of 40 8-metre radio dishes that could be switched from an array 70 metres across to one 3 kilometres in diameter. \n               Plans afoot \n             It is not unusual for radio arrays to have movable antennas. Changing the diameter of the array affects the fine detail seen by the telescope \u2014 what astronomers call angular resolution \u2014 and the whole movable array also works a bit like a zoom lens. For example, astronomers who want to map the large-scale structure of a galaxy use the smallest configuration; otherwise they would have a limited viewing angle (like looking at a freckle on someone's nose rather than on their whole face). If an interesting \u2018freckle\u2019 is detected then they can \u2018zoom\u2019 in and observe it in detail using the larger configuration. Around the time the United States was planning the Millimeter Array project, European astronomers were discussing plans for an array in the Southern Hemisphere \u2014 the Large Southern Array \u2014 whose combined resolution would be equivalent to a dish 10 kilometres across. Despite the fact that the European array was not originally intended to work in the crucial submillimetre range, both sides began discussing a partnership in 1997. Two years later, an agreement was signed to merge the projects into ALMA. Today, Canada, Chile, Japan and Taiwan have all joined the collaboration, making it one of the first truly global projects in ground-based astronomy. And like most big projects it has a big price tag \u2014 about $650 million \u2014 making it, so far, the most costly ground-based telescope ever. ALMA's specialty will be observing cooler parts of the Universe the details of which are easiest to make out in the submillimetre spectrum. From interstellar dust to star-forming giant molecular clouds, many cold features in the cosmos have been glimpsed only by using high-altitude telescopes and special imaging techniques, or with long observation times. Scientists predict that ALMA will be able to image galaxies that formed up to 500 million years after the Big Bang \u2014 when the Universe was dark and star formation was just beginning. \u201cThese galaxies are truly spectacular, with star formation rates up to a thousand times that of the Milky Way today,\u201d says Jason Glenn, an astronomer at the University of Colorado and ALMA science advisory committee member. ALMA will give astronomers data on how fast the galaxies formed, and on the evolving structure of the Universe. Most of ALMA's multi-million-dollar budget will be spent on antenna construction. The original plan called for enough antennas to form an array diameter that ranges from 150 metres to 14 kilometres across. In this plan, the smaller array size could be used to detect planet-forming disks around multiple stars in a galaxy, whereas the larger array would allow astronomers to scrutinize each disk more carefully, say, for the presence of Jupiter-size planets. \n               Price hike \n             ALMA's construction phase started in earnest in November 2003. But in 2004, the estimated costs started to balloon. Chinese demand for structural steel drove steel prices through the roof. Petroleum prices, too, more than doubled, inflating the cost of antenna dishes made from petroleum products. Even a boom in the Chilean economy translated to increased operational costs in that country. The NSF, which is responsible for half of ALMA's construction costs, has to spread its funding across several high-priority projects, including the IceCube Neutrino Observatory at the South Pole and the EarthScope project in North America. Although ALMA's share jumped from $29.8 million in 2003 to about $50 million in 2004, it hasn't risen since. With commodity prices unlikely to decline anytime soon, this means that ALMA could easily spend its $650 million before all the antennas are built. Facing the prospect of cost overruns, the NSF decided late last year to do a spending review. The chances that ALMA will be downsized to 50 or even 40 antennas alarms some radio astronomers. Fewer antennas will mean reduced sensitivity and much longer exposure times to obtain the same resolution. Last year, the US National Academy of Sciences issued a report on the effect a smaller array would have on ALMA's three primary science goals. The first of these, to detect Milky-Way-type galaxies that formed two to three billion years after the Big Bang, would help trace these galaxies' evolution \u2014 something about which astronomers know next to nothing. The second, to obtain high-contrast images of nearby protostars and planet-forming disks around young stars, would help astronomers study the formation of these cosmic structures. And the third, to produce precise images at very high angular resolution, would help reveal the motions of gas within objects such as giant molecular clouds. The academy's report was discouraging. In their view, the first two requirements for these goals, involving sensitivity and high-contrast imaging, would not be met by either a 40- or 50-antenna array. With 40 antennas, the third requirement would also be at risk, even if extremely long exposure times were allowed. Reduced speed and image fidelity would be among the biggest challenges. For ex-ample, the time taken to observe a nearby stellar disk would increase from three days (with 64 antennas) to a week (with 40), and image fidelity would be reduced threefold. Despite the finding that it would fail to complete two of its three goals, the report concluded that a pruned-back ALMA could still address many unanswered questions in astronomy. Michael Turner, the NSF's assistant director for mathematical and physical sciences, is similarly hopeful: \u201cThe original ALMA plan has so much more capability over what already exists \u2014 by more than an order of magnitude, closer to two orders of magnitude \u2014 that even if you cut back by 40% what it can do, it's still transformational.\u201d But if ALMA is to be truly transformational, \u201cit is critical that we do this instrument right, not halfway,\u201d argues Lee Mundy, a member of ALMA's science advisory committee and a radio astronomer at the University of Maryland. According to Mundy, \u201cchipping away at the number of antennas and collecting area saves dollars but it doesn't make sense for an instrument that can produce premier science for the next 30 years and more.\u201d Lucy Ziurys, an astrochemist with the University of Arizona, says that a 50-antenna array is \u201cokay, but disappointing\u201d. Although she believes that reducing the number of antennas is a better response to funding problems than reducing, say, antenna quality, she questions why certain costly decisions were made to begin with. \u201cFunds could have been saved if only one antenna design had been chosen instead of two,\u201d she says. \u201cThat decision will, in all likelihood, increase the cost.\u201d Ziurys is referring to two contracts signed last year. On the US side, the defense contractor General Dynamics, based in Falls Church, Virginia, agreed to supply 25 antennas in July. A similar contract was signed in December between the European Southern Observatory and Alcatel Alenia Space, a European consortium of manufacturers. Ziurys argues that having two antenna designs means maintaining \u201ctwo of everything\u201d \u2014 two sets of spare components and electronics. \n               Unknown Universe \n             This coming March, the international ALMA board will meet in Kyoto, Japan, to finalize a delivery schedule for the rest of the project. That's when the NSF's decision to finance 50, or fewer, antennas will be made public. Last October, however, ALMA scientists reconfigured the antenna array at the Chajnantor site for 50 antennas. So the decision may have already been made. Ziurys thinks the decision to go with 50 was probably made as early as January last year. Although their research may be adversely affected, most astronomers associated with ALMA seem resigned to the smaller array. Many remain hopeful that they can still achieve worthwhile science with 50 antennas. \u201cIn general, science goals do not become impossible when the number of antennas is reduced slightly, they just take more observing time,\u201d claims Glenn. George Djorgovski, an astrophysicist at the California Institute of Technology, Pasadena, sums it up with a prediction: \u201cWhat you have here is clearly a fiscal reality issue, and the politically considered response to it. We will see many more of those in the coming years.\u201d Meanwhile, ALMA continues to move forward. Construction is under way at the site and the first of the antennas is scheduled to arrive in early 2007, with completion scheduled for 2012. For its part, the National Astronomical Observatory of Japan has agreed to build four 12-metre and twelve 7-metre antennas to create the Atacama Compact Array alongside ALMA. This will be used for imaging large-scale structures not well sampled by the main array. Once the decision to go with 50 antennas has been made, could the project planners still build 64 at a later date? Not without an astronomical fairy godmother. \u201cBuying more antennas would be hard,\u201d Turner explains. \u201cYou configure the array to work optimally based on how many antennas you have, and to add more would force you to reconfigure the entire array.\u201d That costs more money, which will be in short supply. Will these few clouds spoil the view from Chajnantor? Paul Vanden Bout, former ALMA director, argues that new discoveries will erase any bleak memories. \u201cBig projects are a long haul,\u201d he says. \u201cBut it's worth it. ALMA is going to be a great telescope and it's going to do exceptional science.\u201d \n                     Funding battle heats up over large array \n                   \n                     Planet formation: Worlds apart \n                   \n                     news in brief \n                   \n                     National Radio Astronomy Observatory \n                   \n                     ALMA \n                   \n                     ESO \n                   \n                     National Academies Press report, \u201cThe Atacama Large Millimeter Array: Implications of a Potential Descope\u201d \n                   Reprints and Permissions"},
{"file_id": "439530a", "url": "https://www.nature.com/articles/439530a", "year": 2006, "authors": [{"name": "Carina Dennis"}], "parsed_as_year": "2006_or_before", "body": "A horrible facial cancer is decimating the Tasmanian devil population. But researchers in Australia think they have found a way to save the species. Carina Dennis reports. Checking her research traps at dawn, Billie Lazenby was saddened to find a female Tasmanian devil with a face marred by cancerous lumps. She had contracted devil facial-tumour disease, a deadly transmissible cancer that is threatening the survival of this feisty marsupial. When reviewing her records later that night, Lazenby, who is part of a scientific team monitoring the devils, was astonished to find that the animal, nicknamed Half pea, had been recorded nearly a year previously as having the disease. This made her one of the longest survivors the team had ever encountered. The fact that Half pea had resisted the tumour for at least twice as long as most other devils meant she might hold a clue for scientists trying to help the species. \u201cAmid this heartbreaking background of losing so many devils, suddenly this animal pops up and it's like, \u2018wow, maybe there is hope\u2019,\u201d says Lazenby. Since this discovery in June 2005, the team has found two other partly resistant females, and researchers now hope to identify the genetic and immunological factors that give devils ( Sarcophilus harrisii ) a better chance of fighting the disease. It is a long-awaited piece of good news after a decade of helplessly watching the animals decline. Although long-term strategies for tackling the problem are some way off, other teams have discovered a way to limit the spread of the disease at least. \n               Deadly love bites \n             Lazenby, a scientific officer at the Tasmanian state government's Department of Primary Industries, Water and Environment (DPIWE), based in Hobart, is part of a team led by Clare Hawkins that traps and checks devils for signs of the cancer. A severely diseased devil is a grotesque sight: large tumours protrude from the face and neck, sometimes pushing out teeth and invading eye sockets. As the tumours interfere with feeding, the animals become emaciated and usually die within six months of showing lesions. First documented by a wildlife photographer in 1996, the cancer has contributed to the deaths of up to 80% of infected devil populations in Tasmania 1 , the only place in the world where the animal is found. Devils normally have a life expectancy of about five years but \u201cnow it's rare to see an animal older than three years of age\u201d, says Lazenby. So far, the evidence points to the disease being transmitted by infectious cancerous cells when animals bite each others' faces in fights or during their tempestuous courtship 2 . In the long-term, a vaccine or therapy will be needed and this is where animals showing some resistance to the tumours may be invaluable. What's more, devils living in western Tasmania, which seem to be genetically distinct from their eastern cousins 3 , have so far remained free of the disease. Stephen Pyecroft, a veterinary pathologist at the DPIWE's animal-health laboratories in Mount Pleasant, is now scrutinizing the genes of devils from different regions to look for any natural genetic variation that may confer resistance. But in the short term, the devils' best chance lies with the work of Menna Jones, a biologist at the DPIWE and the University of Tasmania in Hobart, who is investigating whether removing sick animals from wild populations suppresses the disease. Jones has just completed a pilot study on two connected peninsulas on Tasmania's southeastern coast. These are ideal for restricting the movement of devils because they are separated from the mainland by a single bridge. She removed all infected animals from a 120-square-kilometre region in the peninsulas, and her unpublished results, presented at the 2005 Australasian Wildlife Management Society conference in Hobart, concluded that the population structure remained intact and no new cases of the disease were reported. \u201cThis work is the best hope for the devil yet,\u201d says Nick Mooney, a wildlife officer for the DPIWE. Andy Dobson, an ecologist studying infectious diseases at Princeton University, New Jersey, agrees: \u201cThe initial results are giving us grounds for optimism\u201d. \n               Total wipe-out \n             Jones is now conducting a larger two-year study to completely eradicate the disease from the peninsulas. \u201cIf successful, this will be the only disease-free reserve of Tasmanian devils,\u201d says Jones. They will safeguard the bridge against roaming devils and set up remote cameras across the region to detect any diseased animals that manage to avoid trapping. However, it's not clear that this approach will work for all areas. \u201cIn a larger open system, where animals can come and go, you may be pissing against the wind,\u201d says Mooney. Others are optimistic that tackling the problem from a multitude of angles will ultimately save the devil. \u201cYou can be a pragmatic scientist about it,\u201d says Pyecroft, \u201cbut you also have to have a bit of faith.\u201d \n                     Baiting plan to remove fox threat to Tasmanian wildlife \n                   \n                     DPIWE \n                   Reprints and Permissions"}
]