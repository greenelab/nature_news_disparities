[
{"file_id": "454795a", "url": "https://www.nature.com/articles/454795a", "year": 2008, "authors": [{"name": "Laura Bonetta"}], "parsed_as_year": "2006_or_before", "body": "Researchers now have access to a burgeoning collection of tools for unravelling the epigenome, which could lead to new drug targets and ways to track disease. Laura Bonetta reports.  Just as DNA mutations can cause disease, so too can changes in gene activity that don't depend on DNA sequence. For example, the addition of methyl groups to cytosines within the promoter regions of a gene, a process known as DNA methylation, typically represses the gene's activity. Similarly, the methylation, acetylation or ubiquitination of certain amino acids on histones \u2014 the main protein components of chromatin \u2014 can change the way DNA is wrapped around the histones and can affect which nearby genes are available for activation by regulatory proteins such as transcription factors. Such 'epigenetic' processes can have dire consequences. Many cancers and other diseases have aberrant DNA methylation at particular sites in the genome. And certain patterns of histone modification are key to biological events such as embryonic development, ageing and cell-cycle regulation. \u201cWe have found a gene that is highly methylated in more than 90% of colorectal cancers, regardless of stage and histology,\u201d says Achim Plum, senior vice-president for corporate development at Epigenomics in Berlin, Germany. \u201cYou will have a hard time finding a single somatic mutation that is as prevalent in this particular cancer.\u201d Because these epigenetic changes can potentially be reversed by drugs, they are good targets for the prevention and treatment of disease. To assess the effect of epigenetic regulation on health and disease, researchers are now cataloguing epigenetic variations across the genome \u2014 or epigenome \u2014 in different tissues and at various stages of development. \n               Chips with everything \n             Cytosine is typically methylated when it is next to guanine in what is known as a CpG dinucleotide. Although most CpGs are methylated in mammals, some are not methylated and these are usually grouped in clusters called CpG islands. These tend to be located in the 5\u2032 regulatory regions of genes. In many cancers, these CpG islands become hypermethylated, resulting in the heritable silencing of transcription of downstream genes.  DNA methylation can be detected in several ways. One method compares what happens when genomic DNA is digested by enzymes that are either sensitive or insensitive to methylation. Another approach uses chromatin immunoprecipitation, or ChIP. This involves crosslinking DNA with its associated proteins and then shearing the DNA. The fragments that contain methylated cytosine are extracted by immunoprecipitation with antibodies specific for 5-methylcytosine or fragments associated with other proteins such as transcription factors or histones. The immunoprecipitated DNA is purified, amplified and labelled with a fluorescent tag. This is then applied to the surface of a DNA microarray containing a set of probes \u2014 a procedure commonly referred to as ChIP-on-chip. \u201cThe genome is methylated at a low level everywhere, but it is more of a sprinkling. What we are looking for are regions with higher densities of methylation,\u201d says Mary Harper, chief scientific officer at Genpathway in San Diego, California. \u201cThese can be regulatory regions and there are tens of thousands of these regions across the genome.\u201d ChIP-on-chip has grown in popularity with the availability of high-density arrays of contiguously tiled oligonucleotide probes. Affymetrix of Santa Clara, California, offers whole-genome tiling yeast arrays with a resolution of 5 base pairs (a total of 3.2 million probes). The company also makes human and mouse whole-genome tiling arrays, each as a set of 14 arrays containing around 45 million probes at a spacing of 35 base pairs. Along with other companies such as NimbleGen in Madison, Wisconsin, and Agilent Technologies in Santa Clara, California, Affymetrix provides several array formats for ChIP-on-chip experiments. In addition to whole genome arrays, these companies sell CpG islands arrays, promoter arrays, ENCODE arrays (see  'Tackling the epigenome' ) or custom-made arrays. The choice of the array depends mainly on the type of experiment being done, the resolution needed and cost. \u201cDifferent arrays are not that different,\u201d says ChIP pioneer Kevin Struhl at Harvard Medical School in Boston, Massachusetts. He notes that the greatest source of variation among ChIP-on-chip experiments comes from the sample preparation and immunoprecipitation steps. He and his colleagues have found that although most commercially available arrays perform similarly, those carrying longer oligonucleotides, such as the ones from NimbleGen (50\u201375-mer) or Agilent (60-mer), are slightly more sensitive than shorter ones for lower levels of enrichment 1 .  Agilent make its arrays by inkjet printing, which allows it to respond quickly to design changes, says Kevin Meldrum, manager of the company's genomic collaborations. \u201cOnce an array design has been completed, the sequences are simply sent to the printer and the chip is produced.\u201d This allows Agilent to focus on custom arrays. \u201cRight now many scientists are conducting broad, whole genome scans,\u201d Meldrum says. \u201cBut once they identify regions of interest, it will be more cost effective to do more targeted studies.\u201d \n               All kinds of arrays \n             DNA microarrays have many applications in epigenomics. SwitchGear Genomics in Menlo Park, California, uses custom-designed oligonucleotide arrays from Affymetrix to profile DNA methylation patterns in genomic DNA treated with methylation-sensitive enzymes. The array design covers nearly all of the annotated CpG islands in the genome and about 20,000 additional CpG-rich regions the company has identified as potential regulatory regions. \u201cWe can profile 50,000 distinct 200\u2013800-base-pair regions in the genome and quickly determine whether they are methylated or not,\u201d says Nathan Trinklein, co-founder and chief executive of the company. Epigenomics uses a slightly different microarray design to offer a similar service, which it calls differential methylation hybridization (DMH). \u201cThe data we produce with DMH are highly reproducible and comparable across different projects,\u201d says Plum. \u201cWe are building a database of profiles for healthy and disease tissues.\u201d One of the advantages of using arrays for methylation studies, rather than for monitoring the expression of thousands of mRNAs at once (expression profiling), is that the signal is much easier to score. \u201cCompared with expression profiling there is better signal-to-noise,\u201d says Meldrum. \u201cGenerally what you are measuring is 'have I or have I not had any binding?'.\u201d \n               In sequence \n              As well as using microarrays to analyse ChIP results, the method can also be subjected to sequencing. This technique uses sequencing to localize the methylation sites or interaction points of proteins or histones. ChIP sequencing offers a couple of advantages over ChIP-on-chip. It is better suited for regions of DNA close to repetitive sequences, which can create 'noise' in a microarray experiment, and it can cover a larger portion of the genome. \u201cTiling arrays require masking of the repetitive regions of the genome,\u201d says Harper. \u201cWith sequencing, genome coverage is greater. We are evaluating the next-generation sequencing in combination with our methylated DNA assays to determine the potential for attaining the same high level of information with sequencing that we do with arrays.\u201d The main drawback of ChIP sequencing is that the throughput is much lower than is obtained with microarray screens. Although this will probably increase, sequencing studies are likely to remain a more expensive propositioncompared with microarrays. One of the first companies to perform faster 'next generation' sequencing was 454 Life Sciences of Branford, Connecticut, but its technology is not well suited to ChIP sequencing. \u201cFor ChIP sequencing you don't need long read runs and super accuracy like with the 454 technology,\u201d says Struhl. \u201cRight now Illumina [of San Diego, California] has a better technology for ChIP sequencing and others are being developed.\u201d Other sequencing technologies that can be applied to ChIP sequencing include the SOLiD system from Applied Biosystems in Foster City, California, and the single-molecule sequencing process from Helicos BioSciences in Cambridge, Massachusetts. \n               Bisulphite treatment \n             Another way to detect methylation is to modify DNA using sodium bisulphite, which converts unmethylated cytosine into uracil. This technique used to be fraught with difficulties, but that is no longer the case, says Plum. In fact, the technique is growing in popularity thanks to the number of effective kits available (see  'Tools of the trade' ). Bisulphite-treated DNA is analysed in different ways depending on the resolution and throughput required. Methylation-sensitive PCR uses primers that anneal only those sequences that contain 5-methylcytosines, which don't get converted by bisulphite. For higher-throughput screens, bisulphite-treated DNA is hybridized to microarrays containing two sets of oligonucleotides, one of which is complementary to the unaltered methylated sequence and the other to the converted unmethylated sequence. For a more detailed look at the cytosines that are methylated and their precise location, bisulphite-treated DNA can be sequenced directly. Earlier this year, this 'bisulphite sequencing' was used to decipher DNA methylation patterns in the  Arabidopsis  genome at nucleotide resolution 2 . Epigenomics combines these analytical techniques to identify biomarkers for cancer. The company has so far identified several genes that show changes in methylation in breast, colon, prostate and lung cancers. \u201cWe believe that there are some technical advantages to using epigenetic markers,\u201d says Plum. \u201cFor one thing, we are looking at a signal on the DNA. We can analyse the DNA in paraffin sections. We can look for fragments shed by tumours in the blood stream.\u201d Although methylation can change in different tissues or as a result of ageing and environmental factors, it is not as dynamic as gene expression. As a result, a single methylation marker can often be used to detect a disease, says Plum, whereas several markers would be needed for expression profiling.  Sequenom, based in San Diego, California, provides genomic services and has taken a different approach to analysing bisulphite-treated DNA. The company's EpiTYPER platform identifies and quantifies methylated sequences by gene-specific amplification of bisulphite-treated DNA followed by matrix-assisted laser desorption/ionization time-of-flight (MALDI-TOF) mass spectrometry. \u201cAlthough it does not provide genome-wide analysis, the value of our technique is fine mapping of sample cohorts,\u201d says Mathias Ehrich, Sequenom's group leader for epigenetics. The two main advantages of the technique are its quantitative capabilities \u2014 allowing a researcher to determine whether, for example, methylation at a specific gene increased from 20% to 40% in a tumour sample \u2014 and its long read lengths. \u201cIf you have a 2,000-base-pair CpG island, you want to look at the entire island for changes in methylation. If you only look at single CpG sites using PCR or a microarray you will not have enough information,\u201d says Ehrich. \n               Critical mass \n             When it comes to mapping the combinations of histone modifications, mass spectrometry offers a way to get at important details. \u201cIndividual modifications are like words in a sentence, but you have to know the context,\u201d says Joshua Coon, a biomolecular chemist at the University of Wisconsin, Madison. \u201cFor example, in human embryonic stem cells, methylation at arginine 3 of histone H4 is found only in the presence of dimethylation at lysine 20. That is the context that mass spectrometry can deliver. It is challenging to determine such context with antibodies alone.\u201d Unlike the use of antibodies, in which researchers select their antibodies based on the particular modification they are seeking, mass spectrometry is an unbiased approach. It also avoids the problem caused by one bound antibody inhibiting another from binding a nearby modification. To determine the modification patterns, intact histones \u2014 or at least entire N-termini peptides \u2014 have to be analysed. Such proteins are much larger than those usually examined in conventional 'bottom-up' mass spectrometry. It also requires highly sensitive and precise mass measurements: the difference in mass between a trimethylation and acetylation is only 36 millidaltons, for example. This issue is being successfully addressed by hybrid mass spectrometers, which bring two types of analyser into one instrument. For example, a combination of a very-high-resolution Fourier-transform ion cyclotron resonance (FT-ICR) mass spectrometer with a relatively low-resolution ion trap mass spectrometer has produced some spectacular results, says Coon. \u201cThe hybridization with ion traps resulted in significant gains in routine mass accuracy and resolution, because the ion trap regulates the ion population going to the FT-ICR,\u201d he explains. \u201cYou can get much more detailed information from hybrid mass specs,\u201d says Andreas H\u00fchmer, proteomics marketing director at Thermo Fisher Scientific in Waltham, Massachusetts. These instruments mean that it should be possible to characterize every protein modification in different types of cells, he adds. Another component to analysis involves breaking all the bonds between the amino-acid residues so that the mass spectrometer can read their sequence. This used to be done by colliding ions in a neutral gas, but recent developments have introduced more effective techniques. These include electron capture dissociation (ECD) and electron transfer dissociation (ETD), which induce fragmentation by transferring electrons to positively charged peptides. Coon's group has used an ETD-enabled linear ion trap mass spectrometer to map all the modifications that occur in the first 23 residues of the N-terminal tail of the histone H4 in differentiating human embryonic stem cells 3 . Similarly, Neil Kelleher, a chemist at the University of Illinois at Urbana-Champaign, has used ECD to identify combinations of modifications on the first 50 amino acids of the histone H3 protein, finding more than 150 forms of the protein 4 . The next step for mass spectrometry is the development of methods to determine levels of modifications and the ability to track patterns of change under different conditions. Waters, a liquid chromatography and mass spectrometry firm in Milford, Massachusetts, is working with researchers at the University of Southern Denmark in Odense to determine histone modifications in normal cells and cells undergoing senescence. \u201cWe have combined ion mobility and mass spectrometry,\u201d says James Langridge, director of the proteomics mass spectrometry business at Waters. \u201cWe are also pushing the boundaries to understand where the limits are and how to improve the technology.\u201d The next five years will see a boom in epigenomic research thanks to advances in mass spectrometers, array design and sequencing technologies. Several groups and consortia are taking advantage of these developments to characterize the entire epigenome in both healthy and disease states. The knowledge obtained will increase our understanding of gene regulation and should yield new biomarkers for disease. Reprints and Permissions"},
{"file_id": "455698a", "url": "https://www.nature.com/articles/455698a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "\u201cThere are a lot of small molecules that we do not even know about yet,\u201d says Arthur Castle, programme director for the Roadmap Metabolomics Technology development programme at the US National Institutes of Health in Bethesda, Maryland. Metabolomics has a good handle on analysing human primary metabolites, but when it comes to lipids, secondary metabolites, xenobiotics and the products of gut microflora, we are just scratching the surface, says Castle. The problem is part technological, part informatics. Steven Fischer at Agilent Technologies in Santa Clara, California, points out that some compounds are not stable and undergo chemical transformation during separation. Researchers will probably still see these transformed molecules by mass spectrometry (MS), but they may be misidentified, highlighting the need for follow-up experiments. And this is where metabolomics has an advantage over the other 'omics'. \u201cThere is so much knowledge of biochemistry that when we find a potential biomarker or a new drug mechanism we already know a lot about it,\u201d says Michael Milburn, chief scientific officer at Metabolon in Durham, North Carolina.  Trent Northen, now at the Lawrence Berkeley Laboratory in Berkeley, California, and Oscar Yanes, working in Gary Siuzdak's lab at the Scripps Research Institute in La Jolla, California, may have developed a new way to get at some of these 'unknown' molecules with an ionization technique called nanostructure-initiator mass spectrometry (NIMS) 4 . The idea is to transfer a biomolecule into the gas phase from a nanostructured surface simply by making that surface disappear. \u201cWe came up with the idea of putting a wax underneath, so when the nanostructured surface was irradiated it would melt and vaporize, allowing the molecule to go into the gas phase,\u201d says Northen. They finally came up with a perfluorinate surface for the trapped initiator phase. Siuzdak's group found that NIMS worked not only for proteins, but also for small molecules such as metabolites. \u201cPerfluorinates do not ionize well, so it allows us to see things in the lower mass region where metabolites like to hang out,\u201d says Siuzdak. Using perfluorinated materials may have another advantage for metabolomics. \u201cAs these are highly hydrophobic surfaces, we can apply very dirty complex samples, such as blood, for direct analysis,\u201d says Yanes. If you put a drop of urine or blood on the NIMS chip the metabolites will attach, but all the salts and other chemicals that normally interfere with MS stay in solution. Yanes is now exploiting this property to follow drug metabolism by looking at uptake in blood, clearance in urine and tissue localization. For Siuzdak, exploring the 'unknown' metabolite world is an exciting prospect. \u201cWe are getting involved in an area where we don't know what the molecules' structures are or what they do, so it is really just a fantastic area for discovery.\u201d \n               N.B. \n             Reprints and Permissions"},
{"file_id": "455701a", "url": "https://www.nature.com/articles/455701a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "455697a", "url": "https://www.nature.com/articles/455697a", "year": 2008, "authors": [{"name": "Nathan Blow"}], "parsed_as_year": "2006_or_before", "body": "Until now, metabolomics researchers have had to adapt technology developed mainly for proteomics. But there are now solutions designed with them in mind. Nathan Blow reports. Metabolomics \u2014 the comprehensive study of metabolic reactions \u2014 is gaining ground alongside its older siblings genomics and proteomics. \u201cUnlike some of the other 'omics' that we have seen, metabolomics is going to produce a lot of useful information right from the start,\u201d says Gary Siuzdak, professor of molecular biology at the Scripps Research Institute in La Jolla, California. He is one of a growing number of biologists using advanced technology to explore biochemical questions on a scale that would have seemed impossible a decade ago. \u201cThe metabolome is the best indicator of an organism's phenotype,\u201d says David Wishart at the University of Alberta in Edmonton, Canada. Wishart was one of the instigators of the Human Metabolome Project, a US$7.5-million effort funded by Genome Canada to systematically characterize the metabolites of the human body. He gives the example of a person holding their breath for five minutes. Although genomic or proteomic analysis would not provide any evidence of stress during this short period \u2014 even as the person turns blue \u2014 metabolite profiles would show dramatic changes within the body.  Unlike a genome or even a proteome, however, a metabolome is tricky to pin down. Wishart notes that although researchers know there are 3 billion base pairs in the human genome, if you ask biochemists how many small-molecule metabolites there are in the human body, they come back with numbers ranging from 3,000 to 100,000. And this poses a real challenge for metabolomics research, as both ends of the scale could be correct. The Human Metabolome Project has pegged the number of endogenous metabolites in the human body at around 3,000 \u2014 which most researchers agree on. But humans also take in small molecules from the environment \u2014 preservatives in food, chemicals in the air, metabolites produced through the breakdown of drugs and toxins \u2014 making an exact figure hard to determine. \n               Separation anxiety \n             With metabolites in such a state of flux, researchers do not have an easy task. Nevertheless, advances in chromatography, mass spectrometry (MS) and nuclear magnetic resonance spectroscopy (NMR) are allowing them to make headway in defining different metabolomes and understanding how changes in the concentrations of metabolites relate to human health and disease.  One of the problems is that metabolites come in a variety of chemical forms. \u201cI would say one of the real challenges of metabolomics is that each metabolite is its own unique puzzle,\u201d says Trent Northen, a scientist at Lawrence Berkeley National Laboratory in California. And in most cases the first step in solving the puzzle is isolating the metabolite for analysis. No one separation method works for all metabolites, so researchers rely on combinations of gas chromatography (GC), liquid chromatography (LC) and emerging capillary electrophoresis (CE). Historically, GC separation has had the edge. \u201cGC\u2013MS technology may not be sexy, but huge databases are available,\u201d says Northen. These GC\u2013MS databases, compiled over more than four decades, enable researchers to compare a wide range of spectra to arrive at a chemical identification. Multidimensional GC, often called 'GC\u00d7GC' or two-dimensional GC, offers even better separation. \u201cWhen people are doing GC\u00d7GC, they are trying to get more separation chromatographically of very complex samples,\u201d says Steven Fischer, a senior applications chemist at Agilent Technologies in Santa Clara, California. To achieve this, GC\u00d7GC uses two separation phases, such as a non-polar and a polar phase, in two capillary columns in series in the instrument. Agilent recently introduced the 7890 GC system, which can perform multidimensional GC, and Thermo Fisher Scientific in Waltham Massachusetts, has developed the Trace GC\u00d7GC system. The Pegasus 4D GC\u00d7GC MS time-of-flight (TOF) system from LECO, based in St Joseph, Michigan, uses a thermal modulator placed between the two GC columns to collect effluent from the first column before going into the second phase of separation. The power of the multidimensional approach is starting to be reported. In May this year a group reported the use of GC\u00d7GC with LC\u2013MS to generate a draft metabolic network for the single-celled alga  Chlamydomonas reinhardtii 1 . \n               Class action \n             GC is particularly useful for mixtures of volatiles, such as steroids, saccharides and sugar alcohols, which can be sent directly into the gas phase for separation. Metabolites in human biofluids and tissues therefore present a technical challenge for GC, as most are not volatile. Non-volatile metabolites either need complicated chemical transformation before GC or separation by other types of chromatography. One of these is high-performance liquid chromatography (HPLC), a well-established lab workhorse. It uses a combination of solvents, pressure and matrix particle sizes to separate molecules on the basis of their retention times in a column packed with matrix. HPLC can separate a broad range of metabolites, including non-volatiles, and remains a favourite among metabolomics researchers. Most advances in HPLC involve increases in the pressure applied and changes in matrix particle size. Ultra-performance liquid chromatography (UPLC), commercialized by Waters Corporation in Milford, Massachusetts, is becoming more widely used in the metabolomics community. It takes advantage of higher pressure (83 megapascals compared with 21 megapascals for HPLC) and smaller particles (less than 2 micrometres diameter compared with 3 micrometres for HPLC) to obtain faster separation times. But like GC, HPLC has technical stumbling blocks. Reversed-phase HPLC (in which the stationary phase is non-polar) is often used for metabolomics analysis, but reversed-phase separation often fails with hydrophilic metabolites. These tend to be so water soluble that they interact poorly with the non-polar bonding phase and are rapidly eluted, according to Phil Koerner, a senior technical manager from chromatography specialists Phenomenex in Torrance, California.  So in 2007, Phenomenex introduced the Luna HILIC column. \u201cI like to refer to it as reverse reverse-phase chromatography,\u201d says Koerner. In the HILIC approach, the weak solvent, which is applied first, is a polar organic solvent (not water as in reversed-phase HPLC), and the strong solvent, applied second, is water. This causes the order of elution to be completely reversed, with the most hydrophilic compounds being eluted last. Although Koerner acknowledges that the HILIC approach is not new, it was the need to separate hydrophilic metabolites on the large scale required by metabolomics that led Phenomenex and other companies, such as Waters and Tosoh Bioscience of Stuttgart, Germany, to start supplying a greater number and range of HILIC columns. Capillary electrophoresis followed by MS (CE-MS) is not yet so popular with the metabolomics community as either GC or HPLC, but several developers are hoping to change this. \u201cIt can be very difficult to use this approach,\u201d acknowledges Ryuji Kanno, president of Human Metabolome Technologies based in Tokyo, Japan. This approach uses electrophoretic mobility to separate low-molecular-weight ionic compounds that are difficult to separate by GC or HPLC. The company has been working closely with Agilent to develop optimized reagents and capillary columns, and is providing training with Agilent's CE-qTOF MS system to make the CE approach more accessible to metabolomics researchers, says Kanno.  Mass spectrometry is not the only method that can be used to detect metabolites once separated. Wishart and his colleagues recently compared MS and NMR to look at metabolites in cerebral spinal fluid 2 . They found little overlap in the metabolites detected by the two methods, and the conclusion was clear: \u201cWe do not have a single perfect metabolite detector,\u201d says Wishart. MS and NMR each have their supporters. \u201cOne of the main strengths of NMR is that it is an unbiased, universal detector,\u201d says Jack Newton, a product manager at Chenomx in Edmonton, Canada, which was co-founded by Wishart in 2000. This attribute, along with NMR's ability to determine structure and perform quantitative analysis is particularly attractive to metabolomics researchers who need a way to compare and exchange results between labs. \u201cThe move is afoot \u2014 people want to get to that common language of compound names and concentrations,\u201d says Wishart, as this will make integrating data sets and obtaining systems-level views of cell physiology possible. The challenge with NMR is instrument sensitivity \u2014 NMR is less sensitive than MS, often identifying far fewer metabolites in the same sample. \u201cFor us, the relevant question is how sensitive do you need to be,\u201d says Newton. He says researchers at Chenomx have performed many studies in which biologically meaningful differences between samples were easily captured with NMR, even though some compounds in the samples probably fell below the sensitivity limits of the instrument (see ' Dark matter '). MS, on the other hand, is a very sensitive method for metabolite identification and, unlike NMR, is easily coupled to upstream separation techniques. Siuzdak says his group can see thousands of molecules in an MS analysis \u2014 and that number can be doubled by changing from positive- to negative-ion mode. And by using both reversed-phase chromatography and HILIC columns, they are seeing more hydrophilic compounds in their analyses than before. \u201cI would venture that we are now seeing over an order of magnitude more than what you would see with NMR,\u201d he says. \n               Detector development \n             As researchers in the MS camp turn towards TOF and ion-trap MS instruments for metabolite analysis, developers are responding to their complex needs. Bruker Daltonics in Billerica, Massachusetts, has introduced the maXis ultra-high resolution (UHR)-TOF MS system, which can accommodate both UPLC and CE separation. Applied Biosystems in Foster City, California, in collaboration with MDS SCIEX in Toronto, Ontario, have the ion-trap system 4000 QTrap LC/MS/MS that can interface with Applied Biosystem's LightSight software for small-molecule analysis and identification. Both Agilent and Thermo Fisher Scientific also offer MS systems and software packages designed for metabolite analysis.  Some researchers and developers are designing platforms to bring the two camps closer together \u2014 incorporating NMR and MS instruments in a single system. Bruker BioSpin in Billerica, Massachusetts, has developed the Metabolic Profiler, a system that combines a liquid handler, the Avance III NMR spectrometer and an LC-electrospray ionization (ESI)-microTOF MS, all under the control of a single data-management and analysis system. But what researchers dream of is a single detection 'chip' for all metabolites. \u201cIn my lab we have four platforms, and each platform looks at a certain part of the metabolome,\u201d says Oliver Fiehn, a metabolomics researcher at the University of California, Davis. But he doubts that a single chip could ever become reality. \u201cThe lack of such a technology is the Achilles heel of metabolomics,\u201d says Wishart, noting that the most that researchers can analyse at any one time with current technologies is 10\u201315% of the entire metabolome \u2014 and even that's stretching it. \u201cThe big bottleneck is really compound identification,\u201d says Fiehn. Unblocking it will need the addition of many more well-annotated reference spectra in the databases.  And that will take time. Chenomx was founded with the aim of developing a database for NMR analysis, and that has taken several years of intensive effort, says Newton. Different chemical environments can influence a compound's NMR spectra, so researchers at Chenomx had to acquire spectra at ten pHs, ranging from 4 to 9, for each of the more than 300 reference compounds now in their proprietary database. Metabolite databases for MS have also been springing up as more researchers move into the field. One of the first was METLIN ( http://metlin.scripps.edu ), a publicly accessible database that was started in Siuzdak's lab. \u201cWe currently have 23,000 metabolites in there,\u201d says Siuzdak, of which around 2,500 are identified endogenous metabolites. METLIN also contains a set of about 8,000 theoretical di- and tripeptides along with theoretical lipids, drugs and metabolites. To expand the scope of METLIN, Agilent has collaborated with the Scripps Center for Mass Spectrometry to analyse chromatographic standards and add information about mass and retention time, with the intent of using these properties in addition to isotope pattern matching for identification. \u201cOur goal is to get to the point where the most common metabolites encountered by researchers are easily identifiable,\u201d says Agilent's Fischer. Gregory Stephanopoulos, a chemical engineer at the Massachusetts Institute of Technology in Cambridge, is taking a different approach to metabolite identification. Several years ago a student approached him with an interesting metabolomics project, but the catch was that the lab would first have to increase the number of reference spectra in its library to enable metabolite identification. Although Stephanopoulos liked the project, he did not like the idea of simply collecting spectra to fill a database. \u201cI thought that there had to be a better way to deal with the issue,\u201d he recalls. The result is a web-based program called SpectConnect, which was launched in 2007 to help researchers identify important metabolites that might serve as biomarkers 3 . The SpectConnect algorithm tracks and catalogues GC\u2013MS spectra that are conserved in multiple samples \u2014 an indication that these represent real compounds instead of noise in the sample, helping to guide researchers with their follow-up efforts at full metabolite identification. \n               Numbers game \n              The good news for metabolomics researchers is that NMR and MS metabolite databases are increasing in both number and size as new metabolomes are analysed. \u201cOne of the things that changed for us over the past 18 months is the places we are applying the technology,\u201d says Michael Milburn, chief scientific officer at Metabolon in Durham, North Carolina. Metabolomic approaches are now addressing biological questions in areas ranging from drug discovery and cosmetics development to plant science and winemaking (see ' Wine-omics '). Publicly accessible databases include MassBank for high-resolution ESI mass spectra of metabolites ( http://www.massbank.jp ), BinBase for processing and analysing of dissimilar MS spectra ( http://sourceforge.net/projects/binbase ), and MetWare ( http://msbi.ipb-halle.de/msbi/metware ) for the storage and analysis of metabolomic experiments. Commercial databases include Metabolon's, containing spectra of more than 6,000 reference metabolites, and Bio-Rad's KnowITAll spectral database of more than 1.3 million entries, including MS and NMR references. But there's still a way to go before metabolite identification is as simple as 'query and get a chemical name'. \u201cThe database changes have been encouraging,\u201d say Stephanopoulos. But not enough to change his mind about the need for tools such as SpectConnect. Arthur Castle, programme director for the Roadmap Metabolomics Technology development programme at the US National Institutes of Health, has seen the pieces falling into place over the past couple of years. \u201cThe technology is very close to being there \u2014 it is just a question of putting it all together now,\u201d he says. Reprints and Permissions"},
{"file_id": "454796a", "url": "https://www.nature.com/articles/454796a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "For researchers tackling the epigenome, there are two key tools: bisulphite conversion kits and ChIP-grade antibodies. Getting complete conversion of unmethylated cytosine to uracil has been one of the main challenges for the bisulphite treatment of DNA, not least because the reaction conditions can cause the DNA to degrade. A number of companies have tackled the problem. \u201cThere are incredibly good kits for bisulphite conversion,\u201d says Achim Plum, senior vice-president for corporate development at Epigenomics in Berlin. \u201cIf you use home brews you can make many mistakes, but with kits this is no longer an issue.\u201d QIAGEN of Venlo, the Netherlands, sells EpiTect, a kit based on Epigenomics' technology that Plum says offers nearly 100% conversion without DNA degradation. In June, QIAGEN expanded the EpiTect product line to include a standardized workflow for methylation analysis from sample collection, stabilization and purification to bisulphite conversion, and real-time endpoint PCR methylation analysis or sequencing.  Epigentek in Brooklyn, New York, also sells epigenetic products and kits for DNA methylation and histone modification. Its Methylamp DNA modification kit takes two hours to convert cytosine to uracil with more than 99% accuracy and requires only 50 picograms of starting DNA, according to Adam Li, the company's chief scientific officer. The sample can then be applied for methylation-specific PCR or a methylation array. In June last year, Sigma-Aldrich of St Louis, Missouri, launched a number of products for epigenetic research based on Epigentek's technologies. Other providers of bisulphite conversion kits and related epigenetics products include Human Genetic Signatures in Sydney, Australia; Invitrogen of Carlsbad, California; Promega of Madison, Wisconsin; and Zymo Research of Orange, California. When it comes to antibodies that can be used in ChIP experiments, a fresh set of problems presents itself. \u201cFor most transcription factors there are not that many antibodies,\u201d says Kevin Struhl, a ChIP pioneer at Harvard Medical School in Boston, Massachusetts. \u201cAnd a lot of antibodies don't work well on ChIP.\u201d ChIP-on-chip requires highly specific antibodies that recognize a protein's epitope in free solution and under fixed conditions. \u201cBecause an antibody that works well in ChIP must recognize the target while it is bound to DNA, the epitope that the antibody recognizes must be available in a spatial conformation that is not obscured by the shape the protein takes while bound to DNA,\u201d explains Dana Meents, a product manager at Active Motif in Carlsbad, California. Active Motif is one of several companies now selling products that have been ChIP validated. Abcam in Cambridge, UK, sells just over 200 ChIP-grade antibodies, many of them specific to histone post-translational modifications. As an alternative to antibodies, the HaloCHIP system developed by Promega allows researchers to perform ChIP-on-chip experiments by cross-linking DNA to a protein of interest fused to a modified haloalkane dehalogenase tag (HaloTag). The protein\u2013DNA complex can then be captured on a resin that recognizes the HaloTag for further microarray analysis. \n               L.B. \n             Reprints and Permissions"},
{"file_id": "455699a", "url": "https://www.nature.com/articles/455699a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "For Kirsten Skogerson at the University of California, Davis, wondering about how chemical composition affects the flavour and body of a wine took her from a degree in viticulture and enology into metabolomics research. When Skogerson arrived in Oliver Fiehn's lab as a postgrad she looked for a project that would marry Fiehn's expertise in metabolomics and her interest in wine. \u201cThere are so many questions in wine science that you could start to answer by doing a global analysis,\u201d she says. A deeper understanding of the biochemistry of grape-juice fermentation could help the winemaking industry by complementing the arts of the traditional wine taster. So Skogerson and Fiehn set out to survey wine 'metabolomes', in search of key chemical components contributing to body.  Using proton nuclear magnetic resonance (NMR) and gas chromatography\u2013mass spectrometry (GC\u2013MS), they looked at 17 different white wines with a wide range of body. For GC\u2013MS analysis, they first removed the alcohol under reduced pressure and then ran samples on a LECO Pegasus IV GC TOF MS system and analysed the spectra using the BinBase program developed in Fiehn's lab. Each wine was also directly analysed on a Bruker Daltonics 600 mHZ NMR instrument with the resulting peaks being compared to the commercially available Chenomx NMR database for metabolite identification. \u201cWhen you think about it, you have the grape metabolome being acted on by the yeast, plus the added complexity from the yeast metabolome, so the metabolite profile of a wine is very complex,\u201d says Skogerson. They found a total of 413 metabolites among the wines \u2014 probably only a small fraction of the wine metabolome \u2014 of which 108 could be positively identified. And in both data sets, the amino acid proline showed a positive correlation with body as assessed by trained wine tasters. How proline relates to body is not yet clear, however. \u201cThat is the hard part of being in metabolomics \u2014 you get clues, but the follow-up is the real challenge,\u201d says Skogerson. Still, she thinks proline could be used as marker for a wine's viscosity. Red-wine drinkers have not been forgotten. Bruker Daltonics in Billerica, Maryland, has profiled red wines for important polyphenolic secondary metabolites such as tannins, flavonoids and anthocyanins. This demonstration used the Acquity ultra-performance liquid chromatography system from Waters to separate red wine metabolites for analysis by Bruker's LC-ESI QTOF MS instrument as well as analysis by NMR coupled with Bruker's BioSpin Spectral Base analysis package. Does knowing the chemistry behind that wonderful bottle of wine take away from the pleasure? Not according to Skogerson. \u201cScience has the potential to bring the art of winemaking to a higher level.\u201d \n               N.B. \n             Reprints and Permissions"},
{"file_id": "454799a", "url": "https://www.nature.com/articles/454799a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "454795b", "url": "https://www.nature.com/articles/454795b", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "So far, the approach to the epigenome has been somewhat piecemeal, with disparate groups cataloguing just the modifications in their cells or organisms of interest. Technological advances that allow faster and cheaper mapping of epigenetic modifications are now helping to unite these efforts. The Human Epigenome Project, for example, was established in 1999, when researchers in Europe teamed up to identify, catalogue and interpret genome-wide DNA methylation patterns in human genes. The project is a collaboration between the Wellcome Trust Sanger Institute in Hinxton, UK, the National Centre for Genotyping near Paris, France, and the German company Epigenomics in Berlin. The consortium completed a pilot study of methylation patterns within the major histocompatibility complex followed by a more thorough analysis of the methylation patterns on three chromosomes. After showing the feasibility of such an approach \u201cthe consortium has decided to leave it to others to continue the endeavour\u201d, says Achim Plum, senior vice-president for corporate development at Epigenomics. ENCODE, or the Encyclopedia of DNA Elements, was launched in September 2003 by the US National Human Genome Research Institute (NHGRI) in Bethesda, Maryland. The project aims to identify all the functional elements in the human genome sequence. Following initial success, the NHGRI last October pledged $80 million over four years to scale up the project. Many of the functional elements so far identified by ENCODE are being studied for changes in methylation under a range of conditions. The Epigenome Network of Excellence, headed by Thomas Jenuwein of the Research Institute of Molecular Pathology in Vienna, Austria, brings together 81 laboratories from 10 European countries under a \u20ac12.5-million (US$19.7-million) grant from the European Union to advance epigenetic research over the next five years. The network, which officially began in June 2004, supports members and non-members so that they can attend conferences, workshops and training visits, as well as offering them shared resources. The US American Association for Cancer Research is championing the formation of the Alliance for the Human Epigenome and Disease (AHEAD) project. Under the leadership of Peter Jones, director of the University of Southern California's Norris Comprehensive Cancer Center in Los Angeles, AHEAD would coordinate an international interdisciplinary project to map a defined subset of epigenetic markers in a limited number of human tissues at different stages of development (see  page 711 ). This would provide a reference epigenome to which samples from various diseases could be compared. The alliance would also try to develop a bioinformatics infrastructure to support the collection of epigenomic data. AHEAD's planned efforts will be helped by January's announcement from the National Institutes of Health that it will invest more than $190 million over the next five years to accelerate epigenomics research. Grant applications are now being accepted for epigenome mapping centres, epigenomics data analysis and coordination, and technology development. \n               L.B. \n             Reprints and Permissions"},
{"file_id": "456825a", "url": "https://www.nature.com/articles/456825a", "year": 2008, "authors": [{"name": "Nathan Blow"}], "parsed_as_year": "2006_or_before", "body": "Advances in imaging are allowing researchers to gain better insights into the function of tissues, cells and even individual molecules. Nathan Blow examines the latest technologies lighting the way. Competition is often a stimulus for change. But a competition can also be a mirror of a technology's evolution. Take the annual Nikon International Small World Competition and the Olympus BioScapes International Digital Imaging Competition, for example: these events give scientists from around the world the opportunity to submit their best photographed microscopy images for judging by panels of experts. This year, the winner of the Small World contest, announced in mid-October, was Michael Stringer from Westcliff-on-Sea, UK, for a multicolour, dark-field image of marine diatoms. The winner of the 2008 BioScapes contest was Spike Walker of Penkridge, UK, using Rheinberg illumination of a fairy fly wasp. Entrants in these competitions are using everything from conventional stereomicroscopes to the latest in laser-scanning confocal instrumentation, in some cases with different coloured fluorophores and the newest three-dimensional rendering software, to obtain images ranging from Stringer's diatoms, to a growing bundle of carbon nanotubes, to colourful pictures of individually labelled neurons in a mouse brain. According to Eric Flem, director of the Nikon competition \u2014 now in its 34th year \u2014 the size and scope of the contest has grown exponentially in recent years, something he attributes to advances in the underlying imaging technology. \u201cImaging has become so much easier to do and so much more powerful \u2014 it is the norm rather than the exception in most scientific fields these days,\u201d he says. \n               Probing questions \n             Researchers often use a little beacon of light, a fluorophore, to see the inside world of the cell. Although a great number and range of fluorescent proteins are currently available for this purpose, improving these to meet the expanding needs of cell biologists can be quite a challenge for developers. \u201cOften improvements in one characteristic of a fluorescent protein can be reached,\u201d says Ilya Kelmanson, head of the product department at fluorescent-protein developer Evrogen in Moscow, \u201cbut usually at the expense of another important parameter.\u201d An example of this is monomerization of fluorescent proteins, which helps in tagging proteins within the cell, but will generally result in a decrease in the fluorescence intensity. To deal with this, Evrogen often maintains different lines of fluorescent proteins: bright dimeric probes (made of two monomers) used for cell-labelling applications, and less-bright monomeric versions for tagging purposes. Kelmanson says that Evrogen is developing a third line of fluorescent proteins that will consist of tandem versions of dimeric fluorescent proteins that have stronger fluorescence than their monomeric counterparts, and good performance and brightness as fusion partners with other proteins. There are many types of fluorescent proteins that emit in the middle range of the light spectrum \u2014 green to yellow. But it is a different story at the far ends of the spectrum. \u201cWe still do not have good fluorescent proteins emitting in the near-infrared range,\u201d says Kelmanson. Probes in this range could be very useful for whole-body imaging applications and multicolour labelling experiments, he says. \u201cPeople are moving away from the blue and the green excitation wavelengths and into the red and near-infrared,\u201d says Martin Hoppe, a market manager at microscope developer Leica Microsystems in Wetzlar, Germany. He says that using fluorescent proteins with longer excitation wavelengths helps researchers when it comes to live-cell imaging applications \u2014 an area of keen interest for most cell biologists (see  'Light activated' ) \u2014 because these wavelengths tend to be less damaging to cells. Newer dyes have been optimized for near-infrared excitation. One of these, mCherry, one of the 'fruit fluorescent proteins' developed in the lab of Roger Tsien at the University of California, San Diego \u2014 who shared the 2008 Nobel Prize in Chemistry for the discovery and development of green fluorescent protein \u2014 is now offered by Clontech in Mountain View, California, along with several fluorescent proteins derived from reef corals. Developing fluorescent proteins at longer wavelengths, particularly for fluorescence resonance energy transfer (FRET) applications, which are used to measure interactions between two proteins  in vivo , \u201chas been a challenge\u201d, says George Hanson, a principal scientist working on cell signalling at Life Technologies (formerly Invitrogen) in Eugene, Oregon. Although a challenge, there are potential benefits \u2014 FRET efficiency grows and background fluorescence decreases when using longer wavelength fluorophores, according to Kelmanson. Still, the development of fluorescent proteins that excite at longer wavelengths is only part of the issue for those interested in applying these fluorophores to their FRET studies. Although there are many cyanine to yellow FRET pairs available along with filters and instrumentation preset for reading their interactions, the use of more red-shifted fluorescent protein pairs might take time as optimal parameters and appropriate microscopy hardware and software will need to be developed. At Life Technologies, Hanson has been working to advance the use of FRET in high-throughput screening applications. Here, the challenge of obtaining a strong FRET signal is crucial to overcoming any background fluorescence. Because the simple approach of making a brighter fluorescent protein can be time-consuming, another approach, being used by a number of developers, is to take advantage of the 'states' \u2014 the differences between the excitation and emission \u2014 of a fluorescent protein. Time resolved FRET (TR-FRET) uses long-lived fluorophores that exhibit a time delay between their excitation and emission, minimizing potential interference from background fluorescence. Several companies now offer TR-FRET platforms for cellular interaction assays, including Life Technologies, Cisbio in Bedford, Massachusetts, Covalys of Witterswill, Switzerland, and PerkinElmer located in Waltham, Massachusetts. \n               Quantum of happiness \n             Another route to fluorescence is provided by quantum dots, which \u201cpresent better photostability than organic dyes and allow for increased multiplexing capability\u201d, says Stephen Chamberlain, manager of business development at Life Technologies. Quantum dots are composed of a semiconductor core shell surrounded by an exterior coating. What makes them unique as fluorophores is their ability to be excited by a single wavelength of light \u2014 the emission from a quantum dot is dictated by the size of its core shell. As the size of the core shell can be varied to obtain different light-emission spectra, quantum dots present a particularly interesting solution for researchers interested in experiments using multiple coloured fluorophores. But unlike fluorescent proteins, quantum dots are not genetically encodable, which has been a stumbling block to more widespread usage. \u201cIt is hard to get them into the cell,\u201d says Alice Ting, a chemist at the Massachusetts Institute of Technology in Cambridge, who has been working with quantum dots for several years. \u201cAnd then once inside, how do you target them to a specific protein?\u201d Most researchers and developers are looking towards the outer coating for targeting. At Life Technologies, says Chamberlain, they place a polymer coating around the core shell of their Qdot Nanocrystals along with an additional layer of polyethylene glycol on top of the polymer coating to reduce the sticking together of the Qdots, and to provide a surface for the attachment of antibodies and other molecules for targeting. Despite the challenges of targeting, quantum dots are now being tried in a number of imaging applications. Chamberlain says that like their organic-dye counterparts 20 years ago, researchers are still in the process of developing robust application protocols for quantum dots. Although quantum dots can be used for FRET applications, much like fluorescent proteins, they tend to make better donors in a FRET interaction because they can be excited by light that is blue-shifted relative to their emission. They are being used for  in vivo  imaging applications, such as looking at blood vasculature, because of their high residence times and ability to excite in the near-infrared region of the spectrum. Ting says using quantum dots for single-molecule imaging is a much more straightforward application at the moment, although the issues of delivery and targeting still exist, and several groups are now taking advantage of the greater brightness of quantum dots to track the movement of single molecules within cells. \n               Microscopes respond \n             As the types and range of applications of fluorescent probes and quantum dots continue to grow, developers of confocal microscopes have been working hard to keep instrumentation up to the challenge. \u201cIn general, researchers are still requesting the same three things: higher speed, better signal-to-noise and lower toxicity,\u201d says Michael Davis, a confocal product manager from Nikon Instruments in Melville, New York. But he is quick to add that there are several emerging fluorescent-protein techniques that have required more sophisticated confocal technology. Davis says more researchers are now using photoactivatable and photoconvertible proteins in their research. The idea here is to use a specific wavelength of light to excite the fluorescent protein, which can then be turned off, in the case of photoactivatable proteins, or even converted to another colour, in the case of photoconvertible proteins, with a different wavelength of light. Evrogen now offers both kindling red fluorescent protein (KFP-Red), which can be either reversibly or irreversibly activated for either short or long-term protein tracking in a cell, and PS-CFP2, which is a photoconvertible protein that will switch from cyan to green in response to light activation. And MBL International in Woburn, Massachusetts, offers the photoconvertible proteins CoralHue Kaede and CoralHue Kikume, which convert from green to red in response to light activation. The development of these fluorescent proteins has led to dual scanning systems for confocal microscopes, allowing for the simultaneous imaging and activation of photoactivatable and photoconvertible proteins, along with new spectral imaging technologies that can accurately assay a broad range of possible spectral emissions. Another trend in the development of confocal instrumentation is the emergence of resonance scanners. \u201cPeople would like to see the resolution they get out of a point scanner, but with the speed of a field scanner,\u201d says Davis. Several developers hope that resonance scanning will bridge this divide. With a resonance-scanning confocal system, the scanner mirrors are moved at extremely high frequencies, permitting much higher frame rates and minimizing the time the scanner dwells at any particular point in the sample, allowing researchers to study more dynamic events in a cell while reducing the effects of phototoxicity on it. Both Nikon and Leica Microsystems have introduced resonance-scanning confocal systems in recent years. \n               Small switches \n             Although dual imaging and faster scanning are enhancing fluorescence imaging, the next step in improving the optical resolution of microscopes might require all developers to look more closely at the properties of those fluorescent proteins. \u201cOptics alone can only provide resolution to a certain limit,\u201d says Davis. The ability of a fluorescent protein or a dye to switch states is also at the heart of an emerging field in microscopy. \u201cFluorescence switching is the common basis to all super-resolution methods that have been devised so far,\u201d says Stefan Hell, director of the Max Planck Institute for Biophysical Chemistry in Gottingen, Germany, who created a super-resolution imaging method known as stimulated emission depletion (STED) 'nanoscopy'. Before super-resolution imaging, the resolution limit of optical microscopy was thought to be determined by the diffraction barrier of light. For example, in a high-resolution confocal microscope, a beam of light is focused down to a spot on the focal plane of about 200 nanometres in size. Because all molecules within that spot are excited in parallel, the spatial resolution is limited to about this value. Hell's approach with STED was to use the transition states of fluorophores, or their photoswitching capabilities, to effectively reduce the size of that spot \u2014 potentially all the way down to a single molecule. In a STED microscope, the focal spot is overlapped with a ring of light that switches off fluorescence everywhere in the spot except at the centre of the ring. The outcome is a much smaller fluorescence spot that, when scanned across the specimen, gives super-resolution images automatically. \u201cSTED was the first concept that really showed you could go beyond the barrier of half the wavelength of light in the focal plane,\u201d says Hell. More and more super-resolution approaches are starting to be described in scientific journals as researchers see the potential of the method. For detecting single molecules in cells, William Moerner, a chemist at Stanford University, says photoswitching of single molecules has created a real sense of excitement in the community. By exploiting the ability of fluorophores to switch states, researchers can work with high concentrations of labelling products \u2014 crucial for imaging of structures. And, by having most of the labelled molecules 'off' and only a few 'on' at a given moment, they can improve the resolution down to that single-molecule level of detection. This premise is at the heart of several newer super-resolution approaches. These include photoactivated localization microscopy 1  (PALM) developed by Eric Betzig at the Howard Hughes Medical Institute's Janelia Farm Research Campus in Ashburn, Virginia, and his colleagues; fluorescence-PALM, developed by Samuel Hess and his colleagues at the University of Maine in Orono 2 ; and stochastic optical reconstruction microscopy 3  (STORM), developed by Xiaowei Zhuang and her colleagues at Harvard University. These methods can now image structures in cells at a resolution far below 200 nanometres. Even though fluorophores are used in all these methods, both new and old photoswitchable molecules are proving useful for super-resolution imaging. Although Moerner and his colleagues recently used the readily available enhanced yellow fluorescent protein (EYFP) to image proteins at a resolution of less than 40 nanometres in  Caulobacter crescentus 4 , he is also involved with the National Institutes of Health Molecular Imaging Exploratory Center programme in which his group, along with others at Stanford University and Kent State University in Kent, Ohio, are developing new dyes and fluorophores specifically for single-molecule imaging. The centre's efforts are starting to pay off. Researchers there recently invented a new class of photoswitchable molecules based on the switching of an azide functional group to an amine when exposed to activating light beams 5 , as well as covalently linking cyanine dyes to a molecule to improve switching performance in STORM experiments. \u201cAlthough I know that many groups are working to optimize fluorescent proteins for switching, now the community can work on optimizing small molecules for switching as well,\u201d says Moerner. \n               Higher resolution for all \n             For those researchers interested in breaking the diffraction barrier in their own research, the road to super resolution will become easier in the coming years. \u201cInitially, I think there was quite a reluctance to believing that you would get super resolution,\u201d says Hell. When he started developing STED microscopy, the initial system relied on a series of flexible lasers, creating a complex system that many outside researchers initially thought would be required for any STED imaging application. But today, Hell says the development of laser systems that are easier to use and have the right light structure to perform STED, alongside ever-improving confocal systems with their spectral-imaging capabilities for those interested in the PALM and STORM approaches, are encouraging researchers to try super-resolution imaging. Leica Microsystems is the only company to offer STED capability for super-resolution imaging. The Leica TCS STED Superresolution Microscope is a broad-band confocal platform with multi-photon capability that integrates the STED concept, allowing users to obtain resolutions of better than 100 nanometre. Hoppe says that it took four years for the company to develop a STED system that was both stable and user-friendly. \u201cFor STED to work, the two beams \u2014 for excitation and depletion \u2014 need to be perfectly aligned in space and time,\u201d says Hoppe. This is crucial to super resolution with STED \u2014 if the lasers are out of alignment, researchers cannot obtain super-resolution imaging. To deal with this, engineers at Leica developed fully automated beam-correction functionality on the TCS STED system. If an environmental factor \u2014 for example, the temperature of the room \u2014 changes, researchers can simply push a button and the system realigns the two lasers automatically. Advances in fluorescent probes and microscope instrumentation are pushing resolution barriers and multicolour imaging possibilities. So this year, as microscopists around the globe submit the amazing pictures they capture of the cellular world to those annual imaging contests \u2014 to the delight of scientists and the public alike \u2014 many of those same entries could produce fundamental insights into the inner workings of the cell at a level of resolution that previously had not been possible. The future is bright. Reprints and Permissions"},
{"file_id": "456826a", "url": "https://www.nature.com/articles/456826a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "When scientists first discovered light-absorbing molecules \u2014 in microorganisms \u2014 that could control the flow of ions into a cell, many researchers became interested in comparing how these proteins differed from their visual pigment counterparts. But Karl Deisseroth at Stanford University in California saw something else in these light-activated channels \u2014 a potential tool to dissect the function of the brain. Deisseroth \u2014 along with Edward Boyden, his postdoctoral fellow at the time, now at the Massachusetts Institute of Technology in Cambridge, and his graduate student Feng Zhang \u2014 collaborated with microbiologists to hunt for these light-activated channels from various species. \u201cI come from a synaptic physiology background, so we just sequentially tried these and other proteins to find the ones that worked the best,\u201d says Deisseroth. It was a collaboration with Georg Nagel and Ernst Bamberg at the Max Planck Institute of Biophysics in Frankfurt, Germany, who supplied a clone of a gene called channelrhodopsin-2 from  Chlamydomonas reinhardtii  to Deisseroth that opened the floodgates. Channelrhodopsin-2 is a gated light-sensitive cation channel that uses a molecule of all- trans  retinal to absorb photons. When Deisseroth, Boyden and Zhang expressed channelrhodopsin-2 in hippocampal neurons in the mouse brain then shone blue light on the region, they found the cells with channelrhodopsin-2 responded to the light stimulation, opening the channel and initiating the flow of ions, which resulted in an action potential in those neurons 6 . Deisseroth says that one of the most pleasant surprises to emerge from that first series of experiments turned out to be the precision of the system. \u201cEven though these molecules are not designed to generate action potentials and work on this time scale in neurons,\u201d he says, \u201cit turns out that they can.\u201d This result led to the start of a new field, coined 'optogenetics' by Deisseroth in 2006, where researchers are combining optics with genetics to explore the workings of neural circuits. In the years since the first description of channelrhodopsin-2, Deisseroth's lab has gone on to advance the system and develop new probes. The team identified and developed two more light-activated molecules for optogenetic control of neurons. These were an inhibitor called halorhodopsin, and a  Volvox  channelrhodopsin that can also initiate neuronal activity, but is more than 100 nanometres red-shifted from the peak of the  Chlamydomonas  channelrhodopsin, allowing separable channels of optical control. They also developed various targeting strategies to get channels expressed in specific neuronal cell types and populations, and fibreoptic/laser-diode hardware for adaptation to mammals. The growing number of researchers interested in applying optogenetic approaches to their particular research questions has led to an unexpected effort within Deisseroth's lab. \u201cTechnology distribution and helping other researchers has been another big part of what we are doing,\u201d says Deisseroth. His lab has supplied clones of the channels to more than 300 other labs around the globe for use in organisms ranging from mice to fruitflies. Although the channels have worked in every species tried thus far, in some cases, minor modifications have been required. Although there is sufficient all- trans  retinal in the brains of mice for the channels to function properly, invertebrates must be supplemented with all- trans  retinal through their diet to achieve channel activation. As the basic use of these light-activated channels for studying brain function and circuitry gains more traction in the neuroscience community, Deisseroth is taking the technology a step further. He sees patients at Stanford Medical School and, using optogenetic approaches, his lab establishes different animal models of neuropsychiatric disease. \u201cWe now have models of Parkinson's disease, depression and altered social behaviour relevant to autism,\u201d notes Deisseroth. In all of these disorders, the circuit dynamics are not working well, so Deisseroth's goal is to use his optogenetic models to deconstruct the neural circuitry in an attempt to better understand which parts of the brain are functioning properly, and which are not, in the different disease states. With the ability to work on a wide range of organisms, an expanding tool kit, and growing interest among researchers, Deisseroth sees even greater possibilities for the system. \u201cIn the end, if you can get the gene in, along with the proper light, it works very well.\u201d \n               N.B. \n             Reprints and Permissions"},
{"file_id": "456829a", "url": "https://www.nature.com/articles/456829a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "452901a", "url": "https://www.nature.com/articles/452901a", "year": 2008, "authors": [{"name": "Nathan Blow"}], "parsed_as_year": "2006_or_before", "body": "Sophisticated technologies can now explore nano-scale forces and interactions. But most biologists are staying on the sidelines, waiting to see if these technologies can really help them. Nathan Blow reports. For years, biophysicists have been characterizing the small forces associated with cells and molecules using a variety of techniques born in the worlds of physics, chemistry and even materials science. They are pushing their instruments further in the attempt to measure molecular forces in the sub-piconewton range, and some biologists, at least, are becoming more interested in finding out the extent to which these forces have a role in biological processes. \u201cThe more that the biological community thinks about the role of forces, the more likely it will be that these measurements are made,\u201d says Daniel Fletcher, a bioengineer at the University of California, Berkeley. But he adds that it is a real \u201cchicken and egg\u201d situation: how can you convince yourself that forces are important if you do not measure them, but if they are not important, why measure them? \u201cWe need other biologists to use these techniques,\u201d says Carlos Bustamante, a biophysicist at the University of California, Berkeley. Bustamante has been developing and using 'optical traps' (which hold and manipulate molecules by means of small forces) for more than a decade as part of his work investigating the effects of very small forces on biomolecules. He thinks that if more biologists started to use these tools, the technology would move further, faster. There is consensus on one thing: techniques and instrumentation will have to become more robust and user-friendly if any wider adoption of these methods is going to take place. One emerging trend that might pique the interest of biologists hesitant to explore the nanoscale world is the integration of different technologies. Whether it is combining Foerster resonance energy transfer (FRET) with optical traps or carbon nanotubes with atomic force microscopy (AFM) or even AFM with confocal microscopy, different tools are being integrated in an attempt to maximize our understanding of highly complex molecular interactions, while potentially providing novel insights into the world of the single molecule. \n               Microscopy by force \n             AFM, which was developed more than 20 years ago, has a resolution of a fraction of a nanometre \u2014 1,000 times better than the optical diffraction limit. This resolution is achieved by scanning a sample with a cantilevered tip and measuring deflection of the cantilever with a laser to give topological information such as height, length and shape. Different in principle from optical microscopy, AFM's 'touch scanning' approach can image a surface and measure its associated forces at the same time. AFM was first used in materials science and physics to examine surface properties of semiconductors for process-control applications. And although the interest of biophysicists in applying AFM to study cell surfaces and interactions between single molecules has resulted in advances in the technology, including new modes of scanning in liquids and with live cells, AFM is still a baby in the biology world. \u201cIt is relatively young in its development, but I see more and more cell biologists starting to explore the potential of AFM and similar methods to investigate the role and influence of mechanical properties,\u201d says Sebastian Tille, senior life sciences product marketing manager at Veeco Instruments in Woodbury, New York. \n               Hooke's law \n             The basis of AFM lies in Hooke's law of elasticity \u2014 a simple equation expressing the force generated within a spring when compressed in terms of a 'spring constant' and the length of the spring. Developers have been toying with this relationship for years in their efforts to improve the springs (or cantilevers) used in AFM. \u201cCantilevers are essentially springs, typically 50\u2013200 micrometres in length,\u201d says Roger Proksch, president of AFM specialists Asylum Research in Santa Barbara, California, \u201cand when they are manufactured, there is a degree of uncertainty in their dimension.\u201d As the spring constant depends on these dimensions, this uncertainty can lead to measurement errors. Proksch says this makes spring calibration a crucial procedure for AFM users. In fact, last year he found more than ten new papers in the literature describing new methods of spring calibration or modifications to old methods. What is needed is a way of getting an independent measure of the stiffness of the cantilever. \u201cPeople are still trying to solve this problem in a better way, and getting this into a commercial instrument would be an important step forward,\u201d Proksch says. Advances in the technology behind cantilevers have also enabled the use of AFM to measure forces in liquid samples. Tille notes the use of cantilevers in a 'tapping mode', where the cantilever oscillates across the sample while amplitude is monitored to keep the cantilever at a steady level compared with the surface. This has also helped AFM move towards different biological applications, he says. \u201cIt is a much more gentle method of exploring soft matter such as cells and single molecules,\u201d he says, compared with the 'contact mode' in which the tip is directly moved across the surface. Future cantilever developments might also help to solve another issue facing AFM: speed. Imaging cells with AFM can be slow, taking up to 5 minutes or more to form the image. Developers say that speeding up this experience might entice more biologists to use AFM. Proksch says that if the size of the current cantilevers can be reduced, the result would be faster motion and scan times. He provides a simple analogy: imagine twanging a ruler hanging off the end of a desk, it vibrates at a certain frequency. Now if the ruler were shrunk by a factor of 1,000, the frequency of vibration would dramatically increase. \u201cToday a cantilever in fluid has a resonance frequency of 5\u201350 kilohertz, we would like to see cantilevers that go from 500 kHz to 5 MHz,\u201d he says, noting that a smaller cantilever is something he expects to see in the very near future. Length or, in the case of AFM, the distance from the cantilever tip to the surface, is the other part of Hooke's law and an issue that developers have worked hard on, notably by using piezoelectric materials to move AFM tips. \u201cPiezo's are great: turn a knob, change the potential by a volt and you can move an \u00e5ngstrom,\u201d says Proksch, who also notes that implementing the idea was no small feat. When the potential across the piezoelectric material is changed it will move an \u00e5ngstrom as desired, but then in the next few minutes, or even seconds, it will move another \u00e5ngstrom \u2014 a phenomenon known as creep. Researchers have attacked this problem and a related problem \u2014hysteresis \u2014 in a number of ways, with some AFM developers, including Asylum, deciding to place sensors around the piezoelectric material to measure the creep directly. Fletcher says that with these sensors providing feedback as the tip moves over the surface, this form of drift has been largely eliminated \u2014 although his group has had to work through drift issues of its own (see  'When one cantilever is not enough' ). Many of the advances that AFM developers are working on now involve moving the technology into the hands of biologists. Both Veeco and Asylum have started to combine optical and force microscopes in a single system. Tille notes that Veeco's BioScope II is compatible with advanced light microscopy modalities when the AFM is placed on the top of an inverted microscope. \u201cWith registration between the optical image and the AFM tip, the user can select regions of interest to correlate fluorescence signals in cells with high-resolution topography,\u201d Tille says. In addition to Asylum and Veeco, other companies such as Park Systems in Santa Clara, California, and Novascan Technologies in Ames, Iowa, are exploring combining various optical microscopy techniques with AFM. \n               Feeling single-molecule interactions \n             \u201cI think one of the most important advantages of AFM is that you can measure a structure's dynamics and function at the same time,\u201d says Peter Hinterdorfer, a biophysicist at the Institute for Biophysics at Johannes Kepler University in Linz, Austria. Hinterdorfer's group has been exploring the molecular recognition of ligands and receptors by combining AFM with antibodies to simultaneously obtain cell topography and recognition images in a method the group calls TREC. \u201cThis is basically a counterpart to immunofluoresence in which we try to localize receptors,\u201d explains Hinterdorfer. The AFM tip is functionalized by attaching antibodies and then is brought into contact with the cell membrane. The binding between ligand and receptor can be measured, while obtaining topographical information at the same time. Hinterdorfer is quick to point out that the advantage of AFM for TREC, compared with other optical techniques, is its higher lateral resolution, which can be combined with the ability to obtain mechanical parameters of the cell surface. Although his group developed TREC several years ago, most of the work has been done with either model systems with membrane preparations or fixed cells. \u201cFor single cells there is a lot more to do, because at the moment we are used to TREC on fixed cells, but we are working on live cells as well,\u201d says Hinterdorfer, adding that as high-speed AFM is improved, he expects to see more work following physiological processes in living cells using methods such as TREC. Whereas Hinterdorfer and others use AFM to measure interactions between individual molecules, other groups are trying to visualize such interactions. Some groups are investigating carbon nanotubes as sensors. \u201cOur interest in carbon nanotubes for biological applications is mainly as optical diagnostics,\u201d says Michael Strano, a chemical engineer at the Massachusetts Institute of Technology in Cambridge. \u201cCarbon nanotubes can tell you about the chemical environment of a very small region of space,\u201d he says, and they have optical properties that make them unique for sensor applications (see  'Shrinking down gas chromatography' ). Carbon nanotubes can emit light in the near infrared, which eliminates issues of background fluorescence as this is a part of the spectrum in which very few biological systems autofluoresce. In addition, unlike dyes, or even quantum dots, which can photobleach over time, carbon nanotubes are extremely photostable, allowing measurement of fluorescence changes to obtain molecular-level information. Strano and his group have spent a lot of time working out how to get carbon nanotubes into living cells and to understand how and why, once inside, they move around. Using single-particle tracking methods, they recently showed that the nanotubes can either diffuse around the cell in a confined pattern or be actively transported. \u201cWe are systematically studying these complex behaviours,\u201d says Strano, \u201cit is going to take some effort to sort out and ultimately learn to control them.\u201d Strano's team and others are interested in the application of carbon nanotubes as optical sensors to detect single-molecule binding events. \u201cOur lab has recently demonstrated that we can detect biologically important molecules binding to nanotubes at the single-molecule level in real time,\u201d says Strano. The one-dimensional electronic property of carbon nanotubes means high electron density at a very narrow region of energy. One result of this property is that when a carbon nanotube absorbs a particular photon, the absorbance has a very sharp maximum. This allows a single carbon nanotube to be visualized with spectroscopy, and also gives researchers the opportunity to see if a nanotube responds to a biomolecule in a single-molecule association event. \u201cWe have few tools to study analyte fluxes at very low concentration levels,\u201d notes Strano. His idea is to use carbon nanotubes as local amplifiers of a single interaction event, because the event disproportionately affects the nanotube properties and the nanotube has such a large optical cross-section. Other groups are exploring different technologies to detect single-molecule interactions. Taekjip Ha is a physicist from the University of Illinois at Urbana-Champaign who has been developing fluorescence resonance energy transfer (FRET) approaches for studying single-molecule interactions. \u201cIt took several years to convince ourselves that this technique was going to be very useful to obtain biophysical and biological measurements,\u201d says Ha. Although some studies using single-molecule FRET have been done in living cells, the vast majority of experiments have been  in vitro . \u201cThis is due to technical limitations,\u201d says Ha, \u201cthe probes are not bright enough for robust measurements in living cells.\u201d But he does think that the use of quantum dots could enable the more widespread use of single-molecule FRET in living cells \u2014 with one small change. \u201cIt is currently difficult to do a single-molecule FRET experiment using quantum dots because those made commercially are too big.\u201d A quantum dot plus its coating, which is needed to solubilize the dot and conjugate to a biomolecule, can easily be more than 20 nanometres in diameter \u2014 much larger than the 10 nanometres maximum separation required for a FRET experiment. Ha thinks that single-molecule FRET in living cells will have to wait until the next generation of quantum dots. He thinks this is only a matter of time, as he sees no reason why quantum dots cannot be made smaller. \n               Setting the trap \n             \u201cThe area of single-molecule manipulation goes back to 1991,\u201d recalls Bustamante. \u201cAt that time we did an experiment using gravity and the weight of little beads to extend the DNA.\u201d Bustamante and others soon turned their attention from beads and gravity to a more sophisticated technique called 'optical tweezers', developed by Stephen Chu, currently at University of California, Berkeley, and Arthur Ashkin, now retired from Bell Labs and Lucent Technologies. This technique can trap and manipulate molecules with diameters of nanometres. Optical tweezers make use of a focused laser beam to trap and hold dielectric objects. Developed in the late 1980s and used extensively in the 1990s, researchers only recently started to see a need for even higher-resolution optical tweezer systems. \u201cI think during that earlier time our objective was to be able to follow and extend molecular motors, see molecules unfold and measure the mechanical properties of nucleic acids,\u201d says Bustamante, \u201cand you could do quite a bit of research without going into extremely high resolution.\u201d But today, Bustamante thinks the biology has actually pulled biophysicists towards studying things at a smaller scale \u2014 requiring more stability, less drift, more spatial resolution and improved temporal resolution from optical tweezers. For high-resolution optical tweezers to work, the key issue of drift had to be overcome. One approach, developed by Steven Block's group at Stanford University, California, is to attach a molecule to two beads and use two optical tweezers, neither of which is physically attached to the lab environment, to hold the molecule. Bustamante's group recently added another element to this approach \u2014 the use of a single laser beam that splits into the two optical tweezers, to control for variation in laser light by providing feedback on both traps (J. R. Moffitt  et al .  Proc. Natl Acad. Sci. USA   103 , 9006\u20139011; 2006). Companies are eyeing the area of optical tweezers seriously. Carl Zeiss in Jena, Germany now offers the Palm Micro Tweezers that use either a red or near infrared laser that can be split into two independent traps. Arryx, in Chicago, Illinois, has taken a different approach for its optical-tweezer technology, called BioRyx 200, which is a holographic optical trap created from a spatial light modulator and allows the control of up to 200 objects simultaneously. Ha is starting to combine optical tweezers with single-molecule FRET to study the enzymes involved in transcription and translation. He thinks that the combination of these techniques is necessary as single-molecule research moves towards studying more complex properties of enzymes and proteins. \u201cIt is now possible to measure the fundamental reaction steps of the enzymes using optical traps to obtain single base pair resolution,\u201d says Ha, but he adds that using FRET allows conformational changes of the enzymes themselves to be monitored at the same time. This combination allowed Ha's team to directly measure via fluoresence conformational changes as a function of applied force to dissect the landscape of the Holliday junction \u2014 a mobile junction between four strands of DNA (S. T. Hohng  et al .  Science   318 , 279\u2013283; 2007). \n               Added colour \n             Ha is moving into three-colour FRET with optical traps to get more information from each experiment. Whereas two-colour FRET can measure one distance, multiple-colour FRET will provide additional distance information from each experiment. Ha is using three-colour FRET (one donor and two acceptors) to study the diffusion of protein on single-stranded DNA and the enzymatic activity of the ribosome. Bustamante is also excited about the possibility of using FRET with optical tweezers. \u201cThe bottom line is that it will allow you to follow a particular biological process in a multidimensional fashion,\u201d he says. His lab has built a prototype optical trap, called 'minitweezers', that will incorporate fluorescence. With this, the team hopes to bring together the whole optical table into an instrument the size of two coffee cans, Bustamante says. Measurements of force and interaction at the single-molecule level are being reported with increased frequency. And there is interest from the wider biological community: at the 2007 meeting of the American Society for Cell Biology there was a session focused on the analysis of mechanical forces. \u201cIt was pretty amazing to see a ballroom of more than a thousand people listening to talks on the significance of mechanical properties of cells,\u201d Tille says. And with developers actively trying to engage biologists while continuing to advance the power of these technologies, one has to wonder how many more will be listening next year. Reprints and Permissions"},
{"file_id": "451858a", "url": "https://www.nature.com/articles/451858a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": " In November 2007, two groups headed by James Thompson at the University of Wisconsin-Madison and Shinya Yamanaka at Kyoto University in Japan made headlines when they described methods to reprogramme adult human cells to a pluripotent state. These cells, called induced pluripotent stem (iPS) cells, are genetically modified by the integration of up to four DNA-transcription factors into the adult cell genome. Soon after, in December, George Daley, of Harvard Medical School in Boston, Massachusetts, and his colleagues also demonstrated iPS cells could be generated from a wide variety of adult cells. \u201cFor any patient we can use the technique and take a skin biopsy to establish a pluripotent cell,\u201d says Daley. One of the most valuable aspects of the iPS-cell technology currently, he says, is the ability to perform disease modelling. And to take advantage of this, Daley and his colleagues are working on generating large numbers of disease specific iPS-cell lines. Although an incredible step forward in stem-cell research, iPS cells are in fact at the beginning of a long road. \u201ciPS cells as we make them today are riddled with viruses,\u201d says Daley. He says that these viruses can be mutagenic and have the potential to activate oncogenes, so at the moment iPS cells remain a research tool and not a potential therapeutic agent. But the next step for iPS cells could move them closer to therapeutic applications. \u201cOne of the next big milestones will be making these cells without the use of viruses \u2014 leaving the cells in a genetically pristine state,\u201d he says. Robert Lanza from Advanced Cell Technologies in Worcester, Massachusetts agrees and even sees routes to creating iPS cells without genetic modification. \u201cYou can potentially use fusion proteins or small molecules \u2014 there are many ways to skin the cat here.\u201d But even if iPS cells can be created without genetic modifications, the question that researchers are asking now is, do these cells really have the same properties and potentials as embryonic stem cells? \u201cI am very excited about iPS cells but now we need to look very carefully at the properties of these cell lines,\u201d says Martin Pera of the University of Southern California in Los Angeles. Pera says that these cells might differ in their abilities to differentiate in the same way that embryonic stem cells seem to. \u201cIf you have to make ten lines for each patient \u2014 is patient-specific therapy really realistic or will large banks of iPS cells that are tissue typed be required?\u201d asks Pera. Only time will tell in what directions iPS cells might be taken for basic research or for clinical applications. And while iPS-cell properties are being studied and new methods to derive these cells without genetic modifications are being created, human embryonic stem-cell research will continue. Pera says that this is the best way to proceed at the moment. \u201cWe need to move forward on both fronts.\u201d \n               N.B. \n             Reprints and Permissions"},
{"file_id": "452904a", "url": "https://www.nature.com/articles/452904a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "Imagine looking down at your watch to check the time, while the same watch performs chromatography to determine the chemical composition of the air around you. Although unlikely to be in the shops any time soon, recent advances in carbon nanotube technology and surface chemistry could make very small gas-chromatography devices a future reality. Michael Strano's group at the Massachusetts Institute of Technology in Cambridge has been developing a highly sensitive, miniaturized gas-chromatography instrument based on carbon nanotubes. Sensitivity was key for this device, Strano wanted to be able to detect analytes in the part-per-trillion range \u2014 an amount so small it can be difficult to grasp at first. \u201cThis is like opening a small vial in a crowded high-school gymnasium,\u201d explains Strano, \u201ceven if you open the vial and shut it quickly, you have actually released hundreds of parts per trillion of this mixture everywhere.\u201d For the backbone of its new gas-chromatography instrument, Strano's group created single-walled nanotube, chemi-resistor arrays in which the nanotubes were aligned between electrodes. Changes in the electrical resistance can be analysed as a desired molecule binds the nanotube. To tune the binding of the analyte to the arrays the group also had to develop a new chemistry (C. Y. Lee and M. S. Strano  J. Am. Chem. Soc.   130 , 1766\u20131773; 2008). \u201cYou can use mean basicity to target whether your molecules will bind very strongly, in between, or not at all,\u201d explains Strano. Then using a microelectromechanical systems device it should be possible to have precise control over which molecules bind to the array. As a proof-of-principle, Strano's group created a very simple device by etching a chromatography column on a chip. Using a 100-micrometre trench the team showed that mixtures can be separated and, using a fast detector, it is possible to transduce each peak as it comes out. Strano group's demonstrated parts-per-trillion detection on its device, confirming the potential of this approach to miniaturized, on-chip gas chromatography. And for Strano, seeing an actual physical device working at that level was extremely rewarding. \u201cWe are engineers, so we try to make devices that work.\u201d \n               N.B. \n             Reprints and Permissions"},
{"file_id": "452905a", "url": "https://www.nature.com/articles/452905a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "452903a", "url": "https://www.nature.com/articles/452903a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "\u201cI did my doctoral work building force and optical microscopy systems,\u201d says Daniel Fletcher, a bioengineer at the University of California, Berkeley. \u201cI was an instrument developer and did not really get to use them, as the main goal was always to develop a new contrast mechanism or achieve the best resolution.\u201d But all that changed when Fletcher started his postdoc training and saw cells crawling. \u201cI realized that although we could build these fancy instruments that control forces down to the piconewton level or below, we had no clue how to assemble complex molecular systems that do what the crawling cells could do.\u201d This led Fletcher to study the dynamic actin networks of crawling cells using atomic force microscopy (AFM) and optical traps. The biochemistry of the actin system has been studied for more than 20 years and actin networks can be grown  in vitro , which provides the opportunity to directly probe how network assembly is involved in force generation and shape-changes in cells. \u201cWhat we have been trying to do with both AFM and optical traps is to create a controlled resistance to the growth of these networks and study their behaviour,\u201d he says. One approach that Fletcher's group uses is to grow actin networks on the ends of its AFM cantilever and then monitor how the networks adapt to different loads and how those forces influence the architecture of the structures generated. Using AFM in this manner provided a challenge, requiring Fletcher to use his instrument-development skills. \u201cOne of the things that AFM is not good at doing is measuring sustained displacement over long time periods, because the surface can drift relative to the cantilever,\u201d says Fletcher. Cantilevers are not directly connected to the surface being scanned \u2014 they are held by a fluid cell, held by a mount, which is connected to a moving stage that is connected through an even longer path to the surface. The distance between cantilever and surface helps to make crucial calculations of the extent of deflection of the cantilever by the surface. Fletcher explains that because of this large mechanical path from the surface to the cantilever, which can alter the reference position of the cantilever relative to the surface due to thermal drift, the forces measured over time can change even if the sample does not. At short time scales this might not be an issue, but for his team, trying to measure actin growth continuously on the microscope tip over hours, drift of this sort can potentially influence the data collected. Fletcher decided that one approach to correcting for this drift would be a dual cantilever AFM system. Having two adjacent cantilevers allows one cantilever to always be in contact with the surface while the second measures the growth of the actin networks. Fletcher says that by always having one cantilever on the surface, they can now detect movement or drift directly and use feedback to correct the cantilever's position. Fletcher continues to develop new approaches to making force measurements. His team recently developed an epi-fluorescence 'side-view' AFM device (most commercial systems come with the AFM system mounted on top of a microscope) to visualize growing actin networks between the cantilever and the surface. \u201cWhere people interested in single-, or even multimolecule, biophysics can continue to make contributions is in developing better tools that help to overcome the limitations of existing technologies and reveal new behaviour of biological systems.\u201d \n               N.B. \n             Reprints and Permissions"},
{"file_id": "453691a", "url": "https://www.nature.com/articles/453691a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "453687a", "url": "https://www.nature.com/articles/453687a", "year": 2008, "authors": [{"name": "Nathan Blow"}], "parsed_as_year": "2006_or_before", "body": "Advances in sequencing technology and tools for analysis are allowing researchers to unravel the environmental diversity of microbes faster and in greater detail than ever before. Nathan Blow reports. This year marks the tenth birthday for metagenomics \u2014 the cloning and functional analysis of the collective genomes of previously unculturable soil microorganisms in an attempt to reconstruct and characterize individual community inhabitants. Since the term was coined by Jo Handelsman and her colleagues at the University of Wisconsin in Madison, its scope has expanded greatly with descriptions of the microbial inhabitants of environments as diverse as the human gut, the air over New York, the Sargasso Sea and honeybee colonies. And within these communities researchers are now uncovering a wider range of microorganisms, thanks in large part to advances in DNA-sequencing technology. \u201cWe can look at the metagenomic analysis so much more deeply, at such a better cost,\u201d says Jane Peterson, associate director of the Division of Extramural Research of the National Human Genome Research Institute in Bethesda, Maryland, which recently launched a five-year initiative to explore the human microbiome. Although sequencing technology is creating opportunities for metagenomics research, all these new data are straining downstream analysis. \u201cComputational analysis of metagenomic data still has quite a few outstanding questions,\u201d says Isidore Rigoutsos, manager of the bioinformatics and pattern-discovery group at IBM's Thomas J. Watson Research Center in Yorktown Heights, New York. The assembly and prediction of gene function for high-complexity microbial communities still poses challenges 1 , for example (see  'Benchmarks and standards' ). Maybe it is the promise of rapidly improving sequencing technology or the new environments being explored, but Peterson says that she has seen a growing interest in large metagenomics projects \u2014 particularly the Human Microbiome Project, which aims to unravel the microbial communities associated with various parts of the human body, including the gut (see  page 578 ). \u201cPeople somehow identify with the Human Microbiome Project. It is interesting how this project, especially as it is studying the gut, has really caught a lot of people's attention.\u201d Over the past few years, the race to sequence DNA faster and more cheaply has been taking centre stage. Several next-generation DNA sequencing systems are now available, boasting gigabase outputs for a variety of genetic applications. But when it comes to sequencing environmental samples that contain many different microorganisms in varying amounts, the next-generation options have their limitations. \u201cI would say the only next-generation sequencing technology suitable for metagenomics at the moment is the 454 system,\u201d says Stephan Schuster a biochemist at Pennsylvania State University in University Park. Schuster is not alone \u2014 almost all metagenomic studies currently being reported rely on either 454 technology or conventional Sanger sequencing. The main reason is simple: read length. \n               Long-term tool \n             Developed by 454 Life Sciences in Branford, Connecticut, the 454 system relies on an emulsion polymerase chain reaction (PCR) step that is coupled to pyrosequencing. Individual fragments of DNA, 300\u2013500 base pairs long, are attached to beads  in vitro  and amplified with PCR to generate millions of identical copies on each bead. Fragments are then sequenced by use of a massively parallel reaction format in 1.6 million wells on a picotitre plate. Using this system, researchers can generate around 250 base pairs of sequence per reaction while performing 400,000 reads in a single instrument run. Other next-generation sequencing systems typically generate fewer than 50 base pairs per reaction, relying instead on more reads to generate a greater number of base calls. These shorter read lengths are suitable for applications such as candidate gene resequencing, transcriptional profiling and microRNA discovery. But the 454 system's longer read length has attracted metagenomics researchers, and is enticing the community away from Sanger sequencing, which provides reads of 700\u20131,000 base pairs per reaction. Forest Rohwer, a microbiologist at San Diego State University in California, sees the advantages of 454 when analysing sequence data from environmental samples. \u201cWe know that when you are below 35 base pairs it is hard to get information out, at 100 base pairs you tend to lose information, but if you get to 200 base pairs you start to gain a lot of information,\u201d he says. Although he also adds that it is not clear at the moment how much could be gained from even longer reads. \u201cThe information content might be a little different because the Sanger reads are longer. At present, 454 is between 200 and 400 base pairs, depending on who you talk to, so you are not likely to cover an open reading frame, but you do get a significant amount of data,\u201d says Karen Nelson, an investigator at the J. Craig Venter Institute in Rockville, Maryland. But she notes that studies using 16S ribosomal RNA pose a problem for 454 read lengths. 16S ribosomal RNA is sequenced to give a sense of species abundance and composition in a community without reconstructing entire genomes from the inhabitants. It is around 1,500 base pairs long, which can be sequenced in two or three Sanger reads. But, according to Nelson, with the 454 system, researchers will have to focus on a variable region of the gene to identify the species or come up with other metrics, instead of relying on full-length reads. Nelson thinks one approach might be to combine Sanger capillary sequencing with 454, so that Sanger methodology could be used to sequence the ends of inserts from large-scale cosmid or fosmid libraries, which would act as scaffolds for the placement of the shorter, but more numerous, 454 reads. But the length of 454 runs could soon be less of an issue. \u201cWe are getting very close to entire exons and whole genes with our next version,\u201d says Michael Egholm, vice-president of research and development at 454 Life Sciences, noting that the company plans to introduce a 500-base-pair capacity instrument later this year. \n               Diverse results \n             The widespread use of next-generation sequencing technology to explore microbial communities has brought something else to light \u2014 greater microorganism diversity. Next-generation systems bypass the cloning of DNA fragments before sequencing, a necessary step for most Sanger sequencing, and this has resulted in the discovery of new microorganisms that previously had been missed because of cloning difficulties. The lack of cloning has also made determining relative numbers of microbes in the community easier. \u201cAs there is no cloning, we have very low bias,\u201d says Egholm. Schuster and others have also noted less bias when applying 454 sequencing to their environmental samples. \u201cWhat is very comforting to see is that a lot of tests have been done by different groups finding the same results with 454 sequencing and qPCR,\u201d says Schuster. Egholm and his colleagues are using their sequencing platform for several large-scale metagenomics projects \u2014 the biggest of which actually originated by chance. \u201cThe biggest metagenomic project on Earth might be our Neanderthal genome project,\u201d says Egholm. They are using 454 to sequence the complete genome of a Neanderthal, which Egholm says they hope to release by the end of the year. But 95\u201398% of the DNA in the Neanderthal sample comes from the environment rather than from a Neanderthal. This means that to get the 1\u00d7 coverage, or roughly 3 billion base pairs, of the genome, the team must sequence somewhere between 70 billion to 100 billion base pairs of these environmental samples. As Egholm's team begins to comb through those environmental data contaminating the Neanderthal samples, he says the initial results have been surprising. \u201cWhether these are recent bacteria, or ones that ate the poor guy when he died, we cannot be certain yet, but I can tell you from the first few contigs where we got multi-kilobase lengths \u2014 they matched nothing in GenBank.\u201d \n               The next, next generation \n             \u201cThe biggest change in metagenomics will come from 'third generation' sequencing systems or single-molecule sequencing,\u201d says Schuster. For the metagenomics community this next-next-generation promises longer reads than Sanger sequencing, even higher throughput, lower costs and better quantitation of genes. VisiGen Biotechnologies in Houston, Texas, is working on a method for sequencing single molecules in real-time that uses F\u00f6rster resonance energy transfer (FRET). In this system, the polymerase is engineered to contain a donor fluorophore, and each nucleotide has a differently coloured acceptor fluorophore attached to the gamma phosphate. When one of the nucleotides is incorporated by the polymerase into a native strand of DNA, a unique FRET signal is given off, and the pyrophosphate-containing fluorophore is then released leaving the synthesized strand of DNA ready for the next incorporation event. By imaging the colour changes as each incorporation occurs, VisiGen hopes to sequence single molecules in real-time and apply this on a massively parallel array with an output of up to one million bases per second. Reveo in Hawthorne, New York, is developing a method to sequence DNA using nano-knife-edge probes, which pass over DNA that has been stretched and immobilized in a channel 10 micrometres wide. By using four different nano-knife-edge probes, each 'tuned' to a different frequency, Reveo hopes to non-destructively sequence DNA while eliminating the need for a costly imaging component. And Pacific Biosciences of Menlo Park, California, recently announced its single-molecule sequencing technology based on zero-mode waveguides 2 . These new single-molecule sequencing methods show the promise of generating much longer read lengths in the future, says Schuster. VisiGen's method predicts sequence reads of 1,500 base pairs \u2014 the size of 16S ribosomal RNA \u2014 whereas the Pacific Biosciences approach could produce reads as long as 10 kilobases. And single-molecule sequencing is becoming a reality: in March 2008, Helicos BioSciences of Cambridge, Massachusetts, sold the first single-molecule sequencing system. The HeliScope uses a sequencing-by-synthesis approach in which the DNA is first fragmented into pieces 100\u2013200 base pairs long. Adaptors of known sequence are attached to the ends of the fragments so that they can be captured on the surface of a coated flow cell. Once attached, a mix of labelled nucleotides and polymerase is flowed through the cell and the surface is imaged at different times to determine whether labelled nucleotides have been incorporated. The key to the technology is a method to effectively cleave off the labelled nucleotides following incorporation, permitting additional rounds to be performed. \u201cThe moment metagenomics goes down to the single-molecule level, it will be possible to assess even very low abundance messages at a very large sequence interval,\u201d says Schuster. \n               Information overload \n             \u201cPeople struggle at the moment with extracting basic information from metagenomic data,\u201d says Peer Bork a bioinformatician at the European Molecular Biology Laboratory in Heidelberg, Germany. Bork and others say that most bioinformatic analyses being done on metagenonomic data sets involve relatively basic procedures such as assembly and gene annotation. There are several options for such procedures. Some use web-based tools, such as the Rapid Annotations using Subsystems Technology (RAST) server developed by Argonne National Laboratory and the University of Chicago, in which a metagenomics data set is submitted and an analysis file is returned. Others, such as the Metagenome Analyzer (MEGAN) program, developed by Schuster and his colleagues, run on a desktop computer. \u201cI think being able to handle these large data sets and developing tools for visualization are critical, as they will allow researchers to write meaningful publications,\u201d says Schuster. He explains that MEGAN uses a binning format based on the National Center for Biotechnology Information's taxonomy database to display how often a certain taxon occurs. Other efforts, including the Community Cyberinfrastructure for Advanced Marine Microbial Ecology Research and Analysis, or CAMERA, aim to bring together bioinformatics resources and a data repository to assist with analysis of the data sets. One issue with the various tools available, notes Rohwer, is that none has been adopted as standard. \u201cIt is still pretty much a cottage industry,\u201d he says. \u201cStandardization of annotations and gene-prediction quality during the early stages of metagenomic studies is something that needs to be addressed,\u201d agrees Nelson. She adds that it is now much easier to generate data than to interpret what they mean. \u201cIn a number of situations we are dealing with unknown species that have not had their genomes sequenced, so there will not be a reference genome to align to.\u201d Rigoutsos agrees. \u201cHow can you tell the phylogenetic provenance of a sequence segment if the databases contain no examples of it?\u201d he asks. He sees the situation as being akin to the early days of human genomics when some gene-prediction programs relied on database searches to draw conclusions. His group has developed PhlyoPythia, a software tool that also takes a binning approach to classifying sequence contigs assembled from metagenomic data sets. Annotation is not the only standardization issue. \u201cWe need to do a better job of collecting information when we take samples,\u201d says Rohwer. The time, place and collection method can profoundly affect the microbial composition in a sample. Whether the sample is collected during the day or at night, acquired from a person's right or the left arm, or even if two soil samples are collected 5 millimetres apart, these seemingly small differences can lead to very different communities of organisms. Rohwer thinks that collecting a standard set of information for each sample would make future comparisons between different data sets easier and so provide greater biological insight. There is hope that some of these issues will be addressed as members of the metagenomics community become increasingly involved with large-scale, multi-institution projects (see  'The human environment' ). \u201cThe Human Microbiome Project is going to be one good example where standards are in place for data acquisition, generation and analysis,\u201d says Nelson. \n               Dynamic future \n             With bioinformatics tools and sequencing poised to go even faster at a lower cost, researchers are eyeing the next level of metagenomic analysis \u2014 a move from simply cataloguing the microbes in a community to understanding the interactions and dynamics of the organisms. \u201cI think a systems-level approach that gives a holistic view of these communities will open different research possibilities,\u201d says Rigoutsos, whose group is now working on studying metagenomes for biofuel applications. Rohwer hopes to use metagenomics to manipulate a system and then trace how the community goes through changes at a global level. He thinks that with sequencing speed and cost declining so rapidly, these experiments, which only a few years ago would have been impossible, are now within reach. Bork also wants to move towards analysing global communities. \u201cMost groups concentrate on the early parts, but I think it is time to ask the bioinformatics people to develop methods beyond there, to get ecology concepts projected on those molecular data.\u201d Nelson says that all the technology development and interest in metagenomics is spurring the field along nicely. \u201cI think the technology is very promising and it will get better. It is a great time to be doing this.\u201d Reprints and Permissions"},
{"file_id": "453687b", "url": "https://www.nature.com/articles/453687b", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "The complexity of microbial communities can vary drastically, from a couple of microorganisms to thousands or even millions, making the reconstruction of whole genomes from some samples tricky. \u201cIf the community is low in complexity, it should allow one to reconstruct genomes with high accuracy,\u201d says Isidore Rigoutsos, manager of the bioinformatics and pattern-discovery group at IBM's Thomas J. Watson Research Center in Yorktown Heights, New York. But when it comes to highly complex communities, things are less straightforward. Rigoutsos and his team have tested several genome assemblers and gene-prediction tools on simulated metagenomic data sets with varying degrees of complexity. Knowing the composition of the community allowed the team to benchmark and evaluate the tools. \u201cWe found that as the complexity increased, many of the computational tools had an increasingly hard time,\u201d says Rigoutsos. For most high-complexity samples, he says, the genome assemblers could not generate larger contigs, and several contigs that were assembled were actually chimaeric mixtures of sequences. For metagenomic analysis, smaller contigs and single reads make assigning the sequence to a specific microorganism difficult. \u201cWe want to be able to assign a read of less than 1,000 nucleotides,\u201d says Rigoutsos, which might allow researchers to determine species composition from high-complexity samples without the need to generate larger contigs. Rigoutsos and his colleagues have made three simulated data sets available to researchers interested in testing assembly and prediction programs. The problem of data analysis is not restricted to metagenomics \u2014 a growing number of researchers are using next-generation sequencing platforms and generating the quantity of data that in the past might only have been possible at large genome centres. Several companies are developing software to address this issue. CLC bio in Cambridge, Massachusetts, offers the CLC Genomics Workbench, which provides reference assemblies of data from various next-generation sequencing systems as well as mutation detection. A future version of the program will incorporate algorithms for the  de novo  assembly of Sanger as well as next-generation sequence data. Meanwhile, Geospiza in Seattle, Washington, and GenomeQuest in Westborough, Massachusetts, are developing software to analyse data generated by Applied Biosystems SOLID next-generation sequencing platform. The combination of assembly software and data sets to benchmark results should help solve some of the complexity problems associated with metagenomics. \u201cIf you sequence sufficiently, even 200 base-pair reads are enough,\u201d says Rigoutsos. But he adds that the real question is how many 200 base-pair reads will be needed before we can truly understand complex communities. Others are finding that with enough reads, fewer than 200 base pairs might be sufficient. Jens Stoye from Bielefeld University in Germany has compared a data set of 35 base pair reads generated on the Genome Analyzer from Illumina in San Diego, California, with a 454 data set for the same low-complexity sample. Although 99% of the Genome Analyzer's sequence data were discarded, because the system generates up to 50 million reads he could assign the species in the sample with the same efficiency from both data sets. \n               N.B. \n             Reprints and Permissions"},
{"file_id": "453689a", "url": "https://www.nature.com/articles/453689a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "\u201cI don't know why the Human Microbiome Project bubbled to the top now instead of previously,\u201d says Jane Peterson from the National Institutes of Health (NIH). \u201cCertainly there have been metagenomic studies done with the old technologies.\u201d But Peterson thinks recent reports exploring the human gut microbial community, along with new, advanced sequencing technologies, might have been enough to pique the interest of the reviewers who decided to fund the Human Microbiome Project (HMP). The project is a 5-year, US$115-million effort to study the microbial communities inhabiting several regions of the human body, including the gastrointestinal and female urogenital tracts, oral cavity, nasal and pharyngeal tract, and skin, and how those communities influence human health and disease. The effort is viewed by many researchers in the metagenomics community as particularly timely as most agree improvements in sample collection standards and analysis tools are much needed. Karen Nelson from the J. Craig Venter Institute, who is also a participating investigator in the HMP, says these issues can finally be addressed with a project of this scope as all samples will collected and treated in the same way, and standards will be put in placefor annotation of the metagenomic data. The project will fund research in several areas, although the construction of a data resource for sequencing DNA samples from as many as 250 individuals, and projects aimed at demonstrating how changes in the human microbiome are related to health and disease are centrepieces of the programme. Other project initiatives include the development of new metagenomics technology to isolate bacteria that are currently cannot be cultured, development of new bioinformatic programs and tools for analysis of large genomic data sets, data analysis and coordination centres, and analysing and understanding the ethical, legal and social issues of the project. The HMP's initial sequencing efforts began this year. Peterson says that towards the end of the last fiscal year, the NIH Roadmap office had funds available for the HMP. Through existing relationships with large sequencing groups, the HMP was able to start quickly and generate preliminary sequencing data, which are being used for the other demonstration projects. This initial effort will result in the sequencing of 200 new bacterial organisms, recruitment of patients for metagenomic studies, and some 16S ribosomal RNA metagenomic sequencing to assess microbial diversity at the various sites. Peterson says that the protocols for sampling and recruitment have provided some early challenges. The HMP hopes to sample the same sites on all 250 individuals, but with so many sites, standardizing sampling can be tricky. \u201cThe protocol for the different sites has to be well worked out,\u201d she says, \u201cthe oral community has to be happy with the requirements that the skin community brings to the table.\u201d Although it is just at the beginning, the HMP is scheduled to award its first rounds of grants to researchers this autumn. Peterson says that in the future the project's standardization efforts might not be restricted to the United States. \u201cWe are also forming an international consortium to coordinate international projects.\u201d \n               N.B. \n             Reprints and Permissions"},
{"file_id": "451859a", "url": "https://www.nature.com/articles/451859a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "451856a", "url": "https://www.nature.com/articles/451856a", "year": 2008, "authors": [], "parsed_as_year": "2006_or_before", "body": "In culture, stem cells rely on signals to differentiate to other cell lineages. Although certain growth factors are known to promote some differentiation programmes, it is now becoming clear that physical interactions between cells and mechanical sensing may also help to promote differentiation. So researchers are developing a variety of three-dimensional (3D) matrices for stem-cell differentiation. In 2006, Dennis Discher of the University of Pennsylvania in Philadelphia, and his colleagues, demonstrated the potential of the matrix alone to promote differentiation. \u201cThe idea was to use synthetic gels with a collagen monolayer to mimic the elasticities of certain tissues and see how the cells respond,\u201d says Discher. The work was done under constant serum conditions, without any discriminating soluble factors or growth factors that might promote differentiation in one direction or another. The results showed that by simply varying the elasticity of the matrix, the attached mesenchymal stem cells could undergo either neurogenesis, myogeneis or osteogenesis. They went on to show that once the cells adhere, they begin setting up the stress fibres that actively pull on the adhesions and on the matrix outside. \u201cWe showed that the cells feel the matrix and respond to it,\u201d says Discher.  Stefan Przyborski is the founder of Reinnervate in Durham, UK, the developer of a new scaffold for routine 3D cell culture. \u201cWe create ways of making the  in vitro  environment a more realistic environment for cell growth,\u201d he says. Reinnervate's scaffold is unique, but might seem familiar to biologists who have performed cell culture. \u201cWe created a scaffold made of polystyrene, the same material that people currently grow their cells on.\u201d This is important because researchers know how cells respond to this material in two-dimensional applications. But he also notes that this 3D environment enhances differentiation when compared with cells cultured on two-dimensional plates. Invitrogen of Carlsbad, California has developed Algimatrix, an inert 3D scaffold made of alginate. \u201cThe idea behind Algimatrix is that it provides a framework for cells to reside in that will enable formation of spheroidal structures in a controlled manner,\u201d says Mark Powers, a director at Invitrogen. Powers says that when embryonic stem cells form embryoid bodies, they can aggregate into very large structures where the cells on the inside can be oxygen or nutrient limited. But with Algimatrix the aggregates grow to a consistent size and not beyond the size of the pores provided by the scaffold. And since it is an inert scaffold, it actually promotes cell interactions. \u201cA scaffold such as Algimatrix would allow a researcher to culture cells in 3D aggregates to promote differentiation.\u201d It is becoming clear that mechanical interaction has a role in cell differentiation. \u201cWhat we are really describing is a sense of touch \u2014 the cells have no eyes or ears, so they use this sense of touch to tell where they are \u2014 this is all part of sensing and responding to the environment,\u201d says Discher. \n               N.B. \n             Reprints and Permissions"},
{"file_id": "451855a", "url": "https://www.nature.com/articles/451855a", "year": 2008, "authors": [{"name": "Nathan Blow"}], "parsed_as_year": "2006_or_before", "body": "With the number of stem-cell lines rapidly increasing, technology developers are working to improve systems for culturing and efficient differentiation \u2014 all with an eye on the clinic. Nathan Blow reports. The explosion in stem-cell research that followed the isolation of human embryonic stem cells 1  in 1998 has seen the number of cell lines available to researchers increase dramatically. This burst may be due to the fact that embryonic stem cells are pluripotent \u2014 having the potential to generate all adult and embryonic cell types \u2014 so there are exceptional possibilities for their use in medicine. Since 1998, about 200 embryonic stem-cell lines have been derived along with many more adult stem-cell lines. Supporting all these stem cells are many ways to help propagation and differentiation. So it should come as no surprise to learn that there are no standard culture conditions for stem cells. \u201cPeople sometimes bristle at the word standard,\u201d says Derek Hei, director of the US National Stem Cell Bank (NSCB) in Madison, Wisconsin. \u201cIn the stem-cell community there is not really a standard culture method.\u201d  But 'standard' should not be an alarming word. \u201cIn talking to stem-cell researchers, we found that they are really looking for standardization of culture methods,\u201d says Tori Richmond, strategic initiatives specialist at Thermo Fisher Scientific in Waltham, Massachusetts. Arriving at any such standard would require evaluation of all available cell lines on every culturing system \u2014 a gargantuan task. But Martin Pera, director of the Institute for Stem Cell and Regenerative Medicine of the University of Southern California in Los Angeles, suspects that larger groups, such as the International Stem Cell Initiative, might tackle the standardization issue in the near future. \u201cI am hoping that within the next couple of years we will arrive at one or two cell-culture platforms that everyone can use.\u201d \n               The cell in a haystack \n             Several companies and stem-cell banks are now providing a range of older and more recently derived embryonic stem-cell lines to researchers, which has provided a greater understanding of the properties of these cells. Ethical concerns regarding the use of embryonic stem cells led the US government to establish a list of 21 embryonic stem-cell lines that can be used for research funded by federal sources. To this end, the NSCB was established in 2005 to characterize, hold and distribute these approved human embryonic stem-cell lines. For each of these lines, the bank is currently performing extensive testing, says Hei, including characterization of gene expression profiles, karyotype stability, and other standard embryonic stem-cell assays such as flow cytometry for specific cellular markers of pluripotency. At present the NSCB offers 15 of the 21 federally approved embryonic stem-cell lines to researchers. \u201cWe have set up a website and created master cell banks for each of the lines,\u201d Hei says, adding that not only is the characterization data posted online for interested researchers, but the NSCB also provides protocols for propagating and maintaining cell lines on its site. The University of Massachusetts Medical School in Shrewsbury is also expected to open a stem-cell bank to distribute embryonic stem-cell lines derived by researchers within the state, including more than 30 embryonic stem-cell lines to be supplied by Harvard University, during the next year.  Similar repositories also exist outside the United States. For example, the UK Stem Cell Bank in Hertfordshire, started in 2003, and currently provides eight different human embryonic stem-cell lines. Other new stem-cell banks are on the way. ES Cell International (ESI) in Singapore is one of a number of commercial sources for researchers looking to obtain human embryonic stem cells. It offers six of the US-approved lines, as well as several other human embryonic stem-cells lines derived at the company. Cellartis in Gothenburg, Sweden, and Millipore in Billerica, Massachusetts, also provide cell lines. Cellartis is one of the largest sources for defined embryonic stem-cell lines, boasting 30 different lines that can be obtained at various stages of passage or as subclones of selected lines. Millipore now offers two human embryonic stem-cell lines, human neural progenitors, as well as several mouse embryonic stem-cell lines. \n               Search and you will find \n             Other companies deriving new human embryonic stem-cell lines can also provide them to researchers. \u201cIf people request them, then yes, we will provide lines,\u201d says Robert Lanza, chief scientific officer at Advanced Cell Technology, in Worcester, Massachusetts. He says that Advanced Cell Technology also plans to give several lines to the Massachusetts stem-cell bank.  Having many different cell lines available for research might be critical to understanding the true potential of these cells. \u201cIt is funny, people say 'embryonic stem cells' but the cell lines all have their own behaviour,\u201d notes Lanza. Although a recent study by the International Stem Cell Initiative examining 59 human embryonic stem-cell lines found a high degree of phenotypic similarity between all lines 2 , some researchers have noted differences is the behaviour of the cell lines in culture. \u201cCell lines vary in how easy they are to propagate  in vitro ,\u201d says Pera. And Lanza notes that researchers at Advanced Cell Technology can tell embryonic stem-cell lines apart based on the behaviour of the cells in culture. \n               The mature way to look at stem cells \n             Although embryonic stem cells are pluripotent, adult stem cells are multipotent, and can only regenerate specific adult cell types in the body. And for those researchers interested in investigating the potential of adult somatic stem cells, several companies are advancing the idea of providing adult stem cells that have been qualified for culturing on specific media. Thermo Fisher Scientific supplied media and reagents for culturing adult stem cells, but has now moved into supplying adult stem-cells lines as well. \u201cOur strategy has not only been to create kits to support culturing and differentiation of adult stem cells, but also offer the validated stem cells themselves,\u201d says Alain Fairbank, research market manager at Thermo Fisher Scientific. The company now offers four mesenchymal (multipotent stromal cells) stem-cell lines, a haematopoietic (germinal cells from umbilical cord blood) stem-cell line, and a recently discovered multipotent cord-blood cell line that Fairbank says seems to be less restricted in its multi-lineage potential. The advantage of this approach is that all media, reagents and culture-ware are certified and validated to work with these specific cell lines. \u201cOur focus has been developing tools that are validated to work together with stem cells,\u201d Fairbank says. Invitrogen of Carlsbad, California, is also \u201cheavily focused on mesenchymal research\u201d, says Joydeep Goswami, the company's vice-president of stem-cell research. \u201cThese cells are probably going to be the first non-haematopoietic stem cells to be used for treating patients.\u201d In addition to providing adult stem-cell lines, Invitrogen is also actively researching engineered stem cells. \u201cWe have been doing a lot of research to convert stem cells themselves into tools,\u201d says Goswami. He says that these engineered cell lines act as single-cell reporters providing a visual readout from a live stem cell as it differentiates to separate lineages. But even as certain adult stem cells become more readily available with well-validated approaches to culturing and differentiation, there is still much work to be done. \u201cMany adult stem-cell populations remain difficult to propagate and expand  ex vivo ,\u201d says Pera. And with the exception of mesenchymal stem cells, he says, this has not changed dramatically in recent years. Although Goswami agrees, he also points to another potential issue. \u201cIt is not that adult stem cells have not been isolated, the bigger issue is how do you get well characterized stem cells?\u201d He notes that in some instances researchers will call cells a particular type of stem cell, but the next batch will have different marker profiles, indicating a different cell type. \n               Mouseless developments \n             The culturing of embryonic stem cells has often relied on the use of mouse embryonic fibroblast cells (MEFs) as 'feeder' cells in different media formulations containing serum. It is thought that the MEFs secrete factors that promote the growth of embryonic stem cells in culture. But growing embryonic stem cells using MEFs in undefined media and serum that potentially contains animal products could limit the potential of stem-cell lines for therapeutic applications. \u201cIt all comes down to how the regulators look at these lines and how they have been handled over the years,\u201d says Bruce Davidson, chief scientific officer at ESI. It is for this reason that ESI worked to derive clinical-grade human embryonic stem cells. \u201cWe have four GMP- [good manufacturing practice] certified embryonic stem-cell lines that were developed over the past couple of years and are now ready for distribution,\u201d says Davidson. Cellartis also provides a human embryonic stem-cell line that was derived without any contact with animal products. And many more researchers and companies are trying to identify the important factors for culturing embryonic stem cells and develop robust feeder-free approaches in well-defined media.  The WiCell Research Institute in Madison, Wisconsin, has a major focus on optimizing embryonic stem-cell culture media and conditions to allow long-term culture of stem cells that do not differentiate. In recent years, researchers at WiCell have found that small changes to either media or the physiochemical conditions of the culture can have a profound effect on the viability of cells. But in 2006, WiCell researchers reported culturing embryonic stem cells without using MEFs, instead relying on protein components that were either recombinantly derived or purified from human materials 3 . That medium \u2014 mTeSR-1 \u2014 is now marketed by StemCell Technologies in Vancouver, Canada, who is collaborating with BD Biosciences in San Jose, California, and WiCell on feeder-free cell culture environments for human embryonic stem-cell culture using BD Biosciences Matrigel scaffold. \u201cColonies formed on Matrigel hESC-qualified matrix with mTeSR-1 media tend to be spread out and form a monolayer-like morphology, which makes passage easier and transfection more efficient,\u201d says Marshall Kosovsky, technical support manager at BD Biosciences. Unlike three-dimensional growth matrices, where cell\u2013cell interaction can promote differentiation, Matrigel is coated as a two-dimensional scaffold and is therefore effective for long-term embryonic stem-cell culture. Although Matrigel is a solubilized preparation extracted from a mouse sarcoma \u2014 potentially raising similar issues to the use of serum \u2014 Kosovsky says that they are currently working on more defined formulations of the scaffold.  Invitrogen is one of several companies moving away from culture conditions that rely on either feeder-cells or serum. \u201cWe are providing stem-cell media with a twist \u2014 it is a serum-free and defined media,\u201d says Goswami. This medium, called StemPro hESC SFM, works with 16 embryonic stem-cell lines from around the world. Millipore also offers a serum-free medium called HEScGRO, which works with a variety of embryonic stem-cell lines. \n               Different paths \n             \u201cThe field is moving towards defined media without animal components and without feeders,\u201d says Pera. But some scientists are being cautious about these culture systems. Although signs are encouraging, Pera says that the jury is still out on most of these new systems. And the NSCB is taking a careful approach to feeder-free cultures. \u201cWe made the decision not to use some of the feeder-free methods that are coming out right now, but to stick at this stage to the standard MEF-based methods,\u201d says Hei. The decision was made in part to avoid forcing the research community towards feeder-free methods while they are still in the early stages of development. But Hei also acknowledges that at a later date they might develop banks of cells using feeder-free systems. Although some express concerns that cells cultured in feeder-free systems do not thrive as well, Goswami is confident in these new systems and says that the time has come for feeder-free, serum-free embryonic stem-cell culture. \u201cUsing our media we can get equivalent or higher numbers of cells\u201d compared with any feeder-dependent system, he says. \u201cWe are still learning a lot about what it takes to coax a stem cell to become a particular tissue,\u201d says George Daley from Harvard Medical School in Boston, Massachusetts. Differentiation of embryonic-stem cells or adult stem cells into particular lineages is an area of intense research for developmental biologists and researchers interested in using stem calls for therapeutic applications. \u201cThis is where the battlefront is,\u201d says Lanza, \u201cWe need to learn how to generate different cell types.\u201d He points to haematopoetic stem cells, which give rise to all blood cell types, as an example of how difficult the task can be. In an embryo these cells start out in the yolk sac, then travel to the aortic arch, to the liver and then to the bone marrow, and at each point along that journey are being 'educated' by their surroundings. Replicating this is not easy. Advanced Cell Technology has been working on the differentiation potential of the haemangioblast \u2014 a precursor to hematopoietic cells \u2014 that they isolated. \u201cPeople have got other haemangioblast-like cell types, but these are KDR-negative and CD31-negative, so they are an earlier progenitor cell than what's been described previously, which is why they expand and do things so much better,\u201d says Lanza. Differentiating these haemangioblasts has allowed the company's researchers to generate entire tubes of red blood cells, as well as other hematopoietic lineages and endothelial cells at very high efficiencies. Although success has also been reported in using differentiation protocols for deriving neural progenitor cells, cardiomyocytes and retinal pigment epithelium from embryonic stem cells, in most cases studies have shown that only a very small percentage of cells differentiate into a particular tissue type. Some researchers now think that multiple cells or interactions between cells and surfaces might be required to obtain a desired derivative from stem cells (see  'Beyond the flat world' ). But Pera sees this as one area that is steadily progressing. \u201cI think we are getting more and better differentiation protocols that are more defined and have better endpoints,\u201d he says. However, he does think that there is lack of comparative data on different cell lines and their abilities to differentiate using the current protocols \u2014 although the available data would indicate that there will be differences between certain cell lines in the ability to perform in specific differentiation protocols. \n               Looking up \n             The world might soon be slightly easier for researchers working on mesenchymal stem-cell differentiation. Both Thermo Fisher Scientific and Invitrogen are introducing kits in the coming months for the differentiation of mesenchymal stem cells to adipogenic (fat-producing), osteogenic (bone-producing) and chondrogenic (cartilage-producing) lineages. These kits provide all the necessary growth factors to direct the differentiation of the mesenchymal stem cells. Millipore also offers differentiation media and kits for neural stem-cell lines and mesenchymal stem cells including a human neuronal-differentiation kit and adipogenesis and osteogenesis kits. Although these kits are available for mesenchymal stem cells, the percentages of differentiated mesenchymal cells can still be low for certain lineages. \n               Stem cells hitting the clinic? \n             Even as culture and differentiation research progresses, the race is starting to bring embryonic stem-cell therapies to the clinic. Novocell of San Diego, California, is searching for a diabetes treatment, exploring the potential of using stem cells for treatment. \u201cUsing cadaveric or even fetal cells was a real challenge because you do not have an unlimited source of these cells,\u201d says Alan Lewis, chief executive of Novocell. \u201cWhile developing a delivery system, in the background we were working with embryonic stem cells to derive insulin-producing cells,\u201d he says. And that work is starting to pay off because Novocell can derive definitive endoderm from embryonic stem cells and then differentiate these to insulin-producing cells. Novocell is now working to define these insulin-producing cells, while further refining its polyethylene-glycol or PEG-based delivery vehicle that could help to avoid immune-response issues in patients.  As companies move their products closer to clinical trials, the questions on everyone's mind are when and under what conditions will the US Food and Drug Administration (FDA) fire the starter's gun? Advanced Cell Technology and Geron in Menlo Park, California, are currently in discussions with the FDA regarding clinical trials using embryonic stem cell-based therapies for the coming year. Advanced Cell Technology has developed an application to use embryonic stem-cell-derived retinal pigment epithelium to treat various retinal degenerative diseases such as macular degeneration and retinitis pigmentosa. The company started with this application because in addition to being able to generate large numbers of the required cells, the eye is an immune privilege site, lessening the possibility of an immune response to the cells. Geron has developed a stem cell-based treatment for acute spinal-cord injury. For both of these applications, the companies have utilized stem cells grown with mouse feeder cells. For Advanced Cell Technology this decision was based on how well the stem cells thrived in culture. \u201cIt turns out that although you can derive cells on human feeders or even feeder-free with extracellular matrices, they do not thrive as well,\u201d says Lanza. And for this reason he thinks that in the long term, when generating large batches of cells for clinical applications, the mouse feeders are currently the optimal method. It is not yet clear what the first therapy based on embryonic stem cells to reach clinical trials will be, whether it will be using cells raised on MEFs or feeder-free media. And for some researchers it is not even clear if embryonic stem cells themselves will be the first pluripotent cells first to reach the clinics (see  'A new path to pluripotency' ). George Daley says that it is too early to tell for sure the way stem cell-based therapies will enter the clinics, but he will be watching the developments closely. \u201cI think that it is going to be an exciting time to wait and see,\u201d he says. Reprints and Permissions"}
]