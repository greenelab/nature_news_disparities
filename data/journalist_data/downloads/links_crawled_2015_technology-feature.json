[
{"file_id": "518439a", "url": "https://www.nature.com/articles/518439a", "year": 2015, "authors": [{"name": "Jim Kling"}], "parsed_as_year": "2006_or_before", "body": "Cutting-edge tools that can identify the characteristics of cells are helping researchers to develop more-effective vaccines. Vaccines are a triumph of science over infection. They have defeated smallpox, which the World Health Organization declared eradicated in 1980, and dramatically lowered the toll of many other infectious diseases. But not all. The search for vaccines against conditions such as HIV/AIDS and malaria has been hindered by researchers' incomplete understanding of the human immune system, says Mario Roederer, an immunologist at the US National Institutes of Health's Vaccine Research Center in Bethesda, Maryland. In general, says Roederer, researchers know that a successful vaccine jump-starts antibody production and other defences. But they do not know which of the immune system's thousand or more functional cell types direct the response against individual pathogens. If researchers could identify those cell types, he says, then they could devise vaccines that maximize production of these cells. One of the best ways to identify those cells is flow cytometry, a technique that analyses and sorts cells according to their distinguishing characteristics \u2014 usually proteins on the outer surface \u2014 and reveals much about a cell's function and position in the immune system. But current-generation flow cytometry is detailed enough to place cells only in broad categories \u2014 like identifying something simply as a fish instead of as a great white shark. Researchers know that there is a specific predator that they want, says Roederer. \u201cWe're just trying to figure out how to find it.\u201d The ability to identify specific cells may also improve researchers' understanding of diseases such as multiple sclerosis, in which the immune system attacks the host's own tissues, and metastatic cancer, in which rogue cells from the original tumour migrate into other tissues. Such possibilities have motivated researchers to develop two new approaches, each of which promises to double or even triple the limit of conventional techniques by 2016 (see 'Colour bursts'). One approach is a variant of standard flow cytometry that uses a new type of very intense fluorescent dye and can identify 27 proteins. The other, known as mass cytometry, can record 50 parameters such as cell-surface proteins or parts of proteins, says immunologist Garry Nolan at Stanford University in California. It labels the proteins with metal atoms, then records the weight of the atoms within each cell with a mass spectrometer to provide a signature for each cell type. The labels can also help in microscope imaging of tissues. For instance, they show where cell-surface proteins are located in a slice of excised tumour. That adds layers of information about the cell types present in the tumour and hints at their function within the cancerous growth. \n               Widen the net \n             Like the immune system, flow cytometry uses antibodies to seek out proteins (see 'Go with the flow'). First, researchers create an antibody for each protein they want to study, then they label the antibody with a dye molecule that can absorb light and be made to fluoresce in a specific colour. The sample to be studied is then bathed in the labelled antibodies, which stick to the cells that bear the matching proteins. The antibody-decorated cells are then directed one at a time through a narrow channel. As they pass through, a light pulse triggers the dyes to flash, revealing which proteins are present on the cells. The light from each dye fans out across different wavelengths to produce a readout that looks like a broad mountain peak. At the moment, flow cytometers can process no more than 18 fluorescent dyes at once because when more than this number fluoresce simultaneously, the shoulders of some spectra will overlap with the crests of others, making the crests impossible to discern. A complicating factor is that the abundance of each protein can vary greatly from cell to cell. A cell that has thousands of a specific protein on its surface will attract many identically labelled antibodies, which combine to produce a bright flash of fluorescence. A rare protein will produce a weaker signal that may get overwhelmed by those from more abundant proteins. Researchers can compensate by using brighter dyes to label antibodies targeted at the less common proteins, says Roederer. But they often do not know the relative abundances of proteins in advance, he says, so they may have to spend weeks relying on trial and error to work out which dyes to use on which antibodies. That could well change soon, because researchers have developed a new class of dye 1 . Made from electrically conducting plastic polymers, these dyes act as miniature antennas that absorb energy from the light pulse at multiple points that all fluoresce together, producing a more intense signal. When used in flow cytometry, says Roederer, the intense signal overwhelms any overlap from other dyes, even for low-abundance proteins, and means that more dyes can be used simultaneously. \u201cYou have so much more flexibility,\u201d says Roederer, whose group has used the dyes to survey 30 proteins commonly found on the surfaces of immune cells, although it has yet to publish the results. Other research teams are using these dyes, which are available from two Californian firms \u2014 BD Biosciences in San Jose (where they are marketed as BD Horizon Brilliant dyes), and BioLegend of San Diego (marketed as Brilliant Violet dyes) \u2014 but Roederer knows of no one else who has managed to measure more than 18 parameters simultaneously. Measuring more than that requires specialized instruments, software and chemical expertise. \u201cIt's not trivial\u201d to do, he says. \u201cI've always been lucky to be surrounded by the necessary technologies.\u201d Roederer now plans to use the dyes to study vaccine candidates against several diseases, including Ebola, malaria, tuberculosis and HIV/AIDS. An Ebola vaccine currently being tested in human volunteers is a priority. Roederer joined a team led by immunologist Nancy Sullivan, also at the Vaccine Research Center, that narrowed down the cells that protect monkeys from Ebola \u2014 not quite to shark level, but to about that of a predatory fish 2 . With more dyes, Roederer expects to find his shark and pinpoint the cell populations that give the monkeys immunity. Then Sullivan and Roederer hope to tune the vaccine doses and schedules to get the human immune system to produce and support those cells. \n               Metals, not colours \n             As Roederer forges ahead with the new dyes, DVS Sciences of Sunnyvale, California, is taking flow cytometry in a different direction. The company, now called Fluidigm, introduced the first commercial mass cytometer in 2009 and debuted its next-generation machine, the CyTOF 2, in 2013. Like flow cytometry, mass cytometry involves soaking cells in labelled antibodies then squeezing them into a narrow flow to be screened one by one. But that is where the similarity between the two techniques ends. In place of fluorescent dyes, mass cytometry uses rare earth metals, which are absent from living systems, to label the proteins. And rather than illuminating the cells to reveal the presence of proteins, the mass cytometer uses high-temperature plasma to split the cells into their component atoms. These atoms, which now include the rare-earth labels, then get fed into a mass spectrometer to measure the mass and abundance of each metal, and thus the identity and abundance of each matched protein. There are therefore no problems with signal overlap as there are in fluorescent dyes. A group led by Nolan used the technique to look for specific immune-cell populations in patients recovering from hip-replacement surgery \u2014 which, like most traumas, prompts a complex response from the immune system to orchestrate healing. Nolan reasoned that he could identify the immune cells that promote faster recovery in some patients. His team collected blood from each patient before their surgery and at various times afterward, and tracked 31 proteins using mass cytometry. In patients who recovered quickly, the researchers found unique types of immune cells known as monocytes 3 . \u201cWe were able to see signatures that were changing and that lined up really well with surgical recovery,\u201d says Sean Bendall, a pathologist at Stanford, and a co-author of the study. Researchers are now investigating whether it is possible to use these cells to predict which patients are likely to have delays in recovery, and provide interventions that could improve their recovery time, says Bendall. Mass cytometry and fluorescence flow cytometry compete for some of the same research applications, and researchers tend to have a preferred technique. Roederer points out that mass cytometry destroys cells, whereas fluorescence flow cytometry can preserve them, and can even isolate and sort them at the same time. Bendall does not share that concern. He enthusiastically backs mass cytometry, but says that he often hears objections about the destruction of cells. \u201cI feel like that's the 'gotcha' question,\u201d he says. Many people will dismiss the technique simply because of that, he explains, but cell destruction \u201chas never been a roadblock to anything we wanted to do\u201d. If a mass-cytometry experiment identifies an interesting cell type, he explains, then researchers can always use that information to isolate living counterparts with a traditional flow cytometer. Fluorescence-based flow cytometry does, however, have the advantage of being faster: it can analyse up to 50,000 cells per second, whereas mass cytometry manages no more than 1,000 per second because the cloud of atoms produced by the plasma takes time to clear before the instrument can accommodate the next cell. The intense fluorescent dyes are not without their drawbacks. They are so new that little road-testing has been done using high numbers simultaneously \u2014 and few papers have been published on them. That leaves mass-cyotometry advocate Nolan a little sceptical of the technique. \u201cI've yet to see them implemented in a way that would have me jump ship and go back to fluorescence,\u201d he says. \n               Pictures perfect \n             Nolan, Bendall and Roederer all agree on one thing: the most promising application of metal tags is their potential to improve images of intact slices of tissue \u2014 and that cannot be done with either fluorescence flow cytometry or mass cytometry because both techniques require cells to be dispersed into a flowing stream. One such application, called multiplexed ion-beam imaging, applies metal tags to tissue slices, then showers them with oxygen ions. The oxygen ions react with the metal tags, dislodging them from their accompanying antibodies. A mass spectrometer then measures the metal atoms as they ricochet away from the tissue. Michael Angelo, a pathologist at Stanford, says he has pushed the technique to 45 parameters per cell. The method also records the cell's position within, say, a tumour. \u201cI think it's extraordinarily cool. That will never be done by traditional imaging or flow-cytometry-based technologies,\u201d Roederer says. Last year, Nolan's group used the technique in tissue samples from people with breast cancer using ten metal-labelled antibodies. The technique produced a high-definition picture of the tissue that could then be displayed in different colours to reveal the location of individual proteins within the section 4 . In a related approach, immunologist Bernd Bodenmiller at the University of Zurich in Switzerland and chemist Detlef G\u00fcnther at the Swiss Federal Institute of Technology in Zurich and their colleagues have developed a companion instrument. It records the position of the cells and directs an ultraviolet laser at the tissue to methodically strip away labelled cells and send them to a mass cytometer for analysis 5 . The instrument can analyse up to 40 tags, Bodenmiller says. However, there are now more metal tags available, so researchers will probably soon conduct experiments with even more parameters, says Scott Tanner, Fluidigm's chief technical officer. Images produced by this technique could illuminate how cells respond to local conditions within a tumour, such as low-oxygen environments. \u201cIt gives you yet more information about the nature of that sample,\u201d says Tanner. Fluidigm has licensed the laser ablation chamber and given prototypes to several academic research teams. It hopes to begin selling the device this year. Imaging mass cytometry could be applied beyond cancer, Tanner says. Neurobiologists tell him that they hope to use it to examine how neurons are distributed in the brain or spinal cord. The ability to analyse multiple proteins could help researchers to decipher the functions of neural cells and how they relate to the cell's location within a network of linked neurons. Roederer and Nolan are constantly pushing the boundaries, but Roederer says that he often encounters scepticism of how useful the improvements are. When he achieved 8 parameters, researchers questioned if that many were really necessary. \u201cWhen I got to 12, people were saying 'Is that enough? Are you done yet?',\u201d he recalls. He is not done. He hopes to get to 40 parameters next year, and even more after that. \u201cEvery time we reached a new ceiling, we were looking to crack it within a couple of years.\u201d \n                     Single-cell technologies for monitoring immune systems \n                   \n                     Multiplexed ion beam imaging of human breast tumors \n                   \n                     Webcast of a closer look at cell death \n                   \n                     Flow cytometry network \n                   Reprints and Permissions"},
{"file_id": "526459a", "url": "https://www.nature.com/articles/526459a", "year": 2015, "authors": [{"name": "Michael Eisenstein"}], "parsed_as_year": "2006_or_before", "body": "Nanoscopes capable of super-resolution offer scientists intricate views of a world beyond the limits of conventional microscopes \u2014 but not every technique fits all imaging needs. A variety of innovative \u2014 even Nobel-prizewinning \u2014 approaches to fluorescence microscopy are opening a window for biologists to marvel at structures and processes at the molecular level. Using 'super-resolution' microscopy, life scientists can witness events such as vesicles shuttling loads through the cell membrane or proteins clumping together in Alzheimer's disease \u2014 processes that were previously invisible to the human eye. But scientists who contemplate replicating such dramatic images in their own labs need to anticipate the technical challenges. \u201cA lot of people look at papers that expert labs put out, and everything is gorgeous,\u201d says Christine Labno, technical director at the University of Chicago's Light Microscopy Core Facility in Illinois. Careful thought must go into selecting the right technique for the scientific question at hand. \u201cThere's quite a bit of education that we have to do about how these techniques actually work.\u201d The good news is that universities and institutes have started to incorporate super-resolution microscopy, or nanoscopy, into shared imaging facilities. And with highly experienced researchers available to offer their wisdom, life scientists have been jumping in. \u201cOur earliest adopters were in neurobiology, looking at synapses that are only about 30 nanometers across,\u201d says Labno. \u201cBut we've also had immunologists and cytoskeleton researchers eager to start \u2014 so it's a pretty diverse bunch.\u201d Super-resolution microscopy tore down a natural wall that, for more than a century, was thought to be impossible to climb. In 1873, German physicist Ernst Abbe described a 'diffraction barrier'. He predicted that because of the fundamental properties of light, optical microscopes would not be able to discriminate individual features separated by fewer than 200 nanometres. In practice, when two details that are closer together light up at the same time, the sample would appear as a blur. Then came pioneering work in the late 1990s by scientists who would go on to share the 2014 Nobel Prize in Chemistry for devising strategies that broke Abbe's diffraction barrier: Stefan Hell of the Max Planck Institute for Biophysical Chemistry in G\u00f6ttingen, Germany; Eric Betzig at the Janelia Research Campus in Ashburn, Virginia; and William Moerner of Stanford University in California. \n               Microscope match-up \n             Today, researchers have a suite of super-resolution technologies at their disposal, all with the power to observe molecular-scale details well below the Abbe limit. Neurophysiologist Silvio Rizzoli at the G\u00f6ttingen Graduate School for Neuroscience, Biophysics and Molecular Biosciences uses a technique called stimulated emission depletion (STED), invented by Hell and his colleagues 1 , to study synaptic-vesicle function in nerve terminals. \u201cThe best resolution that we get routinely with STED is around 30 nanometres,\u201d he says. STED is relatively simple for experienced fluorescence-microscope users. The method is based on the same principles as a standard confocal instrument, but instead of illuminating the sample with a single light source, it uses two. One beam is set at a wavelength that excites the fluorophores \u2014 the fluorescent tags \u2014 that are used by researchers to localize and visualize proteins; the other uses a different wavelength that suppresses fluorescence. This beam is doughnut-shaped and overlaps with the first beam, so that only molecules in the central 'doughnut hole' continue to fluoresce. Obtaining a STED super-resolved image is not very complicated. \u201cYou have to play a bit with the parameters to see something nice, but otherwise, you look at it the same way as you would with a confocal,\u201d says Jochen Sieber, product manager for super-resolution technologies at Leica Microsystems in Wetzlar, Germany, which manufactures microscopes. Other super-resolution methods rely on the ability to switch fluorescent labels 'on' and 'off' in a controlled fashion. These 'probe-based' (also known as 'localization-based') techniques carefully tune lighting conditions to ensure that only a few, sparsely distributed individual fluorophores are visible at any given time. The best known of these methods are photoactivated localization microscopy (PALM), developed by Betzig and his colleagues 2 , and stochastic optical reconstruction microscopy (STORM), devised by Xiaowei Zhuang's group 3  at Harvard University in Cambridge, Massachusetts. All fluorescent labels start in a dark state. They are then excited using a controlled pulse of laser light that switches on a tiny fraction of the tags, followed by a second pulse that switches them off again. The process is repeated over and over to generate a series of partial fluorescence images that can be reconstructed into a whole. With these techniques, cell biologists can achieve remarkable spatial resolution in fixed samples, down to single-molecule imaging. \u201cI trust these approaches for anything below 20-nanometre resolution,\u201d says Rizzoli. But interpreting images at this scale requires a careful labelling strategy to avoid introducing artefacts \u2014 inaccurate imaging data that arise from sample staining or processing methods and distort the true structure of the specimen (see  'The antibody problem' ). On the up side, adding PALM or STORM capabilities onto an existing fluorescence microscope is relatively straightforward. Life scientists can also purchase commercial instruments that are designed for probe-based super-resolution, such as the ELYRA from Carl Zeiss Microscopy in Jena, Germany, and the N-STORM from Nikon Instruments in Melville, New York. Amateur users should prepare to invest time in their experimental design (see  'A good way to dye' ). \u201cWhenever we start a new project, we spend a lot of effort on optimizing labelling,\u201d Zhuang says. \u201cI think ironing this part out takes researchers the most time.\u201d \n               Nano Youtube \n             Most super-resolution imaging concentrates on fixed cells, but the great promise of nanoscopy is the ability to image dynamic processes in living cells. Cell biologists want to capture molecules and structures as they assemble, adhere and interact. But collecting super-resolution images in real time has a few trade-offs. In probe-based techniques, for example, briefer illumination times mean that fewer fluorophores become activated each round \u2014 resulting in an image with much-reduced detail. One way around a weak signal is to use stronger illumination, although pumping too much light into cells can create toxic compounds that jeopardize the sample's viability. Some researchers are finding that structured-illumination microscopy (SIM), a technique pioneered 4  by Mats Gustafsson at Janelia Farm, offers a good compromise for live-cell super-resolution imaging. \u201cIn a good SIM experiment, we're probably down to 100\u2013120-nanometre lateral resolution,\u201d says biochemist Jordan Raff at the University of Oxford, UK, who uses SIM to study the structures that help to coordinate cell division. SIM illuminates the sample with patterned lines of light, which generates fluorescent images that override Abbe's constraints. The method requires little sample preparation and is flexible in terms of fluorophore selection. Like PALM and STORM, SIM is easy to build onto an existing instrument, or users can opt for commercial systems such as the DeltaVision OMX SR from GE Healthcare Life Sciences. Daniel Davis, an immunologist at the University of Manchester, UK, has found SIM useful for studying how secretory granules move along actin in natural killer cells engaging in an immune response. But Davis also points out that \u201cit doesn't quite get you down to the resolution you can get with other techniques\u201d. STED microscopes can also be adapted for live-cell imaging, despite their reputation for causing severe damage with the intense beams needed for high-resolution pictures. To reduce bleaching, Hell and his team devised a light-limiting technique called RESCue 5 , which is available using the STED microscopes made by Abberior Instruments, a company co-founded by Hell and based in G\u00f6ttingen, Germany. \u201cIn some samples, it can reduce light load down to 4% or 5% of traditional STED.\u201d Even PALM and STORM systems, which require comparatively time-consuming collection and reconstruction of multiple images that are produced from individual molecular labels, have ramped up their speeds dramatically to capture data from live cells. Scientists have shrunk the time needed for image collection by using brighter fluorescent labels, and a new generation of detectors can harvest image data from larger numbers of pixels more rapidly than was possible with early methods. Optical biophysicist Joerg Bewersdorf and his group at Yale University in New Haven, Connecticut, found 6  that combining such detectors with robust image-analysis enabled them to record the movement of proteins on the surface of living cells at up to 32 frames per second \u2014 essentially producing super-resolution videos. Leica offers a probe-based approach that is founded on the principle of ground-state depletion (GSD). This method uses light to force all but a handful of individual fluorophores into an inactive dark state. Live-cell imaging, however, is out of the question. \u201cBuilding up enough fluorescence events to get a nice-looking image can take 15\u201320 minutes,\u201d says Labno. This is much too long to track a dynamic process. \n               Life in 3D \n             Super-resolution microscopy can also satisfy researchers who crave 3D images. Leica's STED instrument uses two depletion beams, one perpendicular to the other, to generate a 3D-super-resolved zone in the specimen. Alternatively, Abberior's microscopes use a device called a spatial light modulator to achieve an equivalent effect with a single beam. For probe-based methods, introducing depth measurements is straightforward. Zhuang's group found 7  that adding a cylindrical lens to the light path transforms STORM's light spots into elliptical shapes that can be mapped in 3D. This approach, used in Nikon's N-STORM, can yield a depth ('axial') resolution of 50 nanometres without altering lateral resolution. Zhuang's team has achieved still further improvements in axial resolution with an iteration of STORM that resolves all three dimensions at 10 nanometres 8 . Imaging success depends on more than just the instruments: the sample itself is also an important consideration. Tissue specimens are especially hard to image because they are dense and tend to scatter photons, generating blurry images and high levels of background fluorescence. As a result, image quality is best near the sample's surface and worsens as the microscope probes deeper into thick samples. It may be possible to overcome this hurdle by using chemical 'clearing' techniques that render tissues transparent. For now, most researchers find that the simplest solution is to embed fixed samples in plastic, and then sequentially image thin slices shaved off the top. \u201cWe're trying to understand the relationship of one synapse with different postsynaptic partners, which requires us to look at thousands of synapses in tissue at high-resolution in parallel,\u201d says Bernardo Sabatini, a neurobiologist at Harvard Medical School. \u201cI think that in the short term, this approach plus super-resolution will give you that data quickly.\u201d \n               Picture perfect \n             Even a perfectly executed super-resolution study generally needs some sort of computational processing to produce a high-quality image. For scientists who prefer the simplicity of positioning a sample under a microscope and having the image instantly appear on a computer screen, STED might be best because it generally does not require image processing. Some scientists use deconvolution tools to sharpen images and eliminate blur, but Hell avoids this whenever possible. \u201cRaw data may not look as fancy, but it's honest, and you know what it means,\u201d he says. \u201cFor most other techniques, software processing is mandatory.\u201d And Davis says of SIM, \u201cYou're creating a mathematical model of what the cell looks like based on the fluorescence data. You're not literally seeing it.\u201d Raff notes that many of his early experiences with SIM entailed recognizing that pretty pictures can be deceiving because image-processing algorithms can create artefacts that look every bit as real as the cellular structure of interest. \u201cBut if you have people who know what to look for, they can examine the image and tell if something is dodgy,\u201d he says. For PALM and STORM, image-building is like a game of 'join the dots'. The higher the density of the labels, the easier it is for the software to connect those dots, leading to better images. But high density can also cause confusion, by generating overlapping signals that look like single dots \u2014 so clever use of powerful image-processing algorithms is essential to make sense of the data. Given that most super-resolution techniques can be incorporated into existing microscopes, many researchers will probably try their hand at super-resolution imaging in the near future. \u201cIn my view, it doesn't make sense for a facility that routinely uses confocal microscopy not to have STED attached to it,\u201d says Hell. \u201cYou can just stop the STED beam and still have a confocal system.\u201d Those with experience in nanoscopy are helping to train others. Zhuang's team at Harvard University, for example, offers routine STORM workshops. \u201cWe go from sample preparation to analysing images with our software,\u201d she says. \u201cIt's always oversubscribed.\u201d That said, most biologists are still best served by using these instruments in core facilities that provide access to specialists who are familiar with several methods. \u201cAs biologists, we're still far away from understanding the physics \u2014 and some of us never will,\u201d says Raff. \u201cYour best bet is to try multiple different techniques out on your sample in an environment where there are people around who understand it.\u201d \n                     Microscopy: Bright light, better labels \n                   \n                     Nobel for microscopy that reveals inner world of cells \n                   \n                     Through the nanoscope: A Nobel Prize gallery \n                   \n                     Is super-resolution microscopy right for you? \n                   \n                     A collection of articles on super-resolution microscopy \n                   Reprints and Permissions"},
{"file_id": "522373a", "url": "https://www.nature.com/articles/522373a", "year": 2015, "authors": [{"name": "Vivien Marx"}], "parsed_as_year": "2006_or_before", "body": "The body's organs are more complex than any factory. Attempts to mirror their physiology in the laboratory are getting closer to capturing their intricacies.  In their quest to create organs in the laboratory, researchers have come a long way. Engineered tissues are already used in medical research and have even entered clinical trials. But they are much simpler than the real thing. To make a stomach, a lab might use 3D printing to create a mould that could be seeded with the appropriate cells. But without cues provided by blood flow and interactions with other tissues, the result would be simply a stomach-shaped statue, unable to digest or growl. An organ is much more than a mass of cells arranged in a particular configuration: it also has support scaffolds, blood vessels to deliver nutrients and signal molecules, and a hierarchy of intricate control functions that can respond to internal and external cues. All this makes it tough to build a functional, physiologically relevant organ in the lab, says Rosemarie Hunziker at the US National Institutes of Health, who manages the funding of programmes devoted to designing and building artificial organ systems. But tissue engineers are making inroads into the problem. To try to tackle the biological complexity of organs, they can choose from various fabrication approaches. One method is to place cells into elaborate, but still simplified models of an organ the size of a microscope slide, which can then be connected together to probe how organs interact. These miniature 'organs-on-chips' provide a unique vantage into organ function and disease, and for applications such as toxicity tests of drug candidates. An alternative approach is to foster the ability of cells to self-assemble, in the hope that they will recapitulate actual organ development and reveal insights into the process. Whatever the strategy, researchers can start with biologically simple approaches, and then add complexity to the model a little at a time. Just how similar an artificial version of an organ needs to be to its original depends on the questions that are being asked of it, Hunziker says. Artificial organs may look very different from their  in vivo  counterparts but nonetheless be useful for drug testing and basic research. Whether the goal is to understand an organ or to replace it, the eventual aim is an engineered system that functions as reliably as the real thing, Hunziker adds. Researchers across the world are using these systems to address a wealth of important questions. They can, for example, help to reveal how cancer cells detach from a tumour to invade other tissues, and allow scientists to recapitulate processes in disease and development, such as what might go awry in neurodevelopmental disorders. \n               Systems thinking \n             The most highly engineered organ models are the organs-on-chips that look the least like organs in the body. They are made using similar manufacturing techniques to those for silicon microchips in computers. First, a photosensitive material is layered onto silicon, and ultraviolet light is used to etch grooves in a desired pattern into silicone rubber. This guides the production of a 3D network of hollow tubes inside a rubbery rectangle the size of a computer memory stick. The tubes are seeded with cells of the desired types and hooked up to pumps and an external fluid source, providing inlets and outlets through which scientists can mimic blood flow and deliver nutrients and environmental signals. Perfusion by continuously flowing liquid mirrors the dynamic environment in organs. The set-up also lets bioengineers modulate a tissue's stiffness as well as mechanical, chemical and electrical cues to reproduce the signals that cells might receive in healthy or diseased states, says John Wikswo of Vanderbilt University in Nashville, Tennessee (see 'Hooked up'). Researchers can replicate inflammation, for example, by adding the molecular messengers known as cytokines and even living immune cells into the chips' channels \u2014 they then watch the inflammatory response that is characteristic of most tissues when damaged or infected 1 . The chips are usually transparent to allow high-resolution, real-time imaging of cells, says Donald Ingber director of Harvard University's Wyss Institute for Biologically Inspired Engineering in Boston, Massachusetts. The liver, kidney, lung, intestine, fat, muscle and the blood\u2013brain barrier have all been rendered into chip form 2 . Now researchers are combining chips into multi-organ systems that can replicate some of the body's physiology. Gordana Vunjak-Novakovic and her team at Columbia University in New York City are building a model of the heart\u2013liver\u2013blood system with which to probe drug toxicity and disease. Wikswo at Vanderbilt, and his colleagues at the University of Pittsburgh, Pennsylvania, are linking organ chips together to predict the effects of potentially toxic chemicals and drugs. He believes that a liver\u2013kidney model could identify safety problems before a drug reaches testing in humans, because these are the organs in which toxicity first becomes apparent. To emulate  in vivo  situations of health or disease, researchers can grow the appropriate cells in 3D support structures and explore their reactions to cues delivered into the system, says Wikswo. The key to successful mimicry is attention to microstructure, says Hunziker. Careful placement of liver cells across a chip can better replicate real liver tissue, which has different zones close to and away from the main blood supply. These zones differ in the genes that are active, which results in differences in cell development and behaviour, and different responses to chemical stresses, she says.  Microscale organ systems allow experiments that cannot be done in cell cultures, animals or people, says Ingber. Organ chips lined with cells from individual patients enable the assessment of physiological differences between health and disease, and between people, in more detail and over a longer period than would be practical in people or in animal models, he says. Ingber's team has kept multi-organ chips going for more than a month. With an organ chip, it is also possible to adjust parameters to see what happens in a way that is not possible in a patient. Several labs have formed spin-out companies to commercialize their model tissues. Emulate, in Cambridge, Massachusetts, founded by Ingber, is developing organ-on-chip systems for high-throughput drug screening and toxicity testing. The company Hepregen in Medford, Massachusetts, co-founded by bioengineer Sangeeta Bhatia at the Massachusetts Institute of Technology in Cambridge, uses the technique of 'micropatterning' to develop liver models in which different cell types are precisely placed to produce a platform that more closely mimics the complexity of the liver. These are being developed as drug-screening assays. HemoShear Therapeutics in Charlottesville, Virginia, founded by two University of Virginia scientists, has developed several organ modelling systems, including one that specifically mimics blood flow in tissues. In January, HemoShear began a collaboration with pharmaceutical company Pfizer of New York to find better ways to predict injuries to blood vessels, such as inflammation, that drug candidates might cause. Right now, microfabrication is out of reach for many labs. However, there are some companies that offer services to make chips for labs that do not have the necessary equipment or expertise. And many universities offer microfabrication capabilities through core service centres. Meanwhile, labs at the cutting edge are working to make engineered chips better homes for living cells. One challenge is seeding cells evenly throughout the devices and maintaining their growth within the tiny channels, says Ingber. Another is that bubbles in the system can injure the cells. \n               Three-dimensional help \n             In contrast to organs on chips, soft scaffolds seeded with cells can result in artificial organs that look much more like the real thing. This approach blends a variety of synthetic materials to make a support system. It is then seeded with cells that grow and develop throughout the scaffold and thus become arranged in the desired configuration. In one well-known example from the early days of the field, Linda Griffith at Massachusetts Institute of Technology and Charles Vacanti at Massachusetts General Hospital in Boston and their colleagues used such a scaffold implanted under the skin of a mouse to guide bovine cartilage-forming cells to grow tissue in the form of a human outer ear 3 , 4 . The polymers in the scaffold degraded as the tissue formed, leaving behind the structure made of cartilage. Today, Griffith and her team use a custom-built 3D printer to create highly intricate tissue scaffolds. A stream of photoreactive polymer spurts out of the instrument's nozzle and one layer at a time is exposed to ultraviolet light to stabilize the structure. Material is removed in an iterative process to a build micrometre-scale substructure. Scientists have also developed ways of mimicking the mechanical stimuli that seem crucial to tissue development. For example, the early development of teeth in a mammalian embryo involves embryonic cells packing closely together. To mimic this process, Ingber's lab has developed a polymer that acts like shrink-wrap at certain temperatures 5 . When the polymer is warmed to body temperature, it shrinks and compacts the cells it encloses, which activates genes responsible for tooth development. Bioengineers could potentially use this material to induce tissue development for a variety of therapies, Ingber says, because cartilage and other internal organs (such as the lungs and kidneys) also undergo cellular compaction as they develop. Incorporating blood flow into a model organ is particularly challenging, especially when trying to mimic the heart, which pumps rhythmically for a lifetime. Nevertheless, tissue engineers are well under way in their search for therapies to help heal injured hearts, and eventually perhaps, to find alternatives to heart transplants. Starting with a cell-sheet technology that does not incorporate a scaffold, Teruo Okano, a biomedical engineer at Tokyo Women's Medical University and his colleagues have made vascularized heart-tissue patches. The experiments start with thin layers of cells, which they can grow from a variety of cell types, including rat neonatal cardiac cells, human muscle cells and induced pluripotent stem (iPS) cells. These sheets are grown in dishes coated with a temperature-sensitive polymer. When the temperature is lowered, researchers can harvest sheets of cells that remain connected to each other without any kind of scaffold, says Okano's colleague Tatsuya Shimizu. In ongoing clinical trials, the team is evaluating 30 patients with heart problems who have received implanted tissue patches made from muscle-cell-derived sheets. These sheets secrete several types of cytokine, which promote blood-vessel formation and inhibit cell death in the patient's heart tissue. In the future, Shimizu and his colleagues hope to transplant tissues with beating cells. But these sheets are not yet optimal. The ideal grafts need to be thick, especially because events such as heart attacks lead to thin heart tissue. The team has returned to the lab to engineer thicker patches that will be infiltrated with even more blood vessels and should remain viable for longer than the previous versions. They have grown cell sheets from human iPS cells and transplanted them into rats just under the skin on their backs, building up a patch 1 millimetre thick made of 30 cell sheets 6 . After implantation, small blood vessels from the rat sprouted through the layers. By moving smaller stacks into more vascularized areas, the researchers were able to cause more and more blood vessels to grow and eventually to connect the stack directly to larger blood vessels, such as the jugular vein. The heart-muscle cells continued to beat during six months of observation. However, similar multiple surgical interventions could not be carried out in people. So the researchers have developed a technique that relies on a gel on which they can grow multiple layers of rat-cell sheets in the lab. One day, Okano and his team hope, it will be possible to engineer such grafts for use in humans with severe heart failure. The general approach could also be applied to engineer tissue to mimic the liver or kidneys. \n               Self-assembly \n             Other teams rely even more heavily on the intrinsic ability of cells to assemble into complex structures. Stem cells grown in suspension can be coaxed to form organized clusters called organoids, and these have been made for diverse tissues, including intestine, kidney and retina. Organoids are usually much smaller than the actual organ, just a few millimetres across, and with a much simpler assortment of cells, but some teams are now making organoids with more cell types and more complex structures, and even attempting to model the most daunting organ \u2014 the brain. In 2013, Madeline Lancaster and Juergen Knoblich at the Institute of Molecular Biotechnology of the Austrian Academy of Sciences in Vienna generated human brain-tissue organoids about the size of a lentil 7 . They started from groups of human pluripotent stem cells, which are differentiated into neural tissue. Part of the protocol is to let the biology unfold. Under the right conditions, the differentiating cells self-organize into a tight swirl of neural tissue with multiple cell types, including radial glial stem cells that give rise to cells in the brain such as neurons. The swirls even include rudimentary brain structures such as the beginning of a forebrain and retina. \u201cWe pretty much recapitulate the formation of neural tissue in a dish, letting it develop as it does in the embryo,\u201d says Knoblich. These cerebral organoids have helped them to address questions that are hard to answer when growing neurons flat on the surface of a culture dish. The team studies the human neurodevelopmental disorder microcephaly, in which infants have markedly small brains. Although mice can be used to model the disorder, the animals do not show the extreme difference in brain size. But when the team reprogrammed skin cells from a patient with microcephaly into iPS cells that developed into cerebral organoids, the resulting structures bore clear characteristics of the disease. In these organoids, the radial glial cells proliferated less and, in some regions, differentiated into neurons prematurely. Even under normal conditions, radial glial cells do not proliferate in developing mice the way they do in humans, and so human organoids are a promising way to study how these neural precursor cells might be involved in the disorder. Lancaster and Knoblich also used organoids to assess the effects of a gene called  CDK5RAP2  that helps to guide cell division. The patient with microcephaly had a mutation in this gene that probably results in an aberrant protein. When the team introduced an undamaged protein into the organoid, some cells developed into types akin to radial glial cells, indicating that the loss of function of this gene contributes to microcephaly 7 . There are still plenty of challenges for organoid technology. Lancaster and Knoblich point out that their organoids lack a blood supply and the interaction that neural tissue normally has with surrounding tissue. Over time, the organoids begin to die and lose resemblance to early brain tissue. The team has managed to keep them alive for as long as a year, but how useful late-stage organoids are for disease modelling remains to be seen, says Knoblich. Another challenge is consistency, because the organoids take on different shapes from one batch to the next, he says. The lab is continuing to tinker with the growth conditions in the hope of overcoming these problems and being able to model more complex neurodevelopmental disorders. \n               Like an embryo \n             Bioengineers intent on building organ models can tap into the complexity of intercellular signals \u2014 the wealth of biochemical and bioelectrical messages that tell cells to differentiate, migrate, change shape or clump together to form an organ. This approach has already been used to regenerate legs on frogs that are normally too old to naturally regrow an amputated limb. That work, led by Michael Levin, a biomedical engineer at Tufts University in Medford, Massachusetts, might translate more readily to humans than many expect: children can regenerate fingertips, but adults cannot.  To get the frogs' legs to regrow, Levin and his team chemically tinkered with the pattern of electrical charge in the limb so that it matched the bioelectrical gradient found in the limbs of young animals 8 , and this induced the cells at the tip of the amputated limb to grow. They also induced the formation of a two-headed flatworm (see 'Organ building'). The alterations the team had made caused a permanent change in the memory of what to form, which was encoded in an electrical circuit just like memories in our brains. Levin describes the process as \u201cmanipulating information\u201d within the tissues in ways that cause predictable, large-scale changes in growth and form. That, he says, \u201cmakes the job of growing anything much easier\u201d. Instead of trying to micromanage organ building, Levin believes in leveraging the body's own processes. He and his team are developing a physiological 'phrase book' of mathematical models and software. Scientists can use these software tools to search for factors to manipulate in their experiments, and so find ways to tell cells what tissues to build. The goal is to link data sets about genes, proteins and signalling pathways to knowledge about how organ shape and function is regulated. \u201cThese are the kinds of tools that will be indispensable as bioengineers confront the complexity barrier facing the creation of even simple organs,\u201d says Levin. Ultimately, the usefulness of the tool is what is important, not the specific approach that is chosen. Engineered tissues are starting to allow incisive experiments and even replacement therapies. And perfectly mirroring nature may not, in all cases, be what is needed. \u201cWhat is critical is that the organ has enough complexity to accomplish its function,\u201d says Hunziker. Whether it be a patch for damaged hearts, a better toxicity test or an insight into a devastating brain disease, tissue engineering delivers what scientists crave: more understanding, and the potential to help people. \n                     Tissue models: A living system on a chip \n                   \n                     Tissue engineering: How to build a heart \n                   \n                     Taking tissue engineering to heart \n                   \n                     Review Article about 3D printing of tissues and organs \u2013 Nature Biotechnology \n                   \n                     Tissue and Cell Engineering Society \n                   Reprints and Permissions"},
{"file_id": "524503a", "url": "https://www.nature.com/articles/524503a", "year": 2015, "authors": [{"name": "Vivien Marx"}], "parsed_as_year": "2006_or_before", "body": "The United Kingdom aims to sequence 100,000 human genomes by 2017. But screening them for disease-causing variants will require innovative software.  Soon after the Californian twins were born, their parents grew concerned: the children were developing slowly and had floppy muscle tone. A brain scan indicated that the boy might have cerebral palsy, but doctors were puzzled over his sister's tremor and seizures. Batteries of tests failed to confirm diagnoses in either child, and treatment when the children were five with the drug  L -dopa \u2014 used for people with Parkinson's disease \u2014 helped only for a while. It was only in 2010, when the twins reached the age of 14, that whole-genome sequencing ended their diagnostic odyssey. It identified a pair of mutations in a gene that encodes the enzyme sepiapterin reductase, which is involved in production of the neurotransmitters dopamine and serotonin. Doctors modified the treatment to include serotonin; the boy's mobility improved, and the girl was no longer plagued by sudden, breath-stealing spasms 1 . Stories such as this one fuel ambitions to diagnose more quickly and accurately using genomic medicine. Indeed, tests that can probe certain disease-associated genes are increasingly becoming a diagnostic option. But such genetic tests often fail to give a diagnosis because they are too focused on a selection of known genes on one section of the genome. In cases like that of the twins, researchers or clinicians must go further and sample a person's whole genetic sequence to find the disease-causing genes. Currently this is done only in rare cases \u2014 but a number of large-scale initiatives are poised to bring whole-genome analysis into routine medical care. The United Kingdom has taken a giant leap into genomic medicine with the 100,000 Genomes Project, which was launched in 2012 and has been personally backed by Prime Minister David Cameron. As part of the \u00a3300-million (US$467-million) initiative, 100,000 genomes from National Health Service (NHS) patients with cancer, rare disorders and infectious diseases will be sequenced by 2017. The project's aims are to gain scientific insight by linking the disorders with precise genetic signatures; to obtain better diagnoses; to tailor treatments to individual patients; and, ultimately, to spur the development of a UK genomics industry. The state-funded, centralized UK health-care system is ideal for such population-based approaches in genomic medicine, says John Bell, who is a medical researcher at the University of Oxford, UK, and is also on the board of Genomics England, the NHS-owned company set up to run the project. The NHS already holds extensive clinical information on individuals, and pairing this with detailed genomic data will enable powerful insights into the links between medicine and genetics. Evidence that whole-genome interpretation can help in a wide range of disorders is mounting 2 , and in the long term, Bell says, the goal is to make whole genomes part of regular NHS health records. But before that vision can be realized, there are several hurdles that the 100,000 Genomes Project must overcome. Aside from the logistical task of extracting and sequencing DNA from thousands of individuals, there is the problem of identifying which genome variations cause disease and which are harmless \u2014 a daunting, data-heavy and time-consuming process that will require a slew of specialized companies with dedicated software. \n               Considerable cohort \n             Iceland was the first to launch a large-scale genomic analysis of its population. Many nations have followed suit with the explicit goal of linking health care and genomics. In the United States, the Precision Medicine Initiative plans to sequence the genomes of one million volunteers, and the Million Veteran Program is gearing up to do likewise with US military veterans. Similar projects are under way in Canada, Australia, Japan, South Korea, Singapore, Thailand, Kuwait, Qatar, Israel, Belgium, Luxembourg and Estonia.  But the 100,000 Genomes Project is the venture gaining the most steam: it has already enrolled 3,500 people with rare diseases and 2,000 individuals with cancer, and will involve roughly 75,000 people altogether (see 'The clinical genome'). People with rare diseases and their relatives will make up 50,000 of the final figure; 80% of rare diseases are inherited, so the genome of the affected person (usually a child) will be sequenced along with the genomes of two of their closest blood relatives. The remaining group of 25,000 will be composed of people with cancer, who will have their genome sequenced twice (the tumour DNA will be compared with that from a patient's normal cells), giving the grand total of 100,000 genome sequences. The hope is that participants will benefit from clinical insights into their condition. But their genomes will also contribute knowledge of value to the entire patient community. One person's prostate-cancer genome, for example, might reveal specific genetic patterns that a physician can compare against the Genomics England database. The physician can then find other people with similar patterns and learn which drugs and procedures worked best for them. \n               Industry partnership \n             Genomics England is now selecting industry partners for each step in the process, from extracting the DNA to interpreting the genome. Sequencing-instrument manufacturer Illumina, which has its headquarters in San Diego, California, is handling the sequencing as well as the job of identifying genetic variants \u2014 known as variant calling. This is being done from Illumina's site in Little Chesterford, UK, but the company will be moving its sequencing instruments to the Wellcome Trust Genome Campus in Hinxton, UK, over the next few months as the project starts to scale up. Illumina is processing extracted DNA using high-throughput sequencing to obtain short fragments of the strings of As, Ts, Cs and Gs that are the building blocks of DNA. These fragments are computationally assembled back into a contiguous sequence and then, using bioinformatics, scientists will compare the resulting complete genomes with the human reference genome: a representative example of the human genome that is continually updated by the international Genome Reference Consortium. The aim is to note each deviation from this reference: each genetic variant. To identify these variants, the Illumina team will use the company's Isaac workflow, an open-source computational genome-alignment and variant-calling tool 3 . Then, to discover which of each genome's hundreds of thousands of variants have a role in the individual's disease, Genomics England will gather and analyse the sequences and variants from all 100,000 genomes at its secure data centre in Corsham, UK. But the NHS company has still to decide which software to use for this process: it will work with Illumina and academics to test, benchmark and improve the many existing variant-calling algorithms before selecting the final software for the initiative. Illumina has already sequenced more than 3,000 genomes for the project, and more are pouring in daily. For the variant analysis, the firm will collect and report different types of germline and somatic variants found at specific locations in DNA, says Peter Fromen, the company's managing director of population-sequencing initiatives. Variants can include insertions or deletions of a few nucleotides, or the substitution of one nucleotide for another. There can also be structural variants, such as changes in the copy number of a gene. Each variant is then checked against those in existing variant databases, such as dbSNP, a short-genetic-variation database curated at the US National Institutes of Health (NIH); the 1000 Genomes Project, an international research initiative that has compiled a catalogue of human genetic variation across the world; the Exome Aggregation Consortium (ExAC) database, a collection of exome sequencing data (the exome is all DNA that codes for proteins); and ClinVar, an NIH database of variants and associated physical conditions. Making sense of the variants is the next stage, and companies have been vigorously bidding for the job. In spring 2014, Genomics England launched a 'bake off' to gauge the quality of genome interpretation expertise from around the world. Twenty-eight participating companies were asked to provide genetic-variant annotation (describing an individual gene and its protein product) and interpretation (assigning them with a function) for the genomes of 15 people with rare diseases compared with 2 samples from healthy relatives, and for 10 genome pairs of tumour DNA and undiseased DNA taken from the same individual. The ten best-performing companies then took part in a tender to provide annotation and interpretation services for the first 8,000 patients in the project. Genomics England has now narrowed down the pool to four bidders, subject to them passing a test phase and agreeing a contract. Congenica of Cambridge, UK, and Omicia in Oakland, California, will analyse rare-disease genomes; Nanthealth in Culver City, California, will analyse cancer genomes; and WuXi NextCODE in Cambridge, Massachusetts, will analyse both cancer and rare diseases. Cypher Genomics in San Diego, California, and its partner, the advanced technology and defence firm Lockheed Martin in Bethesda, Maryland, are reserve bidders. Most have previous experience in the disease areas for which they were chosen. All companies will use high-performance computing to interpret genomic data and will work in Genomics England's secure data centre, where the analysis will take place. Their aim is to provide a mostly automated service \u2014 interpreting next-generation sequencing data still remains a partly manual process that can take from a couple of hours to weeks. At its peak, says Augusto Rendon, who directs bioinformatics for Genomics England, the project will be receiving up to 200 genomes a day for processing. A manual process will not do if the initiative is to come in on time and on budget \u2014 and, ultimately, if whole-genome sequencing is to serve as a widespread diagnosis tool. \n               The chosen few \n             Each company brings its own, disparate expertise and history to tackling the thorny interpretation task. Congenica, a spin-off of the Wellcome Trust Sanger Institute and the UK Department of Health, already does testing for NHS laboratories. It will report on inherited and acquired rare-gene mutations associated with disease. The company's Sapientia platform has already been put to the test by analysing DNA from 12,000 children taking part in the Deciphering Developmental Disorders study, the world's largest nationwide rare-disease-sequencing programme 4 . This genome-wide study returned a diagnosis to 30\u201340% of participating families whose children presented with undiagnosed birth defects or learning disabilities, says Tom Weaver, chief executive of Congenica. Omicia in the United States also has a track record of providing clinical interpretation, but using open-source, open-access tools. For the 100,000 Genomes Project, it will use gene-interpretation software called Opal to predict which variants are likely to be causing disease. The company's Phevor (phenotype driven variant ontological re-ranking tool) 5  algorithm can also take into account the patient's phenotype \u2014 the physical manifestation of disease. These highly automated tools mean that the firm can avoid having to manually compile information from a bundle of sources, says Martin Reese, the company's chief scientific officer. The algorithms statistically 'triage' variants into those that are known to cause disease, those for which disease links exist and those that need further investigation. Opal then pulls together all of the results into a report with which a physician can make treatment decisions. Cypher Genomics, which is a spin-off of the Scripps Research Institute in La Jolla, California, has developed interpretation software called Mantis that ranks variants according to how likely they are to cause disease. Scientists can then follow up on the tougher cases, which can include manual analysis, says Adam Simpson, the company's chief operating officer. The only finalist with population-wide genomics know-how under its belt is WuXi NextCODE. The firm is a spin-off of the Icelandic genetics company deCODE, which was founded in 1996 by K\u00e1ri Stef\u00e1nsson, who pioneered the idea of linking genealogical and genetic data from the entire Icelandic population to identify human genes associated with common diseases. When deCODE was acquired by the California-based biopharmaceutical firm Amgen, a handful of informaticians and deCODE alumni formed NextCODE, which in turn was acquired by biopharma firm WuXi PharmaTech of Shanghai, China, in January 2015. WuXi NextCODE has created a way to provide information beyond a distilled list of possible variants. The company has built a way to handle large data sets, such that scientists can zoom in and out of a person's entire genome, and the software continuously pulls out the most up-to-date information about a variant, says Jeff Gulcher, WuXi NextCODE's chief scientific officer. A typical question might be \u201cI want to know if this is a real mutation or not\u201d, says Gulcher. That can mean hunting for other individuals with a rare disease and the same genomic variants. And, one day, a physician might want to find people with cancer who have similar mutations and disease course and who were treated with comparable drugs in the past ten years. The company's Genomically Ordered Relational Database is optimized for such hunts, he says. In another case, a geneticist might want to compare 20 genomes, each with one million variants, to find out what ails a patient. Rather than compare the genomes in their entirety, WuXi NextCODE's database architecture arranges variants according to genomic position, pulling in a slice of information at a time, making the process computationally efficient. The platform has also been trained on large data sets, such as the genomic information of 300,000 Icelanders, says the company's chief operating officer, Hannes Smarason. Nanthealth is a privately owned health-care company that is focused on using computational analysis of cancer genomic information to guide diagnosis and therapy. The firm, which did not respond to requests for information, says on its website that it has analysed more than 20,000 genomes. It was founded by physician and biomedical researcher Patrick Soon-Shiong, who also chairs a foundation that funds research and aims to erase disparities in health-care access and heads a non-profit research organization whose goal is to facilitate digital molecular diagnosis. He developed the cancer drug Abraxane (protein-bound paclitaxel), which is used to treat many types of cancers. For Genomics England, the entire project is about providing clinical answers. But before the diagnostic readouts can reach physicians and their patients, teams of scientists and clinicians will need to pore over the data. Genomics England has organized 2,000 UK scientists with expertise in 13 rare diseases and 10 cancer types to quality check and study project results, especially where a connection to disease has not yet been well established. Using cell-biology assays and mouse models, they will explore how variants may cause or contribute to disease, knowledge that will feed back into the 100,000 Genomes Project. Some results may be quick to validate in databases, whereas others will require careful scrutiny with deeper analysis using the literature, software tools and laboratory assays. The NHS is not known as a technological innovator, says Bell. But Genomics England will probably be transformative, he says. The 100,000 Genomes Project has generated a wave of activity that could be a powerful boost for the whole field of genomics. It will engender many commercial and academic opportunities \u2014 perhaps enabling genomic medicine to finally fulfil its promise of delivering widespread benefit to people with disease. Reprints and Permissions"},
{"file_id": "527545a", "url": "https://www.nature.com/articles/527545a", "year": 2015, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "Antibodies used in research often give murky results. Broader awareness and advanced technologies promise clarity.  A mouse first alerted Clifford Saper to the fact that antibodies were misleading the scientific community. As editor-in-chief of the  Journal of Comparative Neurology  between 1994 and 2011, he handled scores of papers in which scientists relied on antibodies to flag the locations of neurotransmitters and their receptors. Around the turn of the century, related investigations began to roll in from researchers using knockout mice, animals genetically engineered to not express a target gene. The results were unsettling. Antibody staining in knockout animals should have shown radically different patterns from those in unmodified animals. But all too often the images were identical. \u201cAs we saw more and more retractions due to this, I began to realize that we had no systematic way to evaluate papers that used antibodies,\u201d recalls Saper, now chair of neurology at Beth Israel Deaconess Medical Center in Boston, Massachusetts. Thus began a one-journal revolution. Saper and his editorial colleagues set up a policy of requiring extensive validation data on each antibody 1 . The policy was good for rigour, but not submissions, he recalls. \u201cMany authors were caught in the middle, and found it easier to publish their papers elsewhere.\u201d But Saper persisted. His efforts eventually culminated in the JCN Antibody Database, an inventory of a few thousand antibodies that can be trusted for neuroanatomy. Today, biomedical researchers still collect tales of antibody woe faster than country-music labels spin out sad songs. The most common grumble is the cheating reagent: the antibody purchased to detect protein X surreptitiously binds protein Y (and perhaps ignores X altogether). Another complaint is 'lost treasure': a run of promising experiments that stalls when a new batch of antibodies fails to reproduce previous findings (see  'A market in a bind' ). But technological advances and shifts in the scientific community now promise to cut through this antibody quagmire. Antibodies are ubiquitous tools in the life sciences. Perhaps their most popular use is in western blotting to reveal the presence of a particular protein in cells or tissue samples, but they are also used to visualize proteins under the microscope by immunohistochemistry and immunofluorescence, as well as in many other applications that stem from an antibody's presumed ability to bind specific biomolecules. A 2015 report from online purchasing portal Biocompare puts the market for research antibodies at US$2.5 billion a year and growing. The choice is dazzling: there are hundreds of vendors supplying products. It is alarming, then, to discover that antibodies can be unreliable reagents. Insufficient specificity, sensitivity and lot-to-lot consistency have resulted in false findings and wasted efforts. Antibody unreliability has taken its toll across studies in cancer, metabolism, ageing, immunology and cell signalling, and in any field concerned with researching complex biomolecules. The waste, in terms of time and resources, is colossal. Losses from purchasing poorly characterized antibodies have been estimated at $800 million per year, not counting the impact of false conclusions, uninterpretable (or misinterpreted) experiments, wasted patient samples and fruitless research time 2 . Mathias Uhl\u00e9n, a protein researcher at the Royal Institute of Technology in Stockholm, says that frustration with research antibodies has been building for years 3  and that the time is finally ripe for improvements. \u201cThere is a big interest in the community to clean this up.\u201d \n               Spurred to act \n             Discontent has spurred action along various fronts. In September, Uhl\u00e9n chaired the inaugural meeting for a working group on antibody validation hosted by the Human Proteome Organization, an international consortium based in Vancouver, Canada, that supports large-scale projects for understanding proteins. That same month, the Federation of American Societies for Experimental Biology hosted roundtables to explore problems with antibodies. It expects to issue recommendations early next year. The US National Institutes of Health (NIH) is also on the case. Starting in January next year, grant applications must include a new section describing efforts to authenticate antibodies and other key resources required for experiments. Far-reaching solutions are likely to be hammered out at a meeting hosted by the Global Biological Standards Institute next September. The gathering will be held in Asilomar, California, where scientists gathered 40 years ago to set cautionary approaches for using recombinant genetic technology to manipulate DNA. \u201cWe're hoping that the community will come up with consensus guidelines,\u201d says Jon Lorsch, director of the US National Institute of General Medical Sciences in Bethesda, Maryland. That way, both grant applicants and reviewers will have resources to turn to when describing how they will authenticate their materials. Such resources could take the form of a menu of broad-strokes criteria. \u201cWe are not talking about good and bad antibodies but antibodies that work in specific assays and specific context,\u201d says Uhl\u00e9n. Evaluation categories might include knockdown and knockout approaches to reveal whether an antibody still binds even in the absence of the target protein. Another approach would be to tag a target protein with a fluorescent marker to reveal whether the antibody also binds untagged proteins. A third category could compare a new antibody with a well-characterized one. Finally, researchers could run the antibody and whatever it binds through a mass spectrometer to analyse bound molecules for the expected protein fragments.  Several vendors have announced their own characterization efforts, and new technologies are helping. Alan Hirzel, chief executive officer of Abcam, a life-sciences reagents provider in Cambridge, UK, says that to verify that its commercial antibodies perform as expected, the company is using a genome editing method called CRISPR\u2013Cas9, which makes precise changes in DNA. The company is testing antibodies on human cell lines in which target genes have been disrupted by CRISPR\u2013Cas9 and then posting results for each reagent tested. \u201cWe now really have the technologies we need that allow us to carry out those characterizations, whereas 5 or 10 years ago, we simply didn't,\u201d says Klaus Lindpaintner, chief scientific officer at Thermo Fisher Scientific, a life-sciences tools provider in Waltham, Massachusetts. Those companies with characterization data are starting to view this as a competitive advantage. In June this year, life-sciences company Bio-Rad in Hercules, California, launched a line of antibodies that have been tested for off-target activity in western blots against 12 different cell lines. Since mid-2014, Proteintech, an antibody manufacturer in Chicago, Illinois, has been using small interfering RNA to knock down gene expression in each new antibody product \u2014 assessing whether the signal subsides with the expression of the target gene. Such efforts are nascent, however, with only a tiny fraction of companies' catalogues being subjected to validation. And not all companies disclose the specific conditions of testing, or whether an antibody has performed poorly under those conditions, says Gordon Whiteley, lab director at the NIH's Antibody Characterization Program, which aims to create reliable antibodies for use in cancer biology. The example his programme sets in terms of supplying testing protocols and resulting data could be just as important as the reagents themselves, he says. There will be no single best way to test antibodies, says Roberto Polakiewicz, chief scientific officer of Cell Signaling Technology, an antibody manufacturer in Danvers, Massachusetts. \u201cDeveloping an antibody is a scientific endeavour. You need people who know what experiments to do to validate an antibody.\u201d If customers cannot see the data and make their own judgements, they need to look for a new antibody, he says. But researchers sometimes take only a cursory look at data, and many do not realize that antibodies' performance in a given tissue or application, such as western blotting, says little about whether it will work in other sorts of experiments. And commercial providers cannot guarantee that a given antibody will work for every tissue type and experimental condition, warns Paul Sawchenko, a neuroscientist at the Salk Institute in San Diego, California. \u201cUnless one is so fortunate as to have had someone else demonstrate specificity in the same tissue from the same species under the same experimental conditions, you should be obliged to do this yourself.\u201d \n               Vital information \n             It would be more efficient to learn from other researchers' work, but fewer than half of the publications that describe antibody experiments report which specific reagent was actually used 4 . Even when authors do include a catalogue number, companies may discontinue products and sell off lines, making them hard to track, says Anita Bandrowski, an information scientist at the University of California, San Diego. Bandrowski is group leader at the Resource Identification Initiative, an NIH-backed programme involving a diverse group of academic collaborators. The initiative has been instrumental in establishing unique identifiers for antibodies and persuading dozens of journals to ask authors to specifically name which antibodies they are using.  Information is beginning to accumulate. More than two dozen web portals have sprung up to help researchers select antibodies. Some collect user reviews on antibody performance and offer comparison tools. The Antibody Validation Channel, a project of the scientific publisher F1000, allows researchers to post their accounts and even request peer review. Biocompare has hired a content editor whose sole focus is to reach out to the research community and get them to write reviews. Some antibody suppliers, such as St John's Laboratory in London, offer researchers free products in exchange for testing and sharing the results. Antibodies-online, a market place for antibodies, arranges for an independent third party to perform validation. At Antibodypedia's knockdown initiative, launched in September, life scientists can earn hundreds of dollars in free reagents if they submit data showing that gene-silencing reagents such as small interfering RNA or CRISPR\u2013Cas9 eliminate an antibody signal for a given target. But many scientists are wary of information from anonymous reviews. Data supplied by both users and companies can be sparse, and some projects share data only if they confirm that an antibody works as expected. \u201cSometimes it seems easier to hire a detective than to order a specific antibody,\u201d concludes an overview of antibody portals 5 . \n               Future assessments \n             Some researchers are developing mechanisms to compare antibodies directly. Aled Edwards at the University of Toronto, Canada, is director of the international Structural Genomics Consortium (SGC). He and his SGC colleagues used mass spectrometry to detect and compare the sets of proteins pulled down by immunoprecipitation with more than 1,000 antibodies 6 . The collaboration ran across 5 reference laboratories, took 4 years and cost US$3 million, not counting in-kind donations. Ultimately, it established a procedure to score antibody quality and share quantitative information about its performance, specifically for 'pull-down experiments', in which proteins are pulled out of solution using antibodies. Fridtjof Lund-Johansen, a proteomics researcher at Oslo University Hospital in Norway, is developing an ambitious bead assay that tests thousands of antibodies at once 7 . The plan is to separate cellular proteins into many different fractions, then profile the proteins in each fraction using two different methods. One is mass spectrometry and the other is a bead-based array with thousands of antibodies. The mass spectrometry data serve as a reference for the results obtained with antibodies. Turning the idea into a refined assay will take considerable work, Lund-Johansen admits. \u201cIt is extremely ambitious. It is totally crazy, but it is the only way to go.\u201d Other scientists are intrigued at the approach but wonder if it will predict antibody performance in common techniques. Blanket assessments of antibodies can be overinterpreted, says Ulf Landegren, a proteomics technology developer at Uppsala University in Sweden. \u201cIt is far more meaningful to discuss the ability of assays to detect the correct protein, rather than whether antibodies or other binders bind the right protein.\u201d A case in point is cross-reactivity, when an antibody binds proteins other than its specified target. Cross-reactivity depends not just on a particular antibody, but also on the complexity of a sample, the concentration of the antibody and the rarity of the target protein. He recommends that rather than relying on a single antibody, researchers should instead test antibodies in pairs that are designed to bind to different parts of a target protein. Parts of a sample labelled with both reagents are less likely to represent off-target binding. One problem with this approach is that it is hard for scientists to know if they are purchasing different antibodies. Vendors often obtain products from different sources and are not required to disclose the original manufacturer. As a result, researchers who want to compare several antibodies may end up comparing identical products sold by several vendors. A handful of companies, including Genlogica and One World Labs, both in San Diego, California, only sell products labelled by the original manufacturer and offer 'trial size' antibody batches so that researchers can test products side by side in their labs. The toughest challenge is not so much in antibody characterization but in persuading cell biologists to hold back on using antibodies until these are thoroughly evaluated, says Edwards, although he doubts that scientists will become savvier unless funders and publishers force the issue. \u201cRight now we have an unregulated market, where you don't have to have any quality to sell your product.\u201d In other words, he says, guidelines, characterization data and conscientious vendors only matter if researchers invest effort into selecting reagents. Reprints and Permissions"},
{"file_id": "520389a", "url": "https://www.nature.com/articles/520389a", "year": 2015, "authors": [{"name": "Vivien Marx"}], "parsed_as_year": "2006_or_before", "body": "Sometimes a drug causes a tumour to completely recede, but only in a tiny percentage of people. Scientists want to decipher such outlier responses for the benefit of all patients.  If Patient X were like most people with advanced bladder cancer, she would probably be dead by now. After her first diagnosis, she received standard chemotherapy. It failed. Then she entered a clinical trial for a drug that was originally approved to treat other tumour types: would it also work in metastatic bladder cancer? Apparently not \u2014 none of the other patients in the trial did well. Yet Patient X thrived. Her tumour completely disappeared, says computational biologist Barry Taylor at Memorial Sloan Kettering Cancer Center (MSKCC) in New York, where Patient X was treated. Today, a little more than five years after treatment, she is healthy and has no evidence of disease 1 . Patient X (her identity is shielded to protect her privacy) is an exceptional responder, one of those rare individuals who have a dramatically positive response to a therapy that does little or nothing for most other patients. This response is not unique to cancer. Immunologists, for example, have discovered why some individuals can be HIV-positive and yet avoid the symptoms of AIDS. By definition, exceptional responses are rare, which makes them hard to study. Their anecdotal nature seems to contradict the teachings on statistically sound results in biomedical research. In a clinical trial, even if there are several exceptional responders, a drug will fail to achieve approval because it does not improve the health of the majority of patients. This means there has been little incentive for researchers or drug companies to investigate thoroughly why a few people respond so well. But that neglect is starting to be addressed as more cases of exceptional responses in cancer reach the published scientific literature and techniques emerge for profiling patients at the molecular level 2 . In Patient X's case, genome sequencing revealed a mutation in her tumour that explains why her cancer is specifically vulnerable to the drug she received on the clinical trial 1 . Such successes indicate that searching for and profiling these patients can potentially help researchers to predict many other patients' responses to potential therapies. The relatively new ability to comprehensively characterize a tumour's genome, transcriptome (its gene expression) and metabolome (its metabolic processes) increases the chance of discovering the reasons behind outlier results, says Kenneth Kinzler, a cancer researcher at the Johns Hopkins Kimmel Cancer Center in Baltimore, Maryland. \u201cThe hope is that a signal seen in an exceptional responder will be seen in other cancer patients and be a predictor of therapeutic response regardless of tumour type,\u201d he says. \n               The exceptional profile \n             There is no universally accepted definition of exceptional responders, says Barbara Conley of the US National Cancer Institute (NCI) in Rockville, Maryland. Conley directs the Exceptional Responders Initiative (ERI), which profiles these patients. The ERI considers a drug response to be exceptional when a tumour disappears or when a patient shows an exceptional response to treatment and lives longer than 90% of others treated similarly. In tough-to-treat and advanced cancers, an exceptional response is when treatment causes a tumour to regress by at least 30% for at least six months, but only in less than 10% of people on the same treatment. In the case of Patient X, for example, her sequenced tumour genome revealed a mutation in a gene called tuberous sclerosis complex 1, which is one of several genes involved in a pathway that regulates cell growth and proliferation. The drug that worked for Patient X, but not for the other patients in the clinical trial, inhibits signalling in that pathway.  But that does not completely explain Patient X's exceptional response. Analysis of tumour samples from 13 other patients in her trial showed that four had a mutation in the same gene, but the drug gave them only a short reprieve. To get a better understanding of Patient X and other exceptional responders, the ERI wants to do comprehensive profiling of a wide variety of parameters, including patients' clinical history, DNA changes, RNA levels of different genes (which reflect their activity) and metabolic pathways. Taylor and his colleagues have long encountered the critique that studying exceptional responders is merely generalizing anecdotes. But even though published studies on exceptional responders are few, he says, \u201cI think the weight of evidence has now shifted that view.\u201d Vincent Miller, a former MSKCC oncologist, agrees that views about outliers are changing and thinks that many more such individuals might be found. Any oncologist has a handful of patients in whom cancer just melts away with no obvious explanation, says Miller, who is chief medical officer of Foundation Medicine in Cambridge, Massachusetts, a company that performs genomic analysis of samples from people with cancer. In January, the pharmaceutical company Roche, based in Basel, Switzerland, bought a majority stake in Foundation Medicine, which is also involved in the ERI. The ERI encourages clinicians to get in touch if one of their patients has an exceptional reaction to a drug. At that point, a multidisciplinary review determines whether a more comprehensive profile is warranted, says Conley. In approved cases, and with the patient's consent, the physician sends in the complete medical record and a tumour sample. Around 160 submissions are currently under review. Conley and her team have been surprised to see submissions about established drugs as well as drugs still under development. The ERI makes sense only because large-scale sequencing efforts such as The Cancer Genome Atlas (TCGA) now offer huge data stores, says David Wheeler, who leads the ERI genome-analysis team at the human genome sequencing centre of Baylor College of Medicine in Houston, Texas. From Baylor, genome data will go to a database that is accessible by the research community. The first few ERI samples are now beginning to arrive at Baylor, and researchers there are all set to potentially perform whole-genome sequencing using their newly arrived equipment \u2014 HiSeq X Ten Illumina sequencers. Whole-genome sequencing is ideal, says Wheeler, because it provides the most complete genomic information. But it also requires enough sample and plenty of time and money; so when the samples are smaller or when only ones with lower tumour purity are available, the team will just focus on protein-coding genes, which make up the exome. For now, the ERI is in a pilot phase. If it proves successful, it could be scaled up by, for example, helping cancer treatment centres to forage for exceptional responders in their biobanks. But the pilot faces a few challenges. One key issue is time, says Kristen Leraas, who is the sample coordinator at the biospecimen processing facility of the Nationwide Children's Hospital in Columbus, Ohio, where all of the ERI's samples are processed (see 'Clock-watchers'). When a sample comes in, she says, scientists have to race against the clock to process, standardize and prepare it for sequencing: DNA and RNA have to be extracted from the sample quickly to avoid any kind of degradation. \u201cWe pretend our hair is on fire and we make sure we extract right away.\u201d Another challenge is that exceptional responses are unexpected, so the cancer centres sending tissue samples to Columbus do not collect them in a standardized way. One sample might be blood from someone with leukaemia, whereas another might come from a solid tumour. And unlike the case with the TCGA, it might arrive without a matched healthy tissue sample from the same person. A sample might be smaller than a pencil eraser and, in some cases \u2014 when it comes from a fine-needle biopsy, for example \u2014 it might even be invisible to the naked eye. TCGA samples weigh on average 260 milligrams, whereas \u201cif we get 100 milligrams, that's a lot\u201d, says Jay Bowen, who directs logistics and data management at Nationwide's biospecimen processing facility. \u201cSometimes we make do with about 20 milligrams.\u201d \n               Comprehensive testing \n             The Nationwide laboratory's top priority with these samples is to extract enough nucleic acid to allow multiple analyses, including exome and messenger RNA sequencing. Some DNA is also sent to Foundation Medicine, where tests can detect and validate four classes of DNA alterations at once: substitutions of bases along the DNA strand, genetic insertions and deletions, changes in the number of copies of genes present in the genome and structural rearrangements 3 . Ideally, if the sample yields sufficient quantities of nucleic acid, whole-genome sequencing or other types of tests, such as analysis of DNA methylation, can be performed. A potential complication is that tumour samples taken during surgery or biopsy are often fixed in formalin and then embedded in paraffin. These formalin-fixed paraffin-embedded (FFPE) samples are standard in medical centres and are preferred by pathologists, who can easily shave off a thin slice when they want to study the tumour's cellular morphology under a microscope as part of diagnosis. But this process can crosslink nucleic acids, and can also oxidize and shear these molecules, says molecular biologist Erik Zmuda, who directs molecular characterization tasks at the Nationwide's biospecimen processing facility. There is a risk that a genomic signal indicative of an exceptional response is actually an FFPE artefact. Thus, for studying the tumour's genome, researchers much prefer frozen tissue. Zmuda and his colleagues at Nationwide and other institutions think they see a way to allow pathologists to continue to use their preferred FFPE preservation method while providing molecular biologists with the ability to profile a sample at the resolution they need. The team's idea is to find a telltale signature of FFPE artefacts in tumour samples, which would allow them to computationally mask these effects in the data. The team is developing an algorithm that would correct for the artefacts and thus make it easier to compare data from FFPE and frozen samples. That, in turn, could open up possibilities to retroactively analyse patient samples from pathology departments in any hospital. As well as helping the hunt for signals in outlier genomes, this method could also be adapted for use in genome analysis more generally when diagnosing and treating patients. Other fields have a longer tradition than cancer research does of looking at exceptional responders, says Stephen Friend, a former director of the oncology division of pharmaceutical company Merck in Kenilworth, New Jersey. Early in the AIDS epidemic, for example, immunologists noticed that some people can be HIV-positive but lack symptoms. This exceptional biology was found to result from a mutation that changes a protein on the surface of the immune-cell type that HIV infects, thus stopping HIV from entering the cell 4 . Such links between a specific mutation and disease have sometimes led to targeted drugs. But genomics is not a black and white world in which certain mutations lead to the same clinical course in all patients, says Friend. Environmental factors and other genetic variants play their part too. This may be why these targeted drugs do not work in 100% of the patients with that mutation, he says. Friend co-directs the Resilience Project ( http://resilienceproject.me ), which is geared towards finding outliers in many diseases 5 . The goal is to find people who harbour DNA changes that cause severe and rare childhood diseases, or that heighten cancer risk, but who have lived into healthy adulthood in spite of their genomes. The programme is run by the non-profit organization Sage Bionetworks, which is based in Seattle, Washington, and is devoted to setting up platforms through which scientists can collaborate and share data. The Resilience Project currently consists of researchers from the Icahn School of Medicine at Mount Sinai Hospital in New York (conversations are also under way with the Gurdon Institute in Cambridge, UK). DNA analysis is in progress on samples from more than half-a-million donors, says Friend, who also directs Sage Bionetworks. If the first analysis of the donor DNA reveals a mutation that could have killed the carrier, researchers can dig deeper into that person's genetics and biochemistry in an effort to understand their resilience. If one mutation is decisive, analysis can be quick, says Friend. But a mutation might act in conjunction with secondary mutations elsewhere in the genome. Searching for such mutation combinations is difficult, he says. But with an outlier genome in hand, researchers are at least trawling through a bucket of data, not an ocean of data. Scientists tend to keep findings under wraps until they publish. But Friend thinks that analysis should be a collaborative task that is spread across multiple laboratories. This would increase the speed at which scientists can decipher which factors \u2014 be they genetic, immunological, environmental or a combination \u2014 have protected resilient individuals. \u201cWhat I'm hoping is that we can get scientists to take it on as a sort of crowd-sourced federated approach,\u201d says Friend. \u201cNo one is paid to do that, no one owns the data.\u201d \n               Rare signals \n             In a clinical trial, scientists strive for numbers: making sure there are sufficient cases of disease and controls to see whether a drug is having an effect, for example. They look for global trends rather than focus on the outliers, says Gustavo Stolovitzky, a researcher for the technology firm IBM in Yorktown Heights, New York, who runs the Dialogue for Reverse Engineering Assessment and Methods, a research venture and competition that, for example, looks at how well different algorithms predict the reaction of cancer cells to drugs 6 . By definition, outliers are too rare to have much statistical power, Stolovitzky says, and are usually dismissed as flukes. But conversely, he says, an exceptional response is a strong signal that is hard to miss. If many scientists hunt for exceptional responders in data from the ERI or the Resilience Project, perhaps 20 or even 50 cases can emerge. \u201cThat's starting to be something,\u201d says Stolovitzky. \u201cIt's a number we can do statistics with.\u201d If so, it may be possible to glimpse patterns that can help to explain how exceptional responders beat the odds. In profiling outliers, scientists will not know which of the molecular signals is decisive, which is why comprehensive profiles are needed for everything \u2014 genome sequencing data, gene expression data, clinical data and other assay results. Comparing these profiles is tricky: for example, it can often be a challenge to compare genomic sequence, says Trey Ideker, a computational biologist at the University of California, San Diego. \u201cWe sequence this individual and they're a snowflake,\u201d he says \u2014 showing patterns that are unique even though the patients have the same type of cancer. Ideker says that one approach to address that diversity is to view cancer as a disease of pathways, in which groups of genes act together to perform functions in the cell. When analysed on a pathway level, he says, patterns do emerge. For example, researchers may find that dissimilar-seeming mutations in a cancer all fall in a certain pathway, meaning that they all impair the same cell function. These network patterns are not complete biochemical explanations of an exceptional response in cancer treatment, says Ideker, but they are indications of what to explore next. Crucially, he says, by considering pathways, an exceptional responder becomes part of a group. Even if it is not a large group, the person is no longer an outlier. Many patients could benefit from ventures to decipher the molecular profile of exceptional responders. A physician might realize that a drug that was not expected to do well in a given patient might actually be surprisingly suitable, says Taylor. This approach to cancer treatment complements an emerging idea that rather than focusing on the organ in which the tumour originated, treatments should be targeted to the molecular profile driving a given cancer. For research on outliers to be of greatest help, the outlier cases must be rigorously selected. Only then can the analysis deliver sound results despite the fact that it remains a profile of only one person, says Friend. Taylor agrees, pointing out that molecular analysis of tumours from patients is increasingly possible and that there is growing acceptance of studying outlier patients. \u201cNevertheless,\u201d he says, \u201cit requires that we stay focused on exploring the most significant outlier responses to ensure the greatest return for patients.\u201d \n                     End of cancer-genome project prompts rethink \n                   \n                     Biomarkers: Exceptional responders\u2014discovering predictive biomarkers \n                   \n                     Nature  Special: Cancer genomics \n                   \n                     The Resilience Project \n                   \n                     Exceptional Responders Initiative \n                   Reprints and Permissions"},
{"file_id": "526147a", "url": "https://www.nature.com/articles/526147a", "year": 2015, "authors": [{"name": "Amber Dance"}], "parsed_as_year": "2006_or_before", "body": "Working at a variety of scales and with disparate organisms and technologies, researchers are mapping how parts of the brain connect. A newborn baby, well fed and sleepy, is swaddled in a blanket and lying on what looks like a tea tray with a helmet attached to one end. Once the infant falls asleep, researchers pull special tabs on the blanket to ease the baby into the helmet. It is a customized receiver coil used for magnetic resonance imaging (MRI), a common method for visualizing brains in living people. The researchers slide the baby-holding contraption along a special trolley into the MRI tube and start collecting images. From about 1,000 such scans, and another 500 of developing fetuses, UK scientists in the Developing Human Connectome Project plan to map how regions of the brain communicate with each other during development. They then hope to work out why preterm babies are at risk for conditions such as autism spectrum disorder or attention deficit hyperactivity disorder, and perhaps to do similar scans to check whether methods to prevent such disorders are working. The project is one of many to unravel the 'connectome', the links between the brain's hundreds of areas and millions of neurons. \u201cThe days of just looking at one part of the brain are waning,\u201d says Arthur Toga, director of the Laboratory of Neuro Imaging at the University of Southern California (USC) in Los Angeles. He and other scientists are already starting to compare the connections in healthy brains with those of people who have connectopathies, diseases caused by aberrant connections, such as schizophrenia, or disrupted connections, like Alzheimer's disease. The subjects studied by connectome researchers range from living people to the preserved brains of tiny animals such as worms and flies. The investigative technologies range from MRI scanners to light microscopes and electron microscopes. Irrespective of the specifics, scientists \u2014 with the aid of computers \u2014 painstakingly chart connections to build an atlas. The map-makers hope that revealing the connectome's structure will help neuroscientists to navigate as they work out how different parts of the brain function together. Like traditional cartography, brain mapping is a matter of scale (see 'Maps across magnitudes'). Researchers such as Toga who study the brains of living people are limited to a global view. \u201cIt's basically a fly-over at 39,000 feet,\u201d Toga says. This approach, called macroscale by some, shows how bundles of axon fibres connect large regions together. With millimetre resolution, it is like a country map that marks major highways. Scientists studying animal brains slice by slice get more detail. At this mesoscale, researchers see how smaller regions of the brain communicate along single axons at micrometre or submicrometre resolution. It is like adding in the lanes of highways and local streets. Finally, microscale images reveal individual neurons and synapses at resolutions of a few nanometres \u2014 akin to a map that shows even footpaths and stepping stones. \n               Fly-over \n             In a major effort to visualize the brain's superhighways, 100 researchers across 10 institutions are close to wrapping up the 5-year, US$30-million Human Connectome Project (HCP), funded by the US National Institutes of Health 1 . By early 2016, they expect to complete MRI scans on 1,200 healthy young adults. They recruited twins \u2014 both identical and fraternal \u2014 and their non-twin siblings to investigate how patterns in brain connectivity might be inherited; they also collected data such as IQ scores and smoking habits to look for correlations with the connectome. By the end of the project, they will have amassed a petabyte's (10 15 ) worth of pictures. HCP researchers image the basic structure of the brain and bundles of axon fibres. They measure blood oxygen levels across the brain as an indicator of activity, looking for areas that fire as people perform tasks or just zone out. Brain areas that are active at the same time are likely to work together. To get the most information out of each subject, HCP collaborators worked with Siemens Healthcare in Erlangen, Germany, to soup up a standard MRI scanner. It generates a 3-tesla magnetic field \u2014 comparable to that in standard machines \u2014 but can control the field more precisely. MRI scanners use gradients of magnetic fields to aim at parts of the brain, and the stronger gradient of the HCP machine offers faster imaging and better resolution. That creates more-detailed images of axon bundles. A version of their machine is now available commercially, known as the MAGNETOM Prisma. Many standard MRI machines collect images through the brain one slice at a time, but others, including the HCP one, collect eight cross-sectional images of the brain at once, helping researchers see which brain regions are working at the same time. HCP collaborators have also scanned some subjects with a 7-tesla machine, getting even higher-quality data. In a separate arm of the HCP project, Toga and another group of collaborators are pushing MRI technology in another way. They are improving how machines visualize axon bundles by making use of the restricted movement of water molecules within them. Such diffusion imaging can typically detect water moving in no more than 64 directions. With the HCP's diffusion spectrum imaging software, MRI machines detect hundreds of directions, and thus reveal smaller axon bundles than regular MRI can. But high-quality images are not enough. To compare images between subjects, researchers use academic-written software such as FSL and FreeSurfer to stretch and squeeze each brain image into a standard shape. The programs must also track how each image was warped, because therein lie key data on what differentiates one human brain from another. Wide-scale comparisons are planned. The 1,200 young adults, aged 22\u201335, who were scanned for the HCP are just the beginning. The NIH will now fund projects that look at children and older adults. Combining those connectomes with results from the Developing Human Connectome Project, scientists will have brain maps of the whole human lifespan. The NIH also plans to sponsor work focusing on people with particular diseases or genetic profiles. Despite the advances, David Van Essen, a neurobiologist at Washington University in St. Louis, Missouri, and co-leader of the HCP, cautions that MRI images can only approximate the wiring of the brain. Scientists working at the mesoscale level get more detail by using light microscopes to look at brain slices. At this scale, scientists work to pick out groups of neurons and their outgoing axons. Mesoconnectome cartographers therefore inject tracers to label a specific brain region and its conversational partners. Most of the work is in mice, but some researchers are getting started with marmosets, primates the size of kittens. At the Allen Institute for Brain Science in Seattle, Washington, Hongkui Zeng and her colleagues put together a mesoconnectome by injecting the brains of living mice with viruses carrying the gene for the glowing marker green fluorescence protein (GFP). The neurons at each injection site accumulate GFP along their axons, and so point to the other neurons that they communicate with. To image the brain, microscopists typically slice it as thinly as possible, but that can mangle the tissue and reduce the quality of the image. Zeng therefore uses a technique called serial two-photon tomography 2  in a system called the TissueCyte 1000 that she helped microscope company TissueVission in Cambridge, Massachusetts, to design. In conventional microscopy, GFP requires only one photon to fluoresce, but Zeng's set-up requires the marker to take a double hit. Any wayward photons that flow above or below her plane of focus make no difference to the image, because it is unlikely that two off-target photons will hit the same GFP molecule. The next trick is that sections are scanned before they are sliced. Researchers embed the mouse brain in a stabilizing matrix of agarose, then image just below the top surface. A cutter integrated into the microscope then shaves off a 100-micrometre-thick section, and the microscope images beneath the new top surface, repeatedly, all through the brain. \u201cIt's never damaged before we do the imaging,\u201d says Tim Ragan, president of TissueVision. The system can scan a whole mouse brain overnight, Zeng says. But imaging is only part of the battle. The microscope yields 2D sections of 3D axons that are tangled together. Scientists \u2014 or rather, their computer algorithms \u2014 must examine each section, tracking the axons through more than 100 images. That can take more time than the scanning. USC neuroscientist Houri Hintiryan, who is working to generate a mouse mesoconnectome using multiple coloured tracers 3 , says that the gold-standard tool for this analysis is the human eye. She spends a lot of time lining up structures in sequential images. \u201cThat is very exhausting,\u201d she says. \u201cHowever, that is probably the most reliable way to do it up to this point.\u201d Eventually, however, the process will need to be automated, says neuroscientist Partha Mitra of Cold Spring Harbor Laboratory in New York. \u201cOne has to build a virtual neuroanatomist,\u201d he says. \u201cI want a machine to look at the slides and make sense of them.\u201d He and other researchers are working toward this goal. At the Allen Institute, Zeng's team already uses in-house software that quantifies the GFP signal in blocks measuring 25 micrometres per side, about the size of a single cortical neuron, rather than tracing individual cells directly. The team has already processed images from more than 2,100 mice, and made the data available online. It expects to add connection information from hundreds more mice over the next couple of years. \n               Zooming in \n             Even the mesoscale connectome offers only part of the brain's story. Microscale neural map-makers want to see connections between neurons \u2014 individual synapses where an outstretched axon meets a spiny dendrite. Every neuron talks to thousands of others, so each might have thousands of synapses. And scientists still want to look at wide sections of the brain. \u201cThis is an attempt to step back, but keep the detail,\u201d says Moritz Helmstaedter, director of the Max Planck Institute for Brain Research in Frankfurt, Germany. \u201cThis is why it's an enormous endeavour.\u201d For this, researchers rely on electron microscopy. In the Fly EM project at the Howard Hughes Medical Institute's Janelia Research Campus in Ashburn, Virginia, collaborators use focused ion beam scanning electron microscopy (FIB-SEM) for a serial approach analogous to what Zeng does with light microscopy. They scan the top of a fruit-fly brain, then use the ion beam to sandblast just 8 nanometres off the top before scanning again, then repeat all through the brain for a total of about 500,000 slices. It takes two or three years to section and image just one fly brain with FIB-SEM, although the researchers can reduce that time by splitting the task between a few microscopes. Fast machines are essential for larger mouse brains, and so Carl Zeiss Microscopy collaborated with connectome scientists to develop the Zeiss MultiSEM microscope. The device uses not one electron beam, but 61 or even 91, so it can do the work of dozens of electron microscopes at once. Imaging one square millimetre of tissue, in a single plane, takes just eight minutes, says Stephan Nickell, a product manager at Zeiss in Oberkochen, Germany. By tiling images together, users can get a picture representing a slice of brain that is several millimetres, and sometimes even centimetres, across, but can still zoom in for nanometre-scale details 4 . Again, the hard part is the data processing, and humans still do it best, says neurobiologist Jeff Lichtman of Harvard University in Cambridge. He and his colleagues are working on an algorithm to take over. \u201cIt's about 95% accurate, which is terrible,\u201d he says; he thinks that they can improve on that. Janelia scientists do not fully trust the computer yet, either; they let it make the first pass at identifying cells and synapses, but then use human proofreaders. Others crowdsource the challenge. For example, Helmstaedter developed a game, called Brainflight, in which players 'fly' through the brain's nerves and software captures those movements to the define the borders of the axons. \u201cEven lay people can do it within minutes,\u201d he says. Helmstaedter and Winfried Denk \u2014 director of the Max Planck Institute for Neurobiology in Martinsried \u2014 have published the largest microconnectome reported so far: a cube of mouse retina measuring 100 micrometres to a side and encompassing about 1,000 neurons and 250,000 synapses 5 . That was about two-millionths of the mouse brain. Helmstaedter's next goal is a cubic millimetre of cortex, which is roughly 1,000 times bigger. Denk's ambition is a full mouse brain. \n               Connectome in action \n             The number and extent of connectomes that are ready for mining will grow quickly over the next decade. Meanwhile, scientists are making headway with the bits and pieces. Thousands have accessed the partial HCP data set, Van Essen says, and Zeng says that thousands visit the Allen connectome database every month. Neuroscientist Ian Meinertzhagen of Dalhousie University in Halifax, Canada, offers a straightforward example of how connectomics contributed to his work with the  Drosophila  vision system. Fruit flies are attracted to ultraviolet light, and certain photoreceptor cells are known to detect this wavelength. Armed with his electron-microscopy maps, Meinertzhagen predicted that certain neurons in the optic lobe would receive input from those photoreceptors. Sure enough, when his collaborators deactivated those connections, the flies no longer preferred ultraviolet light 6 . These connectomes will provide fundamental information for many neuroscientists, says Mark Mattson, chief of the Laboratory of Neurosciences at the US National Institute on Aging in Baltimore, Maryland. \u201cIt's important to know what neurons connect with other neurons in the brain; it's important to know how much variability there is between individuals.\u201d But there is still debate about what information is needed, and the level of detail that will be most useful. Tony Movshon of New York University holds that the mesoconnectome hits the sweet spot to understand neural circuits \u2014 the level of brain function that neuroscientists most want to understand. For example, scientists interested in how the brain processes sound or touch could follow the mesoconnectome pathways to identify possible members of the relevant circuits. The microconnectome, to his mind, provides too much detail to ask those kinds of questions. At that level, scientists are \u201cdoomed to be lost in the forest by looking at all the individual branches\u201d, he says. And the macroconnectome fails to pick up many connections, so scientists will miss important components of the circuit. But others say that all scales are essential to the next phase of neuroscience, even if it is still too early to predict precisely how. Advances such as the light microscope, and the electron microscope after it, revealed a cellular universe unimagined by those lacking such equipment, Lichtman points out. The connectome will do the same, he says, even at the microscale. \u201cFor that reason alone, looking at brains at this level is likely to be interesting.\u201d Eventually, such information will be a resource that scientists depend on, predicts Denk. \u201cIt's like the genome. It'll be something that nobody will want to do without.\u201d \n                     Neuroscience: The brain, interrupted \n                   \n                     Neurobiology: Brain mapping in high resolution \n                   \n                     Neuroscience: The connected self \n                   Reprints and Permissions"},
{"file_id": "525409a", "url": "https://www.nature.com/articles/525409a", "year": 2015, "authors": [{"name": "Marissa Fessenden"}], "parsed_as_year": "2006_or_before", "body": "Cutting-edge tools and analyses are digging deeper than ever before to unveil the intricacies of the diverse human immune system. Vaccines save lives \u2014 but they don't always work. Take the annual influenza shot: by some estimates, flu vaccines are only 50\u201370% effective even when well matched to the virus strains in broad circulation. Despite all the research, scientists still cannot predict whether a given vaccine will work for any specific person. Learning to make vaccines that protect more people means getting a better handle on the immune system \u2014 a bewildering militia of cells that communicate to detect and destroy pathogens. So far, attempts to parse the system's complexity have involved work on mice, rats, rabbits, dogs, non-human primates and even lampreys and sea urchins. Yet results do not always translate to the one species that medicine cares most about. \u201cThere has been a vast zoo of animal models, but the one animal model we haven't yet exploited is us \u2014  Homo sapiens ,\u201d says Bali Pulendran, an immunologist at Emory University in Atlanta, Georgia. Now, researchers are tackling the most difficult animal to study as never before. Advances in technology are helping scientists to dive deeper into the inner workings of single cells and carry out analysis on greater numbers of cells at once. Efforts in data analysis, sharing and collaboration promise to enable work that is too expensive for individual labs. Ultimately, researchers hope to bring fresh insights to the clinic to protect and treat people using the power of an individual's own immune defences. The human immune system is incredibly diverse. Each class of immune cell is actually an army of subtypes. The elite forces \u2014 the lymphocytes, which recognize specific pathogens or wayward body cells \u2014 consist of natural killer (NK) cells, which quickly dispatch infected or cancerous cells, and B and T cells, which bear receptors on their surfaces designed to recognize specific invaders. But B and T cells break down further: there are regulatory T cells, T helper cells, memory B cells, naive B cells and more, each with its own unique role. These lymphocytes coordinate in turn with cells such as macrophages and monocytes, which are further specialized for other functions. Diversity manifests between people, too. Even identical twins vary in terms of the exact molecules and cell profiles that fight off disease. From an evolutionary point of view, variability ensures that some members of a species will survive a deadly disease outbreak \u2014 but it confounds researchers. Gender, ethnicity, genetic background and disease history all affect a person's immune response in unpredictable ways. They influence whether a vaccine will work, and whether someone has allergies or an autoimmune disease \u2014 both resulting from an overactive immune system \u2014 or whether a person will develop cancer, which is caused in part by an inattentive system that fails to remove errant cells. \n               Vaccines unveiled \n             Instead of seeing confusion in such diversity, researchers such as Pulendran see opportunity. With the right combination of sophisticated technologies and data analysis, human variation can offer a natural experiment in what underlies an effective immune response. This reasoning led Pulendran and his team to some groundbreaking research on why a vaccine for yellow fever works so well. Since immunization against the sometimes-deadly tropical disease began in 1937, only 12 cases have been reported among the hundreds of millions of people immunized. Scientists have long known that the vaccine spurs the body to produce T cells that can kill cells infected with the yellow-fever virus \u2014 but they did not know how. In 2009, Pulendran's team published an analysis 1  of changes in the state, number and types of immune cells in the body before and after vaccination. The group found that quantities of a protein called EIF2AK4 spike in key immune cells (mainly dendritic cells, which help T cells to identify invaders) just days after vaccination. The higher the spike in protein levels, the more anti-yellow-fever T cells are later produced. The close correlation suggests the existence of components that foster strong immune responses \u2014 at least for the yellow-fever vaccine. Pulendran and his colleagues 2  have since discovered other proteins that predict similarly strong responses to vaccines for flu and meningococcal disease. Now, they are linking these types of marker to subpopulations of cells and classifying variation across individuals. One major reason that immune responses vary is the vast collections of receptors on the surfaces of T and B cells, which correspond to antibodies that are secreted by the latter cells. To produce a near-endless assortment of these Y-shaped molecules, lymphocytes shuffle their genes as they mature. The myriad receptors and antibodies that result enable the immune system to recognize many different pathogens. Researchers want to sequence genes for these receptors to work out what makes a potent immune response, and so gain clues for developing vaccines and for designing therapies that could spur the immune system to fight cancer. But because each receptor is made from proteins encoded by at least two types of separately shuffled gene segment, sequences alone are not enough. Researchers must also learn how these proteins are paired in an individual cell \u2014 and which combinations show the most promise for fighting disease. At the University of Texas at Austin, chemical engineer George Georgiou has tackled this challenge by studying B cells one at a time. He and his team 3  first encase individual cells in complex droplets: ones with an aqueous core that preserves the cell's genetic material; an outer oil layer to keep the cells separated; and magnetic beads that allow researchers to manipulate each droplet and so capture and extract the genetic material from individual cells. These data can reveal the antibody repertoire elicited by various stimuli \u2014 crucial information for designing vaccines (see 'Profile of an immune cell'). Georgiou's group hopes to publish manual-like methods so that others can use the technique. Investigators who are unfamiliar with it should prepare themselves for a steep learning curve, he says: \u201cEvery method, especially new methods from academic labs, has some nuances.\u201d But for those who are willing to put in the time and elbow grease, precise answers about individual immune cells await. Another sequencing approach relies on specially formulated beads to tag individual cells before DNA analysis. The tags fuse with the cells' genetic material and function as barcodes that can be traced back to the original cell, even when cells are analysed in pools. Hedda Wardemann, an immunologist at the German Cancer Research Center in Heidelberg, has used this strategy to analyse genes encoding paired receptor proteins in more than 46,000 B cells at a time 4 . Most specialized-sequencing approaches are developed by individual labs, such as Georgiou's or Wardemann's, that have engineering know-how. But as the field grows, companies are getting into the game. One of the biggest players in the microfluidics field is Fluidigm in South San Francisco, California. This September, the company began to ship high-throughput chips, which can be used on the company's C1 microfluidics platform to interrogate genomes of 800 individual cells in a single 6.5-hour run. Although it has a much lower throughput than Georgiou's droplet method (which can process 6 million B cells in a day), Fluidigm's technology requires less expertise. The company plans to increase throughput to nearly 100,000 cells per run in the near future. \n               Perturbed populations \n             In addition to profiling individual cells (see 'Nanoarenas for cell attacks'), researchers want to track how cell populations change in response to vaccination or infection. To identify specific cell types, the scientists rely on protein markers studded on the cells' surfaces. For example, two markers dubbed CD4 and CD8 both show up on certain types of memory T cells \u2014 but CD8 is also on NK cells, and CD4 is on monocytes and dendritic cells. So, to measure only memory T cells, researchers may need to screen for three different markers. To isolate an even more-specific subset, the number of markers must increase. Conventionally, researchers have relied on a cell-profiling technology called flow cytometry, in which coloured, fluorescent proteins are attached to specific cell markers so that combinations can be easily detected and the cells scored or sorted. But overlaps in colour spectra generally limit analyses to as few as a dozen markers. The latest iteration of cell-profiling technology \u2014 mass cytometry \u2014 uses rare-earth metals instead of fluorescence and can detect more than 40 markers. Because mass cytometry can identify so many cell types in a single sample, more types of experiments can be done. Studies in babies, for example, are key to understanding the immune system's development. But infants generally cannot tolerate blood withdrawals of more than 4\u20135 millilitres \u2014 and even simple flow-cytometry experiments can require more than 10 ml. Mass cytometry, by contrast, can run on less than 4 ml. Mark Davis, a molecular immunologist at Stanford University in California, used mass cytometry to track hundreds of parameters \u2014 including 72 different immune-cell populations \u2014 in the blood of 210 twins. His team found 5  that much of the variation between people's immune systems can be attributed to environmental factors, rather than to genetic ones. Without mass cytometry, this work would have been too complex to perform, he says. DVS Sciences, now a part of Fluidigm, has invented a mass cytometer called the CyTOF for use in cell profiling. The latest version (as well as upgrades for the older system) boosts sensitivity and sample-processing speed, and can run multiple samples at a time. But these technologies are expensive. The June version of the CyTOF \u2014 the 'Helios' system \u2014 starts at roughly US$500,000, not counting service contracts. At Stanford, Davis and other researchers rely on shared facilities. \n               Assembling the pieces \n             Although scientists are making progress, many tools have been slow to reach the clinic, says Padmanee Sharma, a physician\u2013scientist at the University of Texas MD Anderson Cancer Center in Houston. Every new clinical technology needs standards and quality assurances, which require extensive testing to establish. Clinical trials are only now adopting procedures that might help clinicians to track their patients' immune responses and feed in to treatment decisions. boxed-text Communication is another bottleneck. Information is accumulating rapidly and needs to be shared by collaborators as diverse as statisticians, clinicians, basic biologists and technologists. Coordinating research that involves human participants places huge demands on logistics, resources and expertise, and one major effort to facilitate such work is the Human Immunology Project Consortium (HIPC) funded by the US National Institutes of Health (NIH). The HIPC doles out grants to advance methods, and endeavours to extend the fruits of researchers' labour to all. The consortium offers an online data-analysis and management platform called ImmuneSpace, which helps researchers to place data in a long-term archive called the Immunology Database and Analysis Portal (ImmPORT), also funded by the NIH. The HIPC is spearheading efforts to standardize procedures for commonly performed assays in cytometry as well as alternate methods of immune profiling, such as measuring antibodies in serum samples. Another emerging need is for techniques for easy cross-analysis of many data types, says Steve Kleinstein, a computational immunologist at Yale University in New Haven, Connecticut. \u201cThere's a lot of subtlety in the data, and it's very easy to pick up a piece of code or tool that somebody put out there on the web, run it with your data and get a plot that looks interesting \u2014 but that's a very dangerous thing to do,\u201d he says. To help solve this problem, Kleinstein and his group have developed software called the Repertoire Sequencing Toolkit (pRESTO) 6 , which offers a way to process, annotate and correct raw sequencing data from high-throughput platforms such as Illumina. It also allows researchers to run their data in different computing environments and then return to the pRESTO environment. A separate tool, a web portal known as the VDJServer, is in beta-testing after launching in April. It offers the ability to analyse B- and T-cell-receptor data, with the goal of providing an intuitive interface for users who have not done any programming, says project leader Lindsay Cowell, a bioinformatician and immunologist at the University of Texas Southwestern Medical Center in Dallas. The server will incorporate more analysis tools into the portal as they become available (Kleinstein's pRESTO is already embedded). Moreover, the portal lets researchers share data and even tap into the computing power of the Texas Advanced Computing Center at the University of Texas at Austin. There is still an acute need for human immunology-specific data repositories, notably for T- and B-cell-receptor sequencing data, says Jamie Scott, a molecular immunologist at Simon Fraser University in Burnaby, Canada, who is co-leading an effort to share such data. But perhaps the biggest block is a basic one: a dearth of training. Most analysis requires some programming skills, says John Tsang, head of computational systems biology for the Trans-NIH Center for Human Immunology in Bethesda, Maryland. For now, most tools are limited to the specialist, he says; collaboration with those who can understand the programming is still the best way forward. Creating more collaborations should, in turn, help to ensure that the tools truly further basic knowledge and translate into practical applications. \u201cIt is very attractive to apply the latest gee-whiz 'omics' technology to measure things,\u201d says Pulendran. \u201cBut I think we need to go beyond measuring and accumulation of data \u2014 to knowledge and to understanding.\u201d \n                     Nature  Technology Feature: Cytometry \u2014 measure for measure \n                   \n                     Systems biology of vaccination for seasonal influenza in humans \n                   \n                     Immune profiling in health and disease \n                   Reprints and Permissions"},
{"file_id": "528291a", "url": "https://www.nature.com/articles/528291a", "year": 2015, "authors": [{"name": "Amber Dance"}], "parsed_as_year": "2006_or_before", "body": "The optogenetics techniques that have long been used in neuroscience are now giving biologists the power to probe cellular structures with unprecedented precision. Kevin Gardner opens up a mini-fridge-sized incubator and stares at the flashing blue lights inside, a scene he always finds reminiscent of a 1970s New York disco. \u201cThere are interesting things happening,\u201d he notes \u2014 but rather than the disco lights, he's talking about events at the microscopic scale. Gardner is a structural biologist at the City University of New York's Advanced Science Research Center, where he is a leader in the use of light to control the activity of proteins, a technique known as optogenetics. Thanks to the tools that he and other protein engineers have developed, scientists can now micromanage processes such as cell signalling or movement with an LED or laser flash, rather than just observing them. They can flip proteins on and off, for example, or move organelles back and forth across a cell. Over the past several years, protein engineers have developed nearly a dozen light-sensitive tools that they can use to accomplish such feats. Some are artificial proteins designed by scientists, but many incorporate modified versions of natural light-sensing proteins. A simple example is the light\u2013oxygen-voltage-sensing (LOV) domain. Found in plants, fungi and some bacteria, it contains a portion that winds up into a helix. In the dark, this coil tucks in close to the rest of the protein. But under blue light, the helix lets go and lays bare the structures hidden beneath it. Plants and algae use LOV sensors to cover enzymes or DNA-binding proteins, enabling them to regulate activities such as growth towards light or rearrangements of chloroplasts. But scientists can make a custom light-activated protein by choosing what is hidden underneath the LOV coil \u2014 the active site of an enzyme, for example. Light offers important advantages over standard methods of manipulating cellular activity. One such advantage is speed. Chemicals take minutes to enter a cell, whereas light takes fractions of a second. Thus, cell biologists can probe cellular processes such as signalling pathways or protein movement that take place on time scales of seconds to minutes, says Klaus Hahn, a cell biologist and protein engineer at the University of North Carolina School of Medicine at Chapel Hill. Likewise, a cell or organism with a knocked-out or knocked-down gene gets days, weeks or even longer to adapt to the change, and perhaps activate back-up systems. But if the protein is deactivated by a light switch on the microscope stage, there is no time to compensate \u2014 and researchers may see effects that they would not observe with conventional methods. \u201cThe cell doesn't know what hit it,\u201d Hahn says. And the effects can be reversed by simply turning off the light. Another advantage is that optogenetics offers precise spatial control: instead of flooding every cell in a Petri dish with the same small-molecule treatment, cell biologists can use tightly focused light to flip the switch in just one cell, or even part of a single cell. Optogenetics flourished initially in neuroscience: light-controlled channels were used to make neurons fire at will. But cell biologists have now embraced the technique enthusiastically. \u201cYou're going to see a ton of papers coming out, in every organism you can think of, using these tools, within the next 12 months,\u201d predicts Jared Toettcher, a bioengineer at Princeton University in New Jersey. \n               Protein partners \n             One of the most common tricks in optogenetics is to design two proteins that will bind to each other in the presence of light, forming a 'dimer'. Scientists have been triggering dimer formation with chemicals for some time, but doing it with light is relatively new. The importance of protein\u2013protein interactions in biology makes light-induced dimerization a game-changer, says Chandra Tucker, a biochemist at the University of Colorado School of Medicine in Denver. \u201cIf you are very creative,\u201d she says, \u201cyou can control [protein] activities in many different ways.\u201d For example, scientists can tether one of the proteins on a cellular membrane, and leave the other free-floating. When they turn on the light, the mobile partner will be captured by the membrane-bound partner, thus targeting it to that location. Or they can split a single protein into two inactive fragments and reattach them with a light switch to make the functioning version. Lukas Kapitein, a biophysicist at Utrecht University in the Netherlands, used light-induced dimerization to move individual classes of organelles around like furniture in a house 1 . Scientists have realized lately that cells rely on a certain  feng shui . For example, when there are plenty of nutrients around, lysosomes \u2014 metabolic organelles \u2014 hang out near the cell's edges, promoting the production of new proteins. But when cells are starved, lysosomes retreat to the cell's interior, where they encourage the cell to start digesting itself 2 . Organelle location can even affect a cell's shape. Neurons send out projections called axons to transmit impulses to other neurons, and axons tend to branch into two at spots where mitochondria, the cell's energy facility, have settled 3 , 4 . However, the effects of cellular layout can be difficult to unravel. In the past, cell biologists generally had to rely on wholesale techniques such as dissolving the cytoskeleton or changing the levels of molecular motors that deliver organelles to the right spot \u2014 relatively crude processes that tended to move all of the organelles simultaneously. Conversely, Kapitein's optogenetic method offers the ability to fine-tune the positioning of a single kind of organelle, and it is reversible. The main optogenetic tools he uses are tunable light-inducible dimerization tags (TULIPS), which are based on the LOV photosensor from oats, and an engineered protein\u2013protein interaction domain based on the common PDZ sequence. The LOV helix hides a small peptide, which, when exposed by blue light, binds to the PDZ domain 5 . Kapitein started by attaching the LOV domain to three different kinds of organelles: mitochondria, peroxisomes (metabolic sacs) and recycling endosomes that return internalized membrane components to the plasma membrane. Then he hooked the PDZ domain to one of two different kinds of intracellular motors: kinesins, which drag their cargoes to the cell's perimeter, and dyneins, which tote cargo towards its centre. With a flash of blue light, Kapitein could shuffle specific organelles inward or outward (see 'Light switch'). The researchers applied their TULIP set-up to test how endosome location affects axon growth in neurons. They removed the endosomes from the axon tips, which stopped the axons from extending. They shoved in extra endosomes, and the axons grew faster. Thus, as with mitochondria, the position of these organelles affects the cell's shape. The same system should work for many kinds of organelle, says Kapitein, allowing scientists to ask previously unanswerable questions about cell layout. He has received dozens of requests for his constructs from cell biologists who want to rearrange their own favourite cell structures. Looking ahead, he wants to find a way to move a single organelle (as opposed to all of the organelles of a particular class) and park it at a desired location. \n               Signal of intent \n             Biologists do not have to reposition entire organelles to make waves in a cell; moving a single protein will do. Many signalling pathways start with the binding of some external factor to receptors on the cell membrane, followed by a cascade of interactions that transfers the information inward from one protein to the next. The end result is some appreciable change in the interior, such as a shift in gene expression. Scientists can often mimic these effects by identifying proteins involved in the early stages of the pathway and moving them to the plasma membrane. After they hit the membrane, the proteins act as though they have received the external signal and kick off the downstream cascade. For example, Toettcher and his colleagues used a light-controlled system to study the effects of Ras, a signalling protein that is involved in diverse processes such as cell proliferation and determining cell fate in a developing embryo. This one signalling pathway can mediate such different processes because Ras has a different effect according to when and where in the cell it gets activated \u2014 but researchers were unable to investigate this in great detail until they had the optogenetic tools to turn Ras on and off. Toettcher used the phytochrome B (PhyB)\u2013PIF dimerization system, which optogenetics scientists have borrowed from the plant geneticist's favourite weed,  Arabidopsis 6 . In the plant, visible red light causes PhyB to bind and activate the PIF transcription factor \u2014 a mechanism that  Arabidopsis  uses to turn on genes involved in processes such as germinating seeds or growing away from shade. But unlike other optogenetics systems that simply switch off in the dark, PhyB and PIF stay bound until they are hit with longer-wavelength infrared light. Toettcher hooked PhyB to the plasma membrane, and part of PIF to a Ras activator. When he turned on the red light, Ras would turn on too. Because he could turn Ras back off with infrared rays, Toettcher could precisely control the timing of its activation over minutes or hours, and this made a difference to what happened downstream. For example, turning on Ras in one cell causes its neighbours to phosphorylate STAT3: a transcription factor that works in various processes such as cell growth and death. Two hours of continuous red light stimulated STAT3 phosphorylation. But 1 hour of red light, 15 minutes of infrared light and another hour of red light did not, Toettcher says. Although Ras activation totalled two hours in both cases, the cell could tell the difference, and responded accordingly 7 . The researchers do not know precisely what use STAT3 is being put to after extended Ras signalling, but they surmise that this kind of system would allow a cell to apply the same pathway for various purposes by varying the timing of the extracellular input. \n               At the flip of a gene \n             Cell-signalling systems such as Toettcher's affect the activation of genes only after a cascade of intermediate reaction steps. But optogenetic tools can also modify gene expression directly or even induce permanent changes to the genome. For example, Gardner and his colleague Laura Motta-Mena, a biochemist and cell biologist at the University of Texas Southwestern Medical Center in Dallas, have borrowed a light-activated transcription factor from bacteria to activate genes in a range of organisms 8 . At the University of Tokyo, meanwhile, chemist Moritoshi Sato and his colleagues have devised systems that use light to activate CRISPR\u2013Cas9-based gene targeting to achieve high-precision control of gene editing or expression 9 , 10 . Optogenetic CRISPR tools such as these will be particularly useful for scientists who want to follow cell behaviour in entire organisms, Hahn says. For example, researchers might want to test whether a cell migrates from one organ to another. With light and CRISPR editing, they could mark the cells that they are interested in with an extra gene encoding something obvious such as green fluorescent protein. Then they could use a microscope to check where those cells go. Sato has speculated that scientists could use the optogenetics\u2013CRISPR combination to investigate how a sequence of mutations turns a cell cancerous or how gene activation in different parts of the brain affects the organ's function. Optogenetics techniques now allow scientists to activate individual genes or proteins with the flick of a light switch, but the next step will be to control multiple processes with a whole spectrum of light. Different proteins could be made sensitive to different colours, so researchers could, for example, flick a blue-light switch to turn on one protein and then a green-light switch to activate another in the same cell. \u201cI would love to see, not one red and one blue, but something like one of those big, old-fashioned organs where you have all kinds of switches and levers,\u201d says Gardner. Unfortunately, at this point, most optogenetic switches play the same note \u2014 they usually react to just blue light (although some, such as PhyB, respond to red). But researchers are working on systems that would respond to more diverse colours. Some are even exploring other parts of the electromagnetic spectrum, hoping to activate proteins with microwaves, magnetic fields or radio waves, although that work is in the early stages. There are other disadvantages of the current optogenetics toolkit. For one, many systems are a bit 'leaky' in that they allow some activity even in the dark. And light itself can affect cellular activities such as transcription and signal transduction, points out Masa Yazawa, a stem-cell biologist at the Columbia University Medical Center in New York. This means that scientists should be careful about their negative controls, he says. Just leaving cells in the dark isn't good enough; rather, scientists should engineer a light-insensitive version of their optogenetic proteins and shine the light on those cells, too. Another disadvantage is that some light-sensitive systems require a chemical called a chromophore, which scientists have to add if the cells they want to study do not manufacture it. This can be an inconvenience, but it also makes it easy to perform a negative control experiment \u2014 the chromophore can simply be left out. Illumination can also be toxic in large doses. For experiments in which a quick flip of the light switch is all that is needed, this is no big deal. For Kapitein, it takes only a couple of milliseconds to activate the LOV domain and send organelles on their way, so cells have no problem. By contrast, Yazawa wants to grow cells with light-activated genes for days or weeks as they change from stem cells into brain or heart cells. Light toxicity could be a major issue, but fortunately Yazawa's switches \u2014 which are also borrowed from  Arabidopsis  \u2014 stay on after they have been stimulated, so he does not have to keep them under constant light 11 . Other scientists, such as Gardner, strobe the light on and off to limit cell exposure while keeping their optogenetic tools activated. A further problem is that because the tools are so new, they can still be finicky to use. \u201cThere's no plug 'n' play,\u201d says Tucker. Every cell biologist with a plan to use light will have to optimize their system, figuring out which optogenetics tools work best for them and identifying the best expression level for their light-sensitive genes. Tucker points out one rookie mistake: using the white light of the microscope to focus samples. White light contains all colours, and will activate the optogenetic sensors. It is better to use filtered light in a colour that will not stimulate the proteins. Scientists expect that in the future, it will be easier for cell biologists to adopt optogenetics. Researchers are starting to compare different light-sensitive proteins side-by-side, and their data will help others to select the best tools for their questions. In June 2015, Gardner and Motta-Mena founded a company called Optologix, based in Dallas and New York, to offer standardized kits. Their first product will include the LOV-based gene-activation system they invented, along with an LED lamp. On the bright side, the lighting part of the package is easy. A bench-top light may do, or the filters and lasers on many microscopes can activate proteins as desired. That accessibility could make light-based tools as standard as microscopes and pipettes in cell biology. \u201cTen years from now, these will be workhorse tools for everybody in developmental and cell biology,\u201d Toettcher predicts. \n                     Neuroscience: Solving the brain \n                   \n                     Research at Janelia: Life on the farm \n                   \n                     Activating happy memories cheers moody mice \n                   \n                     Light opens up the larynx \n                   \n                     Flashes of light show how memories are made \n                   \n                     Laser beam makes flies flirt \n                   \n                     Gardner laboratory \n                   \n                     Kapitein laboratory \n                   \n                     Toettcher laboratory \n                   Reprints and Permissions"}
]