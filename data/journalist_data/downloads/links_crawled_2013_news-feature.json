[
{"file_id": "504206a", "url": "https://www.nature.com/articles/504206a", "year": 2013, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "Geophysicists are scouring the globe for evidence of mantle plumes \u2014 the presumed source of some mega-eruptions. Karin Sigloch sailed into the Indian Ocean this autumn to retrieve traps she had set out the year before. She wasn't hunting deep-sea creatures but something deeper and more elusive \u2014 a thin stream of hot rock rising through the bowels of the planet. Sigloch, a geophysicist at Ludwig Maximilians University Munich in Germany, is one of the latest scientists to pursue this long-sought quarry, known as a mantle plume. Such features are thought to feed some of the most active volcanoes on Earth, and could fuel the biggest volcanic outpourings ever seen. The one Sigloch is tracking, for example, has been linked to eruptions in India 65 million years ago that were so massive they may have contributed to the demise of the dinosaurs. Mantle plumes could also explain why the Hawaiian volcanoes and many others appear in the middle of crustal plates, far from where they might be expected. And, by releasing pent-up heat from Earth's interior, plumes could have played a major part in how the planet has evolved over billions of years. The study of mantle plumes has been a mainstay of geophysics since the idea was introduced in the early 1970s 1 . Using seismic waves to image the innards of the planet, researchers have captured hints of structures that appear to be plumes. But questions linger over what these deep features really are, and a small but vocal subset of critics continues to question whether plumes exist at all. This is why Sigloch and her colleague Guilhem Barruol of the University of La R\u00e9union, on the French island of R\u00e9union in the Indian Ocean, have spent the past six weeks trawling the ocean's depths. Battling bad weather, equipment glitches and the occasional shark attack on their instruments, the French\u2013German team hauled up all 57 seismometers that they had left on the ocean bottom in autumn 2012. The instruments have been recording the seismic waves that race from distant earthquakes and shake the sea floor. By analysing those vibrations, the researchers plan to map out the rocks beneath R\u00e9union to see whether a mantle plume feeds the island's main volcano 2 . If they succeed, they may chart a mantle plume in greater detail than has ever been done before. They hope to answer some outstanding questions, such as where plumes originate, whether they rise vertically or corkscrew off to one side, and whether they branch into multiple smaller conduits just before breaching Earth's surface. \u201cUp until now, people have tended to speak of plumes without direct observation,\u201d says Barruol. \u201cOur experiment will be able to see if there is something there.\u201d Ask a geophysicist how many mantle plumes exist, and the answer may range from zero to several dozen. Most scientists, however, settle on 10 to 20 volcanic hot spots, with some of the strongest examples in Hawaii, R\u00e9union and Tristan da Cunha in the South Atlantic Ocean. The problem is knowing exactly how these regions are fed by plumes. Earthquakes offer a tool for answering that question. Because seismic waves travel more slowly through molten or soft rock than through solid stone, researchers can map out temperature and density differences inside Earth by analysing how quickly seismic waves arrive at different stations after an earthquake. This allows them to probe the planet's outer crust and the underlying mantle \u2014 all the way down to the boundary with the core, some 2,900 kilometres below the surface. The problem is that seismic images struggle to pick up narrow features, and plumes are thought to be just several hundred kilometres across until they hit the underside of the planet's outer shell, or lithosphere, where they spread out to form the plume head (see 'Deep heat'). \u201cThey should be there, but it's hard to see them,\u201d says Sigloch, who is moving to the University of Oxford, UK, in January. The key to getting better seismic images is to blanket Earth's surface with instruments. On land, this approach has worked well at places such as Yellowstone National Park in Wyoming, thought to lie atop a mantle plume responsible for some of the biggest volcanic blasts in the past 2 million years. Those data were gleaned from a project called EarthScope that has been creeping across the contiguous United States for the past nine years (see  Nature   503 , 16\u201317; 2013 ), deploying temporary seismometers in a 70-kilometre-square grid. It allowed seismologists to generate the best picture yet of the Yellowstone plume 3 , revealing the structure to be a hot, narrow upwelling that reaches into the lower mantle, at least 900 kilometres deep. But many mantle plumes are thought to underlie the oceans \u2014 and putting instruments on the sea floor is more difficult and expensive than deploying them on land. In the early 2000s, Barruol took a first stab at studying a presumed oceanic plume by installing ten seismic stations on islands in French Polynesia. That turned out to be too few to determine whether a mantle plume was rising beneath the ocean floor in the South Pacific. In 2005, a University of Hawaii-led team upped the game, placing 36 ocean-bottom seismometers around the Hawaiian islands for a year and then shuffling them to different positions for a second year. The project, which went by the acronym PLUME, yielded the best images yet of some kind of deep structure extending far beneath Hawaii 4 . \n               What lies beneath \n             But the more researchers looked at the Hawaiian plume, the more complicated things got. In the classical plume paradigm, a jet of hot, viscous rock rises to the bottom of the lithosphere, where it pools in a sort of pancake some 100 kilometres thick before trickling upwards to feed volcanoes. But under Hawaii, the seismic images suggest a different structure, with an unexpected bulge in the plume well below the bottom of the lithosphere. Maxim Ballmer, a geophysicist at the University of Hawaii at Manoa, thinks rock chemistry can be used to explain what is going on 5 . If the plume is rich in a mineral called eclogite, which is denser than typical mantle materials, it will stall as it reaches a depth of about 400 kilometres. The plume rock should pool there and spread out horizontally. Eventually, after more heat rises from below, the eclogite-rich rock would become buoyant enough to rise as a thin plume, Ballmer says. Researchers have also found other oddities beneath Hawaii. A team led by Catherine Rychert at the University of Southampton, UK, used data from the PLUME experiment to detect a warm pool about 110\u2013155 kilometres below the surface, but it seems to be centred about 100 kilometres west of the archipelago's main island rather than directly beneath it 6 . That could mean that the Hawaiian plume bends as it approaches the surface, Rychert says \u2014 maybe because of some kind of chemical boundary that diverts the plume's flow to one side, or perhaps because the plume rises along a skewed path to begin with. Beyond that, there is some dispute about how deep beneath Hawaii the PLUME experiment could reliably measure \u2014 perhaps only to 1,500 kilometres or so. One of Sigloch and Barruol's goals is to use the dense array of instruments around R\u00e9union to hunt even deeper for its plume. \u201cWe are hoping to complete this picture into the lower half of the mantle, which wasn't possible in the case of Hawaii,\u201d says Sigloch. The problem with Hawaii is that it lies in the middle of the Pacific Ocean. R\u00e9union has the advantage of being close to Madagascar and relatively close to southern Africa, where researchers can place seismic instruments more easily. Sigloch and Barruol are using data from Madagascar and Africa, along with some islands in the Mozambique Channel, to supplement the ocean-bottom seismometers they deployed last year. Altogether, about 120 seismic stations are being used for the R\u00e9union study, covering an area some 2,000 by 3,000 kilometres. But even with all that, the R\u00e9union work may not be definitive, says Cecily Wolfe, a seismologist at the University of Hawaii who led the PLUME study. \u201cIt really depends on what the structure is and what techniques and tricks they can apply to image it,\u201d she says. No matter what the R\u00e9union study finds, its results are unlikely to convince the few critics of the plume hypothesis. One of them is Warren Hamilton, a geophysicist at the Colorado School of Mines in Golden. He says that 'plumologists' are biased towards seeing plumes in seismic data. There's no question that plumes are more complicated than once thought. At this week's American Geophysical Union meeting in San Francisco, California, Rychert is scheduled to talk about her seismic studies of several possible plumes in areas including Hawaii, the Galapagos islands, Iceland and the Afar region of eastern Africa. The particular technique she uses probes only the upper few hundred kilometres and so cannot see deep into the mantle. But in most cases, as in Hawaii, she sees features about 100 kilometres deep that she thinks could be ponds of molten rock fed by plumes. And they aren't always where scientists might expect. In the Galapagos, Rychert has pinpointed three such features: one right where a mantle plume has been identified in the past and two in other locations. That could mean that plumes are smearing around as they hit the lithosphere, diverting and feeding separate volcanoes. \u201cThese seismic methods allow us to look in greater detail, and sometimes we see more complicated phenomena,\u201d she says. \u201cThose are the exciting parts.\u201d Sigloch and Barruol are expecting plenty of excitement in the coming months. Last week, they pulled into port at R\u00e9union, their ship stacked with seismometers and precious data. It will be at least a year before they crunch through all that information and can see what is happening beneath R\u00e9union. But the underwater traps seem to have caught their prey. \n                     Mantle plume propelled India towards Asia 2011-Jul-06 \n                   \n                     Hawaiian hot spot fuels volcano debate 2011-May-26 \n                   \n                     Deep structure imaged under Hawaii 2009-Dec-03 \n                   \n                     RHUM-RUM experiment \n                   \n                     MantlePlumes.org \n                   Reprints and Permissions"},
{"file_id": "503021a", "url": "https://www.nature.com/articles/503021a", "year": 2013, "authors": [], "parsed_as_year": "2006_or_before", "body": "Technologies that probe neural circuitry could greatly advance the study of human cognition. We are entering the golden age of brain science, at least in terms of funding, if not yet understanding. This year, the European Union and United States announced separate long-term programmes to study the human brain that, together, could pour more than US$2 billion into neuroscience during the next decade (that's more than 2 cents for every neuron in the body's most complex organ). Driving these investments is the sense that researchers are on the verge of making leaps in understanding the brain \u2014 advances that could eventually lead to better treatments for mental disorders. In this special issue,  Nature  taps into this excitement with reporting and opinions on efforts to apply current technologies and invent new ones to grasp how all the grey and white matter in the brain actually works. Take the design of 'neuromorphic' hardware \u2014 computer systems that function according to similar principles as neurons and neural circuits in the brain. After years of development, applications in robotics, mobile electronics and neuroprostheses are finally in sight (see  page 22 ). On the biological side, researchers are applying a broad array of techniques to mapping neural connections in the brain (see  page 147 ). On  page 31 , neurologist Kenneth Kosik argues that technologies that can monitor thousands or millions of networked neurons are exactly what are needed to better understand and treat Alzheimer's disease. Meanwhile, researchers with neuroimaging expertise are increasingly in demand, thanks to hopes that the technology will reveal information about everything from the progression of brain diseases to behaviour (see  page 153 ). Amid the excitement, there are also significant questions about the major brain projects (see  page 5 ). In the case of the US initiative, neuroscientists are not sure what kind of research it will support, and many in the field have spent much of the year struggling to define the scope of the project (see  page 26 ). However, everyone agrees that the new opportunities presented by technology warrant exploitation. This message comes through especially vividly on  page 29 , where an anonymous neuroscientist with Parkinson's disease recounts his poignant experience of studying the brain while watching his own begin to fail. \n                     Neurotechnology: BRAIN storm 2013-Nov-06 \n                   \n                     Anthropology of medicine: Where we are now with Alzheimer's 2013-Nov-06 \n                   \n                     Neurobiology: Brain mapping in high resolution 2013-Nov-06 \n                   \n                     Head start 2013-Nov-06 \n                   \n                     Neuroscience: A head start for brain imaging 2013-Nov-06 \n                   \n                     Neuroscience: My life with Parkinson's 2013-Nov-06 \n                   \n                     Nature  special: New angles on the brain \n                   Reprints and Permissions"},
{"file_id": "503186a", "url": "https://www.nature.com/articles/503186a", "year": 2013, "authors": [{"name": "Amy Maxmen"}], "parsed_as_year": "2006_or_before", "body": "Several African nations could strike a major blow against malaria by sacrificing the efficacy of some older drugs. Can they make it work? It is September in southeastern Mali, and Louka Coulibaly is standing in the shade of a squat, concrete building, giving instructions to a dozen men and women perched on a wobbly wooden bench. Coulibaly, a local medical supervisor, hands out nylon backpacks, each filled with bags of pills, plastic cups and a porcelain mortar and pestle that the women pause to admire. By noon, the men and women are packing up and heading back to their respective villages on foot, bicycle and motorcycle. The following day, they and about 1,400 other health workers throughout the region will set up shop in public spaces: under the shade of mango trees, in one-room schools, at market stands and in district health centres. They will mix and mash the pills with the mortar and pestle, dissolve them in water in a cup, and hand the bitter dandelion-coloured liquid to about 164,000 children. The effort is part of a broad campaign to prevent malaria by providing African children with drugs usually used to treat the disease. Nearly 1.2 million healthy children from parts of Mali, Togo, Chad, Niger, Nigeria and Senegal received these drugs during the rainy season \u2014 from around July to November \u2014 when malaria usually ravages the population. The countries' governments are deploying this intervention \u2014 known as seasonal malaria chemoprevention, or SMC \u2014 with financial support from the United States, the United Nations and the medical aid organization M\u00e9decins sans Fronti\u00e8res (MSF), also called Doctors Without Borders. Next year, many plan to expand the campaigns, and other countries hope to launch their own, encouraged by recommendations from the World Health Organization (WHO). Preventive use of anti-malarial drugs is not new: tourists routinely swallow them when travelling. But public-health officials have long instructed people living in regions where the disease is endemic to refrain from taking drugs prophylactically, in part because of concerns that the parasite that causes malaria will develop resistance when many people take the medicine on a long-term basis. That risk has not disappeared. In fact, scientists fully expect SMC to encourage widespread drug resistance. No one knows when, exactly, but it could happen within as few as five years. Until then, SMC has the power to prevent 8.8 million cases and 80,000 deaths each year if implemented in regions with high rates of seasonal malaria. That is considered a powerful enough benefit to justify losing the drugs. \u201cLife is a risk,\u201d says Coulibaly, a Malian hired by MSF to train local health workers. \u201cAnd if you don't take risks, you don't win.\u201d The project is designed to forestall drug resistance as long as possible, and to work in concert with mosquito nets and other preventive methods. Supporters hope that the combination will significantly suppress malaria, so that even if resistance eventually spreads, the caseload should be smaller and manageable with other treatments. But SMC will not be as successful if funding and infrastructure falter \u2014 and so far, programmes have had a shaky start. Still, advocates say that the challenges can be overcome. \u201cSMC is feasible,\u201d says Estrella Lasry, technical adviser for malaria chemoprevention at MSF. \u201cBut it requires organization, a series of steps and money to back it.\u201d \n               When intentions backfire \n             Previous attempts at large-scale malaria chemoprevention offer lessons on what not to do. In the 1950s, David Clyde, a malaria researcher with the British Colonial Medical Service, administered the drug pyrimethamine to villagers in Tanzania. At the time, pyrimethamine had a strong track record of clearing the parasite. But with any drug, there is a slim chance that some strains of parasite will be resistant and will survive to infect others \u2014 a chance that increases when many people take the medicine in an area where the parasites are abundant and circulate year-round. Clyde's experiment drove this concept home: malaria rates dropped at first, but after five months, 37% of infections in the village no longer responded to the drug 1 . Eight years later, pyrimethamine resistance had spread: up to 40% of infections within 25 kilometres of the original intervention site were unresponsive. The 1960s brought more lessons \u2014 this time, when scientists tried adding the drug chloroquine to table salt. Clinical trials had shown 2  that the salt drastically lowered malaria rates. But when the tactic was scaled up and the salt was distributed to markets in Guyana and Brazil, people consumed only what met their tastes. Others opted for untreated salt when they could, because the chloroquine made their skin itch. As a result, many people carried sub-therapeutic levels of the drug \u2014 not enough to reduce the malaria burden, but enough to promote resistance. \u201cThe salt campaigns were a disaster,\u201d says Christopher Plowe, a malariologist at the University of Maryland School of Medicine in Baltimore. Governments and aid organizations mostly shelved chemoprevention programmes after that, but resistance continued to grow \u2014 albeit slowly \u2014 as people used drugs to treat malaria infections. Between 1960 and 2000, chloroquine resistance crept around the globe and the malaria death toll steadily rose. That trend started to reverse around 2005, after the widespread adoption of the drug artemisinin, derived from Chinese sweet wormwood ( Artemisia annua ). Today, artemisinin-based drugs are the gold standard for treating malaria. \n               Second chance \n             Alassane Dicko, a malariologist at the University of Bamako in Mali, was a graduate student in Plowe's laboratory in 2001, when he started to think seriously about reviving chemoprevention. As a child, Dicko had lost his older brother and his best friend to malaria. Later, as a medical student working in hospitals, he was distraught at the number of children he saw dying. \u201cYou really feel it,\u201d he says. \u201cIf we want to do anything for this country in terms of health, we need to stop malaria first.\u201d Dicko suggested that older antimalarials might be repurposed for prevention in places where resistance to them is not yet widespread. By using drugs seasonally, only in uninfected children and in combination rather than alone, he hoped to avoid some of the mistakes of the past. With drug combinations, parasites need to acquire several mutations to survive. These mutations usually come at a cost to the parasite, so removing the selective pressure of the drugs during the dry season would give parasites still sensitive to the treatment a chance to outcompete resistant ones. Dicko proposed using a mixture of sulphadoxine and pyrimethamine called SP, which was known to be relatively safe over the long term. In 2002, his team treated 130 children with SP for two months in a placebo-controlled trial in Mali 3 . The treatment reduced malaria by 68%. Other West African scientists followed the study. Among them was Badara Ciss\u00e9, a Senegalese researcher then pursuing his doctorate with malariologist Brian Greenwood at the London School of Hygiene and Tropical Medicine. Greenwood had been considering chemoprevention since the 1980s, and he and Ciss\u00e9 immediately grasped the potential in Dicko's approach. In 2004, they began a trial in Senegal to test three monthly doses of SP plus artesunate, an artemisinin derivative. Compared with the placebo group, nearly nine out of ten malaria cases were averted 4 . With a US$4.5-million grant from the Bill & Melinda Gates Foundation in 2008, Ciss\u00e9 and his colleagues launched an as-yet-unpublished, 3-year clinical trial to study SP with another drug, amodiaquine (to preserve the efficacy of artemisinin). They treated nearly 200,000 children under 10 years old and found that they had 83% fewer cases of malaria than controls, says Ciss\u00e9. Smaller trials in other African nations reported similar findings. These are impressive numbers, especially given how recalcitrant malaria has been to preventive measures. No vaccine has ever proved fully effective against the disease, for example. And the one that is closest to approval \u2014 RTS,S \u2014 has shown disappointing results in ongoing clinical trials, with less than a 50% reduction in cases (see  Nature 502, 271\u2013272; 2013 ). \n               Resisting the critics \n             SMC raised some concerns that slowed its adoption. Some health officials suggested that natural, partial immunity to the parasite \u2014 built up as a child survives multiple bouts of malaria \u2014 would be compromised. Others fretted about the potential side effects of taking the drugs regularly. But the loudest complaints were about losing the drugs to resistance. In a cramped office in a makeshift building at the University of Dakar, Ciss\u00e9 explains how he was frustrated by the deliberations among public-health officials as malaria waged war on Senegal's children. He slumps in a chair that seems much too small for him and asks, \u201cIsn't it selfish to sit in our offices with air conditioning, saying that we should save these drugs?\u201d He recalls a single night, 20 years ago, when he watched five children die of malaria. There was nothing he could do to save them. \u201cIf this happened to you, you would not be debating about the fear of losing a drug,\u201d he says. In 2012, SMC finally won over most officials. The Cochrane Collaboration \u2014 an international group based in Melbourne, Australia, that specializes in evidence assessment \u2014 analysed results from trials in Senegal, Mali, Burkina Faso, Ghana and Gambia, and concluded 5  that SMC could prevent more than three-quarters of malaria cases in places where the disease struck seasonally. In the trials, the signs of side effects, resistance and reduced immunity were all minimal. According to another report 6 , nearly 21 million children in these regions stood to gain from SMC each year. And prevention is cheaper than treatment. Each month, chemoprevention costs $1.50 per child, which pales in comparison to the costs of travel and medical care for a child who falls ill. In November 2012, the WHO published SMC-implementation guidelines that enabled countries to apply for funds from international organizations 7 . \n               Slow start \n             Implementation has been a challenge, however. Mamadou Lamine Diouf, the drug-procurement manager for Senegal's National Malaria Control Program, says that the rollout there was supposed to reach nearly 600,000 children each month, starting in July and August. But he and the US agency footing the bill for the medicine had underestimated how much time it would take to get these older drugs manufactured anew and assessed by various organizations. By early November, health workers had managed to reach only 53,000 children. \u201cWe are learning by doing,\u201d says Diouf. \u201cNow we know that if we don't master this long supply chain, nothing will be possible.\u201d Drug delays set back chemoprevention pilots in northern Nigeria by a month. Togo's campaign did not start until September. Burkina Faso's project failed to launch when funds came up short. And the size of Mali's intended intervention dropped after a  coup d'\u00e9tat  and an invasion by al-Qaeda affiliates last year sent the nation into disarray. Still, with the lessons learned, supporters say that they will be better prepared next year (see 'A million ounces of prevention'). In March, some countries plan to apply for funding from the Global Fund to Fight AIDS, Tuberculosis and Malaria. Scott Filler, a disease coordinator at the Global Fund, which is based in Geneva, Switzerland, says, \u201cThere are not many things that can prevent malaria in 75% of children, so we will fully support it when countries come to us.\u201d  If we just roll this out without surveillance, we risk repeating all of the mistakes made in the past  As the programmes continue, researchers will keep watch to see if resistance to the drugs mounts. Randomly selected people who come to hospitals to be treated for malaria in Mali, Chad and Niger will have a spot of their blood smeared on filter paper, placed in a ziplock bag and shipped to a laboratory in Bamako, where Dicko and his colleagues will look for mutations associated with resistance to SP and amodiaquine. The University of Dakar will conduct similar tests. For the campaigns to have a long-lasting effect, chemoprevention must work faster than the parasites acquire resistance. Supporters hope that the treatments will destroy most malaria parasites over the next several years, driving down infection rates and keeping them down even when resistance begins to spread. Ramanan Laxminarayan, director of the Center for Disease Dynamics, Economics and Policy, a health-policy think tank in Washington DC, is sceptical. He predicts that imperfect implementation will prevent campaigns from having the benefits seen in clinical trials, and that the disease will bounce back in the end. Importantly, says Paul Milligan, a malaria researcher at the London School of Hygiene and Tropical Medicine, funding agencies must support follow-up evaluations to catch unintended effects such as increased vulnerability to malaria in children who outgrow the interventions. Plowe adds: \u201cIf we just roll this out without surveillance, we risk repeating all of the mistakes made in the past.\u201d Yet surveillance and drug resistance mean little to the mothers who congregate in a small village in the Koutiala region of Mali just after sunrise in September. Awa Damale, 25 years old and clad in an embroidered aqua dress and matching headscarf, arrives by donkey cart with her four children and two from another family. Five of the children swallow their medicine, but one of Damale's sons has felt ill this week. He tests positive for malaria and gets a referral to the nearest clinic. SMC is for prevention only. The boy's illness may be a sign that the drugs he took last month are not 100% effective \u2014 or that he did not swallow all of the medicine \u2014 but his condition does not dampen Damale's enthusiasm. It is the first time this year that one of her children has had malaria. Before the intervention, she constantly juggled working on the farm with caring for sick children. She does not want to hear about the possibility of the programme drying up or the drugs losing potency years down the road. Most of her children are healthy now, and that is what matters most. \n                 See Editorial \n                 page 165 \n               \n                 Tweet \n                 Follow @NatureNews \n               Reprints and Permissions"},
{"file_id": "503182a", "url": "https://www.nature.com/articles/503182a", "year": 2013, "authors": [{"name": "Catherine de Lange"}], "parsed_as_year": "2006_or_before", "body": "To combat the spread of HIV, health officials plan to circumcise 20 million men in Africa, but some have concerns about the aftermath. Marvin is 22 years old, single and \u2014 like many men of his age \u2014 thinking about girlfriends. But for the next six weeks, he will have to give his love life a rest. On this sunny morning in the Zambian capital, Lusaka, Marvin is waiting to get circumcised. Along with three of his friends, he sits tensely on a bench outside the operating theatre, a room usually used for abortions. Are they nervous? Marvin cracks a wide smile. \u201cA little,\u201d he says. In Lusaka, young men like Marvin have grown up with daily reminders of the risks that can accompany sexual relationships. For years, billboards and adverts painted on walls have espoused the importance of safe sex and testing for HIV, which currently infects more than one in five Lusaka residents. But more recently, a new message has been popping up in public spaces and the media: a call for men to get circumcised to reduce their chances of contracting the virus. Marvin's friends told him about the procedure. \u201cA lot of them have been circumcised so I thought of doing the same thing,\u201d he says, adding that it may increase his odds with women. \u201cI hear that the first thing they ask you is 'have you been circumcised?'\u201d he says. But, ultimately, he hopes the surgery will help him \u201cjust to stay healthy\u201d. Scenes like this have become familiar across much of southern and eastern Africa. Since 2007, 14 nations have taken part in a massive public-health campaign aimed at circumcising millions of men in an attempt to drastically reduce the spread of HIV (see 'Making the cut'). About 3 million men in the region have been circumcised since the start of the campaign, and the initiative was made a high priority in late 2011 by the World Health Organization (WHO), the Joint United Nations Programme on HIV/AIDS (UNAIDS) and the US President's Emergency Plan for AIDS Relief (PEPFAR) \u2014 which are funding part of the programme. The rest is being provided by the Bill & Melinda Gates Foundation, the World Bank and other global health organizations. The targets are ambitious: 80% of men of reproductive age in these countries need to be circumcised by 2015. That means more than 20 million circumcisions. If these efforts succeed, the payoff could be considerable. Reaching the 80% goal could cut the number of new HIV infections in the target countries by as much as half, says Bertran Auvert, a public-health specialist at the University of Versailles Saint-Quentin-en-Yvelines in Paris. \u201cThe goal is to have a huge impact on the overall HIV epidemic in Africa.\u201d But some scientists worry that the benefits reported for circumcision in clinical trials will not bear out to the same degree when scaled up to tackle a messy epidemic that is spread as much through behaviour as biology. In particular, they say that men are getting mixed messages about the benefits of circumcision. Another concern is the effect on women, who gain no direct protection from the circumcision campaign, and may even face greater risk. Critics say the programmes could increase the risk of HIV infection in some populations by encouraging people to engage in risky sexual behaviours, such as forgoing condoms. \u201cIt's only been shown to decrease to some degree transmission from the female to the male, and yet we are acting as if it's the only thing to do,\u201d says Philip Thuma, a doctor who runs the Malaria Institute at Macha in southern Zambia. \u201cOur job as scientists is to step back and be a bit sceptical. Should we really be putting this much emphasis on something at the expense of other things that are just as important?\u201d \n               Point of entry \n             The idea that circumcision might reduce the risk of HIV infection is often traced back to Valiere Alcena, a physician now at the Albert Einstein College of Medicine in New York City, who noted in 1986 that the men of Haiti and Central Africa are usually uncircumcised and as a result often develop lesions on the foreskin that serve as a potential entry point for AIDS 1 . The idea was cemented when a landmark study 2  in 1989 showed that of some 400 Kenyan men who visited a group of prostitutes with high rates of HIV, those who were not circumcised had more than eight times the risk of becoming infected than those who were. Michel Garenne, a demographer at the Pasteur Institute in Paris, remembers being at a meeting when the paper was first presented, and it seemed to make sense. \u201cI was working in Senegal, where there was very little AIDS and a lot of circumcision, so I became quite convinced that something was going on,\u201d he says. But others disagreed, arguing that some populations did not fit this pattern and that the correlation could be explained by other factors relating to the prevalence of circumcision, such as religious practice. Garenne himself became sceptical. In 1994, he was working in South Africa, where he found a huge HIV epidemic even though many men were circumcised. \u201cI changed my mind and became convinced that circumcision will never help in controlling HIV/AIDS,\u201d he says. Those who did believe in the protective effect of circumcision called for a systematic analysis to settle the matter. Three randomized clinical trials were carried out in southern Africa between 2002 and 2007. One study 3 , led by Auvert, offered medical circumcision to men in a region of South Africa and compared them with a non-circumcised group. After 21 months, there were 20 cases of HIV in the circumcised group and 49 among the uncircumcised men, equating to a 60% reduction in HIV risk. Two other trials, in Kenya 4  and Uganda 5 , produced similar results. \u201cIt's extraordinary in any public-health measure that you get such consistency of results,\u201d says Robert Bailey, an epidemiologist at the University of Illinois at Chicago, who led the Kenyan study. The WHO also found the consistency compelling and declared the results \u201can important landmark in the history of HIV prevention\u201d. In 2007, the WHO, UNAIDS and PEPFAR recommended implementing voluntary medical male circumcision (VMMC) in 14 high-risk African countries. \n               'A Man who cares' \n             Zambia, with its high prevalence of HIV, low circumcision rates and mixed attitudes towards circumcision, is a microcosm of the broader campaign. During the past five years, it has seen the progressive roll-out of catchy, pro-circumcision slogans on posters, television and radio campaigns. Health-service providers have also developed outreach programmes to spread the message. At the clinic where Marvin is waiting, visitors are greeted by an advertisement for its services: a silhouette of a man standing proudly with his hands hooked inside his belt and, above it, a slogan reading: \u201cMale Circumcision, a man who cares\u201d. Clinics that perform the procedure hope the belt-hook stance will become an unspoken symbol, a way for men to implicitly communicate their circumcised status, which is increasingly viewed as an attractive attribute. If the campaigns can convince enough men to take part, the programme could significantly reduce HIV-transmission rates, according to a number of modelling studies. These project that circumcising 80% of men aged 15\u201349 years old in targeted countries by 2015 would decrease the incidence of HIV by 30\u201350% over 10 years, amounting to some 3.4 million fewer new infections. Carrying out these 20.3 million circumcisions by 2015, plus an additional 8.4 million over the following 10 years, would cost about US$2 billion, but would potentially save $16.5 billion by 2025 owing to avoided treatment and care costs 6 . There may be other benefits too. Before men undergo voluntary medical circumcision, many get an HIV test, which means more cases of HIV will be picked up, and thus more men will be referred for treatment. \u201cI think circumcision is going to have a very significant impact because it's being delivered as a package,\u201d Bailey says. A big part of that package is a one-on-one counselling session run by people such as Jonathan Kabanda, a counsellor for the Society for Family Health in Livingstone, about five hours' drive south of Lusaka. It is his job to make sure that men who come in for circumcision understand the benefits and the risks, not only for themselves but also for their partners. On a Saturday morning, Kabanda sits down and arranges his materials, among them three large wooden phalluses \u2014 one equipped with a retractable fabric foreskin \u2014 and some condoms. He pulls out a small flip chart that details different matters relating to circumcision \u2014 starting with how the foreskin can increase the transmission of HIV and leading on to the benefits and limitations of circumcision. Those who support the campaigns, including the WHO, PEPFAR and the authors of the clinical trials, stress the importance of making sure that patients understand that circumcision provides limited protection and that men should continue to use condoms. But critics ask whether the message is getting through. There is certainly room for confusion. Although Kabanda's presentation is thorough, one page of the flip chart, titled 'MC does not offer 100% protection' shows a picture of a man chasing two women down the street while throwing his condoms away. Other benefits are overstated: \u201cYou are preventing your partner from cervical cancer,\u201d Kabanda says, when, in fact, the reduction in risk is far from complete. Outside the clinic in Lusaka, it is clear Marvin has also received mixed messages from his counselling session. \u201cWe were told there was a sixty-something per cent of being \u2026 I don't know if it's from HIV or some sexually transmitted disease,\u201d he says. \u201cAnd the 45%, I don't know what it is; I didn't get much about that. I think I'll try to ask later.\u201d Many men may face similar confusion. As part of the procedure, participants get only one counselling session on the matter of circumcision and one with their HIV test. By contrast, the men in the clinical trials had sessions before and after circumcision, and every six months during the 21-month follow-up period. Some public-health advocates argue that the protocols used in the trials should be replicated closely in the campaigns, otherwise they will not be as effective. \u201cWhen technologies like male circumcision are demonstrated effective, everybody forgets about the rest of the protocol and just looks at the medical procedure,\u201d says Seth Kalichman, a clinical psychologist at the University of Connecticut in Storrs who studies behavioural responses to HIV-prevention campaigns. \u201cWe need to think about the standard of care within which a procedure is implemented if we're going to hope to get the effects that we saw in the trial.\u201d However, in a campaign that aims to reach millions of men, it would be hard to get participants to return for regular counselling. And some researchers, such as Auvert, dismiss the importance of counselling, arguing that it has little impact on behaviour. \u201cIf counselling was enough to change sexual behaviour to reduce the risk of acquiring HIV, we would have stopped the HIV epidemic in Africa a long time ago,\u201d he says. \n               Risk Assessment \n             The question of behaviour has become a crucial one in the debate over circumcision. A chief concern is that men and women who believe that they are less likely to acquire HIV will engage in more risky sexual behaviour. This means that circumcision \u201ccould have the opposite effect in the long run\u201d, Garenne says. \u201cPeople might be more likely to get HIV than if they were not circumcised at all.\u201d Studies of this phenomenon \u2014 known as risk compensation \u2014 are just starting to shed light on how circumcision alters behaviour, and so far the findings are mixed. In one survey 7  conducted in Kenya directly before the circumcision scale-up took place, Bailey and his colleagues reported that one-fifth of men and women they interviewed said that condom use is less necessary and that HIV is a less serious threat because male circumcision is available. These misconceptions could put women at particular risk. Erika Layer, a health-communications researchers at the Johns Hopkins University in Baltimore, Maryland, explored that possibility by interviewing 33 women following a circumcision campaign in the city of Iringa in Tanzania. She found an overwhelming preference for circumcised men 8 ; women believed them to be cleaner and better educated. Some women also assumed \u2014 incorrectly \u2014 that the reduced risk of contracting HIV applied to women, too. As one woman who was interviewed for the study put it: \u201cCircumcised men who are HIV-positive can have sex with women and the women will not get the infection easily.\u201d Much of the research on risk compensation relies on self-reported behaviours and often involves qualitative studies. But Layer's interviews showed consistent misunderstandings about circumcision, a finding borne out by studies in other countries that suggest women are less likely to require circumcised men to use a condom. \u201cThere is evidence in South Africa, Kenya and Tanzania that shows that generally women are overstating the benefits of circumcision and believe the risk to be lower than it actually is,\u201d she says. Other studies, however, suggest that circumcision does not lead to more risky behaviour by men and women. Earlier this year, Auvert published findings from a survey 9  in the same region as his original trial in 2005. He found that despite a high uptake of circumcision, there was no reported difference in sexual behaviour \u2014 for instance in condom use or number of partners \u2014 between circumcised and uncircumcised men, and the estimated HIV incidence rate was around 60% lower among the circumcised group. Similarly, when Bailey and his colleagues did a follow-up study in Kenya, they found no increase in risk associated with the circumcision. Researchers are at odds over the implications. \u201cAny public-health measure will have the potential for risk compensation, but there is no evidence at all that risk compensation is taking place and indeed there is some evidence that the opposite is occurring,\u201d Bailey says. Kalichman disagrees. He believes that men are getting mixed messages, and that it doesn't make sense that they would agree to circumcision if they were going to consistently use condoms, which offer a much higher level of protection. \u201cWhy bother with the procedure if you have to continue to use condoms?\u201d he asks. \u201cThe whole point is, 'if I have the procedure I don't need to'.\u201d The take-home message on risk compensation, he says, is that \u201cwe don't know the extent of it and we also don't know the impact of it\u201d. It is possible, he adds, that the overall effect is minor because the 60% risk-reduction offered by circumcision is already so substantial. Nonetheless, Layer says it is important to make sure women are also targeted by clear messages about the limitations of circumcision. \u201cIf we do a little bit more to try to engage women I think we could reduce the immediate and the long-term risk for both women and men,\u201d she says. Circumcision advocates say that in the long run, a general reduction in HIV prevalence in the population will ultimately benefit women, even if some women are placed at greater risk in the short term. \u201cI am absolutely convinced that some women will become HIV positive because of the circumcision of their partners,\u201d says Auvert. However, he adds that he will publish some preliminary results soon showing that the impact on women is extremely small. \u201cWe are not concerned,\u201d he says. The issue of balancing individual risks against widespread benefits also comes up in studies looking at how many men have sex during the healing period after circumcision, when open wounds and unhealed stitches increase the risk of transmission. In the counselling session, men are told to abstain from sex for six weeks or, if they can't wait, to use a condom. However, a study 10  by Paul Hewett of the Population Council in Lusaka and his colleagues found that around one-quarter of men said they resumed sex prematurely; of those, 82% engaged in at least one act of unprotected sex, and a number had unprotected intercourse with two or more partners. When the team modelled the effect of this on HIV transmission, it estimated that if 61,000 men were circumcised in one year, early resumption of sex would result in a total of 69 extra HIV infections. But the overall net effect was protective, with 230 fewer HIV infections in one year, predominantly among men. \n               Target driven \n             Both critics and advocates of the circumcision campaigns agree that it will be a struggle to reach the 20-million mark by 2015, given that so far countries have only reached 15% of that overall goal. At health clinics across Zambia, much of the conversation is about targets, says Layer. \u201cThis seems to be the trend \u2014 do whatever it takes to get men in and out that door,\u201d she says. So circumcision providers are coming up with innovative ways to recruit more men. The US embassy in Zambia has launched a new effort to work with chiefs of traditionally non-circumcising tribes in the hope that they will \u201cconvince their own people that even though this isn't a part of their historic tradition this is the right choice\u201d, says Mark Storella, the US ambassador to Zambia. And mobile clinics travel to hard-to-reach areas to recruit men. \u201cThere's a lot of pressure right now in this country because of PEPFAR and others,\u201d says Thuma. The recruitment drive is happening across the region. In Kenya, some service providers are even offering 'moonlight circumcisions' at night, or financial incentives for referrals. More money is also being invested, but this has put significant pressure on local service providers to operate on more men. The Bill & Melinda Gates Foundation, based in Seattle, Washington, has implemented a pay-for-performance model in Zambia, for instance, in which organizations are reimbursed on the basis of their ability to reach a target number of circumcisions. \u201cI think you have to be concerned about the impact on quality of such an incentives approach,\u201d says one person familiar with the programme who wished to remain anonymous. He wonders whether it could cause circumcision providers to cut costs for other essential services, such as counselling. In a statement to  Nature , Sema Sgaier, a senior programme officer with the Gates Foundation, wrote \u201cto ensure that VMMC delivery does not affect established standards of care, it is always delivered as one component of a comprehensive package of services\u201d. These include HIV counselling and testing as well as screening and treatment for other sexually transmitted diseases. Everybody from donors to doctors in the clinic agrees that the circumcision effort must be part of a broader strategy to strangle Africa's HIV epidemic. And although the 20-million goal remains distant, the campaigns could still bring major benefits, says Bailey. \u201cEven if you don't reach the 80% target that's not to say you're not going to have a large impact on the number of new infections,\u201d he says. \u201cEvery circumcision still has a preventive effect.\u201d For people living in the epicentre of the HIV epidemic, that protective effect \u2014 however limited \u2014 is an attractive offering. Marvin, for one, is confident in his decision. \u201cI think, for me,\u201d he says, \u201cit's going to be a good thing.\u201d \n                 See Editorial \n                 page 165 \n               Reprints and Permissions"},
{"file_id": "505014a", "url": "https://www.nature.com/articles/505014a", "year": 2013, "authors": [{"name": "Stephen S. Hall"}], "parsed_as_year": "2006_or_before", "body": "By studying disadvantaged children, Richard Tremblay has traced the roots of chronic aggressive behaviour back as far as infancy. Now he hopes to go back further. Hochelaga was the original Iroquoian name for the village that ultimately became Montreal, but it is also the name of a rough-hewn French\u2013Canadian neighbourhood located east of \u2014 and a world away from \u2014 the cosmopolitan city centre. The district's tidy two- and three-storey brick duplexes, adorned with Montreal's characteristic wrought-iron staircases, predominantly house families that have, because of poverty and lack of education, never quite attained thriving middle-class status. During the 1980s, public-school officials identified Hochelaga and many other impoverished neighbourhoods in the eastern part of Montreal as places where kindergarten children disproportionately displayed severe behavioural problems, such as physical aggression. The school system asked a young University of Montreal psychologist named Richard Tremblay for help. \u201cTheir parents didn't have a high-school diploma, and many of the mothers had their first child before the age of 20,\u201d Tremblay says of the families he began to study, as he walks along Rue Ontario in Hochelaga on a sunny afternoon in September. Those were the women, he adds, \u201cmost at risk of having children who have problems\u201d. Over the past three decades, Hochelaga and similar neighbourhoods have served as living laboratories in the study of the roots of aggression. Since 1984, Tremblay and his collaborators have followed more than 1,000 children from 53 schools in the city from childhood into adulthood. And in 1985, he initiated a ground-breaking experiment in which some families of at-risk children were given support and counselling to help curb bad behaviour. His research overturned ideas about when aggressive behaviour first emerges, and showed that early intervention can deflect children away from adult criminality. The idea that a nurturing environment provides better outcomes for children hardly qualifies as news, but Tremblay has taken this idea in a provocative direction in the past ten years. He has joined researchers at McGill University in Montreal and the US National Institutes of Health (NIH) in Bethesda, Maryland, to investigate how nurturing or adverse environments might exert their effects at the molecular level, influencing gene expression through a mechanism known as epigenetics. Tremblay's Canadian cohorts are part of a growing trend for using longitudinal studies, which follow the same individuals over an extended period of time, to look for epigenetic signatures that might affect health and behaviour later in life. Research in this area is still preliminary \u2014 and not without its critics \u2014 but Tremblay believes that a firm grasp of early epigenetic effects could inform interventions to influence everything from obesity to mental illness. \u201cThere is a body of evidence, from natural experiments and actual experiments, showing that early-life experiences affect long-term outcomes such as crime, health and wages,\u201d says James Heckman, a Nobel-prizewinning economist at the University of Chicago in Illinois who is currently working with Tremblay on an early-intervention study with at-risk pregnant mothers in Dublin. The work of Tremblay and others, he says, \u201chas established a firmer biological basis for how early-life experiences affect these processes\u201d. \n               Setting goals \n             Tremblay's own early life revolved around sport. His father, Wilfrid Tremblay, played Canadian football from 1938 to 1951, and Richard was an accomplished ice-hockey goalie. When Jacques Plante, the Hall of Fame goalie for the Montreal Canadiens, suffered an injury during the Stanley Cup Playoffs in 1961, a team representative called the then-17-year-old Tremblay asking if he could report to the minor league practice rink the next morning. Tremblay, soft-spoken and mild-mannered, allows that he was \u201cinvited to join\u201d the most illustrious franchise in Canadian sports, but concluded that he was too small to play at the professional level. He decided to attend college instead. Tremblay studied physical education at the University of Ottawa. But before his final year, he read a cult novel by J. R. Salamanca called  Lilith  (Simon & Schuster, 1961), about a recreational therapist who falls in love with a young female patient at a psychiatric hospital. To a naive 20-year old, the work sounded fascinating, and when he returned to college that autumn he applied for a job as a recreational therapist at a high-security psychiatric hospital in Joliette, Quebec. He quickly found himself in over his head, working with convicted murderers and violent criminals. But it was during this time that he first started to wonder about the psychology of aggression. \u201cIt shows how a novel can change a life,\u201d he says. The hospital agreed to send him to get a master's degree in psychology, which he pursued at the University of Montreal. As Tremblay likes to say: \u201cThe first thing I did after finishing my master's degree was to go to jail for three years.\u201d That was the Pinel Institute, a new maximum-security psychiatric hospital in Montreal. Most of the people there, he says, \u201chad killed someone or were dangerous to the point of killing themselves, or others\u201d. Despite the danger, he found himself going to work on his days off to play sports with the residents. \u201cI loved it,\u201d he recalls. Then, in 1971, the University of Montreal decided to create a school focused on children with behavioural problems. The university wanted to hire Tremblay, one of the most promising students to come out of its psychology programme, to join the faculty. But he needed a PhD first, so the university paid for his training at the University of London's Institute of Education. That turned out to be a defining experience. Tremblay arrived in London with a sheaf of Rorschach blots and a grounding in psychoanalysis, but there he was exposed to the 'longitudinal' philosophy of pioneering human-growth biologist James Tanner, child psychiatrist Michael Rutter and others. He came away with a lesson that has informed the rest of his scientific career: the best way to study any aspect of human development is to conduct longitudinal studies. He threw away his Rorschach blots and, in the late 1970s, headed back to Montreal. \n               Aggressive start \n             By then, Tremblay was eager to launch his own longitudinal study. He got his chance in the early 1980s. School officials came to him with the problem of hyperactive, physically aggressive kindergarten boys. He had never worked with children before and never imagined doing so, but he recognized it as an opportunity to explore the origins of aggressive behaviour. \u201cThe idea became very clear,\u201d he says. A longitudinal study of kindergarten children would give him a chance to link childhood behaviours with adolescent and adult outcomes. In 1984, he started tracking boys from dozens of schools. Funding was initially provided for three years, but nearly three decades later Tremblay and his colleagues continue to follow many of the men involved. They have published more than 160 papers on the group. Just one year in, when the boys were seven years old, Tremblay obtained a grant to add a randomized, controlled experimental intervention. Teams of four psychologists would visit the families of about 50 boys every two weeks. They counselled parents on identifying and correcting aggressive behaviour, and trained teachers to do the same. In addition, they attempted to socialize unruly boys, and they integrated problematic boys with well-behaved children to provide positive peer role models. The Montreal intervention began at a time known informally among criminologists as the 'nothing works' era, when there was widespread pessimism about the potential to rehabilitate juvenile delinquents and adult criminals. Tremblay's intervention was labour-intensive and extremely expensive, and he recalls fretting that he was spending millions of dollars on a study but might end up with nothing to show for it. \u201cI guess I lost hope \u2014 in working with juvenile delinquents \u2014 that we could make a difference,\u201d he says. The intervention lasted about two years, but the results would take much longer to become apparent. One of the first people to see hints that it was working was Joan McCord, a criminologist at Temple University in Philadelphia, Pennsylvania. McCord had a reputation for ferreting out data that challenged conventional wisdom, most notably when she demonstrated in the 1970s that a famous US longitudinal experiment \u2014 the Cambridge\u2013Somerville Youth Study, in which juvenile delinquents were mentored and supported \u2014 had actually harmed the young men it had aimed to help 1 . Conversely, the Montreal intervention seemed to be working as intended. With each follow-up assessment, boys in the intervention arm displayed not only less delinquent behaviour than controls, but also better school performance, lower consumption of drugs and alcohol, and better social skills. Data gathered 15 years after the intervention ended revealed that it produced persistent positive effects. The boys whose families received support had a 46% graduation rate as opposed to 32% for controls. And, at the age of 24, fewer of them had criminal records: 22%, versus 33% for controls 2 . But Tremblay wasn't just seeking ways to mitigate bad behaviour \u2014 he was looking to uncover where it began. In the mid-1990s, he began to collaborate with Daniel Nagin, a criminologist at Carnegie Mellon University in Pittsburgh, Pennsylvania. Nagin applied a more sophisticated statistical metric to the burgeoning Montreal data set. The results, published in 1999, made it clear that the trajectory towards antisocial behaviour and criminality in adolescence begins very early in life 3 . Most children exhibit decreasing aggression between the ages of 6 and 15: they learn to control their aggressive impulses. Only about 4% of the boys displayed highly aggressive behaviour in early childhood that continued into their teens. The roots of physical aggression \u2014 and, by extrapolation, the origins of violent behaviour later in life \u2014 lie before the age of six, says Nagin. That is, before Tremblay's kindergarten cohort even began. Even as Nagin and Tremblay were analysing the original Montreal data, Tremblay had begun another longitudinal study designed to look at aggression before kindergarten. It was a birth cohort based in Quebec, and the resulting data suggested that aggressive behaviour was evident at 17 months and peaked at around 42 months 4 . This and later work culminated in Tremblay's 'original sin' hypothesis: that physical aggression is the default setting in human behaviour 5 . It peaks between the ages of two and four, and is usually socialized out of children by the time they enter school (see 'Aggression regression'). \u201cWe took the view that violence, and physical aggression, is a part of us as a species,\u201d says Nagin, \u201cso the issue is not how we learn it, but rather how we learn to control it.\u201d Many criminologists dismissed the findings. They argued not that the idea was wrong, but that it was irrelevant \u2014 that chronic childhood aggression is trivial compared with murder and rape in adulthood, and that the former does not explain the latter. Most still focus primarily on delinquency during adolescence, and for good reason, says sociologist Robert Sampson at Harvard University in Cambridge, Massachusetts. \u201cEarly childhood is centrally important, but it's not determinative, because there are still changes [in behaviour] later on.\u201d Yet the Montreal and similar longitudinal studies show that heightened physical aggression at a young age correlates with serious antisocial behaviour in adolescence and adulthood, says Tremblay. He is fond of citing the view that Saint Augustine offered some 1,600 years ago: \u201cIt is not the infant's will that is harmless,\u201d he wrote, \u201cbut the weakness of infant limbs.\u201d \n               Marking time \n             With Saint Augustine's headstrong infants in mind, Tremblay increasingly pondered the effects of the environment at earlier and earlier ages. Like many researchers studying behaviour, he had looked into what role genes might have in aggression, but he was dissatisfied. Genetics did not tell the whole story. Tremblay was primed, therefore, to hear about the work of Moshe Szyf, a cancer biologist at McGill, at a small Vancouver meeting in 2004. Szyf had been tracking the addition and removal of methyl groups to DNA, which can silence or activate genes. Scientists were interested in whether these methylation marks might allow the environment to influence gene expression over an organism's lifetime. Michael Meaney, a developmental neurobiologist also at McGill, collaborated with Szyf to show that newborn rat pups generously licked and groomed by their mothers had different patterns of DNA methylation from those that received less maternal attention 6 . These changes reached the brain, where the methylation pattern altered the activity of a gene that plays a central part in the animal's response to environmental stress. Maternal nurture, Szyf argued, was a form of 'environmental programming' that altered the activity and function of genes in ways that persisted throughout life. For Tremblay, it was \u201cas if the roof blew off\u201d the room. The McGill experiments suggested a biological explanation for what he had been tracking for 20 years. As he walked to dinner with Szyf that evening, Tremblay pressed for a possible collaboration. Human studies of this sort were uncharted territory. So Tremblay initiated a parallel line of animal research with Stephen Suomi, who heads the primate laboratory at the NIH's Eunice Kennedy Shriver National Institute of Child Health and Human Development in Bethesda. Both scientists had noted behavioural similarities between the chronically aggressive, hyperactive boys in the Montreal study and a group of aggressive monkeys that Suomi had raised under conditions of early maternal deprivation. Tremblay, Suomi and Szyf set out to run DNA-methylation studies on two sets of monkeys: a group nurtured by their mothers, and another deprived of maternal nurturing from shortly after birth. It took nearly a decade of difficult molecular-biology work headed up by Nadine Proven\u00e7al at McGill, but in the past year or so, the researchers have begun to publish their findings. The first primate study found distinct differences in DNA-methylation patterns between nurtured monkeys and those separated from their mothers 7 . The epigenetic residue of post-natal adversity was broad, according to Suomi, affecting more than 4,000 genes \u2014 about one-fifth of the genome \u2014 and tending to cluster in certain chromosomal regions. Moreover, the epigenetic modifications seemed to alter expression of a gene that Suomi's group had shown to be crucial to the function of the neurotransmitter serotonin 8 , low levels of which have been associated with elevated stress and aggression in humans. \u201cThese are not random changes,\u201d Suomi says. \u201cThey follow particular pathways.\u201d The marks remained stable in monkeys up to 8 years old \u2014 an age roughly equivalent to 30 in humans. Although the team was able to test both brain and white blood cells from the monkeys, they only had access to blood from the men of the Montreal cohort. Even so, studies are starting to offer a complementary human picture. In July, Szyf and Tremblay reported that men with a history of chronic aggression dating back to kindergarten had significantly lower blood levels of immune molecules called cytokines than normal controls from the cohort 9 . These molecules are typically activated during the body's response to stress, and animal studies have demonstrated a link between aggression and lower levels of a cytokine called interleukin-6, which was also lower in the chronically aggressive men. In a second study, Szyf and Tremblay showed that members of the Montreal Longitudinal Study with a long-standing history of aggression had a distinctly different pattern of DNA methylation in the genes encoding the cytokines, compared to men with a less aggressive behavioural profile 10 . The early human research has its shortcomings. For starters, the sample size is very small: only seven males with a history of aggression could be tracked down from the cohort for testing, along with 25 controls. And white blood cells are by no means the same as neurons, although Suomi notes that there is considerable overlap between the methylation patterns of the two cell types in the primate studies. Moreover, many researchers remain cautious about recent human epigenetic studies. Attributing behavioural consequences to DNA methylation may be overreaching, says Adrian Bird, a geneticist at the University of Edinburgh, UK. \u201cThese are all correlations,\u201d he says, \u201cand often the magnitude of the change is very small indeed.\u201d Tremblay is the first to admit that the story is far from simple: hundreds of genes are involved, and any single expression change is probably subtle. Yet, he says, \u201cit seems relatively clear that there are large differences in DNA methylation between those who have a history of chronic aggression compared to those who have normal development\u201d. He is convinced that the benefits of nurture merit early intervention programmes, regardless of the uncertainties in the biological part of the story. And he thinks that earlier intervention may produce even better results. \u201cIf we support these parents during pregnancy and if we help these women have a better lifestyle during pregnancy, with less stress, it should affect brain development, and these children should be better able to learn how to control their aggressive behaviour,\u201d he says. He is already testing that hypothesis. In 2007, he accepted a ten-year appointment at University College Dublin, where he is assisting on several early-childhood longitudinal studies. One, called Preparing for Life and headed by economist Orla Doyle, is testing a preventive intervention in 200 pregnant women from a disadvantaged area of north Dublin. During their pregnancies, the women received intensive home visits covering everything from nutrition, smoking, alcohol and drug counselling to support in marital relationships. And the support continues until the children reach the age of four. James Heckman, who is also collaborating on the study, says that the plan includes future epigenetic studies of the cohort. \u201cTo solve the aggression problems, which are mainly a male problem, we need to focus on females,\u201d Tremblay says. \u201cIf you ameliorate the quality of life of women, it will transfer to the next generation.\u201d \n                     Fearful memories haunt mouse descendants 2013-Dec-01 \n                   \n                     Children of the 90s: Coming of age 2012-Apr-11 \n                   \n                     Epidemiology: Study of a lifetime 2011-Mar-01 \n                   \n                     Neuroscience: In their nurture 2010-Sep-08 \n                   \n                     Nature Reviews Genetics : Epigenetics \n                   \n                     Nature  Insight: Epigenetics \n                   \n                     Richard Tremblay \n                   \n                     Moshe Szyf \n                   Reprints and Permissions"},
{"file_id": "503327a", "url": "https://www.nature.com/articles/503327a", "year": 2013, "authors": [{"name": "Mark Peplow"}], "parsed_as_year": "2006_or_before", "body": "Graphene's dazzling properties promise a technological revolution, but Europe may have to spend a billion euros to overcome some fundamental problems. Mr G gazes out from a recruitment poster hanging in an engineering building in Cambridge, UK. His cartoon cape billows out behind him, his sketched-in muscles ripple beneath his costume, his chest is emblazoned with a 'G' inside a hexagon \u2014 and his forefinger points straight at the viewer. \u201cI want you for the Graphene Flagship!\u201d declares the cartoon crusader, championing a material as super as he is. Graphene is the thinnest substance ever made: a single sheet of carbon atoms arranged in a hexagonal honeycomb pattern. It is as stiff as diamond and hundreds of times stronger than steel \u2014 yet at the same time is extremely flexible, even stretchable. It conducts electricity faster at room temperature than any other known material, and it can convert light of any wavelength into a current. In the decade since graphene was first isolated, researchers have proposed dozens of potential applications, from faster computer chips and flexible touchscreens to hyper-efficient solar cells and desalination membranes. But harnessing graphene's qualities for practical use has proved a massive challenge. Graphene is complicated and expensive to make in large sheets, which usually have so many atomic-scale flaws and tears that they fail to match the amazing properties of the tiny flakes studied in the laboratory. And even if its quality were good, there are no well-established industrial methods for handling something so thin, or for integrating it with other materials to create useful products. What's more, graphene has a superweakness. Its electrons may be extremely mobile, but other properties make it fundamentally unsuitable for the sort of on\u2013off switching that lies at the heart of digital electronics. Hence Mr G's call to arms. The character was created in 2011 to help publicize a multinational push for a Graphene Flagship project: a decade-long, \u20ac1-billion (US$1.35-billion), all-European effort to take graphene from the laboratory bench to the factory floor. And not just graphene. The project's proponents also wanted to study more than a dozen other atomically thick materials discovered in graphene's wake \u2014 that, when sandwiched together with graphene, might help to overcome its limitations 1 . Mark Peplow explains how graphene researchers plan to spend a large EU grant The campaign worked: the European Commission in Brussels gave its go-ahead to the graphene flagship project in January (see  Nature   493 , 585\u2013586; 2013 ). Already the world's largest research effort on the material, encompassing hundreds of scientists across 17 European countries, it will grow even larger after the flagship puts out its first call for additional project proposals on 25 November. The infusion of funds and energy has galvanized the graphene community, says Andrea Ferrari, director of the Cambridge Graphene Centre and chair of the flagship's executive board. Ferrari, whose office wall sports Mr G's poster, says \u201cNobody has been involved in anything this big before,\u201d \n               Too many cooks? \n             But some question whether the programme is too big. Is an academia\u2013industry collaboration, inevitably fettered by the bureaucracy of such a large venture, the best way to deliver a technological revolution? \u201cThis is not the way products are actually developed,\u201d says Phaedon Avouris, a graphene and nanotechnology researcher at IBM's Thomas J. Watson Research Center in Yorktown Heights, New York. And some researchers involved in the project are concerned that political forces, rather than scientific priorities, will steer the dispersal of funds over the next few years. Still, the flagship's prospects for success seem strong enough that national governments and industry partners, such as Nokia and Airbus, will collectively put up half its funding. (The European Commission will provide the rest.) \u201cI hope that after ten years, technologies based on graphene or other layered materials are mainstream,\u201d says the flagship's director Jari Kinaret, who is based at Chalmers University of Technology in Gothenburg, Sweden. Just as we now do with polymers, semiconductors and ceramics, he says, \u201cwe should take graphene for granted\u201d. The flagship programme is divided into 16 work packages, most of them targeted at developing applications such as high-frequency electronics, sensors and energy storage. Next week's call for proposals, worth \u20ac9 million, comes at the beginning of a \u20ac54-million ramp-up phase that is expected to deliver the first wave of prototypes by 2016. But there will be no graphene computer chips, graphene sensors or graphene solar cells without a steady supply of graphene itself. One of the flagship's first and biggest challenges is to find more economical and reliable ways to produce high-quality sheets of the material. Most research laboratories still make graphene using the method pioneered in 2004 by Andre Geim and Konstantin Novoselov at the University of Manchester, UK, who went on to win the 2010 Nobel Prize in Physics for their studies. Geim and Novoselov found that they just had to touch a strip of household sticky tape to ordinary graphite \u2014 which consists of billions of layers of graphene stacked on top of one another \u2014 and they could peel off thin flakes of carbon. By repeatedly splitting those flakes, they were eventually left with graphene 2 . This was a technique that any laboratory could use, and graphene research exploded. But the method is much too slow and finicky for industrial-scale production. Just one micrometre-sized flake made in this way can cost more than $1,000 \u2014 making it, gram for gram, one of the most expensive materials on Earth. The leading alternative 3  relies on chemical vapour deposition (CVD), whereby methane is piped over a catalytic copper foil heated to about 1,000 \u00b0C. As the methane breaks down, small islands of pure carbon begin to grow on the foil, linking together to form a patchwork polycrystalline sheet of graphene. Harsh chemicals are then used to etch away the copper to free a sheet of graphene tens of centimetres wide, which can be transferred to a silica or polymer substrate. That process brings costs below $100,000 per square metre, but the product is often riddled with defects, impairing its electrical properties and making it much weaker than flakes produced by the sticky-tape method. \n               Industrial action \n             The flagship programme is tackling this problem in part through its industrial partners, such as Graphenea of San Sebasti\u00e1n, Spain, which already makes about 15 square metres of graphene per year. And it should benefit from a deal signed in September that will see fledgling graphene producer Bluestone Global Tech of Wappingers Falls, New York, open a pre-production facility and offices at the National Graphene Institute in Manchester, the hub of Britain's graphene effort. This year, Bluestone began speeding up production and lowering costs by using bubbles of hydrogen to tease large graphene monolayers away from the copper foil without etching 4 , 5 . Yet even Bluestone's manufacturing process is \u201cstill a very complex way of adding graphene to a substrate\u201d, says Tapani Ryh\u00e4nen, head of sensors and material research for Finish company Nokia, and a member of the flagship's advisory council. The flagship aims to refine the CVD process and to improve on alternative production methods. Also problematic is the tricky process of transferring the freshly made graphene from its catalytic foil to a new substrate. Lay it on top of silicon, for example, and the sheet wrinkles and puckers. One solution would be to grow graphene directly on the substrate, or on top of another sturdy, protective monolayer such as boron nitride, a process demonstrated at small scale earlier this year 6 . But ultimately, says Rod Ruoff at the University of Texas at Austin who led the development of the CVD production method, the best way to slash costs and propel graphene into the mainstream would be to make high-quality monolayers from bulk graphite \u2014 exfoliation on an industrial scale. The flagship will investigate chemical treatments, ultrasonic vibration and more, but a practical, scalable method still seems a long way off. \u201cWe need some sort of a breakthrough here,\u201d says Ruoff. Despite its manufacturing challenges, enthusiasts are quick to point out that graphene has already hit the market. Multi-layered graphene, in which many sheets are stacked together, is used to strengthen a tennis racquet made by Head, for example, and forms a conductive circuit in anti-theft packaging produced by Vorbeck Materials in Jessup, Maryland. But these cheaper forms of graphene include a range of different structures that are essentially nanometre-sized chunks of graphite. The properties of this sooty jumble of fragments are no match for Mr G's superpowers, which reach their zenith only in pristine, one-atom-thick layers in which the atomic arrangement is perfect. Only in this state can electrons flow more quickly than in any other material. To get current moving through any crystal, electrons must first clear a hurdle called the band gap: the energy required to knock them loose from individual atoms and set them free to roam. Insulating materials have a large band gap, meaning that electrons tend to be tightly bound to the atoms and need a huge kick to start moving (see 'Mind the gap'). Semiconductors such as silicon and germanium have a much smaller band gap, so only a little jolt of energy is required. Metals have no band gap at all; they are great conductors because at least some of their electrons are always free. But graphene sits right on the boundary, blessed with an infinitesimally small band gap that helps current to zip across its interlocking hexagons 100 to 200 times faster than it can move through silicon 7 . This tiny band gap also makes graphene optically omnivorous. Silicon can only absorb photons with energies greater than its band gap; if weaker photons hit it, they can't free electrons from the parent atoms. Graphene, by contrast, can absorb photons across the visible spectrum and beyond, turning their energy into electrical current. \u201cThere's not really another material that has good properties for both optics and electronics,\u201d says Daniel Neumaier of the contract research company AMO in Aachen, Germany, who is leading the flagship's high-frequency electronics work package. This combination of abilities makes graphene a promising candidate for converting photons into electrical signals. Graphene photodetectors could allow computer chips to communicate with light rather than comparatively sluggish, energy-wasting electrons \u2014 an advance that would cut power consumption and allow computers to handle data more efficiently. Such photodetectors would be smaller than current devices made of germanium, and could handle a wide range of wavelengths, allowing them to interpret multiple signals bundled together into the same beam (see  Nature   http://doi.org/pz2 ; 2013 ). Graphene could also be useful in medical and security scanning that uses terahertz-frequency radiation. The generation and manipulation of terahertz waves, which lie between the infrared and microwave regions of the spectrum, often requires bulky equipment or cryogenic cooling. Graphene-based devices are compact and can generate or detect the waves at room temperature. This may be graphene's best opportunity for a groundbreaking application, says Avouris, because it could find a role not already occupied by other well-established materials. Others think that graphene's most obvious optical property \u2014 its transparency \u2014 may yield its first major application in the electronics industry. Samsung and other Asian companies are developing transparent graphene electrodes to serve as smartphone touchscreens. The indium tin oxide electrodes in use today are brittle, whereas graphene is strong and flexible. And although graphene touchscreens are currently more expensive than the conventional variety, \u201cthe cost is falling rapidly as we ramp up the scale of production\u201d, says Bluestone's co-founder Yu-Ming Lin. \n               Turn off \n             When it comes to digital electronics, however, graphene's greatest strength is also its greatest weakness. In principle, its extremely mobile electrons could allow graphene transistors to process data at very high rates, with some devices already clocking in at more than 400 gigahertz \u2014 many times faster than comparable silicon devices 8 . But graphene's lack of band gap makes it very hard to turn the current off once it starts flowing, a serious impediment to logic operations, which are all about on\u2013off switching. Doping graphene with other materials or slicing it into narrow ribbons can open up a small band gap, but this also slows the flow of electrons. So researchers are trying to tune its electrical properties by combining graphene with other monolayer materials such as boron nitride or creating transistors from molybdenum disulphide and tungsten diselenide 9 , 10 , 11 . But graphene is still a long way from replacing silicon electronics, says Tim Harper of the technology-development company Cientifica, based in London: \u201cNobody will just ditch silicon unless there's a really compelling reason to do so.\u201d In the near term, a graphene transistor's biggest selling point may be its ability to operate over a range of voltages, rather than any ability to switch on or off. Applications might include sensors for environmental pollutants or blood-oxygen levels, or the transmitters and receivers inside mobile phones. By the end of the programme's 30-month ramp-up phase, Neumaier's goal is to build prototypes that demonstrate graphene's potential in these areas. \u201cExpectations at the moment are very large,\u201d he says. So are the concerns of some researchers. As one of Europe's highest-profile science projects, the graphene flagship has some treacherous political waters to navigate. The European Commission wants the flagship to be as inclusive as possible, to ensure that under-represented member states get a piece of the action. One consequence is that next week's call is open only to new partners \u2014 existing flagship research groups are barred from bidding for those funds. \u201cThat came as a surprise,\u201d says Kinaret. The rule excludes all researchers who have signed up en masse through national research networks, including the CNRS in France, the Max Plank Society in Germany and the CSIC in Spain. The networks have lobbied the commission to change that rule, but \u201cwe have been less than successful\u201d, says Kinaret. Kinaret expects that restriction to change next year after the European Union's Horizon 2020 research programme comes into force, and other funding streams are available in the meantime. But some researchers have been left with a sense of foreboding. Ferrari worries that there is a risk of losing sight of the original goal: a genuine technological revolution in ten years. By slicing the flagship's money into smaller chunks and spreading it more widely, Europe could keep more member states happy \u2014 but might dilute the project's impact. \u201cExcellence must be the criterion,\u201d he insists. Meanwhile, Europe faces stiff competition from Asia in the race to commercialize graphene. Although the European Union leads the world in academic publications on the material, the UK government's Intellectual Property Office in Newport reported in March that 15 of the top 20 global graphene patent-holders are Chinese, Japanese and South Korean companies and universities, with Samsung way out in front. Some Chinese manufacturers say that mobile devices bearing graphene touchscreens will hit the market next year. Europe has led in academic research on graphene, but it trails in development. \u201cThat,\u201d says Kinaret, \u201cis what we are hoping to change.\u201d \n                     Graphene makes light work of optical signals 2013-Sep-15 \n                   \n                     Ultrashort laser pulses squeezed out of graphene 2013-May-24 \n                   \n                     Brain-simulation and graphene projects win billion-euro competition 2013-Jan-23 \n                   \n                     Britain\u2019s big bet on graphene 2012-Aug-06 \n                   \n                     Chemistry: The trials of new carbon 2011-Jan-05 \n                   \n                     Nature Outlook : Graphene \n                   \n                     Graphene Flagship \n                   \n                     Cambridge Graphene Centre \n                   \n                     National Graphene Institute \n                   \n                     Bluestone Global Tech \n                   Reprints and Permissions"},
{"file_id": "504357a", "url": "https://www.nature.com/articles/504357a", "year": 2013, "authors": [], "parsed_as_year": "2006_or_before", "body": "Ten people who mattered this year. Feng Zhang: DNA\u2019s master editor | Tania Simoncelli: Gene patent foe | Deborah Persaud: Viral victor | Michel Mayor: In search of sister Earths | Naderev Sa\u00f1o: Climate conscience | Viktor Grokhovsky: Meteorite hunter | Hualan Chen: Front-line flu sleuth | Shoukhrat Mitalipov: The cloning chief | Kathryn Clancy: An eye on harassment | Henry Snaith: Sun worshipper | Ones to watch \n               FENG ZHANG: DNA\u2019s master editor \n             \n               Borrowing from bacteria, a biologist helps to create a powerful tool for customizing DNA. \n               By Daniel Cressey \n             With a nip here and a tuck there, a DNA-cutting mechanism that bacteria use to protect themselves from viruses became one of the hottest topics in biomedical research in 2013. And a young neuroscientist with a penchant for developing tools helped to make it happen. Thirty-two-year-old Feng Zhang of the Massachusetts Institute of Technology in Cambridge is among those leading the charge in using a system called CRISPR/Cas to edit genomes cheaply, easily and precisely. In January, his group showed that the system works in eukaryotic cells \u2014 ones with membrane-bound nuclei, including those of all animals and plants. This confirmed its potential for tweaking the genomes of mice, rats and even primates to aid research, improve human-disease modelling and develop treatments ( L.\u00a0Cong  et\u00a0al. Science    339,  819\u2013823; 2013 ). But as hot as the story has been this year, \u201cthe CRISPR craze is likely just starting\u201d, says Rodolphe Barrangou, a microbiologist at North Carolina State University in Raleigh. CRISPRs (clustered regularly interspaced palindromic repeats) are DNA sequences that many bacteria and archaea use to defend themselves. They encode RNAs that can specifically recognize a target sequence in a viral genome. The RNAs work in complex with a CRISPR-associated protein, or Cas, which snips the DNA of the invader. In 2012, Jennifer Doudna of the University of California, Berkeley, Emmanuelle Charpentier, now at the Helmholtz Centre for Infection Research in Braunschweig, Germany, and colleagues showed that they could reprogram a CRISPR system to cut apart potentially any specific DNA target ( M.\u00a0Jinek  et\u00a0al. Science    337,  816\u2013821; 2012 ). By controlling how the break is repaired, they can edit a gene \u2014 adding, switching or removing parts to change the protein it encodes or disable it altogether. CRISPR is similar to two earlier genome-editing methods: the zinc-finger nuclease (ZFN) and transcription activator-like effector nuclease (TALEN) systems. But both of those locate target sequences using proteins that are often difficult and costly to produce. CRISPRs use RNA, making them easier to design. Zhang says he feels limited only \u201cby what I can imagine is possible\u201d. Although Charpentier and Doudna are generally credited with kick-starting the growth of CRISPR editing, Zhang demonstrated its vast potential by showing that it works in eukaryotes, a finding independently confirmed by George Church at Harvard Medical School in Boston, Massachusetts ( P.\u00a0Mali  et\u00a0al. Science   339,  823\u2013826; 2013 ). Zhang says that he had a head start on many of the teams who jumped in: he had been investigating the technique before it was widely reported, and because his lab had previously fine-tuned ZFNs and TALENs to edit DNA, it had procedures in place for perfecting CRISPRs. Zhang now says that he feels challenged to be creative with other applications. One particularly ambitious project on his slate is to build a library of CRISPRs that can delete any sequence in an organism\u2019s entire genome in 100\u2013200 base-pair increments. This could make it easier to investigate the function of non-coding DNA. But he is most interested in using CRISPR to treat neuro\u00adpsychiatric conditions such as Huntington\u2019s disease and schizophrenia by repairing genes in human tissues. To pursue therapeutic use of the technology, he and other CRISPR pioneers last month launched a company called Editas Medicine, based in Cambridge, that is backed by US$43\u00a0million in venture-capital funding. CRISPR \u201callows us to start to make corrections in the genome\u201d, says Zhang. \u201cBecause it\u2019s easy to program, it will open up the door to addressing mutations that affect few people but are very devastating.\u201d \n               TANIA SIMONCELLI: Gene patent foe \n             \n               A US science-policy expert fought to keep genes open to all. \n               By Heidi Ledford \n             In 2005, Tania Simoncelli managed to shock the senior lawyer at the American Civil Liberties Union (ACLU). Simoncelli, the organization\u2019s first science adviser, informed him that companies were snatching up patents on many human genes. \u201cThat\u2019s ridiculous!\u201d exclaimed the counsel, Chris Hansen. \u201cWho can we sue?\u201d It would not be that easy. Although the ACLU, a non-profit organization based in New York City, has spent nearly a century suing state and federal agencies for infringing civil rights, it had never challenged a patent. And the prospect seemed daunting in this case: the US Patent and Trademark Office had been issuing patents on human genes for nearly 30\u00a0years. But Simoncelli saw the practice as a threat to the right of individuals to access their own medical information, as well as to scientists\u2019 ability to do research on the genes. Over the next four years, Simoncelli helped ACLU\u2019s lawyers to pull together a case and identify a suitable target for a suit \u2014 Myriad Genetics, a firm based in Salt Lake City, Utah, that had been particularly aggressive in defending its patents on two genes that have been linked to breast cancer. And she rallied a consortium of scientists, patients and physicians to support the suit. \u201cShe\u2019s so persuasive,\u201d says Hansen. \u201cShe\u2019s persistent in a way that you don\u2019t notice, until suddenly you\u2019ve agreed with her.\u201d Ultimately, the ACLU pursued the lawsuit to the US Supreme Court and, this June, won. For Simoncelli, the experience offered the kind of interdisciplinary work that she had dreamed of doing since her undergraduate days studying biology and society at Cornell University in Ithaca, New York. \u201cI wanted to be the person who could help bridge the cultures of science and justice,\u201d she says. After Cornell, Simoncelli earned a master\u2019s degree in energy and resources from the University of California, Berkeley, then went to work at the ACLU in 2003. Her intention was to stay only two years before leaving to get her PhD in science, technology and society. She stayed for nearly seven \u2014 sometimes returning even after she left for the US Food and Drug Administration (FDA) in 2010, using her holiday time to continue work on the lawsuit. Skipping that PhD was a difficult decision, recalls Sheila Jasanoff, a specialist in science and technology studies at Harvard University in Cambridge, Massachusetts, who taught Simoncelli as an undergraduate and has mentored her ever since. She suspects that Simoncelli has lost out on jobs and at times struggled for respect because she lacks the degree. But at the ACLU she was given the freedom to make her mark. \u201cIt was a place where her passion and drive didn\u2019t get held back,\u201d says Jasanoff. At the FDA, Simoncelli has focused on policy areas such as nutrition and personalized medicine. She is currently working at the White House Office of Science and Technology Policy on forensic science \u2014 a project that brings her back to the intersection of science and justice. \u201cI\u2019m really looking forward to seeing what\u2019s next for Tania,\u201d says Jasanoff. \u201cShe always surpasses my imagination.\u201d \n               DEBORAH PERSAUD: Viral victor \n             \n               A virologist provides the strongest evidence yet that infants born with HIV can be cured. \n               By Sara Reardon \n             In March, Deborah Persaud was ready to share the news: a baby born with HIV in Mississippi seemed to be virus-free nearly a year after stopping treatment. Persaud, a serious, soft-spoken virologist at Johns Hopkins Children\u2019s Center in Baltimore, Maryland, says she knew that she had to tread delicately. More than 40 similar cases had previously been reported in the literature, and each had fallen apart on closer scrutiny. Genetic analysis revealed that most of the initial tests had generated false positives or had involved specimen mix-ups. But Persaud and her collaborators, Hannah Gay at the University of Mississippi in Jackson and Katherine Luzuriaga of the University of Massachusetts in Worcester, had done the genetic tests on the Mississippi baby themselves and were ready for the critics. What they were not expecting was the media onslaught that followed their announcement. News outlets around the world jumped on the story, and after a month the three were listed among  Time  magazine\u2019s 100 most influential people in the world. Persaud\u2019s role in the case started with a call in September 2012 from Gay, a paediatrician, who was treating a baby born to a woman with HIV. Because the mother had gone untreated for the duration of her pregnancy, Gay gave the baby high doses of three antiretroviral drugs \u2014 zidovudine, lamivudine and nevirapine \u2014 within hours of birth. Tests conducted at the time showed that the baby had HIV, and the mother was told to continue the child on treatment. At one check-up, however, Gay found that the baby had not been getting her drugs for five months. Gay tested the child and found no sign of the virus. To make sure that this was not another false signal, Gay called in Persaud and Luzuriaga. They matched the mother\u2019s DNA with the baby\u2019s to be sure that she had not been switched in the hospital. They took five separate blood samples for the HIV tests and personally checked each lab result. As they ruled out alternative explanations, it looked more and more likely that the initial blast of drugs had wiped out the virus. They published a paper describing the case in November ( D.\u00a0Persaud  et\u00a0al .  N. Engl. J. Med.   369,  1828\u20131835; 2013 ), and so far it seems to be standing up to scrutiny. The media attention after the initial announcement had benefits: it made an ambitious clinical trial more feasible. The International Maternal Pediatric Adolescent AIDS Clinical Trials Group, for which Persaud is a scientific adviser, plans to test these early, heavy drug doses in high-risk infants. HIV transmission rates are low in the United States, so the researchers will need to enrol hundreds of women with the virus to find enough infected infants to make their case. But many people do not want to wait for the results. Some patients who have been taking antiretrovirals since birth are now wondering whether they, too, can stop taking their drugs. \u201cIt comes up in every clinical visit,\u201d says Luzuriaga. For now, people are advised to continue to follow their prescriptions \u2014 missing even one daily pill can be hazardous. But if the results hold up, Luzuriaga says that some of the hundreds of children and teenagers treated at birth may one day be able to try ceasing their treatment. In the meantime, Persaud is looking for ways to test whether someone is in remission before such a step is taken. \u201cIt\u2019s a very high bar,\u201d she says, \u201cbut it could be the tipping point in HIV therapy for children.\u201d \n               MICHEL MAYOR: In search of sister Earths \n             \n               An astronomer with a flair for technology extends his legacy of discovery. \n               B \n               y Elizabeth Gibney \n             Michel Mayor and his team have found hundreds of exoplanets during the past two decades. But 2013 brought the 71-year-old planet-hunter a particularly gratifying discovery: his group determined that the planet Kepler-78b is of a density and size that make it the closest analogue of Earth identified so far. It is far from an exact match \u2014 Kepler-78b orbits so close to its parent star that its surface is molten. But finding a true Earth twin is only a matter of time, says Mayor, an emeritus astronomer at the University of Geneva in Switzerland who is still active in research. Before he truly retires, he says, \u201cI hope to have the possibility to celebrate this discovery.\u201d Mayor has had plenty to celebrate already. In November 1995, he and his then-student Didier Queloz published the first evidence for an exoplanet orbiting a Sun-like star. The evidence was indirect: just a tiny wobble in the motion of the star 51\u00a0Pegasi, caused by the gravity of a world half the mass of Jupiter whipping around its orbit every 4.2\u00a0days. Since then, teams led by Mayor have discovered a substantial fraction of the 1,050-odd exoplanets known so far. That success is largely due to Mayor\u2019s talent for technology, says Geoff Marcy, a rival exoplanet hunter at the University of California, Berkeley. \u201cYear after year, Michel has built an instrument a factor of ten better than the previous one,\u201d says Marcy. \u201cEvery time he does it, I\u2019m amazed.\u201d A prime example is the High Accuracy Radial Velocity Planet Searcher (HARPS), which Mayor and his team have been operating at the La Silla observatory in Chile since 2003. Able to detect stellar wobbles of less than 1\u00a0metre per second, HARPS is the most accurate spectrograph in the world, rivalled only by HARPS-North: a near-identical copy that has been operating on La Palma in the Canary Islands since 2012. It was with HARPS-North that Mayor and his team determined the density of Kepler-78b, one of thousands of exoplanet candidates compiled by NASA\u2019s Kepler spacecraft between 2009 and 2013. Another group, of which Marcy was a member, independently measured Kepler-78b\u2019s size and density. The next step \u2014 to find an Earth-like planet that is far enough from its star to harbour liquid water, and potentially life \u2014 remains a challenge. But with instruments improving all the time, Mayor predicts that it will happen within five years. He has every intention of remaining a part of that search. \u201cWhen you are in the dome, looking at the sky with a new instrument for the first time and you start to see that it is working and better than expected,\u201d he says, \u201cit is a huge pleasure.\u201d \n               NADEREV SA\u00d1O: Climate conscience \n             \n               After Typhoon Haiyan ravaged the Philippines, a diplomat focused the world\u2019s attention \u2014 briefly \u2014 on global warming. \n               By Jeff Tollefson \n             When Naderev Sa\u00f1o offered a tearful statement at the United Nations climate talks in Warsaw in November, he did not know the fate of some of his own relatives, or that of thousands of other fellow Filipinos. The head of the Philippines delegation knew only that his brother was alive and had joined emergency workers collecting dead bodies in the devastating wake of Typhoon Haiyan. He also knew that the colossal cyclone \u2014 one of the strongest on record \u2014 could be a harbinger of what coastal regions will face in the future. \u201cWhat my country is going through as a result of this extreme climate event is madness,\u201d Sa\u00f1o said. He pledged to fast during the talks, \u201cuntil a meaningful outcome is in sight\u201d. His plea for solidarity drew a standing ovation. And he kept his fast for 14 days, until delegates reached a last-minute deal to keep negotiations on track for the next major climate summit, in Paris in 2015. The pace of international progress on global warming has been glacial. Despite more than two decades of negotiations, atmospheric carbon dioxide levels have continued to rise; in May, the daily average concentration topped 400 parts per million for the first time in Hawaii, where the longest-running record is kept. In September, the Intergovernmental Panel on Climate Change released its fifth assessment of the science underlying global warming, which warned of growing threats from climate-related problems such as sea-level rise, extreme weather and droughts. Looking back, Sa\u00f1o is not sure what impact his speech had, but he argues that Typhoon Haiyan helped to put the international spotlight squarely on the climate issue. Having done graduate work on climate and disaster response himself, he knows that scientists shy away from attributing any single weather event to global warming. But there is general agreement that warming oceans will fuel more energetic storms, he says, and extreme storms resonate in a way that scientific charts cannot. \u201cI would hope that beyond the slow climate-change negotiations,\u201d he says, \u201cour sacrifice and statement would translate into something more profound.\u201d \n               VIKTOR GROKHOVSKY: Meteorite hunter \n             \n               A Russian researcher tracked the debris from the biggest object to hit our planet in a century. \n               By Quirin Schiermeier \n             The event that made 2013 special for Viktor Grokhovsky came without any warning. The mighty meteor that fell to Earth on 15\u00a0February had approached our planet from a region of the sky that is inaccessible to ground-based telescopes, so it took astronomers by surprise. Grokhovsky, a metallurgist at the Ural Federal University in Yekaterinburg, Russia, who has studied meteorites for more than 30\u00a0years, was too far away to watch the incoming object light up the morning sky. But when he learned about a powerful explosion that had knocked people off their feet and shattered thousands of windows in the city of Chelyabinsk, he realized that something substantial had hit the planet. In the days after the impact, Grokhovsky worked feverishly to calculate the meteor\u2019s trajectory and predict where fragments might have landed. He supervised searches that unearthed more than 700 pieces of the meteor, weighing a total of 5.5\u00a0kilograms. \u201cIt was a great satisfaction when it turned out that our initial calculations had been correct,\u201d he says. But his greatest catch came later in the year. Calculations of the meteor\u2019s trajectory and a large hole in the ice of a lake to the west of Chelyabinsk had convinced Grokhovsky that the biggest single chunk had landed there. When divers finally searched the lake\u2019s muddy bottom in October, they recovered a 570-kilogram boulder. Thousands of fragments from the meteor are being analysed in labs around the world and they have already begun to reveal their secrets. \u201cIt is hard to overestimate the importance of the Chelyabinsk meteor,\u201d says Grokhovsky. Because of its size and the damage it caused, it has prompted scientists to increase the odds of similar meteors \u2014 or larger ones \u2014 striking our planet. Grokhovsky says that it was a once-in-a-lifetime experience for him. \u201cI was lucky enough to play a part in this exciting story about a space traveller\u2019s adventures on Earth.\u201d \n               HUALAN CHEN: Front-line flu sleuth \n             \n               A virologist helped China to quell an outbreak of H7N9 avian flu in humans. \n               B \n               y Declan Butler \n             In the early weeks of April, the world\u2019s virologists and public-health officials had their eyes fixed on China. An emerging avian influenza virus \u2014 H7N9 \u2014 was jumping to humans from infected poultry, causing severe disease and deaths, with new cases appearing in Shanghai and neighbouring provinces. Hualan Chen, head of China\u2019s National Avian Influenza Reference Laboratory in Harbin, found herself and her lab on the front line of efforts to contain the outbreaks. The scientists pushed all other research aside to focus on H7N9 and to find its route of transmission to humans from birds or other animals. They were so busy, Chen says, that \u201cseveral lost four to five kilograms during the [first] six weeks\u201d. Less than 48 hours after H7N9 cases were first confirmed ( S.\u00a0JianZhong  et al. Chin. Sci. Bull.   58,  1857\u20131863; 2013 ), Chen\u2019s team, along with researchers at the Shanghai Animal Disease Control Center, collected about 1,000 samples from soil, water, poultry farms and live poultry markets in Shanghai and the neighbouring province of Anhui, where the first cases had occurred. Twenty came up positive for H7N9, all from live markets in Shanghai. Authorities quickly closed live markets in the cities where most cases had been reported, and the rate of new infections immediately plummeted. China\u2019s rapid and transparent response has been widely applauded. The low rate of new cases has continued, with only a handful reported from May through to the end of November, giving Chen and other researchers time to learn more about the virus. They know that H7N9 jumps more easily to humans from birds than does another deadly avian flu virus, H5N1. So far, H7N9 has shown no sign that it can pass from person to person, but Chen believes that it may have this potential. Chen was so focused on dealing with the H7N9 outbreak that she paid little attention to a brouhaha that erupted in May over the publication of a study that she and Chinese colleagues had done describing the creation of hybrids of avian H5N1 and 2009 pandemic H1N1 flu that could spread easily between guinea pigs ( Y.\u00a0Zhang  et\u00a0al. Science   340,  1459\u20131463; 2013 ). The work was reminiscent of controversial research published last year that involved the creation of forms of avian H5N1 that were transmissible between ferrets, prompting a moratorium on similar work that was ultimately lifted in January 2013 (see   Nature 493, 460; 2013 ). Critics argued that Chen\u2019s research, like the previous H5N1 studies, had few practical benefits and that the engineered hybrids might spark a pandemic should they escape from the lab. Chen asserts that these experiments help to illustrate the threats posed by new flu strains; the H5N1 and H1N1 strains she used coexist in many countries, and she thinks that the reassortment carried out in the lab is likely to occur in nature. Similar experiments are needed for H7N9, she adds. The lull in new H7N9 cases during the summer and autumn may be the result of the initial market closures, or the fact that avian flus typically spread less frequently during warmer months. Winter has now returned to China, and Chen\u2019s lab is on the look out for any resurgence. \u201cInfluenza virus surveillance is the top priority in our lab,\u201d she says. \n               SHOUKHRAT MITALIPOV: The cloning chief \n             \n               After years of frustration, a biologist has finally developed a line of stem cells from a cloned human embryo. \n               By Erika Check Hayden \n             The hardest part of the cloning advance that Shoukhrat Mitalipov reported in May was not the experiment itself: it was the maze of red tape that came before it. Since 2007, Mitalipov, a reproductive biologist at Oregon Health & Science University in Portland, had wanted to create cells with the potential to cure any number of diseases \u2014 patient-specific stem cells made from embryos cloned from a person\u2019s own skin cells. Beyond the technical difficulties, it was a tricky endeavour because it involves harvesting lots of human eggs and using them to create embryos that will then be destroyed. He had to gain approvals from an institutional review board and a stem-cell-research oversight committee. Ultimately, his university built him a new lab, because the cells could not, by law, be derived in his original, federally funded lab space. Finally, last October, he got started; by Christmas, his lab had succeeded in cloning four cell lines by transferring nuclei into donor eggs. He says that he was able to accomplish the feat, which had long eluded the field, in part because he had spent years perfecting the procedure in monkeys. He had little competition, he says, because heavy regulation had deterred other researchers. His rush to publish led to problems, notably some duplicated and mislabelled figures in his group\u2019s paper ( M.\u00a0Tachibana  et\u00a0al. Cell    153,  1228\u20131238; 2013 ). Despite that, researchers such as Dieter Egli of the New York Stem Cell Foundation, who is trying to replicate the work, are optimistic. \u201cMy sense is that some of the major conclusions in Mitalipov\u2019s paper are likely to stand the test of time,\u201d says Egli. Mitalipov is now seeking federal approval to begin clinical trials of a similar technique, mitochondrial transfer, in women who hope to have children free of mitochondrial diseases. He is also comparing stem cells derived from cloned embryos to those derived from reprogrammed adult cells. He is scrounging for funding and collaborators because federal funders will not pay for research with the cell lines, and the California Institute for Regenerative Medicine will not fund work on lines made the way his were, with eggs harvested from paid donors. And so the work proceeds slowly \u2014 just like before. \u201cIt feels like we\u2019re at the same point where we were a year ago,\u201d he says. \n               KATHRYN CLANCY: An eye on harassment \n             \n               An anthropologist unearths disturbing trends in sexual assaults at field sites \u2014 and suspects she\u2019s just scratching the surface. \n               B \n               y Alexandra Witze \n             Kathryn Clancy loved her doctoral fieldwork in rural Poland. After long days gathering biological specimens from women in a study of reproductive health, she and her fellow scientists \u2014 all women \u2014 spent their evenings playing board games and listening to pop music. It was \u201cthe most magical field experience\u201d, says Clancy, an anthropologist at the University of Illinois at Urbana-Champaign. But years later, she learned that many anthropologists have very different memories of fieldwork. In a halting conversation over coffee, a friend revealed that she had been sexually assaulted by a colleague at a field site run by a university. Clancy decided to take action. She began in January 2012, by posting stories from her friend and others, anonymized, on her  Scientific American  blog. But she soon realized that anecdotes weren\u2019t enough. So she joined forces with three colleagues \u2014 Katie Hinde of Harvard University in Cambridge, Massa\u00adchusetts, Robin Nelson of Skidmore College in Saratoga Springs, New York, and Julienne Rutherford of the University of Illinois at Chicago \u2014 to put out a call for data. They asked biological anthropologists to share their stories of field experiences through a web-based survey. In April this year, Clancy\u2019s team dropped a bombshell. During an ethics symposium at a meeting of the American Association of Physical Anthropologists in Knoxville, Tennessee, the team announced that 59% of the 124 survey participants reported experiencing inappropriate sexual comments, and 18% reported physical harassment or assault in the field. Abuse was systemic, the team found, and frequently played out along lines of power. Young female graduate students were usually the targets; older, more senior men were usually the perpetrators. And sexual harassment was mostly committed by university personnel: postdocs and professors, not local workers hired for the season. Many respondents suggested that work in the field \u2014 removed from the norms of family and friends \u2014 may be particularly likely to elicit unwanted behaviour. (When Clancy\u2019s team later expanded the survey to include 666 respondents in all fieldwork disciplines, from archaeologists to geologists to zoologists, the results were essentially the same.) Clancy and her colleagues acknowledge that the survey may have drawn an unrepresentative number of people who have been harassed, but even so, the results shocked the anthropological community. A reaction came swiftly. Within days, the American Anthropological Association put out a statement asserting a zero-tolerance policy for sexual harassment. Other professional societies quickly followed suit with similar policies to cover additional venues, such as their professional conferences. Through their surveys, Clancy and her colleagues learned that many victims didn\u2019t report the abuse to authorities, fearing that they would be cut off from data or from access to a field site they needed. Others filed formal complaints but were told to keep quiet or \u2018just deal with\u2019 the situation. Only on rare occasions were reports of abuse followed through to the victim\u2019s satisfaction. The effects of the trauma lingered for many. \u201cEvery time you try to do your science, you\u2019re reminded of what happened,\u201d says Clancy. Some young researchers leave science because of their field experiences, she adds. \u201cThere\u2019s no doubt in my mind, we are losing very good people.\u201d Clancy, Hinde, Nelson and Rutherford are putting the final touches to a manuscript detailing the field abuse, which Clancy believes is just part of a larger problem. She has been deluged with e-mails asking her to quantify what takes place in other university settings. Meredith Hastings, a biogeochemist at Brown University in Providence, Rhode Island, and co-founder of the Earth Science Women\u2019s Network support group, says that such work is needed to shine a light on an oft-ignored problem in the research world. \u201cYou don\u2019t recognize how widespread these issues are,\u201d she says, \u201cuntil somebody speaks up.\u201d \n               HENRY SNAITH: Sun worshipper \n             \n               An energetic physicist pushes a promising solar-cell material into the spotlight. \n               By Mark Peplow \n             \u201cI always wanted to be an inventor,\u201d says Henry Snaith happily. The 35-year-old physicist at the University of Oxford, UK, has fulfilled that childhood ambition in spectacular style. This year, Snaith amazed materials researchers by massively boosting the efficiency of solar cells made with perovskite semiconductors ( M.\u00a0Liu et\u00a0al. Nature 501, 395\u2013398; 2013 ). For a few years, other researchers have used these materials to make lower-efficiency, complex photovoltaic devices, but Snaith realized that they could be harnessed in a much purer and cheaper design. They are now on the brink of commercialization. Most of the world\u2019s solar cells rely on silicon, and convert roughly 17\u201325% of the light that falls on them into electricity \u2014 almost 10\u00a0times better than the humble leaf does through photosynthesis. But their thick chunks of pure silicon make them expensive to build. Thin-film solar cells, containing leaner slivers of other semiconductors, are cheaper but generally less efficient; it has taken decades for their efficiencies to creep above 15%. Perovskite cells combine the best attributes of both. Snaith\u2019s cell \u2014 made out of methyl\u00adammonium lead iodide chloride \u2014 already has an efficiency of 15%. \u201cThe performance has very quickly got to a high level,\u201d says Richard Friend, an optoelectronics researcher at the University of Cambridge, UK, who was Snaith\u2019s PhD adviser. Through fine-tuning \u2014 changing the balance of chloride and iodide, for example \u2014 Snaith thinks that perovskite cells could rival the 29% achieved by crystalline gallium arsenide cells, which are used in many satellites but are much too expensive for widespread use. The perovskite cells are easy to make. Snaith uses simple techniques such as smearing the readily available ingredients across a coated glass plate ( S.\u00a0D.\u00a0Stranks  et\u00a0al. Science    342,  341\u2013344; 2013 ). \u201cI don\u2019t think there\u2019s a cheaper photovoltaic material,\u201d he says. \u201cAnd the stuff is fundamentally stable. We can stick these devices under a tap and they still work.\u201d His team is now working on alternative perovskites that do not contain toxic lead and thus would be easier to dispose of. Snaith chose solar cells as the focus for his research because they occupied a happy middle ground in clean energy: wind power, he reckoned, involved too much engineering, and nuclear fusion seemed too distant. Photovoltaic research was just right \u2014 plenty of room for improvement, with new discoveries delivering immediate, practical benefits. He is not alone in his quest. Michael Gr\u00e4tzel at the Swiss Federal Institute of Technology in Lausanne pioneered dye-sensitized solar cells more than 20 years ago, and is now getting spectacular results using perovskites in them ( J.\u00a0Burschka et\u00a0al. Nature 499, 316\u2013319; 2013 ). \u201cThe whole photovoltaic community is excited about it,\u201d says Gr\u00e4tzel. \u201cThe perovskites are kind of magic.\u201d Snaith did his postdoctoral research with Gr\u00e4tzel, and says that they are still very friendly \u201cbut there\u2019s certainly competition there\u201d. Solar-cell manufacturers are starting to take an interest in perovskites, but Snaith hopes he has a jump on the competition. In 2010, he co-founded Oxford Photovoltaics, which aims to incorporate transparent perovskite cells into windows. By 2017, the company hopes to glaze large buildings for not much more than the price of conventional glass. Snaith is also looking forward to his next invention. If photovoltaic power really takes off, he notes, we will need a better way to store solar power for a rainy day. \u201cOnce things calm down a bit,\u201d he says, \u201cI\u2019ll be looking to adapt our work to make better battery electrodes.\u201d \n               Ones to watch in 2014 \n             Masayo Takahashi, RIKEN Center for Developmental Biology  Induced pluripotent stem cells could get their first test in the clinic. Using cells derived from patients, Takahashi plans to create sheets of retinal cells to treat macular degeneration, a common cause of blindness. Chris Field, Intergovernmental Panel on Climate Change  As co-chair of the upcoming report on the impacts of climate change, Field hopes to avoid the errors that undercut the Intergovernmental Panel on Climate Change\u2019s 2007 report on the same topic, an area that has taken on greater urgency in the intervening years. Jean-Pierre Bourguignon, Incoming president, European Research Council  When the French mathematician replaces Helga Nowotny, he will enjoy a bigger budget but must protect the grant-giving institution from political and bureaucratic pressures. Koppillil Radhakrishnan, Chairman, Indian Space Research Organisation  India\u2019s Mars Orbiter Mission \u2014 the country\u2019s first attempt to visit that planet \u2014 is scheduled to reach orbit in September. All eyes will be on the spacecraft as it travels a route that has, more often than not, ended in failure. Gordon Sanghera, Chief executive, Oxford Nanopore  In February, customers are expected to reveal the first data collected using the UK company\u2019s MinION genetic sequencer. If it lives up to its promise, the device could usher in a new era of dramatically faster, cheaper sequencing. \n                     365 days: 2013 in review 2013-Dec-18 \n                   \n                     2013 Editors' choice 2013-Dec-18 \n                   \n                     365 days: Images of the year 2013-Dec-18 \n                   \n                     CRISPR technology leaps from lab to industry 2013-Dec-03 \n                   \n                     Crowdsourcing goes mainstream in typhoon response 2013-Nov-20 \n                   \n                     Risk of massive asteroid strike underestimated 2013-Nov-06 \n                   \n                     Exoplanet is built like Earth but much, much hotter 2013-Oct-30 \n                   \n                     End harassment 2013-Oct-22 \n                   \n                     Myriad ruling causes confusion 2013-Jun-18 \n                   \n                     Stem-cell cloner acknowledges errors in groundbreaking paper 2013-May-23 \n                   \n                     Mapping the H7N9 avian flu outbreaks 2013-Apr-24 \n                   \n                     Infant's vanquished HIV leaves doctors puzzled 2013-Mar-05 \n                   \n                     Cheaper catalyst cleans diesel-car fumes 2010-Mar-25 \n                   \n                     Nature  special: 2013 the year in science \n                   Reprints and Permissions"},
{"file_id": "504202a", "url": "https://www.nature.com/articles/504202a", "year": 2013, "authors": [{"name": "Richard Van Noorden"}], "parsed_as_year": "2006_or_before", "body": "With a serious shortage of medical isotopes looming, innovative companies are exploring ways to make them without nuclear reactors. In 2009, two nuclear research reactors shut down for repairs and maintenance. This was not surprising, given that both were around half a century old. But these reactors happened to produce most of the world's supply of the radioactive tracer technetium-99m, an isotope injected into patients in 70,000 diagnostic scans a day. Hospitals around the world went into a panic. Finding themselves suddenly short of the crucial isotope, doctors cancelled scans, postponed operations or switched to older diagnostic techniques that exposed patients to higher doses of radiation. \u201cIt was the isotope equivalent of an electricity blackout,\u201d says Ronald Schram, who manages one of the affected reactors, the High Flux Reactor at Petten in the Netherlands. Nobody knows exactly how much damage was done, says Fred Verzijlbergen, head of the department of nuclear medicine at Erasmus Medical Center in Rotterdam, but \u201cit was very serious. Many hospitals didn't receive technetium for weeks.\u201d The crash made it painfully clear that the world's medical-isotope supply chain was dangerously fragile, relying heavily on about four government-subsidized reactors built in the 1950s and 1960s. Isotope supplies have taken a hit again and again, most recently last month, when Canada's Chalk River reactor shut down unexpectedly for a few days at the same time as two other reactors. And more shortages are coming. The Chalk River reactor, which produces close to one-third of current global supplies, is slated to end production of isotopes in 2016. Richard Van Noorden takes a look at the looming shortage of isotopes for medical scans,. But for nuclear engineer Greg Piefer, the crisis presents an opportunity. In 2005, fresh out of a nuclear-engineering doctorate at the University of Wisconsin\u2013Madison, he had dreamed up a way to use particle accelerators \u2014 rather than nuclear reactors, with their problematic waste \u2014 to transform uranium into technetium. His idea did not get much attention at the time. After the 2009 disaster, however, politicians demanded new ways of making medical isotopes, particularly in the United States, which accounts for 50% of world medical-isotope demand but has no local production capacity. Piefer's ideas, and those of other aspiring entrepreneurs, were thrown into the spotlight. \n               Technical challenge \n             At least five North American companies and collaborations, including Piefer's firm, SHINE Medical Technologies in Madison, are pioneering methods that should produce medical isotopes in the next few years. It is not clear which will win out \u2014 nor whether they can replace the conventional reactor approach or be ready soon enough to avert another shortage. \u201cIt's boiling down to a rather critical situation in 2015\u201316,\u201d says Schram. Technetium-99m is often called the workhorse of modern medical imaging, because it accounts for about 80% of the world's use of radioactive isotopes in nuclear medicine, 90% of which is diagnostic scans. A \u03b3-ray emitter with a half-life of just 6 hours, it can be attached to a molecule that targets the organ of interest. Medical scans known as single-photon emission computed tomography (SPECT) then pick up the glow of the radioisotope. Such tests are used to check how well blood is flowing to heart muscles, to spot whether cancers have spread through bones and to assess blood flow in the brain. The creation of technetium-99m involves one of those miraculous, globe-crossing supply chains that modern economies have rendered commonplace. The journey starts with enriched uranium from the United States, which is made into plates and shipped to research reactors around the world. Each plate is baked for a week in the glare of a nuclear reactor's neutrons, which fission about 6% of the uranium into molybdenum-99. This has a half-life of 66 hours and slowly decays into technetium. Hospitals across the globe purchase 'moly cows' \u2014 paint-tin-sized devices that hold the molybdenum-99 bound tightly to alumina. By flushing the technetium out with saline solution, hospital technicians can milk the moly cows for fresh supplies of technetium for up to two weeks. Nuclear reactors are the most efficient way to produce molybdenum, says Benard Ponsard, who manages the isotope-producing BR2 reactor in Mol, Belgium. Many others agree. In the 1990s, Canada planned new reactors that would have circumvented many of the shortages, but these were mothballed in 2008 after technical problems emerged that proved too expensive to fix. So other countries are now racing to fill the gap. BR2 aims to start upgrading its medical-isotope capacity at the end of 2014. The OPAL reactor in South Sydney, Australia, is planning upgrades that would quadruple its isotope supplies by 2017. New reactors or upgrades are planned from Argentina to China (see 'Supply fix'). This promises a lot of production capacity. But there could still be problems. The new reactors might not supply enough isotopes to provide a sufficient cushion in case of major breakdowns, notes Robert Atcher, director of the US National Isotope Development Center, created in 2009 by the Department of Energy to help to manage isotope distribution. \n               Price hikes \n             More crucially, the cost of reactor-sourced molybdenum could skyrocket. Because the reactors are involved in research, they are subsidized by their host governments and sell their molybdenum at below-market prices. This means there is little incentive for companies to invest in new production facilities, concluded a post-crisis review by the Nuclear Energy Agency (NEA) of the Organisation for Economic Co-operation and Development (OECD) in Paris. The NEA has endorsed a plan to end the subsidies, which those in the supply chain are now preparing for. According to Ron Cameron, head of nuclear development for the agency, the price of molybdenum from reactors could increase as much as sevenfold when that happens. Meanwhile, the United States has decided to stop exporting highly enriched uranium, because it might be intercepted to make nuclear weapons. By 2020, reactors will have to make do with low-enriched uranium fuel and plates \u2014 which could increase molybdenum costs from reactors by another 40%. The final price to the middle-man is \u201cvery much up in the air\u201d, says Atcher, who guesses it might increase 15-fold. That has led some to seek a radical alternative. Rather than depending on a few distant centres of production, each costing hundreds of millions of dollars, hospitals could get their isotopes locally from facilities that have small medical cyclotrons costing just a few million dollars, says Paul Schaffer, head of the nuclear medicine division at TRIUMF, Canada's national laboratory for particle and nuclear physics in Vancouver, and leader of a team pursuing this idea. In the cyclotron model, neither nuclear reactors nor uranium are needed. A beam of accelerated protons shoots into a target of molybdenum-100, creating technetium-99m directly. Technetium's 6-hour half-life means that the product cannot be transported far: a single cyclotron could cover maybe a 400-kilometre radius. The idea is to have lots of cyclotrons distributed across major urban areas. That is not as ambitious as it might seem, points out TRIUMF spokesman Tim Meyer: many hospitals already use in-house cyclotrons to produce isotopes for a more advanced form of imaging, positron emission tomography. In June, the TRIUMF team announced that by running an upgraded cyclotron in Vancouver overnight, it could make enough technetium to satisfy the city's needs. \u201cThe dozen or so cyclotrons [already] in Canada could cover 90% of the population and 50% of the geography\u201d when adapted with a TRIUMF upgrade kit, says Meyer. The team awaits approval from Health Canada, which will confirm that its technetium is safe for use. Schaffer says that cyclotrons will not produce enough technetium to supply all of Canada's needs by 2016. \u201cBut a decentralized supply is certainly possible in the long term,\u201d he says. Other countries seem to be interested in the strategy. Advanced Cyclotron Systems \u2014 a firm in Richmond, Canada, that sells cyclotrons and is working to make cyclotron technetium \u2014 has had inquiries from the United Kingdom, Saudi Arabia, Thailand and more. \u201cIt's got health authorities in many countries quite excited,\u201d says John Taylor-Wilson, the company's vice-president of marketing and business development. Atcher, however, is not convinced that the approach will be helpful in the United States, where, by chance, most hospitals have lower-power cyclotrons that could not produce as much technetium. And if a cyclotron goes down for repair, an urban area might be left without a back-up plan. \n               Bright ideas \n             Piefer has a plan that is a little less radical than the cyclotron model. His team at SHINE wants to stick with the current distribution system, but get rid of the expensive nuclear reactor at the system's heart. SHINE's technology uses a linear accelerator to slam deuterium ions into tritium gas, producing helium and neutrons. The neutron flux is orders of magnitude less than that emerging from a nuclear reactor. Instead of a small uranium plate, the neutrons are fired into a couple of hundred litres of warm, low-enriched uranium salts. The molybdenum can be rinsed away using ion-exchange resins, and the unconverted uranium recycled for use in the same facility. Nuclear waste is produced from the uranium target, but it is only a small fraction of that produced from reactor fuel, notes Piefer. Piefer hopes to build a facility in Janesville, Wisconsin, which he says would supply half of the United States' need for technetium (about one-quarter of world demand). But construction has not started yet, because the firm \u2014 like any US company working with uranium \u2014 has to wait for a permit from the Nuclear Regulatory Commission. And it needs to raise another US$150 million, on top of $30 million raised so far from a Department of Energy grant and investors. \u201cIf sufficient funding were available, we'd start production by the end of 2016,\u201d Piefer says \u2014 about the same time the Chalk River reactor ends its supply. Piefer will face some competition from right across town. Madison is home to another innovative medical-isotope company: NorthStar, which also hopes to supply half the United States' medical isotope needs in the next few years \u2014 and in the longer term, aims to double that output with a second project. The fact that it has ended up so close to SHINE is \u201ca bizarre coincidence\u201d, says Piefer; the rivals are \u201ccordial but competitive\u201d, he adds. Whereas SHINE avoids using a nuclear reactor but keeps a uranium target, NorthStar aims to do the opposite. It ditches the uranium target but \u2014 at least at first \u2014 still makes use of a reactor. The company's short-term plan is to use the research nuclear reactor at the University of Missouri in Columbia to fire neutrons into a molybdenum-98 target, making molybdenum-99. This would be very quick to get up and running, but it invites complications on another front. Rather than being able to separate molybdenum-99 from the surrounding uranium, which is relatively easy, the company has the trickier task of separating the desired isotope from the molybdenum starting material. So NorthStar has had to design a generator to replace the moly cow. The result, about the size of a microwave oven, requires a computer, pipes and valves to extract technetium \u2014 more complex than the simple saline wash that hospitals use today. In March, NorthStar submitted a New Drug Application to the US Food and Drug Administration (FDA), seeking to prove that its milked technetium is equivalent to what comes out of moly cow generators. It is hoping to receive the final word by the end of the year, says James Harvey, the company's chief science officer. \u201cThe Missouri project will be fully in production by mid-2014,\u201d Harvey says. In the long-term, NorthStar has a more ambitious plan that would again cut reactors out of the system. It plans to use high-energy photons from a linear accelerator to kick neutrons out of molybdenum-100 to produce molybdenum-99. (Prairie Isotope Production Enterprises, a non-profit firm based in Winnipeg, Canada, is looking at a similar system, but on a smaller scale.) Again, NorthStar would need to use its microwave-sized hospital generator, assuming that it gets FDA approval. And although the company has raised $50 million already, it will need much more to get this scheme off the ground: Harvey will not reveal a figure, but says it is a lot less than $150 million. Atcher has doubts about both NorthStar and SHINE. \u201cIn a nutshell, both of these companies are start-up companies,\u201d he says, adding, \u201c2016 is not that far away and they are scrambling to get their programmes going.\u201d \n               Market forces \n             Underlying all the jostling are questions to which no one has good answers: how expensive the technetium from these new technologies will be, and whether the schemes will create enough to replace the nuclear-reactor approach. \u201cObviously, each of these competitors has a secret recipe where they think they can beat the odds,\u201d says Meyer. But the uncertainty in the economics scared away larger companies such as General Electric and Babcock and Wilcox, both of which initially showed an interest in developing medical-isotope schemes but backed out last year. Atcher thinks that reactors will always come out ahead. Others see a more diverse future. \u201cThe long-term scenario will really be driven by the market,\u201d says Schaffer. \u201cI equate it to the electricity market, where we have nuclear, wind, hydroelectric, solar and so on. And the price of that source of electricity pretty much defines its share of the market. I believe the same thing will happen with isotopes, with sources from cyclotrons, linear accelerators and nuclear reactors.\u201d So can hospitals avert catastrophe in 2016, when the Chalk River production facility shuts down? \u201cOn paper, it looks like the world can compensate,\u201d says Schaffer. But, he adds \u201cit's such a dynamic situation\u201d. Cameron, more soberly, says that the answer will not be clear until he sees which reactors and companies ramp up production. \u201cWe have to do a lot of sums to see how the effects will balance out.\u201d All the uncertainty about the technologies is leaving doctors such as Verzijlbergen concerned. \u201cThere is a lot of optimism but we need proof,\u201d he says. \u201cFrom the medical side I am interested in reliable supply.\u201d He still worries that in a few years he will have to go to patients with difficult news: that a widespread shortage of technetium means that they cannot have the diagnostic tests that they need. \n                     Souped-up cyclotrons offer isotope remedy 2012-Feb-21 \n                   \n                     Medical isotope supplies dwindle 2010-Feb-12 \n                   \n                     Medical isotope shortage reaches crisis level 2009-Jul-15 \n                   \n                     Researchers urge action on medical-isotope shortage 2009-Jun-24 \n                   \n                     Accelerating production of medical isotopes 2009-Jan-28 \n                   \n                     Blog post: Canadian accelerator produces a city's-worth of medical isotopes overnight \n                   \n                     SHINE \n                   \n                     NorthStar \n                   \n                     TRIUMF \n                   \n                     Advanced Cyclotron Systems \n                   \n                     PIPE \n                   Reprints and Permissions"},
{"file_id": "503454a", "url": "https://www.nature.com/articles/503454a", "year": 2013, "authors": [{"name": "Ed Yong"}, {"name": "Heidi Ledford"}, {"name": "Richard Van Noorden"}], "parsed_as_year": "2006_or_before", "body": "Reporting suspicions of scientific fraud is rarely easy, but some paths are more effective than others. Are more people doing wrong or are more people speaking up? Retractions of scientific papers have increased about tenfold during the past decade, with many studies crumbling in cases of high-profile research misconduct that ranges from plagiarism to image manipulation to outright data fabrication. When worries about somebody's work reach a critical point, it falls to a peer, supervisor, junior partner or uninvolved bystander to decide whether to keep mum or step up and blow the whistle. Doing the latter comes at significant risk, and the path is rarely simple. Some make their case and move on; others never give up. And in what seems to be a growing trend, anonymous watchdogs are airing their concerns through e-mail and public forums. Here,  Nature  profiles three markedly different stories of individuals who acted on their suspicions. Successful or otherwise, each case offers lessons for would-be tipsters. \n               The analytical \n             Uri Simonsohn sees himself as more of a data-whisperer than a whistle-blower. His day job as a social scientist at the University of Pennsylvania in Philadelphia involves scouring archival data \u2014 from house prices to auction records to college admissions \u2014 as part of his research into judgement and decision-making. He suspects that this background has predisposed him to catching spurious patterns in other psychologists' results. \u201cWith an experiment, you do a  t -test and move on,\u201d he says. \u201cBut people who work with archival data are used to looking at data very carefully.\u201d It was this intuition that stirred when he first came across papers by Dirk Smeesters at Erasmus University Rotterdam in the Netherlands and Lawrence Sanna at the University of Michigan in Ann Arbor in the summer of 2011. In both cases, the data seemed too good to be true, containing an overabundance of large effects and statistically significant results. In one of Sanna's papers, Simonsohn noticed that one experiment \u2014 in which volunteers were supposedly split into different groups \u2014 produced results with uncannily similar standard deviations. In the results of Smeesters' studies, he saw a suspiciously low frequency of round numbers and an unusual similarity between many of the averages. \u201cIf there's too little noise, and the data are too reliable again and again, they cannot be real,\u201d he says. \u201cReal data are supposed to have error.\u201d Simonsohn checked his suspicions by simulating experiments thousands of times to show how unlikely the reported results actually were. He replicated his analyses on other papers by the same authors and found the same patterns, and he carried out negative controls, showing no suspicious patterns in the work of other psychologists who used the same set-ups. Simonsohn contacted both authors and spent months systematically ruling out alternative explanations for the discrepancies he found. Eventually, according to Simonsohn, only one remained \u2014 that they had manipulated their data. He still refrained from accusing anyone, liaising privately with Smeesters, Sanna and their co-authors, asking for raw data, outlining his concerns and asking if another party, such as a student or research assistant, could have tampered with the data. \u201cI was extremely open-minded,\u201d he says. \u201cMy working hypothesis was that it's not in your interest to fake if you're a researcher.\u201d Towards the end of 2011, Simonsohn learned that Erasmus University, which he had contacted, had begun an investigation. He also found out that because of his inquiries, the University of North Carolina at Chapel Hill, where Sanna had performed his work, had also started to investigate. By the summer of 2012, both Smeesters and Sanna had resigned from their posts, and several of their papers have since been retracted. In previous statements, Smeesters has said that he never fabricated data and that the practices he used are common in his field; he chose not to provide a further comment when contacted by  Nature . Neither Sanna nor his former institution have publicly addressed questions about his resignation and Sanna could not be reached for comment. When asked about the two careers that have been broken by his investigations, Simonsohn pauses. \u201cI don't feel bad about it,\u201d he concludes. \u201cIf I'm going to the same conferences as these people, and publishing in these journals, I can't just look the other way.\u201d Joe Simmons, a psychologist at the University of Pennsylvania, says that he admires his colleague's integrity and sense of obligation. \u201cHe couldn't not do something,\u201d he says. Simonsohn hopes that his actions will spur psychologists to instigate reforms to stem fraud \u2014 one option would be to require researchers to post raw data, thereby making them more open to checks by watchful data-sleuths. He also wants researchers to disclose more details of their work at the outset of an experiment, such as the variables to analyse or their planned sample sizes. That would discourage subtler forms of data-tampering \u2014 such as continuing experiments only until results meet significance \u2014 which, in his opinion, flood the psychological literature with false positives (see  Nature   485 , 298\u2013300; 2012 ). Simonsohn's whistle-blowing attracted its share of attention. He has received around a dozen offers to look into suspected cases of dodgy data, typically from people outside science who have personal concerns about, say, the US election. He rarely replies. He has little interest in being drawn into unnecessary disputes and bristles at any suggestion that he has led a witch-hunt \u2014 a term that he associates with the wanton use of poor diagnostic tests, not his own careful review. \u201cSome people think he does it for the fame, but he finds the fame annoying,\u201d says Simmons. Simonsohn, for his part, says he hopes that his new-found identity as a whistle-blower will morph into a different label, as \u201ca person who looks carefully at data. I would be very happy with that reputation,\u201d he says. \n               The quixotic \n             Helene Hill thought she was nearing retirement in 1999 when, one day, she decided to take a peek at a lab mate's culture dishes. A radiation biologist at the University of Medicine and Dentistry of New Jersey in Newark, Hill was collaborating with a junior colleague on a project to study the 'bystander effect', a phenomenon whereby cells exposed to an agent \u2014 in this case radiation \u2014 influence the behaviour of unexposed neighbours. Hill had trained the postdoc, Anupam Bishayee, on the technique and wanted to see how he had fared. The plates, she says, were empty, yet Bishayee later reported cell counts from them. Hill would spend the next 14 years trying to expose what she believes to be a case of scientific misconduct. University panels, the US Office of Research Integrity (ORI), and two courts of law have evaluated and dismissed her concerns. Her journey has cost her thousands of dollars in legal fees and countless hours trawling through more than 30,000 documents. And it could cost her her job. Yet Hill, now 84, has no intention of backing off. \u201cA person has an obligation to do the right thing if they can,\u201d she says. After the first observation, Hill and another postdoc decided to covertly shadow Bishayee's experiments, snapping photos of his cultures in the incubator. When Bishayee reported data from an experiment that they thought was contaminated with mould, Hill and her colleague accused him of fabricating the results and took their concerns to the university's committee on research integrity. But their case soon frayed. Under questioning, her colleague acknowledged that he had moved Bishayee's culture tubes before taking photos of them, which the committee viewed as potentially tampering with the evidence. And Hill explained that she had used a microscope that she was unfamiliar with when checking Bishayee's cultures. The committee determined that Hill did not have enough evidence to prove her case. Hill would not let the matter lie. Bishayee had published his results in a paper that lists Hill as a co-author (A. Bishayee  et al .  Radiat. Res.   155 , 335\u2013344; 2001 ) and his adviser, Roger Howell, used Bishayee's data to support a grant application to the National Institutes of Health (NIH) in 1999. Hill took the case to federal investigators at the ORI, who conducted a small statistical analysis of Bishayee's data. Hill says that in her opinion the patterns therein suggested fabrication, and one ORI investigator, Kay Fields, thought the case had merit. But Fields was overruled by a superior, in part because he believed that the control data for the analysis \u2014 Hill's own \u2014 were also statistically questionable. The ORI determined that there was insufficient evidence to prove misconduct. Hill continued to petition her university and the ORI to review the data. Fields, meanwhile, says that she felt obliged to tell Hill about another option: a ' qui tam ' lawsuit. Such lawsuits, allowed under the US False Claims Act, can be brought by any citizen to aid the government in recouping taxpayers' funds allocated under false pretences. Hill's case could be eligible because of the NIH grant. Qui tam  can be a risky strategy, says David Lewis, director of the research misconduct project at the nonprofit National Whistleblower Center in Washington DC. He has filed two  qui tam  lawsuits in the past, unrelated to Hill's (see  Nature   453 , 262\u2013263; 2008 ). Both were unsuccessful, and Lewis generally doesn't recommend the strategy. In Hill's case, the process dragged on for years and cost her US$200,000 in legal fees. \u201cI don't think my children are too happy with my having lost that much money,\u201d she says, \u201cbut I just felt I had an obligation to see it through.\u201d New Jersey District Court judge Dennis Cavanaugh ruled in favour of Bishayee and Howell in October 2010, and referred to Hill's battle as \u201ca quest of Quixotic proportions that ultimately must be put to rest\u201d. Hill lost her final appeal in October 2011. Still, she says that her investment paid off: the discovery phase of the lawsuit allowed her access to ten years' worth of the Howell lab's notebooks. With those data in hand, she teamed up with statistician Joel Pitt of Georgian Court University in Lakewood Township, New Jersey. Together, they pored over data that Bishayee had hand-recorded from a machine that counts cells. The duo also gathered larger control data sets from others who had used the same machine. Pitt looked at the frequency of the numbers appearing as the least significant digit of each recorded count. These should have a random distribution, but Bishayee's data seemed to favour certain numbers. Pitt calculated the odds of those frequencies arising by chance as less than 1 in 100 billion. In Hill's view, the implication is clear: Bishayee made the numbers up. Along with Pitt, Hill has been trying, so far unsuccessfully, to publish these statistical analyses and further publicize her allegations, actions that Robert Johnson, the dean of her institution \u2014 now part of Rutgers University \u2014 warned in a strongly worded letter in July could lead to \u201cadditional disciplinary action, up to and including termination\u201d. Howell, in a written statement to  Nature , expressed frustration at the time spent revisiting the issue despite no finding of wrongdoing. Bishayee did not respond to  Nature 's request for comment. Fields says: \u201cI admire Dr Hill for the courage of her convictions, but it is difficult to say that she was prudent to pursue the case for so long and at such expense.\u201d Hill, for her part, remains undeterred. \u201cI want to finish,\u201d she says. \u201cIt becomes almost an obsession.\u201d \n               The anonymous \n             Anonymous tipsters are nothing new. But since 2010, someone going by the pseudonym 'Clare Francis' has seriously upped the ante. She or he (or they; many suspect it is a group of people) has sent hundreds of e-mails to life-science journal editors, flagging up suspected cases of plagiarism or instances in which figures appear to be manipulated or duplicated. Her terse, sometimes cryptic complaints have resulted in a handful of retractions and corrections, but editors have felt bombarded by her voluminous notices \u2014 many of which, they say, lead nowhere. Like her or not, Francis has sparked a debate about how editors deal with anonymous tips, which are now poised to grow thanks to the proliferation of websites that allow anyone to publicly air grievances about research papers. Sabine Kleinert, a senior executive editor at  The Lancet  and former vice-chair of the UK-based Committee on Publication Ethics (COPE), calls the recent surge in anonymous comments \u201cthe Clare Francis phenomenon\u201d. Phenomenon is an apt descriptor. Francis estimates that she has e-mailed \u201cabout 100\u201d different editors. And those publishers who agreed to talk to  Nature  said that their editors generally receive multiple messages from her. Diane Sullenberger, executive editor of the  Proceedings of the National Academy of Sciences , says that as many as 80% of the allegations they receive come from Francis. And the scientific publisher Wiley says that in 2011 Francis's name was on more than half of its investigation requests. Anonymity generally makes people uncomfortable, says Ulrich Brandt, editor-in-chief of  Biochimica et Biophysica Acta . \u201cOne has to wonder about the motivation of the whistle-blower,\u201d he says. \u201cIll-founded allegations of scientific misconduct can do harm and may constitute a form of scientific misconduct themselves.\u201d By 2011, editors were growing increasingly frustrated by Francis because \u2014 quite apart from her anonymity \u2014 many of her claims did not check out. \u201cI have no problem taking time to look at an allegation \u2014 but I don't like people wasting my time,\u201d says Eric Murphy, editor-in-chief of  Lipids . Moreover, many of Francis's complaints are oblique and hard to follow, says Sullenberger. \u201cIt is helpful to know specific details about the concerns from a scientific standpoint, not just, 'The bands in the 10- and 60-minute lanes are geometrical and superimposable' or 'Background is silvery smooth',\u201d she says, referring to some of Francis's e-mails. Some journal editors have warned Francis that they are less likely to follow up on her requests than other complaints. In September 2011, Wiley's then legal director, Roy Kaufman, sent her an e-mail saying that the company could \u201cnot guarantee that all anonymous allegations sent to us will be investigated\u201d. Francis made the note public, sparking debate over how such allegations should be handled. Two years on, the attitudes of editors have changed to some degree. In February this year, COPE put out guidelines on \u201cresponding to anonymous whistle blowers\u201d. Francis was not mentioned by name, but was the main driving force behind the work, says Virginia Barbour, COPE's current chair. \u201cEditors were feeling guilty, and upset, and didn't understand how they should approach it.\u201d COPE reminded them that, no matter where they came from, \u201call allegations \u2026 that have specific, detailed evidence to support the claim should be investigated\u201d. But Anna Trudgett, editorial director at the journal  Blood , says that the journal still addresses Francis's e-mails only selectively. \u201cNot all anonymous correspondence is treated the same way,\u201d she says. Wiley has adjusted its practice to investigate all complaints, says spokesperson Helen Bray. Fundamentally, editors are not just reacting to Clare Francis's pseudonymity. They are also irritated by the way she works. \u201cFor some, it's not that Clare Francis is a pseudonym; it's that the pseudonym is Clare Francis,\u201d says Tom Reller, a spokesperson for Elsevier. Some editors bring up what they say is Francis's aggressive tone and pursuit of lost causes. \u201cWhen we determine that the allegation is not founded, it is not uncommon for Clare Francis not to accept the result,\u201d says V\u00e9ronique Kiermer, Nature Publishing Group's executive editor. In Barbour's view, Francis's tactics are not a good model for other anonymous tipsters to emulate. To make up for the inevitable loss of trust that comes from being anonymous, tip-offs gain credence if they are precise, detailed and polite. Francis sometimes meets these standards but often does not. To Francis, such critiques miss the point. Asked about her tone, she wrote back: \u201cI do not have a 'tone'. I try to describe what I can see.\u201d She adds that editors often focus narrowly on their journal when she sends what she says are connected patterns of image manipulation across many journals. \u201cThey will not look at the whole picture, but remain in purdah,\u201d she writes. As for alleged false leads, she says: \u201cThe hit rate would be higher if they paid attention to what is on the page rather than their fantasy world.\u201d One thing that editors and Francis might agree on is that anonymous whistle-blowing is likely to increase, given the increased access to papers by people all around the world and the availability of online tools for spotting potential plagiarism and image manipulation. One site, called PubPeer, is already becoming a venue for anonymous comments \u2014 including postings in a similar vein to Francis's style. The growth here is a sign that whistle-blowers are not being protected enough within the academic environment, says Kleinert. \u201cThis is where we have to do much more. Somebody should feel comfortable to be able to raise issues without fearing retaliation or damage to their own career\u201d. \n                     Misconduct is the main cause of life-sciences retractions 2012-Oct-01 \n                   \n                     The roots of research misconduct 2012-Aug-01 \n                   \n                     The time is right to confront misconduct 2012-Aug-01 \n                   \n                     The time is right to confront misconduct 2012-Aug-01 \n                   \n                     The data detective 2012-Jul-03 \n                   \n                     Science publishing: The trouble with retractions 2011-Oct-05 \n                   \n                     Uri Simonsohn \n                   \n                     Helene Hill \n                   \n                     Pubpeer.com \n                   Reprints and Permissions"},
{"file_id": "503330a", "url": "https://www.nature.com/articles/503330a", "year": 2013, "authors": [{"name": "Megan Scudellari"}], "parsed_as_year": "2006_or_before", "body": "Human papillomavirus is causing a new form of head and neck cancer\u2014 leaving researchers scrambling to understand risk factors, tests and treatments. On a sunny day in 1998, Maura Gillison was walking across the campus of Johns Hopkins University in Baltimore, Maryland, thinking about a virus. The young oncologist bumped into the director of the university's cancer centre, who asked politely about her work. Gillison described her discovery of early evidence that human papillomavirus (HPV) \u2014 a ubiquitous pathogen that infects nearly every human at some point in their lives \u2014 could be causing tens of thousands of cases of throat cancer each year in the United States. The senior doctor stared down at Gillison, not saying a word. \u201cThat was the first clue that what I was doing was interesting to others and had potential significance,\u201d recalls Gillison. She knew that such a claim had a high burden of proof. HPV was known to cause cervical cancer and small numbers of genital cancers, but no other forms. So Gillison started a careful population study comparing people with cancer to healthy individuals. Over seven years, she recruited 300 participants, collected tissue samples, and never once looked at the data. \u201cMy policy, when doing a study, is that we wait until all the data are in, and do all the analyses at once,\u201d says Gillison, who is as careful as she is blunt. \u201cI don't know anything until the data tell me.\u201d Only in 2005 did Gillison finally sit down with a doctoral student to analyse the data. Within an hour, the fruits of those years of labour popped up on the computer screen: people with head and neck cancer were 15 times more likely to be infected with HPV in their mouths or throats than those without 1 . The association backed up some of Gillison's earlier work, which showed 2  how HPV DNA integrates itself into the nuclei of throat cells and produces cancer-causing proteins. Gillison leapt from her chair and began jumping up and down. \u201cThe association was so incredibly strong, it made me realize this was absolutely irrefutable evidence,\u201d she says. Since then, she and a network of other researchers have amassed a mountain of evidence that HPV causes a large proportion of head and neck cancers, and that these HPV-positive cancers are on the rise. The finding has been \u201ca paradigm-shifting realization in the field\u201d, says Robert Ferris, chief of the division of head and neck surgery at the University of Pittsburgh Cancer Institute in Pennsylvania. The medical community is struggling to come to grips with the implications. There is currently no good screening method for HPV-caused cancer in the head and neck, and commercially available HPV vaccines are still prescribed only to people under the age of 26, despite evidence that they could prevent head and neck cancer in all adults. Plus, if HPV can get into the mucous membranes of the mouth and throat, where does it stop? There are hints that HPV is a risk factor for other, even more common, types of cancer, including lung cancer. For now, researchers and doctors need to learn more about how HPV causes cancer, and how best to prevent and treat it, says Gillison. \u201cOur clinics are flooded\u201d with head and neck cancers triggered by HPV, she says, vexation clear in her voice. \u201cBut though I talk about it constantly in public settings and the lay press, it amazes me that it's often as if no one has heard of it.\u201d \n               New threat \n             James Rocco, director of head and neck molecular oncology research at Massachusetts General Hospital in Boston, remembers the first signs that something was changing. Until the late 1990s, most cases of cancer in the back of the throat (the oropharynx) could be blamed on alcohol and tobacco use: the majority of Rocco's patients were men around 50 years old, who had been smoking and drinking for 30 years. But then 40-year-old marathon runners and people in otherwise good health began to trickle \u2014 then stream \u2014 into his office. And when treated with chemotherapy and radiation, these people seemed to have better survival rates than the other head and neck cancer patients. There were also irregularities in the laboratory. When biopsied, the site of the cancer was slightly different in this healthier cohort: instead of beginning on the surface of the tonsil as normal, tumours seemed to start deep in tonsil crevices. And more and more of the tumours lacked mutations in a protein called p53 \u2014 then considered a hallmark of oropharyngeal cancer. \u201cWe kind of knew we were dealing with something different,\u201d recalls Rocco. Gillison started pursuing the issue in 1996, after a passing comment by a colleague. Keerti Shah, a molecular microbiologist at the Johns Hopkins Bloomberg School of Public Health, had mentioned research in Finland that had identified HPV in a cell line developed from an oropharyngeal tumour 3 . As Shah and Gillison walked around campus one day, they talked about the finding. Was it an isolated case? Had HPV contaminated the sample? Or, as Shah suspected, could HPV cause some cases of head and neck cancer? Gillison went straight to her office to do a literature search. She began analysing tumour samples from the Head and Neck Cancer Center at Hopkins and found HPV in about 25% of them. She used multiple techniques to be sure that positive results were not attributable to laboratory contamination. She looked for the virus in early, middle and late stage tumours. HPV was not just present; she found that its DNA had infiltrated the tumours and was producing two potent oncoproteins, an indication it was the cause of the cancer. Gillison also profiled people with HPV to learn about the cancer's clinical characteristics, and identified molecular biomarkers that were absent in tumours without HPV. She worked on the project for 18 months, without taking a day off. She, Shah and their colleagues published their results in 2000 (ref.  2 ), demonstrating that HPV-positive oropharyngeal cancer is a distinct type of cancer that starts deep in the tonsils, has HPV DNA present in the tumour-cell nuclei but not neighbouring cells, has fewer p53 mutations than HPV-negative cancer, has less association with smoking and alcohol consumption and has better survival rates. But many oncologists were sceptical: some suspected that HPV was just a passenger virus, or that its presence was the result of contamination. Others thought that HPV might be just a risk factor, rather than a cause, for head and neck cancer \u2014 one of several ingredients, including drinking and smoking, that when combined together congealed into a cancerous stew. In 2007, Gillison published her seven-year population study showing the link between oral HPV infection and oropharyngeal cancer 1 ; the next year, she released a study 4  showing that HPV-positive and HPV-negative oropharyngeal cancers had completely different risk profiles. People with HPV-positive cancer tended to have had many oral-sex partners, but there was no statistical association with tobacco smoking or drinking; those with HPV-negative cancers were heavy drinkers and cigarette smokers but there was no association with sexual activity. \u201cThese were two completely different diseases,\u201d says Gillison. \u201cThey might superficially look similar \u2014 a patient comes in with a neck mass and their throat hurts \u2014 but I realized what drove the pathogenesis was completely different in the two cases.\u201d By then, all doubts had faded. In 2007, the World Health Organization's International Agency for Research on Cancer in Lyons, France, declared that there was sufficient evidence to conclude that HPV causes a subset of oropharyngeal cancers. Gillison's research has been \u201cdefinitive\u201d, says Jeffrey Myers, director of head and neck surgery research at the University of Texas MD Anderson Cancer Center in Houston. Community acceptance came not a moment too soon. The number of oropharyngeal cancers has been growing over the past 30 years: there are now 10,000 cases in the United States each year, a number that is likely to climb to 16,000 by 2030 (see 'Emerging threat'). An overwhelming majority are caused by HPV. Worldwide, cancer centres report that the virus is responsible for between 45% and 90% of oropharyngeal cancers . \u201cIn Europe, HPV-positive oropharyngeal cancers have almost quadrupled in number over a period of 10 to 15 years,\u201d says Hisham Mehanna, director of the Institute of Head and Neck Studies and Education at the University of Birmingham, UK, who has published a meta-analysis 5  of more than 250 papers on prevalence rates. \u201cOur projection suggests that it's going to continue to increase significantly.\u201d Why rates are escalating is unknown, although one suggestion points to increasing numbers of sexual partners. \n               Problem proteins \n             It turns out that HPV causes throat cancer in much the same ways as it causes cancer in the cervix. The virus's DNA integrates into human DNA in the nuclei of healthy cells, and uses the cells' machinery to produce two harmful proteins, E6 and E7. These bind to, and shut down, two important tumour-suppressor proteins, p53 and pRb. Active pRb prevents excessive cell growth; without it, cells proliferate unchecked. Active p53 arrests the cell-division cycle when DNA is damaged, and then either activates DNA repair or initiates cell death. Without p53, a cell replicates wildly even if it has DNA damage. In cancers caused by HPV, the virus silences p53 but leaves the gene that produces it intact; by contrast, in HPV-negative cancers, the gene is mutated, probably through exposure to carcinogens, and produces an ineffective version of the protein. This may explain why people with HPV-positive oropharyngeal cancer respond better to treatment: early evidence suggests 6  that chemotherapy or radiation may somehow reactivate p53 in HPV-positive cancers, turning the powerful protein back on to fight the tumour. There are other possibilities. It could be that people with HPV-positive cancer are generally healthier than their HPV-negative counterparts: they tend to be younger, generally don't smoke and are more likely to comply with treatment regimes. Another possibility, supported by a study 7  using sequencing data from 74 head and neck cancers, is that HPV-negative tumours are more heterogeneous than HPV-positive tumours. The cells have many more mutations, and a wider range of them. In an HPV-negative tumour, therefore, \u201cthere's more likely to be something in there that will resist therapy\u201d, says Rocco, a co-author of the study. \n               Toxic treatment \n             The fact that people with HPV-positive cancer have better outcomes has caused many clinicians, including Gillison and Ferris, to wonder whether these patients should get different treatments. The current standard therapy for oropharyngeal cancer is a combination of cisplatin \u2014 a toxic, potent chemotherapy drug \u2014 and radiation. This has many potential side effects, including damage to the voice box and throat, which can hinder the ability to speak and swallow. With the younger, healthier HPV-positive patients, who are 58% less likely to die within three years of treatment than HPV-negative patients, clinicians worry about the long-term effects of the treatment, and are exploring techniques including less-toxic chemotherapy regimens. Researchers are also looking at ways to prevent the disease in the first place. More than 90% of HPV-related oropharyngeal cancers are caused by HPV-16, a particularly dangerous strain and the main cause of cervical cancer. The two vaccines approved to prevent cervical cancer, Merck's Gardasil and GlaxoSmithKline's Cervarix, both protect against HPV-16. In theory, therefore, protection against HPV-positive oropharyngeal cancer is already in doctors' cabinets. A clinical trial of 5,840 women, published this year by researchers at the US National Cancer Institute 8 , showed that Cervarix is 93% effective at preventing oral HPV infection in both women with pre-existing cervical infections and those without, none of whom had been previously vaccinated. A major barrier stands in the way of official approval for using the vaccine to protect against oropharyngeal cancer: there is not yet a way to prove that it would work. For cervical cancer, doctors test cells taken from the cervix during routine screening, looking for changes that precede the emergence of cancer. Because HPV-positive oropharyngeal cancer arises deep in the tonsil, checks would have to be much more invasive. \u201cIn theory, we could detect it, but we would need to do a tonsillectomy on everyone in the vaccine trial,\u201d says Gillison. \u201cThat's never going to happen.\u201d There may be another way. Mehanna and his colleagues are in the process of analysing the tonsils of 1,250 people who underwent tonsillectomies for non-cancerous reasons. The researchers have identified what they think are pre-malignant lesions in some HPV-positive samples that may represent the earliest stages of the cancer, and could serve as a biomarker. \u201cWe're now testing to make sure this pre-malignancy is driven by HPV and is not just random,\u201d says Mehanna. Other concerns and questions linger. For example, scientists have yet to determine whether oral HPV infection comes only from sexual acts that involve contact between the mouth and genitals, or also from other acts including deep kissing. And most people who develop an HPV infection do not get oropharyngeal cancer: about 90% of those who become infected orally clear the infection within two years. No one is sure why. Researchers are also investigating whether HPV causes other types of cancer. There have been studies of the relationship between the virus and oesophageal cancer, but findings have been inconclusive. Another area of interest is the lung. There, too, tobacco has been the primary culprit for decades, but some 15\u201320% of lung-cancer cases in men and 50% in women are in people who have never smoked. Doctors have theorized that a virus lies behind them. The available data are conflicting. One paper 9  in 2001 identified HPV DNA in 55% of 141 lung tumours, compared with 27% of 60 non-cancer control samples. And in 2009, researchers led by Iver Petersen, director of the Institute for Pathology at Jena University Hospital in Germany, conducted a meta-analysis 10  of 53 publications examining 4,508 cases of lung cancer, and concluded that \u201cHPV is the second most important cause of lung cancer after cigarette smoking\u201d. They encouraged more research. But many other studies have refuted those observations, including one from Gillison and her colleagues, in which they used sensitive DNA assays to study the lung cancers of 450 patients, and found no HPV (ref.  11 ). With head and neck cancer, however, Gillison is optimistic that new knowledge about HPV as a cause of the disease will help physicians to treat it \u2014 and eventually to prevent it with a vaccine. \u201cIn terms of cancer,\u201d she says, \u201cthere aren't many populations where we've identified the necessary cause and have a potential solution on the shelf.\u201d \n                     Active protection 2013-Jul-17 \n                   \n                     Virus discoveries secure Nobel prize in medicine 2008-Oct-06 \n                   \n                     Viruses found in lung tumours 2008-Apr-25 \n                   \n                     US National Cancer Institute report on HPV \n                   \n                     Maura Gillison \n                   \n                     NHS: What is HPV? \n                   Reprints and Permissions"},
{"file_id": "502287a", "url": "https://www.nature.com/articles/502287a", "year": 2013, "authors": [], "parsed_as_year": "2006_or_before", "body": "Evaluating research output and judging which work to fund is getting harder. Every organization that funds research wants to support science that makes a difference. But there is no simple formula for identifying truly important research. And the job is becoming more difficult. As funding gets squeezed, scientists face stiffer competition for resources and jobs, and it becomes more crucial than ever to develop reliable ways of spotting and supporting the best work. This week,  Nature  examines how the impact of research is measured \u2014 and asks whether today's evaluation systems promote the most important science. A News Feature on  page 288  examines how countries are assessing work through elaborate audit systems. Supporters say that these improve overall research quality, whereas critics charge that they eat up time and money and skew grants towards 'hot topics'. A second News Feature on  page 291  looks at the influence of the leading journals, traditionally recognized as a filter for important research. That role is now being challenged by changes in the publishing industry. And a Careers Feature on  page 397  discusses how grant applicants can articulate the potential impact of their research, as required by many granting agencies. One way of assessing the influence of research is to track citations of papers, but there are concerns that such data are often proprietary and not easily evaluated, writes David Shotton on  page 295 . Shotton is the director of the Open Citations Corpus, a fledgling repository for open scholarly citation data. Researchers are increasingly producing output \u2014 data, videos and code, for example \u2014 that do not mesh well with older systems for evaluating scientific contributions. On  page 298 , Mark Hahnel, founder of figshare, an online tool that allows researchers to publish all their data in a citable, searchable and shareable manner, describes the complexities of tracking the impact of these diverse research products. These stories and commentaries show how evaluation systems are having to evolve rapidly to keep up with changes in the way that science is practised and communicated (see  Editorial, page 271 ). Separating the best from the rest has never been harder. \n                     Nature    special:  Impact \n                   \n                     Nature    special:  The future of publishing \n                   \n                     Nature    special:  Science metrics \n                   \n                     figshare \n                   \n                     Open Citations \n                   Reprints and Permissions"},
{"file_id": "502428a", "url": "https://www.nature.com/articles/502428a", "year": 2013, "authors": [{"name": "Kerri Smith"}], "parsed_as_year": "2006_or_before", "body": "By scanning blobs of brain activity, scientists may be able to decode people's thoughts, their dreams and even their intentions. Jack Gallant perches on the edge of a swivel chair in his lab at the University of California, Berkeley, fixated on the screen of a computer that is trying to decode someone's thoughts. On the left-hand side of the screen is a reel of film clips that Gallant showed to a study participant during a brain scan. And on the right side of the screen, the computer program uses only the details of that scan to guess what the participant was watching at the time. Anne Hathaway's face appears in a clip from the film  Bride Wars , engaged in heated conversation with Kate Hudson. The algorithm confidently labels them with the words 'woman' and 'talk', in large type. Another clip appears \u2014 an underwater scene from a wildlife documentary. The program struggles, and eventually offers 'whale' and 'swim' in a small, tentative font. \u201cThis is a manatee, but it doesn't know what that is,\u201d says Gallant, talking about the program as one might a recalcitrant student. They had trained the program, he explains, by showing it patterns of brain activity elicited by a range of images and film clips. His program had encountered large aquatic mammals before, but never a manatee. Groups around the world are using techniques like these to try to decode brain scans and decipher what people are seeing, hearing and feeling, as well as what they remember or even dream about. Neuroscientists can predict what a person is seeing or dreaming by looking at their brain activity. Media reports have suggested that such techniques bring mind-reading \u201cfrom the realms of fantasy to fact\u201d, and \u201ccould influence the way we do just about everything\u201d.  The Economist  in London even cautioned its readers to \u201cbe afraid\u201d, and speculated on how long it will be until scientists promise telepathy through brain scans. Although companies are starting to pursue brain decoding for a few applications, such as market research and lie detection, scientists are far more interested in using this process to learn about the brain itself. Gallant's group and others are trying to find out what underlies those different brain patterns and want to work out the codes and algorithms the brain uses to make sense of the world around it. They hope that these techniques can tell them about the basic principles governing brain organization and how it encodes memories, behaviour and emotion (see 'Decoding for dummies'). Applying their techniques beyond the encoding of pictures and movies will require a vast leap in complexity. \u201cI don't do vision because it's the most interesting part of the brain,\u201d says Gallant. \u201cI do it because it's the easiest part of the brain. It's the part of the brain I have a hope of solving before I'm dead.\u201d But in theory, he says, \u201cyou can do basically anything with this\u201d. \n               Beyond blobology \n             Brain decoding took off about a decade ago 1 , when neuroscientists realized that there was a lot of untapped information in the brain scans they were producing using functional magnetic resonance imaging (fMRI). That technique measures brain activity by identifying areas that are being fed oxygenated blood, which light up as coloured blobs in the scans. To analyse activity patterns, the brain is segmented into little boxes called voxels \u2014 the three-dimensional equivalent of pixels \u2014 and researchers typically look to see which voxels respond most strongly to a stimulus, such as seeing a face. By discarding data from the voxels that respond weakly, they conclude which areas are processing faces. Decoding techniques interrogate more of the information in the brain scan. Rather than asking which brain regions respond most strongly to faces, they use both strong and weak responses to identify more subtle patterns of activity. Early studies of this sort proved, for example, that objects are encoded not just by one small very active area, but by a much more distributed array. These recordings are fed into a 'pattern classifier', a computer algorithm that learns the patterns associated with each picture or concept. Once the program has seen enough samples, it can start to deduce what the person is looking at or thinking about. This goes beyond mapping blobs in the brain. Further attention to these patterns can take researchers from asking simple 'where in the brain' questions to testing hypotheses about the nature of psychological processes \u2014 asking questions about the strength and distribution of memories, for example, that have been wrangled over for years. Russell Poldrack, an fMRI specialist at the University of Texas at Austin, says that decoding allows researchers to test existing theories from psychology that predict how people's brains perform tasks. \u201cThere are lots of ways that go beyond blobology,\u201d he says. In early studies 1 , 2  scientists were able to show that they could get enough information from these patterns to tell what category of object someone was looking at \u2014 scissors, bottles and shoes, for example. \u201cWe were quite surprised it worked as well as it did,\u201d says Jim Haxby at Dartmouth College in New Hampshire, who led the first decoding study in 2001. Soon after, two other teams independently used it to confirm fundamental principles of human brain organization. It was known from studies using electrodes implanted into monkey and cat brains that many visual areas react strongly to the orientation of edges, combining them to build pictures of the world. In the human brain, these edge-loving regions are too small to be seen with conventional fMRI techniques. But by applying decoding methods to fMRI data, John-Dylan Haynes and Geraint Rees, both at the time at University College London, and Yukiyasu Kamitani at ATR Computational Neuroscience Laboratories, in Kyoto, Japan, with Frank Tong, now at Vanderbilt University in Nashville, Tennessee, demonstrated in 2005 that pictures of edges also triggered very specific patterns of activity in humans 3 , 4 . The researchers showed volunteers lines in various orientations \u2014 and the different voxel mosaics told the team which orientation the person was looking at. Edges became complex pictures in 2008, when Gallant's team developed a decoder that could identify which of 120 pictures a subject was viewing \u2014 a much bigger challenge than inferring what general category an image belongs to, or deciphering edges. They then went a step further, developing a decoder that could produce primitive-looking movies of what the participant was viewing based on brain activity 5 . From around 2006, researchers have been developing decoders for various tasks: for visual imagery, in which participants imagine a scene; for working memory, where they hold a fact or figure in mind; and for intention, often tested as the decision whether to add or subtract two numbers. The last is a harder problem than decoding the visual system says Haynes, now at the Bernstein Centre for Computational Neuroscience in Berlin, \u201cThere are so many different intentions \u2014 how do we categorize them?\u201d Pictures can be grouped by colour or content, but the rules that govern intentions are not as easy to establish. Gallant's lab has preliminary indications of just how difficult it will be. Using a first-person, combat-themed video game called  Counterstrike , the researchers tried to see if they could decode an intention to go left or right, chase an enemy or fire a gun. They could just about decode an intention to move around; but everything else in the fMRI data was swamped by the signal from participants' emotions when they were being fired at or killed in the game. These signals \u2014 especially death, says Gallant \u2014 overrode any fine-grained information about intention. The same is true for dreams. Kamitani and his team published their attempts at dream decoding in  Science  earlier this year 6 . They let participants fall asleep in the scanner and then woke them periodically, asking them to recall what they had seen. The team tried first to reconstruct the actual visual information in dreams, but eventually resorted to word categories. Their program was able to predict with 60% accuracy what categories of objects, such as cars, text, men or women, featured in people's dreams. The subjective nature of dreaming makes it a challenge to extract further information, says Kamitani. \u201cWhen I think of my dream contents, I have the feeling I'm seeing something,\u201d he says. But dreams may engage more than just the brain's visual realm, and involve areas for which it's harder to build reliable models. \n               Reverse engineering \n             Decoding relies on the fact that correlations can be established between brain activity and the outside world. And simply identifying these correlations is sufficient if all you want to do, for example, is use a signal from the brain to command a robotic hand (see  Nature   497 , 176\u2013178; 2013 ). But Gallant and others want to do more; they want to work back to find out how the brain organizes and stores information in the first place \u2014 to crack the complex codes the brain uses. That won't be easy, says Gallant. Each brain area takes information from a network of others and combines it, possibly changing the way it is represented. Neuroscientists must work out  post hoc  what kind of transformations take place at which points. Unlike other engineering projects, the brain was not put together using principles that necessarily make sense to human minds and mathematical models. \u201cWe're not designing the brain \u2014 the brain is given to us and we have to figure out how it works,\u201d says Gallant. \u201cWe don't really have any math for modelling these kinds of systems.\u201d Even if there were enough data available about the contents of each brain area, there probably would not be a ready set of equations to describe them, their relationships, and the ways they change over time. Computational neuroscientist Nikolaus Kriegeskorte at the MRC Cognition and Brain Sciences Unit in Cambridge, UK, says that even understanding how visual information is encoded is tricky \u2014 despite the visual system being the best-understood part of the brain (see  Nature   502 , 156\u2013158; 2013 ). \u201cVision is one of the hard problems of artificial intelligence. We thought it would be easier than playing chess or proving theorems,\u201d he says. But there's a lot to get to grips with: how bunches of neurons represent something like a face; how that information moves between areas in the visual system; and how the neural code representing a face changes as it does so. Building a model from the bottom up, neuron by neuron, is too complicated \u2014 \u201cthere's not enough resources or time to do it this way\u201d, says Kriegeskorte. So his team is comparing existing models of vision to brain data, to see what fits best. \n               Real world \n             Devising a decoding model that can generalize across brains, and even for the same brain across time, is a complex problem. Decoders are generally built on individual brains, unless they're computing something relatively simple such as a binary choice \u2014 whether someone was looking at picture A or B. But several groups are now working on building one-size-fits-all models. \u201cEveryone's brain is a little bit different,\u201d says Haxby, who is leading one such effort. At the moment, he says, \u201cyou just can't line up these patterns of activity well enough\u201d. Standardization is likely to be necessary for many of the talked-about applications of brain decoding \u2014 those that would involve reading someone's hidden or unconscious thoughts. And although such applications are not yet possible, companies are taking notice. Haynes says that he was recently approached by a representative from the car company Daimler asking whether one could decode hidden consumer preferences of test subjects for market research. In principle it could work, he says, but the current methods cannot work out which of, say, 30 different products someone likes best. Marketers, he says, should stick to what they know for now. \u201cI'm pretty sure that with traditional market research techniques you're going to be much better off.\u201d Companies looking to serve law enforcement have also taken notice. No Lie MRI in San Diego, California, for example, is using techniques related to decoding to claim that it can use a brain scan to distinguish a lie from a truth. Law scholar Hank Greely at Stanford University in California, has written in the  Oxford Handbook of Neuroethics  (Oxford University Press, 2011) that the legal system could benefit from better ways of detecting lies, checking the reliability of memories, or even revealing the biases of jurors and judges. Some ethicists have argued that privacy laws should protect a person's inner thoughts and desires as private, but Julian Savulescu, a neuroethicist at the University of Oxford, UK, sees no problem in principle with deploying decoding technologies. \u201cPeople have a fear of it, but if it's used in the right way it's enormously liberating.\u201d Brain data, he says, are no different from other types of evidence. \u201cI don't see why we should privilege people's thoughts over their words,\u201d he says. Haynes has been working on a study in which participants tour several virtual-reality houses, and then have their brains scanned while they tour another selection. Preliminary results suggest that the team can identify which houses their subjects had been to before. The implication is that such a technique might reveal whether a suspect had visited the scene of a crime before. The results are not yet published, and Haynes is quick to point out the limitations to using such a technique in law enforcement. What if a person has been in the building, but doesn't remember? Or what if they visited a week before the crime took place? Suspects may even be able to fool the scanner. \u201cYou don't know how people react with countermeasures,\u201d he says. Other scientists also dismiss the implication that buried memories could be reliably uncovered through decoding. Apart from anything else, you need a 15-tonne, US$3-million fMRI machine and a person willing to lie very still inside it and actively think secret thoughts. Even then, says Gallant, \u201cjust because the information is in someone's head doesn't mean it's accurate\u201d. Right now, psychologists have more reliable, cheaper ways of getting at people's thoughts. \u201cAt the moment, the best way to find out what someone is going to do,\u201d says Haynes, \u201cis to ask them.\u201d \n                     Scientists read dreams 2012-Oct-19 \n                   \n                     Neuroscience: The mind reader 2012-Jun-13 \n                   \n                     Brain imaging: fMRI 2.0 2012-Apr-04 \n                   \n                     Mind-reading with a brain scan 2008-Mar-05 \n                   \n                     I know what you're thinking... 2007-Feb-08 \n                   \n                     Neuropod: Neuroscience podcasts from  Nature \n                   \n                     Gallant lab \n                   \n                     Kamitani lab \n                   Reprints and Permissions"},
{"file_id": "504024a", "url": "https://www.nature.com/articles/504024a", "year": 2013, "authors": [{"name": "Jessica Marshall"}], "parsed_as_year": "2006_or_before", "body": "A billion years ago, a huge rift nearly cleaved North America down the middle. And then it failed. Researchers may be getting close to finding out why. On a bright October Saturday, the trees have reached their full autumn blaze at Interstate State Park on the border between Minnesota and Wisconsin in the US heartland. Crowds of people gawking at leaves thread their way along paths that wind through bulwarks of dark basalt, leading to views of the St Croix River. Along one of the walkways, a photographer directs a young couple in coordinating grey shirts to lean against the rocks as she snaps a romantic portrait. If the two sweethearts are looking to commemorate their everlasting love, they should have picked a different backdrop. The fractured basalt that frames their faces is part of a great gash that opened up in the middle of North America and nearly split the continent 1.1 billion years ago \u2014 hardly a symbol of a happy union. The volcanic rocks are remnants of what is called the Midcontinent Rift, and it is an enormous geological puzzle. Rifts are wounds in Earth's outer layer that can grow to eventually form new oceans. That is how the Atlantic Ocean got its start some 200 million years ago, and an active rift continues to widen that basin. But the Midcontinent Rift was different. It opened a 3,000-kilometre crack in North America and created a basin as big, perhaps, as the Red Sea \u2014 then the system shut down. The wound stopped growing and the continent remained intact. \u201cHow that feature could just totally reorganize the crust of the Earth in the Lake Superior region and not manage to break the continent apart is fairly amazing,\u201d says G. Randy Keller, a geophysicist at the University of Oklahoma in Norman and director of the Oklahoma Geological Survey. \u201cIt's a spectacular failure.\u201d And a forgotten one, too. The rift is mostly buried under thick sediments, which makes it hard to study. And it lies far from the continent's attention-grabbing geological features, such as mountain belts and earthquake zones. \u201cFor a long time, the rift has been a very neglected thing,\u201d says Peter Hollings, a geochemist at Lakehead University in Thunder Bay, Canada. That is now changing. Geologists have started to flock to the region to explore the enormous deposits of ore minerals left by volcanic activity during the creation of the rift: one area in northern Minnesota, for example, is the largest untapped copper\u2013nickel deposit in the world. Another source of interest has come from the US National Science Foundation's EarthScope project and related programmes, which installed dozens of temporary seismometers across the rift to provide an unprecedented picture of Earth's crust and upper mantle there. Researchers are keen to test theories about why the rift began and failed \u2014 and to use the ancient wound to improve more general understanding of how plates move and break apart. What is more, because the lava flows in the rift are stuck in the middle of a continent, they have been left as they were 1 billion years ago, unmangled by the collisions that warp rocks at the edges of continents. The basalts therefore offer an unparalleled record of events on Earth at a time when the continental plates were assembling into a supercontinent dubbed Rodinia, not long after multicellular life evolved. Among researchers, there is a sense that the rift's time has come. \u201cThere's a whole flood of interest on the part of geoscientists who really weren't interested before,\u201d says Keller. \n               Listening posts \n             Some 145 kilometres northeast of where the couple posed for its picture, Suzan van der Lee leans into her shovel, grey hair tucked under a bandana. About a metre below the forest floor, she uncovers a seismometer buried in a black plastic pipe. A geophysicist at Northwestern University in Evanston, Illinois, she is there with a graduate student, Emily Wolin, who squats in front of a laptop as she backs up data from the instrument. The site is 50 kilometres south of Lake Superior, well off the main road amid a dense stand of aspen and oak saplings. Wolin selected the spot two-and-a-half years ago by touring the region, seeking places in or near the Midcontinent Rift that were far enough from roads to avoid vibrations from traffic. Since then, Wolin has been monitoring the stations every six months. On her rounds, she has had to flee an angry dog, don skis after a late-spring snow and seek help from a bear hunter to rescue her car from mud. One of her stations was burned in a wildfire (it still worked, even though a cable had melted), and another recorded the vibrations of trees crashing down in a massive windstorm. At a different site, a hunter apparently used the solar panel that powered the instrument for target practice: Wolin found a bullet hole right through its centre. Today the team is here to recover instruments that have weathered two winters while quietly logging seismic activity across the globe. The stations are part of an EarthScope accessory project known as SPREE, or the Superior Province Rifting EarthScope Experiment. The project aims to fill in details about the Midcontinent Rift by installing extra stations \u2014 82 in total \u2014 tracing and transecting it. The seismometers provided what amounts to medical scans of the top 1,000 kilometres of crust and mantle near the rift. Van der Lee hopes to use that to better understand what is down there, learn how deep the rift extends and perhaps gather some clues as to what caused it. Even though the researchers were careful to site the seismometers in quiet spots, the data coming back contain an inevitable amount of noise. The instruments are so sensitive that they detect not only earthquakes all over the world, but also noise from oceans and all kinds of other seismic background activity. The challenge is to pick that apart at each station and extract a real signal. The team is in the thick of that now and it will be months before it has a fuller picture of the subsurface structure. But the picture that has emerged so far has been intriguing: the data show a significant amount of variation along the rift. \u201cAll the things that we're seeing suggest that we're dealing with a very complex structure,\u201d says van der Lee. \n               Secrets of the deep \n             SPREE and other geophysical studies will be key to unlocking the rift's deepest secrets, because most of the structure is hidden. From magnetic and gravity surveys of the region over the past half-century, geophysicists have determined that the rift is shaped like a horseshoe, with two arms pointing south from Lake Superior (see 'Breaking up is hard to do'). Seismic studies in the 1980s revealed that the rift's basalt layers reach deep below ground, up to 30 kilometres below Lake Superior 1 . All told, the rift produced between 1 million and 2 million cubic kilometres of basalt, making it one of the world's largest deposits of that rock. The sheer volume of erupted basalt has led many to suggest that the rift must have been fed by a mantle plume \u2014 a vertical stream of hot rock rising from the depths of the planet. Another widely held idea is that tectonic plates smashing into the continent from the east stopped the rift from growing. Researchers have used techniques in geochemistry, geophysics and other fields to test these ideas, with conflicting results. \u201cThere are problems with all of the models,\u201d says Hollings. \u201cNone of them really works perfectly.\u201d Still, he adds: \u201cAll the new work that's being done is allowing us to re-evaluate the models and look at different ways this could have happened.\u201d  It's a lot less mysterious than we used to think even a year ago.  Some researchers are starting to see the rift as part of a much bigger puzzle. Geophysicist Carol Stein of the University of Illinois at Chicago discussed this idea at a meeting of the Geological Society of America (GSA) in Denver, Colorado, in October. Working with Keller and others, she suggests that the Midcontinent Rift was not isolated, but in fact was connected to other rifts that caused large changes in Earth's tectonic plates at the time \u2014 all related to the assembly of Rodinia. The hypothesis is based on previous work 2  by Keller, who has proposed that gravity maps show the arms of the rift extending much farther to the south than was thought, towards the edge of Laurentia, the precursor to the North American continent. Other studies have suggested that Laurentia and Amazonia, the precursor to part of the South American continent, were in contact more than 1 billion years ago, and that they began to separate around the time that the Midcontinent Rift became active 3 . Stein proposes a three-armed rift system, formed of the Midcontinent Rift and the two arms that split Laurentia and Amazonia. \u201cIn a lot of locations, when the continents break apart you seem to have three arms where one arm will fail and together the two arms make a new ocean,\u201d she says. \u201cIt's a lot less mysterious than we used to think even a year ago. We used to think of the Midcontinent Rift as this kind of weird feature that started and died in the middle of a continent.\u201d But considered in connection with the rifts at the edge of Laurentia, it makes sense, she says. Stein likens the ancient system to what is happening on the eastern edge of Africa today. Two rift arms in the Gulf of Aden and the Red Sea are pushing the Arabian peninsula away from Africa, and another is starting within Africa. If that East African Rift fails to grow and eventually dies, it will end up looking like North America's Midcontinent Rift, she says. Stein's hypothesis has piqued the interest of other experts. \u201cIt's a very reasonable idea,\u201d says Stephen Marshak, a geologist at the University of Illinois at Urbana-Champaign, although he feels that more testing must be done. He and others agree that understanding the Midcontinent Rift will provide insight into the East African Rift \u2014 what is driving it and how it might propagate in the future. \u201cThey are both informing each other,\u201d he adds. \n               Hot rocks \n             Apart from trying to understand the rift, researchers are also interested in using the feature as a window on the past. Protected in the stable centre of North America, the rift's lava flows have remained undisturbed for 1 billion years \u2014 a rarity for rocks that old. In some places, ripples that formed as the lava cooled are still visible on the basalt. \u201cIt's gorgeous,\u201d says Nicholas Swanson-Hysell, a geologist at the University of California, Berkeley. \u201cHow well these flows are preserved is pretty amazing. You could go to a lava field in Hawaii that erupted in 1950 and the surface would look similar to this 1.1-billion-year-old surface.\u201d Just this kind of preservation is on display at Mamainse Point on the eastern shores of Lake Superior, where Swanson-Hysell has sampled 95 lava flows along 10 kilometres of shoreline. The individual flows, which range from a few metres to 20 metres thick, are part of a 4.5-kilometre-thick formation of rock created during the most active 15 million years of the rift's 30-million-year lifetime. Magnetic grains in these rocks captured the orientation of Earth's magnetic field at the time the lava cooled. The readings from these minute frozen compasses can be used to track how Laurentia wandered around the globe during the span of the rifting. When Swanson-Hysell constructed a magnetic record from the Mamainse flows, he found signs that Laurentia may have been moving faster than any other plate is known to have travelled 4 . His latest estimates, presented at the GSA meeting, put its velocity between 16 centimetres and 45 centimetres per year. For comparison, the next-fastest known plate movement is India's 18-centimetre-per-year rush towards Asia between 50 million and 60 million years ago. \u201cThis velocity is considered to be very fast and near the maximum rate possible for continental motion,\u201d says Swanson-Hysell. Most plates today move only around 4\u20139 centimetres per year. The range Swanson-Hysell has calculated for Laurentia is broad, but he aims to narrow it in the future. Knowing how fast the continent moved gives researchers important information to help them to reconstruct the motion of all of Earth's landmasses at the time the rift formed. Swanson-Hysell says it is possible that the extraordinary velocities recorded there reflect more than just Laurentia's movement. Some of the motion could have been caused by a phenomenon called true polar wander, in which the whole crust and mantle rotate together around the core. This would happen if an extra-dense blob of material in the mantle were migrating towards the equator, taking the crust and mantle with it. If there was true polar wander, it would be a sign of \u201csomething big happening in the interior of Earth\u201d, says Swanson-Hysell. Even if there was not, he adds, the high speed of Laurentia could provide insight into what was driving the motion of the tectonic plates at the time. The truth could be a combination of the two. To test this, Swanson-Hysell wants to construct similar records for sets of rocks on other continents. If they show the same fast motion, it would demonstrate that all the plates were moving together, pointing to true polar wander. But it is no easy task to find such well-preserved rocks from so long ago. Back at Interstate State Park, it begins to drizzle, and the crowds head back to their cars. The raindrops darken the basalt, momentarily giving it the look of a fresh lava flow. Soon the site will empty and only the rocks will remain, full of history that geologists are just starting to unravel. \n                     US seismic array eyes its final frontier 2013-Nov-05 \n                   \n                     Seismology: Quake catcher 2013-Jun-19 \n                   \n                     Seth Stein: The quake killer 2011-Nov-09 \n                   \n                     Seismology: The secret chatter of giant faults 2010-Jul-14 \n                   \n                     Blog post: Hole in the ground available for hire \n                   \n                     Blog post: Earthquake shakes eastern United States \n                   \n                     EarthScope \n                   \n                     SPREE \n                   \n                     Interstate State Park geology \n                   Reprints and Permissions"},
{"file_id": "504022a", "url": "https://www.nature.com/articles/504022a", "year": 2013, "authors": [{"name": "Erica Westly"}], "parsed_as_year": "2006_or_before", "body": "What researchers are learning from an unprecedented survey of mortality in India. In 1975, when Prabhat Jha was growing up in Canada, his family received a report from India that his grandfather had died; the cause was unclear. Like many people living in rural India, Jha's grandfather had died at home, without having visited a hospital. Jha's mother was desperate for more information, so she returned to her home village to talk to locals. Years later, when Jha was at medical school, he reviewed his mother's notes and realized that his grandfather had probably died of a stroke. Now Jha, an epidemiologist at the University of Toronto, is nearing the end of an ambitious public-health programme to document death in India using similar 'verbal autopsy' strategies. The Million Death Study (MDS) involves biannual in-person surveys of more than 1 million households across India. The study covers the period from 1997 to the end of 2013, and will document roughly 1 million deaths. Jha and his colleagues have coded about 450,000 so far, and have deciphered several compelling trends that are starting to lead to policy changes, such as stronger warning labels on tobacco. Public-health experts need mortality figures to monitor disease and assess interventions, but quality mortality data are scarce in most developing countries. Seventy-five per cent of the 60 million people who die each year around the globe are in low- and middle-income countries such as India, where cause of death is often misclassified or unreported. Groups such as the World Health Organization (WHO) typically base mortality estimates on hospital data, but in many developing countries most people die outside hospitals. As global health researchers increasingly turn to indirect computer models, many applaud the MDS's low-tech, on-the-ground approach and see it as a model for assessing true health burdens in the developing world. \u201cFor countries like India, there will almost certainly continue to be a role for verbal autopsy,\u201d said Colin Mathers, coordinator of mortality and burden of disease at the WHO. \u201cIt's a crucial source of information.\u201d \n                     Verbal autopsy methods questioned 2010-Oct-26 \n                   \n                     Gates and Bloomberg team up to tackle tobacco epidemic 2008-Jul-23 \n                   \n                     Southern India sees drop in HIV 2006-Mar-30 \n                   \n                     Prabhat Jha \n                   \n                     Centre for Global Health Research \n                   Reprints and Permissions"},
{"file_id": "503458a", "url": "https://www.nature.com/articles/503458a", "year": 2013, "authors": [{"name": "Jo Marchant"}], "parsed_as_year": "2006_or_before", "body": "Researchers have struggled to identify how certain states of mind influence physical health. One biologist thinks he has an answer. When Steve Cole was a postdoc, he had an unusual hobby: matching art buyers with artists that they might like. The task made looking at art, something he had always loved, even more enjoyable. \u201cThere was an extra layer of purpose. I loved the ability to help artists I thought were great to find an appreciative audience,\u201d he says. At the time, it was nothing more than a quirky sideline. But his latest findings have caused Cole \u2014 now a professor at the Cousins Center for Psychoneuroimmunology at the University of California, Los Angeles \u2014 to wonder whether the exhilaration and sense of purpose that he felt during that period might have done more than help him to find homes for unloved pieces of art. It might have benefited his immune system too. At one time, most self-respecting molecular biologists would have scoffed at the idea. Today, evidence from many studies suggests that mental states such as stress can influence health. Still, it has proved difficult to explain how this happens at the molecular level \u2014 how subjective moods connect with the vastly complex physiology of the nervous and immune systems. The field that searches for these explanations, known as psychoneuroimmunology (PNI), is often criticized as lacking rigour. Cole's stated aim is to fix that, and his tool of choice is genome-wide transcriptional analysis: looking at broad patterns of gene expression in cells. \u201cMy job is to be a hard-core tracker,\u201d he says. \u201cHow do these mental states get out into the rest of the body?\u201d With his colleagues, Cole has published a string of studies suggesting that negative mental states such as stress and loneliness guide immune responses by driving broad programs of gene expression, shaping our ability to fight disease. If he is right, the way people see the world could affect everything from their risk of chronic illnesses such as diabetes and heart disease to the progression of conditions such as HIV and cancer. Now Cole has switched tack, moving from negative moods into the even more murky territory of happiness. It is a risky strategy; his work has already been criticized as wishful thinking and moralizing. But the pay-off is nothing less than finding a healthier way to live. \u201cIf you talk to any high-quality neurobiologist or immunologist about PNI, it will invariably generate a little snicker,\u201d says Stephen Smale, an immunologist at the University of California, Los Angeles, who is not affiliated with the Cousins Center. \u201cBut this doesn't mean the topic should be ignored forever. Someday we need to confront it and try to understand how the immune system and nervous system interact.\u201d \n               The best medicine? \n             In 1964, magazine editor Norman Cousins was diagnosed with ankylosing spondylitis, a life-threatening autoimmune disease, and given a 1 in 500 chance of recovery. Cousins rejected his doctors' prognosis and embarked on his own programme of happiness therapy, including regular doses of Marx Brothers films, and credited it with triggering a dramatic recovery. He later established the Cousins Center, which is dedicated to investigating whether psychological factors really can keep people healthy. At the time, mainstream science rejected the idea that any psychological state, positive or negative, could affect physical well-being. But studies during the 1980s and early 1990s revealed that the brain is directly wired to the immune system \u2014 portions of the nervous system connect with immune-related organs such as the thymus and bone marrow, and immune cells have receptors for neurotransmitters, suggesting that there is crosstalk. These connections seem to have clinical relevance, at least in the case of stress. One of the first researchers to show this was virologist Ronald Glaser, now director of the Institute for Behavioral Medicine Research at the Ohio State University in Columbus. \u201cWhen I started working on this in the 1980s, nobody believed what stress could do, including me,\u201d he recalls. He and his colleagues sampled blood from medical students, and found that during a stressful exam period, they had lower activity from virus-fighting immune cells 1 , and higher levels of antibodies for the common virus Epstein\u2013Barr 2 , suggesting that stress had compromised their immune systems and allowed the normally latent virus to become reactivated. The field of PNI has grown hugely since then, with medical schools worldwide boasting their own departments of mind\u2013body medicine, of which PNI is just one component. It is now accepted that the body's response to stress can suppress parts of the immune system and, over the long term, lead to damaging levels of inflammation. Large epidemiological studies \u2014 including the Whitehall studies, which have been following thousands of British civil servants since 1967 \u2014 suggest 3  that chronic work stress increases the risk of coronary heart disease and type 2 diabetes, for example. Low socio-economic status increases susceptibility to a wide range of infectious diseases, and there is considerable evidence that stress increases the rate of progression of HIV/AIDS. But researchers have a long way to go before they will understand exactly how signals from the brain feed into physical health. \n               Worried sick \n             PNI studies have mostly tended to look at levels of individual immune-cell types or molecular messengers \u2014 such as the stress hormone cortisol and the immune messenger proteins called cytokines \u2014 or the expression of individual genes. But Cole wanted to get a sense of how the whole system was working. His first foray, published in 2007, looked at loneliness 4 . Social isolation is one of the most powerful known psychological risk factors for poor health, but it is never certain whether it causes the health problems, or whether a third factor is involved: lonely people might be less likely than others to eat well, for example, or to visit their doctor regularly. Cole and his colleagues looked at gene expression in the white blood cells of six chronically lonely people \u2014 people who had said consistently over several years that they felt lonely or isolated, and were fearful of other people \u2014 and eight people who said that they had great friends and social support. Out of the roughly 22,000 genes in the human genome, the researchers identified 209 that distinguished the lonely people from the sociable ones: they were either regulated up to produce more of an individual protein or regulated down to produce less. Any individual gene could easily look different by chance, but Cole was struck by the overall pattern. A particularly large proportion of the upregulated genes in the lonely group turned out to be involved in the inflammatory response, whereas many of the downregulated genes had antiviral roles. In sociable people, the reverse was true. It was a small study, but one of the first to link a psychological risk factor with a broad underlying change in gene expression. The researchers have since replicated that result in a group of 93 people 5 . Cole says that he has also seen a similar shift in gene expression in individuals exposed to various types of social adversity, from imminent bereavement to low socio-economic status. The results make evolutionary sense, he says. Early humans in close-knit social groups would have faced increased risk of viral infections, so they would have benefited from revved-up antiviral genes. By contrast, people who were isolated and under stress faced greater risk of injuries that could cause bacterial infection \u2014 and thus would need to respond by ramping up genes associated with inflammation, to help heal wounds and fight off those infections. But modern stresses lead to chronic and unhelpful inflammation, which over time damages the body's tissues, increasing the risk of chronic diseases such as atherosclerosis, cancer and diabetes. To a classical immunologist such as Smale, Cole's results are \u201cintriguing, wonderful observations\u201d, but not yet completely convincing. In future work, he wants to see the rest of the physiological pathway nailed down. \u201cUntil you put together a full understanding of that mechanism, you have this level of uncertainty and scepticism,\u201d he says. That sentiment is echoed by Alexander Tarakhovsky, an immunologist at the Rockefeller University in New York City. Pinning down precise mechanisms \u2014 for example, which neurotransmitters cause which specific effects \u2014 is extremely difficult, he says, because the brain and the immune system are both so complex. Cole's research \u201cmakes you think about what the consequences of social hardship could be, but it doesn't really tell you how it works\u201d. Greg Gibson, director of the Center for Integrative Genomics at the Georgia Institute of Technology in Atlanta, wants to see larger studies but argues that the big-picture \u201cgenetic architecture\u201d that Cole is uncovering is worth studying, even if not every detail of the mechanism is yet understood. \u201cA lot of people are taking a whole-genome approach, but they focus only on a handful of 'top hits'. They are missing the wood for the trees.\u201d \n               Don't worry, be happy \n             In 2010, Cole received an e-mail from Barbara Fredrickson, a friend from graduate school who was now studying emotional well-being at the University of North Carolina in Chapel Hill. \u201cRemember me?\u201d she said. She was interested in the biological correlates of happiness and other positive emotional states, and suggested that the pair collaborate. After years of looking at stress and adversity, Cole loved the idea. \u201cI was bored as hell with misery,\u201d he says. If PNI as a whole has credibility issues, studying well-being is even trickier. It is more slippery to measure than stress \u2014 there is no biological marker such as cortisol to fall back on and no simple way to induce it in the lab, and mainstream biologists tend to look down on fuzzy methods of data collection such as questionnaires. One approach is to test whether it is possible to reverse the adverse effects on gene expression caused by stress. Cole has collaborated in three small, randomized, controlled trials that attempt to do this. Studies involving 45 stressed caregivers 6  and 40 lonely adults 7  respectively found that courses in meditation shifted gene-expression profiles in the participants' white blood cells away from inflammatory genes and towards antiviral genes. A third trial 8 , led by psycho-oncologist Michael Antoni at the University of Miami, Florida, involved 200 women with early-stage breast cancer. In those who completed a ten-week stress-management programme, genes associated with inflammation and metastasis were downregulated compared with those of women in the control group, who attended a one-day educational seminar. Meanwhile, genes involved in the type I interferon response (which fights tumours as well as viruses) were upregulated in the women who took the stress-management course. \u201cOur conclusion was that mood matters,\u201d says Antoni. \u201cIf we change the psychology, physiological changes do parallel that.\u201d Cole and Fredrickson aspired to go further. Instead of looking at the benefits of blocking stress, they wanted to investigate what happens in the body when people are happy. To that end, they asked 80 participants 14 questions, such as how often in the past week they had felt happy or satisfied, and how often they felt that their life had a sense of meaning 9 . The questions were designed to distinguish between the two forms of happiness recognized by psychologists: hedonic well-being (characterized by material or bodily pleasures such as eating well or having sex) and eudaimonic well-being (deeper satisfaction from activities with a greater meaning or purpose, such as intellectual pursuits, social relationships or charity work). The researchers were surprised to find that the two types of happiness influenced gene expression in different ways. People with a meaning-based or purpose-based outlook had favourable gene-expression profiles, whereas hedonic well-being, when it occurred on its own, was associated with profiles similar to those seen in individuals facing adversity. One interpretation is that eudaimonic well-being benefits immune function directly. But Cole prefers to explain it in terms of response to stress. If someone is driven purely by hollow consumption, he argues, all of their happiness depends on their personal circumstances. If they run into adversity, they may become very stressed. But if they care about things beyond themselves \u2014 community, politics, art \u2014 then everyday stresses will perhaps be of less concern. Eudaimonia, in other words, may help to buffer our sense of threat or uncertainty, potentially improving our health. \u201cIt's fine to invest in yourself,\u201d says Cole, \u201cas long as you invest in lots of other things as well.\u201d \n               Perils of positive thinking \n             This is just the kind of advice that attracts some of the most vociferous criticisms of Cole's work. James Coyne, a health psychologist and emeritus professor at the University of Pennsylvania in Philadelphia, says that Cole and Frederickson's well-being study is simply too small to show anything useful. He also argues that the measures of eudaimonic and hedonic happiness are so highly correlated in the study as to be essentially the same thing. Coyne says that early results are being vastly over-sold. \u201cThey claim that if you make the right choices, you'll be healthy. And if you don't, you'll die.\u201d Coyne wants researchers across the field of PNI to stop publicizing claims about health benefits until the science is more solid. \u201cThey're turning it into books and workshops, telling people how to live their lives.\u201d Fredrickson, for example, is the author of two popular books, including  Positivity  (Crown Archetype, 2009), which posits that a specific ratio of positive to negative emotions (2.9013, to be precise) is linked to good health. The book has been praised by eminent psychologists such as Daniel Goleman and Martin Seligman, but the set of equations behind the ratio was criticized this year 10  by Alan Sokal, a physicist at New York University (who famously published a deliberately nonsensical paper in the journal  Social Text  in 1996, intended to expose the lack of rigour in the field of cultural studies). He pointed out that the equations are based on parameters from a 1962 paper on air flow, with no connection to psychological data at all. Fredrickson acknowledges problems with the maths, which she based on a peer-reviewed paper on the complex dynamics of teams 11 , but says that she stands by the fundamental principles described in the book. \u201cThere seems good enough evidence to suggest that emotions contribute to health.\u201d Cole and Fredrickson agree that their study is small and needs to be repeated. But they say that extensive previous research has validated the questionnaire they used and confirmed that it measures two distinct, albeit highly correlated, emotional states. They also note that correlation does not necessarily mean that two states are the same: height and weight are also highly correlated, for example, yet describe different things. Each type of happiness tends to encourage the other, says Fredrickson, \u201cbut we can try to understand which is leading the way towards health\u201d. The researchers are not the first from the PNI community to face accusations of wishful thinking. Indeed, the story of the field's founder \u2014 hailed in the press as proof of the power of positive emotions \u2014 has been questioned. Immunologists have suggested that Cousins was not suffering from ankylosing spondylitis at all, but from polymyalgia rheumatica, which often clears up on its own. His \u201chealth probably coincidentally remitted\u201d, says Cole. Despite the criticisms, and the fact that his work is in its early days, Cole says that he is struck by the evidence that positive emotions can override the biological effects of adversity \u2014 enough to make changes in his own life. Although he no longer has time to engage in the art trade, he has embraced the ways that his hobby helped him. \u201cI have spent most of my career and personal life trying to avoid or overcome bad things,\u201d he says. \u201cI spend a lot more time now thinking about what I really want to do with my life, and where I'd like to go with whatever years remain.\u201d \n                     Social neuroscience and health: neurophysiological mechanisms linking social ties with physical health 2012-Apr-15 \n                   \n                     A matter of the heart 2008-Mar-01 \n                   \n                     Stress and immunity 1999-May-27 \n                   \n                     Steve Cole \n                   \n                     The Atlantic : Meaning is healthier than happiness \n                   \n                     Cousins Center for Psychoneuroimmunology \n                   \n                     Stephen Smale \n                   \n                     Positivity \n                   Reprints and Permissions"},
{"file_id": "502424a", "url": "https://www.nature.com/articles/502424a", "year": 2013, "authors": [{"name": "Brian Switek"}], "parsed_as_year": "2006_or_before", "body": "Even one of the best known dinosaurs has kept some secrets. Here is what palaeontologists most want to know about the famous tyrant. In late 1905, newspaper reporters gushed over the bones of a prehistoric monster that palaeontologists had unearthed in the badlands of Montana. When  The New York Times  described the new 'Tyrant saurian', the paper declared it \u201cthe most formidable fighting animal of which there is any record whatever\u201d. In the century since,  Tyrannosaurus rex  has not loosened its grip on the imaginations of the public or palaeontologists. Stretching more than 12 metres from snout to tail and sporting dozens of serrated teeth the size of rail spikes, the 66-million-year-old  T. rex  remains the ultimate example of a prehistoric predator \u2014 so much so that a media frenzy erupted this year over a paper debating whether  T. rex  predominantly hunted or scavenged its meals 1 . This infuriated many palaeontologists, who say the matter was resolved long ago by ample evidence showing that  T. rex  could take down prey and dismantle carrion. What particularly vexed researchers was that this non-issue overshadowed other, more important questions about  T. rex . The dinosaur's evolutionary origins, for example, are still a mystery. Researchers are eagerly trying to determine how these kings of the Cretaceous period (which spanned from 145 million to 66 million years ago) arose from a line of tiny dinosaurs during the Jurassic period (201 million to 145 million years ago). There is also considerable debate about what  T. rex  was like as a juvenile, and whether palaeontologists have spent decades mistaking its young for a separate species. Even the basic appearance of  T. rex  is in dispute: many researchers argue that the giant was covered in fluff or fuzz rather than scales. And then there is the vexing question of why  T. rex  had such a massive head and legs but relatively puny arms. How much do we really know about the world\u2019s most famous dinosaur? On the bright side, palaeontologists have material to work with. \u201cWe have lots of fossils of  T. rex ,\u201d says palaeontologist Stephen Brusatte of the University of Edinburgh, UK. \u201cIt's rare to have so many good fossils of one dinosaur, so we can actually ask questions about  T. rex  \u2014 such as how it grew, what it ate and how it moved \u2014 that we can't for other dinosaurs.\u201d Here,  Nature  examines how palaeontologists are investigating these and other hot topics for the most charismatic of carnivores. \n               Fuzzy origins \n             In the first few decades after palaeontologist Henry Fairfield Osborn named and described  T. rex , researchers viewed this giant dinosaur as the culmination of a trend towards bigger predators. In this view,  T. rex  was seen as the descendent of  Allosaurus , a 9-metre-long predator that lived more than 80 million years earlier. These and other massive carnivorous dinosaurs were lumped together in a categorical wastebasket called the Carnosauria, with  T. rex  as the last and biggest of the ferocious family. But palaeontologists tore up that evolutionary tree when they started using a more rigorous form of analysis called cladistics in the 1990s. They re-examined relationships between dinosaur groups and found that  T. rex  had its roots in a lineage of small, fuzzy creatures that lived in the shadow of  Allosaurus  and other predators during the Jurassic period. The view that emerged placed  T. rex  and its close relatives \u2014 together known as tyrannosaurids \u2014 as the top twig on a broader evolutionary bush called the Tyrannosauroidea, which emerged around 165 million years ago (see 'In the flesh'). Among the earliest known members of this group was  Stokesosaurus clevelandi , a bipedal carnivore 2\u20133 metres long that lived about 150 million years ago. Little is known about this creature, but evidence from other early tyrannosauroids suggests that  Stokesosaurus  had a long, low skull and slender arms. Early tyrannosauroids were small, agile predators, but their size placed them low in the pecking order during the Jurassic. \u201cThey were more lapdogs than top predators,\u201d says Brusatte. The question for palaeontologists is how tyrannosaurs rose to power from such humble beginnings and why they took over as the apex predators in North America and Asia. At present, the key parts of this story are missing. There are relatively few dinosaur-rich rock formations from the period between 145 million and 90 million years ago, when tyrannosaurs apparently took over, so palaeontologists have yet to fully chart the communities that existed at the time. Shifts in sea level or climate could have triggered events that led to tyrannosaur dominance, Brusatte says, but he admits that such a connection is speculative. \u201cWe really need more fossils from this middle Cretaceous gap to help untangle this mystery.\u201d In the past few years, researchers have started making headway in China, where rock formations record some segments of this key interval. In 2009, Peter Makovicky at the Field Museum in Chicago, Illinois, and his colleagues described a long-snouted tyrannosaur named  Xiongguanlong baimoensis  from rocks in western China dating to between 100 million and 125 million years ago 2 . That animal reached about four metres long, a step up in size from the Jurassic tyrannosaurs. And, in 2012, Xu Xing of the Institute of Vertebrate Paleontology and Paleoanthropology in Beijing and his colleagues described a 9-metre-long tyrannosaur by the name of  Yutyrannus huali 3  from a similar time period (see  Nature   489 , 22\u201325; 2012 ). This may be the crucial transition during which tyrannosaurs overlapped with allosaurs, before the latter faded out in the same habitats. In studies of rocks from northern China, Brusatte and his co-workers have found an allosaur five to six metres long named  Shaochilong maortuensis , which lived about 90 million years ago 4 . \u201cSo it seems like both allosauroids and tyrannosauroids were around in Asia during this time, and had relatively similar sizes,\u201d he says. He hopes that further fossil discoveries will help to flesh out how and when tyrannosaurs took over as the top predator in their ecosystems. \n               Adolescent angst \n             Just as the evolutionary origins of  T. rex  remain murky, so does its youth. In this case, the big debate centres on an creature called  Nanotyrannus lancensis , a tyrannosaur found in the same North American deposits as  T. rex  that may have reached more than 6 metres in length. When it was first discovered, this creature was thought to be a separate species, but some researchers now argue that  Nanotyrannus  is actually just a juvenile  T. rex . According to Thomas Holtz Jr, a palaeontologist at the University of Maryland in College Park,  Nanotyrannus  specimens look remarkably like  T. rex , and the differences between the two are similar to the differences between immature and mature individuals of other tyrannosaur species. The fact that all of the  Nanotyrannus  specimens seem to be juvenile animals and all of the specimens recognized as  T. rex  are subadults or adults, Holtz says, indicates that the two are truly one. Lawrence Witmer, a palaeobiologist at Ohio University in Athens, is not so sure. In 2010, he and his colleague Ryan Ridgely studied computed-tomography scans of a skull from the Cleveland Museum of Natural History in Ohio that is the defining specimen, or holotype, of N. lancensis. \u201cWe went into the project with the bias or assumption that the Cleveland skull was a juvenile  T. rex ,\u201d Witmer says. But they found some unusual indentations in the brain case and sinuses, where air sacs filled the back of the skull in life 5 . These features are very different from those of  T. rex  and may identify the skull as belonging to a different species, says Witmer. Team  Nanotyrannus  has no more vocal an advocate than Peter Larson, president of the Black Hills Institute of Geological Research, a company in Hill City, South Dakota, that collects, prepares and casts fossils. Larson argues that the teeth of  Nanotyrannus  are too finely serrated and closely packed to be those of a young  T. rex . He also points to differences between the two species in the anatomy of the shoulder socket and the openings in the skull. But some of these conclusions were gleaned from fossils not yet described in any publication, and scientists may never have a chance to study them. A skeleton that has been identified as a  Nanotyrannus  that could offer clues will be auctioned off next month in New York City. The hype generated by this specimen and its relevance to the  Nanotyrannus  debate has helped to drive up its price; estimates suggest that it may fetch up to US$9 million. But most palaeontologists refuse to study such specimens unless they are placed in a reputable museum. A private buyer could rob researchers of that opportunity. \u201cThe solution may reside in the tired plea for more fossils,\u201d Witmer says. For  Nanotyrannus  to have a shot at being a separate species, palaeontologists would like to see one of two discoveries: a young tyrannosaur more similar to adult  T. rex  than any  Nanotyrannus  specimen, or an animal that is clearly an adult  Nanotyrannus  that is different from  T. rex . But where an animal as charismatic as  T. rex  is concerned, it may be impossible for researchers to abandon long-held views and resolve decades of debate. \u201cI'm not sure how much data it'll take to break us out of that,\u201d Witmer says. \n               A flap over feathers \n             For generations, artists have depicted  T. rex  covered in scales, much like the modern-day reptiles to which it is only distantly related. But in the past two decades, researchers in China have found specimens from many dinosaur groups bearing feathers or a fuzzy coating. Some of these discoveries include species closely related to  T. rex . In 2004, Xu named  Dilong paradoxus  \u2014 a small, early tyrannosaur 6 . The fossil of this animal showed impressions of fibres around the tail, jaw and other body parts, suggesting the animal had a coat of 'dinofuzz'. The giant  Y. huali  from China also bore plumage 3 . The feathers on these tyrannosaurs were not like those of living birds, but simplified precursors. Xu suggests that the earliest feathered dinosaurs might have used their plumage for visual display. Later animals that were cloaked entirely in feathers might have relied on them for insulation. Because of the close evolutionary link between tyrannosaurs, he suggests that \u201c T. rex  might have had some kind of protofeathers\u201d. Other researchers also favour the idea of feathered tyrannosaurs. \u201cIt is becoming increasingly difficult to reject a fuzz-less  Tyrannosaurus  with a straight face,\u201d Holtz says. That does not mean that  T. rex  looked like a Cretaceous chicken. Brusatte says it may have been covered in fairly inconspicuous hair-like fibres, like many other feathered dinosaurs. As yet, no skin impressions have been found for  T. rex , so researchers cannot say with certainty what kind of body covering it had. And some are not ready to abandon the more conventional view. Thomas Carr, a palaeontologist at Carthage College in Kenosha, Wisconsin, argues, for example, that unpublished fossils with skin impressions from close relatives of  T. rex  show scaly skin. These findings suggest that even though some earlier tyrannosauroids had feathers, the subgroup called tyrannosauridae (which includes  T. rex ), seems to have undergone an evolutionary reversal from fuzz to scales. \u201cThere is no empirical evidence that tyrannosaurids had feathers,\u201d Carr says, \u201cand artists have no business decking them out with plumage until the day comes when a tyrannosaurid is found with feathers.\u201d This argument goes well beyond what the creatures looked like. Whether  T. rex  had feathers will influence how researchers reconstruct the life of this dinosaur, from possible courtship behaviours to how it controlled its body temperature. \n               Arms race \n             One of the biggest mysteries about  T. rex  has nagged palaeontologists for more than a century: what use did the giant have for arms so stubby that they could not even have reached its mouth? Early ideas, later discarded, suggested that the two-clawed arms helped  T. rex  to grip a partner during mating or to rise from repose. Later palaeontologists argued that the arms were vestigial \u2014 an idea beloved by cartoonists, who never tire of showing  T. rex  embarrassed by its useless, puny guns. But research by palaeobiologist Sara Burch at Ohio University suggests that such jokes are unfair. She has studied the musculature of crocodylians as well as that of the only living members of the dinosaur line \u2014 birds. If the arms of  T. rex  had been vestigial, they would have lost the various anatomical landmarks that indicate muscle attachments, but the fossils \u201cretain evidence of substantial musculature,\u201d she says. But knowing that  T. rex  used its arms doesn't reveal what they were used for. To Carr, the arms were part of the dinosaur's arsenal. \u201cTyrannosaurids used their arms in the same way all theropods used their arms, for grasping and stabilizing objects\u201d \u2014 namely prey, he says. Holtz visualizes a less rigorous role for the forelimbs. On the basis of previous estimates of muscle strength, he argues that  T. rex  had weak arms. And because many tyrannosaurs have arms with healed fractures, he says, \u201ctheir life habits could not require constant use of these arms\u201d. Holtz suggests that they were used primarily for display, perhaps during mating or competition\u2014 a possibility that seems more likely if these limbs were cloaked in feathers. He and other palaeontologists plan to keep digging into the secrets of this superlative animal, one of the strongest ambassadors of the past in all of science. \u201cMany aspects of  T. rex , especially behavioural ones or physiological ones, are still unknown,\u201d Holtz says. But perhaps not forever. \u201cAs new methods of investigation are developed, we will have new avenues about their biology to explore.\u201d And as researchers do so, their views on the tyrant king will continue to evolve. \n                     Tyrannosaurus rex hunted for live prey 2013-Jul-15 \n                   \n                     How to eat a Triceratops 2012-Oct-24 \n                   \n                     China's dinosaur hunter: The ground breaker 2012-Sep-05 \n                   \n                     Tyrannosaurs were power-walkers 2011-Nov-07 \n                   \n                     Interactive: How to build a giant \n                   \n                     Blog post: Largest feathered dinosaur yet discovered in China \n                   Reprints and Permissions"},
{"file_id": "502608a", "url": "https://www.nature.com/articles/502608a", "year": 2013, "authors": [{"name": "Linda Nordling"}], "parsed_as_year": "2006_or_before", "body": "Can the Southern African Large Telescope live up to its potential? On 30 August, the Cape of Good Hope lived up to its original name, the Cape of Storms. The cargo ship  Atacama  barely managed to find shelter in her berth at Cape Town before the harbour was slammed by towering waves, icy squalls and gale-force winds. A day passed before the tempest subsided and the ship could safely unload its cargo: an automobile-sized device known as the High Resolution Spectrograph (HRS). It was a fitting welcome. The HRS's final destination, some 350 kilometres inland, was the Southern African Large Telescope (SALT) \u2014 a facility that has weathered many storms of its own. Officially completed in 2005, SALT is only now finishing its second year of normal science operations, and pressure is mounting for the facility to prove itself. Along with the Square Kilometre Array (SKA) of radio telescopes (see  Nature   480 , 308\u2013309; 2011 ), SALT is a major component of South Africa's effort to establish its scientific reputation and inspire a new generation of African scientists. Yet its teething problems have prompted questions about its design, the way it was built and how it has been managed so far. SALT's defenders counter that problems could not have been avoided in building one of the world's largest telescopes on a shoestring. The telescope's first-generation suite of detectors wasn't even complete until the arrival of the HRS. And the spectrograph still needs to be tested and calibrated before it opens for routine use early next year. Nonetheless, Ted Williams, director of the South African Astronomical Observatory (SAAO), which manages SALT, knows that excuses won't suffice. \u201cOur challenge in the coming years is to produce the nice science that will make people recognize that it's a major telescope that's living up to its potential,\u201d he says. \n               Budget build \n             From the moment planning began in 1998, SALT has been an underdog \u2014 not least because the monies available to the SAAO and its partners were never anything like those at the disposal of northern observatories. The SALT team had to find ways to design, build and equip the 11-metre telescope for just US$30 million \u2014 a fraction of the cost of flagship instruments such as the European Southern Observatory's 8.2-metre Very Large Telescope (VLT) in Chile or the 10-metre Keck Telescopes in Hawaii, whose price tags were well in excess of $100 million. \u201cIf the VLT is like a Ferrari, SALT is more like a family car,\u201d says Darragh O'Donoghue, head of instrumentation at the SAAO. Among other things, SALT's location also put it at a technical disadvantage. The site \u2014 a desolate hilltop near the town of Sutherland in South Africa's Northern Cape province \u2014 was chosen both because it is beautifully dark and because it already had roads and power. The SAAO had been operating a 1.9-metre telescope there since 1974, along with several smaller instruments that had been relocated from their original sites near Johannesburg and Cape Town because of light pollution. But the site stands just 1,798 metres above sea level, quite low by astronomical standards, so there would always be a comparatively thick layer of atmosphere above the telescope to blur the incoming starlight. SALT's images, although acceptable, would never achieve anything close to the clarity of those taken at world-class sites such as the 4,200-metre summit of Mauna Kea in Hawaii. Nonetheless, SALT's location would allow it to fill a major gap in observations of the southern sky. When a star, galaxy or planet was at or below the horizon for the existing Southern Hemisphere observatories, most of which are in Australia or Chile, it would still be high overhead in South Africa. That is a big plus for astronomers such as Dimitar Sasselov of Harvard University in Cambridge, Massachusetts, who studies exoplanets (those outside the Solar System) by watching for 'transits': tiny, brief dips in a star's brightness as a planet orbits in front of it. \u201cWe are often faced with time-critical events like planet transits that make scheduling of observations in the Southern Hemisphere tricky,\u201d says Sasselov. So he sees SALT becoming an important complement to large telescopes elsewhere. Recognizing the limitations they faced, SALT's designers decided to make their telescope world class in the realm of spectroscopy, which involves splitting the incoming light into its constituent wavelengths. Spectroscopy can provide a wealth of information about the composition and motion of celestial objects, but does not require the ultimate in crisp imagery \u2014 just a big mirror able to collect lots of light. The trick was to build such a mirror on a very tight budget. The SAAO accomplished this by copying the radical cost-saving design of the Hobby\u2013Eberly Telescope (HET), an 11-metre-wide instrument completed in 1996 at the McDonald Observatory near Fort Davis, Texas. One big cost saving for this instrument came from a technique pioneered in the 1980s at Hawaii's Keck Observatory. This involved piecing together the telescope's mirror from many hexagonal segments of glass, each of which could be manufactured and shipped to the site independently \u2014 a much cheaper proposition than trying to fabricate, polish and transport a single giant mirror. Another big saving came from designing the dish-like mirror to have a spherical curvature. Spherical mirrors cannot bring starlight to a sharp focus; that requires a slightly different shape known as a paraboloid. But paraboloids are much more complex and costly to make from segments because each one has a curve that depends on its position. A spherical mirror's segments are identical \u2014 meaning that SALT's 91 hexagonal mirror elements could be mass-produced relatively cheaply. And the imperfect focus could be fixed by passing the light through an optical system called a spherical aberration corrector (SAC) before sending it on to the detectors (see \u2018Inside SALT\u2019). However, as SALT was taking shape, its designers decided to tweak the design of the SAC and other elements to achieve better image quality, which had turned out to be chronically poor at the HET. The changes also allowed for a bigger field of view. But once the construction of SALT was completed in 2005, the telescope faced problems of its own. The most serious was that, despite the design improvements, SALT's image quality was poor. In images produced by SALTICAM, its main optical camera, and the Robert Stobie Spectrograph (RSS), stars often looked smudged or stretched. However, on rare days, for no obvious reason, the images were perfect. Solving this mystery took the project team several years. The first potential culprit was SALT's main mirror. The team wondered whether some of the mirror segments were imperfectly aligned, which would prevent them from sending light to the SAC with the required precision. It took nine months of testing the segments to rule out the mirror as the source of the image-quality problems. The team then moved on to the second suspect: the revamped SAC. This contains auxiliary mirrors up to half a metre in diameter, which needed to be hung to micrometre precision. Checking and testing these mirrors took more than a year. In the end, the culprit turned out to be the way the SAC was attached to the telescope. The support structure was made out of metals that expanded and contracted differently with changes in temperatures, which affected the alignment of the SAC and explained the variability from day to day. And, during design, the weight of the telescope's instruments had increased, so there had been pressure to keep the overall weight of the SAC down. This had resulted in the use of a design too flimsy to keep the SAC correctly positioned. In an ideal world, the SALT team would have foreseen these problems, says Phil Charles, who was the SAAO's director until 2011 and now heads the astronomy group at the University of Southampton, UK. But, given the real-world constraints on time and money, they simply assumed that the Texas group had got this element of the design right. \u201cWe now know that HET suffered from all the problems that SALT did,\u201d says Charles. \u201cIt's just that their basic image quality was so bad that you couldn't see the underlying problems in the mechanical design.\u201d In the end, it was not until August 2010 that a new and better support structure could be hoisted up and mounted on SALT, putting an end to the device's imaging nightmares. While all this was going on, the team also had to grapple with another major problem, this time with the RSS. Built by one of SALT's major partners, the University of Wisconsin-Madison, it was designed to be the world's most sensitive spectrograph in the near-ultraviolet (UV) range. But, once mounted on the telescope, it managed only average performance at best. Yet another investigation revealed the culprit to be the spectrograph's plumbing. Because the instrument's large lenses expand and contract with temperature changes, they are mounted in a liquid that is contained in a system of plastic ducts and bladders. The manufacturers of the fluid and the plastics thought they had confirmed that the two would not react chemically, but react they did \u2014 producing contaminants that were particularly absorbent in the coveted near-UV range. The spectrograph's optics were dismounted and sent back to the United States, where they were cleaned and the fluid replaced with a different substance. Meanwhile, the cleaning process cracked one of the lenses, so that also had to be replaced. \u201cThat set us back as well,\u201d says Williams. \n               Colour blind \n             Given this history, it is not surprising that nerves were taut during the recent arrival and assembly of the HRS. And the assembly team soon discovered a problem with the instrument's 'blue camera', which captures and analyses blue light. In effect, the HRS was blind in one eye, recalls J\u00fcrgen Schmoll, an optics specialist who had flown in to help with the assembly from Durham University, UK, where the HRS was built. Closer inspection of the blue camera showed that two of the three metal balls holding its detector chip to micrometre precision had come loose in transit, tilting the chip. The team deliberated over what to do. The safest, slowest and most expensive option was to take the blue camera to a dust-free room in Cape Town and fly a technician in from Durham to perform the delicate surgery required to reattach the balls. In the end, however, the team decided to fix the problem on-site \u2014 a risky operation that involved opening the camera, gluing the balls onto screws, manoeuvring them back into place, breaking the screws loose and cleaning the glue off the balls before closing the camera again. Any speck of dust that entered the camera during this process could become a permanent fixture on the sensitive detector. A tense few days followed, but on 22 September SALT team members were finally able to declare the operation a success. \u201cThe sun's back out, the spring flowers have thawed \u2026 &: we have a healthy blue camera again. Life is good,\u201d they trumpeted on the project's blog. Once the HRS is fully up and running next year, the SALT team hopes that it will play an important part in restoring the telescope's global image. For example, the HRS should be very good at detecting the subtle shifts in a star's spectrum that show it is being orbited by an exoplanet \u2014 and should even be able to study the chemistry of the exoplanet's atmosphere. Upcoming projects will study the distribution of dark matter in galaxies, and explore the mechanisms by which galaxies form and grow. But to stand a chance of playing in the big leagues, SALT needs to pick up its productivity, says Dennis Crabtree, an astronomer at the National Research Council Canada in Victoria who keeps tabs on the scientific output of telescopes larger than 3.5 metres. So far, he says, \u201cSALT has had a pretty low profile because of its teething problems, which led to a low production rate of scientific papers\u201d. Could the SALT project have avoided those problems? Probably not all of them, says Matt Mountain, the astronomer who oversaw the construction phase of the US Gemini project, which built twin 8-metre telescopes in Hawaii and Chile at an overall cost of $184 million. Technical glitches are par for the course when building the world's biggest telescopes, he says. The big question is how these problems are dealt with. In the past, Mountain explains, when astronomers worked on telescopes roughly 1 metre across, fixing problems as they arose was fairly straightforward. But with today's giant telescopes, Mountain says, the astronomers need project leaders and experts from industry to fix problems. It has been a painful transition, he says, from the old astronomer-led system to the new one in which project managers are in charge. But South Africa will need to master that shift to complete its share of the SKA project, which is estimated to cost around US$2 billion and will involve building hundreds of radio dishes in the African and Australian outbacks. Williams says that SALT was built through a mixture of the old approach and the new. For both the telescope and the HRS, construction was undertaken with strong project management. However, SALTICAM and the RSS were constructed through the more conventional, astronomer-led approach, he says. SALT's supporters remain optimistic, despite the telescope's technical setbacks. O'Donoghue says that it has fulfilled many of its goals, including placing South Africa on the global astronomy map. \u201cI firmly believe that the SKA would have never come to South Africa if it hadn't been for SALT as a technology demonstrator,\u201d he says. As evidence of SALT's influence, he points to the recent decision by the International Astronomical Union to set up an Office for Astronomy Development at the SAAO in Cape Town. The telescope has also become a technological icon. \u201cBetween 2001 and 2003, an average of 50 people visited Sutherland annually,\u201d says Sivuyile Manxoyi, the SAAO's head of education and outreach. \u201cIn 2012 we had 12,205 visitors.\u201d Tourism in the town has also benefited. Today there are 40 guest houses, as well as more than a dozen guest farms, most of which offer stargazing as an attraction. Astronomy is also booming at the country's universities. Meanwhile, Williams is trying to raise SALT's scientific profile by updating Sutherland's older, smaller telescopes, making it easier for astronomers abroad to carry out observations there by Internet connection (rather than by travelling to South Africa), and by making sure that the science done at SALT is presented at important conferences around the world. But the output so far from the HET suggests that SALT may never be as productive as the more expensive, multi-purpose telescopes. In 2011, for example, only 26 papers were published on work at the HET, whereas mature telescopes averaged 85, according to Crabtree's database. But such gloomy predictions don't seem to daunt the SALT team. After all, says one of its software engineers, the personality of SALT is \u201ca South African upstart, doing things better and more cheaply than the competition\u201d. Besides, as Williams points out, wintry weather doesn't usually last long in Africa \u2014 and \u201cpessimists don't build telescopes\u201d. \n                     Astronomy in South Africa: The long shot 2011-Dec-14 \n                   \n                     Astronomy: Exoplanets on the cheap 2011-Feb-02 \n                   \n                     Construction starts on largest telescope in the south 2000-Sep-07 \n                   \n                     South African telescope to go ahead 1999-Dec-02 \n                   \n                     SALT \n                   \n                     SALT blog \n                   \n                     SAAO \n                   \n                     Astronomy for Development \n                   Reprints and Permissions"},
{"file_id": "502612a", "url": "https://www.nature.com/articles/502612a", "year": 2013, "authors": [{"name": "Hannah Hoag"}], "parsed_as_year": "2006_or_before", "body": "Diane Orihel set her PhD aside to lead a massive protest when Canada tried to shut down its unique Experimental Lakes Area. It was an ominous way to start the day. When she arrived at work on the morning of 17 May 2012, Diane Orihel ran into distraught colleagues. Staff from Canada's Experimental Lakes Area had just been called to an emergency meeting at the Freshwater Institute in Winnipeg. \u201cIt can't be good,\u201d said one. As a PhD student working on her dissertation \u2014 not a staff member \u2014 Orihel was not allowed in. But an hour later, she heard the news. Michelle Wheatley, regional director of Fisheries and Oceans Canada (DFO), the federal department responsible for the day-to-day operations of the Experimental Lakes Area (ELA), had dropped a bombshell: owing to budget cuts, the field station would close on 31 March 2013. Staff members should begin removing their equipment from labs and lakes. Wheatley instructed them not to speak to the media. The closure would strike a blow at the heart of freshwater ecology. The ELA \u2014 a field site of 58 freshwater lakes in the boreal forest of northwestern Ontario \u2014 has since 1968 been the only facility in the world where scientists can manipulate or even intentionally poison an entire lake to monitor the effects. Work there proved that phosphorus from fertilizers sparks algal blooms; quantified the effects of acid rain; showed how mercury accumulates in fish; documented the release of greenhouse gases from hydroelectric reservoirs; and revealed how the synthetic oestrogen in contraceptive pills feminizes male fish. Orihel herself had spent most of a decade doing summer fieldwork in the lakes. Orihel had no experience as an activist, and was not comfortable in the spotlight. But she was immune to the gagging order because she did not work for the government. She stepped up, becoming public historian, promoter and defender of the site. \u201cI felt a moral obligation, a responsibility, to be the voice for ELA because the ELA scientists couldn't,\u201d she says. At first, Orihel had hoped to get the closure decision reversed within three weeks. She ended up putting her PhD on hold for six months. By the end of that time, she had become one of Canada's most outspoken defenders of science funding and evidence-based policy. \u201cShe's the poster child for the 'scientist-turned-activist',\u201d says Katie Gibbs, a biologist who co-founded the science-advocacy group Evidence for Democracy in Ottawa. \n               Courage of her convictions \n             Orihel was always a high achiever. By the time she started her PhD in 2007, she had published four papers based on her master's research. \u201cDiane impressed me almost on first meeting,\u201d says her PhD supervisor, ecologist David Schindler of the University of Alberta in Edmonton. She also had pluck. Schindler hated to see students treat their advisers as gurus, but Orihel was not afraid to defend her point of view. Schindler \u2014 who in 1991 won the prestigious Stockholm Water Prize for his work on nitrification and acidification of lakes \u2014 was an ELA founding scientist and its first director. When he met Orihel, he was 66 years old and shrinking his research group as he readied himself for retirement. He gave Orihel a small grant to investigate an unusual type of algal bloom. She occasionally spoke to residents' associations and advocacy groups about her research, but generally kept to her work. Everything changed on that Thursday morning in May. After hearing the news, Orihel got in touch with a local politician she had worked with on algae before, and got a quick lesson on media relations from his communications staffer. She wrote a press release and began collecting quotes from scientists. She briefed the opposition party so that it would know what was going on. By mid-afternoon, she had blasted her announcement to a list of journalists that she found online. On Friday evening, Orihel's neighbours, concerned by her non-stop work, ordered her over for dinner. But she was preoccupied. \u201cDo you know how to set up a website?\u201d she asked her hosts. With their help, the campaign got a name, Save ELA, and an online home. Over the weekend, Orihel organized dozens of volunteers by e-mail and Skype, and persuaded two postdocs to join her in forming the Coalition to Save ELA. They doled out tasks: fill the website with details of past and present projects; develop a petition to deliver to parliament; organize scientists to send an open letter to the fisheries and environment ministers; write more press releases. By Sunday, Orihel realized that she would need to take leave from her studies. Just three weeks, she told her supervisors. They \u201cweren't keen\u201d, she says. \u201cAll she had left was to write up her thesis. And she gave that up to save ELA,\u201d says Schindler, who regretted the interruption to Orihel's career, but was proud of her gumption. \u201cIt was a remarkable show of spine.\u201d \n               Expanding cause \n             The three weeks passed quickly. By June 2012, more than 11,500 Canadians had signed the coalition's petition and Orihel flew to Ottawa to deliver it to politicians. She packed her trip with speeches, meetings and press conferences. She got the open letter \u2014 signed by eight established Canadian scientists, including John Smol, a limnologist at Queen's University in Kingston, Ontario \u2014 published in a national newspaper. She persuaded scientific societies from the United States, Japan and Australia to send letters of protest to the government. Orihel capped her tour off with a speech to students at Queen's University. Her campaign had grown bigger than just the ELA. \u201cI am you,\u201d she began. \u201cI am shy, quiet, introverted, and get nervous at public speaking.\u201d But in the face of \u201ca government's anti-science, anti-environment agenda\u201d, she told the audience, \u201cI need your help\u201d. Orihel's campaign tapped into a growing sense of unease. Many Canadian scientists felt that the government's cost-cutting measures had unfairly targeted science and environmental programmes: the Polar Environmental Atmospheric Research Laboratory in the Canadian High Arctic, for example, was slated for partial closure (it later reopened with two-thirds of its previous budget). Federal environmental assessments had been overhauled, reducing the number and length of evaluations. And government scientists were fed up with a communications policy, quietly put in place four years earlier, that restricted their relationship with the press: researchers had to get permission to speak to journalists, and interview requests were often denied or responded to with government-controlled quotes. Greg Rickford, Canada's current science minister, declined to be interviewed for this story, e-mailing only a general statement: \u201cOur government is committed to science, technology and innovation and taking ideas to the marketplace.\u201d In July 2012, more than 2,000 people gathered for a 'Death of Evidence' rally on Parliament Hill, to protest against the way the government was undermining science-informed decision-making. \u201cDiane made the ELA one of the best-known examples of funding cuts in Canada,\u201d says Gibbs, who organized that rally. DFO director Dave Gillis insisted at the time that the government had to make the cut \u2014 the ELA closure would save Can$1.5 million (US$1.5 million) a year, less than 2% of the nearly $80 million that the DFO needed to trim by 2015 in austerity measures \u2014 and said that the department hoped other organizations, such as universities, could take on the ELA's costs. But some claim that ideology drove the decision: the ELA and other research facilities might produce damning data about the environmental impacts of, for example, extracting oil from Alberta's tar sands or the use of industrially valuable chemicals. \u201cIt had nothing to do with money. It was inconvenient data,\u201d says Smol. In response, the DFO issued a statement to  Nature  saying in part: \u201cScience is the foundation of the department's business and it will continue to build scientific knowledge about our aquatic environment and fisheries resources to support long-term sustainability and conservation objectives.\u201d By last autumn, things were looking bleak: the idea that the ELA might be run by a consortium of universities had come to nothing. Orihel's dream of getting the original decision reversed was beyond hope: \u201cI knew hell would freeze over before this would ever happen.\u201d Exhausted, she stepped down as head of the Coalition to Save ELA, to return to her PhD. But behind the scenes, she knew, ELA scientists had approached the International Institute for Sustainable Development (IISD), a policy think tank based in Winnipeg, to find another way to keep the site going. \n               Last-minute reprieve \n             It came in the nick of time. By mid-March 2013, just weeks before the planned closure, a work crew had started dismantling old cabins at the ELA, and university scientists were contemplating abandoning their experiments. Then, in April, the IISD announced that it had secured a deal with the Ontario provincial government to keep the site open through the summer. Orihel and her coalition deserve substantial credit, says Matt McCandless, the IISD's project manager for the ELA. By keeping the spotlight on the ELA, they \u201cpaved the way for the negotiations\u201d, he says. Ontario later promised to provide up to $2 million a year to run the ELA; in September, the neighbouring province of Manitoba promised $900,000 over six years. But the battle is not over. On 1 September, the agreement that allowed scientists to pollute the lakes expired. The site is open, but scientists there cannot legally do their game-changing work. \u201cEven today, the ELA is so far from being saved and functional again,\u201d says Orihel. \u201cI don't trust this government to do what's in the best interest of science and the environment and all Canadians. This will drag on and on.\u201d She now has her PhD, but Orihel admits that her actions may have limited her career options: the government, or some universities, might be uncomfortable hiring someone so politically vocal. \u201cShe took a risk, but her credibility as a scientist will come from her publications,\u201d says Smol. Schindler adds: \u201cIf you were to ask me to pick the next leader of the ELA project, I'd pick Diane.\u201d Some say that she could have been more effective by working with the government, rather than fighting it aggressively. But Orihel does not regret her approach, despite having had to rewrite her personality to run her campaign: \u201cI made myself act unlike myself.\u201d Now Orihel's life is back to normal: she is looking for a postdoc position and finishing off research papers. But she hopes that her actions will inspire others. \u201cThings are so bad in Canada, right now,\u201d she says. \u201cScientists can see the writing on the wall. They're seeing a need to speak out.\u201d Orihel says that she has been encouraged to run for public office. \u201cBut I have no political aspirations. I just want to be a scientist.\u201d \n                     Last-minute reprieve for Canada\u2019s research lakes 2013-Sep-02 \n                   \n                     Death of evidence 2012-Jul-18 \n                   \n                     Canada must free scientists to talk to journalists 2010-Sep-29 \n                   \n                     Save the census 2010-Jul-28 \n                   \n                     Blogpost: Canada to investigate muzzling of scientists \n                   \n                     Experimental Lakes Area \n                   \n                     Save ELA \n                   \n                     International Institute for Sustainable Development \n                   Reprints and Permissions"},
{"file_id": "503022a", "url": "https://www.nature.com/articles/503022a", "year": 2013, "authors": [{"name": "M. Mitchell Waldrop"}], "parsed_as_year": "2006_or_before", "body": "Computer chips inspired by human neurons can do more with less power. Kwabena Boahen got his first computer in 1982, when he was a teenager living in Accra. \u201cIt was a really cool device,\u201d he recalls. He just had to connect up a cassette player for storage and a television set for a monitor, and he could start writing programs. But Boahen wasn't so impressed when he found out how the guts of his computer worked. \u201cI learned how the central processing unit is constantly shuffling data back and forth. And I thought to myself, 'Man! It really has to work like crazy!'\u201d He instinctively felt that computers needed a little more 'Africa' in their design, \u201csomething more distributed, more fluid and less rigid\u201d. Today, as a bioengineer at Stanford University in California, Boahen is among a small band of researchers trying to create this kind of computing by reverse-engineering the brain. The brain is remarkably energy efficient and can carry out computations that challenge the world's largest supercomputers, even though it relies on decidedly imperfect components: neurons that are a slow, variable, organic mess. Comprehending language, conducting abstract reasoning, controlling movement \u2014 the brain does all this and more in a package that is smaller than a shoebox, consumes less power than a household light bulb, and contains nothing remotely like a central processor. To achieve similar feats in silicon, researchers are building systems of non-digital chips that function as much as possible like networks of real neurons. Just a few years ago, Boahen completed a device called Neurogrid that emulates a million neurons \u2014 about as many as there are in a honeybee's brain. And now, after a quarter-century of development, applications for 'neuromorphic technology' are finally in sight. The technique holds promise for anything that needs to be small and run on low power, from smartphones and robots to artificial eyes and ears. That prospect has attracted many investigators to the field during the past five years, along with hundreds of millions of dollars in research funding from agencies in both the United States and Europe. Neuromorphic devices are also providing neuroscientists with a powerful research tool, says Giacomo Indiveri at the Institute of Neuroinformatics (INI) in Zurich, Switzerland. By seeing which models of neural function do or do not work as expected in real physical systems, he says, \u201cyou get insight into why the brain is built the way it is\u201d. And, says Boahen, the neuromorphic approach should help to circumvent a looming limitation to Moore's law \u2014 the longstanding trend of computer-chip manufacturers managing to double the number of transistors they can fit into a given space every two years or so. This relentless shrinkage will soon lead to the creation of silicon circuits so small and tightly packed that they no longer generate clean signals: electrons will leak through the components, making them as messy as neurons. Some researchers are aiming to solve this problem with software fixes, for example by using statistical error-correction techniques similar to those that help the Internet to run smoothly. But ultimately, argues Boahen, the most effective solution is the same one the brain arrived at millions of years ago. \u201cMy goal is a new computing paradigm,\u201d Boahen says, \u201csomething that will compute even when the components are too small to be reliable.\u201d \n               Silicon cells \n             The neuromorphic idea goes back to the 1980s and Carver Mead: a world-renowned pioneer in microchip design at the California Institute of Technology in Pasadena. He coined the term and was one of the first to emphasize the brain's huge energy-efficiency advantage. \u201cThat's been the fascination for me,\u201d he says, \u201chow in the heck can the brain do what it does?\u201d Mead's strategy for answering that question was to mimic the brain's low-power processing with 'sub-threshold' silicon: circuitry that operates at voltages too small to flip a standard computer bit from a 0 to a 1. At those voltages, there is still a tiny, irregular trickle of electrons running through the transistors \u2014 a spontaneous ebb and flow of current that is remarkably similar in size and variability to that carried by ions flowing through a channel in a neuron. With the addition of microscopic capacitors, resistors and other components to control these currents, Mead reasoned, it should be possible to make tiny circuits that exhibit the same electrical behaviour as real neurons. They could be linked up in decentralized networks that function much like real neural circuits in the brain, with communication lines running between components rather than through a central processor 1 , 2 . By the 1990s, Mead and his colleagues had shown it was possible to build a realistic silicon neuron 3  (see 'Biological inspiration'). That device could accept outside electrical input through junctions that performed the role of synapses, the tiny structures through which nerve impulses jump from one neuron to the next. It allowed the incoming signals to build up voltage in the circuit's interior, much as they do in real neurons. And if the accumulating voltage passed a certain threshold, the silicon neuron 'fired', producing a series of voltage spikes that travelled along a wire playing the part of an axon, the neuron's communication cable. Although the spikes were 'digital' in the sense that they were either on or off, the body of the silicon neuron operated \u2014 like real neurons \u2014 in a non-digital way, meaning that the voltages and currents weren't restricted to a few discrete values as they are in conventional chips. That behaviour mimics one key to the brain's low-power usage: just like their biological counterparts, the silicon neurons simply integrated inputs, using very little energy, until they fired. By contrast, a conventional computer needs a constant flow of energy to run an internal clock, whether or not the chips are computing anything. Mead's group also demonstrated decentralized neural circuits \u2014 most notably in a silicon version of the eye's retina. That device captured light using a 50-by-50 grid of detectors. When their activity was displayed on a computer screen, these silicon cells showed much the same response as their real counterparts to light, shadow and motion 4 . Like the brain, this device saves energy by sending only the data that matters: most of the cells in the retina don't fire until the light level changes. This has the effect of highlighting the edges of moving objects, while minimizing the amount of data that has to be transmitted and processed. \n               Coding challenge \n             In those early days, researchers had their hands full mastering single-chip devices such as the silicon retina, says Boahen, who joined Mead's lab in 1990. But by the end of the 1990s, he says, \u201cwe wanted to build a brain, and for that we needed large-scale communication\u201d. That was a huge challenge: the standard coding algorithms for chip-to-chip communication had been devised for precisely coordinated digital signals, and wouldn't work for the more-random spikes created by neuromorphic systems. Only in the 2000s did Boahen and others devise circuitry and algorithms that would work in this messier system, opening the way for a flurry of development in large-scale neuromorphic systems. Among the first applications were large-scale emulators to give neuroscientists an easy way to test models of brain function. In September 2006, for example, Boahen launched the Neurogrid project: an effort to emulate a million neurons. That is only a tiny chunk of the 86 billion neurons in the human brain, but enough to model several of the densely interconnected columns of neurons thought to form the computational units of the human cortex. Neuroscientists can program Neurogrid to emulate almost any model of the cortex, says Boahen. They can then watch their model run at the same speed as the brain \u2014 hundreds to thousands of times faster than a conventional digital simulation. Graduate students and researchers have used it to test theoretical models of neural function for processes such as working memory, decision-making and visual attention. \u201cIn terms of real efficiency, in terms of fidelity to the brain's neuronal networks, Kwabena's Neurogrid is well in advance of other large-scale neuromorphic systems,\u201d says Rodney Douglas, co-founder of the INI and co-developer of the silicon neuron. But no system is perfect, as Boahen himself is quick to point out. One of Neurogrid's biggest shortcomings is that its synapses \u2014 of which there is an average of 5,000 per neuron \u2014 are simplified connections that cannot be modified individually. This means that the system cannot be used to model learning, which occurs in the brain when synapses are modified by experience. Given the limited space available on the chip, squeezing in the complex circuitry needed to make each synapse behave in a more realistic manner would require circuit elements about a thousand times smaller in area than they are at present \u2014 in the realm of nanotechnology. This is currently impossible, although a newly developed class of nanometre-scale memory devices called 'memristors' could someday solve the problem. Another issue stems from inevitable variations in the fabrication process, which mean that every neuromorphic chip performs slightly differently. \u201cThe variability is still much less than what is observed in the brain,\u201d says Boahen \u2014 but it does mean that programs for Neurogrid have to allow for substantial variations in the silicon neurons' firing rates. This issue has led some researchers to abandon Mead's original idea of using sub-threshold chips. Instead, they are using more conventional digital systems that are still neuromorphic in the sense that they mimic the electrical behaviour of individual neurons, but are more predictable and much easier to program \u2014 at the cost of using more power. A leading example is the SpiNNaker Project, led since 2005 by computer engineer Steve Furber at the University of Manchester, UK. This system uses a version of the very-low-power digital chips \u2014 which Furber helped to develop \u2014 that are found in many smartphones. SpiNNaker can currently emulate up to 5 million neurons. These neurons are simpler than those in Neurogrid and burn more power, says Furber, but the system's purpose is similar: \u201crunning large-scale brain models in biological real time\u201d. Another effort sticks with neuron-like chips, but boosts their speed. Neurogrid's neurons operate at exactly the same rate as real ones. But the European BrainScaleS project, headed by former accelerator-physicist Karlheinz Meier at Heidelberg University in Germany, is developing a neuromorphic system that currently emulates 400,000 neurons running up to 10,000 times faster than real time. This means it consumes about 10,000 times more energy than equivalent processes in the brain. But the speed is a boon for some neuroscience researchers. \u201cWe can simulate a day of neural activity in 10 seconds,\u201d Meier says. Furber and Meier now have the money to push for bigger and better. Together they constitute the neuromorphic arm of the European Union's ten-year, \u20ac1-billion (US$1.3-billion) Human Brain Project, which was officially launched last month. The roughly \u20ac100 million devoted to neuromorphic research will allow Furber's group to scale up his system to 500 million digital neurons; Meier's group, meanwhile, is aiming for 4 million. The success of these research-oriented projects has helped to stoke interest in the idea of using neuromorphic hardware for practical, ultra-low-power applications in devices from phones to robots. Until recently, that hadn't been a priority in the computer industry. Chip designers could usually minimize energy consumption by simplifying circuit design, or splitting computations over multiple processor 'cores' that can run in parallel or shut down when they are not needed. But these approaches can only achieve so much. Since 2008, the US Defense Advanced Research Projects Agency has spent more than $100 million on its SyNAPSE project to develop compact, low-power neuromorphic technology. One of the project's main contractors, the cognitive computing group at IBM's research centre in Almaden, California, has used its share of the money to develop digital, 256-neuron chips that can be used as building blocks for larger-scale systems. \n               Brain power \n             Boahen is pursuing his own approach to practical applications \u2014 most notably in an as-yet-unnamed initiative he started in April. The project is based on Spaun: a design for a computer model of the brain that includes the parts responsible for vision, movement and decision-making. Spaun relies on a programming language for neural circuitry developed a decade ago by Chris Eliasmith, a theoretical neuroscientist at the University of Waterloo in Ontario, Canada. A user just has to specify a desired neural function \u2014 the generation of instructions to move an arm, for example \u2014 and Eliasmith's system will automatically design a network of spiking neurons to carry out that function. To see if it would work, Eliasmith and his colleagues simulated Spaun on a conventional computer. They showed that, with 2.5 million simulated neurons plus a simulated retina and hand, it could copy handwritten digits, recall the items in a list, work out the next number in a given sequence and carry out several other cognitive tasks 5 . That's an unprecedented range of abilities by neural simulation standards, says Boahen. But the Spaun simulation ran about 9,000 times slower than real time, taking 2.5 hours to simulate 1 second of behaviour. Boahen contacted Eliasmith with the obvious proposition: build a physical version of Spaun using real-time neuromorphic hardware. \u201cI got very excited,\u201d says Eliasmith, for whom the match seemed perfect. \u201cYou've got the peanut butter, we've got the chocolate!\u201d With funding from the US Office of Naval Research, Boahen and Eliasmith have put together a team that plans to build a small-scale prototype in three years and a full-scale system in five. For sensory input they will use neuromorphic retinas and cochleas developed at the INI, says Boahen. For output, they have a robotic arm. But the cognitive hardware will be built from scratch. \u201cThis is not a new Neurogrid, but a whole new architecture,\u201d he says. It will trade a certain amount of realism for practicality, relying on \u201cvery simple, very efficient neurons so that we can scale to the millions\u201d. The system is explicitly designed for real-world applications. On a five-year timescale, says Boahen, \u201cwe envision building fully autonomous robots that interact with their environments in a meaningful way, and operate in real-time while [their brains] consume as much electricity as a cell phone\u201d. Such devices would be much more flexible and adaptive than today's autonomous robots, and would consume considerably less power. In the longer term, Boahen adds, the project could pave the way for compact, low-power processors in any computer system, not just robotics. If researchers really have managed to capture the essential ingredients that make the brain so efficient, compact and robust, then it could be the salvation of an industry about to run into a wall as chips get ever smaller. \u201cBut we won't know for sure,\u201d Boahen says, \u201cuntil we try.\u201d \n                     The Language of the Brain 2012-Sep-18 \n                   \n                     The Human Brain Project 2012-May-15 \n                   \n                     Computer modelling: Brain in a box 2012-Feb-22 \n                   \n                     A Chip that Thinks Like a Brain 2011-Nov-15 \n                   \n                     A Chip that Thinks Like a Brain 2011-Nov-15 \n                   \n                     Neuromorphic Microchips 2005-May-01 \n                   \n                     Think like a bee 2001-Mar-29 \n                   \n                     Nature  special: New angles on the brain \n                   \n                     Kwabena Boahen's Lab \n                   \n                     Institute of Neuroinformatics \n                   \n                     SpiNNaker Project \n                   \n                     BrainScaleS Project \n                   \n                     DARPA SyNAPSE Project \n                   \n                     IBM Neuromorphic Lab \n                   Reprints and Permissions"},
{"file_id": "503026a", "url": "https://www.nature.com/articles/503026a", "year": 2013, "authors": [{"name": "Helen Shen"}], "parsed_as_year": "2006_or_before", "body": "Barack Obama announced his BRAIN Initiative on 2 April. Ever since, neuroscientists have been scrambling to work out what it actually is. A mixture of excitement, hope and anxiety made for an electric atmosphere in the crowded hotel ballroom. On a Monday morning in early May, neuroscientists, physicists and engineers packed the room in Arlington, Virginia, to its 150-person capacity, while hundreds more followed by webcast. Only a month earlier, US President Barack Obama had unveiled the neuroscience equivalent of a Moon shot: a far-reaching programme that could rival Europe's 10-year, \u20ac1-billion (US$1.3-billion) Human Brain Project (see  page 5 ). The US Brain Research Through Advancing Innovative Neurotechnologies (BRAIN) Initiative would develop a host of tools to study brain activity, the president promised, and lead to huge breakthroughs in understanding the mind. But Obama's vague announcement on 2 April had left out key details, such as what the initiative's specific goals would be and how it would be implemented. So at their first opportunity \u2014 a workshop convened on 6 May by the National Science Foundation (NSF) and the Kavli Foundation of Oxnard, California \u2014 researchers from across the neuroscience spectrum swarmed to fill in the blanks and advocate for their favourite causes. The result was chaotic. Everyone was afraid of being left out of 'the next big thing' in neuroscience \u2014 even though no one knew exactly what that might be. \u201cThe belief is we're ready for a leap forward. Which leap, and in which direction, is still being debated,\u201d says Van Wedeen, a neurobiologist at Harvard Medical School in Boston, Massachusetts, and one of the workshop's organizers. Others describe the BRAIN Initiative as a Rorschach test \u2014 an indeterminate entity that invited each researcher to project his or her own hopes and insecurities. But as the initiative has evolved, it has also come to resemble a large-scale sociological experiment, as the sprawling neuroscience community struggles to coalesce around a common research plan under intense public scrutiny and tough financial constraints. \n               A big picture \n             To the public, Obama's announcement seemed to come from nowhere; the president had never focused much on neuroscience before. In fact, the idea behind it had been spawned some 18 months earlier and almost 6,000 kilometres from the White House. At a meeting in Chicheley, UK, a group of neuroscientists and nanoscientists invited by the Kavli Foundation had developed their vision for the future of neuroscience research: to record electrical impulses from thousands, or even millions, of neurons at once. That is the only way in which we might understand how thought emerges from the brain, argues Rafael Yuste, a neuroscientist at Columbia University in New York City who spearheaded the idea. Current technology can make recordings from only single neurons or small groups of neurons at a time \u2014 which, he says, \u201cis like trying to watch a movie on TV by looking at one pixel\u201d. To do better, the architects of the Kavli plan called for a Brain Activity Map (BAM) project: a technology-development programme that would give researchers the tools to start small, produce detailed maps of neural activity in simple organisms such as the fruitfly, and then move on to larger, more complex mammalian systems such as the mouse retina. They predicted that, within 15 years, BAM would be able to simultaneously record all of the activity in a mouse cortex \u2014 and that primates, and even humans, would be next. BAM was intended to be provocative. \u201cThis is not going to happen if we keep waiting on little labs to do little things,\u201d declares Yuste. But, for many outsiders, it was ill-conceived \u2014 \u201ca complete work of science fiction\u201d, says Markus Meister, a neurobiologist at the California Institute of Technology in Pasadena. Critics argued that the effort would take too long, cost too much and, ultimately, run up against the laws of physics, which limit how densely electrodes can be packed inside the brain. Moreover, creating a full activity map covering an organism's entire lifetime could yield a cripplingly large data set, while distracting from what many saw as the real problem: a dearth of computational and theoretical methods with which to interpret the brain's activity. \u201cWe just don't understand the data we have,\u201d says Mehrdad Jazayeri, a neuroscientist at the Massachusetts Institute of Technology in Cambridge. But BAM caught the attention of administrators at the White House, who were on the lookout for a bold presidential initiative (see  Nature 495, 19; 2013 ). The first hint of the administration's interest appeared in the president's State of the Union address on 12 February. Few neuroscientists appreciated the significance until five days later, when they were jolted awake by an article on the front page of  The New York Times , which reported that the White House planned to unveil a ten-year neuroscience initiative based on the Kavli idea. The article suggested that the initiative might receive federal funding on par with the $3.8 billion spent on the Human Genome Project, and would produce a comprehensive, detailed map of neural activity in the human brain within a decade. The news alarmed many neuroscientists, who worried that few of them had been consulted, that the money would be made available at the expense of existing programmes and that failure to meet a seemingly impossible goal would undermine public trust in science. \u201cThis was a very narrow agenda of a small group of people,\u201d recalls Partha Mitra, a neuroscientist at Cold Spring Harbor Laboratory in New York and a vocal critic of BAM. But the administration was already taking a different tack. By the time of the official announcement on 2 April, the project had been rebranded the BRAIN Initiative and carried a comparatively modest price tag: only $110 million in federal funding for the 2014 fiscal year. It no longer had a specific lifetime \u2014 although the White House implied that the project could last ten years or longer. And, unlike BAM, it had no clearly defined goal. Rather than promising to record from any particular number of neurons at once, Obama said simply that new tools were needed to help neuroscientists to develop better pictures of brain circuits in action \u2014 and that such technologies could pave the way to treatments for neurological disorders such as epilepsy, autism, Alzheimer's disease and schizophrenia (see  Nature   499 , 272\u2013274; 2013 ). Many neuroscientists found the announcement reassuring \u2014 at least the BRAIN Initiative wasn't BAM \u2014 but puzzlingly vague. All they knew was that the details would be left up to three government agencies: the Defense Advanced Research Projects Agency (DARPA), which would contribute $50 million in the first year; the National Institutes of Health (NIH), which would pitch in $40 million; and the NSF, which would add $20 million. The initiative would be further supported by four private institutions, which had committed to a total of $122 million over varying lengths of time (see 'Obama's BRAIN'). Thus the crush at the NSF's May workshop: after weeks of uncertainty, researchers were hungry for a chance to weigh in. Participants were asked to submit one-page proposals describing a major obstacle to understanding the brain. Then, in a frenetic pitch-fest, authors took the floor for one minute each to argue their cases. \u201cWhat I care most about is reconstructing circuits accurately and fast,\u201d declared the first speaker, Albert Cardona. A neuroscientist at the Janelia Farm Research Campus near Ashburn, Virginia, he pushed for improved automated techniques to map the brain's anatomy on a super-fine scale. Others called for equally fine-grained recordings from ever-larger numbers of neurons, in the spirit of BAM. Still others championed their favourite model organisms. And some speakers emphasized the importance of big-data storage, as well as the computational and theoretical advances required to make sense of all that information. \n               Blurred vision \n             To the growing exasperation of audience members, however, there was no convergence towards a coherent agenda for the initiative. No one could even say whether the initiative would be funded with new cash outlays or with money diverted from existing research. By the meeting's end, the hotel lobby had become crowded with restless attendees who had abandoned the talks to check e-mails, make phone calls and run their labs from afar. Among those who did stay were members of the NIH's BRAIN Initiative advisory committee, a 15-member panel dubbed the 'dream team' \u2014 a nickname it has since tried, unsuccessfully, to shake off. Co-chaired by neuroscientists Cornelia Bargmann at the Rockefeller University in New York City and William Newsome of Stanford University in California, the panel's first task was to prepare an interim report outlining the NIH's science goals for the project's first year. Then, once that report had been delivered to the NIH in September, the team would start to develop a long-term implementation plan, due in June 2014. Shortly after the NSF meeting, the NIH team started on its first order of business: convening a series of four workshops to gather input from the neuroscience community. These covered molecular techniques; large-scale recording technologies; computational and theoretical neuroscience; and human brain studies. The difference in tone was striking. The NSF event had been like a cacophonous town hall meeting, whereas the NIH workshops felt more like an honorary lecture series. Each one began with public presentations by a dozen or so invited speakers, and the proceedings were carefully controlled. Once the open session was over, all of the speakers disappeared into closed discussions with the dream team. One participant likened that experience to being a delegate to the United Nations, with everyone seated around a long oval table behind printed name cards. Bargmann says that privacy was necessary to allow scientists to speak freely \u2014 and sometimes critically \u2014 about different experimental approaches. But, in the wider neuroscience community, many felt the selection of invited speakers and topics excluded their interests. The dream team weathered criticism from molecular, cellular and developmental neuroscientists who felt underrepresented, as well as from clinical neuroscientists concerned that there was not enough emphasis on disease research. Adding to researchers' anxiety was the fact that no one knew whether they would have any involvement in the arms of the project being run by the two other federal agencies. The deputy director of DARPA's defence science office, Geoffrey Ling, said in June that his agency would not be releasing any road maps for its BRAIN Initiative efforts; meanwhile, the NSF's lead on the project, biological sciences chief John Wingfield, said in September that the agency intended to wait for the NIH report before issuing its own plan, to avoid duplication. \u201cThere are limits to what we can do,\u201d he said, contrasting his agency's roughly $150-million annual expenditure on neuroscience with the NIH's $5.5-billion budget. Meanwhile, almost everyone was worried about where the funding would come from \u2014 especially as it became clear that Congress would not set aside any new money for the BRAIN Initiative's first year. A modicum of new funding should be forthcoming from the NIH: of the $40 million it agreed to commit, $10 million will come from the director's discretionary funds. Officials at the NIH and NSF maintained that the initiative would not derail existing programmes. But the dearth of dedicated new funds meant that the three federal agencies would have to begin, at least in part, by packaging together some ongoing projects. The initiative's private partners, likewise, will mostly stick with existing programmes. The Salk Institute for Biological Studies in La Jolla, California, the Allen Institute for Brain Science in Seattle, Washington, and the Howard Hughes Medical Institute in Chevy Chase, Maryland, were all eager to frame the BRAIN Initiative as a continuation of research they already had under way. \n               Something for everyone \n             By early September, with the private partners determined to do their own thing and two of the three federal agencies all but silent, interest in the NIH report had reached fever pitch: researchers saw it as the de facto national agenda. On 16 September, the advisory committee at last published its interim report on science priorities. Many had feared that it would fail to be sufficiently inclusive, but the document was instead so staggeringly broad that it seemed to encompass all of circuit-based neuroscience. Cataloguing every cell type in the brain, mapping those cells' full anatomical connections, monitoring and manipulating their signals, modelling and simulation \u2014 there was something for everyone. \u201cIt would be hard to disagree with this report,\u201d said Mitra. \u201cIt's written, perhaps, with critics in mind.\u201d NIH director Francis Collins tacitly conceded the report's vastness when he formally accepted it. \u201cThese areas of research are expansive, and undoubtedly cover more research than NIH can fund with $40 million in one year,\u201d he said; the more ambitious elements could shape funding requests in years to come. And not just a few years, adds Yuste. \u201cThis is something you need 15 years and $3 billion to do.\u201d Tough choices lie ahead as the committee starts work on the long-term report it must deliver next June. The team will need to rank research priorities as short-, medium- and long-term goals, set timelines, estimate costs and define specific deliverable outcomes for the next few years \u2014 altogether a daunting task, says Newsome. \u201cIt's much easier to see a year into the future than ten years into the future,\u201d agrees Bargmann. Already, members of the working group are butting heads over questions such as whether ultra-detailed anatomical maps of the brain \u2014 painstakingly obtained with electron microscopy (see  page 147 ) \u2014 should take priority over lower-resolution maps that can be completed much more quickly using light microscopy. And then there is the question of management. Although the three government agencies have kept each other informed of their plans, the White House has so far indicated no intention to coordinate the process more formally. This worries researchers such as Yuste, who is urging the creation of 'brain observatories': multi-agency facilities that, like particle accelerators or giant telescopes, could provide community access to technology too large, costly or specialized for individual labs to maintain. Without higher-level planning, he warns, such efforts will be impossible, and the BRAIN Initiative's investment could end up being squandered on many small grants awarded by the individual agencies. \u201cThe whole effort will not be more than the sum of its parts,\u201d he warns. But others, including Bargmann, argue against throwing limited resources behind a monolithic, centralized project. \u201cThis is not the time to pick one approach and say this is the right approach,\u201d she says. Instead, she hopes to foster the strongest and most creative ideas from individuals and groups of researchers \u2014 and see where they lead. The NIH advisory committee hopes to draw on the creativity of the wider neuroscience community at the upcoming annual meeting of the Society for Neuroscience. On 11 November, hundreds of neuroscientists are expected to pile into room 33C at the San Diego Convention Center in California to weigh in on the NIH's interim recommendations. Armed with only a slightly more defined vision than they had six months ago, they will continue to try to define what the BRAIN Initiative can and should mean for their future. \n                     Neuroscience: New angles on the brain 2013-Nov-06 \n                   \n                     NIH serves up wide menu for US brain-mapping initiative 2013-Sep-17 \n                   \n                     US brain project puts focus on ethics 2013-Aug-14 \n                   \n                     Neuroscience: Solving the brain 2013-Jul-17 \n                   \n                     Behind the scenes of a brain-mapping moon shot 2013-Mar-06 \n                   \n                     Brain-simulation and graphene projects win billion-euro competition 2013-Jan-23 \n                   \n                     Computer modelling: Brain in a box 2012-Feb-22 \n                   \n                     Nature  special: New angles on the brain \n                   \n                     The White House BRAIN Initiative \n                   \n                     NIH on the BRAIN Initiative \n                   \n                     NIH interim BRAIN Initiative report (PDF) \n                   \n                     NSF on the BRAIN Initiative \n                   \n                     NSF workshop, May 2013 \n                   Reprints and Permissions"},
{"file_id": "502288a", "url": "https://www.nature.com/articles/502288a", "year": 2013, "authors": [{"name": "Brian Owens"}], "parsed_as_year": "2006_or_before", "body": "Many governments are assessing the quality of university research, much to the dismay of some researchers. Two years ago, academics at Lancaster University, UK, found themselves in the uncomfortable position of being graded. They each had to submit the four best pieces of research that they had published in the previous few years, and then wait for months as small panels of colleagues \u2014 each containing at least one person from outside the university \u2014 judged the quality of the work. Those who failed their evaluations were offered various forms of help, including mentoring from a more experienced colleague, an early start on an upcoming sabbatical or a temporary break from teaching duties. The university did not undertake this huge exercise just to make sure that the researchers were pulling their weight. The assessment was a drill to prepare for the Research Excellence Framework (REF), a massive evaluation of the quality of research at every university and public research institute in the United Kingdom, which is set to take place in 2014. The idea of the drill \u201cwas to identify areas where we could help people develop their profiles\u201d, says Trevor McMillan, Lancaster University's pro-vice-chancellor for research. Happily, he says, the results suggested that the university would score more highly than it did on the most recent national evaluation, in 2008. But other mock evaluations have proceeded less smoothly. In a survey of more than 7,000 UK academics published on 3 October by the University and College Union (UCU) in London, almost 12% reported having been told that failure to meet their university's REF benchmarks in a drill could lead them to be transferred to a teaching-only contract before the real REF (see  go.nature.com/eqiirr ). Almost 10% said that they faced denial of promotion. At Cardiff University, around ten academics were pressured to switch to teaching-focused contracts after they scored poorly on a practice exercise, so as not to drag down their department, says Peter Guest, an archaeologist at Cardiff and the university's UCU liaison on the REF. This form of game-playing is discouraged, but not expressly forbidden, by the REF \u2014 however, making career decisions solely on the basis of the evaluation is against the university's own policies, as well as those of many other institutions, says Guest. All of the Cardiff cases were resolved in a day or two, with managers being \u201cforcefully reminded\u201d of the rules by the UCU, says Guest. But the experience shows how tempting it is for institutions to make career decisions on the basis of predicted REF scores, which are highly subjective. This is neither reliable nor fair, says Guest. (In response to questions about the incident, a spokesman for the university said in an e-mail: \u201cWe have been running a long-term programme for over four years to ensure our academic staff are on contracts that reflect what they actually do.\u201d) Even many academics who did score well in the mock evaluations resent them. Around the United Kingdom, researchers view these national assessments as a bureaucratic imposition that can stifle creativity. \n               Under pressure \n             Most academics at Lancaster saw the mock REF as little more than a \u201cmildly annoying\u201d bit of bureaucracy, but the real thing is a different matter. \u201cWe have our department's top research professor working on preparing our REF submission, and it's taking up about a third of his time,\u201d says one member of the mathematics and statistics department. \u201cIt seems like a waste of talent.\u201d Too many researchers are focused on winning grants and trying to predict what kind of work will be rewarded in the next assessment, rather than doing the best science they can, says Dorothy Bishop, an experimental psychologist at the University of Oxford, UK. \u201cI think a lot of science is just not very well done these days because people are trying to do too many things.\u201d But university administrators and the government have come to rely on these evaluations to help them decide how to disburse funding. And the idea has been so popular with educational leaders that other countries are following the United Kingdom's example, with similar exercises cropping up in Australia, Italy, Germany and elsewhere. In the late 1980s, the United Kingdom became the first country to systematically evaluate the quality of its university research. The REF is the latest incarnation of these check-ups. Previously known as the Research Assessment Exercise (RAE), the evaluations are widely credited with helping to improve the country's research system. Between 2006 and 2010, citations of UK articles grew by 7.2%, faster than the world average of 6.3%; and the country's share of citations grew by 0.9% per year, according to a 2011 analysis conducted by publishing company Elsevier for the government. The assessment is used by the UK government to distribute more than \u00a31.6 billion (US$2.6 billion) a year in block grants to universities. More than 70% of the pot goes to the top-scoring 20 or so universities \u2014 last year, the University of Oxford got more than \u00a3130 million in quality-related funding \u2014 whereas the smallest, least research-intensive institutions make do with just a few tens of thousands of pounds. Assessment results are eagerly assembled into league tables, showing which universities are performing best in which subjects (see 'Top 5'). \u201cThe reputational aspects of it can be as important as the financial aspects,\u201d says McMillan. Some smaller institutions that are strong in particular subjects \u2014 as Lancaster is in physics \u2014 have reported that they have an easier time attracting students in those areas as a result of the assessments. And it is not just students. \u201cOne of the consequences is that people really want to come to a department that did well in the RAE,\u201d says McMillan. \u201cWe've found it easier to recruit high-quality staff in physics.\u201d For the REF, universities submit a selection of work from most of their active researchers to one of dozens of subject-specific panels known as Units of Assessment that correspond roughly, but not exactly, to university departments. The panels evaluate the quality of the research using peer review and metrics such as citation indexes. And they will also, for the first time, look at the economic and social impact of a university's research. Even critics of the assessments agree that they have had some positive effects on the country's research system. Because the exercises judge academics on the quality of their research, many departments have tried to cut back on other demands, such as administrative work, says Guest. Furthermore, the results make it clear which departments and academics are not pulling their weight, and allow universities to make strategic decisions about how to invest resources. Royal Holloway, University of London, faced that very situation after the first research assessment in 1986, which ranked the university's psychology department in last place nationwide, says Kathy Rastle, a cognitive psychologist and the department's director of research. Recognizing that it would not be able to boost its rating by hiring established stars, the department sought instead to attract and develop young talent. \u201cWe try to focus on people we feel have great potential,\u201d says Rastle. Early-career psychologists at Royal Holloway are now offered \u201csubstantial, but tailored\u201d start-up packages, she says, with hardly any teaching commitments for the first two years. They also get help from more experienced colleagues in preparing funding proposals. In the 2008 RAE, after two decades of nurturing junior staff, the department was ranked among the top ten in the country. It has ambitions to go even higher. \u201cI look forward to the REF as an opportunity to show what we've done, and to move up the ranks,\u201d says Rastle. \n               An idea spreads \n             As other countries begin their own national research evaluations, they hope to achieve the same kinds of benefits. This year, Italy published the results of an evaluation begun in 2011 (see  Nature   http://doi.org/nrx ; 2013 ); its goal is to increase meritocracy in the country's universities, where academics of the same rank and seniority currently receive the same salary, regardless of output. \u201cThere are no incentives to improve your research performance,\u201d says Giovanni Abramo, who studies bibliometrics and research evaluation at the National Research Council of Italy in Rome. \u201cNow some of the money the government gives to universities will be based on this evaluation.\u201d The Italian effort evaluates only three journal articles from each researcher with teaching commitments, whereas Australia assesses all research output as part of its Excellence in Research for Australia (ERA) initiative, most recently in 2012. Only a relatively small pot of funding rides on the results: this year, rankings determined the disbursement of just Aus$68 million (US$64 million). The outcome is mainly used to give institutions an idea of where they stand in terms of national and international quality, says Aidan Byrne, chief executive of the research council. The exercise has added benefits, he says. For example, it helps to verify that the council is distributing its Aus$800-million competitive-grants portfolio in a reasonable way. With a round of assessments costing Aus$4 million, says Byrne, \u201cit's a very efficient method of quality control\u201d. Although there is no formal connection between the ERA and the grants process, the academics who peer-review grant applications are aware of ERA outcomes, and that feeds into their decisions, he says. \n               Growing pains \n             It is too early to know how the newer assessment efforts in Italy, Australia and other countries will affect the research environment there (see 'Stand and be counted'). But researchers say that they have seen enough of the long-lived UK programme to know some of the downsides. One of the main worries that came up in the UCU's survey is the stipulation by many universities that researchers must have produced four high-quality publications between 2008 and 2013, says Stefano Fella, a national industrial-relations official at the union. Of the academics polled, 67% felt that they could not produce the required output without working excessive hours \u2014 and 34% said that the stress was affecting their health. Many have reported changing how they approach their work, says Fella \u2014 for example, some might have rushed to get a publication in the assessment period, even if the work might have benefited from more time. \u201cThey don't think about the best way to present the work,\u201d says Fella, \u201cbut what would be best for the REF.\u201d Frederic Lee, an economist at the University of Missouri\u2013Kansas City, has studied how the UK research-assessment system has affected his discipline. He experienced two rounds of assessments first-hand while working at De Montfort University in Leicester in the 1990s. He says that economists who study alternative theories such as Marxism have been squeezed out because the assessment has consistently favoured mainstream work at elite institutions, published in a small subset of journals. \u201cThere has been a lemming effect that has led to a homogeneity of research topics,\u201d he says. Lee says that he was never pressured to abandon his research on the history of heterodox economic theories in the United Kingdom, but was encouraged to submit his work to particular mainstream journals, where it stood a slim chance of getting accepted. Other academics have told him that they have been pressured to switch to more conventional research topics, and some had been squeezed out of departments at major institutions.  Nature  spoke to one economist as the University of Manchester who studies alternative theories, and who left the department in part because the focus on RAE-friendly theories meant that prospects for advancement seemed essentially non-existent. Academics are particularly worried about the move to assess the impact of research in the REF. They fear that this signals a preference for short-term, applied work over basic research that has no obvious, immediate public benefit. \u201cAs far as I'm concerned, you should do good science, and not think in this appallingly strategic way,\u201d says Bishop. \u201cSome good science takes a long time to do well.\u201d  You should do good science, and not think in this appallingly strategic way.  The time, effort and money being spent on submissions are also a major concern: preparations for the 2008 RAE cost universities \u00a347 million, according to a 2009 review of the exercise. Even smaller universities such as Lancaster asked several academics to spend months reviewing applications for mock REFs. The time burden can be even worse for administrators, who might have to hire extra staff to work on the REF, says Bishop. University College London, for example, has recruited four editorial consultants to work on the impact portion of the assessment. McMillan says that it is natural to spend a bit more time and money when preparing to tackle a new criterion. \u201cIt's a dimension that we're not used to.\u201d He adds that administrators at Lancaster are hiring external professional editors to help with only the final part of the process: polishing the case studies and impact statements that are written by academics and the university's research support office. Still, McMillan himself is currently spending two to three days a week tweaking Lancaster's submissions. \u201cI think the REF is probably taking up more time than previous exercises,\u201d he says. \u201cThe shift to the impact agenda has seen a big increase in the workload.\u201d But some universities have seen the benefits of all that work. The vast improvements made by Royal Holloway's psychology department demonstrate how much periodic evaluations can help, says Rastle. \u201cHaving the REF hanging over our heads makes sure we take all the steps we can to get the best out of our people.\u201d \n                     Italian universities get report cards 2013-Jul-17 \n                   \n                     Experts will assess UK research 'impact' to award funding 2010-Nov-11 \n                   \n                     World view: Wild goose chase 2010-Jan-20 \n                   \n                     UK research funding proposal is 'irresponsible' 2009-Dec-16 \n                   \n                     Italy introduces performance-related funding 2009-Jul-28 \n                   \n                     Good grades, but who gets the cash? 2008-Dec-29 \n                   \n                     Nature  special: Metrics \n                   \n                     Research Excellence Framework \n                   \n                     Excellence in Research for Australia \n                   \n                     Lancaster University \n                   Reprints and Permissions"},
{"file_id": "501297a", "url": "https://www.nature.com/articles/501297a", "year": 2013, "authors": [], "parsed_as_year": "2006_or_before", "body": "As the IPCC finalizes its next big climate-science assessment,  Nature  looks at the past and future of the planet's watchdog. In December 1988, the United Nations General Assembly endorsed a call to create a panel to assess \u201cthe magnitude, timing and potential environmental and socio-economic impact of climate change and realistic response strategies\u201d. Now in its 25th year, the resulting Intergovernmental Panel on Climate Change (IPCC) has grown substantially from its early days, when just a few dozen experts convened to write its first scientific assessment report. Next week, the group will publish its fifth such report, which has been crafted by more than 250 lead authors and editors \u2014 as well as hundreds more contributors and reviewers \u2014 who spent five years on the project and had to deal with a flood of some 52,000 comments submitted in response to early drafts (see  page 298 ). As the IPCC has matured, it has become firmer in its message that humanity is warming the globe to a degree that will threaten much of the world's population over the next century. The panel's warning has grown more confident and specific with time, as have its assessments of strategies to mitigate future problems. In a special issue this week,  Nature  examines how the IPCC and climate science have evolved over the past quarter-century, and how scientific assessments can intersect with policy decisions. One News Feature on  page 300  investigates how much progress the panel has made on the topic of rising sea levels \u2014 one of the most controversial aspects of its previous big report, published in 2007. Another News Feature (see  page 303 ) profiles economist Ottmar Edenhofer, who is leading the IPCC working group that will next April issue a second report, on ways to mitigate climate change and its impacts. A third IPCC report, on the impacts of climate change, will come out next March. Even as the research on global warming has progressed steadily, nations have failed to forestall the problem. In a Comment on  page 307 , climate-policy analyst Elliot Diringer argues that individual actions by countries may be the best way forward. And on  page 310 , energy analyst K. John Holmes offers a look at how large-scale environmental assessments informed policy debates in the nineteenth century. The IPCC has won praise for its work, including a Nobel Peace Prize in 2007, yet many researchers question whether it should continue to produce giant, infrequent reports. Past assessments have already described the basic science on climate change, which is now broadly accepted by governments. The IPCC can still play an essential part in climate mitigation, but it is time to re-evaluate how it offers advice to nations (see  page 281 ). \n                     Nature  special: Outlook for Earth \n                   Reprints and Permissions"},
{"file_id": "501303a", "url": "https://www.nature.com/articles/501303a", "year": 2013, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "Getting hundreds of experts to agree is never easy. Ottmar Edenhofer takes a firm, philosophical approach to the task. Ottmar Edenhofer knows that he sometimes has trouble keeping his composure. So when he takes the front seat as chairman at international climate meetings, he attaches an index card to his desk and glances at it whenever the discussions turn testy. The sign warns: \u201cDon't lose your temper!\u201d He has needed that reminder often during his five years as one of the leaders of the Intergovernmental Panel on Climate Change (IPCC), where science frequently comes into conflict with powerful political interests as nations debate how climate is changing and what we can do about it. Those tensions nearly boiled over in 2011, when Edenhofer presided over a session in Abu Dhabi at which delegates from some 200 nations fought over almost every single sentence in the summary of a report on renewable energies. \u201cIt's an emotional roller coaster,\u201d says Edenhofer, chief economist at the Potsdam Institute for Climate Impact Research in Germany. \u201cThings can get pretty nasty when delegations play their little games of power, and the chairs must be prepared to parry attacks. My little reminder to always stay cool and polite has saved me from fits more than once.\u201d His temper will be tested again during the coming months as he guides his committee \u2014 the IPCC's Working Group III, which looks at ways to mitigate climate change \u2014 towards the end of its five-year reporting process. A week-long meeting in late June and early July in Addis Ababa turned into a marathon for the 232 lead authors of the report, who were faced with addressing more than 16,000 comments submitted by expert reviewers and governments in response to an early draft. Next month, Edenhofer's committee must send a revised version to governments to prepare for April 2014, when lead authors and government representatives will meet in Berlin to hammer out the final report. \n               Blueprints for a green future \n             Edenhofer will need to marshall a unique blend of skills, honed during a stint as a Jesuit philosophy scholar and through research on game-theory, as he completes the most sweeping compendium yet of technology and policy options that might spare humanity from the worst of climate change in the coming decades. The report will lay out a range of scenarios \u2014 and the costs and risks of each \u2014 for transforming societies to stabilize greenhouse-gas concentrations at reasonably safe levels. The results will inform the political process through a round of global climate negotiations intended to culminate in a treaty in 2015. And as nations continue to disagree over the architecture and ambitions of that pact, the IPCC's take on mitigation will come under heavy scrutiny. The IPCC is charged only with laying out the science \u2014 and, in the past, critics have accused the group's leaders of overstepping the boundary between analysing research and advocating for action. Well aware of that risk, Edenhofer has strived to keep his group focused on a strictly scholarly agenda \u2014 and, in so doing, he has won over some of the IPCC's past critics. \u201cOttmar has amazing skills,\u201d says Robert Stavins, an environmental economist at Harvard University in Cambridge, Massachusetts, who did not work on the last IPCC report out of concerns that it had grown too political. \u201cI can't think of anybody better fitted for the job.\u201d Philosophy was Edenhofer's intellectual refuge early on. Hailing from an arch-conservative part of Bavaria in south Germany, he shocked his parents \u2014 and prompted the local book seller to declare him crazy \u2014 when he spent his savings at the age of 14 on the three volumes of  Das Kapital  by Karl Marx. Reading the German philosopher's critique of the capitalist mode of production didn't turn him into an ardent Marxist, but did spark his interest in political economics. A couple of years later, the writings of German sociologist Max Weber prompted him to ponder the value of science and the difficult relationship between values and facts \u2014 problems that are right at the core of any serious debate on the role of the IPCC. In the mid-1980s, Edenhofer studied economics at the University of Munich, Germany, but his academic career soon took an unusual detour. In 1987, he joined the Jesuit order in Munich to immerse himself in Western philosophy and, later, in theology. Soaking up the works of Weber, Ludwig Wittgenstein and John Dewey, he learned to embrace different lines of reasoning. In 1991, he also became involved in setting up a Jesuit Refugee Service in Bosnia and Croatia during the Yugoslav Wars, and later earned a PhD in economics at the Technical University of Darmstadt, Germany. His roots in philosophy are still palpable in the way that he approaches the climate conundrum.\u201cThere is a whole space of morally legitimate standpoints with a view to climate change,\u201d he says one day in May, while sipping coffee between sessions of a workshop on climate agreements in Berlin. His boyish face, framed by round spectacles, grows animated as he lays out the various perspectives. \u201cOne might legitimately argue that the fight against global warming is as morally imperative as abolishing child labour or slavery. One might argue \u2014 just as legitimately \u2014 that poverty and diseases in many parts of the world are more imminent problems that should be addressed first.\u201d However, he adds, some perspectives cannot be tolerated. \u201cDenying out-and-out that climate change is a problem to humanity, as some cynics do, is an unethical, unacceptable position.\u201d The upcoming Working Group III report that Edenhofer is presiding over is a massive, complex tome. A compendium of hundreds of scientific papers, it analyses how societies can slow down climate change and reduce its effects by altering all sectors of the economy, from electricity production to transportation to building design. The importance of his group's work has grown as the greenhouse problem has worsened with no political solution in sight. Since the 2007 IPCC report, emissions of heat-trapping gases have continued to grow despite a global economic recession. In 2012, annual emissions of such gases were equivalent to more than 50 billion tonnes of carbon dioxide and reached an all-time high. And, in May 2013, the concentration of atmospheric CO 2  crossed the ominous threshold of 400 parts per million (p.p.m.) for the first time since human beings appeared on Earth. The report will provide a range of scenarios, and cost estimates, for stabilizing atmospheric CO 2  concentrations at 450 or 550 p.p.m.. It will make it clear that all realistic stabilization scenarios are decidedly at odds with current emissions trends. In fact, if one factors in methane, nitrous oxide and other warming gases that are governed by the 1997 Kyoto Protocol climate treaty, the combined concentration has already surpassed the equivalent of 450 p.p.m. of CO 2 . As a result, stabilization plans must allow for nations to temporarily overshoot a target before concentrations might start to subside. The report will also make clear that the problem will only grow worse without action. Known hydrocarbon reserves still buried in the ground may contain up to four times as much carbon as has been released into the atmosphere since the onset of the Industrial Revolution (see 'The carbon age').  Things can get pretty nasty when delegations play their little games of power.  But in line with the IPCC's mandate \u2014 which requires the group to be policy-neutral \u2014 the assessment of Working Group III will avoid promoting certain mitigation options over others. Maintaining that distance will be a crucial test of Edenhofer's leadership. \u201cThe IPCC is a scientific body whose task is to compare and review the relevant literature,\u201d says Stavins. \u201cUnfortunately, in my view, the IPCC has in the past overstretched its mandate. It has become too political \u2014 and that hasn't done it and the field any good.\u201d Stavins feels, for example, that prominent IPCC members transgressed by lobbying for green policies such as emission cuts and carbon taxes. Edenhofer is keen to steer clear of such territory: he often compares the task of the IPCC to that of map-makers rather than to that of political advisers. But he also knows that the report, by necessity, will weigh in on politically charged issues such as nuclear power, biofuels and geoengineering. When the report's summary for policy-makers \u2014 its most-read and most-disputed section \u2014 goes up for debate in April, the fighting among diplomats and scientists will be even tougher than it was in past sessions. Together with his Cuban and Malian co-chairs, Edenhofer will have to see, as patiently as his nature allows, that the haggling over the tiniest words proceeds in a civilized and productive manner. He faces the date \u2014 his last as an IPCC official \u2014 with a mixture of anxiety and gladiatorial anticipation. \u201cWhere I grew up, the boys were always ready for a fight,\u201d he says. \u201cI'm not like that \u2014 but trust me, I do know when it's time to thump on the table.\u201d The stakes are mounting on his group's report. With its analyses of low-carbon energy options and different policy paths towards stabilizing greenhouse-gas concentrations, the report will be an important contributor to the upcoming negotiations as nations try to hammer out an international treaty over the next year. When the last round of IPCC Assessment Reports came out in 2007, the body came under fire for including a few claims that lacked strong scientific support; in one high-profile gaffe, Working Group II uncritically repeated a baseless assertion that Himalayan glaciers would disappear by 2035. Edenhofer has pushed his own working group to thoroughly overhaul its procedures for evaluating scholarship to avoid such embarrassing blunders in his report. In particular, he has reduced the use of 'grey' literature \u2014 information not subjected to peer review, such as reports from environmental groups, governments and companies \u2014 to a minimum. And, perhaps drawing on his background as a philosopher and theologian, Edenhofer has also broadened the scope of the IPCC's mitigation working group to give ethical considerations greater weight. For the first time, professional philosophers have been invited to contribute to the report's opening chapters on equality, risks and sustainability issues, which set the scene for more technical sections. But even as it details the various options that nations might take, the report will not take sides, he stresses \u2014 because that involves decisions based on values and priorities that fall to elected officials. \u201cScience cannot, and can't be expected to, provide simple yes or no answers,\u201d he says.  Science cannot, and can't be expected to, provide simple yes or no answers.  To tread that neutral line, Edenhofer will have to control his own strong opinions, formed during his years as an economist and climate-policy expert. He favours cap-and-trade schemes, for example, over a direct carbon tax as the most effective way to cut emissions and promote climate-friendly innovation. And he is adamant that the global transformation to a low-carbon economy cannot be achieved \u2014 no matter what goals key emitters might commit themselves to \u2014 without substantially increasing the use of renewable energy in all sectors of the economy. In spite of his own opinions, he has focused on delivering an unassailable product. \u201cOttmar is keen to get the best science for the next IPCC report, which he wants to lift to a new level of quality,\u201d says Massimo Tavoni, deputy coordinator of climate-change programmes at the Eni Enrico Mattei Foundation in Milan, Italy. Edenhofer is also ready to acknowledge the limits of knowledge in his field. The most important uncertainty, he says, concerns the reliability of economic models used to forecast the future. They rely on macroeconomic equations and assumptions that are often thwarted by real-world developments. Economists are well aware that although the models can anticipate broad trends, they have no ability to forecast disruptions such as major financial or political crises. And great uncertainty remains over how nations will tackle climate change. How much will countries cooperate? To what degree will they rely on nuclear power? How quickly will renewable energy be deployed and at what price? Beyond those near-term concerns, researchers must also grapple with more distant potential mitigation strategies such as capturing and storing carbon on a massive scale, or large geoengineering projects aimed at rapidly staving off warming. As Working Group III tackles such uncertainties, it will also wade into an increasingly contentious debate about the benefits of creating biofuels from plants and bacteria. Since the IPCC's 2007 report, a fast-growing body of literature has split over whether the indirect effects of growing crops for fuels do more harm than good to the climate. Fears that excessive bioenergy production might cause food shortages make the debate even fiercer. \u201cClearly,\u201d says Edenhofer, \u201cthis is one of the most controversial issues we're dealing with. By now, debate over bioenergy has outstripped controversy over nuclear energy.\u201d The IPCC, he says, will summarize the pros and cons as authoritatively as scientific knowledge allows. \n               Teary finale \n             Edenhofer is convinced that the IPCC is better placed than any other group to address such thorny issues, because the final reports are vetted not only by scientists but also by political appointees from member nations. No purely science-led exercise could possibly have equal weight, he says. Although some scientists have started to question the utility of the IPCC, especially its drawn-out procedures, Edenhofer says the process should continue. The \u201cmiracle\u201d of the IPCC, as he puts it, is that it forces governments to deal seriously with science. \u201cTo be able to engage and criticize our work, governments do need to carefully read our reports,\u201d he says. \u201cHere's a unique mechanism for bringing science to the very level of government leaders.\u201d As he steers his group through that process, he will make frequent use of his desktop index card. But he has faith in the process. \u201cWhen everything is said and done, and when even the most hard-boiled negotiators have tears in their eyes, it is the cause of science \u2014 and not power interests \u2014 that has the last word.\u201d \n                     Renewable power: Germany\u2019s energy gamble 2013-Apr-10 \n                   \n                     The Kyoto Protocol: Hot air 2012-Nov-28 \n                   \n                     Durban maps path to climate treaty 2011-Dec-13 \n                   \n                     IPCC signs up for reform 2010-Oct-19 \n                   \n                     IPCC flooded by criticism 2010-Feb-02 \n                   \n                     The real holes in climate science 2010-Jan-20 \n                   \n                     Newsmaker of the year: Rajendra Pachauri 2007-Dec-19 \n                   \n                     Nature  special: Outlook for Earth \n                   \n                     Nature  special: After Kyoto \n                   \n                     Mercator Research Institute \n                   \n                     Harvard Project on Climate Agreements \n                   Reprints and Permissions"},
{"file_id": "501298a", "url": "https://www.nature.com/articles/501298a", "year": 2013, "authors": [{"name": "Nicola Jones"}], "parsed_as_year": "2006_or_before", "body": "A graphical tour through the history of the Intergovernmental Panel on Climate Change and the science that underlies it. The Intergovernmental Panel on Climate Change (IPCC) was founded 25 years ago to provide authoritative assessments on the emerging problem of climate change. Since its first report in 1990, the IPCC has issued increasingly complex follow-ups about every six years. The climate models that feed into the assessments have grown bigger and better, but researchers have not succeeded in reducing some key uncertainties about climate change. Where the reports have grown most firm is in declaring that humans are causing the world to warm. Reprints and Permissions"},
{"file_id": "502026a", "url": "https://www.nature.com/articles/502026a", "year": 2013, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "Probing the biological basis of certain traits ignites controversy. But some scientists choose to cross the red line anyway. Growing up in the college town of Ames, Iowa, during the 1970s, Stephen Hsu was surrounded by the precocious sons and daughters of professors. Around 2010, after years of work as a theoretical physicist at the University of Oregon in Eugene, Hsu thought that DNA-sequencing technology might finally have advanced enough to help to explain what made those kids so smart. He was hardly the first to consider the genetics of intelligence, but with the help of the Chinese sequencing powerhouse BGI in Shenzhen, he planned one of the largest studies of its kind, aiming to sequence DNA from 2,000 people, most of whom had IQs of more than 150. He hadn't really considered how negative the public reaction might be until one of the study's participants, New York University psychologist Geoffrey Miller, made some inflammatory remarks to the press. Miller predicted that once the project turned up intelligence genes, the Chinese might begin testing embryos to find the most desirable ones. One article painted the venture as a state-endorsed experiment, selecting for genius kids, and Hsu and his colleagues soon found that their project, which had barely begun, was the target of fierce criticism. There were scientific qualms over the value of Hsu's work (see  Nature   497 , 297\u2013299; 2013 ). As with other controversial fields of behavioural genetics, the influence of heredity on intelligence probably acts through myriad genes that each exert only a tiny effect, and these are difficult to find in small studies. But that was only part of the reason for the outrage. For decades, scientists have trodden carefully in certain areas of genetic study for social or political reasons. At the root of this caution is the widespread but antiquated idea that genetics is destiny \u2014 that someone's genes can accurately predict complex behaviours and traits regardless of their environment. The public and many scientists have continued to misinterpret modern findings on the basis of this \u2014 fearing that the work will lead to a new age of eugenics, preemptive imprisonment and discrimination against already marginalized groups. \u201cPeople can take science and assume it is far more determinative than it is \u2014 and, by making that assumption, make choices that we will come to regret as a society,\u201d says Nita Farahany, a philosopher and lawyer at Duke University School of Law in Durham, North Carolina. But trying to forestall such poor choices by drawing red lines around certain areas subverts science, says Christopher Chabris of Union College in Schenectady, New York. Funding for research in some areas dries up and researchers are dissuaded from entering promising fields. \u201cAny time there's a taboo or norm against studying something for anything other than good scientific reasons, it distorts researchers' priorities and can harm the understanding of related topics,\u201d he says. \u201cIt's not just that we've ripped this page out of the book of science; it causes mistakes and distortions to appear in other areas as well.\u201d Here,  Nature  looks at four controversial areas of behavioural genetics to find out why each field has been a flashpoint, and whether there are sound scientific reasons for pursuing such studies. \n               1 INTELLIGENCE \n             \n               Taboo level: HIGH \n             The comments that Miller made about Chinese families and the government wanting to select for intelligent babies touched a nerve still raw after many years. In the nineteenth century, British anthropologist Francis Galton founded the eugenics movement on the premise that extraordinary abilities, as well as deficits, were inherited. The movement led to abuses, such as forced sterilization of people deemed mentally inferior \u2014 generally minorities, poor people and especially people with mental illnesses \u2014 in countries around the world, including Germany, the United States, Belgium, Canada and Sweden. The term 'intelligence' is also a slippery one. Intelligence tests don't measure a wholly innate ability; it is possible, for example, to improve one's scores with practice. Nevertheless, about 50% of variability in intelligence seems to be inherited, posing an irresistible puzzle to some researchers. No one gene has been linked strongly to intelligence and many that have been weakly linked have also been questioned 1 . Earlier this year, in an attempt to find stronger genetic correlations, Chabris and a large international group of colleagues examined the genomes of more than 125,000 people and found three genetic variants, each of which had a small effect on the length of an individual's school career 2 . The authors speculated that the variants' influence on educational attainment came from their effect on intelligence. But the results triggered the usual rounds of condemnation and concerns over eugenics. Other detractors argued that such studies take the focus and funding away from other, non-genetic, factors such as poverty, which have a much greater effect on social mobility. Chabris says that the work can actually contribute to greater social mobility \u2014 for instance, by helping to identify preschoolers who could be helped by more intensive early childhood education. \u201cThe fact that people in the past interpreted the results in a certain way doesn't mean that it shouldn't be studied,\u201d he says. But not everyone buys that potential misuses of the information can be divorced from gathering it. Anthropologist Anne Buchanan at Pennsylvania State University in University Park wrote on the blog The Mermaid's Tale that rather than being purely academic and detached, such studies are \u201cdangerously immoral\u201d. Critics of the BGI project also point to signs that its data could be misused. After this summer's furore over Miller's interview, Hsu played down the potential for abuse. \u201cThere's a big gap between finding a few hits and finding thousands of hits \u2014 enough to predict the trait on the basis of the genotype \u2014 and we were never saying we were going to get to that point,\u201d he says. But in 2011, before the uproar over the study, Hsu told  Nature : \u201cI'm 100% sure that a technology will eventually exist for people to evaluate their embryos or zygotes for quantitative traits, like height or intelligence. I don't see anything wrong with that.\u201d One of Hsu's collaborators, behavioural geneticist Robert Plomin of King's College London, says that such talk has not been helpful. But after studying intelligence for 40 years, he has high hopes that this project and other sequencing ventures will help to pinpoint the many genetic contributors to the trait. Like Chabris, he says that the work could be used to target educational interventions. Moreover, like all of the intelligence researchers interviewed for this story, he says it is a fundamentally human trait and that it is worth searching for a genetic contribution. \u201cI'm optimistic that we will find it,\u201d he says. \u201cI'm not going to quit until we do.\u201d \n               2 RACE \n             \n               Taboo level: VERY HIGH \n             As far as genetic taboos go, race is probably one of the most heavily policed from within the scientific community, largely because of the way researchers have examined its intersection with other controversial traits, such as intelligence. This is due mostly to suspicion about what motivates the study. There is broad consensus across the social and biological sciences that groups of humans typically referred to as races are not very different from one another. Two individuals from the same race could have more genetic variation between them than individuals from different races. Race is therefore not a particularly useful category to use when searching for the genetics of biological traits or even medical vulnerabilities, despite widespread assumptions. Most researchers who examine genetic differences between populations take care to point out that the differences they observe reflect the geographic origins, reproductive history and migrations of these groups, not markers of some essential differences between them. However, some researchers have asked whether the taboo on the genetics of race has become so severe that it bars legitimate research. In 2005, for instance, geneticist Bruce Lahn of the University of Chicago in Illinois published studies 3 , 4  suggesting that variants of two brain-development genes possibly linked to intelligence are evolving differently in white Europeans and African ethnic groups. This provoked a wave of worried comments by scientists about how the studies might be interpreted. Among those who voiced concerns was then-director of the US National Human Genome Research Institute Francis Collins, now director of the National Institutes of Health (NIH) in Bethesda, Maryland. Lahn and his co-authors eventually found that the gene variants under selection were not linked to elevated intelligence 5 . But that report garnered little attention compared with the explosive studies that came before it. Lahn says he felt \u201cambushed\u201d during the debate over his findings. At meetings, even his co-authors did not defend him. \u201cMy friends said nothing,\u201d he says. Some argue that Lahn should have been more cautious. \u201cScience always plays out in a certain socio-political context, and you have to look at the consequences of how the science might play out,\u201d says John Horgan, a journalist who has written widely on the societal implications of science. \u201cResearch on race and intelligence is much more prone to supporting racist ideas about the inferiority of certain groups, which plays into racist policies.\u201d Horgan says that institutional review boards should ban or seriously question proposed studies on race and IQ. Lahn no longer works on the genetics of race and has urged researchers to have a more transparent discussion about whether such studies should proceed at all. \u201cGiven the history of the way race has been used in this country, maybe the research shouldn't be encouraged because it touches too many raw nerves. I'm OK with that,\u201d he says. \u201cBut I'm not OK with being ambushed by political discussions masquerading as scientific discussions.\u201d \n               3 VIOLENCE \n             \n               Taboo level: MILD \n             A decade ago, forensic psychiatrist Tracy Gunter of Indiana University in Indianapolis was spending her time trying to help people to overcome the behavioural and substance-abuse disorders that had led to their entanglement in the criminal-justice system. But it was becoming increasingly clear to her that once a client fell into an abuse\u2013crime spiral, it was very difficult to bring them back. It was around this time that researchers reported that people with a certain version of a gene called monoamine oxidase A ( MAOA ) have some protection from the effects of childhood abuse 6 . Other people who express low levels of the protein it encodes are more likely to commit crimes if mistreated. Gunter switched fields to work in behavioural genetics, hoping to find ways to identify and preemptively treat high-risk individuals. She soon found her work complicated by the difficulty of defining criminal behaviour precisely; the impossibility of separating environmental and innate influences; and, again, the emerging consensus that behaviour is influenced by numerous small genetic factors. Ten years on, she says, \u201cthe simplistic notions I had about behavioural genetics when I started this work are not true\u201d. Despite these caveats \u2014 and the fact that some studies have failed to replicate the original  MAOA  finding 7  \u2014 some lawyers have used  MAOA  gene tests, combined with history of childhood abuse or life stress, to try to mitigate sentences. In 2009, such testing led to a lesser charge for a Tennessee man who killed his wife's friend, and it convinced a judge in Italy to reduce a murderer's sentence by one year (see  Nature   http://doi.org/cttbjt ; 2009 ). But juries are often underwhelmed by genetic testimony: in the United States in 2008, for instance, defence lawyers attempted to convince a jury to be lenient towards a boy who had shot a bus driver. They presented evidence that the boy had a variant of a promoter of a serotonin transporter gene,  SLC6A4 , that is linked to depression in people under stress. The jury found the boy guilty of first-degree murder anyway. Outcomes are mixed, Farahany says, perhaps because the research is so oblique. \u201cIt doesn't seem to be enough to persuade judges or juries to change guilt or sentencing,\u201d she says. William Bernet, a forensic psychiatrist in Nashville, Tennessee, adds that, \u201ca genetic result does not directly cause a person to behave in a particular way. Juries seem to understand this\u201d. That may change as the science progresses, but so far genetics has held no more sway than conventional mitigating factors, which often include the milieu in which a person grew up. Those two domains are coming together as researchers look for more clues to the environmental factors that interact with genetics in influencing behaviour. Gunter was part of a team that showed that certain epigenetic modifications on the  MAOA  gene are linked to substance abuse in adult women 8 , and these modifications are influenced by a history of smoking. \u201cEvery year that I work in this field has been a lesson that it's not just genes or environment,\u201d she now says. \u201cIt's genes and environment that matter.\u201d Scientists continue to look at the genetics of violence, and of conditions such as psychopathy, although the tension between those who focus on just genes and those looking for genetic and environmental contributors is high, says James Tabery, a philosopher at the University of Utah in Salt Lake City. \u201cMy sense is that we're in a holding pattern; it's not clear what's going to happen next\u201d \u2014 specifically because not many genes have been linked to violence and attempts to replicate the  MAOA  findings have produced mixed results. \n               4 SEXUALITY \n             \n               Taboo level: MILD \n             Sometimes, shifting political winds can destigmatize research. In 1993, for instance, geneticist Dean Hamer, then at the US National Cancer Institute in Bethesda, encountered a firestorm of criticism from political conservatives when he published a report suggesting that a region of the X chromosome might be linked to homosexuality 9 . Scientists also criticized some aspects of his work. Today, studies on the genetics of sexual orientation have been embraced by the US gay community. The successful campaign to strike down a 2008 California ballot measure that banned same-sex marriage enlisted evidence that homosexuality has some basis in genetics. And the NIH has designated research on lesbian, gay, bisexual, transgender and intersex people a priority. \u201cThe tables have turned tremendously,\u201d says geneticist Eric Vilain, director of the Institute for Society and Genetics at the University of California, Los Angeles. But that does not mean that all research into the genetics of sexuality will be equally welcome, he adds. Vilain, for example, wants to study the epigenetics of homosexuality, in search of environmental influences that might affect the trait. The work hasn't been funded, but he predicts that if it is, it could upset some gay rights activists who have seen their cause benefit from the 'hardwiring' theory. He is keeping his fingers crossed. \u201cI hope that now that there have been significant social advances, that scientists can do their work in peace,\u201d he says. Such complexities are unavoidable in a democratic society in which citizens have a say on how public money is spent. Researchers must acknowledge that and take part in the broader conversation about the kinds of topics they want to pursue, Farahany says. \u201cYou hear this refrain in lots of areas of science, that because people will misuse science we shouldn't engage in scientific inquiry. I think that gets it backwards. If we're worried that people will misuse it, we need to create safeguards \u2014 and an open public dialogue that ensures responsible use.\u201d That, rather than censoring science or ignoring its implications, is perhaps the only way that Vilain and other researchers will get their wish: to do their work in peace. \n                 See Editorial \n                 page 5 \n               \n                     Chinese project probes the genetics of genius 2013-May-14 \n                   \n                     Biology and ideology: The anatomy of politics 2012-Oct-24 \n                   \n                     Genome test slammed for assessing \u2018racial purity\u2019 2012-Jun-12 \n                   \n                     Darwin 200: Should scientists study race and IQ? NO: Science and society do not benefit 2009-Feb-11 \n                   \n                     Darwin 200: Should scientists study race and IQ? YES: The scientific truth must be pursued 2009-Feb-11 \n                   \n                     Nature  special: The human genome at ten \n                   Reprints and Permissions"},
{"file_id": "501480a", "url": "https://www.nature.com/articles/501480a", "year": 2013, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "A mammoth undersea US project will soon start streaming data to researchers. But some wonder whether the system is worth its high price. On a sunny day in July, it takes 90 minutes for the R/V  Thomas G. Thompson  to traverse the locks connecting Seattle's inland waters to Puget Sound. On deck, John Delaney looks impatiently out to sea. As an oceanographer at the University of Washington in Seattle, he has made this trip many times to explore beneath the Pacific's waves. But Delaney hopes that this seven-week expedition will be the beginning of the end for these time-consuming journeys. \u201cWe don't want to be ship-bound,\u201d he sighs. Instead, he is spending money \u2014 a lot of money \u2014 to open a permanent window onto the sea floor. Delaney is the architect behind a 925-kilometre network of fibre-optic cable and instruments being installed on the seabed off the coast of Washington and Oregon. If all goes according to plan, these will stream real-time data back to shore by 2015, delivering some of the first live video footage of an underwater volcano erupting, hydrothermal vents growing and clouds of microbes billowing from the sea floor. The cabled network is a key part of the massive US Ocean Observatories Initiative (OOI), which aims to create a flood of continuous information from select sites. Oceanographers have long relied on brief glimpses of data from single research cruises or isolated buoys or moorings. The OOI, and Delaney, aim to exchange those flashes of insight for a constant spotlight. \u201cThe goal is to launch an era of scientific discovery,\u201d Delaney says, thumping his fist on the ship's deck rail. \u201cThis is a game-changer.\u201d Many US oceanographers have not yet considered just how the OOI's broad scope and potential might affect their research. But some who have been watching its development closely warn that the project is an expensive gamble. Construction costs will run to US$386 million, and the programme will then consume about $55 million per year for operations and maintenance. By the end of its planned 25-year lifetime, the OOI will have cost nearly $1.8 billion \u2014 an unprecedented price tag in oceanography. The running costs will eat up about one-sixth of the annual budget for ocean sciences at the US National Science Foundation (NSF), and that proportion could increase. \u201cThat money is being pulled right out of what could otherwise be allocated for peer-reviewed science,\u201d says Charles Eriksen, an oceanographer at the University of Washington who is not involved with the project. Critics also complain that the OOI sites cover only a fraction of a per cent of the world's oceans. Such objections hold little water with Delaney. \u201cPeople say cables are expensive,\u201d he says. \u201cWell, ships are expensive too.\u201d The hour and a half it takes to traverse Seattle's locks costs the University of Washington roughly $4,000. \n               Telecom spin-off \n             The OOI sprouted from a germ of an idea planted by Delaney and Alan Chave, an ocean scientist now at the Woods Hole Oceanographic Institution (WHOI) in Massachusetts. It was the early 1990s, and Delaney was frustrated with getting only enough ship time to visit the sea floor for a day or two every couple of years. Chave had been working for the US telecommunications giant AT&T, and he suggested hooking a piece of old telephone cable up to instruments on the seabed. The fibre-optic cable would provide electricity and stream data back to shore. \u201cThis is transformational,\u201d Delaney thought. The idea of a permanent oceanographic observatory slowly gathered steam at the NSF, and by 2007 the agency had decided to invest some $330 million in the concept. The problem was working out exactly what to build. To qualify as a national facility and justify the cost, the observatory needed to expand beyond sea-floor cables to include instruments that could plumb the full depth of the water column. OOI proponents initially dreamed up an observatory on steroids: multiple cabled arrays along with more than a dozen coastal and deep-water sites. Not surprisingly, the final project design cut back on most areas. \u201cYou have to build what you can afford,\u201d says Deborah Kelley, a marine geologist at the University of Washington and an OOI project scientist. After input from hundreds of researchers, the project team settled on three main components (see 'A mega-ocean observatory'). The first, and most ambitious, is the fibre-optic network southwest of Seattle, most of which has now been laid. This will connect dozens of sea-floor instruments across the Juan de Fuca tectonic plate, which slides under North America and drives seismic activity along the west coast from northern California to Vancouver Island in Canada. The instruments will focus on an active underwater volcano called Axial Seamount, and a formation called Hydrate Ridge, where methane vented from the sea floor feeds a unique ecosystem. The second component involves laying an array of moorings to support instrumentation off the east and west coasts of the United States. In each array, automated profilers will shuttle up and down cables measuring chlorophyll, oxygen and other factors from the sea floor to the surface. Six gliders will rove between moorings to make similar measurements. The third part of the project will use moorings and gliders to monitor four deep-water sites in the far north and south. These six sites will be run by WHOI, the Scripps Institution of Oceanography in La Jolla, California, and Oregon State University in Corvallis. Together, these stations will marshal the forces of about 760 sensors, of 47 different designs, to collect data on variables ranging from water temperature, salinity and density to acidity, carbon dioxide and oxygen levels. \u201cOne of the most transformational things is how interdisciplinary it will be,\u201d says Kelley. Elsewhere, similarly ambitious oceanographic observatories are already in use. Japan has two dense sea-floor observatories \u2014 DONET and DONET2 \u2014 with a focus on earthquake and tsunami studies. And many nations operate networks of buoys: the international Argo project has an array of more than 3,000 floats. The OOI will collect a broader selection of data than those efforts. In Canada, a cousin to the OOI has been up and running since late 2009. That project, called NEPTUNE Canada, involves 800 kilometres of fibre-optic cable laid on the northern part of the Juan de Fuca plate. The Canadian government has spent Can$200 million (US$194 million) on sea-floor observatories. The OOI's cabled observatory will be very similar, but because it is getting a later start, it will have newer designs of sensors and moorings, says Kate Moran, president of Ocean Networks Canada, which oversees NEPTUNE Canada. It is these sensors that Delaney is dreaming of as the R/V  Thompson  reaches the Axial volcano in July. When the captain pulls the ship to a halt above the underwater mountain, engineers manoeuvre a boxy yellow submersible called ROPOS down into the water. From 1,500 metres below the ocean surface, it sends back a murky video feed of an eight-metre-high mineral chimney, its sides festooned with microbial growth, its top spouting shimmering hot water. \n               Snapshot science \n             Visiting Axial like this costs almost $70,000 a day and provides only a snapshot of the volcano's behaviour. Oceanographers have, in the past, dropped off seismometers and hydrophones at Axial and retrieved them later, allowing them to study an eruption after the fact. And, in 2011, an expedition led by the US National Oceanic and Atmospheric Administration happened to arrive just months after the volcano erupted, providing scientists with a fortuitous opportunity to study fresh lava flows. But sightings of underwater eruptions are rare, even though this is the most common type of volcanism on Earth. Axial is expected to erupt again within a decade and OOI researchers plan to catch it in the act. A bottom-pressure tiltmeter will measure gradual changes that could indicate that the volcano is inflating, and a cutting-edge mass spectrometer will sniff the water for hints of magma rising from below. When an eruption seems imminent, a fleet of gliders could be deployed to study chemicals in the water column. A high-definition video camera, installed this summer, will watch a hydrothermal vent on Axial's flanks. All this should yield details about how magma reaches the volcano's summit, how that relates to seismic activity, and how organisms living in this extreme environment deal with an eruption. \u201cThere are so many questions we don't have answers to,\u201d says Kelley. Delaney says that the $126-million cabled observatory is the only way to spy on Axial properly. The OOI's investment in other parts of the ocean will also gather data that cannot be obtained using existing equipment, says Tim Cowles, programme director at the Consortium for Ocean Leadership, the organization in Washington DC that is overseeing the OOI. The four high-latitude deep-sea moorings, for example, will help to study the exchange between air and water where powerful winds whip the sea surface into a froth. Weather forecasters and climate modellers need better information on how energy and gas move between the deep ocean, the sea surface and the atmosphere. But because oceanographic equipment tends to take a beating in fierce weather, measurements are few and far between at high latitudes. A team led by Uwe Send at Scripps deployed 57 instruments at a site in the Gulf of Alaska in July, making it the first deep-sea OOI site to be completed. The coastal arrays, meanwhile, should be useful for studies of algal blooms. Gradual changes in near-surface temperatures can drastically affect how phytoplankton blooms form in the spring, but catching such changes in the act is tricky, says Kendra Daly, a biological oceanographer and OOI project scientist at the University of South Florida in St Petersburg. \u201cWe have no ability to predict exactly when that will happen, so it's hard to get ship time to be out there right when the bloom starts and ends,\u201d she says. Measuring nutrient concentrations and primary productivity on the spot will allow researchers to better quantify how much carbon dioxide is absorbed by blooms, which has implications for understanding how biological systems interact with climate, Daly says. To even start to tackle these science questions, the OOI team will need to get its instruments into the water before a tight deadline of February 2015. After that, the money available to complete the job will dry up. \u201cAll those things that have to happen in the last six months of the project do make all of us nervous,\u201d says Cowles. The project's schedule has been frantic since 2009, when the OOI received an initial input of almost $106 million from the government stimulus bill enacted in the wake of the economic recession. That infusion of funds was welcome, but it left the team scrambling to get everything in place. \u201cIt kicked us out of the gate ahead of schedule, and we've been trying to catch up ever since,\u201d says Cowles. At least one hurdle stands in the way of a timely completion. For now, the team led by the University of Washington can't connect the instruments it has deployed to the backbone power-and-data cables leading to the shore. The NSF requires that the company that made the cables, L-3 MariPro of Goleta, California, ensure that they are in proper working order before handing them over, and L-3 MariPro is running behind. Delaney's team had to shorten this summer's field season and bank some of its ship days for 2014, intending to connect the instruments then. And there are other, bigger worries. One is the daunting cost of maintenance. OOI sites are slated to be serviced every one, three or five years \u2014 depending on the equipment \u2014 which will run up a hefty ship-time bill. And unforeseen glitches are bound to strike. NEPTUNE Canada has run into major technical issues, including the failure of kilometres-long segments of cable and instruments that stopped working after just a year on the sea floor. Ensuring data quality is also a concern. \u201cThat's the big worry in my mind,\u201d says Douglas Luther, an oceanographer at the University of Hawaii at Manoa and an OOI project scientist. The OOI's cyberinfrastructure team is developing automated algorithms to flag up any obvious problems \u2014 such as sensors that record temperatures hundreds of degrees above those of neighbouring devices \u2014 but there is currently no money to fund a big quality-control team. Another question is how much demand there will be for the data. Not everyone will be able to abandon field trips in favour of using the OOI's instruments. Microbial oceanographer Julie Huber of the Marine Biological Laboratory in Woods Hole studies microbes living on and in Axial volcano. The OOI's cabled network isn't much use to her \u2014 so far there is no instrument she could plug in for her microbial monitoring. \u201cIt doesn't replace me having to go out there,\u201d she says. \n               Ocean outreach \n             The OOI's leaders could follow the example of their northern colleagues. Canada's undersea cabled networks, which cost Can$16 million annually to run, have nearly 8,000 active data users per year, which is right around the level their funders wanted to see. In large part, that is because Ocean Networks Canada employs six staff scientists to reach out to researchers and educators with suggested ways to use the data. \u201cUsers just don't come \u2014 you have to work at it,\u201d says Moran. So far, the OOI has no formal outreach plan. There are also fears that NSF programme managers will feel under pressure to fund projects that use OOI data, in order to validate its cost. \u201cMostly, we'll be paying a lot of money trying to make this hardware a success,\u201d says Russ Davis, an oceanographer retired from Scripps who helped to develop key floats for the Argo array. \u201cIf it isn't awfully wonderful, it's going to look bad for the NSF and be bad for science.\u201d For Delaney, the OOI's potential outweighs such concerns. As the  Thompson  makes its way across the Pacific, leaving the underwater volcano behind, he muses about the interconnectedness of the oceans. \u201cA single ship can only be in one place at one time,\u201d he says. \u201cWe need to be present in multiple places in multiple times.\u201d That omnipresent capability is what the OOI is all about. And if it costs a lot of money, Delaney wants the research community to keep the sums involved in perspective: NASA, for comparison, spends billions each year. \u201cOur investment in the ocean is way below our investment in outer space,\u201d he says. \u201cBut the return is much greater.\u201d \n                 See Editorial \n                 page 461 \n               \n                     Harnessing telecoms cables for science 2010-Aug-04 \n                   \n                     Undersea project delivers data flood 2010-Apr-20 \n                   \n                     Ocean science goes deep 2009-Oct-14 \n                   \n                     Oceanography: All wired up 2004-Jan-01 \n                   \n                     Ocean Observatories Initiative \n                   \n                     University of Washington's Interactive Oceans \n                   Reprints and Permissions"},
{"file_id": "501476a", "url": "https://www.nature.com/articles/501476a", "year": 2013, "authors": [{"name": "Heidi Ledford"}], "parsed_as_year": "2006_or_before", "body": "Third Rock Ventures made its name by placing big bets on the biotechnology companies it launched. Now, everyone is waiting for the pay-off. Bioengineer Mikhail Shapiro got a rude shock one day when he arrived for work at Third Rock Ventures, then a brand-new venture-capital firm headed by a handful of biotech elites. Only three weeks into his internship, Shapiro found a notice on the door: \u201cClosed for business.\u201d Inside, 'For sale' signs hung on desks, equipment, everything \u2014 even the office's giant gumball machine. The company had folded, a note explained, because it could not raise enough money. Kevin Starr, a partner at Third Rock, still beams with pride over that 2007 prank, which he and his confederates had filmed to capture Shapiro's reaction. \u201cYou could tell Mikhail was thinking, 'I knew that was going to happen to these guys!'\u201d he recalls. Few would fault Shapiro, now a professor at the California Institute of Technology in Pasadena, for his credulity. By 2007, the technology bubble of the early 2000s had burst, and investors were baulking at the long timelines and high failure rates involved in getting biotechnology products to the market. People laughed, says Starr, when he and Third Rock's other founders told them that the company wanted to raise US$378 million to create an investment fund to build biotech companies from scratch. \u201cThey advised us to aim for about a tenth of that.\u201d But Third Rock, based in Boston, Massachusetts, did raise its initial fund, and it has not slowed down since. The company has brought in $1.3 billion and invested in more than 30 young companies, many based on cutting-edge research in fields such as cancer epigenetics, gene therapy and medical diagnostics (see 'Due diligence'). Products are only just starting to trickle out into clinical testing, but this year brought several signs that the firm has bet well. In January, Third Rock sold off Lotus Tissue Repair \u2014 a tissue-engineering company with an experimental therapy for a devastating rare disease that weakens skin. The deal could garner a 20-fold return for Third Rock if Lotus meets certain milestones. In March, Third Rock's third round of funding \u2014 $516 million to launch up to 16 more companies \u2014 had so many aspiring investors that the firm had to turn some away. And this summer, two of Third Rock's companies went public, their share prices soaring the moment they hit the market. As  Nature  went to press, a third firm \u2014 cancer diagnostics company Foundation Medicine in Cambridge, Massachusetts \u2014 was preparing to follow suit. \u201cFor a long time, people said investing in these early-stage companies was not a great idea,\u201d says Robert Langer, a bioengineer at the Massachusetts Institute of Technology (MIT) in Cambridge who has spun off dozens of companies from his research (see  Nature   458 , 22\u201324; 2009 ). \u201cThird Rock has taken that risk and I think it's paying off.\u201d \n               Laid-back biotech \n             Since 2007, Third Rock has expanded its offices on Boston's trendy Newbury Street \u2014 a neighbourhood filled with high-end boutiques and cafes. On a flaming day this summer, Starr sits in his office arrayed in silver jewellery, camouflage shorts and a green T-shirt that reads \u201cBeach Punk 1982\u201d. A standard business shirt bides its time on a hanger behind the door. Starr's laid-back style has found lots of attention in the business press, and it serves as a reminder that he does not have to be here. In 2003, he left a post as chief operating officer of Millennium Pharmaceuticals, a Cambridge-based biotech powerhouse that had just launched the blockbuster cancer drug Velcade (bortezomib). Millennium founder Mark Levin retired some time after Starr, and the two did the usual things that young retirees with plenty of money do \u2014 travelling the world and producing independent films and Broadway shows. In 2006, Starr says, during an annual pilgrimage to the golf courses and blackjack tables of Las Vegas, Nevada, Levin turned to him and said, \u201cHey Kev, why don't we just go do something again?\u201d Venture capital has a pivotal role in transforming science into medical advances, supporting companies during the long, lean, research-intensive years before they have any hope of turning a profit. In the United States, biotech soaks up billions of dollars in venture capital each year, second only to the software industry. In the mid-2000s, infusions into fledgling companies made up just a tiny fraction of that investment. Most of the money was going to established companies, often with products already in clinical testing. But the pharmaceutical industry was tightening internal research budgets and looking to small biotechnology firms for new medicines. Amid that changing landscape, Starr and Levin saw an opportunity. There would be demand for innovative biotechnology companies, yet few venture capitalists were in a position to fill it. Through a series of meetings at Starbucks, Levin and Starr assembled a skeleton crew of biotech nobility and mapped out their ideal venture-capital firm. \n               Standing out \n             Levin, Starr and Bob Tepper, former head of research and development at Millennium, wanted to do things differently from typical venture capitalists, who sift through ideas and business proposals from external researchers, help to set up a company and then hand over control to a newly recruited executive team. Starr says that he and his co-founders wanted to recreate some of the magic they had felt at Millennium, carrying over its 'anything is possible' mantra. They would hire only the best people, even if that meant interviewing candidates for months. And, rather than relying on proposals from the outside, they would focus on the hottest science, mostly investing in companies conceived by Third Rock's team. \u201cLast year we saw 982 outside plans,\u201d says Starr. \u201cWe invested in zero.\u201d All venture capitalists need to understand the science behind their investments, but Shapiro, who has since worked with other venture-capital firms, says that Third Rock is unique in how far its members personally immerse themselves in the details. \u201cIt's a bunch of nerds,\u201d he says. \u201cYou're in a commercial setting, but the rigour of the science was as high as it was at MIT or Caltech.\u201d Of the more than 40 employees now at Third Rock, only Levin, a chemical engineer by training, had worked in venture capital before. The rest had trained in the trenches as scientists, physicians and biotech business leaders. \u201cThey have decades of real, hands-on experience,\u201d says Michelle Dipp, a venture capitalist at the Longwood Fund in Boston. \u201cIt's an incredibly talented team.\u201d Third Rock also takes its time handing over the reins of its companies to outside executives; it often waits 18 months or longer. That is important for luring top talent, says Langer. \u201cA lot of good chief executives are not willing to take the risk with a new company,\u201d he says. \u201cWith Third Rock, rather than getting the company when it's a newborn baby, a new executive is getting a pretty active 2-year-old.\u201d Finding newborns to raise means exploring promising ideas, something that Third Rock spends about one-third of its time doing. Those that pass muster get up to $2 million and must go through a rigorous and lengthy screening process that employees refer to as the 'Third Rock Ultra Killer Kriteria' (TRUKK). Independent labs must be able to replicate key findings and find no warning signs of toxicity for drug candidates. Third Rock also circulates the project idea to contacts at large pharmaceutical firms. If those companies have internal scientists working on the same project, Third Rock generally will not try to compete. Or if pharmaceutical insiders say that they like the idea but would not invest in it without seeing data from late-stage clinical trials, the project is scrapped. For all the talk of how anything is possible, Third Rock is ruthlessly practical. To meet the TRUKK, a project must be no more than about three years away from clinical testing \u2014 a brutal necessity of the ten-year funding cycles of venture capital, says Starr. This can mean painful decisions. A few years ago, the team evaluated the therapeutic promise of a class of gene regulators called long non-coding RNAs. The field is hot and the team loved its potential, but it was not close enough to the clinic to serve as the basis for a Third Rock company. Although draconian on paper, Third Rock has bent its rules for some early investments. Agios Pharmaceuticals in Cambridge develops drugs to target metabolic changes that fuel tumours. It was among the first companies Third Rock backed, in 2007, but it was August 2013 before the firm began clinical trials on its lead compound \u2014 a drug to combat tumours that contain mutations in the metabolic gene  IDH2 . Agios's chief executive, David Schenkein, says that the company's early drug leads did not work, forcing scientists to develop new candidates from scratch, but he has felt no pressure from Third Rock about the revised timeline. Starr says that Third Rock factors a few setbacks like this into its calculations. And when Agios went public in July, investors showed that the delay did not concern them either: Agios made $106 million and its stock rose 56% on the first day of trading. \n               Dangerous precedent \n             Some will watch companies such as Agios carefully, mindful of the history of Millennium Pharmaceuticals. Millennium was founded in 1993, intended \u2014 like many of Third Rock's companies \u2014 to be a 'product engine': a company based on a single scientific premise or technology that could generate multiple therapies. In Millennium's case, that premise was personalized medicine grounded on emerging human genomics data. That mission failed and, as with other firms of the era, Millennium's internal research programme foundered, burning up cash in the process. But the company did use its expertise to identify promising drug candidates from outside firms, bringing them into Millennium for the final stages of development and marketing. Long-term investors benefited: in 2008, the Japanese pharmaceutical company Takeda, based in Osaka, bought Millennium for $8.8 billion. There may be debate about whether Millennium was a scientific success, but there is no doubt that the company had a tremendous impact on the booming Boston\u2013Cambridge biotechnology industry. Former employees sit at or close to the helms of biotech companies across the region. They are often quick to credit Levin and the culture he created at Millennium. \u201cWe came in as scientists,\u201d says Rosana Kapeller, once director of molecular and cellular biology at Millennium and now chief scientific officer of Nimbus Discovery, a pharmaceutical company in Cambridge. \u201cWe left as entrepreneurs.\u201d \n               Bright promise \n             That environment allowed Millennium to succeed even after its initial mission failed, says Starr. \u201cWe built a company with the right formula and culture to be a successful long-term company,\u201d he says, \u201cas opposed to a company that just stuck with one model and ran right into the wall.\u201d Agios may be echoing Millennium in that respect. In addition to developing new compounds, it last year broadened its mission to include genetic diseases of metabolism. The firm has developed a compound that will enter clinical testing next year in patients with pyruvate kinase deficiency, a rare metabolic condition that causes severe anaemia. Years will pass before Third Rock's success \u2014 whether measured in medical breakthroughs or in cash returns to investors \u2014 can be assessed. But people familiar with the team are certain of one thing: the pranks will continue. Asked to confirm the details of the joke that Starr played on him in 2007, Shapiro was confused. \u201cWhich one?\u201d he asked. \u201cIt's a really high-powered group of people with an incredible track record of achievement,\u201d he says. \u201cBut it's also a group of people who don't take themselves too seriously.\u201d \n                     Biotech boom prompts fears of bust 2013-Aug-27 \n                   \n                     Women in biotechnology: Barred from the boardroom 2013-Mar-06 \n                   \n                     Investment relief for biotech sector 2012-Nov-14 \n                   \n                     Blogpost: French biotech sector shows lively signs in bad economy \n                   \n                     Blogpost: Medical academy unites with venture firm to guide biotech investing \n                   \n                     \n                         Nature Biotechnology  \n                       \n                   \n                     Third Rock Ventures \n                   Reprints and Permissions"},
{"file_id": "502022a", "url": "https://www.nature.com/articles/502022a", "year": 2013, "authors": [{"name": "Devin Powell"}], "parsed_as_year": "2006_or_before", "body": "The Gaia spacecraft will soon launch on a mission to chart the heavens in unprecedented detail. The century-old brass telescope was broken in places and long past its useful life \u2014 but it captured Lennart Lindegren's heart. Forty years ago, when Lindegren was a graduate student at the Lund Observatory in Sweden, he fell in love with the elaborate, once cutting-edge technology that had allowed nineteenth-century astronomers to track and time the motion of the stars. The telescope had an ingenious mechanical stopwatch \u2014 originally invented to time race horses \u2014 and a large metal wheel that could adjust its angle. \u201cI got so fascinated by the beauty of the instrument that I wanted to get it working again,\u201d says Lindegren, who is now on the Lund Observatory's staff. He might as well have fallen in love with a sundial. Astrometry \u2014 mapping the locations and movements of celestial objects \u2014 was once a central concern in astronomy, with roots going back to ancient Babylon and China. But by the 1970s it had long fallen out of fashion. Astronomers had just about reached the limit for improving the precision of such measurements taken from the ground, and most had moved on to other questions. Astrometry, says Lindegren, \u201cwas not regarded as a field that would offer any great prospects for young scientists\u201d. He eventually gave up on repairing the telescope, but never abandoned the idea of reviving astrometry. Better star maps, he argued, could help astronomers to answer some fundamental questions, from how the Milky Way evolved to what makes up the dark matter that accounts for most of the Universe's mass. All researchers would need to do would be to get their astrometric instruments into space, above Earth's turbulent atmosphere, which subtly distorts starlight and limits the precision of measurements. In November, a proposal by Lindegren and like-minded scientists will bear fruit when the European Space Agency (ESA) launches Gaia: an astrometric mission that required many compromises and 13 years to complete, and will cost about \u20ac1 billion (US$1.4 billion). Gaia will make observations for the next 5 years; the results will extend the reach of high-precision maps from the roughly 2.5 million stars near Earth to at least 1 billion stretching to the edge of the Milky Way or beyond. For an estimated 10 million of those objects, Gaia's map will be fully three-dimensional: the spacecraft will measure not just the stars' locations on the sky, but also their distances from Earth, accurate to less than 1%. For now, the distances to only a few hundred stars are known at this level of precision. Michael Perryman, an astronomer at the University of Bristol, UK, and former project scientist for the mission, is optimistic. \u201cGaia will be tremendous and transformational, a huge leap forward both in terms of the number of stars measured and the accuracy of those measurements,\u201d he says. \n               Stellar cartography \n             The keen eyesight that will make this leap possible starts with Gaia's digital camera, which uses light-gathering sensors similar to those found in consumer cameras \u2014 but 106 of them, providing a resolution of more than 900 megapixels. By contrast, the main camera on NASA's Hubble Space Telescope has two sensors with a resolution of just over 16 megapixels. Guiding starlight into the camera are two telescopes that point 106.5\u00b0 apart, to take in a wide field of view. As the spacecraft spins, completing a full revolution once every 6 hours, that view will sweep across the same stars, month after month. Each star will be photographed about 70 times, producing roughly twice as much imaging data in 5 years as Hubble generated during its first 21 years in orbit. When all the data have been analysed, they will provide a pair of coordinates for each star, pinpointing its position in the sky with an error as small as 6 microarcseconds \u2014 the size of a small coin sitting on the Moon as viewed from Earth. That is hundreds of times better than today's best catalogue, and millions of times better than the first known Western star atlas, compiled more than 2,000 years ago through naked-eye observations by the ancient Greek astronomer Hipparchus of Nicaea. Finding a star's position in three dimensions will require further measurements. Because of a geometric phenomenon known as parallax (see 'The parallax effect'), stars appear to move from side to side as Earth orbits the Sun. The closer a star is to Earth, the larger its apparent movement, for much the same reason that trees on the side of a road seem to whiz past a speeding car, whereas a mountain in the distance barely seems to move at all. If astronomers can measure that side-to-side motion precisely, simple geometry will allow them to use the known size of Earth's orbit to calculate the star's distance. Atmospheric turbulence so compromises such efforts that even the best modern ground-based visible-light telescopes can see parallaxes up to only about 100 parsecs (a few hundred light years). Radio telescopes are less affected, so they can see much farther \u2014 but only for objects that emit strong radio waves. From its place outside the atmosphere, Gaia, which is destined for a stable orbit that will remain fixed relative to both the Sun and Earth, should be able to obtain parallax measurements for stars up to about 10,000 parsecs away. The same precision should let it measure a star's 'proper motion' across the sky at even greater distances. Proper motion \u2014 the result of a star's actual movement through space perpendicular to the line of sight \u2014 will show up as a steady sideways drift in the star's position, superposed on its annual side-to-side motion. Finally, Gaia should be able to use changes in the spectrum of light emitted by each star to measure the star's velocity towards or away from Earth. The result will be a complete portrait of the star's position and velocity in three-dimensional space. \n               Astrometry reborn \n             Gaia is not the first high-precision space-astrometry instrument. The feasibility of the exercise was demonstrated two decades ago by Gaia's predecessor, ESA's High Precision Parallax Collecting Satellite (Hipparcos). Launched in 1989, the \u20ac580-million spacecraft ran into trouble almost immediately, when a failed booster rocket left it in the wrong orbit. But even so, it worked: by the time the mission ended in 1993, Hipparcos had provided the distances to about 118,000 stars. Of those, some 400 were measured with an error of 1%. Only 50 had been measured so well from the ground. The Hipparcos star catalogue is still the best one available. However, the mission focused on relatively nearby stars, says Shrinivas Kulkarni, an astrometer at the California Institute of Technology in Pasadena, so the Hipparcos catalogue was more an evolution in the science than a revolution. \u201cIt showed us that our basic understanding of stars was sound,\u201d he says. As a proof of principle, says Erik H\u00f8g, an emeritus astronomer at the Niels Bohr Institute in Copenhagen who drew up the first blueprints for the mission, \u201cHipparcos's success was really invigorating for astrometry. People could gather around this project.\u201d What they could not do, until now, was get a worthy successor off the ground. One after another, advanced astrometry missions were proposed and then failed because they overran their budgets. One of the most spectacular examples was NASA's Space Interferometry Mission (SIM). It had a deliberately narrow focus: it would have concentrated on pinning down the positions and movements of a mere 10,000 stars located fairly nearby. The trade-off was that SIM could have detected wobbles in those stars caused by the gravitational pull of orbiting planets as small as Earth. But the project was postponed several times, and its original $600-million budget ballooned. After NASA had already spent hundreds of millions of dollars on development, projections suggested that the mission would need a further $1.2 billion and SIM was cancelled in 2010. \u201cThe money just wasn't there,\u201d says Michael Shao, an astronomer at NASA's Jet Propulsion Laboratory in Pasadena and formerly project scientist for SIM. Some researchers argue that Gaia will succeed where SIM and others failed because of Europe's style of building spacecraft. \u201cIn the European system,\u201d says Ken Seidelmann, an astronomer at the University of Virginia in Charlottesville, \u201cthe scientists write down the specifications they want, and the contractors come up with cheaper ways of doing things\u201d \u2014 ways that sometimes involve cutting back the mission's capabilities. \u201cIn the United States, the scientists tend to stay more involved,\u201d he adds \u2014 and if they refuse to compromise on the objectives, costs can skyrocket. In Gaia's case, keeping close to the budget set when ESA approved the mission in 2000 required a series of downgrades that substantially reduced the capabilities of the original design, mainly by halving the expected accuracy of its parallax measurements. The cuts meant that some of the problems that Gaia had intended to tackle were now out of reach. For example, the mission would no longer be able to track potentially hazardous near-Earth objects such as asteroids well enough to predict their motion for the next century \u2014 a goal that had been named a top priority by a task force led by the UK minister of science. The downgrades led Perryman to quit the project in 2006, after six years as leader of the scientific team. \u201cI was enormously frustrated by the decision to de-scope this project, which was not made on scientific grounds,\u201d he says. But Gaia survived and is now scheduled for launch as early as 20 November. With a bit of distance and a graveyard of space astrometry missions to reflect on, Perryman now expects big things from the mission. \n               Gaia goes galactic \n             To start with, the cosmic census begun by Hipparcos will continue and expand. Millions of new binary stars are expected to show up, as are tens of thousands of brown dwarfs: 'failed' stars too small to ignite by fusing hydrogen. Gaia should also discover 1,000 Jupiter-sized planets \u2014 or, rather, the wobbles these objects cause in nearby stars. Closer to home, the spacecraft will get at least some data on the hundreds of thousands of Solar System asteroids expected to cross its field of view. Where Gaia will really shine, however, will be in extending astrometry across the Milky Way (see 'Gaia's reach'). \u201cOur unique science goal is to unravel the structure and dynamics and history of our Galaxy,\u201d says Jos de Bruijne, a systems scientist at ESA's European Space Research and Technology Centre in Noordwijk, the Netherlands, and Gaia's deputy project scientist. Astronomers already know the basics, he says. The Milky Way is shaped something like a fried egg, with a bulge of stars in the middle surrounded by a flat stellar disk that tapers at the edges and contains the Galaxy's spiral arms. Around the disk is a diffuse sphere of old stars called the halo. But astronomers are not certain how these structures formed, or in what order (see  Nature   490 , 24\u201327; 2012 ). Gaia will provide one important set of clues by measuring stellar composition and brightness \u2014 data that will reveal for the first time when many stars formed, and will help astronomers to work out the ages of the Galaxy's different parts. Another set of clues will come with Gaia's measurements of stellar movements, which astronomers can extrapolate back in time to find out how the Galaxy has evolved. Typically this is difficult because tiny errors quickly accumulate into large uncertainties. \u201cExactly how far back we can get is very much an open question,\u201d says Lindegren. But the high precision of Gaia's measurements will certainly take the extrapolation much further than before. The measurements will also help to illuminate the many episodes of violence in the Milky Way's history. The Galaxy has grown by cannibalizing other, smaller galaxies; when they got too close, the Milky Way's gravity ripped them apart into long streams of stars that were then pulled towards the galactic centre at various angles. One such stream, torn from a dying object known as the Sagittarius dwarf galaxy billions of years ago, was found in 2002. \u201cThere are other streams out there that encode information about how the Galaxy has been developing,\u201d says Andrew Gould, an astronomer at the Ohio State University in Columbus. \u201cGaia will discover those streams\u201d \u2014 and use its measurements of stellar motions to reveal how the dismemberments unfolded. Knowing precise stellar movements should also help researchers to map out the distribution of invisible dark matter, which permeates the whole Galaxy. Dark matter emits no light, but it exerts a gravitational pull on stars, causing perturbations that should reveal themselves in Gaia's data. Those will allow astronomers to test how clumpy the dark matter is and whether it forms into disks, as theorists have proposed. Whatever Gaia finds, one thing seems certain: its star catalogue, due to be published in 2021, will remain unsurpassed for decades. ESA is considering a planet-hunting spacecraft similar to SIM for a future mission but has yet to choose a successor to carry on Gaia's astrometric work. \u201cWe have to start thinking about it now if we want to realize something in 15 years,\u201d says Lindegren. \u201cBut we don't really know what exactly is the best way to proceed.\u201d Boosting the precision significantly would be an enormous technological challenge. An easier path would be to fly another Gaia mission with the same specifications in 20 years, after the stars have moved noticeably, to better pin down their positions and velocities. Another proposed follow-on mission would examine parts of the Milky Way to which Gaia will be blind. Dust will obscure the Galaxy's bulge and some far-away parts of its disk from Gaia's visible-light gaze \u2014 but would pose no problem to an instrument looking for infrared radiation. Or perhaps Gaia itself will upend the whole discussion. As astrometry sharpens its focus, there is always the exciting possibility that something wholly unexpected could be found. \u201cScience often progresses by making detailed measurements,\u201d says Kulkarni. \u201cSometimes you see a deviation \u2014 something that turns out to be profound.\u201d \n                     Galaxy formation: The new Milky Way 2012-Oct-03 \n                   \n                     No star left behind 2008-May-21 \n                   \n                     Funding edict for mission has NASA over a barrel 2008-Jan-15 \n                   \n                     Space agency pulls the plug on astrometry mission 2002-Jan-17 \n                   \n                     Cannibalism feeds growing galaxies 2001-Jul-05 \n                   \n                     Stars run away for two reasons 2000-Nov-29 \n                   \n                     Gaia \n                   \n                     Hipparcos \n                   \n                     History of astrometry \n                   Reprints and Permissions"},
{"file_id": "502160a", "url": "https://www.nature.com/articles/502160a", "year": 2013, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "Researchers are tracking just how much impact ancient peoples had on the Amazon. Crystal McMichael first led a crew into the Amazon jungle in 2007, looking for signs of ancient human disturbances. Armed with machetes, they hacked their way through thick vegetation while fending off spiders, mosquitoes and bees. They were exploring around Ecuador's Lake Ayauchi, which McMichael knew held the earliest record of maize (corn) cultivation in the Amazon from around 6,000 years ago. But the jungle hid its secrets well. \u201cIf you looked at the forest you wouldn't realize there was any ancient disturbance,\u201d says McMichael, now a research scientist at the Florida Institute of Technology in Melbourne. \u201cYou have to dig.\u201d Scientists have struggled for decades to uncover humanity's historical footprint in the forest and determine what kind of an impact people had centuries to millennia ago. Their goal is to understand the evolution of the rainforest and just how much of the landscape we see today is 'natural' versus how much has been shaped by human hands.  Studies dating back to the 1950s suggested that small indigenous tribes merely scratched out a living in primitive villages before the arrival of Europeans. But more recently, researchers have proposed that the Amazon hosted complex societies that turned swathes of the forest into farms and orchards. Some estimates place the prehistoric population of the Amazon as high as 10 million \u2014 a huge number considering that the current population is around 30 million. The debate is heated. When McMichael and her colleagues reported 1  last year that indigenous occupations might have been rare in the most-remote parts of the jungle, their paper outraged archaeologists. The topic evokes strong emotions in part because it touches on the sensitive issue of indigenous land claims and goes to the heart of conservation philosophy. If prehistoric human populations were limited and today's Amazon is relatively pristine, then one might assume that this otherwise stable and natural ecosystem would be altered by any human disturbance \u2014 let alone the clearance of vast tracts of forest for agriculture (in Brazil alone, an area greater than Germany has been cleared over the past 25 years). By contrast, if the primeval Amazon was filled with people who managed the landscape, then the forest might be capable of absorbing further human impacts. Encouraging indigenous practices, even on a large scale, might allow people to live in balance with the rainforest. Nature reporter Jeff Tollefson asks if the Amazon rainforest is as untouched as we think. \u201cThe people who refuse to accept the human role are never going to understand how the environment that we appreciate today came to be,\u201d says anthropologist Clark Erickson of the University of Pennsylvania in Philadelphia, who believes that people were widespread throughout the Amazon. \u201cAnd if you don't understand that, you will never know how to manage it.\u201d \n               Hostile land \n             When researchers started studying the Amazon, they entered what had long been viewed as an impenetrable and hostile environment. Scientists such as the late Betty Meggers, an archaeologist at the Smithsonian Institution in Washington DC, argued in the 1950s that the region's lush flora grew atop a layer of poor soil that was unfit for cultivation and therefore large-scale civilization 2 . This theory fitted neatly within an old colonial paradigm that portrayed the Amazon as a largely empty jungle open to occupation and exploitation. Meggers documented pottery shards, burial sites and a network of defensive mounds on the island of Maraj\u00f3 at the mouth of the Amazon River. But, she argued, this community was short-lived owing to environmental limitations such as poor soil. This probably prevented large-scale development throughout the basin, she wrote: \u201cIts levelling effect appears to be inescapable.\u201d This view came under attack in the 1980s, first by Anna Roosevelt, an archaeologist at the University of Illinois at Chicago. On the island of Maraj\u00f3, her work revealed a culture that had endured for nearly 1,000 years until about 400  AD  \u2014 more than long enough to put theories about environmental limitations in doubt 3 . By the time Roosevelt published a detailed book 4  about her work on Maraj\u00f3 in 1991, the tide of opinion had begun to turn. While Roosevelt was studying Maraj\u00f3, William Bal\u00e9e, an anthropologist at Tulane University in New Orleans, Louisiana, was spending time with the Ka'apor tribe in the southeastern Amazon. In 1993, he documented the group's knowledge and use of a forest region that contained an unusually high concentration of useful species 5 . Tribal members referred to neighbouring patches of forest, by contrast, as wild and undisturbed. For Bal\u00e9e, this was a sign that many parts of the landscape were at one point cultivated as complex orchards. \u201cPart of this was a backlash against the idea that natives tiptoed through the forest and left no footprints,\u201d says Erickson. Since then, researchers working across the eastern and central Amazon have found deposits of ' terra preta ' (literally 'black earth' in Portuguese), which are fertile soils that are thought to have been created through cycles of fire and cultivation. Further earthworks, including mysterious systems of ditches and mounds, were uncovered throughout the 1990s in the western Amazon. By the mid-2000s, researchers had come to believe that prehistoric people once occupied large areas; built networks of roads, canals and bridges; cultivated crops such as maize and manioc (cassava); and maintained plantations of useful trees such as bananas and palms 6 . \u201cThese societies were fully on par with small to medium-sized populations anywhere on the globe in the 1400s,\u201d says anthropologist Michael Heckenberger at the University of Florida, Gainesville. \u201cThey weren't backward in any way.\u201d After such a radical shift in thinking, it was perhaps inevitable that the scientific pendulum would swing back the other way. In June 2012, a team of researchers led by McMichael and Mark Bush at the Florida Institute of Technology published a paper 1  arguing that human civilization was sparse across the wetter forests of western and central Amazonia. The team had collected 247 soil cores from dozens of sites and found charcoal in many locations \u2014 a sign of human fires. But none of the sites held human artefacts or  terra preta  (see 'Signs of life'). The team documented maize cultivation in only one instance, and just a couple of other sites revealed signs of grasses that suggested repeated clearing of the land. Bush concluded that others had been too quick to extrapolate evidence of dense populations in the eastern Amazon across the entire basin. \u201cWe don't believe in a virgin system, but we don't believe in a completely exploited system either,\u201d he says. \u201cWe cannot assume that Amazonian forests were resilient in the face of heavy pre-Columbian disturbance.\u201d McMichael says that she gleaned something similar from Lake Ayauchi. She found significant amounts of charcoal and tiny fossilized structures from maize crops around the lake, but evidence of occupation fell away just a kilometre from the waters 7 . \u201cPeople were there, but their impact was very localized,\u201d she says. \n               Fiery debate \n             The paper by McMichael  et al . was attacked by both sides. Meggers criticized them for accepting existing evidence of large-scale civilization in eastern and central Amazonia. But the strongest criticism came from a dozen scientists who commented on the  Science  paper online. They argued that the paper underestimated signs of civilization by relying too heavily on soil data, which cannot reveal signs of manioc cultivation or other agroforest management. \u201cSoils tell you one part of the story, but they don't tell you everything,\u201d says Susanna Hecht, a historical ecologist at the University of California, Los Angeles. She notes that the Inca had established a military outpost in the Putumayo region of western Amazonia by the end of the fifteenth century, just before the arrival of the Europeans, presumably to protect against, or trade with, populations in that region. \u201cWhy would you do that if you felt that there were just a bunch of naked people running around?\u201d In November 2012, a team led by Charles Clement, a horticulturist at the National Institute of Amazonian Research in Manaus, Brazil, countered with a paper 8  arguing that human influence on the forests was pervasive. His study showed that trees that are considered useful for humans \u2014 including palms and Brazil nut trees \u2014 are more common near rivers, where populations would have been highest. Given that many of these species have different ecological preferences and would not necessarily grow together, the team argued that people must have had a hand in creating these bounteous patches. But their results also came under fire. Critics questioned both the strength of the statistics and the fact that the team did not attempt to establish a baseline for what might be expected in the absence of humans. For researchers on either side of the debate, part of the problem is the sheer scale of the Amazon basin and the difficulty in reaching remote study sites. To get a clearer image, researchers are focusing on the view from space. After her time in the field with Bush, McMichael joined a team of remote-sensing scientists at the University of New Hampshire in Durham in an attempt to identify  terra preta  through satellite imagery. Their unpublished analysis suggests that it is possible to spot  terra preta  from space by seeking fertile regions where leaves absorb high amounts of nutrients and water. McMichael has also developed a model that uses ecological factors such as distance from a river, elevation and forest type to predict where  terra preta  might be expected, which has helped field scientists to narrow their search. When tested against 2,900 sites where  terra preta  has been confirmed as either present or absent, the model predicts 89% of them correctly. It suggests that about 3% or 4% of the basin might yield  terra preta , compared with earlier, broader estimates of 0.1\u201310%. McMichael is now building a similar model to predict the extent of major earthworks such as those found on Maraj\u00f3 and in western Amazonia. \n               Past and future \n             It might take a much broader view \u2014 in time as well as space \u2014 to get the full picture of human impact on the Amazon. People were living there long before pre-Columbian times, and could have been shaping and determining the composition of the forest for millennia through hunting and foraging. The arrival of humans in the Amazon region coincided with \u2014 and perhaps contributed to \u2014 the widespread extinctions of megafauna, such as giant sloths and a cousin of the modern elephant, towards the end of the last glacial period some 12,000 years ago. A recent study 9  suggests that these extinctions decreased the availability of the important nutrient phosphorous in the forest, and that might explain why the forest is still limited in phosphorous today. An analysis of pollen and charcoal records led by Francis Mayle at the University of Edinburgh, UK, suggests that dense rainforest might have given way to savannah and more open vegetation in many areas around 4,000\u20138,000 years ago thanks to a warmer climate and the use of fire by humans 10 . Mayle wrote that humans might have been \u201cthe key agents of disturbance\u201d in the early forest. Hecht's own work has focused on the more recent past: the 1880s to the 1920s, an era in which massive industrialization and globalization drove a boom in rubber extraction in the Amazon. Hecht compiled data on rubber sales and production and then teamed up with Sassan Saatchi, a remote-sensing expert at NASA's Jet Propulsion Laboratory in Pasadena, California, to map out the impact of rubber production across the basin. Their as-yet-unpublished analysis suggests that a million rubber trees were felled annually for 30 years, with each towering giant bringing down perhaps another ten trees as it fell. Saatchi suspects that the forest is still recovering from this sudden pulse of destruction, and he is trying to see whether signs of regrowth can be detected in the satellite data. If many areas of the Amazon have been regrowing since the early 1900s, that could have temporarily boosted how much carbon dioxide the region is absorbing from the atmosphere, says Saatchi. If true, as that disturbance fades, the forest's role as a carbon sink could shrink. Determining the full extent of human influence on the forest will take time. But Heckenberger says that what we know already offers some lessons for today. The evidence of ancient  terra preta  and cultivated 'orchards' in the forest demonstrates that indigenous peoples knew how to manage life within the Amazon long before the days of chainsaws and artificial fertilizers. Future efforts to develop the kind of agroforest systems that are being uncovered from the past, says Heckenberger, would help modern communities to both preserve the Amazon and carve out a living. \u201cThese are solutions that are quite intelligent and palatable,\u201d says Heckenberger \u2014 for all of the Amazon's population. \u201cI think they can learn a lot from themselves.\u201d \n                     Forest ecology: Splinters of the Amazon 2013-Apr-17 \n                   \n                     Severe drought has lasting effects on Amazon 2012-Dec-24 \n                   \n                     Amazon's extinction debt still to be paid 2012-Jul-12 \n                   \n                     Nature special: The changing Amazon \n                   Reprints and Permissions"},
{"file_id": "502156a", "url": "https://www.nature.com/articles/502156a", "year": 2013, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "Some brain researchers are increasingly using mice to study visual processing, but others fear the move is short-sighted. When Cris Niell said that he wanted to study how mice see, it did not go over well with more-senior neuroscientists. Mice are nocturnal and navigate largely using their noses and whiskers, so many researchers believed that the nursery rhyme \u2014  Three Blind Mice  \u2014 was true enough to make many vision experiments pointless. The obvious alternative model was monkeys, which have large, forward-looking eyes and keen vision. What's more, scientists could rely on decades of established techniques using primates, and it is relatively straightforward to apply the results to the human visual system. \u201cPeople were saying, 'studying vision in mice, that's crazy,'\u201d Niell recalls. But he was convinced that the rodents offered unique opportunities. Since the 1960s, researchers have used cats and monkeys to uncover important clues about how the brain turns information from the eyes into images recognized by the mind. But to investigate that process at the cellular level, researchers must be able to manipulate and monitor neurons precisely \u2014 difficult in cats and monkeys, much easier in mice. If mice and primates turned out to process visual stimuli similarly, Niell thought, that discovery could unleash a torrent of data about how information is extracted from stimuli \u2014 and even, more generally, about how the brain works. He found a rare supporter in Michael Stryker at the University of California, San Francisco, who had already seen his share of crazy experiments in mouse vision. Stryker offered Niell a postdoctoral position in his lab, and the pair began setting up experiments in 2005.  Nearly a decade later, the two researchers are in better company. At last year's annual meeting of the Society for Neuroscience, Niell attended packed sessions on mouse vision. In March 2012, the Allen Institute for Brain Science in Seattle, Washington, announced a ten-year plan to spend more than US$100 million to map mouse visual areas. And in June this year, the curriculum of a two-week course on vision at Cold Spring Harbor Laboratory in New York featured mice front and centre. More than three-quarters of the 22 students investigating how the visual system works were using mice, says course co-director Andrew Huberman, a neuroscientist at the University of California, San Diego, who has worked on animals from cuttlefish to macaques. In 2001, he says, there may have been a student or two using mice to study how the visual system develops, but no one was studying function. \u201cIt's an explosion.\u201d The surge of interest stems largely from advances that give researchers the ability to monitor and control specific mouse neurons using light. Logistical and ethical considerations are also a big draw. Studies with mice are much cheaper, faster and less likely to raise moral concerns than work with monkeys. But whether they will reveal useful information about human vision is very much an open question, says Huberman. \u201cThe mouse visual cortex is like the smartphone of neuroscience,\u201d he says. \u201cEveryone feels the need to get one to play with, but it still remains to be seen if it's merely a convenience, a colossal distraction or the greatest thing since the discovery of electricity.\u201d \n               Drawn to rodents \n             Niell wanted to revisit some of the most well-known and seminal experiments in vision science. In the 1950s and 1960s, David Hubel (who died late last month) and Torsten Wiesel pushed electrodes into the backs of cat and monkey brains, and patched the signals to a speaker to track the activity of neurons. The researchers listened in as the animals viewed tilted lines and moving dots; the crackles they heard revealed that organized regions of neurons respond to motion and edges 1 . The results, which later earned Hubel and Wiesel a Nobel prize, became a canonical example of 'cortical computation', in which interconnected neurons transmit and transform information. It turns out that neurons in the visual cortex process input from the eyes extremely selectively: some respond only to vertical lines, others to horizontal ones, still others to stripes that tilt 40\u00b0 to the left, to dots creeping up 30\u00b0 to the right, and so on. Niell, who now runs a lab at the University of Oregon in Eugene, knew that it would not be easy to discover whether these findings would hold true for mice. The electrodes available at the time often damaged neurons in fragile mouse brains, disrupting activity rather than monitoring it. But after revising their procedures and redesigning their equipment, Niell and Stryker worked out a way to record from individual mouse brain cells using silicon microprobes. Stryker, who had trained with Hubel and Wiesel, recalls seeing the first graph that plotted how a neuron responded to a mouse viewing a series of tilting lines. The graph showed sharp, narrow peaks of neural activity at specific orientations 2 . If the lines were tilted just 20\u00b0 from the preferred angle, the cell fell silent. \u201cI just couldn't believe how pretty it was,\u201d says Stryker. \u201cIt was like the figure in a book.\u201d The experiments showed that neurons in the mouse visual cortex are about as selective as those in the cat or monkey brain. Niell and Stryker considered that to be strong evidence that the mouse could be used as a model for visual processing in higher animals. When word got out, the team soon had visitors. Among the earliest were Hillel Adesnik and Bassam Atallah, two neuroscientists then working in the lab of Massimo Scanziani at the University of California, San Diego. Adesnik and Atallah had been studying dissected slices of mouse brain to catalogue how subtypes of neurons were connected. They could test how cells in the freshly dissected tissue responded when stimulated with electricity, but it was hard to know what kind of processing, if any, happens in a brain slice. They really wanted to probe how brain circuits responded to real physiological stimuli \u2014 the type delivered by the eyes, ears, nose or skin of a living mouse. When they heard of the results from San Francisco, they hopped on their motorcycles and rode 800 kilometres to learn Niell and Stryker's technique. Since that visit, Adesnik, Atallah and others have done experiments that show how neurons interact in intact circuits in the mouse visual cortex. Such work is beginning to reveal how subtypes of neurons cooperate to extract information about the world; it also hints at which stimuli mice notice or ignore. \u201cI consider the mouse visual cortex as the first pillar in a bridge that will link cellular and systems neuroscience,\u201d says Scanziani. It is too early to know how far that bridge will extend, says Edward Callaway, a neurobiologist at the Salk Institute for Biological Sciences in La Jolla, California. \u201cSo far, we haven't learned anything fundamentally new from the mouse visual system. And that's not surprising, since we've studied monkeys for the past 40 years.\u201d But the early data are fuelling mouse researchers' hopes that this simple, easily manipulated model could shed light on more-complex brains. For instance, in a sign that mice have sophisticated processing, researchers showed that, like primates, mice have visual areas that receive input beyond the primary visual cortex 3 . Around the same time that Niell and Stryker were learning that mice can see stripes, Matteo Carandini at University College London switched to mice after years of researching vision in monkeys and cats. He wanted to study neural circuitry in the context of behaviour, but for that he had to record from inside individual neurons. That is hard to do in monkeys \u2014 and although it can be done in cats, they baulk at the behavioural tasks required. So Carandini began working out training programmes to let him explore what patterns mice perceive as they move around, and how they act on their perceptions. He and his team developed one task in which mice press a button when they see stripes. The team also monitored mouse visual processing as the animals ran on a treadmill or explored a virtual environment. Carandini now wants to manipulate particular neurons during such experiments to see how the mouse's behaviour changes. He wants to learn how parts of the brain cooperate: he thinks that work on the mouse's small, flat brain could shed light on processes in higher animals, such as perception and decision-making, and how these are affected by distraction (such as noise) or motivation (such as thirst). \u201cThe frontier at this moment is understanding how different parts of the cortex work together,\u201d he says. \u201cThis is a problem that any animal with a cortex has.\u201d \n               A blurry picture \n             Carandini and others who work on mouse vision all acknowledge that there are limits to this line of research. No one denies that mice see poorly; Niell estimates that they have the equivalent of 20/2,000 human vision (which would qualify them as legally blind). The general rule of thumb is that mouse eyesight is about as good as what humans see in their far-off peripheral vision. So there will be tasks that rodents will not be able to perform \u2014 particularly those related to aspects of facial recognition and visual attention. \u201cTo really get to behaviour in a more meaningful way, we'd probably have to use primates,\u201d says Callaway, who is working with mice as well as improving genetic tools for studying monkeys. Still, enthusiasts say, the similarities between mice and humans outweigh the differences. The mouse visual cortex contains the same neural subtypes as the human visual cortex, in about the same proportions, and the subtypes seem to hook up to each other using the same rules. Evolutionarily, mice are more closely related to humans than are cats. And the fact that a mouse has fewer brain regions than either a primate or a cat, and can distinguish a smaller set of possible images, makes it more experimentally tractable, says Carandini. \u201cThe basic rules of computation, I believe, are more general and canonical. Our chances of discovering them are much better in a mouse.\u201d Not all neuroscientists embrace the mouse as a model. \u201cI don't think there's good evidence that mice use the visual cortex the way primates do. Or that the mouse visual cortex is organized the way that primates' are, or that mice are a good model for vision,\u201d says Tony Movshon, a neuroscientist at New York University. The most obvious difference is size: the entire mouse brain is one-fiftieth as big as the part of the macaque brain devoted to vision alone. A macaque's primary visual cortex has more than 1,000 times as many neurons as a mouse's, and a much greater fraction of the macaque brain is devoted to vision (see 'Eye to eye'). Primates have plenty of neurons in dozens of visual areas with targeted purposes, such as recognizing faces and tracking motion. By contrast, the visual regions identified in mouse brains are \u201ctiny little patches of cortex\u201d, says Movshon; they extend for micrometres and millimetres, rather than centimetres. The areas are simply too small for the extensive, regional communication observed in primate visual areas, he says. \u201cThey can't work the same way.\u201d \n               Busy brains \n             Perhaps the biggest problem is that the mouse visual cortex performs many functions besides vision, so the systems that support visual processing could be fundamentally different from those in the primate brain. It is like studying heart function in some alien organ that not only pumps blood but also takes on the respective gas-exchange and electrolyte-balancing functions of the lungs and kidneys. For all these reasons, instead of switching his vision studies from monkeys to the more tractable mice, Movshon is putting his faith in nascent efforts to take the tools that work so well in mice and adapt them to manipulate subtypes of neurons in monkey brains. \u201cWhat people are doing now is to pretend that the mouse is a tiny monkey with a pointy noise and whiskers and to hope for the best,\u201d he says. Paul Martin, a vision scientist at the University of Sydney in Australia, agrees that scientists could encounter severe problems when they try to relate mouse data to the human experience. \u201cA shopping trolley and a Formula 1 motor car both have wheels and obey Newton's laws of motion. But the motor \u2014 what makes them move \u2014 is quite different,\u201d he says. Nicholas Priebe at the University of Texas at Austin is advocating more comparative studies to tease out how processing differs between mouse brains and those of other species \u2014 and how it is similar. This year, he reported striking differences in how brain regions in cats and mice contribute to selectivity 4 . Discrepancies between mice and primates do not mean that mouse brains have nothing to reveal about human brains, he says, but scientists need to proceed with as much caution as enthusiasm. \u201cIf you try to apply everything you learn from the mouse to our brain, then I think there's a serious problem,\u201d says Priebe. For most, the debate is not about whether to study visual processing in the mouse cortex, but about what questions will also apply to higher animals. Many researchers are hopeful that mouse experiments will generate hypotheses for primate research, and that comparable experiments could one day go back and forth between animal systems. But after decades of focusing on primates, researchers have some catching up to do in rodents, says Callaway. \u201cWe can't begin to do those things in a mouse until we know more about what they use the visual system to do.\u201d That is part of the aim of the initiative by the Allen Institute for Brain Science to map the mouse visual cortex and visual processing areas. The goal, says Clay Reid, who co-leads the project, is to start at the bottom, building up to big questions about how the brain works. Reid and his team plan to catalogue the cell types and connections in the mouse's visual area, and to monitor what happens as the animals look at, and respond to, a stimulus. Then the team will see how responses change when particular neurons are suppressed or activated. Such experiments are about more than just vision. \u201cWe are doing it to try to understand principles of cortical computation and the relationship between cortical activity and behaviour,\u201d says Reid. Armed with some clues to those processes, scientists will be able to test these hypotheses in other animals. Eight years after his initial experiments, Niell is glad to see more researchers embracing what the mouse has to offer. Of course, such a simple system cannot answer every question we have about the human brain, but researchers should learn what they can, says Niell. \u201cYou can make so much headway with a mouse that it's silly not to.\u201d \n                     Vision: Looking to develop sight 2012-Jul-25 \n                   \n                     Neuroscience: Observatories of the mind 2012-Mar-21 \n                   \n                     Neuroscience: The rat pack 2010-May-19 \n                   \n                     Neuroscience: Illuminating the brain 2010-May-05 \n                   \n                     Blind man walking 2008-Dec-22 \n                   \n                     Cris Niell \n                   Reprints and Permissions"},
{"file_id": "502291a", "url": "https://www.nature.com/articles/502291a", "year": 2013, "authors": [{"name": "Eugenie Samuel Reich"}], "parsed_as_year": "2006_or_before", "body": "Publishing in the most prestigious journals can open doors, but their cachet is under attack. Jeffrey Rimer has noticed a change in the way other scientists treat him since his paper on kidney-stone growth inhibitors appeared on the cover of  Science  three years ago. When his colleagues introduce him, they often mention his publications or the publicity he has garnered, which he interprets as a nod to his  Science  paper 1 . \u201cFrom the reaction of colleagues, it's almost like you've joined a club,\u201d says Rimer, a chemical engineer and assistant professor at the University of Houston in Texas. \u201cFair or unfair, it's like you've proved you can do good science.\u201d Researchers often say that publishing in prestigious journals can make a career. And for decades, the most sought after of the bunch have been  Nature  and  Science  \u2014 broadly read journals that reject more than 90% of the manuscripts they receive. A paper in one of these journals, it is said, can bring job opportunities, invitations to speak, grants, promotions and even cash bonuses and prizes. Rimer believes that his  Science  paper contributed to his winning a grant from the Welch Foundation, a chemical-research funding organization based in Houston, in 2012, and he expects that it may help when he seeks tenure at his university. His impressions echo what many other scientists say \u2014 often with gritted teeth \u2014 about premier journals. But the publishing world is rapidly changing, and the leading titles are facing increasing competition. The push for open-access publishing has gathered steady steam; more than 5,000 open-access journals have been launched since Rimer's paper was published in October 2010. These journals, along with the more established open-access publications, are attracting a growing share of submissions, threatening the hold of the leading journals. Beyond that trend, some advocates for the open-access movement have specifically attacked  Science  and  Nature , which they label as 'glamour journals'. They say that the journals' prestige is part of a business model in which hot findings are flaunted as a way to justify their subscription rates. And many senior scientists worry that too much attention is paid to where people publish rather than to what they have done \u2014 that  Science ,  Nature  and similar publications hold too much sway over the careers of working scientists. \u201cIt's like a kind of addiction,\u201d says Stephen Curry, a structural biologist at Imperial College London who has been vocal about the issue on his blog, Reciprocal Space. To get a sense of whether the changes in the publishing landscape have altered the allure and impact of top-tier journals,  Nature  interviewed Rimer and several other early-career researchers who published for the first time in  Nature ,  Science  or other journals in October 2010 (see 'Views from the lab bench'). Geoff Marsh investigates the effects of publishing in a top-tier journal. Several of those researchers say that three years on, they feel that getting a paper in a premier journal helped their careers in concrete ways. Although they cannot know how their careers would have unfolded without these high-profile publications, what they believe is still telling. It is why some of them are reluctant to join established scientists who say that they will not submit to  Nature  and  Science  as a matter of principle, a step many younger researchers are unwilling to take. Yet critics are working hard to change how researchers \u2014 and those who assess their work \u2014 judge the value of different publications. Sandra Schmid, head of the cell biology department at the University of Texas Southwestern Medical School in Dallas, is one of many academics advocating ways to identify promising candidates other than simply looking for leading journals on their CVs. \u201cThe drive to publish in these journals does more harm than good,\u201d she says. \n               Publication publicity \n             Ping Chi, a medical oncologist who landed a paper 2  in  Nature  three years ago, says that she got an important boost towards launching a clinical trial of new cancer drugs, which is now starting up. Her paper investigated how two proteins stabilize the survival of gastrointestinal tumours. Had it been published in a lesser-known journal, she says, she might still have been hired by Memorial Sloan-Kettering Cancer Center in New York, but she probably would not have received such a generous start-up package and would have spent some of the past two years raising funds. Instead, she put her energy into persuading her collaborators and pharmaceutical companies to support the clinical trial of a therapy that inhibits the proteins. The  Nature  paper, Chi says, helped to establish her work as a significant advance, especially because it received media attention (thanks in part to a press release issued by  Nature 's press office). In some developing countries, publishing in top-tier journals has extra appeal; researchers in China and India sometimes receive bonuses or salary increases when they get papers into  Science  or  Nature . Yingjie Peng, a Chinese-born astrophysicist and postdoctoral researcher at the Cavendish Laboratory of the University of Cambridge, UK, says that if he were to seek a faculty position in China, it would be invaluable to have a  Nature  or  Science  paper. \u201cGovernment officials may not understand the work \u2014 the easy thing to do is compare journals,\u201d he says. Peng argues that publishing in elite journals is less important in the United States and the United Kingdom. Most astronomers see papers as soon as they are posted to the  arXiv.org  preprint server. And where a paper is published is not as important as who did the work and how technically adept it is. Peng is doing well in terms of recognition; his paper 3  on galaxy evolution, published in  The Astrophysical Journal  three years ago, has already received a substantial count of more than 150 citations. The Astrophysical Journal  allows longer papers than  Science  and  Nature  typically would, which gave Peng a chance to fully explain his method for extracting laws of galaxy evolution from data rather than deriving them entirely theoretically. He credits the paper with helping him to get his position at the Cavendish. Anke Bill, a cell biologist at the Novartis Institutes for Biomedical Research in Cambridge, Massachusetts, had a similar experience with her 2010 paper 4  in  Cell , a specialized journal that is highly prestigious in the biological sciences. Her paper focused on cytohesins, proteins thought to be involved in human lung cancer. Bill says that she and her adviser had initially aimed for the wider exposure that would come from publishing in  Nature . But they say that they received a tough set of reviews that required more experiments. When Bill resubmitted the paper with the extra data,  Nature 's editors decided that the paper was too long and technical, she says, but  Cell  accepted the paper in its expanded form. Bill says that beyond the world of biomedical science, a  Nature  or  Science  paper would have boosted her reputation more. But within her field, she says, the  Cell  paper had a big impact. It may have helped her to land her current position, especially because the laboratory at the German university where she did her PhD was not well known outside that country. The  Cell  paper showed that she could develop and test a promising novel hypothesis. \u201cI got positive feedback everywhere I applied,\u201d she says. Other researchers point to the advantages of less selective journals, such as  PLoS ONE , which publishes a high volume of papers online. Nicholas Longrich, was a postdoc in palaeontology at Yale University in New Haven, Connecticut, when he published his 2010 paper 5  in  PLoS ONE  showing evidence of cannibalism in  Tyrannosaurus rex . \u201cThe fact that you probably won't get it rejected and have to submit elsewhere means you can get your work out quickly,\u201d he says. Longrich also liked that  PLoS ONE  is open access, which made it easier for his  T. rex  work to be read by others. Still, he says that he did not land his current job as a lecturer at the University of Bath, UK, until he published three more papers, in subscription-based journals ( Nature ,  The Proceedings of the National Academy of Sciences  and  Current Biology ). \u201cDid  Nature  help my career more than  PLoS ? I can't prove it, but I think so,\u201d he says. \n               Measuring impact \n             Critics of the status quo object to evaluating research on the basis of where it is published. The shorthand way to do this is by the journal impact factor \u2014 an index kept by Thomson Reuters, an information-services company based in New York. A journal's 2013 impact factor, for example, would be computed by summing the number of citations garnered this year to papers published in that journal in 2011\u201312 and then dividing that sum by the number of papers the journal published during that span. Curry, who received hundreds of comments on his blog when he criticized impact factors in 2012, says that  Nature  and  Science  may command high reputations in part because they have high impact factors (38.6 and 31, respectively, in 2012), but those figures are averages that are pulled upwards by a few very frequently cited papers. It is not rational, he says, for papers that are not cited as often to get a boost just because they come out in the same journal. Some experts are taking active steps to challenge the sway of the leading journals. In December 2012, hundreds of scientific leaders, funding bodies, journals (including  Science , but not  Nature ) and other organizations gathered in San Francisco, California, to sign the Declaration on Research Assessment (DORA), which criticizes reliance on the impact factor and commits signatories to evaluate research on the basis of its scientific merit. Schmid, the Texas cell-biology chair, signed DORA and published a commentary in  Science Careers 6  saying that her department will no longer filter applicants for faculty jobs on the basis of their publications. Her department fills one or two faculty positions a year and receives as many as 300 applications for each one. In the past, the department weeded out candidates who had not published in top-tier journals, but Schmid dislikes that approach. \u201cHow many brilliant scientists are just outside the spotlight?\u201d she says. She is now filtering candidates on the basis of a covering letter describing their past work and how they envision their future. \n               Rankings race \n             It is hard to assess how widespread such changes are, because research evaluations and hiring processes are often confidential. But Henk Moed, a bibliometrician and scientific adviser at Elsevier, a science publisher in Amsterdam, suspects that the journal impact factor still looms large in many hiring decisions. Evaluators may decide privately to average the impact factors of the journals listed on a CV as a way to rank candidates. He notes that some institutional rankings, such as the Academic Ranking of World Universities, compiled by Shanghai Jiao Tong University, give explicit weight to the number of  Nature  and  Science  papers an institution has produced \u2014 making it likely that some universities would then begin to rank prospective faculty by the same measure. \u201cThere is more and more evaluation, and a need for researchers to prove their quality,\u201d Moed says. \u201cJournal reputations play a role, and that role has increased.\u201d Others echo Moed's sense that  Nature  and  Science  papers are often relied upon implicitly. Amy Ruschak, a biochemist and assistant professor at Case Western Reserve University in Cleveland, Ohio, says that her 2010  Nature  paper 7  on a cellular apparatus that destroys toxic proteins was a highlight of her application for faculty positions and undoubtedly contributed to her success. \u201cIt's central, but no one will specifically say that,\u201d she says. Moed notes that bibliometricians are trying to improve measures of journal quality while also educating researchers about the value and limitations of such metrics. And Stefano Bertuzzi, executive director of the American Society for Cell Biology in Bethesda, Maryland, which spearheaded DORA, says that although the current scientific culture unduly rewards  Nature  and  Science  publications, he thinks that the rapid growth of open access to articles online will change that. \u201cOpen-access articles get read a lot, so they should gain visibility,\u201d he says. Visibility is what motivated Olga Momcilovic, a cell biology postdoc at the Buck Institute for Research on Aging in Novato, California, to send her paper 8  on DNA damage in stem cells to  PLoS ONE  in 2010. \u201cSocial media and Google searches list papers by relevance, not by impact factor,\u201d she says. There are some signs that the leading journals are not keeping pace with the overall growth in publishing. According to information made available by  Nature  and  Science , submissions to both journals have climbed over the past ten years, reaching more than 10,000 per year for  Nature  and more than 12,000 for  Science . However, the number of articles published worldwide in all journals has been rising much more rapidly, suggesting that many researchers are looking to publish elsewhere (see 'Growing competition'). A similar story emerges from data on the most highly cited papers. In 2012, Vincent Larivi\u00e8re, an information scientist at the University of Montreal in Canada, studied the clout of  Nature, Science  and other top journals by examining citation statistics 9 . He found that although these journals are publishing a growing number of highly cited papers each year, they are not keeping up with the industry as a whole; overall, their proportion of the total number of highly cited papers is declining. Nature  and  Science  have press offices that are more active than those of many other journals, however \u2014 making it more likely that papers published there will receive notice. And because electronic publishing has led to a flood of online information, journals that can claim to be highly selective fill a niche by elevating papers worthy of reading, says Larivi\u00e8re. Annele Virtanen, an aerosol chemist who is now an assistant professor at the University of Eastern Finland in Kuopio, agrees. She was a postdoc in 2010, when she published a  Nature  paper 10  showing that organic aerosol particles that most researchers had assumed were liquid were probably solid. The publication opened all kinds of doors for Virtanen. The journal's visibility meant that climate modellers and atmospheric chemists outside her original research field saw her paper, and many wrote to her, helping to drive her current research in a more generally relevant direction. She now has more results and is thinking of submitting to  Nature  again \u2014 or to  Science . Shooting for these publications, she believes, means reaching to do excellent research that will stand out. \u201cIt improves the level of science,\u201d she says. \u201cI can't see so many bad sides.\u201d \n                     UK open-access route too costly, report says 2013-Sep-10 \n                   \n                     The paper trail 2013-Jun-04 \n                   \n                     Investigating journals: The dark side of publishing 2013-Mar-27 \n                   \n                     Open access: The true cost of science publishing 2013-Mar-27 \n                   \n                     \n                         Nature  \n                       \n                   \n                     \n                         Science  \n                       \n                   \n                     Directory of Open Access Journals \n                   \n                     Declaration On Research Assessment (DORA) \n                   \n                     Academic Ranking of World Universities \n                   Reprints and Permissions"},
{"file_id": "500268a", "url": "https://www.nature.com/articles/500268a", "year": 2013, "authors": [{"name": "Moheb Costandi"}], "parsed_as_year": "2006_or_before", "body": "Elizabeth Loftus has spent decades exposing flaws in eyewitness testimony. Her ideas are gaining fresh traction in the US legal system. In the early hours of 9 September, 1984, a stranger entered Mrs M's California home through an open living-room window. Finding Mrs M asleep, he tried to rape her, but fled when other people in the house awoke. Mrs M described her assailant to the police: he was black, weighing about 170 pounds and 5'7\u201d to 5'9\u201d tall, with small braids and a blue baseball cap. Officers cruising her neighbourhood spotted someone roughly matching that description standing beside his car a block away from the house. The man, Joseph Pacely, said that his car had broken down and he was looking for someone to jump-start it. But Mrs M identified him as her attacker and he was charged. At Pacely's trial a few months later, memory researcher Elizabeth Loftus testified on his behalf. She told the jury how memory is fallible; how stress and fear may have impaired Mrs M's ability to identify her assailant, and how people can find it difficult to identify someone of a race other than their own. Noah Baker talks to a lawyer, a psychologist and Elizabeth Loftus about false-memory research. Pacely was acquitted. \u201cIt's cases like this that mean the most to me,\u201d says Loftus, \u201cthe ones in which I play a role in bringing justice to an innocent person.\u201d In a career spanning four decades, Loftus, a psychologist at the University of California, Irvine, has done more than any other researcher to document the unreliability of memory in experimental settings. And she has used what she has learned to testify as an expert witness in hundreds of criminal cases \u2014 Pacely's was her 101st \u2014 informing juries that memories are pliable and that eyewitness accounts are far from perfect recordings of actual events. Her work has earned her plaudits from her peers, but it has also made her enemies. Critics charge that in her zeal to challenge the veracity of memory, Loftus has harmed victims and aided murderers and rapists. She has been sued and assaulted, and has even received death threats. \u201cI went to a shooting range to learn how to shoot,\u201d she says, noting that she keeps a few used targets in her office as a point of pride. Now, the 68-year-old scientist's research is starting to bring about lasting changes in the legal system. In July last year, the New Jersey Supreme Court issued a ruling \u2014 based largely on her findings \u2014 that jurors should be alerted to the imperfect nature of memory and the fallibility of eyewitness testimony as standard procedure. Loftus is working with judges in other states to make such changes more widespread. \u201cWhat's going on now in America really is something of a revolution,\u201d says Martin Conway, a cognitive psychologist at City University London. Loftus' work, he says, has been \u201cprofoundly important\u201d in shaping these changes. \n               Malleable memories \n             Loftus says that her start in psychology was rudderless. As a graduate student in mathematical psychology at Stanford University in California, \u201cI wasn't really captivated\u201d, she says. \u201cI'd sit in the back of the seminars, kind of bored, writing letters to my Uncle Joe or hemming skirts, or whatever.\u201d Eventually a social-psychology class piqued her interest and she began to study how word meanings are stored in the brain, and how people recall them. Still, something was missing. \u201cOne day I was having lunch with a cousin of mine,\u201d she says, \u201cand I told her about our great discovery that people are faster at naming 'a bird that's yellow' than 'a yellow bird'.\u201d Her cousin \u2014 unimpressed \u2014 joked about taxpayers' money being wasted. \u201cThat's when I decided I wanted to work on something that had more practical applications.\u201d Loftus was casting about for a meaningful way to study memory and get funding when a former Stanford engineer working for the US Department of Transportation said that his employer would probably pay for research into car accidents. Following that lead, Loftus won funding in 1974 for a proposal to study witness accounts of accidents, and she soon published the first of several influential studies revealing the limitations of eyewitness testimony 1 . She showed people film clips of car accidents and asked them to estimate the speed of the cars. The wording of the questions, she found, had a profound effect on the estimates. People who were asked, \u201cHow fast were the cars going when they smashed into each other?\u201d gave higher estimates on average than those with whom the verb 'hit' was used. And those who were told that the cars had 'contacted' each other gave the lowest estimates. Those asked about cars smashing into one another were more than twice as likely as others to report seeing broken glass when asked about the accident a week later, even though there was none in the video. \u201cI realized that these questions were conveying information,\u201d says Loftus. \u201cI began to think of it as a process of memory contamination, and we eventually called it the misinformation effect.\u201d She went on to publish several other studies 2 , 3 , 4  showing how memories can be contorted, and that the ability of eyewitnesses to identify suspects from photographs can be unreliable. Any description they might hear has the potential to influence who or what they think they saw. Loftus was eager to translate these findings to the real world, and began consulting on legal cases to get \u201cclose up and personal\u201d with witnesses. Her first case \u2014 of a woman accused of killing her abusive boyfriend \u2014 hinged on whether the woman had acted in self-defence or had committed premeditated murder. Eyewitnesses could not agree on how much time had elapsed between when the defendant had picked up the gun and when she had fired it: some said it was seconds, others said minutes. Loftus cast doubt on the memory of the witnesses, and the woman was acquitted. Loftus described the case, together with her research, in a 1974 article for  Psychology Today  magazine 5 . \u201cOnce that article came out, I started getting calls from all over the place,\u201d she says. \u201cFrom lawyers wanting me to work on their cases, and legal professionals wanting me to lecture at their meetings.\u201d Some of her trials have been high-profile \u2014 including that of the serial killers known as the Hillside Stranglers and the 1992 trial of the police officers indicted for beating construction worker Rodney King. She even consulted on a case involving a young law student named Ted Bundy, who was accused of kidnapping a woman in 1974. Bundy was convicted, only to escape. Recaptured in 1978, he eventually admitted to killing 30 people. The possibility of aiding guilty people does not faze Loftus. \u201cI haven't had a situation where someone was acquitted because of my testimony and then went on to commit some awful crime,\u201d she says. \u201cI would feel horrible if that happened, but I'm only one small piece of a court case.\u201d She is often compensated for her expert-witness work, earning up to US$500 per hour, she says. Nita Farahany, a bioethicist at Duke University School of Law in Durham, North Carolina, says that Loftus's activism in the court is not unique, and that testifying on behalf of unpopular defendants is important. \u201cIt shows that she has tried to be truly impartial, and that her goal is to try to provide an accurate understanding of the science, no matter who is involved.\u201d Still, Loftus has drawn the line at some defendants, such as John Demjanjuk, who in 1988 stood accused in Israel of being 'Ivan the Terrible', a guard who operated gas chambers at the concentration camp Treblinka in Poland during the Second World War. Loftus, herself Jewish, declined to testify because she worried that it would upset family and friends. The case led some to accuse her of double standards. But those criticisms were mild compared with the reactions that she would soon trigger in her most controversial legal work. \n               Digging up the past \n             In 1990, Loftus got a call from a California attorney defending George Franklin, whose daughter claimed that during therapy, she had recovered decades-old memories of him murdering her friend, Susan Nason. Loftus decided to consult for the defence team. \u201cI thought it was pretty fishy and started looking into the literature,\u201d she says. She found little convincing research to support the idea that traumatic memories could be repressed for years. Franklin was convicted despite her testimony. He spent five years in prison before an appeals court reviewed and then overturned his conviction amid doubts over his daughter's statements. The courts went on to see a surge in cases based on recovered childhood memories, fuelled in part by popular books and high-profile accusations. Loftus began to wonder whether it was possible to fabricate complex, believable memories. \u201cI wanted to see if we could implant a rich memory of an entirely made-up event,\u201d she says. An idea eventually came to her as she drove past a shopping mall. Working with a student, Jacqueline Pickrell, Loftus recruited 24 people and, with the cooperation of family members, presented them with four detailed accounts of events from their childhood. Three of the incidents had actually taken place, but the fourth \u2014 a dramatic account of being lost in a mall \u2014 was entirely concocted by Loftus and corroborated by the participants' relatives. One-quarter of the participants claimed to remember the false event 6 . \n               Battle ground \n             Loftus became convinced that well-meaning psychotherapists could inadvertently implant false memories into patients' minds, and her subsequent testimonies led to a row between therapists who believed their patients were recovering lost memories and researchers who thought something else was afoot. To try to settle these 'memory wars', the American Psychological Association (APA) commissioned an expert report about the subject, to be written by three memory researchers, including Loftus, and three clinical psychologists. The groups could not agree, and each ended up writing a separate report. \u201cIt was very polarizing,\u201d says Stephen Ceci, a developmental psychologist at Cornell University in Ithaca, New York, who worked with Loftus on one of the reports. There are ways in which traumatic memories of real events can be recalled after being buried for years, he adds, but without hard evidence, it is impossible to distinguish false memories from real ones in court. It is, therefore, possible that some claims of childhood abuse go unvindicated because of Loftus' testimony, and this is the cause of much of the hostility towards her. Ross Cheit, a political scientist at Brown University in Providence, Rhode Island, started the Recovered Memory Project in 1995 to document and respond to what he says has been a one-sided debate. There are now more than 100 corroborated cases of recovered memory on his website ( http://blogs.brown.edu/recoveredmemory ), he says, including some on which Loftus had consulted. \u201cLoftus is often on the losing side, and she's sometimes wrong in a spectacular way,\u201d Cheit says. Her testimonies, he adds, can be psychologically damaging for the victims. \u201cIf you're telling someone you think their memories are false, when they have corroborating evidence that they were abused, that's corrosive.\u201d Loftus does not believe that Cheit's site corroborates recovered memories. \u201cHe might have some cases of people who didn't think about their abuse for some time and were reminded of it, but as for actual repression, no,\u201d she says. \u201cI cringe at the idea of hurting genuine victims, but when an innocent person is accused, we have a whole new set of victims, and I'm more horrified by an innocent person getting convicted than by a guilty person being acquitted.\u201d But her testimonies and investigations into recovered memories have strained her professional relationships. Towards the end of 1995, two women filed formal complaints against Loftus with the APA. Lynn Crooks and Jennifer Hoult had won civil suits in cases involving recovered memories of childhood sexual abuse, and both claimed that Loftus had distorted the facts of their cases in articles and interviews. Loftus resigned from the APA and critics speculated that she had caught wind of the complaints and left before a formal investigation could take place. But Loftus chalks her resignation up to political disagreements, saying she knew nothing of the complaints at the time. In 1997, Loftus and several colleagues began to dig into a published case study describing an anonymous subject, 'Jane Doe', who had apparently recovered a repressed memory of childhood abuse 7 . They found information that cast doubt on her account, but before they could publish, Doe contacted the University of Washington in Seattle, where Loftus was working, and accused the team of breaching her privacy. The university confiscated Loftus's files, put her under investigation for nearly two years and prevented her from publishing. She was eventually cleared, and published the work 8  in 2002. The next year, however, Doe sued Loftus and her collaborators for fraud, invasion of privacy, defamation and causing emotional distress. It was at around that time that Loftus moved to the University of California, Irvine. The Jane Doe case was eventually settled in 2007, when the Supreme Court of California dismissed all but one of the charges and Loftus agreed to pay a nuisance settlement of $7,500. \u201cIt was such a stressful time, but I can't really say it was detrimental overall,\u201d says Loftus. Her work has now moved from trying to affect single cases to pushing for broader changes in the legal system. Loftus has been working with Pennsylvania trial judge Jeannine Turgeon to compile a set of guidelines similar to those instituted in New Jersey last year. They instruct jurors that memory \u201cis not like a video recording\u201d and ask them to consider the many factors that can alter memories, such as the presence of a weapon, which can draw attention away from the perpetrator's face. \u201cThis has the potential to be really important,\u201d says Farahany. \u201cUsing cutting-edge research to undercut the idea that memory is as stable and precise as people believe it to be can really help us get to a place where we have better truth-seeking in criminal cases,\u201d she says. Loftus wants to go further. Almost every stage of the legal process \u2014 from the identification and questioning of suspects to cross-examination of eyewitnesses in the courtroom \u2014 is prone to error. In a line-up, for example, police officers can influence identification, but this can be avoided if someone who does not know the identity of the prime suspect conducts the line-up (see  Nature   453 , 442\u2013444; 2008 ). \u201cI'd like to see this kind of thing being implemented, and to keep educating people about the workings of memory,\u201d says Loftus. \n               Mind control \n             Meanwhile, her research has shifted into new controversial waters. Taking on board the lesson that memories can be manufactured, she has been investigating the possibility of using those memories to modify behaviour 9 , 10 . \u201cWe've shown that you can plant a memory of getting sick eating particular foods as a child,\u201d she says, \u201cand we can get people thinking they got sick drinking vodka, so they don't want to drink as much of it later on.\u201d There is no evidence that any of this will successfully transfer from the lab to the real world. Even if it does, it would violate therapists' code of conduct, and could have unforeseen consequences. \u201cLying to children is a slippery slope that makes me uncomfortable,\u201d says Judy Illes, a neuroethicist at the University of British Columbia in Vancouver, Canada. \u201cCan't we alter their behaviour in a positive way, instead of using subterfuge?\u201d But Loftus dismisses the concerns, suggesting that even if therapists cannot do it, parents might want to. \u201cParents lie to their kids all the time, about Santa Claus and the tooth fairy. Would you rather have an unhealthy kid, or one with a few false memories?\u201d \n                     Stress: The roots of resilience 2012-Oct-10 \n                   \n                     Memory 'trick' relieves drug cravings 2012-Apr-12 \n                   \n                     Eyewitness identification: Line-ups on trial 2008-May-21 \n                   \n                     Sex sidetracks 2002-Jun-17 \n                   \n                     Nature special: Science in court \n                   \n                     Elizabeth Loftus \n                   \n                     The Recovered Memory Project \n                   \n                     APA report on recovered memories of abuse \n                   Reprints and Permissions"},
{"file_id": "500138a", "url": "https://www.nature.com/articles/500138a", "year": 2013, "authors": [{"name": "Lee Billings"}], "parsed_as_year": "2006_or_before", "body": "Engineered structures with bizarre optical properties are set to migrate out of the laboratory and into the marketplace. Tom Driscoll would be happy if he never heard the phrase 'Harry Potter-style invisibility cloak' again. But he knows he will. The media can't seem to resist using it when they report the latest advances in metamaterials \u2014 arrays of minuscule 'elements' that bend, scatter, transmit or otherwise shape electromagnetic radiation in ways that no natural material can. It is true that metamaterials could, in principle, route light around objects and render them invisible, not unlike the cloak of a certain fictional wizard. And many metamaterials researchers are trying to make cloaking a reality, not least because the military has eagerly funded the development of such capabilities. However, if such applications ever come to pass it will be decades from now. Technologies closer to commercialization are of more interest to Driscoll, a physicist who oversees metamaterials commercialization at Intellectual Ventures, a patent-aggregation firm in Bellevue, Washington. Applications such as cheaper satellite communications, thinner smartphones and ultrafast optical data processing are \u201cwhere metamaterials are poised to make a huge impact\u201d, he says. Noah Baker explores the world of metamaterials and their commercial future Researchers still face some daunting challenges, he adds \u2014 notably, finding cheap ways to fabricate and manipulate metamaterial elements on a scale of nanometres. But the first metamaterial-based products are expected to come onto the market in a year or so. And, not long after that, Driscoll expects that average consumers will start to enjoy the benefits, such as faster, cheaper Internet connectivity on board planes and from mobile phones. Such applications, he says, will move from being the stuff of peoples' fantasies \u201cto becoming things they can't contemplate living without\u201d. The first laboratory demonstration of a metamaterial was announced in 2000 by physicist David Smith and his colleagues at the University of California, San Diego 1 . Following up on theoretical work done in the 1990s by John Pendry of Imperial College London, these researchers showed that an array of tiny copper wires and rings had a negative refractive index for microwaves \u2014 meaning that microwave radiation flowing into the material is deflected in a direction opposite to that normally observed (see 'Wave engineering'). That triggered intense interest in metamaterials, in part because the ability to bend radiation in such a way had potential for creating invisibility cloaks. Since then, Smith and others have explored a host of variations on the metamaterial idea, often looking to manipulate radiation in ways that have nothing to do with a negative refractive index. They have also moved beyond static arrays, devising techniques to change the way the elements are arranged, how they are shaped and how they respond to radiation. The resulting materials can do things such as turn from opaque to transparent or from red to blue \u2014 all at the flick of a switch. \n               Market movers \n             In January, Smith, now at Duke University in Durham, North Carolina, took on a concurrent role as director of metamaterials commercialization efforts at Intellectual Ventures. \u201cI felt that the time was right, and we didn't need to do any more science for some of these things,\u201d he says. A test case may come as early as next year. Kymeta of Redmond, Washington, a spin-off from Intellectual Ventures, hopes to market a compact antenna that would be one of the first consumer-oriented products based on metamaterials. The relatively inexpensive device would carry broadband satellite communications to and from planes, trains, ships, cars and any other platform required to function in remote locations far from mobile networks. At the heart of the antenna \u2014 the details of which are confidential \u2014 is a flat circuit board containing thousands of electronic metamaterial elements, each of which can have its properties changed in an instant by the device's internal software. This allows the antenna to track a satellite across the sky without having to maintain a specific orientation towards it, the way a standard dish antenna does. Instead, the antenna remains still while the software constantly adjusts the electrical properties of each individual metamaterial element. When this is done correctly, waves emitted from the elements will reinforce one another and propagate skywards only in the direction of the satellite; waves emitted in any other direction will cancel one another out and go nowhere. At the same time \u2014 and for much the same reason \u2014 the array will most readily pick up signals if they are coming from the satellite. This technology is more compact than alternatives such as dish antennas, says Smith. It offers \u201csignificant savings in terms of cost, weight and power draw\u201d. Kymeta has already performed demonstrations of this technology for investors and potential development partners. But Smith cautions that the company has yet to set a price for the antenna and that it must still work to bring production costs down while maintaining the strict performance standards that regulatory agencies demand for any device communicating with satellites. Kymeta has shared so few details of its antenna that researchers say it is hard to offer an evaluation. But Smith is highly regarded in the field. If Kymeta brings the product to market, it may first offer its antenna for use on private jets and passenger planes. If buyers respond well, the company hopes to incorporate the technology into other product lines, such as portable, energy-efficient satellite-communication units for rescue workers or researchers in the field. In January, Smith's group turned heads when it announced its demonstration of another metamaterial device: a camera that can create compressed microwave images without a lens or any moving parts 2 . One important application of the device might be to reduce the cost and complexity of airport security scanners. In their current form, these scanners have to physically sweep a microwave sensor over and around the subject. This produces an unwieldy amount of data that has to be stored before it is processed into an image. The Duke group's device requires very little data storage. It takes numerous snapshots by sending beams of microwaves of multiple wavelengths across the target at about ten times per second. When the microwaves are reflected back by the subject, they fall on a thin strip of square copper metamaterial elements, each of which can be tuned to block or let through reflected radiation. The resulting pattern of opaque and transparent elements can be varied very rapidly, with each configuration transmitting a simplified snapshot of a scanned object into a single sensor. The sensor measures the total intensity of radiation from each snapshot, then outputs a stream of numbers that can be digitally processed to reconstruct a highly compressed image of the subject. This is admittedly just a first step: demonstrations carried out so far have been crude affairs restricted to two-dimensional images of simple metallic objects. Expanding it to three-dimensional images of complex objects remains a challenge. But if that challenge can be overcome, says Driscoll, airports could retire the bulky, expensive, slow booths that currently constitute security checkpoints, and instead use a larger number of thin, inexpensive metamaterial cameras hooked up to computers. Such a shift, Driscoll says, could extend security scanning to rooms, hallways, and corridors throughout airports and other sensitive facilities. In the meantime, a key research goal for Smith and his group is the development of robust and marketable metamaterial devices that are not restricted to radio, microwave or infrared wavelengths. If the technologies could be made to work with visible light, they would become much more useful for applications such as fibre-optic communications or consumer-oriented cameras and displays. \u201cIt won't be easy,\u201d cautions Stephane Larouche, a member of Smith's research team at Duke. For any given type of radiation, he explains, metamaterials can wield their exotic powers only if the elements are smaller and more closely spaced than the wavelength of that radiation. \u201cSo the shorter and shorter the wavelength we wish to use, the smaller each metamaterial element must be,\u201d says Larouche. In the microwave and radio regions of the spectrum, this is relatively easy: wavelengths are measured in centimetres to metres. But an optical metamaterial's elements would have to measure considerably less than a micrometre. That is not impossible: today's high-performance microchips contain features only a few tens of nanometres across. But unlike those essentially static features, says Larouche, the metamaterial elements in many applications would need to incorporate ways for software to change their properties dynamically as needed. \u201cToo often we have gorgeous ideas,\u201d he says, \u201cbut we have no way of fabricating them.\u201d \n               Flat focus \n             Despite these difficulties, workable designs for optical metamaterials have begun to emerge. One was published in March 3  by a group working under Nikolay Zheludev, a physicist at the University of Southampton, UK, who directs a research centre focused on metamaterials at Nanyang Technological University in Singapore. The team's device can greatly alter its ability to transmit or reflect optical wavelengths by means of nanometre-scale, electrically controlled metamaterial elements etched from gold film; it could one day serve as a switch in high-speed fibre-optic communications networks. Meanwhile, because it is so hard to make and control three-dimensional metamaterial arrays at optical scales, some researchers are focusing on two-dimensional 'metasurfaces'. In August 2012, a group led by Federico Capasso at Harvard University in Cambridge, Massachusetts, unveiled a flat metamaterial lens that can focus infrared light to a point in much the same way as a glass lens 4 . \u201cI don't want to claim absolute novelty in this,\u201d Capasso says, \u201cbut I believe we are the first group to so clearly put flat optics on the agenda for commercial applications.\u201d A conventional lens relies on refraction to bend light to a point by passing it through varying thicknesses of glass. Capasso's lens passes light through a two-dimensional array of gold metamaterial elements carved out of a 60-nanometre-thick silicon wafer using electron-beam lithography techniques developed for the microchip industry. The elements are fixed, so cannot be tuned after fabrication. But by selecting a specific size and spacing during the manufacturing process, physicists can shape light of a chosen wavelength in exactly the right way to make it come to a point.  Metamaterials are poised to make a huge impact.  Capasso warns that commercial applications of such flat lenses are probably still a decade away. This is partly because silicon is a rigid and fragile substrate for etching the elements; researchers are looking at more robust and flexible alternatives that would be easier to handle on the production line. They are also looking for better ways to control the carving of the nanoscale elements, which has to be done very precisely. But once the technology is mastered, says Capasso, one obvious application is in smartphone cameras. Lenses, along with batteries, are among the most stubborn limiting factors in smartphone thickness, he says, speculating that a smartphone incorporating a flat camera lens could potentially be made \u201cas thin as a credit card\u201d. The flat lens also avoids aberrations that plague glass lenses, such as the coloured 'fringes' created by the inability to focus all wavelengths to the same point. This means that Capasso's flat lens could also be used to make better, aberration-free microscopes. As good as they might ultimately be, the flat lenses would still be subject to the diffraction limit, which dictates that no conventional lens can resolve details much smaller than the wavelength of the light that illuminates its target. This limit averages about 200 nanometres for visible light. But metamaterials offer a means of fabricating 'superlenses' that could surpass such limits, allowing researchers to see sub-wavelength details of target objects such as viruses or the ever-changing structures in living cells. The key is to recognize that the missing details are still there, carried in 'evanescent' waves of reflected light that die away very rapidly with distance from the illuminated object. Normally, these waves have effectively vanished before they can be captured and focused by a lens. But a metamaterial superlens designed to be placed within tens of nanometres of an object can pick up and magnify these waves. An early proof-of-concept superlens was demonstrated in 2005 by a group working under Xiang Zhang, a physicist at the University of California, Berkeley 5 . Zhang's group produced a simple metamaterial consisting of a 35-nanometre-thick layer of silver in a sandwich with nanoscale layers of chromium and plastic. The team has since been working to refine the superlens concept; in 2007 the researchers advanced the idea by developing 'hyperlenses' from curved, nested layers of compounds such as silver, aluminium and quartz 6 . The lenses not only capture evanescent waves, but can also feed them into a conventional optical system. Ultimately, this could allow sub-wavelength details to be viewed through the eyepiece of a standard microscope. But the complex structure and behaviour of hyperlenses makes them difficult to manufacture and use in this way. \n               Reversible focus \n             By pairing conventional optics with superlenses and hyperlenses based on metamaterials, Zhang hopes eventually to find applications far beyond microscopy. Just as these constructs can magnify sub-wavelength detail, they can also be run in reverse, directing beams of light into sub-wavelength focal points \u2014 a property of potentially revolutionary importance for fabricating minuscule structures using photolithography. If superlenses and hyperlenses can be harnessed for this purpose, the ultra-fine beams of light could be used to etch much smaller features than is possible today. This could greatly increase the density of data storage on optical drives, as well as the number of components that can be crammed onto computer chips. Smith is cautious on that score, pointing out that hyperlenses and superlenses tend to dissipate substantially more of the light energy passing through them than other advanced lithographic techniques now in development. This, he says, makes them prime examples of \u201cstrong and compelling science that is not yet practical for any sort of product path\u201d at optical wavelengths. But, he adds, Zhang's efforts are \u201cheroic experiments that illustrate the potential of metamaterials in a fundamental way\u201d. Zhang concedes that hyperlenses and superlenses are not yet ready for prime time, but believes there is plenty of room for ongoing research to change that situation in the coming years. \u201cThe economic impact could be huge,\u201d he says. \u201cI am cautiously optimistic that metamaterials, superlenses and lithography will prove truly revolutionary. If people aren't too short-sighted, what we can do with metamaterials will be limited only by our imaginations.\u201d \n                     Researchers create portable black hole 2009-Oct-15 \n                   \n                     Metamaterials: Ideal focus 2009-May-27 \n                   \n                     Scientists weave invisibility cloak 2009-Jan-15 \n                   \n                     Reversing the prism 2008-Aug-11 \n                   \n                     Kymeta \n                   \n                     Zhang's group \n                   \n                     Capasso's group \n                   \n                     Smith's group \n                   \n                     Centre for Photonic Metamaterials \n                   Reprints and Permissions"},
{"file_id": "500136a", "url": "https://www.nature.com/articles/500136a", "year": 2013, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "The scientific community is sharply divided over the proposed Keystone XL pipeline from Canada's tar sands. Environmental activist Bill McKibben has called it the \u201cfuse to the biggest carbon bomb on the planet\u201d. Famed US climate researcher James Hansen has warned that it would unleash a \u201cmonster\u201d. And protestors have chained themselves to the White House fence, declaring that it would feed a nasty fossil-fuel addiction and enrich the oil industry while dooming the global climate. The object of all that ire, the Keystone XL pipeline, is designed to carry crude oil some 1,900 kilometres from the tar sands of Alberta, Canada, to the US Midwest, where it will link into a network of pipelines supplying refineries on the Gulf of Mexico. Proponents say that it would provide North America with a secure source of energy and reduce dependence on overseas oil. But for environmentalists frustrated by a stalemate in Congress and repeated failures to secure an aggressive international climate treaty, the pipeline has become a key battle \u2014 one that they hope will trigger a popular uprising against unbridled fossil-fuel development. The issue has also divided the scientific community. Many climate and energy researchers have lined up with environmentalists to oppose what is by all accounts a dirty source of petroleum: emissions from extracting and burning tar-sands oil in the United States are 14\u201320% higher than the country's average oil emissions. But other researchers say that the Keystone controversy is diverting attention from issues that would have much greater impact on greenhouse-gas emissions, such as the use of coal. Some experts find themselves on both sides. \u201cI'm of two minds,\u201d says David Keith, a Canadian climate scientist who is now at Harvard University in Cambridge, Massachusetts. \u201cThe extreme statements \u2014 that this is 'game over' for the planet \u2014 are clearly not intellectually true, but I am completely against Keystone, both as an Albertan and somebody who cares about the climate.\u201d \n               Significant figures \n             The pipeline's future rests with US President Barack Obama, who declared in June that Keystone would serve the national interest only if it \u201cdoes not significantly exacerbate the problem of carbon pollution\u201d. The debate now centres on the definition of 'significantly', which requires a bit of context. Canada has an estimated 170 billion barrels of dense, viscous oil locked up in deposits of loose sandstone in Alberta. These tar sands, or oil sands, produced 1.8 million barrels of oil per day in 2012, and that figure is projected to nearly triple by 2030. More than two-thirds of that oil makes its way via pipelines to the United States, where it accounts for around 7% of US oil consumption. But the pipelines are reaching capacity, so companies looking to increase production must first figure out how to get their product out of Canada. Keystone is just the first step and would eventually carry some 730,000 barrels per day from the tar sands to US refineries. To meet the industry's production forecast, however, at least three more pipelines of comparable capacity will be needed. A draft environmental-impact statement by the US Department of State found that halting the Keystone pipeline would have a minimal impact on the development of the tar sands, because oil companies would find other ways to get their product to market. In the short term, that means shipping by rail, which would increase emissions. But the state department's conclusions have come under fire from environmentalists as well as the US Environmental Protection Agency, which urged the state department to conduct \u201ca more careful review\u201d of the economic analysis for its final assessment. There is cause for scepticism about the alternatives to the Keystone pipeline. Although companies are increasingly using rail to ship conventional oil out of North Dakota, where production is booming, that option is less attractive for the oil sands. Rail cars carrying tar-sands oil cost more to run because they must be heated, and they cannot carry as much, because the oil is much heavier than typical crude. Trains from Alberta must also travel much farther to reach the Gulf coast. The combination of factors increases prices by about US$20 per barrel. \n               Barrelling forward \n             Other proposed pipelines could transport the oil, but they also face challenges. Last week TransCanada, the company behind the Keystone XL plan, said that it wants to build a $12-billion pipeline to the country's Atlantic coast, but environmentalists concerned about oil spills have vowed to block that route. And British Columbia's ruling party took a stance in May against a planned pipeline to Canada's Pacific coast. \u201cReally what you are left with is that the Keystone pipeline is the only route forward\u201d to increase production from the tar sands, says Susan Casey-Lefkowitz, director of the international programme for the Natural Resources Defense Council in Washington DC. And because increasing production necessarily increases emissions, she says, Keystone \u201cfails the president's climate test\u201d. But the matter is far from settled. The oil industry has already begun shipping conventional oil out of Alberta by rail, and IHS Cambridge Energy Research Associates, a consulting firm in Englewood, Colorado, projects that tar-sands producers will employ rail if no pipelines are built. \u201cWe think it's economic, and we think it will grow in the absence of pipelines,\u201d says Jackie Forrest, senior director for global oil at the firm. Others argue that the industry would push to build other pipelines if Keystone were to fail. The environmental impact must also be weighed against issues such as safety and energy security, says Andrew Weaver, a climate scientist at the University of Victoria in British Columbia, who was elected this year to the province's parliament as a member of the Green Party. He refuses to weigh in on whether the pipeline should be built, saying that the decision rests with the United States. But he calls the argument for North American energy security \u201cquite compelling\u201d. And he cites the crash last month of an oil-transport train in Quebec, as evidence that the potential for human error is higher with rail than with pipelines.\u201cI think it's mad that we are burning all of our oil,\u201d Weaver says, \u201cbut we've got to put it into perspective.\u201d In 2012, Weaver sought to do just that. He and a student calculated what would happen to global temperatures if the tar sands were fully developed. The proven reserves \u2014 those that could be developed with known technologies \u2014 make up roughly 11% of the global total for oil, and Weaver's model suggested that full development would boost the average global temperature by just 0.03 degrees Celsius (N. C. Swart and A. J. Weaver  Nature Clim. Change   2 , 134\u2013136; 2012 ). Weaver says that the initial focus should be on coal, which he found would have 30 times the climate impact of oil if the world burned all proven coal reserves. \u201cAs a serious strategy for dealing with climate, blocking Keystone is a waste of time,\u201d says David Victor, a climate-policy expert at the University of California, San Diego. \u201cBut as a strategy for arousing passion, it is dynamite.\u201d Well aware that their future prosperity may depend on it, producers as well as the government of Alberta \u2014 which hauled in about $4.3 billion in royalties from the oil sands in the 2011\u201312 fiscal year \u2014 say that they are cleaning up operations there. Within Canada much of the concern has focused on local pollution from the mining operations, which spew exhaust and toxic chemicals into the atmosphere and leave behind large wastewater ponds. But companies are also working to trim the overall greenhouse-gas emissions associated with production. Environment Canada, the country's environment agency, claims that tar-sand producers reduced their emissions by 26% per barrel of oil between 1990 and 2010. But emissions are poised to increase in the coming years as companies probe deeper into the earth. The industry is up against geology. Having depleted many of the tar-sand deposits accessible through surface mining, companies are exploiting deeper formations by injecting steam into the rock layers to liquefy and produce the oil. Producing steam requires natural gas, which can increase emissions by up to 30% compared with the surface-mining process (see 'Dirty oil'). Those deeper deposits now account for roughly half of oil-sands production, but they make up 80% of proven reserves, so their share of production will only climb higher. Alberta is investing more than $700 million in a $1.35-billion demonstration project that would capture and bury up to 1.2 million tonnes of carbon dioxide annually from a facility that upgrades bitumen, the tar-like product from the oil sands, into crude oil for shipment. On its own, however, that project would not have a significant impact on emissions. More generally, Alberta enacted a law in 2007 requiring all major emitters in the province to reduce their annual emissions intensity \u2014 a measure of emissions relative to production \u2014 by 12% or face a levy of $14 for every tonne of emissions in excess of that target. That levy has raised $376 million for clean-energy investments to date, nearly $79 million of which has been invested in projects related to the tar sands. Yet there is no way to cleanly produce oil from the tar sands \u2014 or from anywhere else. \u201cThe way that you drive down emissions in the transportation sector is by driving less, by becoming more efficient, and then by changing your fuels,\u201d says Michael Levi, an energy-policy fellow at the Council on Foreign Relations in New York. Many researchers who have sided with environmentalists on Keystone acknowledge that the decision is mostly symbolic. But in the absence of other action, says Harvard's Keith, it is important to get people involved and to send industry a message that the world is moving towards cleaner fuels, not dirtier ones. For Ken Caldeira, a climate researcher at the Carnegie Institution for Science in Stanford, California, it is a simple question of values. \u201cI don't believe that whether the pipeline is built or not will have any detectable climate effect,\u201d he says. \u201cThe Obama administration needs to signal whether we are going to move toward zero-emission energy systems or whether we are going to move forward with last century's energy systems.\u201d \n                     Canadian oil sands: defusing the carbon bomb 2012-Feb-28 \n                   \n                     Oil-sands vote ends in deadlock 2012-Feb-24 \n                   \n                     Impacts of Canada's oil-sands operations 'exaggerated' 2010-Dec-15 \n                   \n                     Obama may be tough on Canada's tar sands 2009-Feb-13 \n                   \n                     Nature special: After Kyoto \n                   \n                     Keystone XL pipeline \n                   \n                     Keystone application at the Department of State \n                   \n                     Alberta Energy oil sands \n                   Reprints and Permissions"},
{"file_id": "500516a", "url": "https://www.nature.com/articles/500516a", "year": 2013, "authors": [{"name": "Zeeya Merali"}], "parsed_as_year": "2006_or_before", "body": "Many researchers believe that physics will not be complete until it can explain not just the behaviour of space and time, but where these entities come from. \u201cImagine waking up one day and realizing that you actually live inside a computer game,\u201d says Mark Van Raamsdonk, describing what sounds like a pitch for a science-fiction film. But for Van Raamsdonk, a physicist at the University of British Columbia in Vancouver, Canada, this scenario is a way to think about reality. If it is true, he says, \u201ceverything around us \u2014 the whole three-dimensional physical world \u2014 is an illusion born from information encoded elsewhere, on a two-dimensional chip\u201d. That would make our Universe, with its three spatial dimensions, a kind of hologram, projected from a substrate that exists only in lower dimensions. This 'holographic principle' is strange even by the usual standards of theoretical physics. But Van Raamsdonk is one of a small band of researchers who think that the usual ideas are not yet strange enough. If nothing else, they say, neither of the two great pillars of modern physics \u2014 general relativity, which describes gravity as a curvature of space and time, and quantum mechanics, which governs the atomic realm \u2014 gives any account for the existence of space and time. Neither does string theory, which describes elementary threads of energy. Van Raamsdonk and his colleagues are convinced that physics will not be complete until it can explain how space and time emerge from something more fundamental \u2014 a project that will require concepts at least as audacious as holography. They argue that such a radical reconceptualization of reality is the only way to explain what happens when the infinitely dense 'singularity' at the core of a black hole distorts the fabric of space-time beyond all recognition, or how researchers can unify atomic-level quantum theory and planet-level general relativity \u2014 a project that has resisted theorists' efforts for generations. \u201cAll our experiences tell us we shouldn't have two dramatically different conceptions of reality \u2014 there must be one huge overarching theory,\u201d says Abhay Ashtekar, a physicist at Pennsylvania State University in University Park. Finding that one huge theory is a daunting challenge. Here,  Nature  explores some promising lines of attack \u2014 as well as some of the emerging ideas about how to test these concepts (see 'The fabric of reality'). \n               Gravity as thermodynamics \n             One of the most obvious questions to ask is whether this endeavour is a fool's errand. Where is the evidence that there actually is anything more fundamental than space and time? A provocative hint comes from a series of startling discoveries made in the early 1970s, when it became clear that quantum mechanics and gravity were intimately intertwined with thermodynamics, the science of heat. In 1974, most famously, Stephen Hawking of the University of Cambridge, UK, showed that quantum effects in the space around a black hole will cause it to spew out radiation as if it was hot. Other physicists quickly determined that this phenomenon was quite general. Even in completely empty space, they found, an astronaut undergoing acceleration would perceive that he or she was surrounded by a heat bath. The effect would be too small to be perceptible for any acceleration achievable by rockets, but it seemed to be fundamental. If quantum theory and general relativity are correct \u2014 and both have been abundantly corroborated by experiment \u2014 then the existence of Hawking radiation seemed inescapable. A second key discovery was closely related. In standard thermodynamics, an object can radiate heat only by decreasing its entropy, a measure of the number of quantum states inside it. And so it is with black holes: even before Hawking's 1974 paper, Jacob Bekenstein, now at the Hebrew University of Jerusalem, had shown that black holes possess entropy. But there was a difference. In most objects, the entropy is proportional to the number of atoms the object contains, and thus to its volume. But a black hole's entropy turned out to be proportional to the surface area of its event horizon \u2014 the boundary out of which not even light can escape. It was as if that surface somehow encoded information about what was inside, just as a two-dimensional hologram encodes a three-dimensional image. In 1995, Ted Jacobson, a physicist at the University of Maryland in College Park, combined these two findings, and postulated that every point in space lies on a tiny 'black-hole horizon' that also obeys the entropy\u2013area relationship. From that, he found, the mathematics yielded Einstein's equations of general relativity \u2014 but using only thermodynamic concepts, not the idea of bending space-time 1 . \u201cThis seemed to say something deep about the origins of gravity,\u201d says Jacobson. In particular, the laws of thermodynamics are statistical in nature \u2014 a macroscopic average over the motions of myriad atoms and molecules \u2014 so his result suggested that gravity is also statistical, a macroscopic approximation to the unseen constituents of space and time. In 2010, this idea was taken a step further by Erik Verlinde, a string theorist at the University of Amsterdam, who showed 2  that the statistical thermodynamics of the space-time constituents \u2014 whatever they turned out to be \u2014 could automatically generate Newton's law of gravitational attraction. And in separate work, Thanu Padmanabhan, a cosmologist at the Inter-University Centre for Astronomy and Astrophysics in Pune, India, showed 3  that Einstein's equations can be rewritten in a form that makes them identical to the laws of thermodynamics \u2014 as can many alternative theories of gravity. Padmanabhan is currently extending the thermodynamic approach in an effort to explain the origin and magnitude of dark energy: a mysterious cosmic force that is accelerating the Universe's expansion. Testing such ideas empirically will be extremely difficult. In the same way that water looks perfectly smooth and fluid until it is observed on the scale of its molecules \u2014 a fraction of a nanometre \u2014 estimates suggest that space-time will look continuous all the way down to the Planck scale: roughly 10 \u221235  metres, or some 20 orders of magnitude smaller than a proton. But it may not be impossible. One often-mentioned way to test whether space-time is made of discrete constituents is to look for delays as high-energy photons travel to Earth from distant cosmic events such as supernovae and \u03b3-ray bursts. In effect, the shortest-wavelength photons would sense the discreteness as a subtle bumpiness in the road they had to travel, which would slow them down ever so slightly. Giovanni Amelino-Camelia, a quantum-gravity researcher at the University of Rome, and his colleagues have found 4  hints of just such delays in the photons from a \u03b3-ray burst recorded in April. The results are not definitive, says Amelino-Camelia, but the group plans to expand its search to look at the travel times of high-energy neutrinos produced by cosmic events. He says that if theories cannot be tested, \u201cthen to me, they are not science. They are just religious beliefs, and they hold no interest for me.\u201d Other physicists are looking at laboratory tests. In 2012, for example, researchers from the University of Vienna and Imperial College London proposed 5  a tabletop experiment in which a microscopic mirror would be moved around with lasers. They argued that Planck-scale granularities in space-time would produce detectable changes in the light reflected from the mirror (see  Nature   http://doi.org/njf ; 2012 ). \n               Loop quantum gravity \n             Even if it is correct, the thermodynamic approach says nothing about what the fundamental constituents of space and time might be. If space-time is a fabric, so to speak, then what are its threads? One possible answer is quite literal. The theory of loop quantum gravity, which has been under development since the mid-1980s by Ashtekar and others, describes the fabric of space-time as an evolving spider's web of strands that carry information about the quantized areas and volumes of the regions they pass through 6 . The individual strands of the web must eventually join their ends to form loops \u2014 hence the theory's name \u2014 but have nothing to do with the much better-known strings of string theory. The latter move around in space-time, whereas strands actually are space-time: the information they carry defines the shape of the space-time fabric in their vicinity. Because the loops are quantum objects, however, they also define a minimum unit of area in much the same way that ordinary quantum mechanics defines a minimum ground-state energy for an electron in a hydrogen atom. This quantum of area is a patch roughly one Planck scale on a side. Try to insert an extra strand that carries less area, and it will simply disconnect from the rest of the web. It will not be able to link to anything else, and will effectively drop out of space-time. One welcome consequence of a minimum area is that loop quantum gravity cannot squeeze an infinite amount of curvature onto an infinitesimal point. This means that it cannot produce the kind of singularities that cause Einstein's equations of general relativity to break down at the instant of the Big Bang and at the centres of black holes. In 2006, Ashtekar and his colleagues reported 7  a series of simulations that took advantage of that fact, using the loop quantum gravity version of Einstein's equations to run the clock backwards and visualize what happened before the Big Bang. The reversed cosmos contracted towards the Big Bang, as expected. But as it approached the fundamental size limit dictated by loop quantum gravity, a repulsive force kicked in and kept the singularity open, turning it into a tunnel to a cosmos that preceded our own. This year, physicists Rodolfo Gambini at the Uruguayan University of the Republic in Montevideo and Jorge Pullin at Louisiana State University in Baton Rouge reported 8  a similar simulation for a black hole. They found that an observer travelling deep into the heart of a black hole would encounter not a singularity, but a thin space-time tunnel leading to another part of space. \u201cGetting rid of the singularity problem is a significant achievement,\u201d says Ashtekar, who is working with other researchers to identify signatures that would have been left by a bounce, rather than a bang, on the cosmic microwave background \u2014 the radiation left over from the Universe's massive expansion in its infant moments. Loop quantum gravity is not a complete unified theory, because it does not include any other forces. Furthermore, physicists have yet to show how ordinary space-time would emerge from such a web of information. But Daniele Oriti, a physicist at the Max Planck Institute for Gravitational Physics in Golm, Germany, is hoping to find inspiration in the work of condensed-matter physicists, who have produced exotic phases of matter that undergo transitions described by quantum field theory. Oriti and his colleagues are searching for formulae to describe how the Universe might similarly change phase, transitioning from a set of discrete loops to a smooth and continuous space-time. \u201cIt is early days and our job is hard because we are fishes swimming in the fluid at the same time as trying to understand it,\u201d says Oriti. \n               Causal sets \n             Such frustrations have led some investigators to pursue a minimalist programme known as causal set theory. Pioneered by Rafael Sorkin, a physicist at the Perimeter Institute in Waterloo, Canada, the theory postulates that the building blocks of space-time are simple mathematical points that are connected by links, with each link pointing from past to future. Such a link is a bare-bones representation of causality, meaning that an earlier point can affect a later one, but not vice versa. The resulting network is like a growing tree that gradually builds up into space-time. \u201cYou can think of space emerging from points in a similar way to temperature emerging from atoms,\u201d says Sorkin. \u201cIt doesn't make sense to ask, 'What's the temperature of a single atom?' You need a collection for the concept to have meaning.\u201d In the late 1980s, Sorkin used this framework to estimate 9  the number of points that the observable Universe should contain, and reasoned that they should give rise to a small intrinsic energy that causes the Universe to accelerate its expansion. A few years later, the discovery of dark energy confirmed his guess. \u201cPeople often think that quantum gravity cannot make testable predictions, but here's a case where it did,\u201d says Joe Henson, a quantum-gravity researcher at Imperial College London. \u201cIf the value of dark energy had been larger, or zero, causal set theory would have been ruled out.\u201d \n               Causal dynamical triangulations \n             That hardly constituted proof, however, and causal set theory has offered few other predictions that could be tested. Some physicists have found it much more fruitful to use computer simulations. The idea, which dates back to the early 1990s, is to approximate the unknown fundamental constituents with tiny chunks of ordinary space-time caught up in a roiling sea of quantum fluctuations, and to follow how these chunks spontaneously glue themselves together into larger structures. The earliest efforts were disappointing, says Renate Loll, a physicist now at Radboud University in Nijmegen, the Netherlands. The space-time building blocks were simple hyper-pyramids \u2014 four-dimensional counterparts to three-dimensional tetrahedrons \u2014 and the simulation's gluing rules allowed them to combine freely. The result was a series of bizarre 'universes' that had far too many dimensions (or too few), and that folded back on themselves or broke into pieces. \u201cIt was a free-for-all that gave back nothing that resembles what we see around us,\u201d says Loll. But, like Sorkin, Loll and her colleagues found that adding causality changed everything. After all, says Loll, the dimension of time is not quite like the three dimensions of space. \u201cWe cannot travel back and forth in time,\u201d she says. So the team changed its simulations to ensure that effects could not come before their cause \u2014 and found that the space-time chunks started consistently assembling themselves into smooth four-dimensional universes with properties similar to our own 10 . Intriguingly, the simulations also hint that soon after the Big Bang, the Universe went through an infant phase with only two dimensions \u2014 one of space and one of time. This prediction has also been made independently by others attempting to derive equations of quantum gravity, and even some who suggest that the appearance of dark energy is a sign that our Universe is now growing a fourth spatial dimension. Others have shown that a two-dimensional phase in the early Universe would create patterns similar to those already seen in the cosmic microwave background. \n               Holography \n             Meanwhile, Van Raamsdonk has proposed a very different idea about the emergence of space-time, based on the holographic principle. Inspired by the hologram-like way that black holes store all their entropy at the surface, this principle was first given an explicit mathematical form by Juan Maldacena, a string theorist at the Institute of Advanced Study in Princeton, New Jersey, who published 11  his influential model of a holographic universe in 1998. In that model, the three-dimensional interior of the universe contains strings and black holes governed only by gravity, whereas its two-dimensional boundary contains elementary particles and fields that obey ordinary quantum laws without gravity. Hypothetical residents of the three-dimensional space would never see this boundary, because it would be infinitely far away. But that does not affect the mathematics: anything happening in the three-dimensional universe can be described equally well by equations in the two-dimensional boundary, and vice versa. In 2010, Van Raamsdonk studied what that means when quantum particles on the boundary are 'entangled' \u2014 meaning that measurements made on one inevitably affect the other 12 . He discovered that if every particle entanglement between two separate regions of the boundary is steadily reduced to zero, so that the quantum links between the two disappear, the three-dimensional space responds by gradually dividing itself like a splitting cell, until the last, thin connection between the two halves snaps. Repeating that process will subdivide the three-dimensional space again and again, while the two-dimensional boundary stays connected. So, in effect, Van Raamsdonk concluded, the three-dimensional universe is being held together by quantum entanglement on the boundary \u2014 which means that in some sense, quantum entanglement and space-time are the same thing. Or, as Maldacena puts it: \u201cThis suggests that quantum is the most fundamental, and space-time emerges from it.\u201d Reprints and Permissions"},
{"file_id": "500392a", "url": "https://www.nature.com/articles/500392a", "year": 2013, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "In a technical tour de force, Japanese researchers created eggs and sperm in the laboratory. Now, scientists have to determine how to use those cells safely \u2014 and ethically. Since last October, molecular biologist Katsuhiko Hayashi has received around a dozen e-mails from couples, most of them middle-aged, who are desperate for one thing: a baby. One menopausal woman from England offered to come to his laboratory at Kyoto University in Japan in the hope that he could help her to conceive a child. \u201cThat is my only wish,\u201d she wrote. The requests started trickling in after Hayashi published the results of an experiment that he had assumed would be of interest mostly to developmental biologists 1 . Starting with the skin cells of mice  in vitro , he created primordial germ cells (PGCs), which can develop into both sperm and eggs. To prove that these laboratory-grown versions were truly similar to naturally occurring PGCs, he used them to create eggs, then used those eggs to create live mice. He calls the live births a mere 'side effect' of the research, but that bench experiment became much more, because it raised the prospect of creating fertilizable eggs from the skin cells of infertile women. And it also suggested that men's skin cells could be used to create eggs, and that sperm could be generated from women's cells. (Indeed, after the research was published, the editor of a gay and lesbian magazine e-mailed Hayashi for more information.) Ewen Callaway reports on the ethical challenges of using lab-made sperm and egg cells in fertility treatments. Despite the innovative nature of the research, the public attention surprised Hayashi and his senior professor, Mitinori Saitou. They have spent more than a decade piecing together the subtle details of mammalian gamete production and then recreating that process  in vitro  \u2014 all for the sake of science, not medicine. Their method now allows researchers to create unlimited PGCs, which were previously difficult to obtain, and this regular supply of treasured cells has helped to drive the study of mammalian reproduction. But as they push forward with the scientifically challenging transition from mice to monkeys and humans, they are setting the course for the future of infertility treatments \u2014 and perhaps even bolder experiments in reproduction. Scientists and the public are just starting to grapple with the associated ethical issues. \u201cIt goes without saying that [they] really transformed the field in the mouse,\u201d says Amander Clark, a fertility expert at the University of California, Los Angeles. \u201cNow, to avoid derailing the technology before it's had a chance to demonstrate its usefulness, we have to have conversations about the ethics of making gametes this way.\u201d \n               Back to the beginning \n             In the mouse, germ cells emerge just after the first week of embryonic development, as a group of around 40 PGCs 2 . This little cluster goes on to form the tens of thousands of eggs that female mice have at birth, and the millions of sperm cells that males produce every day, and it will pass on the mouse's entire genetic heritage. Saitou wanted to understand what signals direct these cells throughout their development. Over the past decade, he has laboriously identified several genes \u2014 including  Stella ,  Blimp1  and  Prdm14  \u2014 that, when expressed in certain combinations and at certain times, play a crucial part in PGC development 3 , 4 , 5 . Using these genes as markers, he was able to select PGCs from among other cells and study what happens to them. In 2009, from experiments at the RIKEN Center for Developmental Biology in Kobe, Japan, he found that when culture conditions are right, adding a single ingredient \u2014 bone morphogenetic protein 4 (Bmp4) \u2014 with precise timing is enough to convert embryonic cells to PGCs 2 . To test this principle, he added high concentrations of Bmp4 to embryonic cells. Almost all of them turned into PGCs 2 . He and other scientists had expected the process to be more complicated. Saitou's approach \u2014 meticulously following the natural process \u2014 was in stark contrast to work that others were doing, says Jacob Hanna, a stem-cell expert at the Weizmann Institute of Science in Rehovot, Israel. Many scientists try to create specific cell types  in vitro  by bombarding stem cells with signalling molecules and then picking through the resulting mixture of mature cells for the ones they want. But it is never clear by what process these cells are formed or how similar they are to the natural versions. Saitou's efforts to find out precisely what is needed to make germ cells, to get rid of superfluous signals and to note the exact timing of various molecules at work, impressed his colleagues. \u201cThere's a really beautiful hidden message in this work \u2014 that differentiation of cells [ in vitro ] is really not easy,\u201d says Hanna. Harry Moore, a stem-cell biologist at the University of Sheffield, UK, regards the careful recapitulation of germ-cell development as \u201ca triumph\u201d. Until 2009, Saitou's starting point had been cells taken from a live mouse epiblast \u2014 a cup-like collection of cells lining one end of the embryo that forms at the end of the first week of development, just before the PGCs emerge. But to truly master the process, Saitou wanted to start with readily available, cultured cells. That was a project for Hayashi, who in 2009 had returned to Japan from the University of Cambridge, UK, where, like Saitou before him, he had completed a four-year stint in the laboratory of a pioneer in the field, Azim Surani. Surani speaks highly of the two scientists, saying that they \u201ccomplement each other in temperament and in their style and approach to solving problems\u201d. Saitou is \u201csystematic\u201d and \u201csingle-minded about setting and accomplishing his objectives\u201d, whereas Hayashi \u201cworks more intuitively, and takes a broader view of the subject and has outwardly a more relaxed approach\u201d, he says. \u201cTogether they form a very strong team indeed.\u201d Hayashi joined Saitou at Kyoto University, which he quickly found was different from Cambridge. There was much less time spent on theoretical discussions than Hayashi was used to; instead, one jumped into experiments. \u201cIn Japan we just do it. Sometimes that can be very inefficient, but sometimes it makes a huge success,\u201d he says. Hayashi tried to use epiblast cells \u2014 Saitou's starting point \u2014 but instead of using extracted cells as Saitou did, he tried to culture them as a stable cell line that could produce PGCs. That did not work. Hayashi then drew on other research showing that one key regulatory molecule (activin A) and a growth factor (basic fibroblast growth factor) could convert cultured early embryonic stem cells into cells akin to epiblasts. That sparked the idea of using these two factors to induce embryonic stem cells to differentiate into epiblasts, and then to apply Saitou's previous formula to push these cells to become PGCs. The approach was successful 6 . To prove that these artificial PGCs were faithful copies, however, they had to be shown to develop into viable sperm and eggs. The process by which this happens is complicated and ill understood, so the team left the job to nature \u2014 Hayashi inserted the PGCs into the testes of mice that were incapable of producing their own sperm, and waited to see whether the cells would develop 6 . Saitou thought that it would work, but fretted. \u201cIt seemed like a 50/50 chance,\u201d he says. \u201cWe were excited and worried at the same time.\u201d But, on the third or fourth mouse, they found testes with thick, dark seminiferous tubules, stuffed with sperm. \u201cIt happened so properly. I knew they would generate pups,\u201d says Hayashi. The team injected these sperm into eggs and inserted the embryos into female mice. The result was fertile males and females 6  (see 'Making babies'). They repeated the experiment with induced pluripotent stem (iPS) cells \u2014 mature cells that have been reprogrammed to an embryo-like state. Again, the sperm were used to produce pups, proving that they were functional \u2014 a rare accomplishment in the field of stem-cell differentiation, where scientists often argue over whether the cells that they create are truly what they seem to be. \u201cThis is one of the few examples in the entire field of pluripotent-stem-cell research where a fully functional cell type has been unequivocally generated starting from a pluripotent stem cell in a dish,\u201d says Clark. They expected eggs to be more complex, but last year, Hayashi made PGCs  in vitro  with cells from a mouse with normal colouring and then transferred them into the ovaries of an albino mouse 1 . The resulting eggs were fertilized  in vitro  and implanted into a surrogate. \u201cI knew it had worked,\u201d he says, when he saw the pups' dark eyes pressing through their translucent eyelids. \n               Germ-cell bounty \n             Other researchers have been able to replicate the process to generate laboratory-grown PGCs (although none contacted by  Nature  had used them to produce live animals). Artificial PGCs are of particular use to scientists who study epigenetics: the biochemical modifications to DNA that determine which genes are expressed. These modifications \u2014 most often the addition of methyl groups to individual DNA bases \u2014 in some instances carry a sort of historical record of what an organism has experienced (for example, exposure to foreign chemicals in the womb). In a similar way to how they work in other cells, epigenetic markers push PGCs to their fate during embryonic development, but PGCs are unique because when they develop into sperm and eggs, the epigenetic markers are erased. This allows the cells to create a new zygote that is capable of forming all cell types. Faults in subtle epigenetic changes are expected to contribute to infertility and the emergence of disorders such as testicular cancer. Already, Surani's and Hanna's groups have used the artificial PGCs to investigate the role of individual enzymes in epigenetic regulation, which may one day show how the epigenetic networks are involved in disease. Indeed, the  in vitro -generated PGCs offer millions of cells for scientists to study, instead of the 40 or so that can be obtained by dissecting early embryos, says Hanna. \u201cThis is a big deal because here we have these rare cells \u2014 PGCs \u2014 that are undergoing dramatic genome-wide epigenetic changes that we barely understand,\u201d he says. \u201cThe  in vitro  model has provided unprecedented accessibility to scientists,\u201d agrees Clark. \n               Clinical relevance \n             But Hayashi and Saitou have little to offer to the infertile couples begging for their help. Before this protocol can be used in the clinic, there are large wrinkles to be ironed out. Saitou and Hayashi have found that the offspring generated by their technique usually seem to be healthy and fertile, but the PGCs themselves are often not completely 'normal'. For example, the second-generation PGCs often produce eggs that are fragile, misshapen and sometimes dislodged from the complex of cells that supports them 1 . When fertilized, the eggs often divide into cells with three sets of chromosomes rather than the normal two, and the rate at which the artificial PGCs successfully produce offspring is only one-third of the rate for normal  in vitro  fertilization (IVF). Yi Zhang, who studies epigenetics at Harvard Medical School in Boston, Massachusetts, and who has been using Saitou's method, has also found that  in vitro  PGCs do not erase their previous epigenetic programming as well as naturally occurring PGCs. \u201cWe have to be aware that these are PGC-like cells and not PGCs,\u201d he says. In addition, two major technical challenges remain. The first is working out how to make the PGCs convert to mature sperm and eggs without transplanting them back into testes or ovaries; Hayashi is trying to decipher the signals that ovaries and testes give to the PGCs that tell them to become eggs or sperm, which he could then add to artificial PGCs in culture to lead them through these stages. But the most formidable challenge will be repeating the mouse PGC work in humans. The group has already started tweaking human iPS cells using the same genes that Saitou pinpointed as being important in mouse germ-cell development, but both Saitou and Hayashi know that human signalling networks are different from those in mice. Moreover, whereas Saitou had 'countless' numbers of live mouse embryos to dissect, the team has no access to human embryos. Instead, the researchers receive 20 monkey embryos per week from a nearby primate facility, under a grant of \u00a51.2 billion (US$12 million) over five years. If all goes well, Hayashi says, they could repeat the mouse work in monkeys within 5\u201310 years; with small tweaks, this method could then be used to produce human PGCs shortly after. But making PGCs for infertility treatment will still be a huge jump, and many scientists \u2014 Saitou included \u2014 are urging caution. Both iPS and embryonic stem cells frequently pick up chromosomal abnormalities, genetic mutations and epigenetic irregularities during culture. \u201cThere could be potentially far-reaching, multi-generational consequences if something went wrong in a subtle way,\u201d says Moore. Proof that the technique is safe in monkeys would help to allay concerns. But how many healthy monkeys would need to be born before the method could be regarded as safe? And how many generations should be observed? Eventually, human embryos will need to be made and tested, a process that will be slowed by restrictions on creating embryos for research. New, non-invasive imaging techniques will enable doctors to sort good from bad embryos with a high degree of accuracy 7 . Embryos that seem to be similar to normal IVF embryos could get the go-ahead for implantation into humans. This might happen with private funding or in countries with less-restrictive attitudes towards embryo research. When the technology is ready, even more provocative reproductive feats might be possible. For instance, cells from a man's skin could theoretically be used to create eggs that are fertilized with a partner's sperm, then nurtured in the womb of a surrogate. Some doubt, however, that such a feat would ever be possible \u2014 the Hinxton Group, an international consortium of scientists that discusses stem-cell ethics and challenges, concluded that it would be difficult to get eggs from male XY cells and sperm from female XX cells. \u201cThe instructions that the female niche is supplying to the male cell do not coordinate with each other,\u201d says Clark, a member of the consortium. Saitou used iPS cells from male mice to create sperm and from female mice to create eggs, but he says that the reverse should be possible. If so, eggs and sperm from the same mouse could be generated and used for fertilization, producing something never seen before: a mouse created by self-fertilization. Neither Hayashi nor Saitou is ready to try this. \u201cWe would only do this [in mice] if there were a good scientific reason,\u201d says Saitou. Right now he does not see one. The two scientists already feel some pressure from patients and Japanese funding agencies to move forward. The technique could be a last hope for women who have had no luck with IVF, or for people who had cancer in childhood and have lost the ability to produce sperm or eggs. Hayashi warns those who write to him that a viable infertility treatment could be 10 or even 50 years in the future. \u201cMy impression is that it is very far away. I don't want to give people unfeasible hope,\u201d he says. Patients see the end result \u2014 success in mice \u2014 and often ignore the years of painstaking work that led to such a technical tour de force. They do not realize that switching from mice to humans means starting again almost from scratch, says Hayashi. The human early embryo is so different from the mouse that it is almost \u201clike starting over on a process that took more than ten years\u201d. \n                     A future, on ice 2013-Aug-06 \n                   \n                     Generation of eggs from mouse embryonic stem cells and induced pluripotent stem cells 2013-Jul-11 \n                   \n                     Mouse stem cells lay eggs 2012-Oct-04 \n                   Reprints and Permissions"},
{"file_id": "501020a", "url": "https://www.nature.com/articles/501020a", "year": 2013, "authors": [{"name": "Claire Ainsworth"}], "parsed_as_year": "2006_or_before", "body": "How Eric Karsenti's quest to understand the cell launched a trip around the world. It is 3 a.m. in mid-December 2010, an hour before dawn, and  Tara  is anchored amid an angry swell near the entrance to the Strait of Magellan. In the lee of Argentina's cliffs, the 36-metre schooner and her crew have sought haven against the impending weather that has earned these latitudes the nickname the Furious Fifties. But as the winds gather to hurricane force, they snap the safety cord that eases tension on the anchor chain. As the crew struggles to replace it, the storm crescendoes, plucking  Tara 's anchor from the seabed twice before it can be secured again. Bracing himself inside the aluminium hull is Eric Karsenti. Compact and bright-eyed beneath a bushy mop of white hair, he has the air of a seasoned seafarer. But he is also an accomplished molecular cell biologist who has spent most of his career studying microtubules \u2014 rod-like structures that form part of the cell's internal scaffolding. Approaching retirement, he has switched fields, borrowed a famous fashion designer's yacht and launched a 2.5-year expedition around the world to survey ocean ecosystems in unprecedented breadth and detail. Motivated by a love of adventure as much as by science, Karsenti has found his share of both. \u201cThis was really crazy,\u201d Karsenti says later, reflecting on the trip from the safety of the European Molecular Biology Laboratory (EMBL) in Heidelberg, Germany. \u201cI was on the boat for the worst leg of the whole expedition.\u201d The project, called Tara Oceans, set sail from Lorient, France, in September 2009 for a 115,000-kilometre voyage to collect plankton \u2014 microscopic marine organisms \u2014 at 154 distinct sites around the world. Findings from the expedition are now starting to be published, and the bulk of the data will soon be made publicly available. Although other surveys, such as the 2004\u201306 Global Ocean Sampling Expedition, piloted by genomics impresario Craig Venter, have sampled microscopic life in the seas (see  Nature 446, 240\u2013241; 2007 ), the Tara Oceans project is taking a broader approach \u2014 to \u201cstudy it all\u201d 1 . Instead of focusing only on microbes, the scientists collected billions of organisms, from millimetre-scale zooplankton down to viruses 100,000 times smaller. These marine organisms exert tremendous power over the planet, collectively forming a giant engine that drives the cycling of elements such as carbon, nitrogen and oxygen. Photosynthetic marine microbes produce about as much of the world's oxygen as do land plants. Ocean ecosystems are also hives of evolutionary activity in which countless viruses shuttle genes between organisms. Understanding how these complex marine ecosystems work requires a holistic approach, says Karsenti. Tara Oceans scientists are identifying the plankton through a range of techniques including genomics, proteomics and automated high-throughput imaging. To link the organisms to their environments, the researchers also measured properties such as the temperature, pH and salinity of the water around each sample, which they plan to cross-reference with the biological data. Although the project is limited by its ability to sample areas at only one time, \u201cthe data they collect could be used in revolutionary ways\u201d, says Jack Gilbert, a microbial ecologist at Argonne National Laboratory in Illinois. By working out how the different plankton species interact with each other and the environment, the Tara Oceans project members hope to understand how ecosystems emerge from the sum of the interactions between their parts. This huge data set, they say, will help researchers to tackle big issues, such as calculating the biodiversity in the oceans, predicting how marine organisms will respond to environmental shifts and, perhaps, gaining insight into how evolution acts on networks of organisms in ecosystems or of molecules in cells 2 . Given his research background, Karsenti would seem an odd fit for this holistic enterprise. For decades, molecular cell biology was a byword for reductionism. But through his work on microtubules, Karsenti, like a growing number of biologists, came to see that reductionism could reveal only half the story, and that cells must be understood in terms of how the interactions between their components form a greater whole. Although best known as a lab scientist, Karsenti has been interested in the sea since childhood, when he spent summers on France's Brittany coast. Later, he taught sailing to help fund his education. As his studies progressed, however, he left his fascination with the sea mostly behind. At Paris Diderot University, Karsenti studied the typical range of science disciplines, but he also developed an interest in statistical physics, a sideline that would later change the course of his research. After obtaining a PhD in immunology and cell biology from the Pasteur Institute in Paris in 1979, Karsenti joined the lab of cell biologist Marc Kirschner, then at the University of California, San Francisco, to study the cell cycle and cell division. Karsenti was interested in mitosis \u2014 the process of cell division \u2014 particularly in the mitotic spindle, an intricate structure that forms during cell division to ensure that new cells inherit only one copy of each chromosome. Under a microscope, the spindle, formed from microtubules, looks like rigging strung from each end of the cell. Hundreds of proteins collaborate to control this elaborate molecular machine, from the motor proteins that crawl along its filaments to the signalling molecules that synchronize its movements with specific points in the cell-division cycle. The early 1980s was the heyday of classical molecular cell biology and its reductionist approach, and Karsenti found himself purifying proteins and performing biochemical experiments to identify individual molecules and determine their roles in the cell cycle and mitosis. But it became clear to him that this approach, although extremely valuable, could not fully explain the spindle's complex behaviour. \u201cI started to realize that you cannot understand this by simply identifying molecules: you need to understand how they work together,\u201d he says. In 1985 Karsenti set up his own lab at the EMBL. Returning to his interest in statistical physics, he started collaborating with EMBL biologists Thomas Surrey (now at the London Research Institute) and Fran\u00e7ois N\u00e9d\u00e9lec, as well as Stanislas Leibler, a physicist at the Rockefeller University in New York. They were interested in a phenomenon known as self-organization, and in how it could be applied to biology. Physicists, chemists and mathematicians have long studied self-organization to understand how disordered systems can give rise to ordered, dynamic structures such as ocean currents and gyres. These structures are not organized from the top down, but instead emerge spontaneously from the interactions of individual components. By the 1990s, biophysicists had started to apply these concepts to molecular structures in cells. Karsenti says their work was revelatory. \u201cI suddenly saw how you can with formulas encapsulate the whole biological process,\u201d he says. He also saw links to his own work on the mitotic spindle, which meets three key criteria for a self-organizing system. First, it seems to form a stable structure or pattern, but is actually highly dynamic: its individual tubules are constantly building and demolishing themselves. Second, it requires energy to keep itself going. And third, it involves interactions between fixed, or deterministic, factors \u2014 such as the forces exerted by the motor proteins \u2014 and random, or stochastic, factors, such as the probability of a motor protein meeting a microtubule. The final product, the spindle, cannot be explained by any simple cause, but instead emerges from this network of interactions. Karsenti was among a number of researchers who were applying these principles to build computational models of spindle formation. In 2001, he and his colleagues showed how individual interactions between microtubules and motor proteins could result in the emergence of different structures, such as vortices or star-shaped 'asters', at the ends of the spindle 3 . Altering the conditions of these interactions changed the organization: higher concentrations of motor proteins led to more aster formation, for example, whereas slowing down the rate at which the motors move along the microtubules reduced the number of asters. \u201cI was struck by the similarity to what happens in the formation of fish schools and flocks of birds that can also be modelled using stochastic simulation,\u201d says Karsenti. \u201cYet, both phenomena occur at entirely different scales.\u201d \n               Anchors aweigh \n             By 2006, with compulsory retirement looming, Karsenti says, \u201cI had the feeling that I had to do something different.\u201d Keen to raise public awareness of biology and evolution, he took some inspiration from Charles Darwin's  The Voyage of the Beagle  (1839). \u201cHe really describes in this book how travelling helped him to ask important, fundamental questions about the evolution of Earth and the evolution of life,\u201d says Karsenti. Launching a similar expedition seemed a promising idea \u2014 and a timely one, given the impending bicentennial, in 2009, of Darwin's birth (see  Nature 's Darwin 200 special,  www.nature.com/darwin ). Karsenti discussed the idea with a friend and colleague of more than 30 years \u2014 Christian Sardet, a biologist at France's Oceanological Observatory of Villefranche. They floated a plan to produce a reality-television show centred around such a voyage. When that fell through, Sardet introduced Karsenti to Gaby Gorsky, an oceanographer at the observatory, and the three repaired to a local watering hole, Le Cockpit. \u201cWe started to dream,\u201d says Gorsky. \u201cIf we can get an interesting boat, what kind of project can we push forward?\u201d Both Gorsky and Sardet work on plankton, and the discussions about their research reawakened Karsenti's interest in ocean life. The three decided that it would be fascinating to study planktonic ecosystems as a whole. Other researchers had already begun to apply the principles of cellular systems biology to ecosystems, using total gene sequences, or 'metagenomes', of microbial communities to pick apart all the metabolic processes they are carrying out. Ecosystem modellers were showing how ocean circulation, together with differences in factors such as nutrient content and temperature, cause microbial species to self-organize into defined geographical distributions 4 . Karsenti could see the basic principle of deterministic and stochastic factors working together to create a dynamic system. The models were elegant, but they lacked data. Karsenti, Sardet and Gorsky aimed to rectify that deficit \u2014 all they needed was to find a boat and money. Through some of Karsenti's old sailing contacts, they found  Tara . A schooner is an unlikely candidate for oceanographic research, but  Tara  had a sound science pedigree. She was built in 1989 as a polar-exploration yacht and later owned by Peter Blake, a legendary explorer from New Zealand. After Blake was murdered by pirates in 2001, the schooner was bought and renamed by Etienne Bourgois, a keen yachtsman and the son of the French fashion designer Agn\u00e8s B. The family established Tara Exp\u00e9ditions, a non-profit organization that uses the vessel to support and promote environmental research. The foundation's outreach work, as well as the boat herself, made  Tara  the perfect choice for a scientific expedition aimed at popularizing biology. And, Karsenti says, \u201cA sailing boat is much more romantic than a motor boat.\u201d Thanks to his connections in the French competitive yachting community, Karsenti was able to arrange a meeting with Bourgois. The pair co-founded the Tara Oceans project and eventually secured financial backing from a range of funders, including the French government and the EMBL. Karsenti now needed to gather his scientific team. He cast his net wide to secure scientists with the right expertise, including oceanographers, microbial ecologists, taxonomists, cell and systems biologists, bioinformaticians and ecosystem modellers. The project snowballed, eventually including more than 100 researchers, who took turns working aboard  Tara . Also taking part were non-scientists such as French journalist Vincent Hilaire, who blogged from the ship about the expedition's adventures (see 'Gathering data, logging adventure'). Some 5,000 children also visited  Tara  over the course of the voyage, and the project's outreach continues online with projects such as  The Plankton Chronicles , a video series about the strange organisms the team found. \n               Stormy science \n             Doing science aboard  Tara  posed a unique set of challenges. A sailing boat was cheaper and more environmentally friendly than a motor vessel, but dangling delicate measuring equipment hundreds of metres down into the water demanded a stable platform, hard to achieve with a boat of  Tara 's relatively small size. And because Tara Oceans selected discrete, identifiable water masses as sampling stations, the boat had to get to the right place at the right time and under the right weather conditions. Satellite remote sensing allowed the team to monitor oceanographic features such as currents and gyres in near real-time, and detailed forecasts helped to predict whether the elements were going to cooperate. They often did not. On the white-knuckle run down the coast of Argentina in December 2010, for example, the researchers wanted to sample from three different water masses. For three weeks they endured a succession of low-pressure systems sweeping through the area.  Tara 's sturdy design meant they could weather even the storms that nearly robbed them of their anchor, but they had to time their stops carefully. \u201cWe tried to reach the sampling station when we knew the wind would go down to 20 knots,\u201d says Karsenti. \u201cThen we had 20 hours to do the sampling, then bwough! It started again.\u201d Cramming all the work into the brief weather windows then stowing the samples in  Tara 's bows as she ploughed to the next station was physically and emotionally demanding. Karsenti \u2014 on board for four legs of the voyage, a total of three months \u2014 led by example. \u201cHe was always ready to do the difficult stuff,\u201d says Gorsky. \u201cHe was the first to clean the dishes or the toilet \u2014 no problem. He was never hiding behind his status as leader.\u201d Back on land, the challenge for researchers has been integrating and interpreting the genomic, imaging and environmental data from the project, and making it accessible to the research community. They analysed 27,882 samples, using DNA and RNA sequencing to estimate the diversity of organisms present and study their gene functions and ecological roles. The researchers are also developing high-throughput imaging systems to scan samples and automatically identify and quantify the species present \u2014 a task that would take an eternity to do by hand. Preliminary data are rolling in, and the numbers presented by Karsenti at the 8th Annual Genomics of Energy and Environment meeting in Walnut Creek, California, this March, are impressive. Genome data suggest that the oceans hold hundreds of thousands or even a million different kinds of eukaryotes, nearly all of which are not yet known to science. The researchers are now using single-cell sequencing to study these organisms in more detail. There is also an abundance of diverse viruses, including a number of giant viruses \u2014 enigmatic viruses that are larger than many bacterial cells. One litre of water from the ocean's upper, sunlit reaches contains about 45 million of them. \n               Sea of possibility \n             Beyond the big numbers, the team is working to deduce potential ecological relationships. Seeing which organisms occur alongside others in samples, for example, can suggest possible interactions. Hiroyuki Ogata, a Tara Project collaborator and microbiologist at the Mediterranean Institute of Microbiology in Marseille, France, and his team dug through Tara Oceans data and found that a family of giant viruses called Megaviridae occurred alongside filamentous organisms known as oomycetes 5 . Evidence of gene transfer between the two organisms gives the first hints, say the researchers, that oomycetes might be hosts for giant viruses. Karsenti says that such discoveries show how Tara Oceans data could help to unpick the parasitic and symbiotic relationships that have shaped evolution in the ocean. Tara Oceans is now taking part in an arctic sampling mission, and scientists around the world will be able to access data from its first global circuit later this year, when the raw sequences, together with the environmental measurements, will be released in an open-access database hosted by the EMBL European Bioinformatics Institute in Hinxton, UK. Researchers will then be able to compare the Tara Oceans data with results from other large marine surveys, such as Venter's Global Ocean Sampling Expedition and Spain's 2010\u201311 Malaspina project, which focused on samples from deep marine ecosystems. They can also compare the data with the plethora of smaller, more local marine-ecological studies that sample the same area over time so that scientists can see how the ecosystems change. Gilbert says that it is crucial that the data from Tara Oceans be viewed in the context of other longitudinal studies, because the static view from a single voyage is limited. \u201cThis has always been the criticism of these kinds of biogeographic surveys,\u201d he says. Robert Friedman, chief operating officer at the J. Craig Venter Institute in San Diego, California, agrees: \u201cFor us to truly understand the oceans, one was not sufficient; two is probably not sufficient. We're going to need many more.\u201d Karsenti hopes other researchers will take up that challenge and that some of them will, as he did, step outside their comfort zones to do so. Today's cell and developmental biologists, he suggests, should look beyond their yeasts, fruitflies and mice, and observe the extraordinary creatures inhabiting the seas. Perhaps future cell biologists will look first to the oceans, inspired by the tale of a sailor who peered into the heart of a single cell and caught a glimpse of the world. \n                     Giant viruses open Pandora's box 2013-Jul-18 \n                   \n                     Tiny fossils hint at effects of ocean acidification 2012-Sep-28 \n                   \n                     One genome from many 2012-Feb-02 \n                   \n                     Spain's ship comes in 2011-Jul-05 \n                   \n                     International Polar Year: In from the Cold 2009-Feb-25 \n                   \n                     Tara Expeditions \n                   \n                     The Plankton Chronicles \n                   Reprints and Permissions"},
{"file_id": "501150a", "url": "https://www.nature.com/articles/501150a", "year": 2013, "authors": [{"name": "Brendan Borrell"}], "parsed_as_year": "2006_or_before", "body": "To track the fate of threatened species, a young scientist must follow the jungle path of a herpetologist who led a secret double life. Before leaving for the Philippines as an undergraduate in 1992, Rafe Brown scoured his supervisor's bookshelf to learn as much as he could about the creatures he might encounter. He flipped through a photocopy of a 1922 monograph by the prolific herpetologist Edward Taylor, and became mesmerized by a particular lizard,  Ptychozoon intermedium , the Philippine parachute gecko. With marbled skin, webs between its toes and aerodynamic flaps along its body that allow it to glide down from the treetops, it was just about the strangest animal that Brown had ever seen. Brown learned that Taylor had collected the first known example, or type specimen, near the town of Bunawan in 1912, and had deposited it at the Philippine Bureau of Science in Manila. But the specimen had been destroyed along with the building during the Second World War, and the species had never been documented again in that part of the country. \u201cWhat are the chances I'm going to see one of the rarest geckos in the world?\u201d he wondered. Rafe Brown recalls his jungle adventure in search of Edward Taylor's 'lost' species. He was driven by more than curiosity. Given the rampant deforestation in that part of the Philippines, he wanted to determine whether the species still existed there and if so, how similar it was to geckos collected in other areas. He wanted to see, in other words, whether Taylor's 70-year-old taxonomic decisions were still valid. On their first night in the field, Brown and his colleagues drove to the edge of the forest and caught two red eyes in the beam of a headlamp. It was a  Ptychozoon . Back at their hotel, Brown photographed the gecko, took tissue samples for DNA sequencing, and carefully prepped it and stuck it in a jar. It became the neotype to replace Taylor's lost specimen, and in 1997, Brown published a new description of the species 1 . It marked the start of an obsession. As Brown made his career studying biodiversity in the Philippines over the next two decades, he could not escape Taylor's long shadow. The elder herpetologist had logged 23 years in the field over his lifetime, collecting more than 75,000 specimens around the world, and naming hundreds of new species. There is a darker side to Taylor's legacy, however. He was a racist curmudgeon beset by paranoia \u2014 possibly a result of his mysterious double life as a spy for the US government. He had amassed no shortage of enemies by the time he died in 1978. An obituary noted that he was, to many, \u201ca veritable ogre\u2014and woe to anyone who incurred his wrath\u201d 2 . More damaging, perhaps, were the attacks on his scientific reputation. After the loss of his collection in the Philippines, many of the species he had named were declared invalid or duplicates. The standards of taxonomy had advanced beyond Taylor's quaint descriptions, and without the specimens to refer to, his evidence seemed flimsy. Nevertheless, Brown felt a connection with his maligned predecessor. It was a bond that intensified when, in 2005, Brown became curator of herpetology at the University of Kansas Natural History Museum in Lawrence, the same institution at which Taylor had spent much of his career. Over the years, Brown has rebuilt some of Taylor's collection and resurrected many of his species. Now, as he finishes a major monograph on a group of Philippine frogs, he is more convinced than ever: \u201cTaylor was right.\u201d Brown's reassessment could prove crucial. Since Taylor's time, taxonomy has become more than just a naming exercise. Designating a group of organisms as a new species, or lumping it in with an old one, can affect the animals' legal protection and influence the allocation of scarce conservation resources. Amphibian declines, in particular, have made headlines around the world, and the Philippines ranks second only to Sri Lanka for sheer proportions of imperilled species: 79% of Philippine amphibians are found nowhere else on Earth, and 46% are under threat of extinction. But following Taylor's trail has given Brown cause for optimism. \u201cA lot of the things people thought were extinct,\u201d he says, \u201cif you go right where Taylor said to go, you can find them.\u201d \n               A lust for adventure \n             On the fourth floor of the Kansas museum, Brown is walking through the herpetological collections. Lizards float upside down in yellow-tinged alcohol. Snakes coil like corkscrews, and two dozen tiny, dark frogs embrace in a specimen jar. On one shelf, the jars have red ribbons tied around their lids to signify that their contents are type specimens: the standards on which species descriptions are based. When scientists disagree on whether something is a new species or a variant of a known one, they often need to refer back to the type specimen or even return to where it was collected. Brown opens a jar and extracts a small lizard that has a tin tag tied to its waist with twine. It is one of Taylor's originals, on loan from the California Academy of Sciences in San Francisco. \u201cPreserved properly, well labelled and deposited in a safe institution,\u201d says Brown, \u201cthese will last forever.\u201d That is the kind of legacy to which every taxonomist aspires, and Taylor was no exception. Born in Maysville, Missouri, on 23 April 1889, he was still a teenager when he began depositing specimens at this museum. At 23, he joined the civil service and became what he called \u201ca one-man Peace Corps\u201d in the Philippines \u2014 then a US territory \u2014 setting up a school for members of a headhunting tribe in central Mindanao, where he collected the parachute gecko among other species. Next, he worked for the fisheries department in Manila and then completed his PhD on Philippine mammals, but his true passion was always herpetology. It came at the expense of just about everything else in his life. \u201cI named about 500 species,\u201d he would later tell a reporter, \u201cbut I can't always remember the names of my own children.\u201d His wife, Hazel, could not bear his long absences, and they divorced in 1925. By then, Taylor had described more species than most of his peers could achieve in a lifetime: 42 amphibians, 40 lizards and 30 snakes. He sold some of his specimens to museums in the United States, but many remained at the Bureau of Science in Manila, where he thought they would be secure forever. He joined the faculty at Kansas in 1926, and over the next two decades he wandered the globe from Mexico and Costa Rica to parts of Africa, lugging a folding army cot and subsisting on rice and evaporated milk as he collected specimens. In his 60s, however, Taylor found himself under attack. In 1954, Robert Inger, a herpetologist at the Field Museum in Chicago, Illinois, published a withering taxonomic review of Philippine amphibians 3 . Inger, who studied only specimens in museums, axed 44 of the 87 species that Taylor had personally named or approved. \u201cThe differences between Taylor's frogs will be recognized as the differences to be expected between individuals,\u201d Inger wrote. In other words, Taylor was a hack. On his personal copy of Inger's text, Taylor scribbled the word, \u201cHooey.\u201d More recently, herpetologists have levelled other serious allegations against Taylor's character. In 1993, the Kansas Herpetological Society posthumously published his 1916 master's thesis on Kansas reptiles. In a foreword, one of his former students, Hobart Smith, revealed that Taylor had plagiarized large sections from the nineteenth-century palaeontologist and herpetologist Edward Drinker Cope. For those who knew Taylor as a man of principle, it was a devastating revelation, but it also explained why Taylor had never tried to publish the work himself. Then, in 2002, herpetologist Jay Savage at the University of Miami in Coral Gables, Florida, charged that Taylor had secretly copied the field notes of a rival in order to scoop him on his next collecting trip to Costa Rica 4 . Taylor had other demons. He had voiced support for eugenics programmes and reportedly refused to take on Jewish students. Brown makes no apologies for the man, but Taylor's reputation \u2014 for good or ill \u2014 is intertwined with the history of the Kansas museum. \u201cIn the end, we consider him our own,\u201d says Brown. \n               A legacy revisited \n             Brown's interest in Taylor grew when he was a graduate student at the University of Texas at Austin in the late 1990s. He devoured Taylor's monographs to plan his own collecting. He hunted through museum records to find out where Taylor's specimens were, and made visits to see them at the Field Museum and the California academy. But time and time again, he came to a dead end when he wanted information on type specimens that Taylor had deposited at the Philippine Bureau of Science. He soon learned the tragic story of that institution: in February 1945, when US General Douglas MacArthur launched an all-out attack on Manila to expel the Japanese occupiers, the Bureau of Science was reduced to rubble, and all of its botanical and zoological specimens were destroyed, including 32 of Taylor's type specimens. \u201cThe loss is an irreplaceable one,\u201d Taylor's friend Elmer Merrill, a legendary botanist, wrote in  Science 5 . Plant specimens were gradually replenished, but no one had systematically tried to replicate Taylor's efforts. For many years, hostile tribes kept most interlopers away from species-rich regions. In the 1990s, threats of terrorism made it difficult to access places such as the Sulu Archipelago, where Taylor collected types for a dozen species. Despite the danger, Brown resolved to retrace Taylor's steps. In July 1998, he hired a boy to guide his team through the mountains of northern Luzon Island. It was the same place where Taylor had been ambushed by a machete-wielding native in a loincloth. While Brown tromped through streams on his quest, a rumour spread through a town below that Westerners had kidnapped the boy. A dozen locals took up torches, canes and machetes and marched to the home of the village chief on their way to find the kidnappers. When Brown returned, he diffused the situation by producing sacks of amphibians \u2014 his only captives. Taylor, when ambushed, had produced a rifle. During the 1998 trip, Brown and his collaborators found 5 species of reptiles and amphibians not seen for many decades; 13 potentially new to science; and 30 never before reported from the region 6 . One night, Brown caught several  Platymantis  frogs making an insect-like chirp high in the trees. They turned out to be from a species that Taylor had caught and named  rivularis  in 1920. The type specimen still existed, but it was bleached of colour and in pretty bad shape, and there were not many other examples to examine. Accordingly, Inger had lumped  rivularis  in with another species,  hazelae  (named after Taylor's wife). But after hearing its mating calls and seeing its colours in life, Brown decided he would resurrect  P. rivularis  as its own species. Inger, says Brown, favoured more inclusive groupings and was draconian in his decisions. \u201cIf he had any doubt, he would sink a species.\u201d Over the past two decades, Brown and his close collaborator Arvin Diesmos, a herpetologist at the National Museum of the Philippines in Manila, have collected more than 15,000 Philippine specimens \u2014 about one-fifth of Taylor's lifetime haul. To establish evolutionary relationships, Brown also collects DNA, which cannot be extracted from Taylor's formaldehyde-preserved specimens, and he records frog mating calls, a key tool for identifying species. By the time he has finished his own review of the Philippine  Platymantis  frogs, which has been in the works since 2003, he expects to have doubled the number of species from 30 to 60, resurrecting many of Taylor's names. \n               Croak and dagger \n             Brown's fascination with Taylor has gone beyond taxonomy. Early in his research, he became intrigued by \u201cherpetology gossip\u201d about Taylor's extracurricular activities. As he trotted around the globe, Taylor seemed to be conducting field work in conflict zones and, in his memoirs, he alluded to duties outside science 7 . While working for the fisheries department in Manila, he helped to investigate the murder of an Englishman, traded tips with the Swedish secret service and scouted for mercury that could be used in munitions during the First World War. On his river journeys, he occasionally noticed Japanese people, and warned the local governor that they were \u201cspying out of the land\u201d. It was never clear to Taylor's few confidantes whether he used wars as an excuse to get into the field, or vice versa. In his obituary, a former student suggested that Taylor's later activities during the Second World War \u201cprobably will never be known in detail\u201d 2 . But the true nature of Taylor's work is finally coming into focus as intelligence records are declassified and research materials surface. They reveal that Taylor was indeed a spy, and that he continued to do intelligence work after the First World War, when he was sent to Siberia. His official purpose was to join the Red Cross to stop a typhus epidemic, but he was also gathering information on the Communist revolt in Russia and, later, the fate of grand duchess Anastasia, daughter of murdered tsar Nicholas II. Taylor was called to duty again in 1944, when he was 54 and war raged in the Pacific. According to records in the US National Archives, he joined the Office of Strategic Services (OSS), a precursor to the Central Intelligence Agency (CIA), to train agents in Sri Lanka \u2014 then a British territory that provided ready access to Myanmar, Malaysia, Indonesia and other areas that the Japanese had infiltrated. Scientific work, an OSS officer explained to one of Taylor's superiors, was \u201cexcellent cover\u201d. Taylor taught jungle survival at Camp Y, a steamy settlement on the coast. With a penetrating stare and a lantern jaw, he seemed more imposing than his 1.8 metres. In his spare time, he occasionally dodged gunfire to nab specimens, which he studied for two monographs published after the war. \u201cHave just described five new forms of blind snakes from the island,\u201d he wrote to S. Dillon Ripley, a young ornithologist who served with him and would later lead the Smithsonian Institution in Washington DC. In a later letter, he offered \u201csome 500 species\u201d of mollusc shells to the Smithsonian. After the war, Taylor helped the British in Malaysia to investigate Japanese war crimes against civilians. His work documenting rape, torture and murder may have contributed to his antipathy towards the Japanese people. Never an easy-going person, his experiences at war seem to have wounded him. He failed in a bid to become head of the Kansas museum, and grew increasingly paranoid in daily life. He studied Russian and made inquiries about working for the CIA. Smith, who died in March this year, told  Nature  that Taylor sprinkled flour on the floor of his office to detect trespassers during his absences. \u201cI was wary of him,\u201d said Smith. William Duellman, a herpetologist at the University of Kansas who first met Taylor in 1951, thinks that Taylor's symptoms could today meet the standards of post-traumatic stress disorder. Nevertheless, Taylor kept working. In his later years, he studied a group of poorly known, legless amphibians called caecilians. He published a sprawling, 800-page taxonomic review 8  of them in 1968. \n               On the brink \n             Taylor's herpetological legacy in the Philippines has taken on new importance now that the country has lost more than 95% of its native forest. Species collectors such as Brown know that their work has conservation implications, but there are often differences between scientific studies and conservation classifications. In the late 1990s, for example, the International Union for Conservation of Nature (IUCN) labelled the Polillo Island frog \u2014  Platymantis polillensis , first described by Taylor \u2014 as critically endangered. All but 4 square kilometres of Polillo's forests had been razed for coconut plantations. But in 2004, Brown was listening to his recordings when he noticed that the Polillo frog had a mating call similar to that of a frog that he had collected on Luzon. Brown applied for permission to get genetic samples from Taylor's original collecting site, and confirmed his hunch: the frog is widespread. Last year, he reported 9  that seven frog species once considered vulnerable or endangered by the IUCN are actually widespread on Luzon. The challenge for taxonomists is that although many agree that global biodiversity is in crisis, threat levels are hard to gauge accurately because advocates for every taxon and ecosystem are clamouring for attention and real data are scarce. \u201cGlobal threat assessments for large taxonomic groups is a very inexact science,\u201d says Walter Jetz, an ecologist at Yale University in New Haven, Connecticut. \u201cWe need more boots on the ground.\u201d Brown is sceptical about conservation assessments in general, but one threat to Philippine amphibians does concern him: the chytrid fungus  Batrachochytrium dendrobatidis , which has been linked to the decline or extinction of hundreds of amphibian species around the world (see  Nature   465 , 680\u2013681; 2010 ). In 2009, Brown identified the fungus on five species in the Philippines, and it has since been found on more. The chytrid threat, he says, combined with habitat destruction and climate change, could push Philippine amphibians over the edge. Time is running out to document the biodiversity of the Philippines, but also to determine Taylor's place in history. Brown has found that Taylor's species descriptions, although brief, often zeroed in on the precise trait that set one group apart from its relatives. \u201cHe had a sharp eye,\u201d says Brown. More than a dozen species whose names were erased by Inger and others have proved to be valid after all. Inger, who is 93, is impressed by the emerging evidence and the way that Brown has approached the subject. \u201cI think he's probably right,\u201d he says, but adds, \u201cI'm still a little uneasy about over-fragmentation.\u201d Back at the University of Kansas, Brown takes a seat inside an archival library and dips once more into some of Taylor's work, including the battered leather books that the man used for his field notes and specimen catalogues. Paging through one of those catalogues for the first time, Brown is stunned to find that Taylor had crossed out the name attached to an Asian spadefoot toad that he caught on Mindoro Island \u2014 a strange, gangly creature that crawls rather than hops. Next to it, Taylor had written, \u201cnew sp!!\u201d. As recently as 2009, Brown had designated it as a new species,  Leptobrachium mangyanorum , because it was so different from previously described relatives 10 . \u201cEd was way ahead of us,\u201d says Brown. \u201cWhy he never named it, we'll never know. But it's pretty satisfying to come along 90 to 100 years later and arrive at the same conclusion.\u201d \n                     'Extinct' frog is last survivor of its lineage 2013-Jun-04 \n                   \n                     Trade rules must be tightened to halt frog-killing fungus 2012-Jun-01 \n                   \n                     Ecology: Emergency medicine for frogs 2010-Jun-09 \n                   \n                     Ecology: Wish you were here 2010-Jan-06 \n                   \n                     Bagged and boxed: it's a frog's life 2008-Mar-26 \n                   \n                     Web focus: Linnaeus at 300 \n                   \n                     Edward H. Taylor: Recollections of an Herpetologist \n                   \n                     History of herpetology at the Kansas Museum \n                   Reprints and Permissions"},
{"file_id": "501154a", "url": "https://www.nature.com/articles/501154a", "year": 2013, "authors": [{"name": "Philip Ball"}], "parsed_as_year": "2006_or_before", "body": "Physicists have spent a century puzzling over the paradoxes of quantum theory. Now a few of them are trying to reinvent it. If the truth be told, few physicists have ever really felt comfortable with quantum theory. Having lived with it now for more than a century, they have managed to forge a good working relationship; physicists now routinely use the mathematics of quantum behaviour to make stunningly accurate calculations about molecular structure, high-energy particle collisions, semiconductor behaviour, spectral emissions and much more. But the interactions tend to be strictly formal. As soon as researchers try to get behind the mask and ask what the mathematics mean, they run straight into a seemingly impenetrable wall of paradoxes. Can something really be a particle and a wave at the same time? Is Schr\u00f6dinger's cat really both alive and dead? Is it true that even the gentlest conceivable measurement can somehow have an effect on particles halfway across the Universe? Many physicists respond to this inner weirdness by retreating into the 'Copenhagen interpretation' articulated by Niels Bohr, Werner Heisenberg and their colleagues as they were putting quantum theory into its modern form in the 1920s. The interpretation says that the weirdness reflects fundamental limits on what can be known about the world, and just has to be accepted as the way things are \u2014 or, as famously phrased by physicist David Mermin of Cornell University in Ithaca, New York, \u201cshut up and calculate!\u201d 1 But there have always been some who are not content to shut up \u2014 who are determined to get behind the mask and fathom quantum theory's meaning. \u201cWhat is it about this world that forces us to navigate it with the help of such an abstract entity?\u201d wonders physicist Maximilian Schlosshauer of the University of Portland in Oregon, referring to the uncertainty principle; the wave function that describes the probability of finding a system in various states; and all the other mathematical paraphernalia found in textbooks on quantum theory. Over the past decade or so, a small community of these questioners have begun to argue that the only way forward is to demolish the abstract entity and start again. They are a diverse bunch, each with a different idea of how such a 'quantum reconstruction' should proceed. But they share a conviction that physicists have spent the past century looking at quantum theory from the wrong angle, making its shadow odd, spiky and hard to decode. If they could only find the right perspective, they believe, all would become clear, and long-standing mysteries such as the quantum nature of gravity might resolve themselves in some natural, obvious way \u2014 perhaps as an aspect of some generalized theory of probability. \u201cThe very best quantum-foundational effort,\u201d says Christopher Fuchs of the Perimeter Institute for Theoretical Physics in Waterloo, Canada, \u201cwill be the one that can write a story \u2014 literally a story, all in plain words \u2014 so compelling and so masterful in its imagery that the mathematics of quantum mechanics in all its exact technical detail will fall out as a matter of course\u201d. \n               A very reasonable proposal \n             One of the earliest attempts to tell such a story came in 2001, when Lucien Hardy, then at the University of Oxford, UK, proposed that quantum theory might be derived from a small set of \u201cvery reasonable\u201d axioms about how probabilities can be measured in any system 2 , such as a coin tossed into the air. Hardy began by noting that a classical system can be specified completely by measuring a certain number of 'pure' states, which he denoted  N . For a coin toss, in which the result is always either heads or tails,  N  equals two. For the roll of a dice, whereby the cube must end up with one of six faces uppermost,  N  equals six. Probability works differently in the quantum world, however. Measuring the spin of an electron, for example, can distinguish two pure states, which can be crudely pictured as a rotation clockwise or anticlockwise around, say, a vertical axis. But, unlike in the classical world, the electron's spin is a mixture of the two quantum states before a measurement is made, and that mixture varies along a continuum. Hardy accounted for that through a 'continuity axiom', which demands that pure states transform from one to another in a smooth way. This axiom turns out to imply that at least  N 2  measurements are required to completely specify a system \u2014 a relationship that corresponds to the standard quantum picture. But, in principle, said Hardy, the continuity axiom also allows for higher-order theories in which a complete definition of the system requires  N 3 ,  N 4  or more measurements 3 , resulting in subtle deviations from standard quantum behaviour that might be observable in the lab. He did not attempt to analyse such possibilities in any detail, however; his larger goal was to show how quantum physics might be reframed as a general theory of probability. Conceivably, he says, such a theory could have been derived by nineteenth-century mathematicians without any knowledge of the empirical motivations that led Max Planck and Albert Einstein to initiate quantum mechanics at the start of the twentieth century. Fuchs, for one, found Hardy's paper electrifying. \u201cIt hit me over the head like a hammer and has shaped my thinking ever since,\u201d he says, convincing him to pursue the probability approach wholeheartedly. Fuchs was especially eager to reinterpret the troubling concept of entanglement: a situation in which the quantum states of two or more particles are interdependent, meaning that a measurement of one of them will instantaneously allow the measurer to determine the state of the other. For example, two photons emitted from an atomic nucleus in opposite directions might be entangled so that one is polarized horizontally and the other is polarized vertically. Before any measurement is made, the polarizations of the photons are correlated but not fixed. Once a measurement on one photon is made, however, the other also becomes instantaneously determined \u2014 even if it is already light years away. As Einstein and his co-workers pointed out in 1935, such an instantaneous action over arbitrarily large distances seems to violate the theory of relativity, which holds that nothing can travel faster than light. They argued that this paradox was proof that quantum theory was incomplete. But the other pioneers stood fast. According to Erwin Schr\u00f6dinger, who coined the term 'entanglement', this feature is the essential trait of quantum mechanics, \u201cthe one that enforces its entire departure from classical lines of thought\u201d. Subsequent analysis has resolved the paradox, by showing that measurements of an entangled system cannot actually be used to transmit information faster than light. And experiments on photons in the 1980s showed that entanglements really do work this way. Still, this does seem an odd way for the Universe to behave. And this is what prompted Fuchs to call for a fresh approach to quantum foundations 4 . He rejected the idea, held by many in the field, that wave functions, entanglement and all the rest represent something real out in the world (see  Nature   485 , 157\u2013158; 2012 ). Instead, extending a line of argument that dates back to the Copenhagen interpretation, he insisted that these mathematical constructs are just a way to quantify \u201cobservers'  personal  information, expectations, degrees of belief\u201d 5 . He is encouraged in this view by the work of his Perimeter Institute colleague Robert Spekkens, who carried out a thought experiment asking what physics would look like if nature somehow limited what any observer could know about a system by imposing a \u201cknowledge balance principle\u201d: no observer's information about the system, as measured in bits, can ever exceed the amount of information he or she lacks. Spekkens' calculations show that this principle, arbitrary as it seems, is sufficient to reproduce many of the characteristics of quantum theory, including entanglement 6 . Other kinds of restriction on what can be known about a suite of states have also been shown to produce quantum-like behaviours 7 , 8 . \n               Knowledge gap \n             The lesson, says Fuchs, isn't that Spekkens's model is realistic \u2014 it was never meant to be \u2014 but that entanglement and all the other strange phenomena of quantum theory are not a completely new form of physics. They could just as easily arise from a theory of knowledge and its limits. To get a better sense of how, Fuchs has rewritten standard quantum theory into a form that closely resembles a branch of classical probability theory known as Bayesian inference, which has its roots in the eighteenth century. In the Bayesian view, probabilities aren't intrinsic quantities 'attached' to objects. Rather, they quantify an observer's personal degree of belief of what might happen to the object. Fuchs' quantum Bayesian view, or QBism (pronounced 'cubism') 9 , 10 , is a framework that allows known quantum phenomena to be recovered from new axioms that do not require mathematical constructs such as wavefunctions. QBism is already motivating experimental proposals, he says. Such experiments might reveal, for example, new, deep structures within quantum mechanics that would allow quantum probability laws to be re-expressed as minor variations of standard probability theory 11 . \u201cThat new view, if it proves valid, could change our understanding of how to build quantum computers and other quantum-information kits,\u201d he says, noting that all such applications are critically dependent on the behaviour of quantum probability. Knowledge \u2014 which is typically measured in terms of how many bits of information an observer has about a system \u2014 is the focus of many other approaches to reconstruction, too. As physicists \u010caslav Brukner and Anton Zeilinger of the University of Vienna put it, \u201cquantum physics is an elementary theory of information\u201d 12 . Meanwhile, physicist Marcin Paw\u0142owski at the University of Gda\u0144sk in Poland and his colleagues are exploring a principle they call 'information causality' 13 . This postulate says that if one experimenter (call her Alice) sends  m  bits of information about her data to another observer (Bob), then Bob can gain no more than  m  classical bits of information about that data \u2014 no matter how much he may know about Alice's experiment. Paw\u0142owski and his colleagues have found that this postulate is respected by classical physics and by standard quantum mechanics, but not by alternative theories that allow for stronger forms of entanglement-like correlations between information-carrying particles. For that reason, the group writes in their paper, \u201cinformation causality might be one of the foundational properties of nature\u201d \u2014 in other words, an axiom of some future, reconstructed quantum theory. What is striking about several of these attempts at quantum reconstruction is that they suggest that the set of laws governing our Universe is just one of many mathematical possibilities. \u201cIt turns out that many principles lead to a whole class of probabilistic theories, and not specifically quantum theory,\u201d says Schlosshauer. This is in itself a valuable insight. \u201cA lot of the features we think of as uniquely quantum,\u201d he says, \u201care actually generic to many probabilistic theories. This allows us to focus on the question of what makes quantum theory unique.\u201d \n               Poised for success? \n             Hardy says that the pace of quantum-reconstruction efforts has really picked up during the past few years as investigators begin to sense they are getting some good handles on the issue. \u201cWe're now poised for some really significant breakthroughs,\u201d he says. But how can anyone judge the success of these efforts? Hardy notes that some investigators are looking for experimental signs of the higher-order quantum correlations allowed in his theory. \u201cHowever, I would say that the real criterion for success is more theoretical,\u201d he says. \u201cDo we have a better understanding of quantum theory, and do the axioms give us new ideas as to how to go beyond current-day physics?\u201d He is hopeful that some of these principles might eventually assist in the development of a theory of quantum gravity. There is plenty of room for scepticism. \u201cReconstructing quantum theory from a set of basic principles seems like an idea with the odds greatly against it,\u201d says Daniel Greenberger, a physicist who works on quantum foundations at the City College of New York 5 . Yet Schlosshauer argues that \u201ceven if no single reconstruction program can actually find a universally accepted set of principles that works, it's not a wasted effort, because we will have learned so much along the way\u201d. He is cautiously optimistic. \u201cOnce we have a set of simple and physically intuitive principles, and a convincing story to go with them, quantum mechanics will look a lot less mysterious\u201d, he says. \u201cI think a lot of the outstanding questions will then go away. I'm probably not the only one who would love to be around to witness the discovery of these principles.\u201d \n                     Photons test quantum paradox 2013-Apr-15 \n                   \n                     Experts still split about what quantum theory means 2013-Jan-11 \n                   \n                     A boost for quantum reality 2012-May-08 \n                   \n                     Quantum theorem shakes foundations 2011-Nov-17 \n                   \n                     Physics: Quantum all the way 2008-Apr-30 \n                   \n                     Lucien Hardy \n                   \n                     Christopher Fuchs \n                   \n                     Maximilian Schlosshauer \n                   \n                     Chris Fuchs on QBism \n                   Reprints and Permissions"},
{"file_id": "501300a", "url": "https://www.nature.com/articles/501300a", "year": 2013, "authors": [{"name": "Nicola Jones"}], "parsed_as_year": "2006_or_before", "body": "Researchers struggle to project how fast, how high and how far the oceans will rise. The world's leading climate scientists kicked up a storm in 2007, when they issued their best estimates of how quickly the oceans would swell as the globe warms. The Intergovernmental Panel on Climate Change (IPCC) projected that sea levels would rise by somewhere between 18 and 59 centimetres by the last decade of this century \u2014 an upper limit that seemed far too low to other scientists, given the pace of melting in Greenland and other changes. \u201cWe were hugely criticized for being too conservative,\u201d says Jerry Meehl, a climate modeller at the US National Center for Atmospheric Research in Boulder, Colorado and one of the authors of the IPCC's 2007 report 1 . The panel had previously projected much higher rates of sea-level rise, but its 2007 assessment admitted that it could not tackle the entire problem: the predictions did not include the possibility of rapid changes in ice cover in Greenland or the Antarctic because the authors had concluded that it was impossible to forecast such behaviour with the knowledge and models then available. Yet as early as 2009, it was clear that real sea-level rise was on pace to exceed the 2007 projections 2 . As the IPCC prepares to release its latest summary of climate science next week, researchers say that they now have a better grasp of the problem. Although the final report is not yet complete and the numbers could change, a leaked draft from June forecast a significantly greater rise in sea level \u2014 possibly close to 1 metre by 2100. But there is still huge uncertainty over how fast the oceans will rise, how the pattern will vary around the globe and what the ultimate high-water mark will be. Here,  Nature  investigates some of the big questions remaining about sea-level rise. \n               How fast will it rise? \n             Stefan Rahmstorf, a physical oceanographer at the Potsdam Institute for Climate Impact Research in Germany, is deeply unsatisfied with the standard tools for forecasting sea-level rise: 'process' models that try to represent the physics of every contributing factor. One reason for this discomfort was clear back in 2007. When researchers added up all the individual processes that contributed to rising seas, they could account for only 60% of the observed lift from 1961 to 2003 (see 'Too much water'). \u201cThe whole was bigger than the sum of its parts,\u201d says John Church, co-lead author of the chapter on sea-level rise in the forthcoming IPCC report and an oceanographer at the Australian Commonwealth Scientific and Industrial Research Organisation in Hobart. The two biggest effects \u2014 the expansion of water as it warms, and the addition of water to the oceans from melting glaciers \u2014 each accounted for about one-quarter of the total. A little extra was added in from the melting of the Antarctic and Greenland ice sheets. That left a gaping hole. So Rahmstorf decided to pursue an entirely different type of model. He looked at the annual rate of sea-level rise from the 1880s onwards, and then matched it with air temperatures at those times. He found a simple relationship: the warmer it got, the faster the sea level rose. In 2007, too late to be considered by that year's IPCC assessment, his model predicted 3  up to 1.4 metres of sea-level rise by 2100 \u2014 more than twice the IPCC number. 'Semi-empirical' models such as this have advantages: by definition, they accurately model the rise that has already occurred, and they do not require a full understanding of how and why it is happening. But no one knows how long the relationship at the heart of these models will hold, particularly as melting ice sheets become a bigger factor. The models, says Rahmstorf, \u201ccould be good for 50 years, or 100 years. We don't know.\u201d When it comes to making projections, the choice of models has big consequences. Process models generally predict rather less than 1 metre of rise by 2100, whereas semi-empirical models top out at between 1 and 2 metres \u2014 enough, at the higher end, to flood the homes of 187 million people. These high-end, semi-empirical estimates are extremely controversial, and the IPCC has low confidence in them. \u201cThe only advantage of these models is that they're easy to calculate,\u201d says Philippe Huybrechts, an ice modeller at the Brussels Free University. \u201cI think they're wrong.\u201d Process-based modellers have made great progress since 2007, thanks to improved understanding of factors such as how much heat is flowing into the oceans \u2014 and thus causing the water to expand \u2014 and how much groundwater makes its way into the oceans because of people's unquenchable thirst for fresh water pumped up from below. As a result, modellers can now explain all of the observed rise in sea level, particularly that in recent decades. But that does not guarantee accurate forecasts. Everyone acknowledges that there are still big issues with process-based projections \u2014 in particular, modellers have only a tenuous grasp of how the big ice sheets in Greenland and, especially, the Antarctic might behave and whether they will melt and flow catastrophically into the sea. In all, the ice sheets hold enough water to raise sea levels by more than 65 metres in the long term, compared with as much as 0.4 metres from all the world's glaciers and ice caps. Despite these problems, the IPCC has decided that researchers finally have a good enough handle on ice behaviour in Greenland and \u2014 to a lesser extent \u2014 Antarctica to forecast how ice sheets will respond, at least provisionally, says Don Chambers, a sea-level researcher at the University of Texas at Austin. The latest estimates add between 3 and 21 centimetres to the predicted sea-level rise by 2100, although tens of centimetres more are possible, according to the most recent IPCC report draft. The end result is set to be a much higher forecast for sea-level rise than in 2007. Direct comparisons are difficult because the latest report uses different time frames and emission scenarios, but the leaked draft puts the range of estimates between 28 and 97 centimetres of rise by 2100. That is still not as high as semi-empirical estimates, but process-based results are edging upwards \u2014 and the difference is narrowing. \u201cI consider it something of a vindication,\u201d says Rahmstorf. \n               How much will it vary? \n             When Jeff Freymueller, a geophysicist at the University of Alaska Fairbanks, visited Alaska's Graves Harbor more than a decade ago, his marine charts showed three isolated little islands; what he saw, instead, were three grassy peninsulas connected to the mainland. That was because water levels in some parts of Alaska are dropping \u2014 by up to 3 centimetres per year. The ground there is lifting upwards, in a slow-motion rebound that has been going on for 10,000 years, since the glacial ice sheet that once weighed down the continent receded at the end of the last ice age. Gravitational influences on the oceans are also at work: as local glaciers recede and the Greenland ice sheet melts, their gravitational pull is subtly reduced, allowing more ocean water to slop southwards. Trends in local sea level can differ strongly from the global average, which is increasing by around 3.2 millimetres per year. \u201cSome places, sea-level rise is ten times faster than the average,\u201d says Jerry Mitrovica, a geophysicist at Harvard University in Cambridge, Massachusetts. One side of this equation is the movement of the land. Canada's Hudson Bay, for example, was once buried under more than 3 kilometres of ice, and the release from that load is now causing the land to rise at about 1 centimetre per year. As that part of North America moves upwards, land to the south is being levered down: the US east coast is dropping by millimetres per year. Subsidence can cause some areas to sink much faster. Compaction of river sediments and hollowing out of the earth by groundwater extraction, for example, are causing parts of China's Yellow River delta to sink at up to 25 centimetres per year 4 . Adding to the complexity, the oceans do not rise evenly all over the world as water is poured in. Air pressure, winds and currents can shove water in a given ocean to one side: since 1950, for example, a 1,000-kilometre stretch of the US Atlantic coast north of Cape Hatteras in North Carolina has seen the sea rise at 3\u20134 times the global average rate 5 . In large part, this is because the Gulf Stream and the North Atlantic current, which normally push waters away from that coast, have been weakening, allowing water to slop back onto US shores. Finally, waters near big chunks of land and ice are literally pulled up onto shores by gravity. As ice sheets melt, the gravitational field weakens and alters the sea level. If Greenland melted enough to raise global seas by an average of 1 metre, for example, the gravitational effect would lower water levels near Greenland by 2.5 metres and raise them by as much as 1.3 metres far away.  \u201cSome places, sea-level rise is ten times faster than the average.\u201d  Scientists and engineers are only just starting to wrangle all these effects into local projections. In June, the New York City Panel on Climate Change updated its estimates of sea-level rise by including the local effects of gravitational shifts 6 . Panel members concluded that they expect to see 30\u201360 centimetres of rise by 2050. Finding and combining the right data sets took about six months; the exercise should pave the way for other cities to do the same, says Cynthia Rosenzweig, a climate-impact researcher at NASA's Goddard Institute for Space Studies in New York City. \u201cWe really are working to get the best science.\u201d Aim\u00e9e Slangen, now a member of Church's group in Australia, last year put out one of the first global maps of regional sea-level change that takes all these factors into account, but it had only rough resolution, with pixels of more than 100 kilometres each 7 . Researchers want to provide city-level predictions, but are hampered because these depend heavily on decadal shifts in winds and ocean currents. Predicting such changes is \u201cvery problematic\u201d, says Chambers. Regional figures for sea-level rise are of interest not only to people trying to plan for local impacts, but also to those trying to model global effects. For the latter, the numbers bring some good news. Gravitational shifts caused by melting ice in the Antarctic should actually help to prevent catastrophic collapse of the West Antarctic ice sheet: as Antarctica loses some of its ice, local sea levels will fall, which will cause some floating edges of the ice sheet to come to rest on the sea floor. Firmly grounded ice is less susceptible to runaway melting than floating ice. \u201cThat's going to stabilize the ice sheet,\u201d says Mitrovica. \n               How high will it go? \n             \u201cSea-level rise isn't going to stop in 2100,\u201d says Church. \u201cI think that's something that people don't really take on board.\u201d Eventually, they will. Projections of sea-level rise far into the future jump from tens of centimetres to tens of metres. For the past few years, Maureen Raymo, a marine geologist at the Lamont-Doherty Earth Observatory in Palisades, New York, has traipsed around abandoned diamond mines in South Africa, visited quarries in Australia and examined road cuts on the east coast of North America, looking for shells and other remnants of beaches from 3 million years ago. She hopes to reconstruct sea levels from the Pliocene epoch, the last time when carbon dioxide concentrations were as high as they are today: about 400 parts per million of the atmospheric volume. That, in turn, should provide a glimpse of what the world might look like in thousands of years, once the planet has had time to react fully to today's emissions. Current estimates of sea-level rise in the Pliocene range from very little to 40 metres, says Raymo. \u201cBut that's not very helpful,\u201d she says. The difference between the lower and higher estimates is the difference \u2014 crucially \u2014 between much of the vast East Antarctic ice sheet melting and staying frozen. Whether or not it melted in the Pliocene, in turn, provides insight for modellers who are trying to work out whether \u2014 and how fast \u2014 ice sheets might collapse in the next few hundred years. The trick to pinning this down is not just finding Pliocene beaches, but also working out how the land has moved since they were laid down, as a result of both rebound from the loss of ice sheets and the ongoing movement of mantle rock under the continents. To estimate how such processes have played out over millions of years, researchers rely on models of how much ice covered the continents and how viscous the mantle is \u2014 factors that are subject to extreme debate. \u201cToday's models all assume a viscosity of the mantle that is untestable, highly controversial and differs between groups,\u201d says Raymo. This motion of Earth makes a big difference in estimating past events. Previous work, for example, in Bermuda and the Bahamas hinted that the coastline there was 20 metres higher than it is today during a warm period 400,000 years ago. In 2012, however, Raymo and Mitrovica calculated how the ground there had moved \u2014 and concluded 8  that half of the apparent sea-level rise was attributable not to rising waters but to sinking land, cutting the sea-level-rise estimate in half. Given the large error bars, the only way to pinpoint Pliocene sea levels is to get data from many sites and to calculate one best-fit answer for global sea level. Raymo and her team have so far surveyed thousands of kilometres of coast, gathering evidence from dozens of beach sites. She says she needs perhaps eight more locations and five years to finish the job. But, she admits, whatever she finds will not be a worst-case scenario because greenhouse-gas concentrations are already climbing beyond where they were in the Pliocene. \u201cThe real worst-case scenario is we don't limit fossil-fuel combustion,\u201d she says. \u201cThen it's 'Hello Eocene'\u201d \u2014 returning to a world akin to a warm period 55 million years ago, with maybe just a trace of ice at the poles. Nearly 70 metres of sea-level rise would drown all of Florida and much of Brazil, and swamp the Statue of Liberty up to her waist. But that might not happen until so many thousands of years from now that humanity has time to adapt \u2014 even if that means surrendering much of the land to the waves. Reprints and Permissions"},
{"file_id": "500020a", "url": "https://www.nature.com/articles/500020a", "year": 2013, "authors": [{"name": "Andrew Curry"}], "parsed_as_year": "2006_or_before", "body": "When a single genetic mutation first let ancient Europeans drink milk, it set the stage for a continental upheaval. In the 1970s, archaeologist Peter Bogucki was excavating a Stone Age site in the fertile plains of central Poland when he came across an assortment of odd artefacts. The people who had lived there around 7,000 years ago were among central Europe's first farmers, and they had left behind fragments of pottery dotted with tiny holes. It looked as though the coarse red clay had been baked while pierced with pieces of straw. Looking back through the archaeological literature, Bogucki found other examples of ancient perforated pottery. \u201cThey were so unusual \u2014 people would almost always include them in publications,\u201d says Bogucki, now at Princeton University in New Jersey. He had seen something similar at a friend's house that was used for straining cheese, so he speculated that the pottery might be connected with cheese-making. But he had no way to test his idea. Mark Thomas talks about human evolution and the rise of dairying. The mystery potsherds sat in storage until 2011, when M\u00e9lanie Roffet-Salque pulled them out and analysed fatty residues preserved in the clay. Roffet-Salque, a geochemist at the University of Bristol, UK, found signatures of abundant milk fats \u2014 evidence that the early farmers had used the pottery as sieves to separate fatty milk solids from liquid whey. That makes the Polish relics the oldest known evidence of cheese-making in the world 1 . Roffet-Salque's sleuthing is part of a wave of discoveries about the history of milk in Europe. Many of them have come from a \u20ac3.3-million (US$4.4-million) project that started in 2009 and has involved archaeologists, chemists and geneticists. The findings from this group illuminate the profound ways that dairy products have shaped human settlement on the continent. During the most recent ice age, milk was essentially a toxin to adults because \u2014 unlike children \u2014 they could not produce the lactase enzyme required to break down lactose, the main sugar in milk. But as farming started to replace hunting and gathering in the Middle East around 11,000 years ago, cattle herders learned how to reduce lactose in dairy products to tolerable levels by fermenting milk to make cheese or yogurt. Several thousand years later, a genetic mutation spread through Europe that gave people the ability to produce lactase \u2014 and drink milk \u2014 throughout their lives. That adaptation opened up a rich new source of nutrition that could have sustained communities when harvests failed. This two-step milk revolution may have been a prime factor in allowing bands of farmers and herders from the south to sweep through Europe and displace the hunter-gatherer cultures that had lived there for millennia. \u201cThey spread really rapidly into northern Europe from an archaeological point of view,\u201d says Mark Thomas, a population geneticist at University College London. That wave of emigration left an enduring imprint on Europe, where, unlike in many regions of the world, most people can now tolerate milk. \u201cIt could be that a large proportion of Europeans are descended from the first lactase-persistent dairy farmers in Europe,\u201d says Thomas. \n               Strong stomachs \n             Young children almost universally produce lactase and can digest the lactose in their mother's milk. But as they mature, most switch off the lactase gene. Only 35% of the human population can digest lactose beyond the age of about seven or eight (ref.  2 ). \u201cIf you're lactose intolerant and you drink half a pint of milk, you're going to be really ill. Explosive diarrhoea \u2014 dysentery essentially,\u201d says Oliver Craig, an archaeologist at the University of York, UK. \u201cI'm not saying it's lethal, but it's quite unpleasant.\u201d  Most people who retain the ability to digest milk can trace their ancestry to Europe, where the trait seems to be linked to a single nucleotide in which the DNA base cytosine changed to thymine in a genomic region not far from the lactase gene. There are other pockets of lactase persistence in West Africa (see  Nature   444 , 994\u2013996; 2006 ), the Middle East and south Asia that seem to be linked to separate mutations 3  (see 'Lactase hotspots'). The single-nucleotide switch in Europe happened relatively recently. Thomas and his colleagues estimated the timing by looking at genetic variations in modern populations and running computer simulations of how the related genetic mutation might have spread through ancient populations 4 . They proposed that the trait of lactase persistence, dubbed the LP allele, emerged about 7,500 years ago in the broad, fertile plains of Hungary. \n               Powerful gene \n             Once the LP allele appeared, it offered a major selective advantage. In a 2004 study 5 , researchers estimated that people with the mutation would have produced up to 19% more fertile offspring than those who lacked it. The researchers called that degree of selection \u201camong the strongest yet seen for any gene in the genome\u201d. Compounded over several hundred generations, that advantage could help a population to take over a continent. But only if \u201cthe population has a supply of fresh milk and is dairying\u201d, says Thomas. \u201cIt's gene\u2013culture co-evolution. They feed off of each other.\u201d To investigate the history of that interaction, Thomas teamed up with Joachim Burger, a palaeogeneticist at the Johannes Gutenberg University of Mainz in Germany, and Matthew Collins, a bioarchaeologist at the University of York. They organized a multidisciplinary project called LeCHE (Lactase Persistence in the early Cultural History of Europe), which brought together a dozen early-career researchers from around Europe. By studying human molecular biology and the archaeology and chemistry of ancient pottery, LeCHE participants also hoped to address a key issue about the origins of modern Europeans. \u201cIt's been an enduring question in archaeology \u2014 whether we're descended from Middle Eastern farmers or indigenous hunter-gatherers,\u201d says Thomas. The argument boils down to evolution versus replacement. Did native populations of hunter-gatherers in Europe take up farming and herding? Or was there an influx of agricultural colonists who outcompeted the locals, thanks to a combination of genes and technology? One strand of evidence came from studies of animal bones found at archaeological sites. If cattle are raised primarily for dairying, calves are generally slaughtered before their first birthday so that their mothers can be milked. But cattle raised mainly for meat are killed later, when they have reached their full size. (The pattern, if not the ages, is similar for sheep and goats, which were part of the dairying revolution.)  On the basis of studies of growth patterns in bones, LeCHE participant Jean-Denis Vigne, an archaeozoologist at the French National Museum of Natural History in Paris, suggests that dairying in the Middle East may go all the way back to when humans first started domesticating animals there, about 10,500 years ago 6 . That would place it just after the Middle Eastern Neolithic transition \u2014 when an economy based on hunter-gathering gave way to one devoted to agriculture. Dairying, says Roz Gillis, also an archaeozoologist at the Paris museum, \u201cmay have been one of the reasons why human populations began trapping and keeping ruminants such as cattle, sheep and goats\u201d. (See 'Dairy diaspora'.) Dairying then expanded in concert with the Neolithic transition, says Gillis, who has looked at bone growth at 150 sites in Europe and Anatolia (modern Turkey). As agriculture spread from Anatolia to northern Europe over roughly two millennia, dairying followed a similar pattern.\u00a0 On their own, the growth patterns do not say whether the Neolithic transition in Europe happened through evolution or replacement, but cattle bones offer important clues. In a precursor study 7 , Burger and several other LeCHE participants found that domesticated cattle at Neolithic sites in Europe were most closely related to cows from the Middle East, rather than indigenous wild aurochs. This is a strong indication that incoming herders brought their cattle with them, rather than domesticating locally, says Burger. A similar story is emerging from studies of ancient human DNA recovered at a few sites in central Europe, which suggest that Neolithic farmers were not descended from the hunter-gatherers who lived there before 8 . Taken together, the data help to resolve the origins of the first European farmers. \u201cFor a long time, the mainstream of continental European archaeology said Mesolithic hunter-gatherers developed into Neolithic farmers,\u201d says Burger. \u201cWe basically showed they were completely different.\u201d \n               Milk or meat \n             Given that dairying in the Middle East started thousands of years before the LP allele emerged in Europe, ancient herders must have found ways to reduce lactose concentrations in milk. It seems likely that they did so by making cheese or yogurt. (Fermented cheeses such as feta and cheddar have a small fraction of the lactose found in fresh milk; aged hard cheeses similar to Parmesan have hardly any.) To test that theory, LeCHE researchers ran chemical tests on ancient pottery. The coarse, porous clay contains enough residues for chemists to distinguish what type of fat was absorbed during the cooking process: whether it was from meat or milk, and from ruminants such as cows, sheep and goats or from other animals. \u201cThat gave us a way into saying what types of things were being cooked,\u201d says Richard Evershed, a chemist at the University of Bristol. Evershed and his LeCHE collaborators found milk fat on pottery in the Middle Eastern Fertile Crescent going back at least 8,500 years 9 , and Roffet-Salque's work on the Polish pottery 1  offers clear evidence that herders in Europe were producing cheese to supplement their diets between 6,800 and 7,400 years ago. By then, dairy had become a component of the Neolithic diet, but it was not yet a dominant part of the economy. That next step happened slowly, and it seems to have required the spread of lactase persistence. The LP allele did not become common in the population until some time after it first emerged: Burger has looked for the mutation in samples of ancient human DNA and has found it only as far back as 6,500 years ago in northern Germany. Models created by LeCHE participant Pascale Gerbault, a population geneticist at University College London, explain how the trait might have spread. As Middle Eastern Neolithic cultures moved into Europe, their farming and herding technologies helped them to out-compete the local hunter-gatherers. And as the southerners pushed north, says Gerbault, the LP allele 'surfed' the wave of migration. Lactase persistence had a harder time becoming established in parts of southern Europe, because Neolithic farmers had settled there before the mutation appeared. But as the agricultural society expanded northwards and westwards into new territory, the advantage provided by lactase persistence had a big impact. \u201cAs the population grows quickly at the edge of the wave, the allele can increase in frequency,\u201d says Gerbault. The remnants of that pattern are still visible today. In southern Europe, lactase persistence is relatively rare \u2014 less than 40% in Greece and Turkey. In Britain and Scandinavia, by contrast, more than 90% of adults can digest milk. \n               Cattle conquest \n             By the late Neolithic and early Bronze Age, around 5,000 years ago, the LP allele was prevalent across most of northern and central Europe, and cattle herding had become a dominant part of the culture. \u201cThey discover this way of life, and once they can really get the nutritional benefits they increase or intensify herding as well,\u201d says Burger. Cattle bones represent more than two-thirds of the animal bones in many late Neolithic and early Bronze Age archaeological sites in central and northern Europe. The LeCHE researchers are still puzzling out exactly why the ability to consume milk offered such an advantage in these regions. Thomas suggests that, as people moved north, milk would have been a hedge against famine. Dairy products \u2014 which could be stored for longer in colder climes \u2014 provided rich sources of calories that were independent of growing seasons or bad harvests. Others think that milk may have helped, particularly in the north, because of its relatively high concentration of vitamin D, a nutrient that can help to ward off diseases such as rickets. Humans synthesize vitamin D naturally only when exposed to the sun, which makes it difficult for northerners to make enough during winter months. But lactase persistence also took root in sunny Spain, casting vitamin D's role into doubt. The LeCHE project may offer a model for how archaeological questions can be answered using a variety of disciplines and tools. \u201cThey have got a lot of different tentacles \u2014 archaeology, palaeoanthropology, ancient DNA and modern DNA, chemical analysis \u2014 all focused on one single question,\u201d says Ian Barnes, a palaeogeneticist at Royal Holloway, University of London, who is not involved in the project. \u201cThere are lots of other dietary changes which could be studied in this way.\u201d The approach could, for example, help to tease apart the origins of amylase, an enzyme that helps to break down starch. Researchers have suggested that the development of the enzyme may have followed \u2014 or made possible \u2014 the increasing appetite for grain that accompanied the growth of agriculture. Scientists also want to trace the evolution of alcohol dehydrogenase, which is crucial to the breakdown of alcohol and could reveal the origins of humanity's thirst for drink. Some of the LeCHE participants are now probing further back in time, as part of a project named BEAN (Bridging the European and Anatolian Neolithic), which is looking at how the first farmers and herders made their way into Europe. Burger, Thomas and their BEAN collaborators will be in Turkey this summer, tracing the origins of the Neolithic using computer models and ancient-DNA analysis in the hope of better understanding who the early farmers were, and when they arrived in Europe. Along the way, they will encounter beyaz peynir, a salty sheep's-milk cheese eaten with nearly every Turkish breakfast. It is probably much like the cheese that Neolithic farmers in the region would have eaten some 8,000 years ago \u2014 long before the march of lactase persistence allowed people to drink fresh milk. \n                 Tweet \n                 Facebook \n                 LinkedIn \n                 Weibo \n               \n                     Pottery shards put a date on Africa\u2019s dairying 2012-Jun-20 \n                   \n                     Ancient DNA solves milk mystery 2007-Feb-26 \n                   \n                     Human evolution: How Africa learned to love the cow 2006-Dec-21 \n                   \n                     LeCHE project \n                   \n                     BEAN project \n                   Reprints and Permissions"},
{"file_id": "498021a", "url": "https://www.nature.com/articles/498021a", "year": 2013, "authors": [], "parsed_as_year": "2006_or_before", "body": "One hundred years after Niels Bohr published his model of the atom, a special issue of  Nature  explores its legacy \u2014 and how much there is still to learn about atomic structure. July 1913 saw Danish physicist Niels Bohr publish the first of three papers setting out a radical new view of the nuclear atom. His idea \u2014 a positively charged nucleus ringed by electrons in orbits of discrete energies \u2014 explained the frequencies of light emitted by hydrogen as electrons made leaps between orbits. Quantum rules determined the electrons' energies, preventing the instabilities that had plagued previous mechanical models of atoms. This special issue of  Nature  explores the origin and legacy of Bohr's quantum atom, a model that has resonated ever since. In 1911, Bohr began a postdoctoral year in England that planted the seeds of his thinking. In a Comment on  page 27 , historian John Heilbron relates how letters from Bohr to his brother Harald and to his fianc\u00e9e, Margrethe N\u00f8rlund, published this year, chart the dauntless physicist's work with J. J. Thomson and Ernest Rutherford, and his study of the papers of John William Nicholson, which presaged his breakthrough. The kaleidoscopic nature of the electron is illuminated by physicist Frank Wilczek in a second Comment ( page 31 ). For most practical purposes, electrons behave like simple point particles \u2014 but at high energies, they reveal their constituents in showers of quarks, gluons and neutrinos. Physicists are still striving to understand puzzling manifestations of electrons such as coupled states in superconductors and fragments with fractional charges. Other researchers are testing the limits of the Bohr model by, for example, using powerful X-ray lasers to blast away inner electrons and create 'hollow' atoms. A News Feature explores these and other extreme atoms, including giant, superheavy and antimatter forms ( page 22 ). Such explorations may hit limits on atomic and nuclear size, as two physicists discuss in a News and Views Forum on  page 40 . Wildly courageous and at ease with ambiguity, even Bohr would have struggled to anticipate the impacts of his vision. \n                     Nature special: Bohr's atom \n                   \n                     Nature special: Alan Turing \n                   \n                     Bohr archive \n                   Reprints and Permissions"},
{"file_id": "498022a", "url": "https://www.nature.com/articles/498022a", "year": 2013, "authors": [{"name": "Richard Van Noorden"}], "parsed_as_year": "2006_or_before", "body": "Physicists are stretching, stripping and contorting atoms to new and bizarre limits. One way to obliterate an atom is to shoot it with the planet's most powerful X-ray gun. Linda Young tried that experiment in October 2009, when she was testing the newly opened X-ray free-electron laser at the SLAC National Accelerator Laboratory in Menlo Park, California. A single pulse from the US$420-million machine packs the same energy as all the solar radiation hitting Earth at that moment, but focused down to one square centimetre. \u201cIt will destroy anything you put in front of it,\u201d says Young. When the laser pulse slammed into the neon atoms in that experiment, it made them explode, stripping away each atom's 10 electrons within 100 femtoseconds (1 femtosecond is 10 \u221215  seconds). But it was the manner of this destruction that most interested Young, who heads the X-ray science division at Argonne National Laboratory in Illinois. The X-rays first removed the atom's inner electrons, leaving the outer ones in place. For a brief moment, the neon atoms in the path of the laser became hollow. That exotic form of neon is one of a number of strange species created by physicists intent on contorting atoms. Some teams have inflated atoms to the size of dust particles. Several research collaborations are creating anti-atoms out of antimatter. And others have loaded atomic nuclei with protons and neutrons in the quest to forge new superheavy elements. Some of the experiments aim to investigate atomic structure; others use atoms as the first steps in modelling more complicated systems. They are all descendants of the revolution in atomic theory catalysed by Danish physicist Niels Bohr 100 years ago. But Bohr would have had difficulty imagining how far scientists could go in poking and prodding atoms into such extreme forms. \n               Hollow atoms \n             The atom that Bohr proposed 1  in July 1913 looked like a miniature Solar System, with electrons arranged in concentric orbits around a positively charged nucleus. In Bohr's model, electrons were point-like particles that were quantized, meaning that they could jump from one orbit to another but could not exist in between. The advent of quantum mechanics in the 1920s retained the concept of orbits but re-imagined electrons as spreading everywhere around the nucleus. The location of each electron can be described only in probabilities, in the form of a mathematical wavefunction. Electrons furthest from the nucleus can be kicked free with the least amount of added energy, so are usually the first to be stripped away. Yet X-rays, which pack a concentrated punch, can remove more tightly bound electrons from inner orbits. A medical X-ray takes out just one of those inner electrons before another from an outer shell drops down to fill the space. But the SLAC X-ray laser is in a class by itself. The beam is so intense and focused that every 100-femtosecond pulse sends 100,000 X-ray photons flying past each square \u00e5ngstr\u00f6m of space (1 \u00e5ngstr\u00f6m is 10 \u221210  metres). That allowed Young to blast away all the inner electrons of the neon atoms in her 2009 experiment 2 . When electrons from the outer shells dropped into the abandoned inner shells, the beam soon kicked those out as well. \u201cIf you tune your X-rays properly, you can pick which shell you want to empty out first,\u201d says Young. \u201cBeing able to control the inner-shell dynamics is very cool.\u201d The current record for this kind of atom-hollowing was reported last November 3  by a group at the Center for Free-electron Laser Science in Hamburg, Germany, which used the SLAC laser to strip away, from the inside out, the 36 inner electrons of a 54-electron-strong xenon atom. Young hopes that research on hollow atoms will prove helpful when the laser is ready for one of its intended uses \u2014 creating images of biological molecules such as DNA and proteins by scattering X-rays off their atoms. Those pictures come at a price: the beam quickly destroys the molecules it is imaging. Knowing how hollow atoms form during this process may help researchers to interpret how the scattering pattern changes as a molecule explodes, Young says. Two decades ago, several research groups made hollow atoms using a different process: first stripping almost all of the electrons from atoms, then depositing the resulting highly charged, slow-moving ions onto a surface. When the ions were a few tens of \u00e5ngstr\u00f6ms away from the surface, they attracted electrons from it, creating momentarily hollow atoms with electrons in outer but not inner shells. Those outer electrons then fell inwards, and the hollow atoms expelled a burst of energetic electrons and photons. \u201cA hollow atom is nothing but a fireball of an enormous amount of energy,\u201d says Joachim Burgd\u00f6rfer, a physicist at the Vienna University of Technology, who worked on developing the theory of the process 4 . Several research groups pursued hollow atoms in the late 1980s and 1990s, with some scientists exploring how the burst of photons from their formation might clean surfaces by removing the topmost layers without doing deeper damage. Although that procedure has been patented, it has not captured the attention of industry, says Fritz Aumayr, a physicist at the Vienna University of Technology. The closest it has come to an application so far was in 2008, when researchers invoked the process to explain how heavy ions spewed from the Sun can damage the surfaces of planets such as Mercury 5 . The ions become hollow atoms as they drop onto the planet, and release bursts of energy as they land. This year, Aumayr published a paper 6  showing that the energy expelled from ions dropping onto carbon membranes can create nanoscale pores whose size is controlled by the strength of the ion's charge (that is, how many electrons it was missing). That might be a useful route for making nanosieves for filtering small molecules, he says, or for creating nanopores to pass DNA through for sequencing. \n               Giant atoms \n             From the perspective of an atomic nucleus, all electrons are far-flung voyagers. Whereas a nucleus measures femtometres in diameter, a bound electron typically travels 100,000 nuclear diameters away from the core. But Rydberg atoms, the colossi of the atomic world, have outer electrons so pumped with energy that they can travel 100 billion nuclear diameters \u2014 tens or hundreds of micrometres \u2014 from their nucleus. The largest Rydberg atoms even approach the size of the full stop at the end of this sentence. Named after nineteenth-century Swedish physicist Johannes Rydberg, these giant atoms have been studied extensively since the 1970s, with the introduction of lasers that could excite electrons out to such vast distances. Like any distant traveller, the outer electron in a Rydberg system can be lonely and vulnerable. The attraction to the distant core is faint and easily disturbed by stray electromagnetic fields or collisions, so the atoms must be created in high vacuum. If carefully isolated from outside forces, such inflated atoms can be maintained for anything from a few hundredths of a second up to multiple seconds. For Barry Dunning, a physicist at Rice University in Houston, Texas, the joy of Rydberg atoms is that they give physicists exquisite control over the motion of an electron. That is not possible with normal atoms because the electrons move much too quickly for even the fastest lasers. But the motion of an inflated electron in a Rydberg atom is much slower: it can be controlled with carefully directed nanosecond electric-field pulses, which allow researchers to herd the electron cloud by knocking it back and forth. In 2008, researchers led by Dunning reported 7  that they had managed to squeeze the normally spread-out electron into a tight packet that briefly orbited the nucleus. Last year, they added radio waves that enabled that motion to be maintained indefinitely 8 . \u201cIt only took a century, but we recreated Bohr's atom,\u201d says Dunning proudly. His next idea is to try exciting and controlling two outer electrons at once, creating a system analogous to how Bohr might have pictured helium. This kind of atom-stretching has some potential applications. Two gaseous atoms a few micrometres apart cannot normally affect each other. But inflate one (or both) to a Rydberg state, and the negatively charged electron clouds start to repel each other, distorting the energy levels of the atoms so that they are no longer isolated systems. Mark Saffman, a physicist at the University of Wisconsin-Madison, has used this property to make a quantum logic gate 9  \u2014 a fundamental part of a quantum computer \u2014 with lasers switching on a Rydberg interaction between two atomic quantum bits, or qubits. He and other researchers hope next to add more atoms. A cloud of cold gas atoms might, if suitably excited, create a kind of hovering crystalline array of Rydberg interactions, says Matthew Jones, a physicist at Durham University, UK. That approach might prove a useful model for studying the physics of 'strongly correlated' solid-state systems. These are systems, such as high-temperature superconductors, in which unusual properties emerge because particles interact strongly with their neighbours. An array of Rydberg atoms would not be a perfect model for the messy interactions in real solid-state systems, but the simplicity of the approach is a strength, says Burgd\u00f6rfer. \u201cIt's a wonderful testing ground for probing many of these ideas about how strongly correlated physics actually works,\u201d he says. \n               Antimatter atoms \n             The Large Hadron Collider at CERN, Europe's particle-physics lab near Geneva, Switzerland, currently lies in pieces, with engineers working on boosting its power. At the same time, in a side hall, an upgrade is taking place to an experiment that may allow physicists to measure the properties of atoms of antimatter. It is a goal that researchers have been chasing since the first antihydrogen atoms were made at CERN in 1995. An antihydrogen atom consists of an antiproton and a positron, which respectively have the same mass as an ordinary proton and electron, but opposite charge. Beyond that, researchers know very little about antihydrogen. \u201cDo matter and antimatter atoms obey the same laws of physics?\u201d asks Jeffrey Hangst, spokesman for ALPHA, one of the collaborative efforts to make and analyse antihydrogen. The experiments at CERN might also help to explain why there is more matter than antimatter in the visible Universe. The Big Bang should have created equal amounts of the two that would have annihilated on contact. But somehow, matter gained an advantage. Differences have been observed between the behaviour of some matter and antimatter particles, such as kaons and mesons, but these are far too small to explain the Big Bang conundrum. To create antihydrogen atoms, researchers at CERN first make antiprotons by bombarding atoms with accelerated protons, then slow them down by passing them through metallic foil, cool them with cold electrons and trap them with electromagnetic fields. A similar trap accumulates positrons that are emitted by radioactive materials. When the clouds of charged particles are mixed, they make neutral antimatter atoms. But because these have no overall charge, in early experiments they easily escaped the electromagnetic fields used to trap the charged antimatter particles. By 2002, two collaborations had been able to make as many as 50,000 atoms of antihydrogen, but the atoms quickly annihilated on the walls of their container. It took until 2010 before researchers at ALPHA showed 10  how to trap the atoms using three magnets with a combined field sufficient to restrain antihydrogen, with its tiny magnetic moment. At that time, the antimatter was held for just 170 milliseconds, and only about one atom was trapped for every eight times the group ran the 20\u201330 minute experiment, says Hangst. But the team has improved its equipment to trap one atom per experiment, and hold it for about 1,000 seconds. ALPHA is now trying to probe the properties of the anti-atoms. This year, the team reported 11  watching the tracks of hundreds of antihydrogen atoms after they were released from their magnetic cage, to test whether antimatter falls up or down under gravity. The researchers do not yet have an answer, but the experiment works in principle, says Hangst. And in the upgrade, the team is moving in some lasers, with the idea of testing next year whether antihydrogen absorbs and emits light at the same frequencies as hydrogen. Other teams at CERN are experimenting with different aspects of antimatter, such as how antihydrogen responds to changing magnetic fields. And researchers elsewhere are looking at even more exotic atoms: Ryugo Hayano, a physicist at the University of Tokyo, leads a team studying mixed matter\u2013antimatter atoms, such as antiprotonic helium, in which a helium nucleus is surrounded by one electron and one negatively charged antiproton, an arrangement that lasts for a few microseconds. In the end, such experiments may not find differences between matter and antimatter that are big enough to explain why the former has prevailed over the latter. But, says Hangst, \u201cone never knows where the new physics might show up. You just have to keep looking.\u201d \n               Heavy atoms \n             Anti-atoms are rare, but researchers working with them are swimming in data compared with those chasing superheavy atoms. In an experiment that required prodigious patience, researchers at the GSI Helmholtz Centre for Heavy Ion Research in Darmstadt, Germany, spent almost five months last year firing titanium-50 ions \u2014 each with 22 protons and 28 neutrons \u2014 into a berkelium-249 target at the rate of about 5 trillion particles per second. The hope was that, just once or twice, two atoms would fuse to make an element with 119 protons, more than any created before. Smashing beams of heavy atoms together has served physicists well over the past 70 years, allowing them to create increasingly heavy agglomerations of protons and neutrons, and to expand the periodic table far beyond the heaviest naturally occurring elements. The confirmed record-holder is element 116, livermorium, with 116 protons and, depending on the isotope, between 174 and 177 neutrons. There have been claims to elements 117 and 118 too, but these have not been officially confirmed. And so far, \u201cnone of the current experiments have reported finding 119 or 120\u201d, says Christoph D\u00fcllmann, spokesman for the GSI-led collaboration \u2014 although he adds that his own team's analysis of last year's work is not quite complete. There is a strong sense that the quest is coming to a dead end. The chance that nuclei will fuse decreases as they get heavier, because the protons and neutrons resist sticking together. Most researchers agree that beyond 120, the chance of getting two nuclei to fuse directly is vanishingly small. \u201cSo this leaves us with the question,\u201d says D\u00fcllmann, \u201cwhat do we do next?\u201d To answer that requires an understanding of what motivates the superheavy search. Curiosity and national pride undoubtedly have a role, with politicians and scientists both looking to stamp their country's name into a new box on the periodic table. But each superheavy element is extremely short-lived, splintering in milliseconds. Theorists have posited that some superheavy combinations of protons and neutrons will turn out to be stable for seconds, minutes or even days. This fabled 'island of stability' is thought to exist at between 114 and 126 protons, and around 184 neutrons. It is now clear that any attempt to make new superheavy elements by smashing a light particle into a heavier one will not reach the island: the isotopes spat out have too few neutrons. So researchers are changing tactics by trying to make heavier isotopes of elements that have already been created. That is what scientists will attempt next year at the Joint Institute for Nuclear Research in Dubna, Russia. They plan to make neutron-rich isotopes of element 118 by firing beams of calcium-48 into radioactive californium-251. The Russian team and others also want go back to the elements already made and create hundreds or thousands of atoms, rather than the handful necessary to claim a discovery. \u201cWe should set ourselves the goal of making not one or two atoms, but macroscopic quantities that we can use to study chemistry and nuclear structure in much greater detail,\u201d says Rolf-Dietmar Herzberg, a physicist at the University of Liverpool, UK. That might allow theorists to make more accurate predictions about where the island of stability lies. But the temptation to expand the periodic table is strong. Researchers will probably turn away from head-on collisions and instead try knocking two heavy nuclei together in a glancing blow, which may stand a better chance of successfully fusing them to create new elements. Physicists have a history of surprising themselves in their quest to create ever heavier atoms. In the early 1990s, no one thought that they could get past element 112 and then a tweak to the fusion process made it possible, says GSI team member Michael Block. \u201cThe next element is always the hardest.\u201d \n                     Element 113 at last? 2012-Sep-27 \n                   \n                     Stars draw atoms closer together 2012-Jul-19 \n                   \n                     Antimatter trapped for more than 15 minutes 2011-Jun-06 \n                   \n                     X-ray free-electron lasers fire up 2009-Oct-07 \n                   \n                     Nature special: Bohr's atom \n                   \n                     Nature special: Alan Turing \n                   \n                     ALPHA experiment at CERN \n                   \n                     Search for superheavy elements at the GSI \n                   Reprints and Permissions"},
{"file_id": "498152a", "url": "https://www.nature.com/articles/498152a", "year": 2013, "authors": [{"name": "Zeeya Merali"}], "parsed_as_year": "2006_or_before", "body": "The launch of several science mega-prizes is making some researchers millionaires \u2014 but others question whether such awards are the best way to promote their field. As reactions to winning a multimillion-dollar prize go, Alexander Polyakov's words were less than gushing. It was the culmination of a glittering award ceremony in Geneva, Switzerland, in March, hosted by Hollywood actor Morgan Freeman and featuring an operatic interlude from British singer Sarah Brightman. After a hushed pause, the physicist from Princeton University, New Jersey, was revealed as the winner of the 2013 Fundamental Physics Prize and an accompanying payment of US$3 million. \u201cThis new prize is an interesting experiment,\u201d a flustered Polyakov said moments later, after walking off stage clutching his sculptured silver trophy. \u201cSuch big prizes could become very influential and they can have a positive impact, or they can be very dangerous.\u201d Polyakov's ambivalence echoes the sentiments of many scientists towards the rash of big-money science prizes that have emerged over the past year. Sponsored by billionaire entrepreneurs, including Russian Internet mogul Yuri Milner, Facebook supremo Mark Zuckerberg, Google co-founder Sergey Brin and property developer Samuel Yin, the new awards outstrip the $1.2-million Nobel prizes in monetary value. In addition to Milner's Fundamental Physics Prize, the Internet billionaires have together created the Breakthrough Prize in Life Sciences, and Yin has introduced the Tang Prize as an Asian complement to the Nobels. The founders of these 'new Nobels' hope that the winners will act as role models, inspiring future generations to pursue science, and that they will attract status and funding to the entire discipline. \u201cWe wanted to choose an amount that would be shocking,\u201d says Anne Wojcicki, a biotechnology analyst and Brin's wife, who sits on the board of the Breakthrough Prize. \u201cWe wanted to create science superheroes.\u201d But the lavishness and ambition of the prizes have sparked criticism. \u201cI don't want to run these awards down, but I find it offensive that people are trying to either borrow the prestige of the Nobel, or buy it,\u201d says Frank Wilczek, a physicist at the Massachusetts Institute of Technology in Cambridge, who won a share of the Nobel Prize in Physics in 2004. \u201cThe suspicion is that these provide more benefit to the egos of the founders than to science,\u201d adds Jack Stilgoe, a lecturer in science policy at University College London. And although they support the goals of the prizes, critics say that the strategy for achieving them is at best misguided, and at worst, could backfire. By bestowing riches on a few individuals, they say, the prizes could funnel money and attention towards people and fields that are already prestigious and well funded or, in some cases, could reward weak scientists or untested ideas. \u201cPrizes are a good thing, but the question is, if your goal is to help science, are large prizes the most efficient way to do that?\u201d asks Wilczek. \n               Towards the next generation \n             First awarded in 1901, the Nobel prizes have become established as the benchmark of excellence in the sciences. Since then, other awards have sprung up and gained prestige within specific disciplines. Some, such as the Fields Medal and the Abel Prize for mathematics, were designed to reward achievements in disciplines that are not covered by the Nobels. Others, such as the Lasker Awards for medical sciences, have gained a reputation for predicting future Nobel winners. The Fundamental Physics Prize was the first of the latest breed of awards (see 'Follow the money'). It burst on the scene in July 2012, when Milner announced that he had given nine awards of $3 million each to prominent theoretical physicists, and that he planned to sponsor one additional award each year. (Polyakov was the first single-award winner.) Milner, who himself pursued graduate studies in theoretical physics, says that he wants to show that foundational research can be as financially rewarding as careers in sports, entertainment or business; indeed, he chose the size of the prize to mirror the type of annual earnings seen in the financial world. \u201cThe best minds should make at least as much as any trader on Wall Street,\u201d Milner says. \n               boxed-text \n             In January, Yin launched the Tang Prize, four awards of 40 million Taiwanese dollars (US$1.3 million) for the winners, plus a grant of 10 million Taiwanese dollars each for their research. The Tang Prize will be awarded every two years from 2014 onwards and will recognize advances in sustainable development, biopharmaceutical science, Chinese studies and law. \u201cFor the past 100 years, it is mainly Western countries and Western research institutions that have fostered talented researchers,\u201d Yin says. \u201cNow, as Asian economies are developing well, we should also shoulder part of the responsibility and contribute to world development.\u201d The Breakthrough Prize in Life Sciences, announced in February, also originates with Milner \u2014 but this time, he brought in friends and colleagues including Zuckerberg, Wojcicki and Brin. They split $33 million equally between each of 11 laureates, and have committed to giving five new prizes of $3 million each year. \u201cWe all have a background in science, though we weren't all the best students,\u201d says Wojcicki. \u201cThis is a way for us to reconnect with that and to give something back.\u201d March saw the inaugural award of the United Kingdom's \u00a31-million (US$1.5-million) Queen Elizabeth Prize for Engineering, which is supported by charitable donations from corporate sponsors and was set up by the Royal Academy of Engineering explicitly to give engineers a taste of the glamour and recognition that comes with the Nobels. After the initial surprise at the big sums involved, the first question in most people's minds was how the winners would spend their cash. \u201cI really admire these billionaires for wanting to give back to science \u2014 but I do hope some of these large amounts goes into research,\u201d says geneticist and entrepreneur Craig Venter, of the J. Craig Venter Institute in San Diego, California. \u201cIt's not so great if the winners just go and buy a bigger house with their prize money.\u201d Many of the winners seem a little sheepish about their windfall; those who are willing to be interviewed tend to mumble that they have not yet decided what proportion to keep and what to give to research. One person who has decided is Tejinder Virdee, a particle physicist at Imperial College London, who in December 2012 shared a $3 million 'special' Fundamental Physics Prize with six other leaders of the hunt for the Higgs boson at the Large Hadron Collider (LHC) at CERN, Europe's particle-physics lab near Geneva. (The special prizes were in addition to the nine prizes Milner had previously announced.) Virdee plans to use his winnings to pay for science equipment in schools in sub-Saharan Africa and to support an exchange programme to bring teachers from these schools to visit the LHC. \u201cI wanted to find the way to get the most leverage out of the money,\u201d he says. \u201cIt costs relatively little to train a teacher, but in turn they could reach 500 students in the next few years.\u201d Even if winners invest in their work, some researchers worry that the prize money will largely reward those who already have ample funding and recognition. Although the physics prizewinners do not include any Nobel laureates, between them they have won pretty much every other major award, including the Wolf Prize in Physics, the Fields Medal for mathematics and the MacArthur 'genius grant'. \u201cThese are not underfunded or unappreciated people,\u201d says Peter Woit, a mathematician at Columbia University in New York. Many of the Breakthrough Prize winners are already regarded as shoo-ins for future Nobels; one, Shinya Yamanaka, a stem-cell biologist at Kyoto University in Japan, already won a share of a Nobel last year. This means that the prizes could end up increasing the divide between the scientific haves and have-nots. \u201cThere's a huge disparity between the money that the big-wigs are getting and that going to other scientists,\u201d says Bob O'Hara, a biostatistician at the Biodiversity and Climate Research Centre in Frankfurt, Germany. \u201cUS$33 million could fund my whole institute for three to four years.\u201d O'Hara and other researchers also complain that the Breakthrough Prize recognizes the same popular fields as the Nobels. \u201cOne frustration for biologists is that the Nobel prize is focused on physiology and medicine and so it neglects other areas of the life sciences, which are just as important,\u201d O'Hara says. The Breakthrough Prize does little to redress the balance, overlooking areas such as ecology and evolutionary biology in favour of research into molecular biology and disease. What is more, O'Hara says, \u201cthere was an emphasis on diseases of the rich, in the West, at the expense of diseases that are prevalent in the developing world\u201d. Wojcicki notes that there is a catch-22: awards that recognize the most extraordinary scientists will reflect existing trends in funding, because popular areas will provide a bigger pool of candidates from which to choose. But, she says, \u201cI do agree that we should use the awards to drive change.\u201d \n               Expensive gamble \n             Although the Breakthrough Prize has been censured for playing it safe, critics are arguing that the Fundamental Physics Prize is making choices that are too risky. Woit notes that five out of the nine initial physics recipients \u2014 four of whom are based at the Institute of Advanced Study in Princeton, New Jersey \u2014 study string theory, the idea that elementary particles are composed of vibrating loops of energy. A vocal critic of string theory, Woit has long argued that this research area has attracted a disproportionate amount of funding, despite lying beyond the range of direct experimental testability. \u201cThe obvious danger is that you may be giving a large award to someone for a completely wrong idea,\u201d he says. Polyakov, whose own award was partly for contributing mathematical techniques to string theory, sees this willingness to gamble on ideas as the new prize's niche. \u201cFor me, ideas have their own reality,\u201d he says. But because in future the Fundamental Physics and Breakthrough prizes will be awarded by committees made up of the previous laureates, critics fear that current biases will be reinforced. Woit points to this year's selection of Polyakov. \u201cWhen string theorists at the Institute of Advanced Study give their first award to a colleague who works on the same stuff as them, then it is a serious problem,\u201d he says. Milner counters that next year the winners of the 'special' prizes \u2014 the seven Higgs hunters and physicist Stephen Hawking \u2014 will shift the balance on the judging panel. \u201cIt is a self-correcting loop,\u201d he says. \n               Putting on the ritz \n             The physics prize's black-tie ceremony in Geneva, consciously modelled on the Oscars, highlighted the ambition of its founders to inspire current and future scientists. \u201cI don't see why millions shouldn't ultimately watch this ceremony,\u201d says Milner. Scientists in the audience were both entertained and bemused; one described it as \u201clots of fun\u201d, another as \u201cexcruciatingly long\u201d. The question is, will the money and razzle-dazzle have any real impact? Stilgoe challenges Milner's claim that the awards will encourage early career scientists to stay the course, rather than \u2014 like Milner, Zuckerberg and Brin \u2014 leaving for more lucrative pastures. \u201cThe idea that anyone would make a career choice based on the minuscule chance of winning, say, a Nobel, is ridiculous,\u201d he says. \u201cScientists, on the whole, are not in it for the money \u2014 and I am not sure we should want them to be.\u201d  I find it offensive that people are trying to either borrow the prestige of the Nobel, or buy it.  Fred Cooper, a physicist and a visiting scholar at Harvard University in Cambridge, Massachusetts, also questions whether the awards will really speak to the public. \u201cVisit YouTube and you'll see that the public already turns to science celebrities \u2014 Michio Kaku, Brian Greene and Sean Carroll \u2014 to learn about physics, not to the winners of the Fundamental Physics Prize,\u201d he says. \u201cIf outreach is your aim, then give money to those that are already great communicators.\u201d Even if the awards do inspire young people, Stilgoe argues that they send out the wrong message. \u201cThe prizes reinforce the mythology of science in which lone geniuses come up with brilliant ideas on their own,\u201d he says. And some say that the prize money would be better used to drive research directly. In 2011, for example, Venter joined forces with the X Prize Foundation and health-care firm Medco Health Solutions, based in Northampton, UK, to offer a US$10-million prize to the first team to sequence accurately the genomes of 100 centenarians. \u201cI'm always more excited by awards that push or drive innovation, rather than ones that just recognize past achievements,\u201d Venter says. Many researchers favour the idea of targeting awards at promising scientists early in their careers. \u201cA small award at this stage is a fantastic idea,\u201d says Cooper. At this point, scientists are in a vulnerable position, struggling to win grants and often supporting young families. \u201cIt will just free scientists up to do more research \u2014 it's about getting the biggest bang for your buck,\u201d Cooper says. This month, billionaire investor Len Blavatnik launched an award with prizes of $250,000 for young scientists. The scheme, which builds on a previous, regional version that has been running since 2007, will be administered by the New York Academy of Sciences. ( Nature Publishing Group , Wilczek and Venter are among the advisers for the award.) Blavatnik says that he saw a gap in the prizes market when he attended a Nobel ceremony several years ago. \u201cWhat struck me was that most of the laureates were quite old and they received the award for something they did 30 or 40 years prior to that,\u201d he recalls. \u201cI thought, in terms of impact on the world, it would be good to award young people and create something that would allow them to thrive.\u201d One of the first winners of the regional Blavatnik Awards, Ruslan Medzhitov, an immunobiologist at the Yale School of Medicine in New Haven, Connecticut, says that the honour enabled him to attract funding and more prizes, including a share of the $1-million Shaw Prize in Life Science and Medicine. As for the new breed of mega-prizes, even some of the critics acknowledge \u2014 with a laugh \u2014 that they would accept one if it were offered to them. And Hans Clevers, a molecular geneticist at the Hubrecht Institute in Utrecht in the Netherlands and one of the inaugural Breakthrough Prize winners, says that the new Nobels could rival the prestige of the old ones in 30 years or so, if they can consistently identify high-calibre winners. The organizers of the Nobel prizes, however, remain unruffled by the upstarts. \u201cFor us, the important issue is to continue the good history and track record that we have,\u201d says Lars Heikensten, executive director of the Nobel prizes, based in Stockholm. \u201cIf we fail, it will be because we fail to maintain that level of respect, not because other prizes are acting as rivals to us. We've been in this business for 110 years and we plan to be in it forever.\u201d \n                 See Editorial \n                 page 138 \n               Reprints and Permissions"},
{"file_id": "498156a", "url": "https://www.nature.com/articles/498156a", "year": 2013, "authors": [{"name": "Emma Marris"}], "parsed_as_year": "2006_or_before", "body": "Unmanned aerial vehicles are poised to take off as popular tools for scientific research. The Tempest \u2014 wingspan 3.2 metres, cruising speed 75 knots \u2014 was designed to fly into severe storms. But during a test run in March for a new project, it is soaring through the bluest of skies. On the ground below, PhD student Maciej Stachura of the University of Colorado (UC), Boulder, is tapping on a tablet computer, transferring control to the aircraft's own computer after a manual take-off. Systems engineer James Mack keeps his hands loose around a controller in case a problem arises, while Neeti Wagle, another PhD student, scans the skies to make sure the Tempest does not collide with anything. The plane's job today is to locate a beacon sending out a simulated distress signal. As it circles overhead, the Tempest's gas-powered engine makes the distinctive lawnmower-like noise that calls to mind the informal name often given to such aircraft: drones. Unmanned aerial vehicle (UAV) is more commonly used in scientific circles. The UC Boulder team watches and listens as the 40 minutes or so of flight time tick by and the Tempest becomes a distant speck in the bright sky. Then a note of concern enters Stachura's voice. \u201cIt is not doing a great job. It should be getting closer to us at this point,\u201d he says. Finally, the drone turns and heads back towards the beacon. \u201cOh, there it goes,\u201d says Stachura, clearly relieved. The use of drones in science has taken a similarly roundabout route. NASA first experimented with custom-built UAVs in high-altitude research during the 1970s, but unmanned planes have been slow to catch on. Drones with top-notch sensors were too expensive to tempt researchers and cheap versions could not offer much of value. During the past decade, however, lower prices and technical advances \u2014 from on-board navigation using the Global Positioning System (GPS) to miniaturization of autopilots \u2014 have lured many scientific groups to experiment with UAVs. Already, they offer an efficient way to gather data and are making important advances in polar research, volcano studies and wildlife biology. \u201cThey are on their way to becoming this indispensable and revolutionary technology,\u201d says Adam Watts, an ecologist at the University of Florida in Gainesville who has flown drones for years. But technical and legal hurdles stand in the way of their wider use. Researchers are trying to improve the autonomy, manoeuvrability and endurance of UAVs. And regulations, particularly in the United States, place strict limits on where and how researchers can use the devices. If these rules loosen up \u2014 and there are signs that they may \u2014 flying science robots may start taking to the skies in much greater numbers. \n               Lofty heights \n             The drones used by military forces to hunt down enemies have attracted growing scrutiny in recent years, but some of them have also been used for science. NASA has conducted hurricane and climate studies with Northrop Grumman's Global Hawk, which can reach an altitude of nearly 20 kilometres \u2014 much higher than commercial planes fly. The agency got the drone for free from the Air Force, but interested scientists must be prepared to pay US$20 million for such a craft \u2014 no sensors included. Most researchers have to make do with much smaller and cheaper systems. A radio-controlled fixed-wing UAV such as the Tempest can be bought off the shelf for a few thousand dollars. And quadrotor helicopters can be purchased for just $300. Slap on a few sensors, an autopilot and a cheap computer preloaded with algorithms, and researchers have an unmanned aerial system (UAS). Despite the differences in equipment, military and civilian drone-research programmes have been closely linked, with advances flowing between the two sides. Many university UAV programmes are, in fact, part-funded by the military. For now, most researchers working with drones are focused on improving the technology to make the devices more agile, more autonomous and better able to work in groups. Autonomy requires a suite of algorithms to interpret data from sensors, make decisions about where to fly, control the plane's path and classify objects captured by the UAV's cameras. And all of that computing has to happen in real time on tiny, light computers bouncing around in three-dimensional space. One area of focus is vision-based navigation. Systems that rely on the GPS can achieve little better than 3-metre resolution at best \u2014 fine for open outdoor landscapes, but not good enough for urban areas or indoor settings. Drone developers would like to send their machines into earthquake-damaged buildings to look for survivors, which would mean avoiding errant beams, power lines and closed windows. To do this, an aircraft requires a complex system of cameras, gyroscopes and accelerometers to figure out where it is \u2014 and where the obstacles are. A team led by Ashutosh Natraj, now at the University of Oxford, UK, has taught drones with fish-eye cameras how to 'find' themselves. The robots' algorithms divide up the circular visual field into sky and ground, identify a horizon line between the two and then derive the drone's altitude and orientation. For city flying, the team is writing algorithms that recognize and use the verticals and horizontals of buildings and streets as guides to navigate up and down, forwards and backwards. At night, the drone could project a laser pattern onto its surroundings to orient itself. Camera-based navigation is smart, says Natraj, because a single camera collects more quality information than a number of expensive, heavy sensors such as laser range-finders, obviating the need to integrate many sensors. As part of a three-year project to design UAVs that can deliver medical care after natural disasters, Natraj is developing systems to do all the image processing on-board the helicopter, rather than through a wireless connection to a separate computer. The Oxford disaster-relief UAV project is hoping to develop multiple UAVs that talk to one another. Such research on swarms is a hot area, says Hyunchul Shim, director of the Center of Field Robotics for Innovation, Exploration and Defense at the Korea Advanced Institute of Science and Technology in Daejeon, South Korea. \u201cIf you go fast, go alone. If you go long, go together,\u201d he says. Data collection and search-and-rescue missions are faster and more efficient with a team of drones to pool data and provide redundancy in case some machines fail. But the use of more vehicles also adds complexity. Drones working together have to be able to communicate with one another and make collective decisions. Researchers are also focusing on increasing the endurance of UAVs, most of which are fuelled by gas engines and batteries. To keep the weight and costs down, researchers often use tiny drones with limited fuel capacity, which means short flights. Some groups are working on miniaturizing batteries, others on making the planes smart enough to take advantage of thermal updrafts and wind features, as birds and gliders do. Roland Siegwart, head of the Autonomous Systems Lab at the Swiss Federal Institute of Technology Zurich, has a team developing solar-powered planes that would never have to land. \u201cI call them 'low-flying satellites',\u201d he jokes. They could actually work better than satellites for collecting data, because researchers could direct them. \u201cYou can have an up-to-date image of bush fires, move them over illegal logging operations or look for people lost in the ocean,\u201d Siegwart says. \n               Video stars \n             Teams working on UAVs tend to keep abreast of each other's work through videos posted online. The field's biggest YouTube 'star' is Vijay Kumar at the University of Pennsylvania in Philadelphia. Kumar's group controls quadrotor helicopters indoors with a modified Vicon system \u2014 the motion-capture system used in Hollywood and by the video-game industry. His videos show drones flying in tight formation, transporting two-by-fours, and even \u2014 in one video with more than 3 million views \u2014 performing the James Bond theme on multiple instruments. \u201cThe Internet has changed the rules,\u201d says Shim. And, Siegwart says, \u201cIt also spreads the information a little further, which helps attract good students.\u201d With new talent helping to make drones smarter and cheaper, the regulations that control unmanned flight will be the biggest barrier to their expanded use in research. \u201cThis is still the major issue,\u201d says Siegwart. That is particularly true in the United States, where Federal Aviation Administration (FAA) rules make it laborious to get permission to fly drones outside (except for non-commercial hobbyists, for whom the rules are looser). \u201cWe need permission to go out into a field on campus and fly something that is six inches off the ground,\u201d grouses Eric Frew, head of the Research and Engineering Center for Unmanned Vehicles at UC Boulder. \u201cIt is a one-size-fits-all approach.\u201d The FAA, based in Washington DC, requires that would-be drone operators apply for and receive one of two certificates for their research programme if they want to fly their UAVs outdoors. The applications request a lot of information, \u201cso the FAA can determine if the operation can be conducted without hazard to other aircraft or people and property on the ground\u201d, according to the agency's communications office. This means that certifications are not granted for flights in cities and other crowded areas. These certifications are also limited to a 20-mile-square area (around 32 square kilometres), so when the UC Boulder team took Tempest storm-chasing across swathes of the country, they needed 59 separate permissions. Once a certification is obtained, usually within 60 days, a group can fly its aircraft during daylight hours in the designated spot for a year or two as long as they file a NOTAM \u2014 a Notice to Airmen \u2014 in advance with the FAA every time they want to fly. Each flight also requires a certified pilot. During the March test flight, that was Stachura, who spent most of the test staring into the plane's controls on his tablet. The FAA also requires that an observer be on hand to watch for potential collisions and that someone be monitoring the radio from the local airport. \n               Flight delays \n             Eric Johnson, who studies UAVs at the University of Georgia in Athens, has looked at regulations around the world and says that \u201camong NATO countries, the United States is about the worst\u201d. But as long as there are no accidents, the consensus seems to be that the regulations will loosen. The FAA Modernization and Reform Act, which passed last year, calls for the US Department of Transportation to produce a plan by late 2015 for \u201cthe safe integration of civil unmanned aircraft systems into the national airspace system\u201d. By contrast, says Johnson, Australia and Canada allow the most types of operation, perhaps because both countries have a lot of airspace and smallish bureaucracies. Salah Sukkarieh, who studies robotics and intelligent systems at the University of Sydney in Australia says that the country's liberal regulations are allowing the UAS field to grow there, despite its funding being a fraction of that available to US scientists. Although most drone research has focused on improving the UAVs themselves, some scientists have been putting the devices to use. In March, NASA used a small electric military drone, the Dragon Eye, to sample and photograph the noxious gas plume spewing from Turrialba Volcano near San Jose in Costa Rica. The team compared the Dragon Eye's measurements of sulphur dioxide to those made by the Terra satellite in an effort to calibrate the space-based readings. It would have been too risky to send a human pilot near the volcano, where there are strong updrafts and ash that could choke a plane's engines. James Maslanik, a remote-sensing expert at UC Boulder, has been involved in a number of studies using drones to measure various qualities of sea ice in the polar regions since 2000. Here, too, UAVs can venture into regions too dangerous for a manned aircraft. In the Arctic, Maslanik says, \u201cwe are flying these things at 100 feet off the ice, wind is 80 knots, temperature of minus 40 \u00b0C.\u201d At the opposite end of Earth, researchers from UC Boulder have used UAVs to measure jets of wind that scream down from the Antarctic plateau into Terra Nova Bay. Such measurements could help scientists to understand the dynamics of sea-ice formation around Antarctica, which creates dense salty water that sinks and helps to drive global ocean currents. \u201cNobody had an aircraft out there during winter when the winds are strongest and took measurements because the conditions are too extreme,\u201d says Maslanik. The data collected so far, he says, show unexpectedly complex wind patterns, including fierce, localized jets that push sea ice off shore and speed up its formation. Biologists are also starting to use UAVs in their field work. In India, the conservation group WWF is using drones to look for rhino poachers. Tom McKinnon, a retired engineer and managing director of InventWorks, a product-development firm in Boulder, is outfitting autonomous helicopters with nets to capture rare Mongolian vultures so that scientists can attach transmitters and study their movements. On the plant side, Sukkarieh has developed a system using a fixed-wing UAV and a helicopter in tandem to locate weeds in remote rangelands and spray them with herbicide. And several groups are teaching drones how to tell one kind of plant from another, so that they can make maps of vegetation. Rather than purchase advanced sensors, which add weight and increase costs, Sukkarieh's team is writing code to allow the UAV to map and classify vegetation using just GPS, a camera and an inertial measuring unit, which collects data on the position of the aircraft in space. The challenge of making trade-offs between sensors and weight has prompted Sukkarieh to think about designing UAV systems from scratch around their specific tasks, rather than just bolting sensors to an off-the-shelf aircraft. \u201cWhat if the wings were sensors themselves?\u201d he wonders. For researchers without engineering expertise, however, the available UAVs offer plenty of opportunities. The Scottish Environmental Protection Agency, for example, purchased a drone in 2012 from Swiss company senseFly to survey estuaries for algal blooms \u2014 something that is difficult to do on foot. Susan Stevens, a scientist at the agency, says \u201cyou can get involved and use the technology without being an expert in it\u201d. Still, the best landings come with experience. As the UC Boulder team finishes up testing the Tempest, Mack, who was a UAV hobbyist before he joined this research team, gently sets the drone down on its belly in a patch of short grass. He picks it up in one hand to carry it back to the van. Everyone is relaxed, having spent most of the 40-minute test flight doing little more than watching the Tempest and enjoying the spring day. If this is the future of field research, it looks pretty easy. \u201cIf everything goes well, it is fairly boring,\u201d Stachura acknowledges. \u201cBecause it is autonomous, right?\u201d \n                     Eyes in the Sky 2013-Feb-19 \n                   \n                     A Drone in Every Driveway 2012-Dec-18 \n                   \n                     Projective simulation for artificial intelligence 2012-May-15 \n                   \n                     Military robotics and ethics: A world of killer apps 2011-Sep-21 \n                   \n                     Unmanned planes take wing for science 2010-Mar-03 \n                   Reprints and Permissions"},
{"file_id": "498286a", "url": "https://www.nature.com/articles/498286a", "year": 2013, "authors": [{"name": "Nicola Jones"}], "parsed_as_year": "2006_or_before", "body": "D-Wave is pioneering a novel way of making quantum computers \u2014 but it is also courting controversy. \u201cI've been doing combative stuff since I was born,\u201d says Geordie Rose, leaning back in a chair in his small, windowless office in Burnaby, Canada, as he describes how he has spent most of his life making things difficult for himself. Until his early 20s, that meant an obsession with wrestling \u2014 the sport that, he claims, provides the least reward for the most work. More recently, says Rose, now 41, \u201cthat's been D-Wave in a nutshell: an unbearable amount of pain and very little recognition\u201d. The problem of lack of recognition is fast disappearing for D-Wave, the world's first and so far only company making quantum computers. After initial disbelief and ridicule from the research community, Rose and his firm are now being taken more seriously \u2014 not least by aerospace giant Lockheed Martin, which bought one of D-Wave's computers in 2011 for about US$10 million, and Internet behemoth Google, which acquired one in May. Nicola Jones talks about D-Wave\u2019s computers But the pain has been real \u2014 much of it, critics would argue, brought on by Rose himself. In 2007, his company announced its first working computer with a showy public demonstration at the Computer History Museum in Mountain View, California. By the current standards of quantum computing \u2014 which in theory offers huge advances in computing power \u2014 the device's performance was astonishing. Here was a prototype searching a database for molecules similar to a given drug and solving a sudoku puzzle, while the best machines built using standard quantum approaches could at most break down the number 21 into its factors 1 . Sceptics bristled at the 'science by press conference' tone of the introduction, and wondered whether the D-Wave device wasn't just a classical computer disguised as a quantum one. \u201cThis company from Canada popped out of nowhere and announced it had quantum chips,\u201d says Colin Williams, who published one of the first texts on quantum computing in 1999, and who joined D-Wave last year as business-development director. \u201cThe academic world thought they must be crazy.\u201d Today, those criticisms have been quietened to some degree by the release of more details about D-Wave's technology. But they have been replaced by subtler questions: even if the D-Wave computer is harnessing quantum powers, is it really faster or better than a conventional computer? Will it ultimately crack problems that currently take computers decades or more to solve? Or will its capabilities hit a wall? \n               Universal vision \n             When Rose founded D-Wave in 1999, he had an engineering degree, a few years' progress towards a PhD in theoretical physics at the University of British Columbia in Vancouver \u2014 and no idea how to build a quantum computer. He did have inspiration, from a class on entrepreneurship that he had taken with Haig Farris, one of Canada's best-known technology venture capitalists. Business, says Rose, \u201cappealed to me as being harder than physics or math. There's no prescription for making people do what you want.\u201d Williams' then-new textbook helped to convince Rose that quantum computing would make a suitable target for a new venture. A cheque for Can$4,059.50 (US$3,991) from Farris let him buy a laptop and printer to produce a business proposal. By the early 2000s, D-Wave had attracted millions of dollars in capital, which Rose invested in 15 different research groups to look for the best technology to pursue. \u201cI was like an evangelist, pitching the vision\u201d of a quantum computer, he says. At the heart of that vision was quantum computing's promise to solve otherwise-intractable problems by drastically reducing the time required to find an answer. The quintessential example is factorizing: like splitting 21 into 3 \u00d7 7, but with numbers hundreds of digits long. That is the basis of the encryption algorithms widely used to protect digital data. Encryption security rests on the fact that conventional computers have to look at every possible factor in turn \u2014 a process that takes exponentially longer as the numbers get bigger. The bottleneck arises because conventional computers store and process information in an either\u2013or fashion, using 'bits' that can each exist in only one of two states, denoted 1 or 0. In most modern computer chips, each bit is represented by the presence or absence of an electric charge. Quantum computers, by contrast, exploit the fuzzy world of quantum mechanics by using 'qubits' that can exist as both 1 and 0 at the same time. In principle, they can explore different solutions simultaneously \u2014 reducing a multi-year calculation to seconds. By the time Rose began his search for the right technology with which to build a quantum computer, researchers had begun to make qubits from many physical systems, including photons that encode zeroes and ones in the direction of their polarization, and ions that encode them in their electron states. They were also working on ways to combine and manipulate the quantum information carried by these qubits, in much the same way that transistor logic gates manipulate the flow of bits in a conventional computer. The goal was to produce 'universal' quantum computers that could carry out any conceivable computation, like a modern classical machine. But this model entailed some huge engineering challenges \u2014 starting with the fact that quantum bits are extremely susceptible to outside interference. They are like pencils balanced precariously on their points: the slightest perturbation can knock them off balance, causing an error in the calculation. If each qubit is 99% accurate, an operation involving 10 of them will yield the right answer only 90% of the time, and one with 100 qubits will do so only about 36% of the time. Yet practical applications might require thousands or millions of qubits. To compensate, developers go to great lengths to shield their qubits from noise, and to devise clever error-correction schemes. But then and now, says Andrew Landahl, who works on quantum computing at Sandia National Laboratories in Albuquerque, New Mexico, \u201cif you look at the redundancy and fidelity you need, it's extremely demanding\u201d. Like a rocket that requires tonnes of fuel to hoist a tiny payload, a gate-model quantum computer might need billions of error-correcting qubits just to get 1,000 functional qubits to do something productive. By 2003, Rose was convinced that this model was \u201cjust a bad, bad, bad idea\u201d, he says. So he shifted his focus to what was then a research backwater: adiabatic quantum computing 2 . This technique is best suited to optimization problems \u2014 the kind in which the best possible outcome must be found for a number of criteria simultaneously. Examples include trying to arrange the seating for a wedding at which some guests are best friends and others sworn enemies; or finding the most energetically stable way to fold a protein in which the various amino acids attract or repel each other. All the possible solutions to such problems can be imagined as a mountain range in which the higher elevations correspond to configurations that violate most of the criteria \u2014 enemies sitting next to enemies, so to speak \u2014 and the lowest points correspond to solutions in which most or all of the criteria are satisfied. The trick is to find those low points. A conventional adiabatic computer can do that through the equivalent of huffing and puffing over the mountain passes, systematically looking for dips. But a quantum adiabatic computer does a rapid global search. It starts with the analogue of tipping water onto a flat landscape \u2014 a state in which the qubits are in a perfect quantum superposition of zeroes and ones \u2014 then lets the mountains rise slowly, so that the water naturally pools in the best solutions. The key to such a computer is that its qubits are meant to stay in their lowest energy state at all times \u2014 the precariously balanced pencils have already fallen over. This gives it the massive advantage of being relatively resistant to outside interference, so that little or no error correction is needed until the computer has thousands of qubits or more. And although it is not very useful for factorizing large numbers \u2014 the thing that spurred research into quantum computers in the first place \u2014 its approach could potentially be used on applications ranging from language translation and voice recognition to working out flight plans for spacecraft. In 2003, little was known about how to make or program an adiabatic quantum computer, and no one had put in the money and time to build a prototype. Rose decided that D-Wave should try. Using qubits made from superconducting loops of niobium, cooled to 20 millikelvin above absolute zero to keep them in their lowest energy states, D-Wave's engineers created a usable computer before even they were sure how it worked. \u201cThe name of the game from the outset was to make a functional computer,\u201d says Williams. \u201cThen they could probe it to see where it was operating correctly.\u201d From there, D-Wave ramped up quickly. The company's 2007 demonstration used a 16-qubit device. By 2011, the D-Wave One machine purchased by Lockheed Martin had 128 qubits (see  Nature   474 , 18; 2011 ). This year's D-Wave Two, the model acquired by Google and collaborators including NASA, has 512 (see  Nature   http://doi.org/mt2 ; 2013 ). Their computer looks like the proverbial black box: it is a shiny black cube about the size of a sauna. Most of the space is occupied by a cryogenic cooling system; the quantum chip itself is the size of a fingernail. D-Wave aims to double the number of qubits on that chip every year. \n               Hostile audience \n             From the start, D-Wave generated a lot of bad feeling. \u201cI think it is not too strong to say they were initially ridiculed by the academic community,\u201d says Jeremy O'Brien, a physicist at the University of Bristol, UK, who invented the computer that can factorize 21. The problem was not so much the adiabatic-computing approach \u2014 it has a solid, if sparse, academic history \u2014 but the company's brash style. Most quantum-computing experts feel that Rose and his colleagues should have started by soberly publishing papers characterizing their qubits, rather than putting out press releases. Scott Aaronson, a computer scientist at the Massachusetts Institute of Technology in Cambridge and a long-time D-Wave sceptic, remains unimpressed by what the company has actually shown that it can do. \u201cThey are marketing types who are trying to make the most dramatic claims possible,\u201d he bristles. Rose neither denies nor apologizes for the brashness. He has frequently been quoted as saying, in effect, that his approach is how you build a company. Rose also insists that he has no regrets about the company's 2007 press event \u2014 particularly given that it got the attention of Google, which started working informally with D-Wave soon afterwards. \u201cWe're not in this business to be popular,\u201d he says. Business style aside, the D-Wave computer is so different from anything else that exists that not even experts know exactly how to judge it. \u201cYou do these demonstrations, and how do you know if it's any more significant than factoring 15?\u201d says John Martinis, a physicist at the University of California, Santa Barbara, who heads one of the leading groups working on gate-model quantum computers. Some of the suspicion is easing as it becomes clearer how the computers operate. In 2011, D-Wave published evidence for quantum behaviour in its 8-qubit chip 3 . Outside the company, the group that has spent the most time on the question is the University of Southern California's Quantum Computing Center in Los Angeles, set up in collaboration with Lockheed Martin when the firm bought its D-Wave computer. In April, a team including the centre's scientific director, Daniel Lidar, circulated results seeming to confirm that the 128-qubit D-Wave One works on a quantum level 4  \u2014 although in the fuzzy quantum world nothing is certain, and the results have been challenged 5 , 6 . Still, D-Wave has chipped away at its credibility problem, concludes O'Brien, \u201cand now they're taken ever more seriously\u201d. \n               Practical considerations \n             Regardless of how the D-Wave computer works, the practical question is whether it can be used for real-world problems. It can \u2014 sort of. In 2009, for example, a Google research team developed a D-Wave algorithm 7  that could learn to judge whether or not a photo showed a car \u2014 an example of a 'binary image classifier' that could in principle be used to tell whether a medical image shows a tumour, or a security scan shows a bomb. Finding ever-better ways of doing this sort of task is at the heart of artificial intelligence, and is one area in which an adiabatic quantum computer is expected to excel. In 2012, researchers at Harvard University in Cambridge, Massachusetts, used a D-Wave machine to find the lowest-energy folding configuration for a protein with six amino acids 8 . They did not have enough qubits to code the problem properly, but even so, on a problem that no other quantum computer could touch, the D-Wave machine found the best solution 13 times out of 10,000 runs. And many of the other answers were good solutions, if not the best. Meanwhile, Lockheed Martin and University of Southern California researchers have developed an algorithm that allows D-Wave machines to tell whether a piece of software code is bug-free 9  \u2014 something that, Lockheed Martin notes, is impossible with classical computers. \u201cYou would never know\u201d for sure if a piece of classical-computer code was clean, says Ray Johnson, chief technology officer for Lockheed Martin in Bethesda, Maryland. All anyone could say was that no fault had been found after years of testing. \u201cBut now you can say with certainty,\u201d says Johnson. \u201cWe have great hope, and confidence, in the ability of the computer to scale to real-world complex problems.\u201d D-Wave also competes well against conventional computers in terms of speed, although direct comparisons are difficult. Earlier this year, D-Wave asked Catherine McGeoch, a computer scientist at Amherst College in Massachusetts, to put the D-Wave Two through its paces to satisfy Google before the Internet giant confirmed its deal. McGeoch found that in the optimization-type problems that the D-Wave was designed to solve, it came up with the right answers in half a second, compared with 30 minutes for a top-level IBM machine 10 . \u201cThat's one of the most exciting things to happen in quantum computing,\u201d says O'Brien. It is far from clear how long that advantage will last, however, if only because there is no good theory to describe how quantum adiabatic computers will behave on a larger scale. \u201cWe are absolutely certain we can build the next generation of this device, but we have absolutely no idea how well it will work,\u201d laughs Rose. And since McGeoch presented her results 10  at a meeting in May, other computer scientists have been trying to write yet-faster codes for classical computers. Aaronson says that speed should not be taken as proof of how the device is working. \u201cEven if the machine does get to a solution faster than an ordinary laptop,\u201d he says, \u201cthen you still face the question of whether that's because of quantum effects, or because a team of people spent $100 million designing a special machine optimized to these types of problems.\u201d In the meantime, work continues to make qubits for universal gate-model quantum computers more reliable, or easier to mass-produce. O'Brien, who admits that his 4-year-old daughter can factorize 21 faster than his computer, is optimistic about the future. \u201cIn 10 years' time, I'd be hugely disappointed if we didn't have a machine capable of factoring a 1,000-bit number, involving millions of qubits,\u201d he says. But Rose remains a devotee of the adiabatic church \u2014 and is convinced that D-Wave's next generation will prove that it can solve exponentially more difficult problems without taking exponentially more time. \u201cThere's going to be absolutely no hope for classical computers if this thing next year behaves as we expect,\u201d he says. Rose goes so far as to consider the hardware problem solved: the real challenge, he says, will be the software. \u201cProgramming this thing is ridiculously hard,\u201d he admits; it can take months to work out how to phrase a problem so that the computer can understand it. But D-Wave has teams working on that \u2014 including Rose. Rose expects tough competition. But with his instinct for fighting, he seems ready for it. \n                     Google and NASA snap up quantum computer 2013-May-16 \n                   \n                     First sale for quantum computing 2011-May-31 \n                   \n                     Quantum computers move a step closer 2010-Sep-29 \n                   \n                     Blogpost: Quantum computer passes speed test \n                   \n                     Blogpost: Further proof for controversial quantum computer \n                   \n                     Web focus: 2020 - Future of Computing \n                   \n                     USC\u2013Lockheed Martin Quantum Computation Center \n                   \n                     Scott Aaronson on D-wave \n                   \n                     D-Wave Blog: Hack the Multiverse \n                   Reprints and Permissions"},
{"file_id": "498290a", "url": "https://www.nature.com/articles/498290a", "year": 2013, "authors": [{"name": "Joanne Baker"}], "parsed_as_year": "2006_or_before", "body": "With earthquake death tolls rising, Ross Stein is building a global risk model to mitigate future disasters. In a darkened room in Pavia, Italy, a jumble of stubby arrows spreads out across a large screen like a swarm of ants on the march. To Ross Stein, the marks on this map of the Balkans reveal where earthquakes are most likely to strike, and he urgently wants to share what he sees. Stein, a geophysicist with the US Geological Survey (USGS) in Menlo Park, California, jumps up from his chair and runs his hand in an arc down the map. Seated in the room are eight seismologists from the former Yugoslav republics and Albania who are analysing their data together for the first time. Ross explains to them how compression is thrusting rocks upwards along faults in some areas and pushing them sideways in others. That pent up energy could be released in devastating tremors, he says, just as it was in July 1963 in Skopje, Macedonia, killing more than 1,000 people. Such a comprehensive view of the quake risks in the Balkans has been missing, in part because researchers there have limited funding and because some nations prefer to sell geological data rather than disseminate it for free. Two of the workshop's participants, from Slovenia and Albania, are long-time collaborators who could not afford to meet face-to-face in the past decade. Stein aims to change all that \u2014 in the Balkans and elsewhere \u2014 by bringing people and data together. He is one of the leaders of the Global Earthquake Model (GEM), an ambitious project to build an open-source digital network of databases and tools focused on seismic dangers around the world. By helping nations, businesses and researchers to assess and minimize risks, Stein hopes to counter the conditions that have led earthquake death tolls to rise over the past century as cities \u2014 many with poor building practices \u2014 have swelled in quake-prone regions. After more than five years in development, GEM is nearing major milestones. Next week, the project will release a database of quakes that have occurred over the past millennium, along with a basic version of its software engine, OpenQuake, which will allow users worldwide to calculate their vulnerability to seismic shocks. In December, GEM will unveil a list of all known active faults in the world. \u201cYou'd think that our community would have an inventory, but no one's tried to build one,\u201d Stein says. \u201cThat's what GEM plans to do.\u201d Over the course of 2014, GEM will add in information about buildings and socio-economic indicators, such as poverty, which could help cities such as Istanbul in Turkey decide how to prioritize the strengthening of vulnerable schools and hospitals. \u201cIt's extraordinary to me how much they have accomplished,\u201d says Lori Peek, a sociologist and co-director of the Center for Disaster and Risk Analysis at Colorado State University in Fort Collins, whose research has informed the project. Leading the GEM effort has marked a major career shift for Stein, a well-respected researcher who has frequently appeared in the media warning citizens about quake risks in the United States. Now he is on a much bigger stage, trying to drum up support for this international project from scientists, governments and companies. \u201cIt's been quite an education,\u201d he admits. And it is far from over. Stein must still complete GEM and demonstrate its value. Some critics charge that the effort will not save many lives by offering more sophisticated assessments of seismic risk. Roger Bilham of the University of Colorado at Boulder says that corruption, ignorance and poverty are much greater barriers to safety than lack of information about quakes. \n               Stressful start \n             Stein, 59, got his first big taste of seismology as a teenager in Los Angeles, when \u201cterra firma became jello\u201d during the 1971 San Fernando Valley quake, which killed 65 people. But he did not settle on studying Earth science until his college room-mate at Brown University in Providence, Rhode Island, introduced him to the joys of field trips. Stein started a doctorate in glaciology at Stanford University in California and endured the \u201ccoldest, wettest, windiest fieldwork\u201d. Then, wanting to pursue a topic with social impact, he switched to earthquakes after hearing a talk from a USGS scientist. He joined the agency in 1981. In his research, Stein has focused on how an earthquake in one spot transfers stress to other regions. His modelling efforts have provided a means of estimating whether tremors will increase or decrease the likelihood of earthquakes elsewhere. That and other work, notably in Turkey and Japan, made Stein the second most highly cited earthquake scientist from 1993 to 2003. And his impact has spread far beyond the research community. He has appeared in numerous documentaries and is often in front of a camera after a large quake. Stein's research trajectory was drastically altered by the 2004 Sumatra\u2013Andaman earthquake and tsunami, which killed more than 230,000 people in 14 countries. That event, he says, \u201ccrystallized our failure as a community\u201d by revealing how little scientists had done to help the region to prepare for the hazards expected in that area. \u201cIn some ways I felt there was blood on my hands,\u201d says Stein. He decided that it was more important to address seismic risks in poor countries than in California or Japan, where a long tradition of research and strong building codes has already reduced dangers. From Jakarta to Port-au-Prince, urban populations are skyrocketing near major faults and along tectonic-plate boundaries. The influx of people is filling poorly constructed houses that become death traps in quakes, Stein says. Seismologists predict that, before long, a large shock will kill a million people. In 2006, after an earthquake workshop in Potsdam, Germany, Stein and two other seismic-risk experts \u2014 Jochen Zschau at the Helmholtz Centre in Potsdam and Domenico Giardini of the Swiss Federal Institute of Technology (ETH) Zurich \u2014 decided to combat that trend by setting up GEM. A raft of international, governmental and non-governmental organizations already helps at-risk communities to prepare for and respond to quakes, but those efforts are fragmented. The Office of US Foreign Disaster Assistance (OFDA), which sponsored Stein's Balkan workshop, helped to develop a tsunami warning system in Indonesia after the 2004 event and is running seismic projects in Haiti and the Dominican Republic. And GeoHazards International (GHI), a non-profit organization based in Menlo Park, has worked in more than 20 countries to raise awareness and train construction engineers in quake safety. But no single organization can span every town and city, and no country can afford to reinforce or insure every building. Knowing where risk is highest is key, Stein says. Information is also splintered. Peek, who advises the GHI, participated in a study for the GEM consortium that showed that communities from San Francisco, California, to Chincha in Peru all need a central resource on earthquake risks \u2014 one that pools data on seismic threats, construction issues, and economic and social factors. That would help local officials to prioritize which public buildings or regions to strengthen, and allow emerging cities such as Kathmandhu or Lima to plan how to grow without increasing their seismic risk. GEM aims to provide that resource through OpenQuake. Built using a geographical information system, this platform will include analytical tools that allow anyone \u2014 scientists, governments and companies \u2014 to estimate the chances of economic and human losses from earthquakes (see 'Trouble spots'). The calculators will draw on the GEM's global databases of quakes, faults, housing types and socio-economic information, which are being rolled out over the next 18 months. In January, GEM released a reference catalogue of more than 20,000 global earthquakes of magnitude 5.5 and above that have occurred since 1900. To produce it, the consortium reanalysed all the seismic data involved, improving estimates of earthquake epicentres and magnitudes. It is the biggest resource of its type and has allowed seismologists to see, among other things, how seismic activity concentrates on a major fault below Guatemala, says Stein. To get the project off the ground, Stein and his collaborators had to persuade funders to back the plan. The Paris-based Organisation for Economic Co-operation and Development (OECD), which sponsored the Potsdam workshop where the GEM idea was seeded, gave Stein and the founders access to governments officials. In the wake of the Sumatran tsunami and a major quake in Pakistan, OECD member states in high-risk regions wanted to minimize their exposure to giant economic losses. Stein's contacts grew from there. In 2007, Munich Re became the first company to get involved, giving \u20ac5 million (US$6.6 million) over five years. It saw an opportunity in the global data being collected by GEM, which could help insurance companies and reinsurance brokers to diversify their portfolio to avoid being wiped out by a single earthquake. Today, 16 governmental agencies, such as the OFDA, and 10 insurance and engineering companies have joined GEM, which is a non-profit public\u2013private partnership headquartered in Pavia and has some 20 staff. These sponsors have contributed more than 90% of the \u20ac24 million needed to release the full OpenQuake platform, which is planned for November 2014. In addition, nine organizations, including the World Bank, have become associate non-paying members. Dealing with the disparate interests has been a steep learning curve for Stein. \u201cAll have a stake,\u201d he says, and \u201cissues to champion\u201d. At GEM board meetings, he says, the different sectors sit in groups around a U-shaped table \u2014 the countries on one arm, companies on the other and the non-governmental organizations in the centre. Some scientists, however, are unhappy that companies have a seat at the table at all. \u201cSuppose you could manipulate hazard forecasts to justify higher quake-insurance premiums in built-up areas,\u201d muses Robert Geller at the University of Tokyo. But Stein is pragmatic. \u201cIf you are talking to a finance minister you have to talk about economics or they won't pay attention,\u201d he says. To be widely used and trusted, Stein says that GEM must be seen as independent, transparent and accessible. That's why OpenQuake uses open-source software \u2014 and why GEM plans to give away the project's data and products to anyone, including the public, scientists and governments, if they are engaged in non-commercial work. Companies wanting to use the data commercially will need to sponsor the organization. Governmental agencies are asked to make a contribution that is proportional to their total investment in research and development. For Ecuador, that runs to \u20ac15,000 per year, whereas Germany is paying \u20ac275,000 annually. The founders hope that banks and companies will join in order to build new markets or products. They could use GEM data and tools to develop 'catastrophe bonds', a type of insurance in which investors take the risk in return for payments if a specified event does not occur. Companies have offered such bonds since the mid-1990s, but governments are now getting in on the act. Earlier this year, a group including the Turkish government issued a bond that will release US$400 million if Istanbul experiences a major shock in the next three years. \n               Science diplomacy \n             Stein, who chairs GEM's scientific advisory board, has to do more than marshal the seismic data and models. He is part of the human glue that melds the sectors together, a post that requires the skills of a diplomat and a salesman. Both skill sets were on display at the Balkan workshop, where the assembled seismologists began arguing over funding inequities and other problems in previous regional initiatives to analyse earthquake risk. At one point, some participants shouted at each other across the table. Stein let them have their say and then stepped in to calm the waters. He asked each in turn to express their views and offered to visit each country that autumn, to convince government officials and university heads to back the researchers. As GEM becomes more visible, Stein knows that he will have to contend with critics. Some members of the seismology community say that it is misleading to map hazards on the basis of past earthquakes because the historical record is too short, and large earthquakes often occur where none has previously been witnessed. In northeastern Japan, for example, risk maps for the Tohoku region did not anticipate a monster quake of the size that struck in 2011. Other researchers, such as Bilham, question whether the project's engineering goals will ever be enacted; they argue that many countries already have adequate building codes but fail to enforce them \u2014 so better risk models won't help. Stein has dealt with some of the criticism by inviting naysayers to participate in GEM. Seth Stein (no relation), a seismologist at Northwestern University in Evanston, Illinois, who is a long-standing opponent of some seismic-hazard maps (see  Nature   479 , 166\u2013170; 2011 ), attended a GEM workshop. Although Seth Stein sees GEM's open-source, standardized and modular approach as \u201ca good step in the right direction\u201d, he also hopes that the seismology community will take advantage of the resource to do broader analyses exploring the limitations of seismic-hazard analysis. Looking forward, Ross Stein seems most concerned about securing funding. GEM will need more subscribers to pay for the curation and updating of its databases in the future and is seeking a further \u20ac10 million to fund allied regional programmes to enhance the local detail of the risk databases. To attract and retain sponsors over the long term, the project must keep rolling out useful features on related risks \u2014 such as models including tsunamis, landslides and liquefaction, which happens when seismic shaking weakens soil to a point at which it begins to behave like a fluid. The most difficult challenge long term, however, may be handling the backlash over risks identified by GEM. Stein says that GEM \u201cis not an advocacy organization\u201d and will not get involved in policy decisions on the basis of its assessment. Even so, \u201cwe will face abuse\u201d, Stein accepts. \u201cSome governments will push back against GEM's assessments because they differ from their priorities.\u201d In Pavia, as the Balkan workshop winds up, Stein practises the diplomatic skills he will need to make GEM succeed. Moving beyond the earlier rancorous discussion, he suggests that all the participants write a joint publication and apply to the OFDA for funds to enable them to meet again in six months. All the seismologists pledge to continue the collaboration. Such a meeting will be essential \u201cif we want to build a harmonized model for the whole Balkan area\u201d, says Barbara Sket Motnikar of the Jo\u017eef Stefan Institute in Ljubljana. Three weeks later, the OFDA agrees to fund a second workshop for the group. The decision underscores some of Stein's parting words to the Balkan seismologists: \u201cNever underestimate the power of your enthusiasm.\u201d \n                 See Editorial \n                 page 271 \n               \n                     Risk management 2013-Jun-19 \n                   \n                     Seth Stein: The quake killer 2011-Nov-09 \n                   \n                     Corruption kills 2011-Jan-12 \n                   \n                     Nature special: Japan earthquake and nuclear crisis \n                   \n                     Debate on seismic hazard \n                   \n                     Global Earthquake Model \n                   \n                     US Geological Survey \n                   \n                     Ross Stein \n                   \n                     Geohazards International \n                   Reprints and Permissions"},
{"file_id": "497306a", "url": "https://www.nature.com/articles/497306a", "year": 2013, "authors": [{"name": "Henry Nicholls"}], "parsed_as_year": "2006_or_before", "body": "Ecuador has successfully eradicated invasive pigs and goats from most of the Galapagos archipelago. Now it is taking on the rats. The helicopter appears as a speck on the horizon, moving slowly on a dead-straight path over the black volcanic island. Beneath it hangs a huge metal cone: an industrial-scale hopper that is sending a steady stream of blue pellets raining down on the scrubby landscape of Pinz\u00f3n, one of the Galapagos Islands. Erin Hagen watches through her binoculars. She is standing on the deck of the  Sierra Negra , one of three vessels moored just off the island on this morning in November 2012. When the helicopter reaches the rocky shoreline, it changes course, heads across the ocean and hovers just above the boat. At Hagen\u2019s instruction, a team of conservationists comes to life. Two men stand by to service the hopper. Three others prepare to reload it with more than 400\u00a0kilograms of poisoned rat bait. Within three minutes, the loading is complete and the helicopter is heading back to lace Pinz\u00f3n with more toxic bait. \u201cIt\u2019s kind of like a pit-stop,\u201d explains Hagen, a project manager for Island Conservation, an international non-governmental organization with expertise in the eradication of invasive species. \u201cThere\u2019s a real buzz from everyone.\u201d Five years ago, most of the major islands and smaller rocky outcrops in the Galapagos were home to a plague of invasive mice and rats. The rodents feed on the eggs and young of seabirds, land birds and reptiles, and have brought several species\u00a0\u2014\u00a0including the rare Pinz\u00f3n giant tortoise ( Chelonoidis   duncanensis )\u00a0\u2014\u00a0to the brink of extinction. In 2007, the Galapagos National Park Service (GNP) and the Charles Darwin Foundation (CDF) developed an initiative code-named Project Pinz\u00f3n, a military-style plan-of-action to kill invasive rodents on three islands \u2014 starting with North Seymour (1.8\u00a0square kilometres), then moving on to R\u00e1bida (5\u00a0square kilometres) and, finally, Pinz\u00f3n (18\u00a0square kilometres) \u2014 plus around a dozen smaller outcrops and islets (see \u2018Rat race\u2019). The effort, costing some US$3 million so far, is not the biggest rat eradication ever attempted. But it is one of the most high-profile and challenging. Before conservationists and scientists could start attacking the rodents, they had to ensure that their poison would not take out some of the unique \u2014 and endangered \u2014 mockingbirds, finches, rails, iguanas and tortoises famously described by Charles Darwin. And whereas most rat eradications so far have targeted remote, uninhabited islands, the Galapagos is home to some 30,000\u00a0people and receives around 180,000 visitors each year. With so much boat traffic, the risk of reinvasion will be very high, says James Russell, an ecologist at the University of Auckland in New Zealand who has a special interest in rat invasions. \u201cTheir real challenge is going to be that biosecurity,\u201d he says. For those involved, the anti-rat campaign is worth the trouble and the risks. It promises to allow unique species to flourish again and, building on the prior removal of feral pigs and goats from much of the archipelago, to make Ecuador a world leader in the eradication of invasive species. \u201cGalapagos is up there in the front line looking to make the next big leap in multi-species pest management,\u201d Russell says. By the time Darwin arrived in the Galapagos in 1835, the rodents had long since settled in. Mice and black rats were probably the first to arrive, introduced by pirates or whalers in the seventeenth century; since the 1980s, Norway rats have found their way there too 1 . Galapagos experts have little doubt that the rodents have devastated native wildlife, even though the creatures\u2019 effects have not been studied systematically. \u201cI just hated the immigrant killers because I could see what they were doing,\u201d says Felipe Cruz, a lifelong conservationist who grew up on Floreana, one of four inhabited islands in the archipelago. In the early 1980s, Cruz spent nine months of the year camped in the Floreana highlands deploying a cocktail of rodenticide to prevent rats from destroying eggs and chicks in the most important breeding colony of the Galapagos petrel ( Pterodroma phaeopygia ), a species that has been listed as critically endangered since 1994 (ref.  2 ). His dedication was rewarded, says Cruz. \u201cThere were more birds, more plants, more lizards. It was like an island within an island.\u201d It was also a transformative experience. \u201cIt\u2019s something that fills me with pride, satisfaction and somehow shaped my life,\u201d he says. \n               Scaling up \n             Before long, Cruz had an opportunity to eradicate black rats on a bigger scale: from the entire island of Pinz\u00f3n, where they were destroying the island\u2019s endemic tortoise species by devouring hatchlings. In 1988, \u201cthere was a massive, massive drought and we began to see dead rats everywhere\u201d, recalls Linda Cayot, then at the Charles Darwin Research Station on the central island of Santa Cruz and now science adviser for the Galapagos Conservancy in Fairfax, Virginia. Cayot and Cruz saw the perfect opportunity to finish what the drought had started, and convinced the directors of the GNP and the CDF to allow a team to spread bait laced with rodenticide. The rat population recovered after a few months, but Cayot describes the exercise as \u201ca successful failure\u201d. The researchers realized that very young rats may not have encountered the bait, and that they needed to apply it twice. \u201cI think we came really close,\u201d Cayot says. \u201cWe just learned so much about how to run a massive field operation.\u201d This paved the way for an onslaught against much larger invasive mammals\u00a0\u2014\u00a0mainly pigs and goats\u00a0\u2014\u00a0whose relentless grazing had stripped out much of the vegetation, with knock-on consequences for native herbivores. Project Isabela, a staged initiative that began in 1997 and cost nearly $10.5 million, resulted in the eradication of invasive pigs from the huge island of Santiago 3  and some 140,000 goats from more than 5,000 square kilometres on several islands. According to Cruz and others involved in the project, it is \u201cthe world\u2019s largest island restoration effort to date\u201d 4 . The effort also gave Galapagos conservationists the confidence to think big, says Cruz. They resolved to launch a new assault on the rats. In 2007, the GNP and the CDF held a workshop to consider how best to approach the rat problem. This pulled together expertise from around the world, most notably from New Zealand, a country that has some 50\u00a0years of experience in the eradication of invasive species ranging from rabbits to wallabies. New Zealand is also the record holder for rat eradication: the 113-square-kilometre Campbell Island has officially been rat-free for almost a decade ( http://eradicationsdb.fos.auckland.ac.nz ). \u201cIt\u2019s a bit of an industry in New Zealand,\u201d says John Parkes, a collaborator with Landcare Research, an environmental research organization based in Lincoln, New Zealand, and a key participant in the workshop. \u201cThere\u2019s lots of research that shows the benefits of removing these exotic species from islands far outweigh the short-term costs imposed by the control technologies themselves.\u201d The workshop resulted in Project Pinz\u00f3n, which, by working from small to bigger islands, had the aim of \u201ctaking on progressively larger and more complex eradications\u201d, says Karl Campbell, senior programme director for Island Conservation. Soon after Ecuador\u2019s environment ministry signed off on the plan, the GNP and the CDF set to work on North Seymour, with Island Conservation entering into the project in 2008. Later that year, with signs of success on North Seymour, Island Conservation approached Bell Laboratories, a company in Madison, Wisconsin, that specializes in industrial-scale rodent control. Would the firm donate sufficient bait\u00a0\u2014\u00a0almost 45 tonnes\u00a0\u2014\u00a0to cover all the remaining islets and islands identified in the Project Pinz\u00f3n roadmap? The company, attracted by the philanthropic cause, agreed. The biggest uncertainty was what impact the active ingredient\u00a0\u2014\u00a0an anticoagulant called brodifacoum\u00a0\u2014\u00a0might have on non-target species. In birds and mammals, the chemical prevents the repair of capillaries that rupture naturally, resulting in internal bleeding and, if the dose is high enough, death. What was not known was how the \u201ceminently curious\u201d fauna of the Galapagos, as Darwin described it, would respond if exposed to the bait. \u201cWe had to start building from scratch,\u201d Campbell says. This meant doing a risk assessment for all vertebrates and threatened species that might be affected on the islands yet to be baited. Among the species of greatest concern were the Galapagos mockingbirds and finches, which might happily peck away at the bait. So, in 2009, graduate student Ana Luc\u00eda Carri\u00f3n Bonilla of the University of San Francisco in Quito set about determining what colour these iconic birds like least 5 . Once she had the answer \u2014 blue \u2014 Bell Laboratories cooked up a blue version of an existing rat bait. Trials of a non-toxic version were promising: a suite of key endemic species turned their noses up at the bright blue blocks. Subsequent tests showed that a toxic version of the bait killed invasive house mice occupying the small uplifted island of North Plaza. In an added twist, the bait here and on most subsequent islands was spiked with a fluorescent dye to help researchers to track its movement through the environment by tell-tale traces on trails, nests, faeces and the animals themselves. \u201cYou go out with a big ultraviolet spotlight and you can see basically where this bait has gone,\u201d says Campbell. This suggested that some finches and lava lizards were nibbling at the bait, but were not killed by it. Although the taste tests indicated that the Galapagos\u2019 world-famous tortoises and other reptiles would not gorge themselves on the bait, nobody was taking any chances. \u201cWe needed to get some hard data on what to expect if tortoises did eat the bait,\u201d says Penny Fisher of Landcare Research. In 2010, Fisher fed the bait pellets to captive tortoises of hybrid origin \u2014 which conservationists consider less valuable than wild-living, purebred animals \u2014 then drew blood samples over the course of several weeks and measured how long they took to clot as an indicator of toxic effects. \u201cIt was a nerve-wracking trial to do,\u201d she says. But, for reasons that are not yet clear, the coagulation time remained fairly constant, suggesting that tortoises would not face a serious risk of poisoning. Protecting other species required more drastic measures. The Galapagos hawk perches at the top of the local food chain and, with a diet that ranges from young iguanas to sea lion afterbirth, faced the greatest risk of inadvertent poisoning. So the team decided to bring all territorial hawks into captivity until six weeks after the first bait drop. In January 2011, when the helicopter delivered bait over R\u00e1bida and a handful of smaller islets, Julia Ponder of the University of Minnesota looked after 20 hawks in makeshift aviaries on the nearby island of Santiago. All of the birds survived and were released back to the wild. And after a thorough survey in November 2012, the GNP declared R\u00e1bida free of invasive rodents. Now, with a small population of tortoises in captivity as insurance, it was time to prepare for Pinz\u00f3n itself. The helicopter pilot would need a three-day window of clear skies to bait the landscape with the necessary accuracy, and he would navigate along tight, pre-ordained flight lines just 35\u00a0metres apart. \u201cKeeping those lines straight is what makes or breaks a project,\u201d says Hagen. \u201cThen we know we can ensure full coverage.\u201d In the run-up to the operation, Hagen and the baiting team practised loading and dropping bait on an abandoned US military airstrip on the island of Baltra. Then, in mid-November 2012 and with the weather forecast looking good, the operation began in earnest. \n               Signs of success \n             When Hagen stepped onto Pinz\u00f3n after the second baiting in December, the island looked much as normal. \u201cTypically what animals do when they are exposed to the rodenticide is go into safe areas because they don\u2019t feel well,\u201d she says. That means the only evidence of mass poisoning is the occasional smell of rotting flesh and perhaps a skeletal rat carcass. It usually takes around two years of monitoring \u2014 using traps for live animals, \u2018bite cards\u2019 distributed across the landscape, and searches for rat footprints and faeces \u2014 before an eradication effort is declared a success. Assuming the rats are gone, the team will monitor how the ecosystem responds over the next five to ten years. Regular on-the-ground surveys will document how key species fare and acoustic sensors will gather data on the abundance and diversity of bird life from their calls. \u201cFor each of these threatened species the ultimate measure will be growing populations, possibly even self-sustaining,\u201d says Nick Holmes, director of science at Island Conservation. One archipelago-wide indicator will be land snails of the genus  Bulimulus . Although not as famous as Darwin\u2019s finches, these small snails with pine-cone-shaped shells offer an even more compelling illustration of natural selection\u2019s creative force: there are some 70\u00a0documented species, all of which could be descended from a single common colonizing ancestor 6 . But rats feed on them, which might explain why more than 50 of the species are threatened \u2014 and their damaged shells offer a good way to quantify rat predation. \u201cInvertebrates in general are going to give us a faster idea of the response to the eradication programme,\u201d says Christine Parent of the University of California, Berkeley, who in years to come will help to monitor Pinz\u00f3n\u2019s snails for signs of a rebound. But if there is one certainty in such a complex operation, it\u2019s that not everything will go to plan. Although Ponder and her colleagues succeeded in keeping 60 Galapagos hawks in captivity over the course of the Pinz\u00f3n campaign, the birds have not fared so well after their release. \u201cWe have 16 confirmed dead,\u201d says Ponder. She thinks that the birds dined on small reptiles that had consumed the bait. In future, it may pay to keep the hawks in captivity for longer. Even if every last rodent is removed from an island, constant vigilance will be needed to prevent a reinvasion. In January 2011, the GNP and Island Conservation assessed the probability of reinvasion by baiting the islands\u00a0of\u00a0Bartolom\u00e9 and Sombrero Chino,\u00a0which lie within 500\u00a0metres \u2014 swimming distance for a rat \u2014 of the still rat-infested island of Santiago. In November 2012, they found evidence of rats on both of the treated islands. (Fortunately, R\u00e1bida and Pinz\u00f3n are beyond rat swimming distance of neighbouring islands.) Tourist vessels and other boat traffic will also need to be monitored for stowaway rats. And there is always the risk of human sabotage, which occurred several times after Project Isabela. In 2009, for instance, some malcontent set six goats down on Santiago, which by then had been goat-free for around three years. The GNP put the cost of monitoring the island and removing these animals at $32,393, more than $5,000 a beast 4 . In spite of such setbacks, Ecuador\u2019s environment ministry is pushing ahead with its programme of ecological restoration in Galapagos. For 2014, the central government has committed several million dollars to attempt the extermination of rats from the 173-square-kilometre island of Floreana, where the human population adds to the challenge. \u201cDropping poison around people adds a layer of complexity,\u201d says Parkes. But success on an island the size of Floreana would set an example for the rest of the world. It would also make it easier to realize conservationists\u2019 long-standing ambition of reintroducing mockingbirds and tortoises to the island, not to mention the respite it would bring for the Galapagos petrel. And that would be the realization of a lifelong dream for Cruz. \u201cWhen I travel between the islands and I get to see groups of petrels flying about, I have to be quite honest,\u201d he says. \u201cMy heart beats faster.\u201d \n                     The legacy of Lonesome George 2012-Jul-18 \n                   \n                     Invasive species turns parasites into hosts 2012-Feb-15 \n                   \n                     Ecology: Ragamuffin Earth 2009-Jul-22 \n                   \n                     Ecosystem devastated after predators wiped out 2009-Jan-13 \n                   \n                     Nature special: Darwin 200 \n                   \n                     Blogpost: Ecuador could soon lead in anti-rat race \n                   \n                     Island Conservation \n                   \n                     Galapagos National Park \n                   \n                     Galapagos Conservancy \n                   \n                     Charles Darwin Foundation \n                   Reprints and Permissions"},
{"file_id": "497428a", "url": "https://www.nature.com/articles/497428a", "year": 2013, "authors": [{"name": "Virginia Hughes"}], "parsed_as_year": "2006_or_before", "body": "More and more studies show that being overweight does not always shorten life \u2014 but some public-health researchers would rather not talk about them. Late in the morning on 20 February, more than 200 people packed an auditorium at the Harvard School of Public Health in Boston, Massachusetts. The purpose of the event, according to its organizers, was to explain why a new study about weight and death was absolutely wrong. The report, a meta-analysis of 97 studies including 2.88 million people, had been released on 2 January in the  Journal of the American Medical Association  ( JAMA ) 1 . A team led by Katherine Flegal, an epidemiologist at the National Center for Health Statistics in Hyattsville, Maryland, reported that people deemed 'overweight' by international standards were 6% less likely to die than were those of 'normal' weight over the same time period. The result seemed to counter decades of advice to avoid even modest weight gain, provoking coverage in most major news outlets \u2014 and a hostile backlash from some public-health experts. \u201cThis study is really a pile of rubbish, and no one should waste their time reading it,\u201d said Walter Willett, a leading nutrition and epidemiology researcher at the Harvard school, in a radio interview. Willett later organized the Harvard symposium \u2014 where speakers lined up to critique Flegal's study \u2014 to counteract that coverage and highlight what he and his colleagues saw as problems with the paper. \u201cThe Flegal paper was so flawed, so misleading and so confusing to so many people, we thought it really would be important to dig down more deeply,\u201d Willett says. But many researchers accept Flegal's results and see them as just the latest report illustrating what is known as the obesity paradox. Being overweight increases a person's risk of diabetes, heart disease, cancer and many other chronic illnesses. But these studies suggest that for some people \u2014 particularly those who are middle-aged or older, or already sick \u2014 a bit of extra weight is not particularly harmful, and may even be helpful. (Being so overweight as to be classed obese, however, is almost always associated with poor health outcomes.) The paradox has prompted much discussion in the public-health community \u2014 including a string of letters in  JAMA  last month 2  \u2014 in part because the epidemiology involved is complex, and eliminating confounding factors is difficult. But the most contentious part of the debate is not about the science per se, but how to talk about it. Public-health experts, including Willett, have spent decades emphasizing the risks of carrying excess weight. Studies such as Flegal's are dangerous, Willett says, because they could confuse the public and doctors, and undermine public policies to curb rising obesity rates. \u201cThere is going to be some percentage of physicians who will not counsel an overweight patient because of this,\u201d he says. Worse, he says, these findings can be hijacked by powerful special-interest groups, such as the soft-drink and food lobbies, to influence policy-makers. But many scientists say that they are uncomfortable with the idea of hiding or dismissing data \u2014 especially findings that have been replicated in many studies \u2014 for the sake of a simpler message. \u201cOne study may not necessarily tell you the truth, but a bulk of studies saying the same thing and being consistent, that really is reinforcing,\u201d says Samuel Klein, a physician and obesity expert at Washington University in St Louis, Missouri. \u201cWe need to follow the data just like the yellow brick road, to the truth.\u201d \n               Throwing a curve \n             The notion that excess weight hastens death can be traced back to studies from the US insurance industry. In 1960, a thick report based on data from policy-holders at 26 life-insurance companies found that mortality rates were lowest among people who weighed a few kilograms less than the US average, and that mortality climbed steadily with weight above this point. This spurred the Metropolitan Life Insurance Company (MetLife) to update its table of 'desirable weights', creating standards that were widely used by doctors for decades to come. In the early 1980s, Reubin Andres, who was the director of the US National Institute on Aging in Bethesda, Maryland, made headlines for challenging the dogma. By reanalysing actuarial tables and research studies, Andres reported that the relationship between height-adjusted weight and mortality follows a U-shaped curve. And the nadir of that curve \u2014 the weight at which death rates are lowest \u2014 depends on age (see 'Weight watching'). The weights recommended by MetLife may be appropriate for people who are middle-aged, he calculated, but not for those in their 50s or older 3 , who were better off 'overweight'. It was the first glimmer of the obesity paradox.  We need to follow the data just like the yellow brick road, to the truth  Andres's ideas were roundly rejected by the mainstream medical community. In an often-cited  JAMA  paper 4  published in 1987, for example, Willett and JoAnn Manson, an epidemiologist at the Harvard School of Public Health, analysed 25 studies of weight\u2013death relationships and claimed that most were tainted by two confounders: smoking and sickness. Smokers tend to be leaner and die earlier than non-smokers, and many people who are chronically ill also lose weight. These effects could make thinness itself seem to be a risk. Manson and Willett backed up that idea in a 1995 report that analysed body-mass index (BMI) \u2014 the 'gold-standard' measure of weight, defined as weight in kilograms divided by height in metres squared \u2014 in more than 115,000 female nurses enrolled in a long-term health study 5 . When the researchers excluded women who had ever smoked and those who died during the first four years of the study (reasoning that these women may have had disease-related weight loss), they found a direct linear relationship between BMI and death, with the lowest mortality at BMIs below 19. (That is about 50 kilograms for a woman who is 1.63 metres tall.) \u201cIt didn't seem to be biologically plausible that overweight and obesity could both increase the risk of life-threatening diseases and yet lower mortality rates,\u201d Manson says. The study proved, she says, that this idea \u201cwas more artefact than fact\u201d. Around the same time, the world was waking up to obesity. Since 1980, rates of overweight and obesity had begun to rocket 6 , 7 , 8 , and in 1997, the World Health Organization (WHO) held its first meeting on the subject, in Geneva, Switzerland. That meeting resulted in the introduction of new criteria for 'normal weight' (BMI of 18.5\u201324.9), 'overweight' (BMI of 25\u201329.9) and 'obese' (BMI of 30 or higher). In 1998, the US Centers for Disease Control and Prevention (CDC) lowered its BMI cut-offs to match the WHO's classifications. \u201cWe used to call [obesity] the Cinderella of risk factors, because nobody was paying attention to it,\u201d says Francisco Lopez-Jimenez, a cardiac physician at the Mayo Clinic in Rochester, Minnesota. They were now. \n               Statistical sparring \n             Flegal was one of those raising the alarm. At the statistics centre, which is part of the CDC, she has at her fingertips data from the agency's National Health and Nutrition Examination Survey (NHANES). Based on interviews and physical examinations of about 5,000 people a year, the NHANES has been running since the 1960s. Flegal and her colleagues used it to show that rates of overweight and obesity in the United States were climbing 6 , 7 . In 2005, however, Flegal found that NHANES data confirmed Andres's U-shaped mortality curve. Her analysis showed that people who were overweight \u2014 but not obese \u2014 had a lower mortality rate than those of normal weight, and that the pattern held even in people who had never smoked 9 . Flegal's study got a lot of press, says Willett, because she works at the CDC and it seemed to be a sanction for gaining weight. \u201cA lot of people interpreted this as being the official statement of the US government,\u201d he says. Just as they did earlier this year, Willett and his colleagues criticized the work and put together a public symposium to discuss it. The academic kerfuffle drew a lot of negative media attention to Flegal's study. \u201cI was pretty surprised by the vociferous attacks on our work,\u201d says Flegal, who prefers to focus on the finer points of epidemiological number-crunching, rather than the policy implications of the resulting statistics. \u201cParticularly initially, there were a lot of misunderstandings and confusion about our findings, and trying to clear those up was time-consuming and somewhat difficult.\u201d Over the next few years, other researchers found the same trend, and Flegal decided to carry out the meta-analysis that she published earlier this year 1 . \u201cWe felt it was time to put all of this stuff together,\u201d she says. \u201cWe might not understand what it all means, but this is what's out there.\u201d Her analysis included all prospective studies that assessed all-cause mortality using standard BMI categories \u2014 97 studies in total. All the studies used standard statistical adjustments to account for the effects of smoking, age and sex. When the data from all adult age groups were combined, people whose BMIs were in the overweight range (between 25 and 29.9) showed the lowest mortality rates. The Harvard group contends, however, that Flegal's approach did not fully correct for age, sickness-related weight loss and smoking. They say that the effect would have vanished in younger age groups if Flegal had separated them out. They also argue that not all smokers have the same level of exposure \u2014 people who smoke heavily tend to be leaner than occasional smokers, for example \u2014 so the best way to remove smoking as a confounder is to focus on people who have never smoked. Willett points to one of his studies 10 , published in 2010, that was not included in Flegal's analysis because it did not use standard BMI categories. Analysing data from 1.46 million people, Willett and his colleagues found that among people who have never smoked, the lowest mortality occurs in the 'normal' BMI range, of 20\u201325. Flegal, in turn, criticizes the Willett study for scrapping large swathes of the raw data set: nearly 900,000 people in all. \u201cOnce you delete such large numbers, and they are really large, you don't quite know how the never-smokers in the sample differ from the others,\u201d she says. Never-smokers could be richer or more educated, for example. What is more, says Flegal, Willett's study relies on participants' self-reported heights and weights, rather than objective measures. \u201cIt's a huge deal,\u201d Flegal says, because people tend to underestimate how much they weigh. This could skew death risks upwards if, for example, people who are obese and at high risk say that they are merely overweight. \n               Healthy balance \n             Many obesity experts and health biostatisticians take issue with the harsh tone of Willett's statements about Flegal's work. They say that there is merit in both Willett's and Flegal's studies, that the two are simply looking at data in different ways and that enough studies support the obesity paradox for it to be taken seriously. \u201cIt's hard to argue with data,\u201d says Robert Eckel, an endocrinologist at University of Colorado in Denver. \u201cWe're scientists. We pay attention to data, we don't try to un-explain them.\u201d What they are trying to explain is the reason for the paradox. One hint lies in the growing number of studies over the past decade showing that in people with serious illnesses such as heart disease, emphysema and type 2 diabetes, those who are overweight have the lowest death rates. A common explanation is that people who are overweight have more energy reserves to fight off illness. They are like contestants on the television show  Survivor , says Gregg Fonarow, a cardiologist at the University of California, Los Angeles: \u201cThose that started off pretty thin often don't come out successful.\u201d Metabolic reserves could also be important as people age. \u201cSurvival is a balance of risks,\u201d says Stefan Anker, a cardiology researcher at Charit\u00e9 Medical University in Berlin. \u201cIf you are young and healthy, then obesity, which causes problems in 15 or 20 years, is relevant,\u201d he says. With age, though, the balance may tip in favour of extra weight. Genetic and metabolic factors may also be at play. Last year, Mercedes Carnethon, a preventive-medicine researcher at Northwestern University in Chicago, Illinois, reported that adults who develop type 2 diabetes while they are of normal weight are twice as likely to die over a given period as those who are overweight or obese 11 . Carnethon says that the trend is probably driven by a subset of people who are thin yet 'metabolically obese': they have high levels of insulin and triglycerides in their blood, which puts them at a higher risk for developing diabetes and heart disease. All this suggests that BMI is a crude measure for evaluating the health of individuals. Some researchers contend that what really matters is the distribution of fat tissue on the body, with excess abdominal fat being most dangerous; others say that cardiovascular fitness predicts mortality regardless of BMI or abdominal fat. \u201cBMI is just a first step for anybody,\u201d says Steven Heymsfield, an obesity researcher and the executive director of the Pennington Biological Research Center in Baton Rouge, Louisiana. \u201cIf you can then add waist circumference and blood tests and other risk factors, then you can get a more complete description at the individual level.\u201d If the obesity-paradox studies are correct, the issue then becomes how to convey their nuances. A lot of excess weight, in the form of obesity, is clearly bad for health, and most young people are better off keeping trim. But that may change as they age and develop illnesses. Some public-health experts fear, however, that people could take that message as a general endorsement of weight gain. Willett says that he is also concerned that obesity-paradox studies could undermine people's trust in science. \u201cYou hear it so often, people say: 'I read something one month and then a couple of months later I hear the opposite. Scientists just can't get it right',\u201d he says. \u201cWe see that time and time again being exploited, by the soda industry, in the case of obesity, or by the oil industry, in the case of global warming.\u201d Preventing weight gain in the first place should be the primary public-health goal, Willett says. \u201cIt's very challenging to lose weight once you're obese. That's the most serious consequence of saying there's no problem with being overweight. We want to have people motivated not to get there in the first place.\u201d But Kamyar Kalantar-Zadeh, a nephrologist at the University of California, Irvine, says that it is important not to hide subtleties about weight and health. \u201cWe are obliged to say what the real truth is,\u201d he says. Flegal, meanwhile, says that the public's reaction to her results is not her primary concern. \u201cI work for a federal statistical agency,\u201d she says. \u201cOur job is not to make policy, it's to provide accurate information to guide policy-makers and other people who are interested in these topics.\u201d Her data, she says, are \u201cnot intended to have a message\u201d. \n                 See Editorial \n                 p.410 \n               \n                     Shades of grey 2013-May-22 \n                   \n                     Global survey reveals impact of disability 2012-Dec-18 \n                   \n                     Treat obesity as physiology, not physics 2012-Dec-12 \n                   \n                     Treat obesity as physiology, not physics 2012-Dec-12 \n                   \n                     Obesity: Insensitive issue 2012-Jun-20 \n                   \n                     Dietary advice: Flash in the pan? 2005-Feb-23 \n                   \n                     Nature special: Can science feed the world? \n                   \n                     World Health Organization on obesity \n                   \n                     US CDC on obesity and overweight \n                   Reprints and Permissions"},
{"file_id": "497424a", "url": "https://www.nature.com/articles/497424a", "year": 2013, "authors": [{"name": "Alexandra Witze"}], "parsed_as_year": "2006_or_before", "body": "Ed Stone has spent 36 years guiding the twin Voyager spacecraft through the Solar System. Next stop, interstellar space. The 44 notebooks lined up neatly in Ed Stone's office span just half a metre of shelf space. But inside these journals, in meticulous black printing, Stone has chronicled the longest journey that humans have ever launched. Since they left Earth in 1977, the twin Voyager spacecraft have conducted pioneering explorations of Jupiter, Saturn, Uranus and Neptune, revealing these gas giants and their moons to be far more active than scientists had expected. Now the two probes are cruising towards the edge of the Solar System \u2014 a boundary that has yet to be crossed by any emissary from Earth. Stone has chaperoned the Voyagers from their conception. He is the mission's first and so far only project scientist, tasked with juggling the competing needs of the scientists who use the Voyagers' instruments against those of the engineers that fly the craft. By all accounts, he has succeeded. \u201cSomehow he got that discordant orchestra to play together,\u201d says Andrew Ingersoll, a planetary scientist at the California Institute of Technology (Caltech) in Pasadena who was on the team as the spacecraft flew past Jupiter and Saturn. To many, the Voyagers are synonymous with the unflappable Stone. The man and the mission are bound together, even as the probes enter yet another phase in their storied lifetime. Almost 19 billion kilometres from Earth, Voyager 1 is flirting with the edge of interstellar space, the medium between the stars. Last July, it saw the flood of charged particles from the Sun subside to a mere trickle \u2014 a sign that the spacecraft may soon break out of the Solar System. Stone, who is now 77, plans to be around when it happens. He is not about to dial back his legendary work habits \u2014 not as Voyager nears such a historic milestone, with its promise of provocative science. Sitting in his Caltech office, with the Voyager logs behind him, the trim, elfin physicist looks mildly astonished at any suggestion of retirement. Talking about his spacecraft, Stone could just as easily be describing his own drive. \u201cWhat keeps Voyager still alive,\u201d he says, \u201cis that it's still discovering.\u201d \n               Atomic ambitions \n             Stone grew up in Burlington, Iowa, a town on the Mississippi River where his father worked in construction and was always tinkering with machines. Young Edward devoured copies of  Popular Science  magazine and  The Book of Knowledge , and taught himself to build radio receivers. The atomic age was well under way as Stone finished high school, and a teacher at his junior college told him about the world-renowned physics programme at the University of Chicago, Illinois, home to nuclear pioneer Enrico Fermi. Off Stone went, planning to study nuclear physics. But in October 1957, the launch of Sputnik inspired him to switch to space physics. For his graduate work, he helped to develop balloon- and satellite-borne detectors to track fast particles and cosmic rays streaming into Earth's atmosphere from space. In 1964, that led to a research position \u2014 and then a faculty job \u2014 at Caltech, which was starting a space-physics group. His successful work on cosmic-ray detectors caught the eye of engineers at the Jet Propulsion Laboratory (JPL) in Pasadena, who were developing a mission initially called Mariner Jupiter\u2013Saturn '77. They recruited Stone in 1972 to serve as project scientist \u2014 the person who oversees a spacecraft's scientific goals. The mission grew into the most ambitious planetary exploration ever. Its two probes would each survey the outer Solar System with 10 instruments and a radio-science experiment \u2014 a set of investigations more sophisticated than those carried out by the Pioneer 10 and 11 craft that had reached Jupiter and Saturn before them. The two spacecraft launched from Cape Canaveral, Florida, in August and September 1977, carrying golden records engraved with messages and recordings from Earth. Before the launch, they were renamed Voyager 1 and Voyager 2. Kerri Smith talks to Alexandra Witze about Voyager. It was not all smooth sailing. The boom carrying the scientific instruments for Voyager 2 could not deploy fully after launch. And the main radio receiver on the same probe failed completely in the spring of 1978, forcing engineers to turn to a back-up system. Those and other glitches put added pressure on Stone, who had to coordinate between the principal investigators in charge of the instruments and the mission engineers who were troubleshooting the issues. He was the one who had to work out what science was achievable, given the constraints of the craft. The results started to pour in during 1979, when first Voyager 1, then Voyager 2, flew past Jupiter. They spotted sulphur volcanoes belching from Jupiter's moon Io, as it was flexed by the giant planet's powerful gravity. Passing by the moon Europa, the probes photographed long, dirty-looking fractures in its icy surface, a possible clue to a subsurface ocean that could harbour extraterrestrial life. And they discovered a plasma, with temperatures of hundreds of millions of degrees Celsius, enshrouding Jupiter's magnetosphere. These and dozens of other finds now fill planetary-science textbooks. When the probes arrived at Saturn in 1980\u201381, they uncovered 'shepherd' moons that herd the ice and dust in Saturn's outermost ring, which turned out to have a strange kinked appearance. The Voyagers also studied gigantic aurorae that swallowed much of the planet's northern and southern poles. Next, mission controllers sent Voyager 1 on a trajectory out of the plane of the planets and towards the boundary with interstellar space. Voyager 2 sailed on to Uranus in 1986, where it found two new rings, ten new moons and an odd magnetic field oriented far away from the planet's rotation axis. Images taken of Miranda, the smallest and innermost of Uranus's major moons, showed a strikingly complex face with a deep, chevron-shaped groove that hints at a bizarre geological past. When Voyager 2 reached the seemingly placid blue disk of Neptune in 1989, it discovered winds blowing at 2,100 kilometres per hour, the fastest in the Solar System, which fuelled storms such as an Earth-sized Great Dark Spot. Scientists had not expected such violent atmospheric activity on a planet that receives just 0.1% of the solar energy that bathes Earth and drives its weather patterns. Stone is fond of saying that the Voyagers have returned 200% of the science he had expected: \u201cWe learned so much more than we possibly could have imagined.\u201d \n               Team players \n             Voyager alumni say that Stone deserves a major share of the credit for that achievement. As project scientist, he has from the beginning chaired the science-steering group, which is composed mainly of the 11 principal investigators. When the group could not agree on which observations to prioritize, Stone stepped in and made that decision \u2014 essentially choosing who would get to make discoveries. Hence his shelf of notebooks. By carefully recording everyone's input, Stone says, he let everyone know that he considered all perspectives. \u201cWe always knew he was fair,\u201d says Ellis Miner of the JPL, who was assistant project scientist for the Saturn, Uranus and Neptune encounters. In a shift from usual practice at the time, Stone forced Voyager scientists to work across the clique-like teams that had sprung up around each of the 11 investigations. Before the Jupiter fly-by, he created overarching working groups around four key themes: moons, rings, atmosphere and magnetosphere. Each group was tasked with focusing on the scientific questions that could be answered by all of Voyager's instruments. This forced members of the instrument teams to talk across boundaries, Stone says, so that \u201cI didn't have to be a referee all the time\u201d. Tension still cropped up. Stamatios Krimigis, the principal investigator for the instrument that measures low-energy charged particles, remembers one particular stand-off with the plasma-instrument group. Krimigis's particle detector worked by rotating sensors to scan different parts of the sky, which shook the nearby plasma instrument and irritated the scientists in charge of it. Stone negotiated a compromise schedule: sometimes the particle instrument rotated quickly, sometimes it rotated slowly to reduce the vibrations, and at other times it remained still. \u201cWe were all in the end equally unhappy,\u201d says Krimigis, of the Johns Hopkins University Applied Physics Laboratory in Laurel, Maryland, and the Academy of Athens. \u201cThat's the sign of a good negotiator.\u201d After Voyager 2 left Neptune in 1989, the two craft earned a new name to reflect their next assignment: the Voyager Interstellar Mission (see 'Going, going\u2026'). The title carried a fair dose of hope, given that no one knew how long it would take to coast to the edge of the Solar System. In the intervening years, however, Stone had plenty to keep him busy. From 1991 to 2001 he served as director of the JPL, overseeing mission successes such as the 1997 Pathfinder landing on Mars as well as spectacular failures such as the loss of both a Mars orbiter and a Mars lander that followed Pathfinder. It was the era of NASA's 'faster, better, cheaper' approach to spacecraft, and Stone admits that the missions failed because the JPL pushed that ethos too far. After retiring from that hectic post, Stone returned to teach and do research at Caltech. Today, he rarely travels up the 210 freeway to Voyager mission control at the JPL. There is little need, because engineers carry out the daily tasks necessary to keep the probes healthy and in touch. Their job gets tougher every day: Voyager 2 is 15.2 billion kilometres from the Sun and Voyager 1 has reached 18.6 billion kilometres \u2014 more than three times the distance between the Sun and Pluto.  Whenever people stop paying attention to me, I pretend to leave the Solar System.  One early morning in April, the team engages in an agonizingly slow conversation with Voyager 1. Engineer Roger Ludwig is testing a new command sequence that would allow Voyager to delay some tasks to better cope with the limits on Earth-bound communications. It took more than 17 hours for the commands to travel at light speed to the spacecraft, and a similar span for the response to return. Now, in the predawn hours, Ludwig is eager for the answer. A slow stream of numbers pops up on a pair of computer monitors, and Ludwig checks to see whether the sequence has worked. The results look promising, but the Voyager team will test the code several more times before making any changes. This long into the mission, engineers don't want to make any silly mistakes. \u201cWe all feel like we're flying a national treasure,\u201d says Ludwig. That includes Stone, who has stuck with Voyager because he is eager for more discoveries. It has been a long wait since Neptune. The team has talked so many times about the impending departure into interstellar space that office doors around mission control are decorated with a photo of a forlorn-looking Voyager with the quote: \u201cWhenever people stop paying attention to me, I pretend to leave the Solar System.\u201d \n               The final frontier \n             The exit has turned out to be more complicated than scientists had anticipated. Voyager 1 is somewhere near the edge of the heliosphere, the giant cocoon of charged particles from the Sun that surrounds the Solar System and protects the planets from the high-energy particles that streak through interstellar space (see  Nature   489 , 20\u201321; 2012) . In December 2004, the low-energy-particles instrument on Voyager 1 indicated that the solar wind had slowed abruptly, a sign that the craft had entered a turbulent boundary region surrounding the heliosphere. Then, in July and August 2012, the speed of the solar wind dropped to essentially zero even as Voyager began recording higher-energy particles. Krimigis calls those changes \u201ctotally and completely unanticipated\u201d. By themselves, they might suggest that Voyager 1 had crossed from the boundary region into interstellar space. But the science team has remained cautious because another expected signal has not yet appeared. When Voyager 1 enters interstellar space for real, Stone and others expect the orientation of the magnetic field to change from predominantly east\u2013west (as driven by the Sun) to randomly changing directions. So far, data from Voyager 1's magnetometer show essentially no change in the direction of the magnetic field. The team has struggled to make sense of these signals. One day in April, Stone wrinkles his forehead while looking over some plots of particle data. \u201cWe're now seeing what's outside, even though we're not outside,\u201d he says. \u201cThe magnetic field says we haven't gone out yet.\u201d He and other mission scientists think that Voyager 1 is on some kind of 'magnetic highway' that connects the Sun's magnetic field lines to those of interstellar space, allowing charged particles to enter the border zone. The team described the latest results in December at the American Geophysical Union (AGU) meeting in San Francisco, California, and will report details in an upcoming suite of papers in  Science . Stone works to keep tight control over the Voyager message. In March, for example, he moved quickly to counter a press release from the AGU announcing that Voyager 1 had left the Solar System. It will leave when he says it does. At that point, the spacecraft will enter a realm completely new to science. The particle detector will measure galactic cosmic rays that are too weak to penetrate the heliosphere and enter the Solar System. Voyager's magnetometer will gauge the strength of the magnetic field between nearby stars for the first time. Scientists will finally get a glimpse of what truly deep space is like. The information coming back from Voyager 1 as it leaves the heliosphere \u201cis the only data we'll ever get from this region, and it's incredible,\u201d says Merav Opher, an astrophysicist at Boston University in Massachusetts, who is not part of the Voyager team. With the key transition so close, Stone is getting nervous. To make sure that scientists catch the change when it happens, he made a strong pitch for more coverage by the worldwide system of giant antennas known as the Deep Space Network. The mission is now getting as many as ten precious hours of antenna time daily. \n               Energy crisis \n             But Stone and his team know that they have limited time. Voyager 1 is 124 times as far from the Sun as the Earth\u2013Sun distance, and gaining 3.6 of these astronomical units every year. Signals are steadily fading. Both spacecraft are powered by about 315 watts from the radioactive decay of on-board plutonium-based generators, but that power drops by about 4 watts every year. Of the 10 original instruments, five are still working on Voyager 2 and four on Voyager 1. By 2020, the power will have ebbed to the point that mission managers will have to start switching off more scientific instruments, one by one. The job of choosing which goes first will fall to the project scientist. Stone says that he has not yet thought about which instruments to let die, should he be around to decide. By 2025, all the plutonium power will be gone, and the Voyagers will become lifeless hulks. They will probably never come as close to another star as they have been to the Sun, and their famous golden records will drift mutely through space. Yet Stone has no time to get nostalgic; there is more discovering to be done. Among other jobs, he serves as vice-chairman for the board of the Thirty Meter Telescope, which will be the world's largest optical telescope when it is completed in the early 2020s on Mauna Kea, Hawaii. One day, its giant mirrors will gaze past the Voyagers towards distant star systems. Stone is also helping to develop Solar Probe Plus, a mission designed to go closer to the Sun than any other spacecraft. It will follow an elaborate looping path that will take it past Venus seven times and then into a series of close solar encounters, swooping within 10 solar radii of the Sun's surface again and again. The probe's heat shield will protect it from temperatures of 2,000 \u00b0C, and its instruments will collect information about how and where the solar wind is born. Standing outside his office, Stone gazes at a bulletin board covered with printouts of the Sun's activity over the past few solar cycles. \u201cI've never been frustrated with exploration,\u201d he says. \u201cI've been lucky to have the right thing to do, to be on projects that have been successful.\u201d Solar Probe Plus is planned for a 2018 launch, which would put its first close encounter with the Sun in December 2024. By then Stone will be pushing 89. But even so, he would like to be around when the science starts streaming in \u2014 about the same time that the Voyagers will be fading beyond communication into the cold cosmos. \n                     So, has Voyager 1 left the Solar System? Scientists face off 2013-Mar-21 \n                   \n                     Voyager\u2019s long goodbye 2012-Sep-05 \n                   \n                     Voyager at the edge 2011-Jun-15 \n                   \n                     Scientific exploration: What a long, strange trip it's been 2008-Jul-02 \n                   \n                     Voyager \n                   Reprints and Permissions"},
{"file_id": "498422a", "url": "https://www.nature.com/articles/498422a", "year": 2013, "authors": [{"name": "Meredith Wadman"}], "parsed_as_year": "2006_or_before", "body": " In 1962, Leonard Hayflick created a cell strain from an aborted fetus. More than 50 years later, WI-38 remains a crucial, but controversial, source of cells.  The woman was four months pregnant, but she didn't want another child. In 1962, at a hospital in Sweden, she had a legal abortion.  The fetus \u2014 female, 20 centimetres long and wrapped in a sterile green cloth \u2014 was delivered to the Karolinska Institute in northwest Stockholm. There, the lungs were dissected, packed on ice and dispatched to the airport, where they were loaded onto a transatlantic flight. A few days later, Leonard Hayflick, an ambitious young microbiologist at the Wistar Institute for Anatomy and Biology in Philadelphia, Pennsylvania, unpacked that box.  Working with a pair of surgical scalpels, Hayflick minced the lungs \u2014 each about the size of an adult fingertip \u2014 then placed them in a flask with a mix of enzymes that fragmented them into individual cells. These he transferred into several flat-sided glass bottles, to which he added a nutrient broth. He laid the bottles on their sides in a 37 \u00b0C incubation room. The cells began to divide. Thea Cunningham talks to Meredith Wadman and the creator of WI-38, Leonard Hayflick, about the controversial cell line.  So began WI-38, a strain of cells that has arguably helped to save more lives than any other created by researchers. Many of the experimental cell lines available at that time, such as the famous HeLa line, had been grown from cancers or were otherwise genetically abnormal. WI-38 cells became the first 'normal' human cells available in virtually unlimited quantities to scientists and to industry and, as a result, have become the most extensively described and studied normal human cells available to this day.  Vaccines made using WI-38 cells have immunized hundreds of millions of people against rubella, rabies, adenovirus, polio, measles, chickenpox and shingles. In the 1960s and 1970s, the cells helped epidemiologists to identify viral culprits in disease outbreaks. Their normality has made them valuable control cells for comparison with diseased ones. And at the Wistar Institute, as in labs and universities around the world, they remain a leading tool for probing the secrets of cellular ageing and cancer.  \u201cHere's a clump of cells that has had an enormous impact on human health,\u201d says Paul Offit, chief of the division of infectious diseases at the Children's Hospital of Philadelphia. \u201cThese cells from one fetus have no doubt saved the lives of millions of people.\u201d  Few people, however, know the troubled history of the cells \u2014 one that may offer lessons for modern researchers seeking to work with human tissues. Six years after deriving his famous strain, Hayflick made off with stocks of the cells and later started to charge for shipping them, prompting an epic legal battle with the US National Institutes of Health (NIH) in Bethesda, Maryland, about who owned the cells. That struggle nearly destroyed Hayflick's career and raised questions about whether and how scientists should profit from their inventions.  What's more, the WI-38 strain has helped to generate billions of dollars for companies that produce vaccines based on the cells, yet it seems that the parents of the fetus have earned nothing. That recalls the earlier development of the HeLa cell line, named after the woman whose tumour gave rise to the cells and chronicled in Rebecca Skloot's book  The Immortal Life of Henrietta Lacks  (Crown, 2010). As with HeLa, the WI-38 case highlights questions about if, and how, tissue donors should be compensated that are still urgently debated today. Last month, for example, some scientists in the United States found themselves barred from using new stem-cell lines derived from human embryos because women had been paid for the eggs used to make them (see  Nature   http://doi.org/mv2 ; 2013 ).  The story of WI-38, unlike that of HeLa, also has its own controversial twist because it was derived from an aborted fetus. For 40 years, anti-abortion activists have protested against the use of WI-38 and vaccines developed from it. \u201cIt's still a live issue,\u201d says Alta Charo, a professor of law and bioethics at the University of Wisconsin Law School in Madison. \u201cWe still have people who refuse to take these vaccines because of their origins in fetal tissue.\u201d \n                Seeking cells \n              When Hayflick opened up that icy package from Sweden in 1962, he was working at the vanguard of virus research in the United States. At the time, the Wistar Institute was led by Hilary Koprowski, a polio-vaccine pioneer who hired Hayflick to run the centre's cell-culture laboratory and supply cells to researchers. But Hayflick also began investigating whether some human cancers might be caused by viruses. To do so, he needed a resource that did not yet exist: verifiably normal human cells that could be reliably grown in the lab. Fetal cells, he thought, were an ideal candidate, because they were less likely to have been exposed to viruses than adult cells.  Although abortions were technically illegal in Pennsylvania at the time, they were still performed when doctors said they were medically necessary. Hayflick says he was able to obtain fetuses straight from the operating room of the University of Pennsylvania Hospital across the street from Wistar. Unless the tissue was put to some use, he reasoned, \u201cit was definitely going to end up in an incinerator\u201d. The University of Pennsylvania says that it is unable to find records to confirm the source of fetal tissues used by Hayflick.  Hayflick developed 25 different fetal-cell strains, numbered WI-1 to WI-25. But several months into the project, he began to notice something strange. Scientific orthodoxy held that cells in culture, properly treated, would replicate forever. But his oldest cell strains were beginning to replicate more slowly. Eventually, they stopped dividing altogether.  In 1961, Hayflick and his colleague Paul Moorhead published a paper 1  that would become one of the most cited publications in biology. Entitled 'The serial cultivation of human diploid cell strains', it showed that normal fetal cells stop replicating after about 50 population doublings. The paper launched a new field: the study of cellular ageing. And the wall that the cells hit \u2014 which was later found to arrive much earlier for adult cells, which have already divided many times 2  \u2014 became known as 'the Hayflick limit'.  Crucially, Hayflick and Moorhead also showed that the fetal cells remained viable after months in the freezer and that, once thawed, they would 'remember' how many replications they had been through and would pick up where they left off. \u201cIt's apparent,\u201d the authors wrote, \u201cthat by freezing cells at each subcultivation, or every few subcultivations, one could have cells available at any given time and in almost limitless numbers.\u201d What's more, the pair's cells turned out to be easy to infect with a broad range of human viruses, suggesting that they would be perfect vehicles in which to grow viruses for vaccines.  Hayflick decided to derive a fetal cell strain that he hoped would become both a ubiquitous laboratory resource and a substrate for industrial-scale vaccine manufacturing. He had support: in February 1962, the National Cancer Institute awarded Wistar, with Hayflick as co-principal investigator, a contract \u201cto produce, characterize, store and study human diploid cell strains and to distribute such cell strains to all qualified investigators\u201d. \n                Successful strain \n              By this time, Hayflick had turned to a different source for his fetal tissues: Sven Gard, chairman of the department of virology at the Karolinska Institute in Sweden, where abortion was legal. In June 1962, Hayflick received the set of lungs that would give rise to WI-38. He cultured the cells for weeks, splitting them when they covered the bottom of a bottle, so that two bottles became four, four became eight and so on. By the time the original cell population had doubled nine times, there were hundreds of bottles.  On 31 July, in a marathon session for which he recruited a small army of technicians, Hayflick dispensed the cells into more than 800 tiny glass ampoules, sealing each one with a quick pass through the flame of a Bunsen burner. Later, he transferred the precious ampoules to a liquid-nitrogen freezer in the Wistar's basement.  A year later, Hayflick received information from Sweden assuring him that the mother of the fetus and her family were free of cancer and hereditary diseases, something vaccine manufacturers would want to know. Although there is some indication that the mother consented to use of the tissue,  Nature  does not know for sure that she did. Swedish law at the time did not require such consent and, says Niels Lyn\u00f6e, professor of medical ethics at the Karolinska Institute, \u201cresearch ethical awareness in Sweden as well as in the US was rather low\u201d, before the Helsinki declaration, a statement of human research ethics adopted by the World Medical Association in 1964. In Sweden, \u201cresearch material like tissues from aborted fetuses were available and used for research without consent or the knowledge of patients for a long time\u201d, both before and after consent rules were tightened later in the 1960s, says Solveig J\u00fclich, a historian of medicine at Stockholm University.  Armed with the ampoules, Hayflick now launched WI-38 on its march around the globe. During his frequent flights abroad, he often toted a small liquid-nitrogen freezer bearing WI-38 ampoules. In this way, he hand-delivered the cells to colleagues in London, Moscow, Leningrad and Belgrade. He also mailed out hundreds of 'starter' cultures grown from the ampoules. Scientists were hungry for the cells in part because they were a cheap, plentiful model for studying the fundamental biology of normal human cells \u2014 and soon papers began to appear, probing everything from the cells' respiration 3  to their constituent fatty molecules 4 .  WI-38 found a greater use in virology, where the ease of infecting the cells with a panoply of human viruses quickly made the strain an important virus-identification tool. In 1967, the cells became a workhorse in a World Health Organization survey of viruses causing lower respiratory tract infections in hospitalized children on four continents.  Hayflick also supplied WI-38 liberally to aspiring vaccine-makers. One was Stanley Plotkin, a Wistar scientist and a physician who had seen at first hand the effects of the huge rubella epidemic that swept the United Kingdom and the United States in the early 1960s. Rubella can be devastating to fetuses whose mothers are infected: those that are not killed  in utero  are frequently born blind, deaf, mentally disabled or with some combination of these conditions.  Working at the Wistar, Plotkin grew rubella in WI-38 at 30 \u00b0C, cooler than body temperature, creating a weakened strain that still fired up the immune system enough to protect against future infections. Trials showed that his vaccine induced better immunity against rubella than competitors 5 . Plotkin's vaccine was licensed in Europe in 1970 and in the United States in 1979. A version made by the pharmaceutical company Merck, based in New Jersey, is today the only rubella vaccine available in the United States, and GlaxoSmithKline uses Plotkin's weakened virus in a rubella vaccine that it markets in Europe and Australia.  The rubella vaccine was only one of many made using WI-38. In the 1960s, a WI-38-based measles vaccine was licensed in the former Soviet Union and Koprowski developed a rabies vaccine using the cells. In the early 1970s, the pharmaceutical company Wyeth (now part of Pfizer) launched an oral adenovirus vaccine developed using WI-38 and Pfizer, based in New York, used WI-38 to make a vaccine against polio. Today, the cells are also used by Merck to make vaccines against chickenpox and the painful nerve infection shingles. \n                Sense of exclusion \n              Despite his groundbreaking paper and the growing prominence of WI-38, Hayflick felt like a second-class citizen at the Wistar Institute. He was never promoted to a full member, and he believed that Koprowski, much as he publicly bragged about WI-38, saw him as more of a technician than a scientist. (Koprowski died last April.)  Hayflick's simmering sense of exclusion boiled over when one day, Hayflick says, he learned that Koprowski had offered a guaranteed supply of WI-38 to the British drug-maker Burroughs Wellcome (one of the companies that merged into GlaxoSmithKline), along with Hayflick's cell-culture technology for producing live polio vaccine  6 , all in exchange for royalties to the institute. Hayflick says that he was shocked that Koprowski intended the institute to profit from WI-38 and believes that it had kept him in the dark.  Hayflick found a new job as a professor of medical microbiology at Stanford University in California, to start in July 1968. In January that year, he met to discuss the fate of the 370-odd remaining WI-38 ampoules with Koprowski and representatives from the NIH and the American Type Culture Collection (ATCC), then in Rockville, Maryland, a non-profit organization that distributes cell cultures. The participants agreed that Hayflick could take ten ampoules of WI-38 with him to Stanford, and that ten would stay at the Wistar. The rest would remain the property of the NIH's cancer institute and were to be transferred to the ATCC, which would handle distribution from that point on.  Hayflick was troubled by the plan, which he says he felt under pressure to sign. And he felt a sense of injustice. Companies, and the Wistar, he now believed, were profiting from cells he had created and handed to them freely. \u201cTo then have [them] descend on what I had struggled so hard to give value to, and try to take it for their own benefit,\u201d he says. \u201cI think that an average person would be capable of understanding why I was \u2014 to put it mildly \u2014 concerned.\u201d The Wistar Institute says that it acted ethically in conducting research that led to the development of WI-38 and that it received royalties from licensed vaccines grown in WI-38 cells but not from licensing the cells.  At some point after that January meeting, Hayflick made a quiet trip to the Wistar basement and packed all the WI-38 ampoules into a portable, 30-litre liquid-nitrogen tank. In June 1968, he strapped the container into the back seat of his green Buick LeSabre next to two of his children, and motored to California. \u201cI just absconded with the cells,\u201d Hayflick says with a wry smile.  Once in Stanford, Hayflick began charging for many of the WI-38 cultures that he was sending out to hundreds of scientists who were still asking for them. His fee was US$15 \u2014 the same amount charged by the ATCC for cell shipments \u2014 and he banked the money in an account he called 'Cell Culture Fund'. By May 1975, he had accrued more than $66,000.  Hayflick was determined, he says, to keep the funds in a separate account until some independent legal authority could determine who owned the cells. The issue didn't come up until the spring of 1975, when he was interviewed at the NIH as a candidate to direct its new National Institute on Aging. The NIH decided to turn to its Division of Management Survey and Review, an office that investigated allegations of mismanagement of NIH funds. It sent three accountants to Hayflick's Stanford lab, where they spent days going over records and assessing his inventory of WI-38.  Their report became public in March 1976, when the NIH provided it under the Freedom of Information Act (FOIA) to several journalists. Accounts of its contents soon appeared in  Science  and on the front page of  The New York Times . \u201cWithin 24 hours my career was in the sewer,\u201d Hayflick says. The report said that Hayflick had sold \u201cthe property of the United States Government\u201d and banked the money; that the WI-38 ampoules had been poorly accounted for; and that some ampoules were contaminated with bacteria. Hayflick strongly disagrees with the report. He says that no legal decision gives the government title to WI-38; that he sequestered the funds received for preparing and shipping WI-38 in an account until ownership could be established; and that no evidence has ever been provided for the assertion of mismanagement. Hayflick explains that, contrary to common practice in 1962, he had not laced the cells with antibiotics at the outset because vaccine manufacturers feared allergic reactions to the drugs.  Shortly before the  Science  article 7  was published, Hayflick sued the NIH. He argued that the agency had violated the 1974 Privacy Act by making his name and the allegations against him available under the FOIA without including his rebuttal. He also sued for title to WI-38 and its proceeds. By then, Hayflick was also facing a criminal investigation: Stanford University had alerted local prosecutors that the case could be one of criminal theft of government property. (The prosecutors subsequently found no grounds for criminal investigation and dropped the case.) Meanwhile, some vaccine manufacturers, fearing that there would not be enough stock of WI-38 to meet future needs, switched much of their work to an alternative fetal cell strain, MRC-5.  Hayflick resigned from Stanford in February 1976 and was soon in an unemployment line collecting $104 a week. Not only was he jobless, he was without the cells that he described to  Science  that spring as \u201clike my children\u201d. The NIH had taken them from his lab while he was at a conference the previous year. \n                Changing times \n              Some months later, Hayflick landed a job across the San Francisco Bay at the Children's Hospital, Oakland, and sought to revive his research on ageing. In 1977, peer reviewers approved his application for a three-year NIH grant and, after a lengthy fight with the NIH to get both the funding and some WI-38 cells, in January 1981 he received six of the original ampoules of cells.  One month earlier, the Bayh\u2013Dole Act had become law, giving institutions the right to claim title to inventions made using government funds, as long as they gave the inventors a piece of the royalties. Hayflick's invention predated the law, but the new mindset that Bayh\u2013Dole represented made it harder for the government to justify the continued legal fight over WI-38, which by then had stretched on for nearly five years. In summer 1981, the Department of Justice wrote to Hayflick's lawyers, offering to settle the lawsuit out of court, and Hayflick assented. With both sides agreeing that the issues were in reasonable dispute, and neither side admitting liability, the settlement allowed Hayflick title to the six original WI-38 ampoules now in his possession, and to their progeny. The government would retain title to the 19 original ampoules in its hands. As for the proceeds from his sales of WI-38, which, with interest, had grown to around $90,000, Hayflick would keep it. He spent it all, he says, and more, to pay his lawyers; he has never profited financially from WI-38, he says.  Scientists, meanwhile, were continuing to benefit academically from the cells. By the mid-1980s, thanks to revolutionary new tools in molecular biology, WI-38 was helping them explore everything from gene expression in human leukaemias 8  to the effects of the just-cloned tumour necrosis factor 9 , an important immune regulatory protein.  The cells have played \u201ca very critical role in studying cellular senescence,\u201d adds Rugang Zhang, who works in this field at the Wistar Institute. That's because they so reliably stop replicating after about 50 divisions and because scientists have, over time, built up a wealth of knowledge about the reasons why. In the 1990s, for instance, WI-38 was used to discover the most widely used marker of cellular senescence 10 . More recently, Zhang's team used the cells to discover a pathway by which the complex of DNA and proteins known as chromatin controls cell proliferation 11 .  But the controversies surrounding the cells have rumbled on. Back in July 1973, Hayflick received a call at home from a senior medical officer at NASA. Skylab 3 had taken off several hours earlier from the Kennedy Space Center in Florida, bound for the Space Station. The NASA physician was contending with anti-abortion demonstrators who were protesting about the presence aboard of WI-38 cells, which were going to be used to detect the effects of zero-gravity on cell growth and structure. Once Hayflick explained that the abortion from which the cells were derived had occurred legally in Sweden, the physician said that he would defuse the situation \u2014 but concerns among anti-abortionists about WI-38 have lasted to this day.  \u201cOther vaccines are produced in a completely morally non-objectionable way. So why aren't we doing this with all vaccines?\u201d says Debi Vinnedge, the executive director of Children of God for Life, a group based in Largo, Florida, that opposes the use of WI-38 in vaccine-making. In 2003, Vinnedge wrote to the Vatican asking for an official position on whether Catholics could ethically receive vaccines made using cells from aborted fetuses. She waited two years for an answer. The letter, when it came, concluded that where no alternative exists, it is \u201clawful\u201d for parents to have their children immunized with vaccines made using WI-38 and MRC-5, to avoid serious risk to their own offspring and to the population as a whole.  Still, the Vatican wrote, faithful Catholics should \u201cemploy every lawful means in order to make life difficult for the pharmaceutical industries\u201d that use such cells. Merck, a major producer of Plotkin's rubella vaccine, has been a perennial target of abortion opponents, who have pressed the issue at Merck's US shareholder meetings. (Merck said in a statement to  Nature  that \u201cit would be exceedingly difficult, if at all possible, to develop and test an alternative\u201d, and emphasized the vaccine's long record of safety and effectiveness.) The irony of the protest is not lost on Plotkin. \u201cI am fond of saying that rubella vaccine has prevented thousands more abortions than have ever been prevented by Catholic religionists,\u201d he says.  Profits from Merck's rubella vaccine represent a big slice of the billions of dollars that have been made from products that have involved the use of WI-38. Among the other companies that have made money from WI-38 are Barr Laboratories (now part of Teva Pharmaceuticals, based in Petach Tikva, Israel), which today makes the adenovirus vaccine given to all US military recruits, and Sigma Aldrich in St Louis, Missouri, which charges $424 in the United States for a vial of the cells.  Legal experts say it is unlikely that the parents of the fetus, or their heirs, would have any legal grounds to demand compensation for tissue collected over 50 years ago. At the time that WI-38 was derived, use of tissue without consent was routine in the United States, as it was in Sweden. Under current rules, researchers supported by US government grants are free to make use of surgically removed tissue \u2014 including aborted tissue \u2014 that has been stripped of its identifiers, without consent. However, some states have stricter rules.  But, says Charo, \u201cif we continue to debate it entirely in legal terms, it feels like we're missing the emotional centre of the story\u201d. It could be argued, she says, \u201cthat if somebody else is making a fortune off of this, they ought to share the wealth. It's not a legal judgment. It's a judgement about morality.\u201d  The scientists and academic institutions that have worked with WI-38 and that commented for this story say that they do not see their work on the cells as unethical, in part because of the standards that existed at the time the cell strain was created. It is unfair, say some, to examine past acts by today's more stringent ethical expectations. \u201cAt the time [the fetus] was obtained there was no issue in using discarded material,\u201d says Plotkin. \u201cRetrospective ethics is easy but presumptuous.\u201d Most companies in this story declined to comment; GlaxoSmithKline says that it is committed to upholding high ethical standards.  Regarding the situation today, Scott Kominers, a research scholar at the Becker Friedman Institute at the University of Chicago, Illinois, argues that offering donors a share in future profits from their tissues could encourage them to donate and fuel medical progress 12 . \u201cWe think that if you offer some sort of value-based compensation you'd be likely to boost tissue supply,\u201d he says. But Steven Joffe, a paediatric oncologist who directs the ethics programme at Harvard's translational medicine centre in Boston, Massachusetts, is concerned that compensating donors may paradoxically decrease their willingness to donate tissues, by taking altruism out of the equation. What's more, he says, the one-to-one relationship of WI-38, or of HeLa, to a donor, is rare. Far more often, modern medical products \u2014 such as therapeutic proteins extracted from donated blood \u2014 come from many samples combined. In these cases, he says, \u201ctrying to account for all these multiple holders of rights to income streams would just bring science to a standstill\u201d.  If nothing else, the WI-38 story highlights the benefits of discussing the issues of compensation and consent with tissue donors at the outset. In the case of WI-38, suggests Charo, returning to the donor now, even with an offer of compensation, \u201cmay also be a way of reopening an experience that may for her have been painful. You have to be careful.\u201d  Hayflick argues that there are at least four stakeholders with title to WI-38 or any human cell culture: the tissue donors, the scientists whose work gave it value, the scientists' institution and the body that funded the work. \u201cLike me\u201d, he adds, \u201chundreds of other scientists had their careers advanced using WI-38 and other human cell cultures so we all owe a moral debt to the tissue donors.\u201d  Now 85 and regarded as a grand old man of ageing research, Hayflick hung onto his ampoules of WI-38 for decades, keeping them for many years in the garage of his home in California. But in 2007, weary of monthly treks to collect fresh liquid nitrogen, he donated them to the Coriell Institute in Camden, New Jersey, which, he says, he trusts to bank them safely.  In the end, he says, letting the cells go was no more traumatic than launching his own five biological offspring into the world: \u201cIt was about time that my 'children' \u2014 now adults \u2014 should leave home.\u201d See Editorial   page 407 . \n                     A culture of consent 2013-Jun-26 \n                   \n                     US scientists chafe at restrictions on new stem-cell lines 2013-Jun-04 \n                   \n                     Delays in updates to ethics guidelines for research spark concern 2013-May-07 \n                   \n                     Correcting misperceptions about cryopreserved embryos and stem cell research 2013-Apr-05 \n                   \n                     Biobanks: Validate gene findings before telling donors 2012-Apr-25 \n                   \n                     Incidental benefits 2012-Mar-21 \n                   \n                     Hayflick, his limit, and cellular ageing 2000-Oct-01 \n                   \n                     Science  poll results on whether redundant tissue donors should be paid \n                   \n                     Science  policy forum on tissue donor compensation \n                   \n                     Hayflick\u2019s letter to  Science \n                   \n                     ATCC catalogue entry for WI-38 \n                   \n                     Article by Rebecca Skloot on the publication of the HeLa genome \n                   \n                     Recordings made by Hayflick for Web of Stories \n                   Reprints and Permissions"},
{"file_id": "497554a", "url": "https://www.nature.com/articles/497554a", "year": 2013, "authors": [{"name": "Ron Cowen"}], "parsed_as_year": "2006_or_before", "body": "The Hubble Space Telescope is giving astronomers a glimpse of the Universe's first, tumultuous era of galaxy formation. For one sleepless week in early September 2009, Garth Illingworth and his team had the early Universe all to themselves. At NASA's request, Illingworth, Rychard Bouwens and Pascal Oesch had just spent the previous week staring into their computer screens at the University of California, Santa Cruz, scanning through hundreds of black-and-white portraits of faint galaxies recorded in a multi-day time exposure by a newly installed infrared camera on the Hubble Space Telescope. NASA simply wanted the three astronomers to preview the images and make sure that the camera was working correctly, before the agency released the data more widely. But Illingworth, Bouwens and Oesch were hoping that they would find more \u2014 that at least some of those smudges of light would prove to be among the first galaxies to form in the Universe, less than 1 billion years after the Big Bang. Even a faint glimpse of such objects could provide fresh insights into some of the biggest questions in cosmology, ranging from the nature of the first stars to the tumultuous beginnings of galaxy formation. That week, the astronomers began to focus on two dozen tiny candidate images \u2014 each so dim and grainy that they might easily be noise in the camera's digital sensors. But as their analysis proceeded, it became clear that these patches of light had the right colour, appearing only in the camera's reddest filters \u2014 exactly what would be expected of newborn galaxies seen at a very great distance and very high redshift. And when the three colleagues started digitally adding together exposures of each candidate, says Illingworth, \u201csuddenly there they were\u201d \u2014 fuzzy, but undeniable images of galaxies. \u201cThat week in September was one of the most exciting times of my career!\u201d By the week's end, he, Bouwens and Oesch had posted two draft papers to the arXiv preprint server 1 , 2 , detailing their first-ever collection of more than 20 galaxies from the age of galaxy formation, some 13 billion years ago, when the cosmos was only 600 million to 800 million years old. Since then, other researchers have made further observations of the same small patch of sky, known as the Hubble Ultra-Deep Field (HUDF), and four other larger regions. They have expanded that initial roster to some 1,400 young galaxies, from the same era. The data from this growing catalogue are already hinting at a still-unseen time \u2014 an infant Universe thronged with countless small galaxies and lit by primordial stars so massive that they burned out and blew up in a cosmic eye-blink. And a new generation of instruments promises to bring that era into clear view. They include the Atacama Large Millimeter/submillimeter Array (ALMA) of radio telescopes in Chile, which is already beginning such observations; and Hubble's successor, the infrared James Webb Space Telescope (JWST), which is set for launch in late 2018. It's a heady time for early-Universe astronomers, says cosmologist Avi Loeb of Harvard University in Cambridge, Massachusetts. \u201cWe're looking at our origins,\u201d he says. \u201cThe first galaxies were the building blocks of the Milky Way, and the desire to understand them is a search for our roots.\u201d \n               Deep background \n             Over the past few decades, observers have developed a general storyline describing how galaxies formed (see 'Dawn's early light'). Astronomers know, for example, that the raw material was a hot, ionized plasma of hydrogen and helium that emerged from the Big Bang, then rapidly cooled as the Universe expanded. Once its temperature had fallen far enough, about 370,000 years after the Big Bang, protons and electrons combined to make neutral atoms and created a light-absorbing haze that plunged the Universe into a cosmic 'dark ages'. Astronomers also know that this cosmic haze was almost perfectly uniform at the start \u2014 but immediately began to clump together as gravity began to magnify slight fluctuations in the material's density. And they are reasonably sure that, after several hundred million years, the densest of the growing clumps began to form the first stars, which ignited by thermonuclear fusion and reionized the neutral gas that remained. The veil of gas became a transparent plasma again, bringing the cosmic dark ages to a spectacular end (see  Nature   490 , 24\u201327; 2012 ). But from this point onwards, very little is certain. The formation of succeeding generations of stars and galaxies was a swirling chaos of heating and cooling gas clouds, detonating supernovae, black-hole accretion and fierce stellar winds strong enough to eject matter from small galaxies \u2014 a process far too messy and complex to understand without extensive observations. Such observations are a major goal of the HUDF project, which aims to gather enough images of distant galaxies to discern patterns in their sizes, shapes and colours. Located south of Orion in the constellation Fornax, and measuring just one-tenth of the diameter of the full Moon as seen from the ground, the HUDF is an otherwise typical patch of dark sky that happens to be relatively devoid of foreground stars and galaxies. But just as astronomers expected, an 11.3-day time exposure of the field taken by Hubble in late 2003 and early 2004 revealed that it was, in fact, filled with a multitude of faraway galaxies seen as they were billions of years ago. In August and September 2009, the field was re-examined in an additional two-day exposure taken by Hubble's Wide Field Camera 3 (WFC3): an instrument installed by astronauts the previous May that is exquisitely sensitive at infrared wavelengths \u2014 exactly where visible and ultraviolet light from the farthest galaxies is expected to end up after being redshifted by the cosmic expansion. These were the images that Illingworth, Bouwens and Oesch saw. Knowing that the WFC3 could detect distant galaxies about 30 times fainter than its predecessor could, or about 4 billion times fainter than anything visible to the human eye, the astronomers initially thought that they might have caught one of the very first generations of galaxies in the act of being born. When they estimated the objects' distance and composition by examining their colours in three different filters \u2014 the faint smudges were far too dim for Hubble to get a spectrum \u2014 the team found that they were relatively blue, exactly as expected of extremely young galaxies glimpsed in their first frenzy of star formation. But this conclusion was far from ironclad. Testing the idea was a prime motivation for a team of astronomers led by Richard Ellis at the California Institute of Technology in Pasadena. In 2012 they re-examined a small part of the centre of the HUDF, this time with an additional colour filter and a time exposure totalling about 23 days. These newer observations, which Ellis's team reported in January this year at a meeting of the American Astronomical Society in Long Beach, California 3 , 4 , 5 , reveal that the galaxies are in fact redder, and therefore contain older stars, than initially calculated. The very youngest galaxies that Hubble has identified, imaged as they appeared 560 million to 780 million years after the Big Bang, contain stars that are 100 million to 200 million years old. So these galaxies had already been around for at least that long. The new HUDF observations also reveal puzzling features of the tumultuous era of reionization, as explained at the January meeting by Brant Robertson of the University of Arizona in Tucson. This was the time when the first galaxies were growing bigger and more numerous, and when ultraviolet light from the first stars was becoming strong enough to ionize the veil of thick hydrogen gas that enveloped them. Other observations show that reionization began roughly 250 million years after the Big Bang, and that it was complete at a cosmic age of roughly 1 billion years \u2014 at which point starlight could stream freely into space and the cosmos was mostly transparent, just as we see it today. But although the galaxies Hubble saw in the 2012 (and the 2009) HUDF observations were presumably the largest and brightest ones around all those billions of years ago, there simply were not enough of them to reionize the Universe. This means, according to Ellis, Robertson and their colleagues, that there must have been a large population of unseen small fry that did most of the work \u2014 a conclusion also reached by Illingworth and his team 6 . \u201cWe now know there's a whole population of small galaxies at even earlier times\u201d than Hubble's detectors can record, says Ellis \u2014 which leads to an exciting set of questions for the newer telescopes such as ALMA and JWST, including how these bodies formed and how they coalesced into the larger galaxies that came later. Another set of questions relates to the very first generation of stars, which coalesced from almost pure helium and hydrogen forged in the Big Bang. Theory suggests that they were more than 100 times as massive as our Sun \u2014 far larger than any stars that form today. If so, then theory also suggests that these monsters were so short-lived that none of them would have survived in the galaxies that Hubble can see. Their extreme size would have caused these stars to destroy themselves in spectacular supernova explosions after only some 2 million years. But did they? And did their death throes delay the birth of the next generation of stars, by disrupting the thick interstellar gas clouds in which new stars were forming? The HUDF data already suggest that the answer to the last question is 'No', says theorist Volker Bromm at the University of Texas at Austin, who was not involved in the 2009 and 2012 HUDF studies. Because the colours of the galaxies seen in the ultra-deep field indicate that they had been forming stars for at least 100 million years already, this suggests that there was little or no lag between the death of the very first generation of stars and the birth of the second, he says. The generations may even have overlapped. But untangling exactly what happened will be a job for future telescopes. \n               The next frontier \n             In the meantime, Hubble astronomers haven't been idle. NASA is pursuing a tactic that could turn the observatory into a telescope as powerful as the JWST will be, along some limited fields of view. To achieve this, astronomers are scanning the heavens to select six fields of view that, unlike the HUDF, each contains a high-mass cluster of foreground galaxies. As first predicted by Einstein, such clusters act as cosmic zoom lenses, gravitationally magnifying and brightening images of distant galaxies that lie directly behind them. Hubble's visible-light and infrared cameras will take turns at looking through these lenses, which should reveal distant galaxies 10 to 50 times fainter than any previously known \u2014 among them the multitude of small fry whose existence is indicated by the reionization data. Collection of data from the first four 'frontier fields' is scheduled to be completed over the next two years, says Mark Dickinson of the National Optical Astronomy Observatory in Tucson, Arizona. In Chile, ALMA will join the hunt for distant galaxies from this summer (see  Nature   495 , 156\u2013159; 2013 ). In contrast to Hubble, which records starlight, ALMA's microwave measurements will reveal the gas and dust that gives rise to stars in these remote bodies. Paradoxically, says James Dunlop of the University of Edinburgh, UK, a member of the 2012 HUDF team, this will allow ALMA to make the most accurate measurement yet of starbirth at such distances. Newborn stars radiate most of their light at ultraviolet wavelengths, he explains, but much of that light is absorbed by gas and dust and reradiated at infrared wavelengths, which are then redshifted by cosmic expansion into ALMA's millimetre range. In addition, ALMA's high spatial resolution will enable the array to break radio emissions into their component wavelengths and therefore record the actual redshifts \u2014 bona fide measurements of distance \u2014 of many of the remote galaxies that Hubble has studied, says Chris Carilli at the National Radio Astronomy Observatory in Socorro, New Mexico. Astronomers can then translate those distance measurements into ages, which will give them a much better handle on where these objects fit in cosmic history. \u201cHubble has been amazing at finding candidate galaxies from redshift 7 to 10, but none of these has been confirmed with spectra and the potential for [spurious candidates] is severe,\u201d says Carilli. Carilli and his collaborators reported 7  in February that ALMA can measure redshift-7 galaxies (objects 3,955 megaparsecs or 12.9 billion light years from Earth) using just 20 of its eventual complement of 66 antennas. A report from another team in  Nature 8  provides further evidence. ALMA will \u201cquickly make the jump to redshift 8\u201d by the end of the year, says Carilli, and if the array gets a new set of receivers \u2014 a possibility still several years in the future \u2014 it could study and measure distances to galaxies out to redshift 11. These objects would be seen as they appeared just 425 million years after the birth of the Universe. ALMA could become the \u201credshift machine of choice\u201d for the first galaxies, he says. Nevertheless, most astronomers are eagerly awaiting the 6.5-metre JWST, whose  raison d'\u00eatre  is to image the faint, primitive bodies that Hubble can only glimpse \u2014 but that were the earliest ancestors of modern galaxies such as the Milky Way (see  Nature   467 , 1028\u20131030; 2010 ). Hubble's observations provided \u201cthe first hints of the first galaxies\u201d, says Ellis, but \u201cwe really need the JWST to push back into that even earlier period from 200 million years to 500 million years after the Big Bang\u201d. Back in 2009, even as NASA astronomer and astronaut John Grunsfeld glimpsed the first images of distant galaxies from the infrared camera he had helped to install on Hubble, the JWST had come to his mind. \u201cI couldn't help but feel awed by the power of Hubble,\u201d Grunsfeld recalls, but \u201cthe views of the HUDF also gave me great satisfaction that the JWST would have plenty to see.\u201d \n                     Astronomy: Dusty galaxies come into view 2013-Apr-24 \n                   \n                     A dust-obscured massive maximum-starburst galaxy at a redshift of 6.34 2013-Apr-17 \n                   \n                     Astronomy: A cosmic growth spurt in an infant galaxy 2013-Apr-17 \n                   \n                     Astronomy: The ALMA telescope shows its true colours 2013-Mar-13 \n                   \n                     Galaxy found at record-breaking distance 2012-Dec-12 \n                   \n                     A candidate redshift z\u2009\u2248\u200910 galaxy and rapid changes in that population at an age of 500\u2009Myr 2011-Jan-26 \n                   \n                     Space science: The telescope that ate astronomy 2010-Oct-27 \n                   \n                     Homepage of the Hubble Ultra Deep Field 2012 \n                   \n                     Information on the Hubble frontier fields \n                   \n                     ALMA home page \n                   Reprints and Permissions"},
{"file_id": "497550a", "url": "https://www.nature.com/articles/497550a", "year": 2013, "authors": [{"name": "Kerri Smith"}], "parsed_as_year": "2006_or_before", "body": "Karl Deisseroth is leaving his mark on brain science one technique at a time. When Karl Deisseroth moved into his first lab in 2004, he found himself replacing a high-profile tenant: Nobel-prizewinning physicist Steven Chu. \u201cHis name was still on the door when I moved in,\u201d says Deisseroth, a neuroscientist, of the basement space at Stanford University in California. The legacy has had its benefits. When chemistry student Feng Zhang dropped by looking for Chu, Deisseroth convinced him to stick around. \u201cI don't think he knew who I was. But he got interested enough.\u201d Deisseroth is now a major name in science himself. He is associated with two blockbuster techniques that allow researchers to show how intricate circuits in the brain create patterns of behaviour. The development of the methods, he says, came from a desire to understand mechanisms that give rise to psychiatric disease \u2014 and from the paucity of techniques to do so. \u201cIt was extremely clear that for fundamental advances in these domains I would have to spend time developing new tools,\u201d says Deisseroth. Karl Deisseroth discusses using light to turn neurons on and off. His measured tone and laid-back demeanour belie the frenzy that his lab's techniques are generating in neuroscience. First came optogenetics 1 , which involves inserting light-sensitive proteins from algae into neurons, allowing researchers to switch the cells on and off with light. Deisseroth developed the method shortly after starting his lab, working with Zhang and Edward Boyden, a close collaborator at the time. Optogenetics has since been adopted by scientists around the world to explore everything from the functions of neuron subtypes to the circuits altered in depression or autism. Deisseroth has lost count of how many groups are using it. \u201cWe sent clones to thousands of laboratories,\u201d he says. Now his lab is gearing up for another rush, after publishing a method called CLARITY 2 . The technique uses a chemical treatment to turn whole brains transparent, allowing researchers to examine the brain's structure in detail in three dimensions. It could help researchers to connect the brain's function with its structure. \u201cI think it'll be de rigueur for people to have a picture showing the pathways and how they sit in the brain,\u201d says Richard Tsien, the neuroanatomist under whom Deisseroth earned his PhD at Stanford. Deisseroth takes risks and encourages his team to do the same, often pulling together people with varying expertise and keeping abreast of many different fields. Christof Koch, chief scientific officer at the Allen Institute for Brain Science in Seattle, Washington, puts Deisseroth's success down to these \u201ccatholic interests\u201d. But Deisseroth also has a laser-like focus on tool-building, fuelled, he says, by his work as a practising psychiatrist. And although he is excited about his lab's newest technique, Deisseroth plays it down, shrugging a broad shoulder: \u201cI'm not very demonstrative,\u201d he says. \n               Taking control \n             When Tsien, who now works at New York University's Langone Medical Center, first met Deisseroth, he was struck by his determination. \u201cIt was an unusual encounter \u2014 he came for an interview and told me rather shyly that the reason he came to Stanford was because he wanted to work with me,\u201d Tsien remembers. It was 1993 and Tsien's lab was \u201ccrammed to the gills\u201d, he says, but Deisseroth was persistent and persuasive. As part of a joint programme to earn a PhD and a medical doctorate, he began a project looking at calcium channels in neurons. In 2000, he followed that up with a multi-pronged appointment: a postdoctoral fellowship in the lab of Stanford neuroscientist Robert Malenka and \u2014 after toying with the idea of neurosurgery \u2014 a residency in psychiatry at Stanford Medical School. \u201cEverything changed when I did my psychiatry rotation,\u201d says Deisseroth. \u201cA person can be right in front of you who looks intact, not obviously injured, and yet their brain is constructing for them a completely different reality. At the same time I saw how deep the suffering was.\u201d Studying depression or anxiety in a dish of cells was inadequate, he reasoned, because only whole brains can give rise to the sophisticated functions \u2014 and disorders \u2014 that characterize human behaviour. And techniques for studying whole brains in humans and model organisms were often limited to simply watching them at work. So Deisseroth began thinking about ways to examine and control intact systems. \u201cI was having a lot of discussions with a lot of people,\u201d he says. During his residency, Deisseroth met Boyden, a PhD student with similarly ambitious aims. The two began talking about ways to manipulate individual neurons as a side project. \u201cIt was a very adventurous collaboration, full of exploration,\u201d says Boyden. One idea involved using light to control neuronal firing. Boyden and Deisseroth knew about light-sensitive channel proteins called opsins, which algae use for generating energy, among other functions. Several groups \u2014 including that of Tsien's brother Roger at the University of California, San Diego \u2014 were trying to insert these proteins into neuron cell membranes. The project needed \u201cthe wherewithal to spend the money and find the graduate students\u201d, says Deisseroth. In 2004, having secured his own lab, he could do just that. By July of that year, Deisseroth had managed to coax neurons into expressing opsins in their cellular membranes. Using cells borrowed from Malenka and recording equipment from Richard Tsien, he, Zhang and Boyden hunkered down to see if the technique would fly. \u201cIt worked on pretty much the first try,\u201d Boyden recalls. \u201cIt was like riding a wave of serendipity.\u201d In the following year, Deisseroth finally secured funding from the US National Institutes of Health (NIH) to further the work. He had faced multiple rejections. Projects based on technologies have a harder time attracting support than hypothesis-driven projects, says Thomas Insel, director of the US National Institute for Mental Health in Bethesda, Maryland, the source of the initial federal funding for Deisseroth's optogenetics research. \u201cIt was a little hard for people in the field to understand what he was trying to do,\u201d says Insel. But neuroscientists saw the potential in 2005, when Deisseroth's group published its first big paper showing that the technique worked in a dish 1 . Researchers could now stimulate a specific type of neuron and see how that affected the cell's \u2014 or even an entire animal's \u2014 behaviour. Requests for the technology were pouring in. The technique has since been used for everything from studying the development of neural stem cells 3  to prompting mice to recall fearful memories 4  (see  Nature   465 , 26\u201328; 2010 ). Deisseroth and his team, with their focus on psychiatric conditions, have used rodent models to explore the network of brain areas that gives rise to anxiety and have shown that one 'hub' controls diverse symptoms such as an elevated breathing rate and feelings of panic and discomfort 5 . They have switched on and off the activity of mouse neurons that use the neurotransmitter dopamine, to show how they contribute to the symptoms of depression 6 . The team has even used optogenetics to prevent cocaine-addicted rats from seeking out the drug 7 . The work, says Deisseroth, could help scientists to design anti-anxiety drugs that are less addictive than current treatments \u2014 affecting anxiety pathways but leaving reward circuitry untouched. Karl Deisseroth talks about finding the area of the brain that controls anxiety symptoms. In 2006, Boyden started his own lab at the Massachusetts Institute of Technology in Cambridge. Rumours emerged that his relationship with Deisseroth was souring. Neither is keen to discuss the details, but in 2007, the two groups published separate papers on halorhodopsin, an 'off-switch' opsin that they had previously studied together 8 , 9 . \u201cWe didn't know Ed was working on that until we sent our paper to  Nature ,\u201d says Zhang. \u201cWell you know, in any competitive scenario people are interested in being first,\u201d says Boyden. The experience might have left its mark on Deisseroth: he is careful to attribute credit precisely to his team members and trainees, particularly with respect to CLARITY. \u201cKarl is quite sensitive about who came up with the idea,\u201d says Kwanghun Chung, first author of the CLARITY paper 2 . Creating see-through brains was Deisseroth's dream, but the chemical know-how came from Chung. They describe the finding as a co-discovery. \n               Seeing clearly \n             Standard techniques for examining fine-scale brain structure involve slicing the brain into tissue-thin segments, analysing them under a microscope, and then \u2014 laboriously and often imprecisely \u2014 stacking the images back together. To bypass that process, Deisseroth had been looking for a means of chemically treating the brain to make it transparent. Some components of brain cells \u2014 especially lipids \u2014 are notoriously opaque to microscopes, so the idea was to get rid of them, while preserving the neuronal structure. It was a very different approach from optogenetics, and recruiting Chung \u2014 a chemical engineer by background \u2014 displayed Deisseroth's willingness to take risks. \u201cI remember people wondering what I was doing, what sort of plan there was in hiring this person and the direction the lab was going, because it was such a marked shift,\u201d he says. At first, Deisseroth wanted to immobilize neurons by filling them with a suitable material and removing the surrounding tissue. It turned out to be very difficult to build a stable structure from inside the neurons, but it was easier to build one around them. The team tried a few scaffolding materials, including keratin and a cellulose-like structure, before settling on a hydrogel \u2014 a gel made mainly of water and already widely used in biological studies. The researchers found that a hydrogel scaffold could bind components of neurons in place, including proteins, neurochemicals and even DNA and RNA (see 'A light touch'). With the structure locked down, a detergent called SDS could wash away lipid membranes, leaving the tissue transparent (see  Nature 496, 151; 2013 ). Within a week of publication, Deisseroth had already received several dozen requests for information. \u201cWe've been very much\u2026 I'm trying to find a verb that doesn't make it seem like a problem,\u201d he says. He settles on describing the e-mails that the lab received as an \u201coutpouring\u201d. CLARITY stunned people in the same way that optogenetics did, says Insel. \u201cUsually when someone has at an early stage of their career made an important advance, they tend to rest on those laurels. Karl quite quickly decided to do the next big thing.\u201d Other researchers will tailor CLARITY to their own projects. David Van Essen, a neuroscientist at Washington University in St. Louis, Missouri, is interested in testing CLARITY in the brain's white matter, the bundles of neuronal projections called axons that carry nerve impulses. This will help his group study patterns of connectivity that link brain regions. Using CLARITY with techniques such as optogenetics, or using it to analyse brains after behavioural studies, will be a powerful way to extract information about how brain networks function, says Eve Marder, a neuroscientist at Brandeis University in Waltham, Massachusetts. But analysing large networks in detail is challenging. Marder usually works with simple circuits of 30 or so neurons, and says that even those have a scarily exponential number of permutations. \u201cMy only personal hope is that people who venture into doing circuit work in larger brains pay attention to the lessons learned and mistakes that we made along the way,\u201d she says. CLARITY is already shedding light on clinical disorders. In the paper describing the technique 2 , the researchers analysed brain tissue from a seven-year-old boy who had autism spectrum disorder. They found that neurons in his cortex had joined together in ladder-like patterns, rather than the branches seen in typical brains. Animal models of autism-like conditions had hinted at this difference, but CLARITY made it possible to look for the irregularity in human brain samples. \n               Risk and reward \n             Deisseroth's office and much of his lab are still in the same basement that he moved into in 2004, although the team \u2014 now including 35 people \u2014 has branched out into lab space elsewhere. The labs are strewn with gadgetry, like the bedroom of a technology-obsessed teenager. There is no natural light in the basement, but Deisseroth likes it; being at the bottom of the building keeps vibrations to a minimum, which is necessary when using the sophisticated microscopes essential for his work. The current team includes computational neuroscientists, medics, chemists and engineers. And a US$22.5-million Transformative Research Award from the NIH, granted last year, allows Deisseroth to keep encouraging them to take chances. \u201cI have everyone have a hand in a high-risk and a low-risk project,\u201d he says, \u201clike a hedged portfolio.\u201d One of the latest high-risk projects uses light-field microscopy, which records images from a variety of perspectives simultaneously, to image the brain. Elsewhere in the lab, Chung and collaborator Viviana Gradinaru are still trying to build hydrogel structures inside neurons in the hope of preserving networks of specific cell types \u2014 freezing in place all the neurons of a certain type, for example, or all the cells that express a particular gene. The team continues to improve optogenetics, too, developing new opsins and perfecting methods for delivering light to the brain. Deisseroth has already added space to the laboratory to cater for all the visiting scientists coming to learn how to use optogenetics and CLARITY. There were more than 200 in 2010\u201312. The group has \u201cdone yeoman's work in making this relatively straightforward to set up\u201d, says Van Essen. \u201cThey're not going to hold hostage a methodology that people really can set up in their own lab.\u201d The tools are freely available; Stanford has filed patents on some aspects, says Deisseroth, but that it is to ensure unfettered access to the technology. He does not make money from the patents. Deisseroth struggles with his decision to spend more time on research than with the patients who inspire it. He still runs a weekly psychiatry clinic, but has to balance this with a packed travel schedule and mentoring duties. He carries on, however, hopeful that his studies will ultimately benefit those that medicine cannot. \u201cWe can help patients right now with many psychiatric symptoms,\u201d he says. \u201cBut for others we can't. So that really helps me in mentally freeing up the time needed for research.\u201d That desire to help is buoyed by simple, boundless curiosity. \u201cI think all scientists have a little bit in them,\u201d he says. \u201cThat wonder and curiosity about the natural world \u2014 not only how it is, but how it could be.\u201d \n                     See-through brains clarify connections 2013-Apr-10 \n                   \n                     Allen Institute aims to crack neural code 2011-Mar-29 \n                   \n                     Method of the Year 2010 2010-Dec-20 \n                   \n                     Be still my light-controlled heart 2010-Nov-11 \n                   \n                     Neuroscience: Illuminating the brain 2010-May-05 \n                   \n                     Deisseroth lab \n                   \n                     Boyden lab \n                   \n                     Zhang lab \n                   Reprints and Permissions"},
{"file_id": "499020a", "url": "https://www.nature.com/articles/499020a", "year": 2013, "authors": [{"name": "Brendan Maher"}], "parsed_as_year": "2006_or_before", "body": "With thousands of people in need of heart transplants, researchers are trying to grow new organs. Doris Taylor doesn't take it as an insult when people call her Dr Frankenstein. \u201cIt was actually one of the bigger compliments I've gotten,\u201d she says \u2014 an affirmation that her research is pushing the boundaries of the possible. Given the nature of her work as director of regenerative medicine research at the Texas Heart Institute in Houston, Taylor has to admit that the comparison is apt. She regularly harvests organs such as hearts and lungs from the newly dead, re-engineers them starting from the cells and attempts to bring them back to life in the hope that they might beat or breathe again in the living. Taylor is in the vanguard of researchers looking to engineer entire new organs, to enable transplants without the risk of rejection by the recipient's immune system. The strategy is simple enough in principle. First remove all the cells from a dead organ \u2014 it does not even have to be from a human \u2014 then take the protein scaffold left behind and repopulate it with stem cells immunologically matched to the patient in need.  Voil\u00e0 ! The crippling shortage of transplantable organs around the world is solved. Brendan Maher and Takanori Takebe discuss the techniques being used to create hearts and livers for transplant. In practice, however, the process is beset with tremendous challenges. Researchers have had some success with growing and transplanting hollow, relatively simple organs such as tracheas and bladders (see  go.nature.com/zvuxed ). But growing solid organs such as kidneys or lungs means getting dozens of cell types into exactly the right positions, and simultaneously growing complete networks of blood vessels to keep them alive. The new organs must be sterile, able to grow if the patient is young, and at least nominally able to repair themselves. Most importantly, they have to work \u2014 ideally, for a lifetime. The heart is the third most needed organ after the kidney and the liver, with a waiting list of about 3,500 in the United States alone, but it poses extra challenges for transplantation and bioengineering. The heart must beat constantly to pump some 7,000 litres of blood per day without a back-up. It has chambers and valves constructed from several different types of specialized muscle cells called cardiomyocytes. And donor hearts are rare, because they are often damaged by disease or resuscitation efforts, so a steady supply of bioengineered organs would be welcome. Taylor, who led some of the first successful experiments to build rat hearts 1 , is optimistic about this ultimate challenge in tissue engineering. \u201cI think it's eminently doable,\u201d she says, quickly adding, \u201cI don't think it's simple.\u201d Some colleagues are less optimistic. Paolo Macchiarini, a thoracic surgeon and scientist at the Karolinska Institute in Stockholm, who has transplanted bioengineered tracheas into several patients, says that although tissue engineering could become routine for replacing tubular structures such as windpipes, arteries and oesophagi, he is \u201cnot confident that this will happen with more complex organs\u201d. Yet the effort may be worthwhile even if it fails, says Alejandro Soto-Guti\u00e9rrez, a researcher and surgeon at the University of Pittsburgh in Pennsylvania. \u201cBesides the dream of making organs for transplantation, there are a lot of things we can learn from these systems,\u201d he says \u2014 including a better basic understanding of cell organization in the heart and new ideas about how to fix one. \n               The scaffold \n             For more than a decade, biologists have been able to turn embryonic stem cells into beating heart-muscle cells in a dish. With a little electrical pacemaking from the outside, these engineered heart cells even fall into step and maintain synchronous beating for hours. But getting from twitching blobs in a Petri dish to a working heart calls for a scaffold to organize the cells in three dimensions. Researchers may ultimately be able to create such structures with three-dimensional printing \u2014 as was demonstrated earlier this year with an artificial trachea 2  (see  Nature   http://doi.org/m2q ; 2013 ). For the foreseeable future, however, the complex structure of the human heart is beyond the reach of even the most sophisticated machines. This is particularly true for the intricate networks of capillaries that must supply the heart with oxygen and nutrients and remove waste products from deep within its tissues. \u201cVascularity is the major challenge,\u201d says Anthony Atala, a urologist at Wake Forest University in Winston-Salem, North Carolina, who has implanted bioengineered bladders into patients 3  and is working on building kidneys (see  Nature   http://doi.org/dw856h ; 2006 ). The leading techniques for would-be heart builders generally involve reusing what biology has already created. One good place to see how this is done is Massachusetts General Hospital in Boston, where Harald Ott, a surgeon and regenerative-medicine researcher, demonstrates a method that he developed while training under Taylor in the mid 2000s. Suspended by plastic tubes in a drum-shaped chamber made of glass and plastic is a fresh human heart. Nearby is a pump that is quietly pushing detergent through a tube running into the heart's aorta. The flow forces the aortic valve closed and sends the detergent through the network of blood vessels that fed the muscle until its owner died a few days before. Over the course of about a week, explains Ott, this flow of detergent will strip away lipids, DNA, soluble proteins, sugars and almost all the other cellular material from the heart, leaving only a pale mesh of collagen, laminins and other structural proteins: the 'extracellular matrix' that once held the organ together. The scaffold heart does not have to be human. Pigs are promising: they bear all the crucial components of the extracellular matrix, but are unlikely to carry human diseases. And their hearts are rarely weakened by illness or resuscitation efforts. \u201cPig tissues are much safer than humans and there's an unlimited supply,\u201d says Stephen Badylak, a regenerative-medicine researcher at the University of Pittsburgh. The tricky part, Ott says, is to make sure that the detergent dissolves just the right amount of material. Strip away too little, and the matrix might retain some of the cell-surface molecules that can lead to rejection by the recipient's immune system. Strip away too much, and it could lose vital proteins and growth factors that tell newly introduced cells where to adhere and how to behave. \u201cIf you can use a milder agent and a shorter time frame, you get more of a remodelling response,\u201d says Thomas Gilbert, who studies decellularization at ACell, a company in Columbia, Maryland, that produces extracellular-matrix products for regenerative medicine. Through trial and error, scaling up the concentration, timing and pressure of the detergents, researchers have refined the decellularization process on hundreds of hearts and other organs. It is probably the best-developed stage of the organ-generating enterprise, but it is only the first step. Next, the scaffold needs to be repopulated with human cells. \n               The cells \n             'Recellularization' introduces another slew of challenges, says Jason Wertheim, a surgeon at Northwestern University's Feinberg School of Medicine in Chicago, Illinois. \u201cOne, what cells do we use? Two, how many cells do we use? And three, should they be mature cells, embryonic stem cells, iPS [induced pluripotent stem] cells? What is the optimum cell source?\u201d Using mature cells is tricky to say the least, says Taylor. \u201cYou can't get adult cardiocytes to proliferate,\u201d she says. \u201cIf you could, we wouldn't be having this conversation at all\u201d \u2014 because damaged hearts could repair themselves and there would be no need for transplants. Most researchers in the field use a mixture of two or more cell types, such as endothelial precursor cells to line blood vessels and muscle progenitors to seed the walls of the chambers. Ott has been deriving these from iPS cells \u2014 adult cells reprogrammed to an embryonic-stem-cell-like state using growth factors \u2014 because these can be taken from a patient in need and used to make immunologically matched tissues. In principle, the iPS-cell approach could provide the new heart with its full suite of cell types, including vascular cells and several varieties of heart-muscle cell. But in practice, it runs into its own problems. One is the sheer size of a human heart. The numbers are seriously under-appreciated, says Ott. \u201cIt's one thing to make a million cells; another to make 100 million or 50 billion cells.\u201d And researchers do not know whether the right cell types will grow when iPS cells are used to recapitulate embryonic development in an adult heart scaffold. As they colonize the scaffold, some of the immature cells will take root and begin to grow. But urging them to become functional, beating cardiomyocytes requires more than just oxygenated media and growth factors. \u201cCells sense their environment,\u201d says Angela Panoskaltsis-Mortari, who has been trying to build lungs for transplant at the University of Minnesota in Minneapolis. \u201cThey don't just sense the factors. They sense the stiffness and the mechanical stress,\u201d which in turn pushes the cells down their proper developmental path. So researchers must put the heart into a bioreactor that mimics the sensation of beating. Ott's bioreactors use a combination of electrical signals \u2014 akin to a pacemaker \u2014 to help to synchronize the beating cardiomyocytes seeded on the scaffold, combined with physical beating motions induced by a pump (see 'Customized organs'). But researchers face a constant battle in trying to ape the conditions present in the human body, such as changes in heart rate and blood pressure, or the presence of drugs. \u201cThe body reacts to things and changes the conditions so quickly it's probably impossible to mimic that in a bioreactor,\u201d says Badylak. When Taylor and Ott were first developing bioreactors, for decellullarized and repopulated rat hearts, they had to learn as they went along. \u201cThere was a lot of duct tape in the lab,\u201d Ott says. But eventually the hearts were able to beat on their own after eight to ten days in the bioreactor, producing roughly 2% of the pumping capacity of a normal adult rat heart 1 . Taylor says that she has since got hearts from rats and larger mammals to pump with as much as 25% of normal capacity, although she has not yet published the data. She and Ott are confident that they are on the right path. \n               The beat \n             The final challenge is one of the hardest: placing a newly grown, engineered heart into a living animal, and keeping it beating for a long time. The integrity of the vasculature is the first barrier. Any naked bit of matrix serves as a breeding ground for clots that could be fatal to the organ or the animal. \u201cYou're going to need a pretty intact endothelium lining every vessel or you're going to have clotting or leakage,\u201d says Gilbert. Ott has demonstrated that engineered organs can survive for a time. His group has transplanted a single bioengineered lung into a rat, showing that it could support gas exchange for the animal, but the airspace fairly quickly filled with fluids 4 . And an engineered rat-kidney transplant that Ott's group reported early this year survived without clotting, but had only minimal ability to filter urine, probably because the process had not produced enough of the cell types needed by the kidney 5  (see  Nature   http://doi.org/m2r ; 2013 ). Ott's team and others have implanted reconstructed hearts into rats, generally in the neck, in the abdomen or alongside the animal's own heart. But although the researchers can feed the organs with blood and get them to beat for a while, none of the hearts has been able to support the blood-pumping function. The researchers need to show that a heart has much higher ability to function before they can transplant it into an animal bigger than a rat. With the heart, says Badylak, \u201cyou have to start with something that can function pretty well\u201d from the moment the transplant is in place. \u201cYou can't have something pumping just 1 or 2 or 5% of the ejection fraction of the normal heart and expect to make a difference,\u201d he says, referring to a common measure of pumping efficiency. There is little room for error. \u201cWe're just taking baby steps,\u201d says Panoskaltsis-Mortari. \u201cWe're where people were with heart transplant decades ago.\u201d The decellularization process being cultivated by Ott and others is already informing the development of improved tissue-based valves and other parts of the heart and other organs. A bioengineered valve, for example, may last longer than mechanical or dead-tissue valves because they have the potential to grow with a patient and repair themselves. And other organs may not need to be replaced entirely. \u201cI'd be surprised if within the next 5\u20137 years you don't see the patient implanted with at least part of an artery, lobes of a lung, lobes of a liver,\u201d says Badylak. Taylor suspects that partial approaches could aid patients with severe heart defects such as hypoplastic left heart syndrome, in which half the heart is severely underdeveloped. Restoring the other half, \u201cessentially forces you to build the majority of the things you need\u201d, she says. And these efforts could hold lessons for the development of cell therapies delivered to the heart. Researchers are learning, for example, how heart cells develop and function in three dimensions. In the future, partial scaffolds, either synthetic or from cadavers, could allow new cells to populate damaged areas of hearts and repair them like patches. The jars of ghostly floating organs might seem like a gruesome echo of the Frankenstein story, but Taylor says her work is a labour of love. \u201cThere are some days that I go, 'Oh my god, what have I gotten into?' On the other hand, all it takes is a kid calling you, saying 'Can you help my mother?' and it makes it all worthwhile.\u201d \n                 See Editorial \n                 page 6 \n               \n                     Presumed consent 2013-Jul-03 \n                   \n                     Miniature human liver grown in mice 2013-Jul-03 \n                   \n                     Lab-grown kidneys transplanted into rats 2013-Apr-14 \n                   \n                     Devices: Artificial inspiration 2012-Sep-26 \n                   \n                     Making lungs in the lab 2010-Jun-24 \n                   \n                     Ghost heart has a tiny beat 2008-Jan-13 \n                   \n                     Harald Ott \n                   \n                     Doris Taylor \n                   \n                     ACell \n                   Reprints and Permissions"},
{"file_id": "499142a", "url": "https://www.nature.com/articles/499142a", "year": 2013, "authors": [{"name": "Ewen Callaway"}], "parsed_as_year": "2006_or_before", "body": "Fiona Fox and her Science Media Centre are determined to improve Britain's press. Now the model is spreading around the world. Depending on whom you ask, Fiona Fox is either saving science journalism or destroying it. But today, she is touting its benefits to a roomful of reluctant scientists. \u201cYour voice has to be heard,\u201d the charismatic and sometimes combative head of Britain's Science Media Centre (SMC) tells the audience of more than 70. Most of these scientists work at the UK Food and Environment Research Agency (FERA), a sprawling government laboratory based in York, which studies hot-button issues such as pesticides and genetically modified (GM) crops. FERA scientists have a reputation for being closed to the media and, this May afternoon, Fox is trying to convince them to open up. \u201cYou're not alone, it's scary out there,\u201d says Fox. That is a message that Fox has honed well since establishing the SMC in London in 2002. The centre's aim is to get scientific voices into media coverage and policy debates \u2014 and by doing so, to improve the accuracy with which science is presented to the public. It tries to do this by providing select journalists with a steady flow of quotes and information from its database of about 3,000 scientists, and by organizing around 100 press briefings a year. \u201cOur philosophy is we'll get the media to do science better when scientists do the media better,\u201d says Fox. All this means that when science makes the news in the United Kingdom, the SMC has often played a part. Scientists adore it, for getting their voices heard. And many journalists appreciate how the non-profit organization provides accurate and authoritative material on deadline. But Fox and the SMC have also attracted some vehement critics, who say that they foster uncritical media coverage by spoon-feeding information to reporters, that they promote science too aggressively \u2014 the SMC has been called 'science's PR agency' \u2014 and that they sometimes advance the views of industry. Regardless, the SMC model is now spreading around the world, with the latest franchise slated to open in the United States around 2016. The centres are all run independently, but they abide by a unified charter crafted by Fox. This means that Fox is about to take her message to a much wider audience. \u201cI think there are problems with her reach,\u201d says Connie St Louis, director of the science-journalism course at City University London and one of Fox's loudest critics. \u201cShe's becoming one of the most powerful people in science.\u201d \n               The publicity bug \n             \u201cI'm basically a press officer\u201d is the first thing that Fox says about herself. After completing a journalism degree in 1985, she took a media-relations job with Brook Advisory, a London-based charity that provides reproductive health advice to young people. Days after she started, a member of parliament proposed increasing restrictions on abortions, and things kicked off. \u201cIt was an exciting six months \u2014 we were in the national spotlight all the time, on TV, in the national news,\u201d says Fox. \u201cI got the bug.\u201d Fox went on to other media-relations positions, first in a group working for one-parent families and then in one promoting international aid, but by the late 1990s she was ready for a change. She looked around to see what was making the headlines, and found that many of them came from messy issues in science. One of the messiest had blown up on 10 August 1998, when Britons woke up to headlines screaming that GM potatoes were a danger to their health. \u00c1rp\u00e1d Pusztai, a toxicologist at the Rowett Institute of Nutrition and Health in Aberdeen, had told a television programme about his unpublished research showing that an experimental GM potato, never intended for human consumption, could damage the immune systems of rats. The British public and media were already highly sceptical of GM food, and the 'Pusztai affair' pushed things into hyper drive. GM crops stayed in the headlines for the next two years, and some sections of the British press actively campaigned against them. At the time, most scientists buried their heads, hoping that the furore would subside, even as a few scolded the media for its poor grasp of complex scientific issues. The press, they grumbled, had already raised unwarranted concerns about food safety during the 1996 scare over mad cow disease, and had dangerously undermined public health when, in 1998, it reported on a link between vaccines and autism that was later debunked. \u201cIt was a bit of a war out there,\u201d says Fox. In 1999, the House of Lords Select Committee on Science and Technology responded by launching an investigation into the role of science in society. It concluded that \u201cthe culture of United Kingdom science needs a sea-change, in favour of open and positive communication with the media\u201d, and aired the idea of a new institution to sit on the front lines, independent of the government and media. That idea took shape as the SMC. When Fox read about plans for the centre, she saw a media-relations opportunity. She applied to lead it and soon landed an interview with a panel that included  Nature 's editor-in-chief Philip Campbell and Susan Greenfield, then director of the Royal Institution, Britain's oldest science-outreach organization. Fox was offered the job the next morning. \u201cI knew it would have to be someone who was quite tough,\u201d Greenfield recalls. \u201cWe had to have her.\u201d In March 2002, as the centre got under way, Fox and her team released something of a manifesto, stating that the SMC would be \u201cunashamedly pro-science\u201d, would \u201coperate like a newsroom\u201d and would be \u201cfree of any particular agenda within science\u201d. It also stipulated that a single donor could provide no more than 5% of the SMC operating budget, to ensure the centre's independence. That rule that still stands today, with a few exceptions, including London-based biomedical charity the Wellcome Trust and the UK Department for Business, Innovation and Skills, which last year provided 6.3% and 6.6%, respectively. Industry funding \u2014 from donors including Proctor & Gamble, agribusiness firm Syngenta and GlaxoSmithKline \u2014 makes up about one-third of the SMC's budget. In the past two years, Nature Publishing Group has given the SMC a total of \u00a310,000. At the start, the SMC made some prominent stumbles. In early 2002, the organization learned that the BBC was to air a drama called  Fields of Gold , in which experimental GM crops are linked to mysterious deaths amid an industry cover-up. Fox got hold of an advance copy, invited leading scientists to a viewing \u2014 complete with free popcorn \u2014 and sent their reviews to reporters. \u201cThen the shit hit the fan,\u201d Fox says. Robert May, then president of the Royal Society, called the film \u201can error-strewn piece of propaganda\u201d and some newspapers echoed his and other scientists' criticism. The film's two writers, one of whom was Alan Rusbridger, editor of newspaper  The Guardian , hit back, accusing the SMC of being a pro-GM mouthpiece for the companies that fund it. The same criticism has been aired since, in part because the SMC gives voice to scientists who favour GM and other commercial applications of research. But Fox argues that the cap on donations insulates the centre from undue influence. Early on, Fox and her staff also had trouble developing relationships with general reporters in the print and broadcast news, who, they believed, needed the most help covering science. The centre created laminated cards that read, \u201cIf you need a scientist, phone us\u201d, and posted them to newsrooms. \u201cWe'd phone them up and ask them if you got the card, and of course they said, 'Fuck off, I'm busy,'\u201d Fox says. So the SMC instead began reaching out to specialist science and health reporters, and found them far more receptive. \u201cWe give them an advantage in their newsroom. When a big science story breaks, we are helping the science correspondents stay on the story,\u201d says Fox. The centre started to get scientists on board too, by offering to act as a trusted conduit to the press. Today, Fox and her staff of seven work hard to identify researchers who can speak on topical issues, and to make their comments more insightful for reporters. Avoiding unwanted contact with the media is one of the SMC's major selling points to scientists. \u201cIf you're on our database, we never ever, ever hand your number to a journalist,\u201d Fox told the FERA scientists in York. Perhaps the biggest criticism of Fox and the SMC is that they push science too aggressively \u2014 acting more as a PR agency than as a source of accurate science information. In December 2006, for example, the UK government indicated that it planned to ban scientists from creating hybrid embryos containing cells from humans and other animals. A public consultation had found unease with the research, and early media coverage tended to focus on the ethical concerns, quoting critics such as members of the Catholic clergy. Researchers, funders and scientific societies organized a campaign to change the government's mind. The SMC coordinated the media outreach, hosting five briefings at which scientists played down ethical qualms and said that hybrid embryos were a valuable research tool that might lead to disease treatments. The resulting media coverage reflected those views, according to an analysis of the campaign's effectiveness commissioned by the SMC and other campaign supporters. More than 60% of the sources in stories written by science and health reporters \u2014 the ones targeted by the SMC \u2014 supported the research, and only one-quarter of sources opposed to it. By contrast, journalists who had not been targeted by the SMC spoke to fewer supportive scientists and more opponents. The SMC was \u201clargely responsible for turning the tide of coverage on human\u2013animal hybrid embryos\u201d, says Andy Williams, a media researcher at the University of Cardiff, UK, who carried out the analysis. (The eventual bill would allow hybrid-embryo research.) But Williams now worries that the SMC efforts led reporters to give too much deference to scientists, and that it stifled debate. \u201cIt was a strategic triumph in media relations,\u201d he says. Members of the scientific community are quick to go to bat for the SMC. One of those is Val Summers, the regulatory-affairs associate at lab-animal supplier Harlan Laboratories, based in Blackthorn, UK. Harlan is a target of animal-rights activists, and the company's long-standing policy has been for its employees not to speak to the media. But in 2011,  The Sunday Times  newspaper contacted Harlan about a story it planned to run on animal cruelty at the company's dog facility. At Fox's urging, Harlan and Summers hosted a reporter and a photographer from the paper at the facility. \u201cShe's given me the confidence to speak out,\u201d Summers says of Fox. \n               Daily presence \n             Fox and the SMC are now a routine part of the day for many British journalists. Some attend the centre's frequent briefings, which are often chaired by a smartly dressed Fox. And more than 300 reporters \u2014 including some at  Nature  \u2014 receive the SMC's daily strings of e-mails. On 21 May, for example, the day after a tornado killed two dozen people in an Oklahoma town, Ian Sample,  The Guardian 's science reporter, was assigned a fast-turnaround story on the science of tornadoes. That day, the SMC sent him three e-mails containing tornado facts and comments from 11 researchers, many addressing the controversial link between extreme weather and global warming. Sample worked the material into a story, and called some of the scientists for more detail. \u201cThat information was really handy,\u201d he says. Sample is less comfortable working this way when it comes to controversial topics. \u201cIt's a really dangerous thing and an easy thing for journalists to start relying on SMC comments,\u201d he says. \u201cWe should be picking who we're talking to and picking which questions we're asking.\u201d That over-reliance has been highlighted by St Louis. In the latest spat, a forum article last month on the website of the  Columbia Journalism Review , St Louis accused the SMC of \u201cfuelling a culture of churnalism\u201d. Because journalists have started attending SMC briefings rather than digging for stories, she wrote, \u201cthe quality of science reporting and the integrity of information available to the public have both suffered\u201d. Fox disputed the charge, pointing out that the SMC works with journalists on original stories. She has no qualms about the centre's success or its promotion of science. \u201cWe were set up to get the voice of science in the debate,\u201d she says. And she bristles at the idea that the SMC feeds lazy journalists canned quotes. \u201cThere is nothing canned, processed or simple about this,\u201d Fox says. \u201cI can't see why it's so much purer for a journalist to phone their contact at Sussex University than to phone the SMC and get us to do it.\u201d \n               Global media \n             Science media centres inspired by the British one have already opened in Australia, New Zealand, Canada and Japan, and more are planned in Germany, Denmark and France. But an SMC in the United States \u2014 with its vast, fragmented media and bitter controversies over certain scientific issues \u2014 may provide the fiercest test of Fox's model. Last year, at Fox's urging, Julia Moore, a senior scholar at the Woodrow Wilson International Center for Scholars in Washington DC, set up an exploratory committee for a US SMC. Moore has since started fund-raising: \u201cIt's going full steam ahead,\u201d she says. The US centre will focus more on helping journalists to reach scientists than the other way around, as its UK counterpart does. \u201cThey need help writing stories about the latest research on stem cells or climate change or the latest controversy on evolution,\u201d says Moore. Ivan Oransky, head of the health team for news agency Reuters in New York, does not think that the well-sourced journalists with whom he typically works will need such help, but he says that local newspapers and websites without that expertise could use an SMC. Still, he worries that such a centre could end up having an undesirable influence on the news. \u201cIf it's a force for smoothing over some of the legitimate disagreements that scientists have, if it is a force for putting science in the best possible light because of who the funders are, I don't think it's really doing all that much,\u201d he says. Fox says that she hears every day from people seeking advice on how to set up and run a science media centre. But the part of her job in which she takes the most pride, she says, is convincing once-timid scientists to join the SMC database and speak out. \u201cA real triumph for us is getting a scientist who has worked for 30 years on a really controversial issue and has never spoken to the media,\u201d she says. The FERA scientists, however, are going to take more persuasion. Even after a half-day workshop and a wine reception, only five researchers sign up. But Fox is undeterred, pointing to workshops at other institutes, where she has had vastly more success. \u201cTen years ago, when we started, lots of people were like that, scared of the media, scared of getting in trouble with government,\u201d says Fox. \u201cThat's no longer the case.\u201d \n                 See Editorial \n                 page 126 \n               \n                     Two nations divided by a common purpose 2012-Mar-14 \n                   \n                     Two nations divided by a common purpose 2012-Mar-14 \n                   \n                     Science journalism: Supplanting the old media? 2009-Mar-18 \n                   \n                     Critical journalism 2008-Mar-26 \n                   \n                     Science media centre launches 2001-Nov-08 \n                   \n                     UK Science Media Centre \n                   \n                     US Science Media Centre \n                   \n                     House of Lords report on science and technology (2000) \n                   \n                     Columbia Journalism Review : Science media centers & the press \n                   Reprints and Permissions"},
{"file_id": "499272a", "url": "https://www.nature.com/articles/499272a", "year": 2013, "authors": [{"name": "Alison Abbott"}], "parsed_as_year": "2006_or_before", "body": "The United States and Europe are both planning billion-dollar investments to understand how the brain works. But the technological challenges are vast. When neurobiologist Bill Newsome got a phone call from Francis Collins in March, his first reaction was one of dismay. The director of the US National Institutes of Health had contacted him out of the blue to ask if he would co-chair a rapid planning effort for a ten-year assault on how the brain works. To Newsome, that sounded like the sort of thankless, amorphous and onerous task that would ruin a good summer. But after turning it over in his mind for 24 hours, his dismay gave way to enthusiasm. \u201cThe timing is right,\u201d says Newsome, who is based at Stanford University School of Medicine in California. He accepted the task. \u201cThe brain is the intellectual excitement for the twenty-first century.\u201d It helped that the request for the brain onslaught was actually coming from Collins's ultimate boss \u2014 US President Barack Obama. Just two weeks after that call, on 2 April, Obama announced a US$100-million initial investment to launch the BRAIN Initiative, a research effort expected to eventually cost perhaps ten times that amount. The European Commission has equal ambitions. On 28 January, it announced that it would launch the flagship Human Brain Project with a 2013 budget of \u20ac54 million (US$69 million), and contribute to its projected billion-euro funding over the next ten years (see  Nature   482 , 456\u2013458; 2012 ). Although the aims of the two projects differ, both are, in effect, bold bids for the neuroscientist's ultimate challenge: to work out exactly how the billions of neurons and trillions of connections, or synapses, in the human brain organize themselves into working neural circuits that allow us to fall in love, go to war, solve mathematical theorems or write poetry. What's more, researchers want to understand the ways in which brain circuitry changes \u2014 through the constant growth and retreat of synapses \u2014 as life rolls by. Reaching this goal will require innovative new technologies, ranging from nanotechnologies to genetics to optics, that can capture the electrical activity coursing through neurons, prod those neurons to find out what they do, map the underlying anatomical circuits in fine detail and process the exabytes of information all this work will spit out. \u201cThink about it,\u201d says neuroscientist Konrad Kording of Northwestern University in Chicago, Illinois. \u201cThe human brain produces in 30 seconds as much data as the Hubble Space Telescope has produced in its lifetime.\u201d Researchers are already chipping away at the problem: the past few years have seen startling advances in techniques that allow the stimulation of precise neurons deep in the brain using light, for example, and the construction of anatomical maps with unprecedented detail. So far, most neuroscientists are using simpler species such as mice or worms to learn basic principles that evolution may have conserved in humans. Here,  Nature  examines some of the technological advances that will be necessary to drive further, and faster, into the brain. \n               1) Measuring it \n             If researchers are to make sense of the frenzy of electrical signals coursing through the brain's circuits, they will need to record simultaneously from as many neurons as possible. Today, they typically gauge neuronal activity by inserting metal electrodes into the brain, but this approach comes with enormous challenges. Each electrode needs its own wire to carry out the measured analogue signal \u2014 the voltage change \u2014 and the signals can easily be lost or distorted as they travel along the wire to instruments that will convert them into the digital signals needed for analysis. Moreover, the wires must be hair-thin to avoid tissue damage. Advances in electrode technologies have seen the number of neurons that researchers can record from double roughly every seven years over the past five decades, such that probes can now reach a couple of hundred neurons simultaneously 1 . But the ultimate challenge will require them to reach many more cells and to record higher-quality signals. That is becoming possible with a new generation of neuroprobes made from silicon, which allows extreme miniaturization. Instruments such as analogue-to-digital converters can be carved out of the same tiny piece of silicon as the electrodes, so the vulnerable analogue signal does not have to travel. A prototype 'neuroprobe' of this type was unveiled in February at the International Solid-State Circuits Conference in San Francisco, California, by imec, a nanoelectronics research organization based in Leuven, Belgium. One-centimetre long and as thin as a dollar bill, the probe packs in 52 thin wires and switches that neuroscientists can flip seamlessly between 456 silicon electrodes. When inserted into a mouse brain, for example, the electrodes dotted across the imec probe can span \u2014 and record from \u2014 all layers of the animal's brain simultaneously, from the cortex to the thalamus in the brainstem. This could help neuroscientists to unpick the circuitry that connects them. \u201cThis prototype can be scaled up,\u201d says Peter Peumans, director of bio- and nanoelectronics at imec. Within three years, he says, the neuroprobes will have up to 2,000 electrodes and more than 200 wires. But rather than just passively measuring electrical activity in neural circuits, researchers also want to test what those circuits do by activating them and observing the changes that occur in electrical activity and animal behaviour. Each imec probe includes four stimulating electrodes, and in future models this will be increased to 20 or more. But as recording and stimulating electrodes can interfere with one another, researchers are also trying to stimulate neurons with light instead of electricity. These 'optogenetic' techniques generally involve inserting light-sensitive ion-channel proteins called opsins into neurons, so that a laser light shone into the skull through an optic fibre opens the channels and activates the neurons. One research group recently used optogenetics in mice, for example, to produce repetitive behaviours that are considered to be a model for obsessive-compulsive disorder 2 . The next generation of optogenetic neuroprobes will include systems that are able to deliver light directly into the brain exactly where it is needed, without the need for cumbersome optical fibres. In April, for example, Michael Bruchas at Washington University in St Louis, Missouri, and his team described their wireless prototype: an optogenetic chip with light-emitting diodes that can be activated by a radio signal to trigger the opsins 3 . When the team implanted a chip into mice that could stimulate the brain's reward centre, the animals quickly learned to switch it on themselves by poking their noses into a hole \u2014 showing that the chip worked and could change behaviour. The search is on for other natural or genetically engineered opsins that respond to different wavelengths of light and might allow researchers to activate and test various elements of a circuit. Eventually, neuroprobes may not only routinely record from and stimulate hundreds or thousands of neurons in mice and non-human primates, but also include sensors to identify neurotransmitters and measure physiological parameters such as temperature, which can affect neuronal activity. And the future could bring much more radical methods. Some scientists have proposed the idea of nanometre-scale light-sensitive devices that could embed themselves in the membranes of neurons, power themselves from cellular energy and wirelessly convey the activity of millions of neurons simultaneously 4 . Another idea is to do away with measuring devices and instead capture the post-mortem trace left by an action potential as it passes through the brain. Kording is part of a team trying to do this by exploiting DNA polymerase, the enzyme that cells use to build DNA from its component bases. He and his colleagues have designed a synthetic DNA polymerase that, when surrounded by high levels of calcium, inserts the wrong base into the artificial DNA strand it constructs 5 . If this polymerase could be added to neurons, then an action potential, which causes intracellular calcium levels to spike, would trigger errors in the DNA strand, and the time that this occurred could be determined retrospectively from the length and sequence of the DNA. That's the theory, anyway, says Kording. \u201cBut we are only getting started.\u201d \n               2) Mapping it \n             However researchers go about collecting information about neuronal activity and circuitry, it will be essential to map this onto a reliable and highly detailed anatomical atlas of the brain. It is like trying to understand traffic flow in a city: the better the map (the anatomy), the better the predictions of how it will change during rush hour (the active circuits). For more than a century, the method used to map neuroanatomy has been to slice a brain as thinly as possible, stain the slices to render the cells visible and look at them under the light microscope. But, computationally, it is extremely challenging to align large numbers of slices in order to reconstruct the tangled web of neurons densely packed into a human brain. Even so, Katrin Amunts of the Research Centre J\u00fclich in Germany and her team announced that they had done it last month, when they published a three-dimensional reconstruction of a human brain in unprecedented detail. To build it, they painstakingly sliced the brain of a 65-year-old woman into 7,400 layers 20 micrometres thick, stained them, imaged them with a light microscope and then used 1,000 hours on two supercomputers to piece the terabyte of data together 6 . The atlas reveals in detail folds of the human brain, which tend to get lost in two-dimensional cross-sections. The whole project took a decade, says Amunts, who has already started work on a second human brain to look at variation between individuals \u2014 a project she expects to move a lot faster. Attempting to take another leap farther, Jeff Lichtman at Harvard University in Cambridge, Massachusetts, and Winfried Denk of the Max Plank Institute for Neurobiology in Munich, Germany, are working with the German optics company Carl Zeiss on a new electron microscope that would image even thinner slices \u2014 25 nanometres, or one-thousandth the thickness of an average cell. \u201cThen you get to see every little damn thing in the brain, from every neuron to every subcellular organelle, from every synapse to every spine neck \u2014 everything,\u201d says Lichtman. Using conventional electron microscopes, with their single scanning beam of electrons, researchers have so far been able to reconstruct only a cubic millimetre of brain tissue. It would take many decades to scan a whole mouse brain's worth of ultra-thin slices, says Denk. The new machines, which should be delivered to the two labs next year, will have 61 scanning beams operating in parallel and will shrink this time down to months. Denk estimates that this will allow them to make a computational reconstruction \u2014 \u201ca mouse brain in a box\u201d, as he puts it \u2014 within five years. What Lichtman and Denk have not yet resolved is how to reconstruct a full three-dimensional picture of the tissue from these images. In a trial project using a conventional electron microscope, Denk's lab scanned minuscule volumes of mouse retina, one of the simplest parts of the mammalian brain 7 , 8 . But computing alone was not able to reconstruct the 300 gigabytes of image data the effort generated, so the lab enrolled 230 people to help to trace, by eye, the neurons as they meander through the slices. \u201cIt won't be practical to do that sort of crowd-sourcing on a larger scale,\u201d says Denk. \u201cWe'll have to develop algorithms to get machines to do the job as well as the human eye.\u201d There may be easier ways to carry out brain mapping at lower resolutions. One possibility is a technique called CLARITY, which generated excitement when it was unveiled in April. Karl Deisseroth at Stanford University and his colleagues have developed a way to chemically replace the opaque lipids in the brain with a clear gel, rendering the tissue transparent and allowing the internal arrangements of neurons to be viewed without the need for slicing 9 . Deisseroth has already applied the technique to brain tissue from a boy who had autism spectrum disorder, and found unusual ladder-like arrangements of neurons in his cortex. Other researchers are clamouring to use the method to trace circuitry in normal brains (see  Nature   497 , 550\u2013552; 2013 ). And however efficient the various activity-measuring and anatomy-mapping techniques turn out to be, many researchers hope that it won't be necessary to view \u2014 or record from \u2014 every individual neuron to get a working picture of the whole brain. \u201cPatterns will emerge from which it will be possible to extrapolate,\u201d says Newsome. \n               3) Making sense of it \n             Perhaps the most daunting part of the brain challenge lies in storing and handling data. One cubic millimetre of brain tissue will generate an estimated 2,000 terabytes of electron-microscopy information using Lichtman and Denk's new microscope, for example. Denk estimates that an entire mouse brain could produce 60 petabytes and a human brain about 200 exabytes. This amount of data will rival the entire digital content of today's world, \u201cincluding Facebook and all the big data stores\u201d, says Lichtman. That is just the start. Neuroscientists will eventually want to collect this type of anatomical information for many human brains \u2014 each of them unique \u2014 and layer onto it information about neuronal activity. They will need to store and organize all these diverse data types so that scientists can interface with them. Europe's Human Brain Project, which aims to provide a brain simulation that researchers can interact with in real time, adds another level of demand. \u201cOne of our challenges is to develop computer languages that allow a supercomputer's capacity to be used efficiently,\u201d says Jesus Labarta Mancho of the Barcelona Supercomputing Center in Spain, which is a partner of the Human Brain Project. Current supercomputers would be overwhelmed by experiments requiring different parts of the brain to be simulated in different fractions of a second. So the idea is to develop ways to allow the supercomputer to compress information about some brain areas, freeing up resources for computation on the ones that are relevant to the problem at hand. Even assuming that the data can be neatly packaged, theorists will have to work out what questions to ask of it. \u201cIt is a chicken and egg situation,\u201d says theoretical neuroscientist Christian Machens at the Champalimaud Centre for the Unknown in Lisbon. \u201cOnce we know how the brain works, we'll know how to look at the data.\u201d Theorists argue about the scale of the task ahead of them; Kording is one of many who think it will be horrendous. \u201cIt make's Google's search problems look like child's play,\u201d he says. \u201cThere are approximately the same number of neurons as Internet pages, but whereas Internet pages only link to a couple of others in a linear way, each neuron links to thousands of others \u2014 and does so in a non-linear way.\u201d But Partha Mitra, a biomathematician at Cold Spring Harbor Laboratory in New York, thinks that the bigger challenge to knowing the brain will be sociological. \u201cChasing after the workings of the brain is not like chasing after the Higgs boson, where everyone goes after the same single target,\u201d he says. \u201cIt is about the community setting goals in a deliberate manner and working towards them in a disciplined manner.\u201d Setting those goals is now consuming Newsome's summer, just as he predicted. He is taking part in a series of expert workshops to define the goals of the BRAIN Initiative and shaping a report on it that is due in September. The report won't promise to solve all the challenges of the brain, he says, but it will set a course that, in the long term, just might. \u201cWe'll eventually learn what all the twinkling of the neurons means in terms of our behaviour,\u201d says Newsome, \u201cand that's what really matters.\u201d See Editorial  page 253 \n                     Mapping brain networks: Fish-bowl neuroscience 2013-Jan-23 \n                   \n                     High-throughput anatomy: Charting the brain's networks 2012-Oct-10 \n                   \n                     Neuroscience: Making connections 2012-Mar-21 \n                   \n                     Computer modelling: Brain in a box 2012-Feb-22 \n                   \n                     Neural circuits: Putting neurons on the map 2009-Oct-22 \n                   \n                     Cash boost for mapping the human brain 2009-Jul-22 \n                   \n                     BRAIN Initiative \n                   \n                     Human Brain Project \n                   \n                     imec \n                   \n                     Barcelona Supercomputing Centre \n                   Reprints and Permissions"},
{"file_id": "499139a", "url": "https://www.nature.com/articles/499139a", "year": 2013, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "Efforts to predict the near-term climate are taking off, but their record so far has been patchy. In August 2007, Doug Smith took the biggest gamble of his career. After more than ten years of work with fellow modellers at the Met Office's Hadley Centre in Exeter, UK, Smith published a detailed prediction of how the climate would change over the better part of a decade 1 . His team forecasted that global warming would stall briefly and then pick up speed, sending the planet into record-breaking territory within a few years. The Hadley prediction has not fared particularly well. Six years on, global temperatures have yet to shoot up as it projected. Despite this underwhelming result, such near-term forecasts have caught on among many climate modellers, who are now trying to predict how global conditions will evolve over the next several years and beyond. Eventually, they hope to offer forecasts that will enable humanity to prepare for the decade ahead just as meteorologists help people to choose their clothes each morning. These near-term forecasts stand in sharp contrast to the generic projections that climate modellers typically produce, which look many decades ahead and don't represent the actual climate at any given time. \u201cThis is very new to climate science,\u201d says Francisco Doblas-Reyes, a modeller at the Catalan Institute of Climate Sciences in Barcelona, Spain, and a lead author of a chapter that covers climate prediction for a forthcoming report by the Intergovernmental Panel on Climate Change (IPCC). \u201cWe're developing an additional tool that can tell us a lot more about the near-term future.\u201d In preparation for the IPCC report, the first part of which is due out in September, some 16 teams ran an intensive series of decadal forecasting experiments with climate models. Over the past two years, a number of papers based on these exercises have been published, and they generally predict less warming than standard models over the near term. For these researchers, decadal forecasting has come of age. But many prominent scientists question both the results and the utility of what is, by all accounts, an expensive and time-consuming exercise. \u201cAlthough I have nothing against this endeavour as a research opportunity, the papers so far have mostly served as a 'disproof of concept',\u201d says Gavin Schmidt, a climate modeller at NASA's Goddard Institute for Space Studies in New York, which declined to participate in the IPCC's decadal-predictions experiment. \n               Initial ideas \n             To make its climate prediction, Smith's team used its standard climate model, but broke the mould by borrowing ideas from the way meteorologists forecast the weekly weather. Typical climate projections start some way back in the past, often well before the industrial era, in a bid to capture the average climate well enough to forecast broad patterns over the long term. Weekly weather forecasts, however, begin with the present. They make multiple simulations with slightly different initial meteorological conditions to give an array of outcomes that has some statistical validity despite the weather's inherent chaos. Smith and his team applied this same approach. They collected a slew of climate measurements \u2014 air temperature, wind speed and direction, atmospheric pressure, ocean temperature and salinity \u2014 for 20 days during 2005. For each prediction, they 'initialized' the Hadley Centre's main climate model by plugging in a single day's data. Then they ran the model forward for a decade under the influence of various factors such as rising greenhouse-gas concentrations. By starting in the present with actual conditions, Smith's group hoped to improve the model's accuracy at forecasting the near-term climate. The results looked promising at first. The model initially predicted temperatures that were cooler than those seen in conventional climate projections \u2014 a forecast that basically held true into 2008. But then the prediction's accuracy faded sharply: the dramatic warming expected after 2008 has yet to arrive (see 'Hazy view'). \u201cIt's fair to say that the real world warmed even less than our forecast suggested,\u201d Smith says. \u201cWe don't really understand at the moment why that is.\u201d The answer may lie in the oceans. Although the atmosphere largely controls day-to-day weather, the slow-moving oceans hold so much more energy and heat that they dominate how the climate changes from year to year. Researchers suspect that much of this variability is tied to widespread cycles, such as the El Ni\u00f1o warming and La Ni\u00f1a cooling system in the eastern tropical Pacific. In theory, the fact that salt water circulates more slowly than air should also make the oceans a little easier to model. In 2008, a group of climate modellers led by Noel Keenlyside, now at the University of Bergen in Norway, made a prediction through to 2030 that incorporated the effects of sea surface temperatures in the Atlantic 2 . They focused on one of the Atlantic's dominant current patterns, the meridional overturning circulation. This carries sun-baked waters from the tropics to the north Atlantic, where it releases heat into the atmosphere, before sinking into the deep ocean and travelling south again. The model predicted that this circulation would weaken, helping to stabilize or even cool global temperatures over the next several years. The prediction sparked a furore: some researchers questioned the Keenlyside team's analysis as well as the way the model was initialized. The highly publicized study also became wrapped up in a broader debate in the media about whether global warming had paused. Shortly after the study came out, a group of scientists led by Stefan Rahmstorf, an oceanographer at the Potsdam Institute for Climate Impact Research in Germany, publicly refuted the paper and challenged Keenlyside's group to a pair of bets together worth \u20ac5,000 (US$6,525) if the predictions bore fruit. \u201cWe felt a need to make it publicly known that this was not climate science as such that was predicting a cooling period,\u201d Rahmstorf says. Keenlyside and his team did not take the bets, which turned out to be a smart choice. The circulation did not flag and the temperatures were higher than predicted, says Rahmstorf. Keenlyside acknowledges the model's shortcomings, but says that it captured at least the initial trends in global temperatures, which did not rise in the first few years of the prediction period. \u201cOur system was very crude, but we were able to show that initializing the oceans is very important in these models,\u201d he says. Despite their faults, such efforts helped spark a wave of research among modellers who are hungry for ways to test and improve their calculations. The global climate-modelling groups that took part in the IPCC's experiments invested a substantial portion of their modelling time to produce the first systematic predictions of how the global climate will evolve in the coming years. These models predict cooler temperatures: on average 15% less warming over the next few decades compared with standard climate projections 3 . To determine whether these projections are likely to hold, the groups ran the usual test of seeing how well their models performed when hindcasting, or predicting the past. The teams plugged in all of the observational data and ran decadal climate predictions at least every five years beginning in 1960, comparing the resulting hindcasts to the actual climate as well as standard climate models. In one such analysis 4 , Doblas-Reyes and his colleagues say that their model anticipated the slowdown in global warming up to five years in advance. Their paper also bolstered the theory that the deep oceans, notably the Atlantic and tropical Pacific, had stalled atmospheric warming by absorbing much of the heat being trapped by rising concentrations of greenhouse-gas concentrations in the air (see \u2018Lost heat\u2019). \n               boxed-text \n             \n               Error correction \n             These results have yet to win over sceptics such as Rahmstorf, who questions whether the models are accurately anticipating variations in Earth's climate, but many others say that the newer simulations are showing some skill at a regional level, particularly within the oceans. \u201cWe do see that there are some improvements,\u201d says Lisa Goddard, a climate scientist at Columbia University in New York who is heading a systematic analysis and comparison of the predictions from the IPCC models 5 . Many models, for instance, captured a sudden warming of sea surface temperatures in the North Atlantic that began around 1995. \u201cThey all predict the shift beautifully,\u201d Goddard says. \u201cUnfortunately, from what I hear, different models are doing it for different reasons.\u201d If so, the models' success could be deceptive: whatever accuracy they show for the first year or two of their predictions might stem in part from the fact that the simulations start off with a snapshot of the current climate. Because the climate does not usually change drastically from one year to the next, the model is bound to start off predicting conditions that are close to reality. But that effect quickly wears off as the real climate evolves. If this is the source of the models' accuracy, that advantage fades quickly after a few years. Although the prediction experiments show limited forecasting skill at the moment, modellers are trying to use these exercises to improve their creations. One key challenge is the way in which the models are initialized. To start a simulation, modellers plug as many values as possible into a three-dimensional grid of the oceans and atmosphere. But modellers must make assumptions for areas without data, including the deep oceans. Another challenge stems from the fact that each model has its own equilibrium state \u2014 the climate that it generates naturally if left on its own. By plugging in actual values for the ocean and atmosphere, researchers pull the model away from its natural state. When the model starts to run forward in time, it immediately begins to drift back to its preferred climate, which can introduce additional complications. \u201cWhat are the causes of that drift?\u201d asks Doblas-Reyes. By comparing prediction simulations with conventional climate projections, scientists hope to correct for that drift and detect problems in the models that would otherwise remain hidden. \u201cIf these models can help scientists identify systematic errors, it will benefit the entire climate-modelling community,\u201d says Doblas-Reyes. Schmidt says that these efforts are \u201ca little misguided\u201d. He argues that it is difficult to attribute success or failure to any particular parameter because the inherent unpredictability of weather and climate is built into both the Earth system and the models. \u201cIt doesn't suggest any solutions,\u201d he says. Even advocates have no illusions about the challenges ahead. Kevin Trenberth, a climate scientist at the National Center for Atmospheric Research in Boulder, Colorado, says that it could be a decade or more before this research really begins to pay off in terms of predictive power, and even then climate scientists will be limited in what they can say about the future. But many people might welcome hints about what's to come. \u201cFor a farmer in Illinois,\u201d Trenberth says, \u201cany indications about what to expect could turn out rather valuable.\u201d Smith says that his group at the Hadley Centre has doubled the resolution of its model, which now breaks the planet into a grid with cells 150 kilometres on each side. Within a few years, he hopes to move to a 60-kilometre grid, which will make it easier to capture the connections between ocean activities and the weather that society is interested in. With improved models, more data and better statistics, he foresees a day when their models will offer up a probabilistic assessment of temperatures and perhaps even precipitation for the coming decade. In preparation for that day, he has set up a 'decadal exchange' to collect, analyse and publish annual forecasts. Nine groups used the latest climate models to produce ten-year forecasts beginning in 2011. An analysis of the ensemble 6  shows much the same pattern as Smith's 2007 prediction: temperatures start out cool and then rise sharply, and within the next few years, barring something like a volcanic eruption, record temperatures seem all but inevitable. \u201cI wouldn't be keen to bet on that at the moment,\u201d Smith says, \u201cbut I do think we're going to make some good progress within a few years.\u201d \n                     Climate forecasting: A break in the clouds 2012-May-09 \n                   \n                     Earth science: The climate machine 2010-Feb-24 \n                   \n                     Atmospheric science: Climate's smoky spectre 2009-Jul-01 \n                   \n                     Nature special: After Kyoto \n                   \n                     IPCC \n                   \n                     Hadley Centre decadal forecasting \n                   Reprints and Permissions"},
{"file_id": "497302a", "url": "https://www.nature.com/articles/497302a", "year": 2013, "authors": [{"name": "Tim Appenzeller"}], "parsed_as_year": "2006_or_before", "body": "The earliest known cave paintings fuel arguments about whether Neanderthals were the mental equals of modern humans. In a damp Spanish cave, Alistair Pike applies a small grinder to the world's oldest known paintings. Every few minutes, the dentist-drill sound stops and Pike, an archaeologist from the University of Southampton, UK, stands aside so that a party of tourists can admire the simple artwork \u2014 hazy red disks, stencilled handprints, the outlines of bison \u2014 daubed on the cave wall tens of thousands of years ago. He hopes that the visitors won't notice the small scuff marks he has left. In fact, Pike's grinder \u2014 and the scalpel that he wields to scrape off tiny samples \u2014 is doing no harm to the actual paintings, and he is working with the full approval of the Spanish authorities. Pike is after the crust of calcite that has built up over the millennia from groundwater dripping down the wall. The white flecks that he dislodges hold a smattering of uranium atoms, whose decay acts as a radioactive clock. A clock that has been ticking ever since the calcite formed on top of the art. Hear more on Neanderthal art from archaeologist Jo\u00e3o Zilh\u00e3o and writer Tim Appenzeller. The results of an earlier round of sampling in El Castillo cave, published last June 1 , showed that the oldest of the paintings, a simple red spot, dates to at least 40,800 years ago, roughly when the first modern humans reached western Europe. Pike and his colleagues think that when they analyse the latest samples, the paintings may turn out to be older still, perhaps by thousands of years \u2014 too old to have been made by modern humans. If so, the artists must have been Neanderthals, the brawny, archaic people who were already living in Europe. The answer won't be known for at least a year, but if it favours the Neanderthals, it could tip \u2014 if not resolve \u2014 a debate that has rumbled for decades: did the Neanderthals, once caricatured as brute cavemen, have minds like our own, capable of abstract thinking, symbolism and even art? It is one of the most haunting questions about the people who once shared a continent with us, then mysteriously vanished. An early date for the paintings would also be a vindication for the slight, dark-haired man watching as Pike works: Jo\u00e3o Zilh\u00e3o, who has emerged as the leading advocate for Neanderthals, relentlessly pressing the case that these ice-age Europeans were our cognitive equals. Zilh\u00e3o, an archaeologist at the Catalan Institution for Research and Advanced Studies at the University of Barcelona in Spain, believes that other signs of sophisticated Neanderthal culture have already proved his point. But he is willing to debate on his opponents' terms. \u201cTo my mind, we don't need that evidence,\u201d he says of the paintings. \u201cBut I guess for many of my colleagues this would be the smoking gun.\u201d The front line in the Neanderthal wars runs through another cave: Grotte du Renne, 1,000 kilometres away in central France. As early as the 1950s, excavations there unearthed a collection of puzzling artefacts. Among them were bone awls, distinctive stone blades and palaeolithic baubles \u2014 the teeth of animals such as foxes or marmots, grooved or pierced so that they could be worn on a string. They were buried beneath artefacts typical of the first modern humans in Europe, suggesting that these objects were older. A startling possibility loomed: that artefacts of this style, collectively known as the Ch\u00e2telperronian industry, were made by Neanderthals. \n               Minds at work \n               Close cousins of modern humans, Neanderthals evolved in western Eurasia and had Europe to themselves for more than 200,000 years, enduring several ice ages. In spite of their survival skills and big brains \u2014 comparable to our own \u2014 they had never been linked to sophisticated tools of this kind, or to ornaments. Yet in 1980, archaeologists reported finding a Neanderthal skeleton among Ch\u00e2telperronian tools at another site in France 2 . And in 1996, French palaeoanthropologist Jean-Jacques Hublin and his colleagues reported that a skull fragment from the ornament layer in the Grotte du Renne was unmistakably Neanderthal 3 . Ever since then, the Grotte du Renne has been exhibit A in the case that Neanderthals, like ourselves, trafficked in symbols, using ornaments as badges of identity for individuals or groups. Hublin himself did not go that far. He suggested that the Neanderthals had fallen under the spell of strange new neighbours: modern humans, who were thought to have reached Europe around the time of the Ch\u00e2telperronian industry. Neanderthals might have acquired the ice-age bling from modern humans, or made the pendants themselves under the influence of the new arrivals. That conclusion infuriated Zilh\u00e3o, turning him into the passionate advocate he is today. He questioned the evidence that modern humans were already on the scene and detected a bias against our extinct cousins. \u201cWhy was the equally if not more legitimate hypothesis \u2014 that the Neanderthals themselves had been the authors of this stuff and made it for their own use \u2014 not even considered?\u201d asks Zilh\u00e3o. On a visit to rock-art sites in Portugal, he discussed the paper with Francesco d'Errico, an archaeologist who is now at the University of Bordeaux in France. D'Errico had the same reaction, Zilh\u00e3o recalls. \u201cAnd he said: 'OK, let's do something about it.'\u201d Since then, the pair has fought a two-front war, advancing evidence for Neanderthal capabilities while challenging studies that reserve symbolism and abstract thinking for modern humans. \n               Unknown artists \n             More than 15 years later, the Grotte du Renne continues to be a battleground. Since 2010, three papers have given duelling interpretations of the artefact-bearing layers. In the first, a group led by dating expert Thomas Higham of the University of Oxford, UK, used new carbon dates to argue that the layers were scrambled, mixing older remains with younger 4 . If that was correct, said Higham's team, the relics adjacent to the telltale skull fragment might not have belonged to Neanderthals after all. Within months, Zilh\u00e3o, d'Errico and their colleagues fired back with an analysis 5  of how artefacts of different types were distributed in the Grotte du Renne, concluding that the layers were undisturbed and that the Neanderthal link could be trusted. A group led by Hublin (now at the Max Planck Institute for Evolutionary Anthropology in Leipzig, Germany) presented its own dates last year, backing Zilh\u00e3o's claim 6 . But Hublin still denied the Neanderthals full credit. Neanderthals did make the objects, now dated to between 45,000 and 40,000 years ago, he said \u2014 but only after they encountered modern humans. And this time he had fresh evidence to draw on. Carbon dates measured by Higham and others at caves in Italy, Britain and Germany suggest that modern humans began expanding into Europe as early as 45,000 years ago, several thousand years earlier than was thought (see  Nature   485 , 27\u201329; 2012 ). Zilh\u00e3o strenuously disputes those claims, doubting whether the shells or animal bones used for dating truly reflect the age of the human fossils at the sites, or whether the human remains are modern. \u201cThe evidence to show an early presence of modern humans in Europe is worse today than it was 20 years ago,\u201d he declares. Hublin, however, has no doubt that our ancestors had already entered the picture when Neanderthals in France began making bone awls and animal-tooth pendants. To assume that Neanderthals invented these technologies on their own is to accept \u201can incredible coincidence\u201d, he says. \u201cJust as modern humans arrive with these things in their pocket \u2014 bingo!\u201d \n               Like minds \n             Despite the stalemate, Zilh\u00e3o says that the record of Neanderthal behaviour tens of thousands of years before modern humans arrived in Europe proves his point (see 'Minds at work'). Neanderthals are believed to have buried their dead, suggesting that they had some kind of spirituality. They made glue for securing spear points by heating birch sap while protecting it from the air, a feat that even modern experimental archaeologists have trouble replicating. Many Neanderthal sites include lumps of pigment \u2014 red ochre and black manganese \u2014 that sometimes seem to be worn down like stone-age crayons. Zilh\u00e3o and others think that the Neanderthals painted themselves, creating striking patterns on their pale, northern skin that were every bit as symbolic as the art and ornaments of modern humans. \u201cYou don't need to have shell beads, you don't need to have artefacts with graphical representation to have behaviour that can be defined archaeologically as symbolic,\u201d he says. \u201cBurying your dead is symbolic behaviour. Making sophisticated chemical compounds in order to haft your stone tools implies a capacity to think in abstract ways, a capacity to plan ahead, that's fundamentally similar to ours.\u201d Where Zilh\u00e3o sees a clear pattern, sceptics see uncertainties. Harold Dibble, an anthropologist at the University of Pennsylvania in Philadelphia, is re-examining supposed Neanderthal burial sites. At one, the French cave of Roc de Marsal, he says that what seemed to be a deliberately excavated grave is actually a natural pit. At another, La Ferrassie, he sees evidence that sediments swept into the cave by water \u2014 not grieving kin \u2014 could have buried Neanderthal remains. As for the ochre crayons, Dibble is dismissive. \u201cYou see some wear on a piece of ochre and soon you've got Neanderthal body painting,\u201d he says. \u201cWhat a lot of logical leaps.\u201d He and others say that the pigment has many possible uses: as an insect repellent, a preservative for food or animal skins, an ingredient in adhesives. Even Wil Roebroeks of the University of Leiden in the Netherlands, who found evidence for ochre use as early as 250,000 years ago at a Dutch Neanderthal site 7 , says that Zilh\u00e3o \u201cjumps too fast from the presence of ochre to body decoration\u201d. Ask Dibble, Hublin and other sceptics what would persuade them that Neanderthals had minds like ours, and their answer is simple: a pattern of art or other sophisticated symbolic expression from a time when no modern humans could possibly have been around. \u201cBut I don't think it exists,\u201d says Hublin. Zilh\u00e3o, however, points to a singular finding from a Neanderthal site in southern Spain that he reported three years ago 8 : three cockle shells each with holes near one edge, as if they had been worn as ornaments. One contains a trace of red pigment, and a fourth shell is stained with a mixture of colours, as if it had been used as a paint container. The shells, says Zilh\u00e3o, imply symbolic thinking fully equivalent to that of the modern humans who left troves of beads in South Africa 75,000 years ago. And at roughly 50,000 years old, he says, the Spanish shells date from a time well before modern humans reached the region. Critics are not satisfied. The perforations are natural, as Zilh\u00e3o himself noted, which suggests to Hublin and Dibble that rather than systematically fashioning ornaments, Neanderthals might have picked up a few odd shells on a whim. \u201cWhen you've got isolated occurrences, one-offs, that's not going to convince most of us,\u201d says Dibble. The paintings in El Castillo could help to establish a pattern. The research group was conservative with the ages it reported last June 1 , which put the earliest calcite at nearly 41,000 years old. Nervous about damaging the pigment, the team left several millimetres of the veneer intact at each sampled spot. Deeper, older layers might push back the paintings' minimum ages by several thousand years. That prospect brought the team back to El Castillo last October. Grinding and scraping through a long day, the researchers concentrate on the red disks and hand stencils that had yielded the earliest dates last time around. The goal, says Zilh\u00e3o, is \u201cto date pigments in these paintings to an age that is clearly and to everyone's satisfaction beyond the range of modern humans in Europe\u201d. Yet an early date may not settle the long-running dispute. Hublin sets the bar high. \u201cIf Zilh\u00e3o finds a date of earlier than 50,000 years ago, I'll be convinced!\u201d he says. Any younger, and modern human influence would remain a possibility, he says, noting recent hints that our ancestors had advanced into Turkey or even central Europe by 50,000 years ago. And one example of crude painting \u2014 what Dibble calls \u201cNeanderthal doodling\u201d \u2014 might not be enough to win over the doubters. Zilh\u00e3o's knockout blow may simply lead to more fighting. Yet signs of a middle ground are emerging. Chris Stringer, a palaeoanthropologist at the Natural History Museum in London, says that 20 years ago, he believed that if the Neanderthals made the Ch\u00e2telperronian ornaments, they were blindly imitating modern humans. \u201cOur interpretation was that they were copying but that they didn't have the brainpower to give full value\u201d to the objects. He wouldn't say so now. Two decades of discoveries of sophisticated Neanderthal tools and weapons have made him think that \u201cthe gulf was not as great\u201d: that the difference between Neanderthals and ourselves was a matter more of culture than of ability. \u201cYou can see the Neanderthals were held back by various factors that were not down to their brains,\u201d he adds. The climate of ice-age Europe kept their population size \u201cfrighteningly small\u201d, he says \u2014 at times just a few thousand people across a whole continent, most of them dead by the age of 30. How could such a sparse, beleaguered people develop and sustain a sophisticated culture? That's not so different from what d'Errico, Zilh\u00e3o's comrade-in-arms for almost 20 years, now says. He still thinks that the Neanderthals probably invented the Ch\u00e2telperronian artefacts before modern humans were on the scene. But he is open to the idea that aspects of modern human culture preceded their wholesale arrival in Europe. \u201cIt's possible that some influence did spread,\u201d says d'Errico. \u201cI'm less militant than Jo\u00e3o.\u201d That takes nothing away from the Neanderthals, he adds. \u201cThe fact that Neanderthals can absorb influences, can re-elaborate them, can make them part of their own culture, is very modern behaviour.\u201d But there is a final stretch of ground that neither side will concede. Were the Neanderthals truly the same as us, cognitively? No, says Stringer. The Neanderthal genome, decoded 9  in 2010, differs from that of modern humans in some regions linked to brain function, he notes. And this year, he suggested that, compared with modern humans, larger volumes of Neanderthals' brains were devoted to vision and to controlling their heavier bodies 10  (see \u2018Two kinds of human\u2019). That might have left them with less capacity for social awareness and interaction. \u201cIf you imagine a Neanderthal in modern society, there would still be differences,\u201d says Stringer. Zilh\u00e3o rejects any distinctions. Emerging from the cave into a rainy evening, he muses that if he pushes back the age of the El Castillo paintings, his critics may argue that he has simply proved an earlier presence of modern humans in Europe. \u201cTo which I will say, 'Of course. Neanderthals were modern humans too.'\u201d \n                     Human evolution: Cultural roots 2012-Feb-15 \n                   \n                     Palaeoanthropology: The earliest modern humans in Europe 2011-Nov-23 \n                   \n                     African cave's ancient ochre lab 2011-Oct-13 \n                   \n                     Nature special: Peopling the planet \n                   Reprints and Permissions"},
{"file_id": "499398a", "url": "https://www.nature.com/articles/499398a", "year": 2013, "authors": [{"name": "Beth Mole"}], "parsed_as_year": "2006_or_before", "body": "Microbiologists are trying to work out whether use of antibiotics on farms is fuelling the human epidemic of drug-resistant bacteria. The sight of just one boot coming through the doorway cues the clatter of tiny hoofs as 500 piglets scramble away from Mike Male. \u201cThat's the sound of healthy pigs,\u201d shouts Male, a veterinarian who has been working on pig farms for more than 30 years. On a hot June afternoon, he walks down the central aisle of a nursery in eastern Iowa, scoops up a piglet and dangles her by her hind legs. A newborn piglet's navel is an easy entry point for bacterial infections, he explains. If this pig were infected, she would have an abscess, a lump of inflamed tissue, just below the navel. \u201cIn human terms, she'd be an outie instead of an innie,\u201d he says, rubbing the pig's healthy, pink belly button. Nearly six years ago, an outbreak of 'outies' at this nursery marked the first known infection with methicillin-resistant  Staphylococcus aureus  (MRSA) in pigs in the United States. MRSA has troubled hospitals around the world for more than four decades and has been infecting people outside of health-care settings since at least 1995 (see  Nature   482 , 23\u201325; 2012 ). It causes around 94,000 infections and 18,000 deaths annually in the United States. In the European Union, more than 150,000 people are estimated to contract MRSA each year. Its first appearance on a US farm signalled the expansion of what many believe is a dangerous source of human infection. Male investigated the infections with Tara Smith, an epidemiologist at the University of Iowa in Iowa City, who has since launched one of the most comprehensive investigations yet of where MRSA lives and how it spreads into and out of agricultural settings. She has surveyed farms and grocery stores as well as people's homes, noses and pets. Her findings could help to end a raging debate about whether farms' use of antibiotics is contributing to the rise of drug-resistant bacterial infections in humans. Scientists and health experts fear that it is, and that drug-resistant bacteria from farms are escaping via farmworkers or meat. Last year, the US Food and Drug Administration (FDA) recommended more restraint in the use of antibiotics in livestock, following the lead of regulatory authorities in other countries (see  Nature   481 , 125; 2012 ). But the meat and agricultural industries are fighting those restrictions. They claim that MRSA and other drug-resistant bacteria that cause human infections arise in hospitals, and that meat production includes safety measures, such as sanitation rules in slaughterhouses, that prevent resistant bacteria from spreading to and infecting people. \u201cThere's a long way between the farm and the table,\u201d says Ron Phillips, a representative for the Animal Health Institute, a trade organization based in Washington DC that represents veterinary-medicine companies. The major problem has been lack of data. Many farmers are reluctant to allow scientists access to their facilities, and farmworkers \u2014 many of whom, in the United States, are undocumented immigrants \u2014 are wary of anyone who might want to sample them. But Smith and a small group of researchers are starting to fill the void. They have \u201creally shaped the state of knowledge in the United States\u201d, says Christopher Heaney, an epidemiologist at Johns Hopkins University in Baltimore, Maryland. Smith's current research, says Heaney, could allow officials to \u201ctruly say where these bacteria in people's noses are coming from\u201d. \n               Profit and loss \n             At a concentrated animal-feeding operation (CAFO) about an hour's drive west from Ames, Iowa, the usual din of the nursery is punctuated by the sound of piglets sneezing, thanks to an outbreak of H1N2 influenza. Craig Rowles, a veterinarian and the farm's manager, surveys his charges, some of which have mucus dripping from their snouts. \u201cIt's just like when you bring kids to a day-care centre,\u201d he says. \u201cAfter a while, they're going to come home with a snotty nose.\u201d Rowles is using a vaccine to fight the outbreak, but he is also dosing the pigs with two antibiotics \u2014 chlorotetracycline and Denagard \u2014 to prevent secondary bacterial infections. The combination is also routinely used to prevent bacterial diarrhoea and other common ailments in piglets. Such practices have been common for decades. But few CAFOs have veterinarians on staff to advise on antibiotic use. They are not required to: veterinary antibiotics are generally available over the counter. And some CAFO operators use antibiotics much more liberally than Rowles does. Small doses of antibiotics in feed curb low-grade infections that might otherwise stymie livestock growth. Studies have found that certain antibiotics can increase pigs' growth rate by 2.5%, enough to make the difference for farmers between profit and loss. In the current US market, a farmer might get around US$1 per pound for a pig that costs about $0.94 per pound to produce. Although farm owners do not always reveal the quantities or types of antibiotics they use, an analysis of FDA data by researchers at the Johns Hopkins Center for a Livable Future in Baltimore found that in 2009, some 13.1 million kilograms \u2014 80% of the antibiotics sold in the United States that year \u2014 were used on farms. Antibiotic use on such a broad scale leads to resistant microbes. In a 1976 study, Stuart Levy, a microbiologist at Tufts University School of Medicine in Boston, Massachusetts, found that when farmers started using tetracycline, the numbers of tetracycline-resistant bacteria on the farms spiked 1 . Within months, resistance had spread to microbes in farmworkers' intestinal tracts. \u201cYou don't have to look that far to see resistant bacteria moving to the environment,\u201d Levy says. In humans,  S. aureus  generally lives peacefully on the skin and in the nose. But if the bacterium enters the body through a wound, for example, it can become an aggressive pathogen and eventually make its way into the bloodstream to cause deadly infections. Most infections succumb to antibiotics, but resistant varieties, including MRSA, can be difficult if not impossible to cure. There are 270,000 strains, each potentially harmful. Smith and her colleagues are distinguishing the strains of  S. aureus  around Iowa City in part by sequence type (ST), a categorization based on DNA sequences from several places in the genome. The sequence type that Male and Smith found in the Iowa nursery in 2007 was ST398 (ref.  2 ). Before then, researchers had seen ST398 mostly in Europe, where it was found in livestock and farmworkers but usually did not cause infection. But two years earlier, ST398 had been reported in a hospital in Hong Kong 3 , from patient samples dating back to the early 2000s. For Smith and her collaborator Lance Price, an epidemiologist at George Washington University in Washington DC, it was a sign that the boundaries between animal and human infections were blurring. In February 2012, Price, Smith and their colleagues published a genetic analysis of strains related to ST398 isolated from animals and humans around the world 4 . They found that the lineage that gave rise to ST398 originated in humans. At some point, it crossed into livestock, where it acquired genes conferring drug resistance and a preference for pigs before jumping back to humans. As of 2012, ST398 was the cause of up to 20% of human cases of MRSA in the Netherlands, although the infections are generally mild. \u201cNo one has even looked at these strains in the United States,\u201d says Smith. Doctors, she says, often don't determine what strain of MRSA is causing an infection, so it is possible that the bug has been stealthily migrating between farms and hospitals for years. Smith's next step was to see if there were other ways MRSA might make it off the farm. At a grocery store on the outskirts of Iowa City, Smith pulls a shopping cart from a metal corral, takes a sterile swab from her purse, wipes the cart's handle and deposits the swab in a plastic sheath. Then she heads to the meat case. \u201cI think the average consumer doesn't think of this as a risk,\u201d Smith says, picking up a shrink-wrapped tray of bright red steaks. Her swabs have told a different story. Beginning in January 2012, Smith and her research assistant, Dipendra Thapaliya, spent a year collecting weekly swabs and meat samples from local grocery stores, including this one. They found  S. aureus  on nearly every type of surface. Five per cent of grocery carts carried MRSA. Of meat samples, 30% harboured  S. aureus , 11% had  S. aureus  resistant to multiple antibiotics and 3% carried MRSA. The data, which have not been published, also showed that pork products had some of the highest levels of MRSA, whereas meat labelled 'antibiotic free' had little or none. This mirrors what Smith and her colleagues found in samples from farms across the state. \n               On the menu \n             Now Smith is conducting detailed genetic analyses of the samples to identify MRSA subtypes and where they might have originated. She and her colleagues have found ST398 in the grocery-store samples. But to their surprise, they also found ST5, which is generally found in hospitals and in 'community' infections with no obvious link to farms or hospitals. Last year, Smith and Timothy Frana, a microbiologist and veterinarian at Iowa State University in Ames, found that veterinary students who carried MRSA in their noses after visiting pig farms picked up mainly ST5, most of which was resistant to tetracycline 5 . The presence of ST5 among livestock suggests that  S. aureus  strains may move easily between pigs and people, and may become resistant on farms. \u201cIt's the most interesting finding from our study,\u201d says Frana. In lab tests, Smith and her colleagues have found that 30% of the  S. aureus  harboured in meat is resistant to tetracycline. Given Levy's data from the 1970s, this is not surprising, says Smith. But researchers, including Heaney and his group at Johns Hopkins, are finding that a bacterium's drug-resistance profile can give information about where the bug came from that sequence type and other gene-based categorizations may not. In a study comparing workers from different farms, tetracycline-resistant MRSA showed up only in workers from farms where antibiotics were used 6 . In the last phase of their research, Smith and her colleagues will try to determine whether MRSA is trafficking between farms, households and clinics (see 'From the farm and back again'). They are taking nasal samples from 1,300 people and swabbing doorknobs, kitchen sinks and even family pets in 96 households around Iowa City. The researchers are comparing the  S. aureus  strains they find in these samples with the MRSA strains they have found in grocery stores and farms and with strains local doctors find in infected patients. If a strain shows up in all the locations, the researchers will sequence the whole genomes of individual isolates to retrace their movement and evolution. The results have the potential to create the first complete link between farms and clinical cases of MRSA. \n               Ban versus beast \n             CAFO supporters acknowledge that farm strains of drug-resistant bacteria could theoretically spread to people. But \u201cI don't see this equating to human health risk\u201d, says Scott Hurd, a veterinarian and epidemiologist at Iowa State University who has conducted multiple studies to assess the risk of drug-resistant bacteria spreading through meat production. He says that the average person has a greater chance of dying from a bee sting than of contracting MRSA from pork. Hurd argues that limiting the use of antibiotics on farms could be harmful to human health. Even Smith's grocery-store study found that meat sold as 'antibiotic free' had the highest levels of garden-variety  S. aureus , suggesting that untreated animals harbour more pathogens. \u201cAnimals really do need to be treated,\u201d Hurd says. Nevertheless, regulatory authorities have clamped down on antibiotic use on farms. The European Union began phasing out antibiotics for growth promotion in the late 1990s. Denmark led the charge with a full ban in 2000. (China, however, which claims half the world's pig population, has yet to rein in antibiotic use.) The bans' effects on drug resistance and human and animal health have been murky. Levy and other supporters of the bans say that the result in Denmark has been positive, pointing to data showing a drop in the use of antibiotics on farms and an increase in meat production. But opponents, including the Animal Health Institute, point out that the use of antibiotics to treat acute illness in Denmark has increased, as have animal deaths. Last year, amid mounting pressure from several groups, including the National Resources Defense Council, based in New York, the FDA released new guidance calling for the \u201cjudicious use\u201d of antibiotics on farms. The agency discouraged the use of antibiotics for growth promotion and urged label changes to the drugs and more veterinary oversight for their application. Not all the guidelines are yet approved, and compliance is voluntary. Nevertheless, the agency has suggested that it will enforce tougher rules if farmers and drug-makers do not adopt the guidelines within about three years. Few are satisfied with the FDA's policy. Pig farmers and meat-industry representatives consider the move a blow to farmers and animal welfare, and supporters of antibiotic restriction say that the voluntary guidelines do not go far enough. Scientists, meanwhile, have pressed the FDA to reveal more data on how farmers are using antibiotics, so far without success. Smith, who is concerned that farmers are still overusing antibiotics, hopes that the results of her current research will sway their opinions. Antibiotics on farms can trigger the emergence of resistant strains, she says, and those strains turn up on meat, in grocery stores and in homes, and they can infect people. \u201cFor me, that's enough,\u201d she says. At the same time, Smith says that she sympathizes with CAFO operators who are trying to produce meat as safely and efficiently as possible. And although human health should take priority over farm animals, she says, farmers will be reluctant to change until researchers can come up with safe and cost-effective practices to replace the use of antibiotics. For now, Smith says, \u201cwe're kind of stumped\u201d. \n                 See Editorial \n                 page 379 \n               \n                     DNA sequencers stymie superbug spread 2012-Nov-14 \n                   \n                     Vaccine development: Man vs MRSA 2012-Feb-01 \n                   \n                     Rules tighten on use of antibiotics on farms 2012-Jan-10 \n                   \n                     Antibiotic resistance marching across Europe 2011-Nov-22 \n                   \n                     Emerging Microbes & Infections : Drug resistance \n                   \n                     Tara Smith \n                   \n                     FDA guidance on antimicrobial resistance \n                   Reprints and Permissions"},
{"file_id": "499394a", "url": "https://www.nature.com/articles/499394a", "year": 2013, "authors": [{"name": "Maryn McKenna"}], "parsed_as_year": "2006_or_before", "body": "Health officials are watching in horror as bacteria become resistant to powerful carbapenem antibiotics \u2014 one of the last drugs on the shelf. As a rule, high-ranking public-health officials try to avoid apocalyptic descriptors. So it was worrying to hear Thomas Frieden and Sally Davies warn of a coming health \u201cnightmare\u201d and a \u201ccatastrophic threat\u201d within a few days of each other in March. The agency heads were talking about the soaring increase in a little-known class of antibiotic-resistant bacteria: carbapenem-resistant Enterobacteriaceae (CREs). Davies, the United Kingdom's chief medical officer, described CREs as a risk as serious as terrorism (see  Nature 495, 141; 2013 ). \u201cWe have a very serious problem, and we need to sound an alarm,\u201d said Frieden, director of the US Centers for Disease Control and Prevention (CDC) in Atlanta, Georgia. Move over MRSA \u2013 there are new antibiotic-resistant bacteria that could be even more dangerous, as Maryn McKenna explains. Their dire phrasing was warranted. CREs cause bladder, lung and blood infections that can spiral into life-threatening septic shock. They evade the action of almost all antibiotics \u2014 including the carbapenems, which are considered drugs of last resort \u2014 and they kill up to half of all patients who contract them. In the United States, these bacteria have been found in 4% of all hospitals and 18% of those that offer long-term critical care. And an analysis carried out in the United Kingdom predicts that if antibiotics become ineffective, everyday operations such as hip replacements could end in death for as many as one in six 1 . The language used by Davies and Frieden was intended to break through the indifference with which the public usually greets news about antibiotic resistance. To close observers, however, it also had a tinge of exasperation. CREs were first identified almost 15 years ago, but did not become a public-health priority until recently, and medics may not have appreciated the threat that they posed. Looking back, say observers, there are lessons for researchers and health-care workers in how to protect patients, as well as those hospitals where CREs have not yet emerged. \u201cIt is not too late to intervene and prevent these from becoming more common,\u201d says Alexander Kallen, a medical epidemiologist at the CDC. At the same time, he acknowledges that in many places, CREs are here for good. Hindsight is key to the story of CREs, because it was hindsight that identified them in the first place. In 2000, researchers at the CDC were grinding through analyses for a surveillance programme known as Intensive Care Antimicrobial Resistance Epidemiology (ICARE), which had been running for six years to monitor intensive-care units for unusual resistance factors. In the programme's backlog of biological samples, scientists identified one from the Enterobacteriaceae family, a group of gut-dwelling bacteria. This particular sample \u2014 of  Klebsiella pneumoniae , a common cause of infection in intensive-care units \u2014 had been taken from a patient at a hospital in North Carolina in 1996 (ref.  2 ). It was weakly resistant to carbapenems, powerful broad-spectrum antibiotics developed in the 1980s. Antibiotics have been falling to resistance for almost as long as people have been using them; Alexander Fleming, who discovered penicillin, warned about the possibility when he accepted his Nobel prize in 1945. Knowing this, doctors have used the most effective drugs sparingly: careful rationing of the powerful antibiotic vancomycin, for example, meant that bacteria took three decades to develop resistance to it. Prudent use, researchers thought, would keep the remaining last-resort drugs such as the carbapenems effective for decades. The North Carolinan strain of  Klebsiella  turned that idea on its head. It produced an enzyme, dubbed KPC (for  Klebsiella pneumoniae  carbapenemase), that broke down carbapenems. What's more, the gene that encoded the enzyme sat on a plasmid, a piece of DNA that can move easily from one bacterium to another. Carbapenem resistance had arrived. At first, however, microbiologists considered this CRE to be a lone case. Jean Patel, a microbiologist who is now deputy director of the CDC's office of antimicrobial resistance, says that CDC staff were reassured by the fact that the sample had been collected four years earlier and that testing of the remaining archives revealed no further instances of resistance. \u201cIt wasn't that there was a lack in interest in looking for these,\u201d Patel says. Instead, the attitude at the time was, \u201cWe have a system for identifying these and it's working, and if more occur we'll hear about it\u201d. But the CDC's surveillance programme was limited: it tracked only 41 hospitals out of some 6,000 and its analyses lagged far behind sample collection. So when carbapenem resistance emerged again, years passed before anyone noticed. \n               A dire trend \n             The State University of New York (SUNY) Downstate Medical Center in Brooklyn draws patients from some of the poorest neighbourhoods in New York City, so it tends to be a place where dire health trends surface. It was not part of the CDC's ICARE programme, but physicians there conduct their own bacterial surveillance to scan for emerging infectious threats. In 2003, a review of results from the centre's microbiology lab and some collaborating ones at nearby hospitals picked up something that city physicians had never seen before. Over the previous six years, a handful of patients spread across seven institutions had been diagnosed with  Klebsiella  infections that were partially resistant to carbapenems. \u201cThese had been infrequent and they were flying under the radar,\u201d says John Quale, a medical researcher at Downstate. \u201cAnd at about the time we picked them up, they just exploded.\u201d The infections were very serious. In one Brooklyn hospital outbreak, 9 out of 19 patients died. In another, two infections blossomed into more than 30 in just six months, despite stringent infection-control measures. And the organism spread around the city \u2014 from Harlem Hospital at the north end of Manhattan to Mount Sinai Hospital on the Upper East Side, and then to Saint Vincent's in Greenwich Village in the south, where one patient died of a  Klebsiella  infection despite doctors throwing every drug they could at it. One of the reasons why the resistant strains spread so rapidly was that they were difficult to detect. Most clinical microbiology labs no longer painstakingly culture bacteria over days to determine which drugs they are susceptible to: instead, automated systems, which expose bacteria to graduated dilutions of drugs, can give a result in hours. But these tests, Quale and his collaborators realized, were giving misleading results and were causing physicians to give patients doses or drugs that would not work. And because the infections were not eliminated, the resistant strain could be passed on. By 2007, 21% of all  Klebsiella  bacteria in New York City carried the carbapenem-resistance plasmid, compared with an average of 5% across the rest of the United States 3 . Such a rapid dissemination hinted that CREs were travelling from person to person rather than arising independently in each location. This made sense. Many Enterobacteriaceae, including  Klebsiella , reside in the intestines and can easily be carried by an asymptomatic patient. If patients develop diarrhoea, as often happens after the administration of drugs during intensive care, the infectious bacteria can spread far, contaminating equipment or the hands of care-givers inside the hospital and out. So it was easy to imagine how CREs might ride the subway from Brooklyn to Manhattan. But it took a few years, and a much larger outbreak, to illustrate just how far CREs had travelled (see 'The resistance movement'). \n               Rapid spread \n             In late 2005, one patient at Tel Aviv's Sourasky Medical Center was diagnosed with a KPC-positive infection that was closely related to a New York strain. Within months, CRE infections stormed through the hospital, and then through Israel's small, tight-knit health-care system. By March 2007, there were 1,275 cases nationwide 4 . They were turning up across a network of hospitals, nursing homes, dialysis clinics and rehab centres. Israel has a shortage of acute-care beds, explains Mitchell Schwaber, an infection-control physician who was on the Sourasky faculty when the KPC epidemic began. \u201cWhenever a patient can be discharged, especially from internal medicine, they are \u2014 which creates a lot of movement from acute-care facilities to long-term care facilities, and then back to either the same hospital or a different one.\u201d In response, the Israeli Ministry of Health created a national task force on CREs, headed by Schwaber. It demanded daily national-surveillance reports by e-mail, and instituted strict isolation precautions, including dedicated wards, equipment and nurses. The new rules were backed up by surprise inspections and mandatory lab analyses to ascertain where new infections were coming from. By mid-2008, Israel had reversed its soaring trend of resistant  Klebsiella . But control came too late to prevent the pathogen from emigrating: patients, physicians and nurses had brought bacteria carrying the KPC enzyme to Italy, Colombia, the United Kingdom and beyond. \n               Alarm call \n             In January 2008, a urine culture performed on a sample from a 59-year-old man hospitalized in Sweden identified a  K. pneumoniae  strain that was resistant to multiple drugs, including carbapenems 5 . But rather than using KPC, the bacterium dismantled the antibiotics with a different enzyme, a metallo-\u03b2-lactamase. Within three years, more cases involving bacteria carrying this enzyme were identified in the United Kingdom and in the United States. These provoked immediate alarm: they were even more resistant to carbapenems than the KPC-carrying  Klebsiella  bacteria, and included other Enterobacteriaceae such as  Escherichia coli . Initially, most individuals carrying bacteria with the new resistance factor had some link to clinics in India, through medical tourism or health care needed while abroad. In accordance with taxonomic conventions, doctors named the new enzyme New Delhi metallo-\u03b2-lactamase (NDM), after the place where the initial Swedish patient was thought to have picked it up. The name proved unexpectedly controversial: Indian media and the Indian parliament denounced the acronym for stigmatizing India's medical-tourism industry. Further work by the team that first identified NDM only increased the outrage when it established that bacteria carrying the enzyme were present in sewage and municipal water in south Asia 6  (see  Nature   http://doi.org/dgcs33 ; 2011 ). The controversy obscured NDM's real significance: not only had another resistance mechanism emerged, but CREs were now flourishing beyond hospital walls. Researchers were still struggling to pin down exactly how NDM was spreading. In the second half of 2012, staff at the University of Colorado Hospital in Aurora discovered that their institution had unknowingly hosted eight patients with NDM-positive  Klebsiella  bacteria, the largest cluster in the United States so far. The first three cases, all in patients with pneumonia, were found during a routine review of clinical specimens. When the hospital escalated its search, it also identified five asymptomatic carriers. \u201cThere was no obvious pattern,\u201d recalls Michelle Barron, the hospital's infection-control physician. \u201cThese patients had been in the hospital a long time. They had been on multiple units. There was no single piece of equipment that had been used on all of them.\u201d Even when the CDC sequenced the bacterial genomes from all eight patients, the data could not explain how the bacterium had spread. Barron hypothesizes that, at some point, the hospital harboured a \u201cghost patient\u201d \u2014 someone who escaped detection despite the surveillance dragnet. She is still looking for that person: the hospital is attempting to call back and sample all 1,700 patients who were treated during the crucial period. The episode ended well. The five carriers never fell ill, the three who were ill recovered, and once the hospital became aware of the cluster no further spread occurred. They might not be so lucky next time. Fresh threats are on the way. Researchers have spotted other carbapenem-resistance factors moving around the globe; one has already appeared in the United States, and others are clustered in southern Europe and South America. Because each is genetically different, they are likely to present new challenges to detection. Infectious-disease specialists say that they have learned major lessons from CREs. Drug-resistant bacteria can emerge and spread much faster than patchy public-health surveillance systems and outdated laboratory-detection methods can pick them up, and what seems like adequate infection control cannot always contain their spread. Some countries are trying to take those lessons on board. Hospitals in Israel now practise 'active surveillance', meaning that if a new patient has been to any other health-care institution in the past six months they are checked for CREs. And anyone who tests positive for such bacteria is flagged as a carrier in national-health records, which are accessible to hospitals, nursing homes and community physicians. France and the United Kingdom follow similar rules, but unfortunately many countries do not. Earlier this month, the European Centre for Disease Prevention and Control in Stockholm published a candid self-assessment by 39 European countries of their CRE burden and ability to counter these organisms 7 . Only 21 said they have achieved the kind of national coordination that allowed Israel to contain its epidemic. The United States operates a patchwork of surveillance systems. The CDC looks for CREs through three separate data networks, but none of these covers the entire country. At least nine states have made reporting CRE cases to their health departments mandatory. The CDC has also created a robust tool kit of best practices for health departments and hospitals, such as restricting staff assignments and equipment use in hospitals, and identifying infections in the long-term care facilities that feed patients into hospitals. These measures helped institutions in Illinois and Florida to shut down outbreaks in 2008 and 2009. \n               Limited options \n             Meanwhile, lab-detection methods have improved; the CDC's use of whole-genome sequencing to solve the Colorado episode was the first time that the agency deployed that technology to tackle a hospital outbreak. And public-health departments' ability to identify threats has been bolstered by boluses of federal money after the 2001 World Trade Center attack and subsequent anthrax attacks, and in the 2009 stimulus package. But these investments might be rolled back during the current federal budget sequester. Physicians who treat patients unlucky enough to be caught up in these outbreaks have no better medicines than they did when CREs first emerged. Some organisms respond to two drugs, tigecycline and colistin (also known as polymyxin E). Neither works in every patient, and colistin is notorious for damaging the kidneys. Physicians find themselves caught between using bad drugs or using no drugs at all. It seems unlikely that new drugs will become available soon. Perversely, the rapid advance of resistance and the consequent need to use these drugs sparingly has convinced pharmaceutical companies that antibiotics are not worth the investment. That means, say infectious-disease experts, that their best tools for defending patients remain those that depend on the performance of health personnel: handwashing, the use of gloves and gowns, and aggressive environmental cleaning. Yet even research that could improve best practices has been short-changed, says Eli Perencevich, an infectious-diseases physician and epidemiologist at the University of Iowa in Iowa City who studies how resistant bacteria move around hospitals. \u201cWe haven't invested in research in how to optimize even standard infection-control practices. We just blame the health-care workers when they go wrong.\u201d \n                 See Editorial \n                 page 379 \n               \n                     MRSA: Farming up trouble 2013-Jul-24 \n                   \n                     Antibiotic threat 2013-Jul-24 \n                   \n                     The antibiotic alarm 2013-Mar-12 \n                   \n                     India moves to tackle antibiotic resistance 2012-Sep-11 \n                   \n                     Drug-resistant bacteria go undetected 2012-Apr-27 \n                   \n                     Antibiotic resistance shows up in India's drinking water 2011-Apr-07 \n                   \n                     Emerging Microbes & Infections : Drug resistance \n                   \n                     CDC guidance for CRE control \n                   Reprints and Permissions"},
{"file_id": "499268a", "url": "https://www.nature.com/articles/499268a", "year": 2013, "authors": [{"name": "M. Mitchell Waldrop"}], "parsed_as_year": "2006_or_before", "body": "Confronted with the explosive popularity of online learning, researchers are seeking new ways to teach the practical skills of science. The academic world is in upheaval over MOOCs: massive open online courses that make university lectures available to tens of thousands of students at a time. For roughly a year, universities around the world have been rushing to partner with the major MOOC companies in a move that many believe could revolutionize higher education (see  Nature   495 , 160\u2013163; 2013 ). But for many people working in education, MOOCs do not yet take the revolution far enough. Online lectures by video are fine for conveying facts, formulas and concepts, but by themselves they cannot help anyone learn how to put those ideas into practice. Nor can they give students experience in planning an experiment and analysing data, participating in a team, operating a pipette or microscope, persevering in the face of setbacks or exercising any of the other practical and social skills essential for success in science 1 . \u201cYou only understand something when you know how to do it,\u201d says Chris Dede, who studies simulations for education at Harvard University in Cambridge, Massachusetts. \u201cAnd that's not possible to abstract in a lecture.\u201d Almost by definition, practical skills have to be acquired through experience. They require the hands-on, problem-solving activities that have traditionally been the domain of laboratory courses, field trips, internships and, eventually, project work in the lab of a more senior academic. Bringing such experiences online is tricky, but education-technology researchers have been making substantial progress over the past decade. Thanks to smartphones, immersive gaming software and other rapidly evolving technologies, says James Gee, an education-technology researcher at Arizona State University in Tempe, \u201cwe can do problem-focused learning way better now\u201d \u2014 and can make it available to students around the planet. \u201cIt's a way to give everyone the kind of education we used to think of as a luxury,\u201d he says. \n               Remote hands-on \n             In the sciences, the standard vehicle for teaching practical skills is the lab course. \u201cLabs are where we offer students the opportunity to engage with real lab equipment, to analyse authentic data, to experience the wonder of observation,\u201d says Mike Sharples, an education-technology researcher at the Open University in Milton Keynes, UK. But herding students into a conventional lab has never been an option for the Open University, which was founded in 1969 to offer degrees at a distance and now has more than 240,000 students around the world. Until the late 1990s, the science courses would post students kits that might include microscopes, circuit boards, chemistry sets, fish tanks or even lasers. The students would do experiments at home and then post the kits back. But that was expensive and cumbersome, says Sharples. Today, almost all the lab work is available online through the university's OpenScience Laboratory. Just like many working scientists, students can collect real data from remotely controlled instruments \u2014 among them a \u03b3-ray spectrometer for identifying elements and isotopes, and a 0.43-metre telescope in Majorca, Spain. Students can also explore real data with simulated instruments such as the virtual microscope, with which they look at high-resolution images instead of real specimens. \u201cThey zoom in, adjust the focus and control where in the sample they're looking,\u201d says Sharples \u2014 just as they would on real instruments. Paulo Blikstein, director of the Transformative Learning Technologies Lab at Stanford University in California, is going further, with a new generation of digital lab courses. One example uses remotely controlled instruments at a centralized biology lab \u2014 a project he is developing with Ingmar Riedel-Kruse, a bioengineer at Stanford. \u201cThe idea is to have a room with 10,000 Petri dishes, each a few millimetres wide, and a robot that works like an ink-jet printer,\u201d says Blikstein. \u201cA student would tell the robot, 'Go to my dish and add X drops,' and a camera would watch what happens.\u201d But some researchers worry that a completely virtual lab can never fully replace time at the bench. If students go on to pursue a master's degree or PhD, they could be at a disadvantage in a real lab. \u201cI'm a conservative in this sense,\u201d says Beverly Park Woolf, a computer scientist who works in digital education at the University of Massachusetts Amherst. \u201cYou should touch the equipment\u201d and get a real sense of what it means to tweak a dial or measure out a reagent, she says. \n               The lab in your pocket \n             Even conventional labs can be disconnected from reality, says Michael Schatz, a physicist at the Georgia Institute of Technology in Atlanta. \u201cStudents get the idea that it's all about some specialized room with specialized equipment,\u201d he says, \u201cand then they walk back out into the real world, where none of what they learned there applies.\u201d That is why Schatz created Introductory Physics I with Laboratory, a MOOC that started in May and is devoted to the elementary science of motion. One of the first MOOCs to thoroughly integrate hands-on learning, it relies on the fact that these days, virtually every student is walking around with a camera-equipped smartphone. \u201cWe start by asking them to go out and capture a video of an object in their environment moving in a constant direction at a constant speed,\u201d says Schatz. (Later labs involve more complex motions, such as a basketball arching toward a hoop.) Next, the students analyse their videos using open-source software that extracts the object's position over time. Then they formulate a theory to explain their data and build models to implement it. Finally, they explain their results and their model in a 5-minute video lab report, which is uploaded to YouTube for the other students to discuss and critique online. It will not be clear how this works with thousands of students until the course is completed in August, says Schatz. But if the approach does prove effective at helping students master the material, he and his colleagues hope that it could be a model for all online science courses. The Open University is also exploring the educational uses of mobile devices. In 2008 it launched iSpot, in which people roaming outdoors can upload digital photographs of plants, birds, insects, fungi and other organisms, along with their best guess as to what they are. The programme, which is used in some of the university's biology courses but is also open to non-students, currently has some 30,000 participants in the United Kingdom and South Africa. Each uploaded photograph sparks a lively online discussion about what the organism is and what its presence means to the health of an ecosystem \u2014 including comments from scientists who are using the iSpot data for their own studies. \u201cSo it's a way of doing practical science outdoors,\u201d says Sharples, \u201cbut in a collaborative way.\u201d In effect, he says, the participants are becoming apprentice biologists. \n               Lab as video game \n             Systems such as iSpot and Schatz's MOOC are heavily influenced by the philosophy of 'enquiry-based' learning. Instead of trying to fill students' heads with knowledge through a lecture or a recipe-style laboratory exercise \u2014 telling them the answer, so to speak \u2014 enquiry-based learning puts them to work in teams, challenges them with a question and lets them struggle to find their own answer. A huge body of evidence 2  suggests that such approaches are much more effective than lecture-and-drill-based techniques, says Blikstein. Unfortunately, he adds, \u201cin any big, national-scale debate, these approaches always lose out\u201d, in part because they are considered too expensive and time-consuming to use in a classroom. These days, however, they are finding a natural venue in the multiuser virtual environments (MUVEs) pioneered by online games such as  World of Warcraft . A prime example is EcoMUVE, a course on ecosystems developed by Dede and his colleagues at Harvard. Students form teams to spend two weeks exploring a virtual pond and its surroundings. One virtual day, they discover that the fish are dying, and they have to find out why. The teams decide what data to gather: they might, for example, measure run-off of potentially contaminated water from the nearby housing development and golf course, monitor changes in pond colour or look at pond life through their virtual microscope. Next, the teams have to work out how to analyse those data and agree on an explanation for what is happening, applying mathematical concepts and debating causality to understand how distant actions, such as the spread of fertilizer on a golf course, can affect ecosystems. The EcoMUVE software has been tested with school students aged 11\u201313, who showed substantial gains in their understanding of concepts such as quantitative measurement, food webs and watersheds. David Shaffer, an educational psychologist at the University of Wisconsin-Madison, and his colleagues are using a similar enquiry-based approach to develop a virtual internship for undergraduate engineering students. \u201cWhen kids show up for their first year they're all excited to design and build stuff,\u201d says Shaffer. But first they have to spend two years taking maths and physics, and many get discouraged. Instead, Shaffer and his team get them building things right away. In the exercise, the students are interns at a fictional dialysis-machine manufacturer, and they form teams to design a next-generation system to filter waste from blood. The students do research and run simulations looking at cost, performance and marketability of various systems and then they work together to decide what final experiments to conduct before making their report. This virtual internship has been tested with students at Wisconsin, the University of Pennsylvania in Philadelphia, and the University of Pittsburgh in Pennsylvania. Before-and-after assessments show that it tends to sustain students' confidence and enthusiasm. Now Shaffer and his collaborators are developing a full simulation-based introductory engineering course, and turning their system into a software platform that can support internships in any subject. Some educators complain that enquiry-based learning lacks focus and instruction. \u201cIf you just let kids interact by themselves in a game environment, they may have fun \u2014 but they won't learn much about science,\u201d says Art Graesser, a psychologist at the University of Memphis in Tennessee. To address that, designers generally include some kind of digital or human mentor to keep students on task. One key element in effective mentoring is conversation, says Graesser. \u201cWhen people just read a textbook or listen to a lecture they get shallow knowledge,\u201d he says. But when they talk about the material they start to understand it deeply. Graesser and his collaborators, for example, have developed a system based on 'trialogues', in which a student interacts with two animated computer agents \u2014 a tutor and a student \u2014 that converse in natural language and adapt their behaviour to the real student's response. Real students can interact with the tutor directly, with the student agent chiming in as a kind of sidekick, or they can deepen their own knowledge by teaching the material to the virtual classmate. One system that uses trialogues is  Operation ARIES! , a game designed to teach critical thinking and scientific reasoning to secondary-school and university students. Players sign up with the 'Federal Bureau of Science' to save Earth from aliens who are using bad science to turn humans into mindless consumers. Students work together to evaluate realistic media reports, blogs and press releases. They identify those that have research flaws such as claims that a correlation implies causation, and are therefore evidence of alien activity. \n               Bringing it together \n             One of the biggest barriers to widespread adoption of systems to teach practical skills is that so many are one-off experiments not connected to MOOCs or anything else. Once the project is finished, \u201cyou end up putting your app up on an obscure website where almost nobody can find it\u201d, says Daphne Koller, co-founder of the largest MOOC company, Coursera in Mountain View, California. Coursera is trying to change that, says Koller \u2014 not only by encouraging experiments such as Schatz's physics MOOC, but also by rewriting its own software so that it can deliver practical apps alongside lecture courses. Once the new version begins to roll out, she says, MOOC instructors should be able to plug in a module for iSpot, for example, or a virtual environment such as EcoMUVE, or a practical-skills app of their own devising. The hope is that this will create a common marketplace for these applications, and give them much wider exposure. But that is just one example of the tectonic shifts that the MOOC revolution has set in motion, says Shaffer, echoing a point made by many other observers. \u201cOur education system is like a big, old, comfortable, fuzzy sweater,\u201d he says. It has lasted forever and seems indestructible. But pull on a loose bit of yarn, say by putting lectures online, and the whole thing starts to unravel. \u201cThe way in which the pattern held together before doesn't work anymore,\u201d he says. The fabric is being reknitted even as we speak, he says, with results that no one can foretell. But that makes this moment exciting, he says. \u201cWe're in a place that we can begin to think about education in a whole new way.\u201d Reprints and Permissions"},
{"file_id": "496416a", "url": "https://www.nature.com/articles/496416a", "year": 2013, "authors": [{"name": "David Adam"}], "parsed_as_year": "2006_or_before", "body": "Research suggests that mental illnesses lie along a spectrum \u2014 but the field's latest diagnostic manual still splits them apart. David Kupfer is a modern-day heretic. A psychiatrist at the University of Pittsburgh in Pennsylvania, Kupfer, has spent the past six years directing the revision of a book commonly referred to as the bible of the psychiatric field. The work will reach a climax next month when the American Psychiatric Association (APA) unveils the fifth incarnation of the book, called the  Diagnostic and Statistical Manual of Mental Disorders  ( DSM ), which provides checklists of symptoms that psychiatrists around the world use to diagnose their patients. The  DSM  is so influential that just about the only suggestion of Kupfer's that did not meet with howls of protest during the revision process was to change its name from  DSM-V  to  DSM-5 . Although the title and wording of the manual are now settled, the debate that overshadowed the revision is not. The stark fact is that no one has yet agreed on how best to define and diagnose mental illnesses.  DSM-5 , like the two preceding editions, will place disorders in discrete categories such as major-depressive disorder, bipolar disorder, schizophrenia and obsessive\u2013compulsive disorder (OCD). These categories, which have guided psychiatry since the early 1980s, are based largely on decades-old theory and subjective symptoms. The problem is that biologists have been unable to find any genetic or neuroscientific evidence to support the breakdown of complex mental disorders into separate categories. Many psychiatrists, meanwhile, already think outside the category boxes, because they see so many patients whose symptoms do not fit neatly into them. Kupfer and others wanted the latest  DSM  to move away from the category approach and towards one called 'dimensionality', in which mental illnesses overlap. According to this view, the disorders are the product of shared risk factors that lead to abnormalities in intersecting drives such as motivation and reward anticipation, which can be measured (hence 'dimension') and used to place people on one of several spectra. But the attempt to introduce this approach foundered, as other psychiatrists and psychologists protested that it was premature. Research could yet come to the rescue. In 2010, the US National Institute of Mental Health (NIMH) in Bethesda, Maryland, launched an initiative, called the Research Domain Criteria project, that aims to improve understanding of dimensional variables and the brain circuits involved in mental disorders. Clinical psychologist Bruce Cuthbert, who heads the project, says that it is an attempt to go \u201cback to the drawing board\u201d on mental illness. In place of categories, he says, \u201cwe do have to start thinking instead about how these disorders are dysregulation in normal processes\u201d. But that will be too late for the  DSM . Kupfer says that he now sees how hard it is to change clinical doctrine. \u201cThe plane is in the air and we have had to make the changes while it is still flying.\u201d \n               Manual evolution \n             The Catholic Church changes its pope more often than the APA publishes a new  DSM . The first and second editions, published in 1952 and 1968, reflected Sigmund Freud's idea of psychodynamics: that mental illness is the product of conflict between internal drives. For example,  DSM-I  listed anxiety as \u201cproduced by a threat from within the personality\u201d. Symptoms were largely irrelevant to diagnosis. Things got more empirical around 1980. Shocked by the discovery that patients with identical symptoms were receiving different diagnoses and treatments, an influential group of US psychiatrists threw out Freud and imported another role model from central Europe: psychiatrist Emil Kraepelin. Kraepelin famously said that the conditions now known as schizophrenia and bipolar disorder were separate syndromes, with unique sets of symptoms and presumably unique causes.  DSM-III , published in 1980, turned this thinking into what is now called the category approach, with solid walls between conditions. When the existing version,  DSM-IV , came out in 1994, it simply added and subtracted a few categories. Since then, an entire generation of troubled individuals has trooped into psychiatric clinics and left with a diagnosis of a  DSM -approved condition, including anxiety disorder, eating disorders and personality disorders. Most of those conditions will appear in the pages of  DSM-5 , the contents of which are officially under wraps until the APA annual meeting \u2014 which starts in San Francisco, California, on 18 May \u2014 but have been an open secret since the APA published a draft on its website last year and invited comment. But even as walls between conditions were being cemented in the profession's manual, they were breaking down in the clinic. As psychiatrists well know, most patients turn up with a mix of symptoms and so are frequently diagnosed with several disorders, or co-morbidities. About one-fifth of people who fulfil criteria for one  DSM-IV  disorder meet the criteria for at least two more. These are patients \u201cwho have not read the textbook\u201d, says Steve Hyman, who directs the Stanley Center for Psychiatric Research, part of the Broad Institute in Cambridge, Massachusetts. As their symptoms wax and wane over time, they receive different diagnoses, which can be upsetting and give false hope. \u201cThe problem is that the  DSM  has been launched into under-researched waters, and this has been accepted in an unquestioning way,\u201d he says. Psychiatrists see so many people with co-morbidities that they have even created new categories to account for some of them. The classic Kraepelian theoretical division between schizophrenia and bipolar disorder, for example, has long been bridged by a pragmatic hybrid called schizoaffective disorder, which describes those with symptoms of both disorders and was recognized in  DSM\u2013IV . Basic research has offered little clarification. Despite decades of work, the genetic, metabolic and cellular signatures of almost all mental syndromes remain largely a mystery. Ironically, the ingrained category approach is actually inhibiting the scientific research that could refine diagnoses, in part because funding agencies have often favoured studies that fit the standard diagnostic groups. \u201cUntil a few years ago we simply would not have been able to get a grant to study psychoses,\u201d says Nick Craddock, who works at the Medical Research Council Centre for Neuropsychiatric Genetics and Genomics at Cardiff University, UK. \u201cResearchers studied bipolar disorder or they studied schizophrenia. It was unthinkable to study them together.\u201d \u201cWe need to give researchers permission to think outside these traditional silos,\u201d says Hyman. \u201cWe need to get them to re-analyse these conditions from the bottom up.\u201d In the past few years, some researchers have taken up the challenge \u2014 and the findings from genetics and brain-imaging studies support the idea that the  DSM  disorders overlap. Studies with functional magnetic resonance imaging show that people with anxiety disorders and those with mood disorders share a hyperactive response of the brain's amygdala region to negative emotion and aversion 1 . Similarly, those with schizophrenia and those with post-traumatic stress disorder both show unusual activity in the prefrontal cortex when asked to carry out tasks that require sustained attention 1 . And in the largest study yet undertaken to try to pinpoint the genetic roots of mental disorder, a group led by Jordan Smoller at the Massachusetts General Hospital in Boston screened genome information from more than 33,000 people with five major mental-health syndromes, looking for genetic sequences associated with their illness 2 . At the end of February, the team reported that some genetic risk factors \u2014 specifically, four chromosomal sites \u2014 are associated with all five disorders: autism, attention deficit hyperactivity disorder, bipolar disorder, major depression and schizophrenia. \u201cWhat we see in the genetics mirrors what we see in the clinic,\u201d Hyman says. \u201cWe are going to have to have a rethink.\u201d \n               Rival approach \n             At the same time that research and clinical practice are helping to undermine the  DSM  categories, the rival dimensional approach is gaining support. Over the past decade, psychiatrists have proposed a number of such dimensions, but they are not used in practice \u2014 partly because they are not sanctioned by the  DSM . The frequent co-morbidity between schizophrenia and OCD, for instance, has led some to suggest a schizo-obsessive spectrum, with patients placed according to whether they attribute intrusive thoughts to an external or internal source. And in 2010, Craddock and his colleague Michael Owen proposed the most radical dimensional spectrum so far 3 , in which five classes of mental disorder are arranged on a single axis: mental retardation\u2013autism\u2013schizophrenia\u2013schizoaffective disorder\u2013bipolar disorder/unipolar mood disorder (see 'Added dimensions'). Psychiatrists would place people on the scale by assessing the severity of a series of traits that are affected in these conditions, such as cognitive impairment or mood disruption. It is a massively simplified approach, Craddock says, but it does seem to chime with the symptoms that patients report. More people show the signs of both mental retardation and autism, for example, than of both mental retardation and depression. When Kupfer and his  DSM-5  task force began work in 2007, they were bullish that they would be able to make the switch to dimensional psychiatry. \u201cI thought that if we did not use younger, more-basic science to push as hard as we could, then we would find it very difficult to move beyond the present state,\u201d Kupfer recalls. The task force organized a series of conferences to discuss how the approach could be introduced. One radical and particularly controversial proposal was to scrap half of the existing ten conditions relating to personality disorder and introduce a series of cross-cutting dimensions to measure patients against, such as degree of compulsivity. But this and other proposals met with stinging criticism. The scales proposed were not based on strong evidence, critics said, and psychiatrists had no experience of how to use them to diagnose patients. What is more, the personality-disorder dimensions flopped when they were tested on patients in field trials of the draft  DSM  criteria between 2010 and 2012: too many psychiatrists who tried them reached different conclusions. \u201cIntroducing a botched dimensional system prematurely into  DSM-5  may have the negative effect of poisoning the well for their future acceptance by clinicians,\u201d wrote Allen Frances, emeritus professor of psychiatry at Duke University in Durham, North Carolina, in an article in the  British Journal of Psychiatry 4 . Frances had served as head of the  DSM-IV  task force and was one of the strongest critics of proposals to introduce dimensionality to  DSM-5 . The proposal was also unpopular with patient groups and charities, many of which have fought long and hard to make various distinct mental-health disorders into visible brands. They did not want to see schizophrenia or bipolar disorder labelled as something different. Speaking privately, some psychologists also mutter about the influence of drug companies and their relationship with psychiatrists. Both stand to profit from the existing  DSM  categories because health-insurance schemes in the United States pay for treatments based on them. They have little incentive to see categories dissolve. \n               Change of tack \n             In the middle of 2011, the  DSM-5  task force admitted defeat. In an article in the  American Journal of Psychiatry 5 , Kupfer and Darrel Regier, vice-chair of the  DSM-5  task force and the APA's research director, conceded that they had been too optimistic. \u201cWe anticipated that these emerging diagnostic and treatment advances would impact the diagnosis and classification of mental disorders faster than what has actually occurred.\u201d The controversial personality-disorder dimensions were voted down by the APA's board of trustees at the final planning meeting in December 2012. The APA claims that the final version of  DSM-5  is a significant advance on the previous edition and that it uses a combination of category and dimensional diagnoses. The previously separate categories of substance abuse and substance dependence are merged into the new diagnosis of substance-use disorder. Asperger's syndrome is bundled together with a handful of related conditions into the new category called autism-spectrum disorder; and OCD, compulsive hair-pulling and other similar disorders are grouped together in an obsessive\u2013compulsive and related disorders category. These last two changes, Regier says, should help research scientists who want to look at links between conditions. \u201cThat probably won't make much difference to treatment but it should facilitate research into common vulnerabilities,\u201d he says. The Research Domain Criteria project is the biggest of these research efforts. Last year, the NIMH approved seven studies, worth a combined US$5 million, for inclusion in the project \u2014 and, Cuthbert says, the initiative \u201cwill represent an increasing proportion of the NIMH's translational-research portfolio in years to come\u201d. The goal is to find new dimensional variables and assess their clinical value, information that could feed into a future  DSM . One of the NIMH-funded projects, led by Jerzy Bodurka at the Laureate Institute for Brain Research in Tulsa, Oklahoma, is examining anhedonia, the inability to take pleasure from activities such as exercise, sex or socializing. It is found in many mental illnesses, including depression and schizophrenia. Bodurka's group is studying the idea that dysfunctional brain circuits trigger the release of inflammatory cytokines and that these drive anhedonia by suppressing motivation and pleasure. The scientists plan to probe these links using analyses of gene expression and brain scans. In theory, if this or other mechanisms of anhedonia could be identified, patients could be tested for them and treated, whether they have a  DSM  diagnosis or not. One of the big challenges, Cuthbert says, is to get the drug regulators on board with the idea that the  DSM  categories are not the only way to prove the efficacy of a medicine. Early talks about the principle have been positive, he says. And there are precedents: \u201cPain is not a disorder and yet the FDA gives licences for anti-pain drugs,\u201d Cuthbert says. Going back to the drawing board makes sense for the scientists, but where does it leave  DSM-5 ? On the question of dimensionality, most outsiders see it as largely the same as  DSM-IV . Kupfer and Regier say that much of the work on dimensionality that did not make the final cut is included in the section of the manual intended to provoke further discussion and research.  DSM-5  is intended to be a \u201cliving document\u201d that can be updated online much more frequently than in the past, Kupfer adds. That's the reason for the suffix switch from V to 5; what comes out next month is really  DSM-5.0 . Once the evidence base strengthens, he says, perhaps as a direct result of the NIMH project, dimensional approaches can be included in a  DSM-5.1  or  DSM-5.2 . All involved agree on one thing. Their role model now is not Freud or Kraepelin, but the genetic revolution taking place in oncology. Here, researchers and physicians are starting to classify and treat cancers on the basis of a tumour's detailed genetic profile rather than the part of the body in which it grows. Those in the psychiatric field say that genetics and brain imaging could do the same for diagnoses in mental health. It will take time, however, and an entire generation will probably have to receive flawed diagnoses before the science is developed enough to consign the category approach to clinical history. \u201cI hope I'll be able to give a patient with possible bipolar a proper clinical assessment,\u201d Craddock says. \u201cI'll do a blood test and look for genetic risks and send them into a brain scanner and ask them to think of something mildly unhappy to exercise their emotional system.\u201d The results could be used to trace the underlying cause \u2014 such as a problematic chemical signal in the brain. \u201cI'll then be able to provide lifestyle advice and treatment.\u201d He pauses. \u201cActually it won't be me, because I will have retired by then.\u201d \n                 See Editorial \n                 p. 397 \n               \n                     Across the divide 2013-Apr-24 \n                   \n                     Diagnostics tome comes under fire 2012-Jan-31 \n                   \n                     In retrospect: The five lives of the psychiatry manual 2010-Nov-10 \n                   \n                     Schizophrenia special \n                   \n                     American Psychiatric Association DSM homepage \n                   Reprints and Permissions"},
{"file_id": "496286a", "url": "https://www.nature.com/articles/496286a", "year": 2013, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "Decades after Thomas Lovejoy isolated fragments of the Brazilian rainforest in a grand experiment, researchers are building on his legacy around the world. Ecologist Thomas Lovejoy tucks his trousers into his socks with a casual warning about chiggers and then hikes off into the Amazon jungle. Shaded by a tall canopy and dense with ferns and underbrush, the old-growth forest looks healthy, but Lovejoy knows better. Three decades ago, the surrounding forest was mowed down and torched as part of a research project, and the effects have spread like a cancer deep into the uncut area. Large trees have perished. The spider monkeys have moved out, as have the army-ant colonies, and many of the birds that depend on them. Lovejoy and his team have been studying this 10-hectare fragment of forest since the late 1970s as part of the largest and longest-running experiment in tropical ecology. In collaboration with ranchers, they cleared the trees around this and ten other plots of varying size to create islands of intact forest. The researchers have been monitoring the plots ever since, documenting how deforestation harms the adjacent untouched forest as specialist plants and animals gradually give way to generalists and pioneer species that prefer disturbed habitat. \u201cWe are chronicling the simplification of these forests,\u201d says Lovejoy, a professor at George Mason University in Fairfax, Virginia. Covering roughly 1,000 square kilometres in an area north of Manaus in the central Amazon, the experiment was set up to test fundamental theories about the viability of small, disconnected ecosystems. By documenting pervasive changes in the forest fragments, Lovejoy and his co-workers provided the first hard data that conservationists needed to promote the preservation of extensive areas of intact forest. \u201cIt's the most important ecological experiment ever done,\u201d says Stuart Pimm, a conservation ecologist at Duke University in Durham, North Carolina, who has collaborated on the project. \u201cWe knew that small and isolated was bad, but we needed to know how bad.\u201d The researchers are now exploring the long-term effects of habitat fragmentation, but the ecological record there is ironically threatened by forest that is taking over abandoned pastures. Although Lovejoy has struggled to maintain financing for long-term monitoring, the US National Science Foundation is breathing new life into the project by funding the team to isolate some of the plots anew. The experiment has also helped to train and inspire a generation of 'fragmentologists', who are working around the world to understand the cascade of ecological impacts that follow human development. Most notably, in early April, an international team started chopping down trees in Borneo as part of an nearly \u00a36-million (US$9-million) experiment that replicates and extends the Brazilian one. \u201cThe Amazon experiment changed the game,\u201d says Rob Ewers, principal investigator on the Borneo project at Imperial College London. \u201cI like to think of our project as the next step.\u201d \n               The ambassador \n             \u201cWelcome to Camp 41,\u201d says Lovejoy, beaming at a group of guests he invited to tour the experiment \u2014 and do a little bird-watching \u2014 over New Year's Eve, an annual tradition. Fit at 71, he has a slight paunch, a crop of thinning hair and pale skin that is a touch reddish from the heat and the hike to his forest base. Lovejoy offers a quick tour of the open-air shelters that house hammocks and dining facilities as well as the bathrooms, showers and a makeshift pool down a trail by the stream. Over the years, he has entertained a long list of high-profile guests here, ranging from Al Gore (when he was a senator) to actor Tom Cruise and high-ranking Brazilian officials. Lovejoy has always served as a scientific ambassador and chief fund-raiser, and left the fieldwork to others. After cleaning up from a day tramping around the forest, he sits beneath a cashew tree and begins plying his guests with caipirinhas, the national cocktail of Brazil. Peering over wire-framed glasses, he guides conversations about the strange beauty of tropical creatures, environmental policy and the history of science and development in the Amazon. Darkness falls, and an orchestra of frogs claims the forest. Lovejoy arrived in the Amazon to study birds as a graduate student from Yale University in New Haven, Connecticut, in 1965, just as concerns about development in the region were rising among scientists and politicians. That same year, Brazil enacted its modern Forest Code, which at the time required ranchers and farmers in the Amazon to maintain a 'legal reserve' on 50% of their land (the legal reserve is now 80%). Two years later, entomologist Edward O. Wilson and biologist Robert MacArthur published their influential  The Theory of Island Biogeography , which laid the foundations for the modern understanding of species diversity and rates of extinction in isolated habitat, whether surrounded by water or by agricultural fields. Soon after Lovejoy earned his PhD in 1971, ecologists became embroiled in what became known as the SLOSS debate, which stood for 'single large or several small'. The question was whether it would be better to protect massive continuous landscapes or many smaller biodiversity hotspots. Lovejoy thought about the Brazilian law and realized that the legal reserve could provide a way to probe these questions. \u201cThe ranchers were going to clear the land anyway,\u201d he says. \u201cMy crazy idea was that maybe you could arrange the 50% and create a giant experiment.\u201d The project kicked off in 1978, with $500 a month from conservation group the WWF and the support of the Brazilian National Amazonian Research Institute (INPA) in Manaus. Lovejoy hired another former student from Yale, ecologist Rob Bierregaard, to run the project. A year later, he and a team of Brazilian scientists began surveying the forests areas they were planning to isolate, which came in sizes of 1, 10 and 100 hectares (see 'Fractured forest'). The first wave of machetes and chainsaws came through in June 1980, and in September, Bierregaard's team walked the perimeter of the plots, dripping burning rubber onto forest debris. When the conflagration died down, the first two square patches of old-growth rainforest were left standing amid swathes of smoking embers that remained hot enough to cook the crew's beans for days. The early phase of the experiment was hardly smooth. The Brazilians complained that it was too much of a US initiative, and the ranchers were slow to clear the rest of their land. One year passed, then two. \u201cIt was really frustrating,\u201d says Bierregaard, who is now at the University of North Carolina at Charlotte. \u201cWe were publishing totally unreplicated results from the 1- and 10-hectare reserves.\u201d By 1983, rather than waiting for the ranchers, Lovejoy secured more funding from the WWF to create another pair of fragments. The results began rolling in immediately, with the edges of the plots showing a substantial loss of key species. Yet, as the experiment grew, Lovejoy's team soon had more data than it could deal with, and in 1996 he brought on Bill Laurance to help make sense of the plant data. In 1997, Laurence and the team reported that up to 36% of the biomass in the first 100 metres of the forest fragments had disappeared in 10\u201317 years of isolation 1 . \u201cIt really taught people how edge effects are driving rapid changes in ecology,\u201d says Laurance, who is now stationed at James Cook University in Cairns, Australia. The main drivers are sunlight and air circulation. As the pastures and forest edges heat up each day, the air over those regions rises, drawing cool moist air out of the forest. The hot dry air takes a toll on large hardwood trees such as mahogany and ebony. The open fields also expose the forest to wind, which blows down trees and further opens up the canopy. Over time, these gaps are filled with fast-growing trees and vines. These pioneer species eventually seal off the forest like a scab, helping to delay further impacts, but neither the carbon density nor the diversity of the forest recovers quickly. Today, the researchers continue to track these effects as they work their way through the forest. Those early results suggested that scientists were underestimating the broader impact of fragmentation, and in 1998, Laurance extrapolated the findings across the tropics. His team's calculations suggested that the biomass loss around forest edges could produce up to 150 million tonnes of carbon emissions annually 2  \u2014 exceeding emissions from the United Kingdom. In 2003, Lovejoy and his fellow fragmentologists took their first stab at one of the questions that had originally inspired the project: how big is big enough? Documenting a 50% decline in the number of bird species living beneath the canopy in the 100-hectare plots over the first 15 years of isolation, they derived a kind of half-life for extinction: the time it takes to lose half of the species increases roughly tenfold with a 1,000-fold rise in the area of a reserve 3 . The calculations were based only on birds and may not capture the ecological dynamics of larger reserves. But for Lovejoy, they suggest that a reserve would need to be on the order of 100,000 hectares in area to maintain relatively stable levels of biodiversity. Given that it is impractical in most places to set aside such large areas, this suggests that most fragmented landscapes around the globe may be doomed to a continuing decline in biodiversity. Yet, the fragment experiment also points to a possible solution: the secondary forests that have sprung up in abandoned pastures. The regrown forests created wildlife corridors, which allowed army-ant colonies, birds and howler monkeys and other small mammals to migrate from intact forest into the experimental plots. For Pimm, those findings served as a call to action. In 2005, he created a non-profit conservation organization called SavingSpecies, with a goal of identifying biodiversity hotspots such as Brazil's Atlantic rainforest that would benefit from habitat corridors \u2014 a kind of lifeline to the remaining forest. \u201cThis is where the science comes together with the strategic and tactical conservation efforts,\u201d Pimm says. In 2007, his group and its partners purchased a ranch that separated a small population of endangered golden lion tamarins from a larger forested area. After the team moved out the cattle and replanted some areas, secondary forest started to grow. Last year, the researchers saw the first evidence of golden lion tamarins and cougars moving through former ranchland. \u201cOur bet on studying forest fragments paid off,\u201d he says. Looking forward, Lovejoy says that many crucial questions remain about the processes playing out in the fragmented ecosystems. How many of the species in the plots are doomed to extinction? How quickly and deeply will these impacts move through the fragmented forests? Will rapid shifts in insect and other animal populations drive long-term changes in seed dispersal and thus plant diversity? And what role will global warming have? Continued monitoring of the fragments could produce some answers, but the secondary growth around the plots has undermined the viability of the experiment. The US National Science Foundation provided money for a new bout of clearing later this year, which is part of a larger $450,000, five-year grant to continue bird research there. \u201cThis grant will extend our record to nearly 40 years,\u201d says Phil Stouffer, an ornithologist at Louisiana State University in Baton Rouge and principal investigator on the grant. For Lovejoy, it is a new lease on life for the experiment. As the project continues, long-term data from the fragments and from the larger control plot in the adjacent primary forest could be invaluable for studying the impacts of global warming, says Adalberto Val, who heads the INPA, which provides part of the half-million-dollar or so annual operating costs of the experiment. The rest comes from the Smithsonian Tropical Research Institute in Panama and various foundations. He calls the forest-fragments project a \u201cscientific treasure\u201d. \n               Global spread \n             In 1984, a team led by Robert Holt, now at the University of Florida in Gainesville, implemented one of the first follow-on studies, focusing on experimentally designed patches of secondary regrowth amid the agricultural fields of Kansas (see 'A family of fragments'). By 1990, another four projects were under way, and a survey published in 2003 identified a total of 21 fragmentation experiments of various scales and durations 4 . Andrew Gonzalez, an ecologist at McGill University in Montreal, Canada, has replicated Lovejoy's experiment at the bench- and chamber-scale with isolated patches of moss; he has used the results to inform his own work on wildlife corridors in Montreal 5 . The experiments have produced a range of results, but most of them confirmed significant impacts along the edges of the fragments that broadened over time. And the subsequent studies have reinforced the findings that habitat corridors can help to sustain the isolated fragments. Lovejoy's latest scientific progeny \u2014 third-generation fragmentologist Ewers \u2014 initially worked in the Amazon as a 25-year-old postdoc under Laurance. Ewers first visited Camp 41 in 2004 while conducting unrelated research on deforestation. He is now setting up a similar experiment in the state of Sabah in Borneo. Whereas Lovejoy's experiment focused on primary old-growth forest, Ewers wanted to observe the evolution of landscapes that have already been degraded and transformed by humans \u2014 something more representative of modern environmental realities. He set up fragments in logged forests and palm plantations as well as plots within forest reserves to investigate the ecological and conservation value of different landscapes. \u201cThe general idea of Lovejoy's project was just breathtaking 30 years ago,\u201d says Ewers. \u201cBut we have new questions today, and I think we are in a much better place to make sense of these data in the long run.\u201d His initiative, known as the Stability of Altered Forest Ecosystems (SAFE) project, is receiving some \u00a36 million pounds in core funding over ten years from the Sime Darby Foundation in Kuala Lumpur, the philanthropic arm of one of the world's largest palm-oil producers. Forty-two plots mirroring the sizes used in the Amazon project will be located at various distances from the surrounding forests. Dozens of scientists have been surveying the sites, and the project now has data on 3,000 to 4,000 tropical species, from ants and beetles to birds, bees and trees \u2014 a database much larger than the one Lovejoy and Bierregaard were able to compile at the outset of their experiment. After a year's delay, loggers fired up the chainsaws on 4 April. The unpublished baseline data show a landscape severely affected by development. For instance, the average amount of carbon locked up in trees in the primary forests has been estimated at 243 tonnes per hectare, compared with 49 tonnes per hectare in logged forest and just 4 tonnes per hectare within palm plantations. And yet, while many logged forests have been hard-hit ecologically, they still show pockets of remarkable biodiversity, including all five of the native cat species. Ed Turner, an ecologist at the University of Cambridge, UK, who helped to set up the experiment, says that the project is designed to inform discussions about how to conserve habitat in a region dominated by palm-oil production. \u201cThe lessons from SAFE have the potential to have a very far-reaching effect,\u201d he says. Another component of the Borneo project will focus on how nitrogen and other nutrients move through soils and plants, with the goal of understanding the biogeochemical processes at work throughout the forests as they are affected by fragmentation. The UK National Environmental Research Council is expected to announce in the coming months nearly \u00a35.7 million for a series of projects focused on the relationship between biodiversity and biogeochemical cycles. For example, initial surveys have suggested that leaf litter decomposes roughly twice as fast in logged and primary forests as it does in palm-oil plantations. Once scientists have isolated fragments in each of these systems, they will be able to track changes in the decomposition rates as edge effects take hold, then relate those changes to soil nutrient cycles and impacts on plant communities. \u201cIt's about doing fundamental science to help us understand what happens to a forest when you change it in these ways,\u201d says Ken Norris, an environmental biologist at the University of Reading, UK, and biodiversity theme leader for the UK National Environmental Research Council. \u201cAnd if you are going to do ecosystems science, you need experiments outdoors at the scale at which these systems work.\u201d Surveying the ongoing work, Lovejoy is pleased to see that his crazy idea still has legs. The project in Borneo will advance the field, perhaps leading to new collaborations and comparative studies with researchers at his own experiment in the Amazon. \u201cI started out thinking this would be a 20-year experiment, and then I would get the answer and we would be done,\u201d he says, gazing into the rain one afternoon at Camp 41. \u201cIt turned out to be more complex. So many new questions arose.\u201d In the end, he says that much of the project's impact \u2014 and his own \u2014 has come from broader educational and advocacy efforts that have advanced conservation initiatives in Brazil and around the world. He notes that Brazil has now protected nearly half of its share of the Amazon, more than scientists like himself could have hoped for several decades ago. He also points to the hundreds of Brazilian scientists who have moved through the project, including people such as Rita Mesquita, a researcher at the INPA who first came to the project as a student in 1985 and rose up to become scientific coordinator and an important state environmental official. Lovejoy says that the scientific value of the Amazon fragment experiment only increases with time. Basic questions about the rate of species extinctions in fragmented habitat still plague the field of ecology, and the fragments provide a unique way to explore the issue. Ultimately, he envisages raising enough money to buy the ranchland surrounding the project and then convert it into an educational facility for science and ecotourism, and he has a new advisory board that is looking into options. \u201cIf we can get this place stabilized and institutionalized, we could do a lot of things,\u201d he says. One way or another, he seems confident that the research will continue. \u201cEven if my plane goes down on the way to Miami, I think it will happen.\u201d Lovejoy falls silent, and the murmur of rainfall envelopes Camp 41. After a pause, he heads for the hammocks to join his guests for an afternoon nap. Soon enough, the rains will stop and Lovejoy will once again don his safari hat and binoculars, and set off to explore the fragmented forest.\n \n                     Will we kill off today's animals if we revive extinct ones? 2013-Mar-20 \n                   \n                     Climate adaptation: Survival of the flexible 2013-Feb-05 \n                   \n                     President prunes forest reforms 2012-Jun-04 \n                   \n                     Nature's Rio+20 special \n                   \n                     Nature's Changing Amazon special \n                   \n                     Biological Dynamics of Forest Fragments Project \n                   \n                     Stability of Altered Ecosystems Project \n                   Reprints and Permissions"},
{"file_id": "496290a", "url": "https://www.nature.com/articles/496290a", "year": 2013, "authors": [{"name": "Ewen Callaway"}], "parsed_as_year": "2006_or_before", "body": "Finding and vaccinating Nigerian nomads may be one of the last obstacles to the eradication of polio. Mohammed Abubakar's home is not on any map \u2014 at least not yet. To reach his settlement in a desolate part of northern Nigeria, four health workers creep over deep-rutted roads in an old Peugeot for an hour, then ride motorcycles over narrow dirt trails for another 30 minutes \u2014 stopping only for the odd herd of cattle. Finally, they spot a cluster of mud-brick huts, known to the Fulani nomads who live there as a ruga. \u201c As-salamu alaykum ,\u201d \u2014 peace be with you \u2014 says Ardo Babangida, a traditional leader accompanying the team. Children swarm around the visitors, and Daniel Santong, an easy-going veterinarian and leader of the group, asks to meet Abubakar, the head of the household. Meanwhile, a young colleague whips out a smart phone and uploads the settlement's Global Positioning System coordinates into a database. Abubakar arrives, clad in a lavender tunic and white skull cap, and Santong tells him that they are trying to eliminate polio in nomadic people. Abubakar clasps his guest's hands in appreciation. He says that he cannot remember the last time that health workers came to vaccinate his children. It is a story that Santong and his colleagues are now accustomed to hearing, even though door-to-door immunization campaigns happen on a near-monthly basis in the region. These dusty paths are the front lines of polio eradication. A 25-year, US$10-billion global effort has taken the number of polio cases from hundreds of thousands per year to just hundreds, but it is now struggling to stamp the virus out of its final strongholds in Pakistan, Afghanistan and Nigeria, where transmission has never been interrupted. Of these, Nigeria was the only one to see an increase in cases from 2011 to 2012, and public-health experts worry that the virus's recalcitrance here will prevent global eradication, and eventually lead to a wider resurgence of the disease. Ewen Callaway talks about the health workers who try to track down and give polio vaccinations to nomadic populations in Nigeria. The barriers to polio eradication in Nigeria are complex and numerous. The country does not have a working public health-care system, and some local government officials are less than committed to the cause. In urban centres in the north, widespread distrust of the government leads many parents to refuse vaccination for their children. What is more, in February several polio workers were murdered \u2014 for unknown reasons \u2014 at health clinics in Kano, northern Nigeria's largest city. But epidemiologists have identified one barrier that might be overcome cheaply and safely: locating and counting remote populations, including the nomadic livestock herders who drift through the region with the changing seasons. Records of their numbers and movements are incomplete, but the population is thought to include hundreds of thousands of young children, many of whom have received none or only some of the multiple oral polio vaccine doses required to achieve full protection. Proponents of the programme say that nomads are a polio reservoir, spreading disease around the country during their migrations. So in June 2012, the National Stop Transmission of Polio (N-STOP) programme, organized through the Global Polio Eradication Initiative (GPEI) and supported by the Nigerian government, started a census of Fulani nomads and other hard-to-reach populations, as part of a global emergency action plan against polio. \u201cUntil we solve the problem of these unvaccinated nomads, we're not going to fix polio,\u201d says Frank Mahoney, a veteran field epidemiologist leading the project from Abuja. \u201cWe're not going to be able to eradicate it.\u201d \n               Roadblocks ahead \n             On a scorching, cloudless December day, a procession of several hundred livestock has taken over a road running between the states of Bauchi and Kaduna in northern Nigeria. Men walk alongside their cows and sheep, while women and children creep along on motorcycles. It is the height of the dry season, the landscape is parched, and the group is heading south to graze its livestock. Lere, a local government area (LGA) in Kaduna is travelled by a sizeable population of Fulani pastoralists like these during their biannual migrations. It is also one of dozens of 'high-risk LGAs' on which N-STOP teams are focusing their efforts. No polio cases have yet been detected here, but Lere is not far from the borders of Kano and Bauchi, which both recorded cases in the past year. Mobile and remote populations are often strongholds for disease. Somali nomads contained some of the final cases of smallpox, and the vaccination of herds in remote patches of east Africa was crucial to eradicating the cattle disease rinderpest, completed in 2011. In Nigeria, Fulani nomads receive little education or health care from the government. \u201cNobody looks after them, nobody takes primary health care to them, nobody remembers they exist,\u201d says Endie Waziri, a member of one of the N-STOP teams. When polio vaccination workers do visit their remote settlements, she says, they tend to visit only the first ruga they see and not look for others in the vicinity. Nigeria made significant headway against polio after starting an eradication programme in 1996. But those gains were erased in 2003, when Muslim clerics in the northern state of Kano called for a boycott of the polio vaccine over fears that the eradication campaign was a Western conspiracy to sterilize the population. Soon, Kano, Kaduna and other northern states had halted all polio vaccination campaigns. The boycott ended a year later, but by then polio had exploded across northern Nigeria and started to seep into nearby countries, such as Cameroon and C\u00f4te d'Ivoire, that had previously vanquished the virus. Nigeria has since made lurching progress against the disease. Cases fell from more than 1,000 in 2006 to 21 in 2010, before rising again to 122 last year. Many more cases probably went undetected. Religious opposition to vaccines among settled populations has now given way to refusals driven by disenchantment. \u201cPeople want things other than polio vaccination,\u201d says David Heymann, chairman of the advisory board for Public Health England and the former head of the polio-eradication efforts for the World Health Organization (WHO). \u201cThey can't understand why people are coming once a month to give them vaccination when what they want are treatments for their children with fever or diarrhoea.\u201d The nomads, however, rarely refuse polio vaccination for their children, and they are eager to receive other health and veterinary services. \u201cIf we hit these areas, we get a much bigger bang for the buck,\u201d says Chima Ohuabunwo, a Nigerian epidemiologist who took a sabbatical from his position at Morehouse School of Medicine in Atlanta, Georgia, to serve as the field coordinator of the nomads project. Still, the lack of a working public health-care system has been a problem, says Heymann, who points out that neighbouring countries with large nomadic populations such as Chad have successfully interrupted transmission. \u201cOther countries with migrant populations have done the job,\u201d he says. India, for instance, offered polio vaccinations at train stations to catch migrants, and Chad offers combined veterinary and polio vaccination services to encourage Fulani nomads \u2014 who depend on the health of their livestock \u2014 to take part. N-STOP's census programme was designed to support on-going vaccination efforts, uncovering areas of need and directing resources and local vaccination teams to them. In fact, so as not to interfere with local efforts, N-STOP teams didn't bring vaccine stocks with them until the federal government asked them to. Since August, the N-STOP surveys have uncovered more than 32,000 settlements and identified more than 700,000 children \u2014 nearly 40,000 of whom had never been vaccinated against polio. Although little more than 3% of the 122 polio cases reported last year in Nigeria occurred in nomad children, the teams discovered more than 100 probable cases that went unreported, supporting the idea that nomads form an important link in the chain of transmission. When not on long migrations, Fulani nomads interact with other people at markets. More than one-third of confirmed polio cases in 2012 were among children who lived in close proximity to nomadic communities. And nomad migration routes are hotspots of low vaccination coverage, according to data from the US Centers for Disease Control and Prevention (CDC) in Atlanta, Georgia. The surveys are \u201creally a strategy that's been missing in the tool box for a long time, and it needs to be urgently done\u201d, says Mahoney. But Mahoney also notes that the project is a work in progress. Emmanuel Musa, the WHO coordinator in Lere, is not sure that it will be feasible for his district's vaccination team to reach all the nomad communities that N-STOP teams are uncovering. \u201cThere are inadequate funds,\u201d he says. At a catch-up in December, Fureratu Zakari, the WHO coordinator for Kaduna state, also questioned whether the mapping data will be enough to allow health workers to locate nomadic communities. \u201cThey should be showing teams where settlements are and not just writing it down,\u201d she tells Olaniran Alabi, the programme's field coordinator for Kaduna. Zakari complains that the N-STOP programme isn't coordinating its day-to-day activities with local health authorities, which could result in confusion and duplicated efforts. The sheer scale of the GPEI initiative may explain some of the tensions. The public\u2013private effort, which includes the WHO, CDC, the United Nations Children Fund (UNICEF), Rotary International and the Bill & Melinda Gates Foundation, has become the world's costliest public-health initiative, and one of the longest running. Since it started in 1988, it has missed three deadlines for halting transmission \u2014 in 2000, 2005 and 2012 \u2014 and it now burns roughly $1 billion per year chasing the last remaining pockets of disease. \n               Cause for optimism \n             Organizers are optimistic about winning the war, however. India was long thought of as the Waterloo of the initiative because its high population density and poor sanitation provided ideal conditions for the virus to spread. But it celebrated its second year without a case of polio in January. And Pakistan and Afghanistan both saw significant reductions in cases between 2011 and 2012 (see 'Last holdouts'), despite deep security challenges that limited the reach of vaccination campaigns. Programme officials in Nigeria say that national and state governments are increasingly committed to eradication, but that complacency and corruption are still common among the officials in charge of local immunization campaigns. In October 2012, for instance, dozens of officials in Kano state were fired for treating the eradication programme as a \u201cmoney-making venture\u201d, in the words of the state's governor, Rabiu Musa Kwankwaso. Counting and vaccinating nomads will not solve all Nigeria's polio troubles, but it is easier to achieve than tackling domestic terrorism, vaccine refusal and other challenges. When asked how N-STOP will measure the success of the nomad programme, Mahoney puts it simply: \u201cStopping polio transmission. That's the big indicator.\u201d But he also points to the thousands of settlements and children that have been mapped and counted so far. As  Nature  went to press, Nigeria had recorded 11 cases of polio this year; it had 17 in the same period last year. President Goodluck Jonathan has vowed to bring that number to zero before his term ends in 2015. Michael Galway, senior programme officer at the Bill & Melinda Gates Foundation in Seattle, Washington, is optimistic that the push to reach remote populations is paying off. \u201cThe work of the nomads project has been extremely beneficial in opening the eyes of the [polio eradication] programme to this additional piece of the puzzle,\u201d he says. The Nigerian project could hold lessons for future efforts to eradicate disease (measles is on some agendas), which are likely to hinge on reaching mobile and remote populations. Targeting vaccination efforts on nomadic children is the right strategy, says Paul Rutter, a spokesman for the independent monitoring board set up to evaluate global efforts at polio eradication in 2010. But it will be the people behind it who ultimately dictate its success. \u201cThis kind of dogged determination to reach every last child will be what rids Nigeria of polio.\u201d \n                     Vaccine switch urged for polio endgame 2013-Jan-14 \n                   \n                     Polio campaign at turning point, after Pakistan killings 2012-Dec-21 \n                   \n                     Polio\u2019s last stand 2012-May-28 \n                   \n                     Public health: Polio clings on in Pakistan 2011-May-25 \n                   \n                     Nature Special: Vaccines \n                   \n                     Podcast \n                   \n                     Global Polio Eradication Initiative \n                   \n                     Polio Eradication Emergency Action Plan \n                   \n                     US CDC Stop Transmission of Polio Program \n                   \n                     Video report on story \n                   Reprints and Permissions"},
{"file_id": "496412a", "url": "https://www.nature.com/articles/496412a", "year": 2013, "authors": [{"name": "Meredith Wadman"}], "parsed_as_year": "2006_or_before", "body": "There are almost as many firearms in the United States as there are citizens. Garen Wintemute is one of few people studying the consequences. With his crisp blue suit and wire-framed spectacles, Garen Wintemute hardly looked frightening as he stepped to the podium last month to address a conference on paediatric emergency medicine in San Francisco, California. But his presence there made the organizers nervous. Wintemute, an emergency-department doctor, is better known as the director of the Violence Prevention Research Program at the University of California (UC), Davis. As such, he has published dozens of papers on the effects of guns in the United States, where widespread gun ownership and loose laws make it easy for criminals and potentially violent people to obtain firearms. Wintemute has pushed the bounds of research, going undercover into gun shows with a hidden camera to document how people often sidestep the law when purchasing weapons. He has also worked with California lawmakers on crafting gun policy and helped to drive a group of gun-making companies out of business. All this made Wintemute a potentially risky speaker for the conference funder, a branch of the US Department of Health and Human Services, which is barred by law from funding any activities that advocate or promote gun control. The meeting organizers had told Wintemute to stick to facts and avoid any mention of policies. But with the nation still reeling from the murder of 20 children and 6 educators, who were shot in their school in Newtown, Connecticut, in December, the conference organizers were not sure what Wintemute would say. He stuck to the facts, but also managed to make clear how he feels about the funding prohibition, which has effectively killed off most research on gun violence. \u201cWe don't have a labour force,\u201d Wintemute told the assembled doctors. That has led to a striking imbalance in US medical research. Firearms accounted for more than 31,000 deaths in the United States in 2011 (see 'Gun deaths'). But fewer than 20 academics in the country study gun violence, and most of them are economists, criminologists or sociologists. Wintemute is one of just a few public-health experts devoted to this research, which he has funded through a mixture of grants and nearly US$1 million of personal money. His undercover gun-show tactics have led him into situations where he feared for his safety, and they have also raised protests from some gun-rights advocates, who charge that Wintemute is more a biased campaigner than a researcher. But even a few of his ideological opponents praise Wintemute's work. \u201cGaren is one of the very best in terms of his research skills,\u201d says David Kopel, the research director at the Independence Institute in Denver, Colorado, a think tank that supports gun-owners' rights. And Wintemute, who is 61, makes no apologies for his passion or his methods. \u201cI believe just as strongly as I can articulate in the value of free inquiry,\u201d he says, \u201cespecially when the stakes are so high \u2014 when so many people are dying through no fault of their own; when so much of the country simply turns its back on this problem.\u201d \n               Aiming true \n             Wintemute grew up in a home in Long Beach, California, where his father, a decorated veteran of the Second World War, kept a Japanese officer's sabre and infantry rifle, a Winchester carbine and a Marlin .22 calibre rifle in a bedroom cupboard. Wintemute learned to shoot, and begged to go hunting. That chance came when he was around 12, and his father asked him to help clear out sparrows from the rafters of his company's warehouse. Wintemute's aim was good, he recalls. \u201cBut I held those birds and looked at the finality of it all and felt them turn cold in my hands and decided this was not for me.\u201d As an undergraduate at Yale University in New Haven, Connecticut, Wintemute flirted with oceanography and neuroscience, but eventually decided that he wanted to be a physician. After completing medical school and a residency in family practice, both at UC Davis, Wintemute went to work in 1981 as medical coordinator at the Nong Samet Refugee Camp, just inside Cambodia's border with Thailand. The camp was in an area that had only recently been liberated from the Khmer Rouge dictator Pol Pot, and Wintemute took care of gunshot wounds on a daily basis. Even more common were shrapnel injuries from land mines. There was no electricity, and amputations were done under local anaesthetic. \u201cI never once met an intact family,\u201d Wintemute recalls. \u201cEverybody had lost somebody. There came a point where I said: 'I need to pick up a rifle. I can't be on the sidelines'.\u201d But instead of grabbing a gun, Wintemute decided to pursue 'big-picture' international health. He left Cambodia and enrolled in a one-year master's programme in public health at Johns Hopkins University in Baltimore, Maryland. One of his first courses was taught by a former trial lawyer named Stephen Teret, who is now director of the Center for Law and the Public's Health at Johns Hopkins. Teret remembers the day in September 1982 when the students of that class introduced themselves and Wintemute stunned him with his charisma and eloquence. \u201cI said to myself: 'I'm going to get to know this guy',\u201d recalls Teret, and the two of them soon became friends and collaborators. On a cold winter day several months later, some close friends of Teret's dropped their 21-month-old son off at the house of his caregiver. Around noon, the caregiver laid him down for a nap and left the room, whereupon her four-year-old son took his father's loaded handgun from a nearby drawer, pointed it at the sleeping infant and shot him through the head. Within weeks, Teret switched his main research focus from motor-vehicle injuries to gun injuries, an area in which public-health research was all but non-existent. Wintemute began assisting him, and their first project was a law-review article laying out a legal strategy for suing gun-makers who fail to use available safety technologies to prevent accidental gun deaths 1 . Wintemute returned to UC Davis, with the goal of focusing on gun injuries. In Cambodia and then in the Sacramento emergency department, Wintemute learned the hard lesson that, as a doctor, he had little chance of saving many people with gunshot wounds; most of those who died did so before they even reached the hospital. He realized that if he wanted to reduce deaths from firearms, he needed to prevent shootings in the first place. One day, he set himself a question as he left for a run in the foothills east of Sacramento. Looking to make an impact, he wondered: \u201cWhat subset of firearm injuries can people simply not turn away from?\u201d By the time he got back, he had decided to focus on the kind of shooting that had shattered the lives of Teret's friends. Garen Wintemute talks about his research into gun violence in the United States. In June 1987, Wintemute published a paper called 'When children shoot children: 88 unintended deaths in California' 2 . He reported that in 36% of these cases, the shooters didn't think that the gun was loaded or was real, or they were too young to tell the difference. Forty per cent of the childrens' fatal injuries were self-inflicted, including separate incidents in which a 5-year-old boy and a 2-year-old boy, using .38-calibre revolvers \u2014 one found under a pillow, the other in his parents' bedroom \u2014 each shot himself in the head. To illustrate one facet of the problem, Wintemute borrowed several of the guns used in the shootings from the Sacramento medical examiner. He then bought toy lookalikes, mounted the paired guns on a piece of plywood and, when the paper was published, called a press conference. Few of the reporters who attended could tell the toy guns from the real ones. His work and other events that year focused scrutiny on toy guns, and in December, toy retailers began to pull realistic-looking toy guns from their shelves. The next year, California banned their sale and manufacture. Wintemute was increasingly convinced that gun manufacturing was a pressure point that could be turned to advantage, by tying the industry to the public-health consequences of its products. He was contemplating how to do that when the  Wall Street Journal  published an article about a group of companies in and around Los Angeles, California, owned by one extended family that made small-calibre, inexpensive handguns known as Saturday Night Specials. Poorly made and lacking some safety features, the guns were disproportionately used in crime, particularly by juveniles. The article contained a trove of details about the family that ran the companies, and Wintemute decided to follow that trail. The result was  Ring of Fire , a book published in 1994 that described the enterprise and impact of the six companies, which in 1992 produced 34% of the handguns made in the country. Ring of Fire  painted such a stark portrait of the problematic guns that \u201cit became the focus of the rallying cry for local legislative action\u201d, says Sayre Weaver, a lawyer who represented West Hollywood, the first of several Los Angeles communities to ban the sale of the Saturday Night Specials. In 1999, the California legislature followed by making it illegal to manufacture and sell the handguns. Within several years, 5 of the 6 companies were out of business. \n               Battle to survive \n             Although his book had a big impact, Wintemute's research soon hit a snag. With grant support from the US Centers for Disease Control and Prevention (CDC) in Atlanta, Georgia, Wintemute had been conducting a retrospective cohort study looking at whether handgun buyers with prior misdemeanour convictions are more likely than those without a criminal history to be charged with new crimes, particularly those involving firearms and violence. (Many states allow purchases by criminals who have been convicted of misdemeanours, such as assault.) But as he was digging into the study, his source of funding came under attack from the National Rifle Association (NRA), a powerful lobbying group based in Fairfax, Virginia, that supports gun ownership. NRA leaders were upset with the CDC for funding work by another researcher who had found that people with a gun in their home were 2.7 times more likely than those without to be murdered 3 , and 4.8 times more likely to commit suicide 4 . In 1996, the NRA persuaded congressman Jay Dickey (Republican, Arkansas) to insert language into a budget bill to prohibit the CDC from advocating or promoting gun control. (That ban has been renewed every year since then.) Dickey's amendment also stripped $2.6 million from the agency's 1997 funding \u2014 the exact amount that the CDC had spent on firearm research the previous year. In 1996, Wintemute had received $292,000 from the CDC for the misdemeanour study, but after the change, the agency provided just $50,000 to close down the programme. The research restrictions were extended in 2012 to encompass all of the CDC's parent agency, the Department of Health and Human Services. And they have had a measurable effect. According to an analysis of Elsevier's Scopus database by the group Mayors Against Illegal Guns, the proportion of all publications dealing with US firearms and their impacts declined by 60% between 1996 and 2010. US researchers still produce more papers per capita on the topic than do investigators from other countries. But the subject may not be as high on other countries' research agendas because gun ownership is so much lower in most developed nations (see 'Top gun'). The United Kingdom, for example, banned private possession of handguns in 1998 after a gunman shot and killed 16 children and their teacher in a school in Dunblane, Scotland 1 . Wintemute was rare in staying devoted to gun research after the restrictions were imposed. He turned to the California Wellness Foundation, a large private charity based in Woodland Hills that focuses on health care and health education, and the foundation provided the funds to complete his study. Wintemute followed up nearly 6,000 authorized handgun purchasers, most of them for 15 years. He found that men who had had two or more convictions for misdemeanour violence were 15 times as likely as those with no criminal history to be charged with the most violent crimes 5 . Today, Wintemute runs the four-person Violence Prevention Research Program at UC Davis, on about $300,000 a year, none of which comes from the federal government. Of this, $50,000 is from the California Wellness Foundation. Until last year, Wintemute also received substantial funding from both the California and US departments of justice. Since 2005, he has donated $945,000 from his own savings and stock sales to the programme. In July, the university announced that it would endow two professor slots to support Wintemute's programme, each of which comes with $75,000 a year. Wintemute has assumed one and is looking to fill the other one, a position in violence epidemiology. The hiring comes at a time of renewed activity in the field. After the December school shooting, President Barack Obama ordered the CDC to resume research into the causes of gun violence and the ways to prevent it; his 2014 budget request, released on 10 April, asks Congress to provide $10 million for the research. This week in Washington DC, Wintemute spoke to an Institute of Medicine panel that has been formed to advise the CDC on which research questions are most pressing. \n               Inside out \n             As Wintemute delved into gun research in the 1980s, he decided to immerse himself in the gun culture. He joined the NRA and the rifle and pistol club in Davis, where he practised shooting at an indoor range. In 1999, he started to visit gun shows, good opportunites to observe firearm purchases. \u201cGun shows are sort of like zoos,\u201d he says. \u201cYou can easily see a wide range of behaviours.\u201d At his first show in Milwaukee, Wisconsin, the signs used to advertise guns caught his attention. One licensed retailer displayed a Mossberg Model 500 shotgun with a pistol grip next to a poster that read \u201cGreat for Urban Hunting\u201d. Another sign, beside a Savage rifle, read: \u201cGreat for Getto [ sic ] Cruisers\u201d. Wintemute says that he was astonished by the blatant promotion of guns as murder weapons. \u201cIt was clearly a story that had to be told \u2014 bearing witness is part of the job \u2014 but I wanted to figure out a way to tell the story quantitatively, scientifically.\u201d It took several years of trial and error at shows before he was confident enough of his methods to begin collecting data. He cut off his waist-length ponytail so he would not stand out in the crowds, bought a small camera and placed it in a bag of Panda liquorice with a lens-sized hole cut in the side. A pen and notepad would attract too much notice, so he set up his office voicemail so that he could call it from his mobile phone and record long messages. He later added a video camera disguised to look like a button on his shirt. Several times, Wintemute was accused of taking unauthorized photos, and his phone was temporarily confiscated by security personnel, who examined it and found no pictures. After one such episode, he says, a colleague overheard a group of men planning to attack Wintemute outside the show, but Wintemute successfully avoided them. Altogether, he attended 78 gun shows in 19 states, strolling the aisles while apparently deep in a phone conversation. A paper on the findings showed, among other things, that the restrictive policies regulating gun shows in California resulted in fewer illegal 'straw' purchases \u2014 in which someone buys a gun on behalf of a person legally barred from doing so \u2014 than in other states 6 . That publication, and Wintemute\u2019s statements in the media, prompted a backlash from proponents of gun ownership. In June 2007, David Codrea, the author of a blog called WarOnGuns, posted Wintemute's photo online with the note: \u201cWARNING! IF YOU SEE THIS MAN, NOTIFY SECURITY IMMEDIATELY.\u201d The post identified Wintemute by name and called him an \u201canti-gun 'researcher'\u201d who stalked gun shows with hidden cameras and recorders. But by that point, Wintemute says, he had learned all he could and stopped going to shows. \n               Critical approach \n             Last month, on the day after Wintemute spoke to the emergency researchers in San Francisco, the NRA posted a critique slamming a study 7  that reported that states with more firearm laws had lower rates of firearm fatalities. The NRA quoted from an unlikely source to attack the paper: Wintemute, who had published a sharp rebuttal to the paper in the same journal 8 . Wintemute had argued that the association between more laws and fewer deaths disappeared when the authors accounted for firearm ownership in a state \u2014 meaning that it is impossible to say whether the restrictive gun laws save lives by inhibiting gun ownership or whether laws are simply easier to enact in states in which ownership rates are already low. The latter is a more plausible explanation, he wrote. One of the paper's authors, Eric Fleegler, an emergency physician at Boston Children's Hospital in Massachusetts, responds that \u201cwhen you look at firearm-related homicides, even controlling for firearm ownership, firearm-related homicides do decrease in states with more gun laws\u201d. This is not the first time that Wintemute has attacked papers he perceives to be weak, even if they point towards policies he would like to see adopted. And he goes no easier on policies that he views as ineffective, even ones that seek to limit firearm ownership. He has, for instance, repeatedly criticized the assault-weapons ban enacted by Congress in 1994, in part because the ban was easily circumvented. Instead, he advocates three steps informed by research: requiring background checks for all US gun sales, forbidding alcohol abusers and those convicted of violent misdemeanours from buying guns and rewriting current federal restrictions on gun ownership to better capture people who are mentally ill and at risk of violence to themselves or others. Wintemute's rigour has earned the respect of some ideological opponents, but others say that his work betrays anti-gun biases by, for instance, selectively citing the literature in a way that minimizes the value of firearms for self-defence. \u201cWe have followed his research for many years. Pro-gun scholars have criticized it for just as long,\u201d says John Frazer, director of the Research and Information Division at the NRA's lobbying arm, the Institute for Legislative Action in Fairfax. Wintemute's work at gun shows has also triggered complaints. Kopel, the Independence Institute's researcher, says that Wintemute's hidden-camera tactics were \u201csleazy\u201d. \u201cI have a higher opinion of him as a guy who looks at the data and analyses them in a serious way,\u201d Kopel says. Now, Wintemute is focusing on a new project. He is designing a randomized trial to study roughly 20,000 people who purchased guns legally in California but have since lost the right to own firearms because they committed a violent crime, were served with a domestic-violence restraining order or were judged mentally ill and potentially violent. Unlike in other states, authorities in California have begun to take guns away from those people. Wintemute is hoping to test the effectiveness of the policy by comparing re-offence rates among those whose guns are seized quickly versus those who keep them for longer. The money for his own work, at least in the short term, will probably have to come from California or from private sources. Wintemute is not optimistic that funds for CDC firearm research will be forthcoming from Congress in the short term. Whether or not the federal money materializes, Wintemute will continue the work he began 30 years ago. For him, it is part of his mission as a physician to relieve suffering. \u201cEverything that was true of firearm violence in the early 1980s is still true today,\u201d he says. \u201cThere is a fundamental injustice in violence. People don't ask for it; it comes to them.\u201d \n                     Under the gun 2013-Mar-27 \n                   \n                     Seven days: 18\u201324 January 2013 2013-Jan-23 \n                   \n                     Who calls the shots? 2012-Aug-08 \n                   \n                     Blogpost: Obama orders research into gun violence \n                   \n                     Garen Wintemute \n                   \n                     Wintemute bibliography \n                   \n                     National Rifle Association post quoting Wintemute criticism of Fleegler et al \n                   \n                     Mayors Against Illegal Guns report citing decline in gun research literature \n                   Reprints and Permissions"},
{"file_id": "495296a", "url": "https://www.nature.com/articles/495296a", "year": 2013, "authors": [{"name": "Ann Finkbeiner"}], "parsed_as_year": "2006_or_before", "body": "As an early adopter of astronomical technology, Andrea Ghez is revealing secrets about the giant black hole at the Galaxy's centre. The technology was a complete joy, says Andrea Ghez, thinking back to the mid-1980s and her first time helping out at an observatory. She wanted to learn everything. \u201cHow to open the dome! How to fill the instrument with liquid nitrogen! Develop the plates! Reduce the data! Coding!\u201d And then there was the science. Ghez did not know much at the start; she was majoring in physics at the Massachusetts Institute of Technology (MIT) in Cambridge, working for an astronomer as her undergraduate research experience. But as she learned more about his research into unusual cosmic sources of X-rays, Ghez became enthralled by the thought that some of those sources might be black holes \u2014 singular points with a gravitational pull so strong that not even light can escape them. \u201cIt got me completely fascinated by black holes,\u201d she says. By the time she had spent two undergraduate summers working at telescopes in Arizona and Chile, Ghez was hooked. \u201cI fell in love with the whole profession.\u201d Now an astronomer at the University of California, Los Angeles, she still feels the same. Her fascination with black holes has led her into a pioneering, decades-long study that has proved the existence of the biggest black hole in our cosmic neighbourhood: the 4.1-million-solar-mass behemoth that lies at the centre of the Milky Way 1 , 2  (see 'The monster in the middle'). This work earned her a MacArthur 'genius' award in 2008, and half of the Crafoord prize, astronomy's Nobel, in 2012. Ghez's love of technology helps to explain why her quest has been so fruitful. Most astronomers use only the tools they know, but Ghez is an enthusiastic early adopter \u2014 first in line to try out cutting-edge detectors and optical techniques that are barely out of the laboratory. \u201cI like the risk of a new technology,\u201d she says. Maybe it won't work. But maybe it will open a fresh window on the Universe, answering \u201cquestions you didn't even know to ask\u201d, she says. \u201cAny time you look, you're astounded!\u201d Reinhard Genzel, a director of the Max Planck Institute for Extraterrestrial Physics in Garching, Germany \u2014 the co-winner of the 2012 Crafoord prize and Ghez's sharpest competitor on the Galactic Centre work \u2014 puts it very simply. \u201cAndrea,\u201d he says, \u201cis one of a rare adventurous class.\u201d \n               Deep focus \n             Ghez's devotion to her work would make her seem fierce \u2014 if she weren't always smiling, and her sentences didn't keep exploding into verbal capitals. As it is, with her barely controlled curls, straight-across eyebrows and direct gaze, she conveys a cheerful intensity. She doesn't digress when she talks; she focuses. And she has always had a certain determination. According to Ghez family legend, when 4-year-old Andrea watched the first Apollo Moon landing with her parents in Chicago, Illinois, on 20 July 1969, she announced that she, too, was going to the Moon as an astronaut. True, she also wanted to be a ballerina. But while attending the progressive University of Chicago Laboratory Schools, she says, she became \u201creally clear\u201d that she loved mathematics and science. That passion took her to MIT in 1983 and then, after her epiphany in the observatory domes, to the California Institute of Technology (Caltech) in Pasadena for graduate studies in astronomy. Caltech, Ghez explains, \u201chad the best toys by far\u201d. Among them was the 5-metre Hale Telescope, then one of the world's largest, on California's Palomar Mountain. But the toy that particularly captured Ghez's interest was an experimental speckle imager, an instrument intended to get around astronomers' eternal problem with air. Earth's atmosphere is transparent but turbulent \u2014 a collection of bubbling 'cells' that are warmer here, cooler there, and constantly moving. Looking at the sky through all that is like looking at pebbles on the bottom of a rippling stream: the light coming into the telescope flickers, dances and fragments, smearing the point-like image of each star into a fuzzy ball. Speckle imaging freezes the dancing images in place with a camera that captures very short exposures every few milliseconds, taking maybe 10,000 or more shots in total. The result is a sequence of very faint images in which the distorted light from each star produces a scattering of spots: the speckles. Computer processing recombines the speckles into one spot per star. Then all the exposures can be aligned and stacked to produce a final image with the worst of the atmospheric smearing removed. At the time, speckle imaging was well established but done mostly at optical wavelengths and used infrequently, because it was so computationally intensive. Ghez joined a Caltech group that was developing a speckle imager capable of working at infrared wavelengths emitted from interstellar-dust-shrouded objects such as active galaxies: spirals and ellipticals that had dusty but exceptionally bright regions at their centres. The thinking was that both the dust and the light emissions came from vast quantities of stars and gas spiralling into black holes millions or billions of times the mass of the Sun. Ghez was given the job of helping to write image-analysis software to give the speckle-imaging device the highest possible resolution. \u201cOh, this sounds great,\u201d she says, remembering her reaction. \u201cBlack holes. Technique. I'm good.\u201d In the end, however, the imager couldn't quite see the nuclei of active galaxies. So for her PhD dissertation, Ghez turned to brighter targets: newborn stars in our Galaxy. Astronomers knew in broad terms that stars are born in thick, interstellar gas clouds, when gravity pulls the gas into hot, dense knots that ignite with thermonuclear fusion. But they didn't understand why a substantial fraction of the stars in the Sun's neighbourhood are binaries: pairs of stars that orbit one another, often at close range. Were binaries born that way? Or did they somehow pair up later in life? Theory was not much help; the available star-formation models focused on single stars such as the Sun. Ghez wanted to resolve the issue with data: do binary stars form as binaries, yes or no? (She had developed a taste for questions that could be answered crisply. \u201cIf you ask mushy questions,\u201d she says, \u201cyou usually get mushy answers.\u201d) Beginning in 1990, Ghez used her speckle-imaging techniques in two known star-forming regions to survey T Tauri objects \u2014 Sun-like stars just beginning their lives. With the higher resolution, she could see that many of the infant stars were indeed binaries, too young and too close together for the companions to have formed separately and then coalesced 3 . Ghez's finding continues to have implications for issues such as the search for extrasolar planets, because the complex gravitational fields in a binary system are thought to make it difficult for planets to form in the vicinity. Ghez focused on this topic exclusively until 1994, when she was hired at the University of California, Los Angeles, and gained access to a 10-metre instrument opened only a few years before at the Keck Observatory on Mauna Kea in Hawaii, and jointly owned by Caltech and the University of California. The biggest of a new generation of very big telescopes, Keck was ideal for catching the light of faint objects. But like all telescopes on the ground, its resolution was limited to what the atmosphere allowed. So Ghez applied her expertise in speckle imaging, realizing that its high resolution, combined with the telescope's prodigious light-gathering power, would finally let her look at black holes. Astronomers had long suspected that a supermassive black hole lay at the centre of every galaxy; active galaxies were the ones that just happened to have above-average quantities of matter feeding the black hole. If that was true, a giant black hole ought to lie at the centre of the Milky Way. That region is difficult to observe from Earth because there is a lot of interstellar gas and dust in the way and only certain wavelengths of radiation can make it through. But observations in \u03b3-rays, X-rays and radio and infrared emissions were consistent with the presence of gas moving at high velocities, and stars crowding together in high numbers. They had to be orbiting something massive and hard to see. But what? To remove any doubt about the nature of the central object, observers would have to show that it was too small to be anything but a black hole. (The fundamental equations of gravity guarantee that, for a given mass, a black hole is smaller than any possible system made of ordinary matter.) Ghez and her team decided to tackle the problem with a straightforward but tedious strategy: track stars for years and decades as they orbit the central object. The radius and period of each orbit would give the central object's mass, and the distance of closest approach would put an upper limit on its size. The astronomers started in 1995 by mapping the positions of stars in a dense cluster near the suspected location of the black hole. Every year or so, they mapped the positions again. \u201cWith two points you draw a line and get the velocity,\u201d says Ghez. \u201cWith three points, you actually believe your line.\u201d And then you keep going. \u201cYou wait long enough, keep taking pictures\u201d and you should be able to start tracing out the curves of the stars' orbits. By 1999, Ghez and her colleagues had done exactly that with three stars 4 . \n               Seeing the light \n             These observations showed the orbit in only two dimensions, projected onto the plane of the sky. Getting the full orbit required the third dimension, the star's motion towards and away from Earth, which meant measuring the spectrum of the light it emitted and watching how its velocity shifted the wavelength of the spectral lines \u2014 measurements that required more light than speckle imaging could supply. Ghez, however, was already working with astronomers developing a more advanced technology called adaptive optics. The idea was to take the light gathered by the telescope and bounce it off a flexible mirror that could be continuously deformed to counteract distortions caused by the atmosphere. The technique wasted much less light than speckle imaging did, so it promised higher resolution and the ability to capture the spectra of fainter objects. Ghez was doing proof-of concept observations, and was \u201cright there and ready to snatch it up the minute it was ready to go\u201d, she says. In June 2002, Ghez and her team used adaptive optics to observe one of their three stars, called S0-2, and found that by adding in older speckle data they could map its complete orbit 1 . The orbit took 16 years, implied a central mass of 4.1 million Suns and showed that the star's closest approach to that mass was less than twice the distance from Pluto to the Sun \u2014 meaning that the central object was no larger than the Solar System. With that much mass in that small a space, says Ghez, \u201cthat's the evidence for a black hole. There are no alternatives that we know of.\u201d That result was satisfying but not unexpected. Much more surprising was that S0-2's spectrum showed it to be less than 10 million years old. No star orbiting close to a black hole should be that young, because the black hole's tidal forces would shred any cloud of gas and dust before it could form. There was no good explanation for why the young stars are there, says Thomas Prince, an astronomer at Caltech and Ghez's first adviser, \u201cand there still is not\u201d. In subsequent years, as adaptive optics improved, Ghez and her team measured the velocities of thousands of stars in the Galactic Centre, and estimated the orbits of about two dozen; the latest is a star named S0-102, which has the shortest orbit yet at 11.5 years (ref.  2 ). Most of them also proved to be equally young and enigmatic. \u201cIt's beautiful work really,\u201d says Rosemary Wyse, an astronomer at Johns Hopkins University in Baltimore, Maryland, who studies the Milky Way. \u201cIt's one of those painstakingly technical tours de force that produces good science results.\u201d Only one other team in the world is doing this type of work: Genzel's. Both groups have access to large telescopes and sophisticated adaptive-optics systems, and the word is that competition is fierce \u2014 which neither Ghez nor Genzel denies, although they try to play it down. Perhaps more important is that they agree on the same measurements on the same stars, differing mainly in interpretations. The latest example concerns an object that might help to show how a black hole destroys its prey. Both Ghez and Genzel agree that it is hot, red and heading straight towards the Galactic Centre. Genzel says that it is a cloud of gas with the mass of three Earths, that it will be torn to smithereens by the black hole and that in the process it will radiate X-rays fiercely 5 . Ghez maintains that it is hard to say what the object is; it could equally be a star moving through gas, in which case the black hole is unlikely to disrupt it. Genzel says that the object's closest approach to the black hole will be this autumn; Ghez says maybe later and in any case the timing is uncertain. Avi Loeb, an astronomer at Harvard University in Cambridge, Massachusetts, agrees with both Ghez and Genzel that this kind of competition over difficult, expensive observations makes the resulting astronomy more credible. \u201cOverall,\u201d he says, \u201cit's good for science.\u201d Ghez is already looking to the next technologies she can adopt. She is on the scientific advisory committee of the Thirty Meter Telescope, which is due to start observations from Mauna Kea towards the end of the decade and will gather roughly ten times more light than Keck. She is also advising the developers of a version of adaptive optics that promises even higher resolutions than are currently available. \u201cI'm positioning myself,\u201d says Ghez. \u201cI want to know how to use these systems, I want to know how to use the data, I want this all to work.\u201d Like other great scientists, says Thomas Soifer, an astronomer at Caltech and another of her early advisers, \u201cshe has this single-minded attitude of, 'I'm going to beat this problem into submission'\u201d. Ghez is 47 years old and the stars that she has been tracking since 1995 have orbits ranging from tens to hundreds of years. How long does she plan on living? \u201cI don't know,\u201d says Ghez. \u201cI figure the more fun I have, the longer I'll live.\u201d \n                     Galaxy formation: The new Milky Way 2012-Oct-03 \n                   \n                     A gas cloud on its way towards the supermassive black hole at the Galactic Centre 2011-Dec-14 \n                   \n                     Astrophysics: How galaxies got their black holes 2011-Jan-19 \n                   \n                     Bright light hints at a dark centre to the Galaxy 2009-Oct-19 \n                   \n                     A long time ago, in a galaxy not so far away 2007-Apr-04 \n                   \n                     Black holes: Sparks of interest 2003-Oct-30 \n                   \n                     Andrea Ghez \n                   \n                     Keck Observatory \n                   \n                     Andrea Ghez at the 2009 TED conference \n                   \n                     Stars orbiting the Milky Way black hole \n                   \n                     Interview with Reinhard Genzel \n                   Reprints and Permissions"},
{"file_id": "495300a", "url": "https://www.nature.com/articles/495300a", "year": 2013, "authors": [{"name": "Brian Owens"}], "parsed_as_year": "2006_or_before", "body": "The world's longest-running experiments remind us that science is a marathon, not a sprint. Although science is a long-term pursuit, research is often practised over short timescales: a discrete experiment or a self-contained project constrained by the length of a funding cycle. But some investigations cannot be rushed. To study human lifespans or the roiling of Earth's crust and the Sun's surface, for instance, requires decades and even centuries. Brian Owens talks about the efforts to continue recording sunspots and pitch drops Here,  Nature  takes a look at five of science's longest-running projects, some of which have been amassing data continuously for centuries. Some generate hundreds of papers a year; one produces a single data point per decade. Experiments operating at this pace are challenged by shifting research priorities and technologies, and their existence is regularly threatened by funding droughts and changes in stewardship. But they are bound together by the foresight of the scientists who started them and the patience and dedication of those who carry the torch. If persistence predicts a long and healthy life \u2014 as one 90-year study of human longevity has suggested \u2014 then the scientists featured here could set some records themselves. \n               400 years: Counting spots \n             Astronomers have been recording the appearance of sunspots ever since the telescope was invented more than 400 years ago; even Galileo recorded his observations. But early observers had no knowledge of what the dark patches on the Sun's surface were, or of the magnetic fields that created them. That began to change when, in 1848, the Swiss astronomer Rudolf Wolf began making systematic observations and developed a formula that is still used today to calculate the international sunspot number, also known as the Wolf number, which gives a measure of how solar activity is changing over time. In 2011, Fr\u00e9d\u00e9ric Clette became director of the Solar Influences Data Analysis Center, based at the Royal Observatory of Belgium in Uccle, which curates sunspot counts gleaned from photographs and hand drawings of the Sun's surface made by more than 500 observers since 1700. The data are invaluable for predicting sunspot activity, says Leif Svalgaard, a solar physicist at Stanford University in California. The activity seems to wax and wane over the course of 11 years or so, and the streams of charged particles that the sunspots spray into space can affect satellites and electronics on Earth. The detailed records help researchers to understand why that cycle happens, and to refine predictions of particularly intense events. \u201cThe longer the time series is, the better we can check our theories,\u201d Svalgaard says. Around 200 papers a year cite sunspot data, in fields extending beyond solar physics to geomagnetism, atmospheric science and climate science. But the enterprise runs largely on goodwill. Each month, the Belgian centre collates sunspot numbers from about 90 observers, two-thirds of them amateurs, who use small optical telescopes no more powerful than those available 200 years ago. And although it is a World Data Centre recognized by the International Council for Science in Paris, it receives no funding from the organization. Clette works with one other part-time person to maintain the database, in addition to his 'night-job' as an astronomer at the Royal Observatory of Belgium. Still, says Clette, it is fascinating to 'work' with colleagues from hundreds of years ago. For instance, he says that even though Galileo's coverage of the Sun was spotty because Galileo was \u201cbusy with planets and other things\u201d, the drawings are detailed enough to reveal information about the magnetic structure of the sunspot groups and the size and tilt of the star's dipole. \u201cYou can extract from those drawings exactly the same information as from a drawing made today,\u201d he says. More than that, however, he is taken with his forebears' foresight. They faithfully recorded what they saw, thinking that it could be useful later on, he says. \u201cIt's a fundamental aspect of science,\u201d he says, \u201cnot worrying what will be the final result.\u201d \n               170 years: Monitoring an irritable giant \n             Although consistently active, every few thousand years, Mount Vesuvius erupts in spectacular style. The last time it did so, in  AD  79, it consumed the city of Pompeii in the flames and before that, about 3,800 years ago, it covered all of present-day Naples in hot gas and rock (see  Nature   473 , 140\u2013141; 2011 ). The Vesuvius Observatory, the oldest volcano research station in the world, has been keeping an eye on its inhospitable subject since 1841, logging all the volcano's seismic rumbles to try to spot approaching danger. Originally perched 600 metres up the side of the volcano, far enough from the summit to be safe from ejected debris and high enough on a knoll to avoid the lava flows, the observatory has shaped the way that volcanology and geology is done, says Marcello Martini, its current director. Macedonio Melloni, the observatory's first director, did pioneering work on the magnetic properties of lava that was crucial to later studies of palaeomagnetism \u2014 the history of Earth's magnetic field as recorded in rocks. In 1856, its second director, Luigi Palmieri, invented the electromagnetic seismograph, which was much more sensitive to ground tremors than previous machines and allowed him to predict eruptions. Under Palmieri and subsequent directors the observatory contributed to the development of much of the instrumentation used to monitor volcanoes worldwide. In the early twentieth century, for example, Giuseppe Mercalli developed the scale still used today to classify volcanic activity. But the building itself no longer has the same role. \u201cIn the early stages it was important to be as close to the action as possible, but that's not necessarily the case any more,\u201d says Haraldur Sigurdsson, a volcanologist at the University of Rhode Island in Kingston. Most of the monitoring is now done by remote, ground-based sensors, with the data sent back to the lab of the National Institute of Geophysics and Volcanology in Naples. The original buildings were turned into a museum in 1970. In addition to informing scientific theory, the observations are used to predict trouble and protect the public \u2014 as they did successfully in 1944. The Naples lab, where scientists are on duty 24 hours a day, also keeps an eye on Mount Stromboli, on an island north of Sicily; the Campi Flegrei caldera west of Naples; and the island of Ischia. Sigurdsson, says, however, that the future of volcanology lies not with putting sensors on the volcanoes already known to be dangerous, but with satellite-based radar that can study ground deformation everywhere and pick out the risky regions irrespective of geologists' expectations. \u201cWe should be moving towards an internationally coordinated system of volcano monitoring that is not tied down to bricks and mortar on the side of a volcano, but looking at it in a really comprehensive way globally,\u201d he says. \n               170 years: Harvesting data \n             The stewards of long-term research projects are keen to maintain the integrity of the work, but also to keep it relevant. That is the case for Andy Macdonald who, in 2008, inherited a set of agricultural experiments that have been testing the effects of mineral fertilizers and organic manure on crop production since 1843. Started by the fertilizer magnate John Lawes on the grounds of his country estate at Rothamsted, north of London, these studies have been used to test how nitrogen, phosphorus, potassium, sodium, magnesium and farmyard manure affect the yields of several staple crops, including wheat, barley, legumes and root crops. \u201cAfter 20 or 30 years, many of the basic questions about the relative importance of different fertilizers were pretty well answered,\u201d says Macdonald, manager of the 'classical experiments' now run at Rothamsted Research. Nitrogen has the largest effect, followed by phosphorus. So the experiments are periodically updated to test new ideas and keep them relevant to current farming practice. In 1968, for example, the long-strawed cereal crops that had been grown since the experiments started were replaced by the higher-yielding short-strawed cereal crops being adopted by farmers. Macdonald says that these new crops turned out to need more fertilizer than the traditional cultivars because of the additional nutrients they were extracting from the soil, so farmers had to adapt. \u201cRothamsted is the granddaddy of long-term agricultural research,\u201d says Phil Robertson, director of the W. K. Kellogg Biological Station, a long-term agricultural research site at Michigan State University in Hickory Corners. The unbroken chain of data is invaluable, he says. Not only is Rothamsted able to study environmental and biological trends \u2014 such as carbon storage in soil or the effects of invasive species \u2014 that become apparent only over long timescales, but it also provides a platform for shorter studies on, say, nitrate loss in soil. The Rothamsted archive holds about 300,000 preserved plants and soil samples that have been collected since the experiments began. In 2003, scientists extracted the DNA of two wheat pathogens from archived samples dating back to 1843 and showed that industrial sulphur dioxide emissions affected which one was dominant 1 . It can be a struggle to keep funding agencies interested. Rothamsted gets by on a mixture of government funding, competitive grants and a trust fund set up by Lawes before he died. \u201cAs a funder you have to commit to maintaining observations even during periods where there may not be any exciting results,\u201d says Robertson, who was involved in setting up the US Department of Agriculture's Long Term Agro-Ecosystem Research network last year. Macdonald and his team take pride in their history. \u201cI think back sometimes to John Lawes,\u201d Macdonald says. \u201cI feel a great responsibility to ensure that the experiments are handed on in good condition for the next generations. They're not a museum piece, they're part of our living scientific community.\u201d \n               90 years: Watching genius blossom \n             In 1921, psychologist Lewis Terman of Stanford University in California started tracking more than 1,500 gifted children \u2014 as identified by the Stanford\u2013Binet IQ test that he developed \u2014 born between 1900 and 1925. It was one of the world's first longitudinal studies and it has now accrued the longest in-depth record of human development, having tracked the participants for nine decades \u2014 looking at their home lives, education, interests, abilities and personality. One of Terman's main goals in his 'Genetic Studies of Genius' was to disprove the then-common assumption that gifted children were sickly, socially inept and not well-rounded. But even by the standards of his day, the study design was plagued with problems. His selection method was haphazard, administering the tests based largely on recommendations from teachers, and the sample was far from representative (more than 90% were white and upper or middle class, and Terman even enrolled his own children). What is more, Terman skewed the very life outcomes he was trying to study, writing letters of recommendation for his 'Termites', as the participants became known, and helping several to get into Stanford. By tracking the children into adulthood, Terman showed that they were just as healthy and well-adjusted as the general population and that they generally grew into successful, happy adults. And, as it progressed, researchers tweaked the study to try to overcome some of its flaws. In the 1980s, for example, George Vaillant, a psychologist at Harvard Medical School in Boston, Massachusetts, began using Terman data to supplement his own long-running study of adult development and started collecting the death certificates of the Termites. Using these records, Howard Friedman, a psychologist at the University of California, Riverside, was able to come up with one of the Terman study's most significant findings. He showed that conscientiousness \u2014 namely prudence, persistence and planning \u2014 measured in both childhood and adulthood is a key psychological factor predicting longevity, and was associated with as many as six or seven extra years of life 2 . \u201cIt would not have been easy to discover this without a lifespan longitudinal prospective data set,\u201d says Friedman. Longitudinal studies evolve according to scientific fashions, says Laura Carstensen, director of the Stanford Center on Longevity. New investigators will add measures and modify or discard those that they think are no longer interesting or are obsolete. \u201cToday we would measure emotional well-being, for example, in a very different way from in 1900,\u201d she says. So in many ways \u201clooking at a longitudinal data set is almost like writing a history of psychology\u201d. \n               85 years: Waiting for the drop \n             On his second day of work at the University of Queensland in Brisbane, Australia, in 1961, physicist John Mainstone came across a quirky little experiment that had been quietly running in a cupboard for 34 years. Fifty years later, he is still looking after it \u2014 and still waiting to witness its most dramatic activity. The pitch-drop experiment started when Thomas Parnell, the university's first professor of physics, set up a demonstration for his students to show that a sample of pitch, a black tar distillate that is brittle enough to shatter with a hammer when cold, will act like a liquid and flow through a funnel, dripping out of the bottom like the world's slowest hour glass. It did, at a rate of about one drop every 6\u201312 years. Mainstone expects \u2014 cautiously \u2014 the ninth drop to fall sometime towards the end of this year. The experiment is not exactly a hotbed of discovery. In 86 years, it has yielded exactly one scientific paper 3 , which calculated that the pitch was 230 billion times more viscous than water. And in 2005, it earned the dubious distinction of an Ig Nobel prize (see  Nature 437, 938\u2013939; 2005 ), a cheeky parody of the Nobels. There is, however, still some science to be gleaned. No one has ever seen a drop fall \u2014 a webcam recording the experiment failed just when the most recent drop fell, in November 2000 \u2014 so exactly what happens when the drop detaches from the body of pitch above is unknown. It will also take another few decades to tease out how weather, the introduction of air conditioning and the vibrations from renovation work in the building influence the drip rate. But Mainstone says that the experiment's value lies not in its science, but in its historical and cultural impact: it has inspired sculptors, poets and writers to muse about the passage of time and the pace of modern life. It also provides a link to scientific history, and a sense of constancy. \u201cIt's going about its business while the world is going through all sorts of turmoil,\u201d Mainstone says. And with a great deal of pitch left in the funnel, it has the potential to serenely ignore that turmoil for another 150 years or so. Luckily, 78-year-old Mainstone has already convinced a younger colleague to watch over the experiment after he is gone. \n                     Giant Sun scope clears final hurdle 2012-Nov-15 \n                   \n                     Volcanology: Europe's ticking time bomb 2011-May-11 \n                   \n                     Epidemiology: Study of a lifetime 2011-Mar-01 \n                   \n                     GM under fire again 2004-Mar-05 \n                   \n                     Pitch-drop experiment \n                   \n                     The vexing legacy of Lewis Terman \n                   \n                     Rothamstead Research \n                   \n                     Mount Vesuvius Observatory \n                   \n                     Solar Influences Data Analysis Centre \n                   Reprints and Permissions"},
{"file_id": "495430a", "url": "https://www.nature.com/articles/495430a", "year": 2013, "authors": [{"name": "Richard Monastersky"}], "parsed_as_year": "2006_or_before", "body": "As scientific publishing moves to embrace open data, libraries and researchers are trying to keep up. A few passing students do a double take as Sayeed Choudhury waves his outstretched right arm. In his crisply pressed dress shirt and trousers, the engineer looks as if he is practising dance moves in slow motion. But he is really playing with astronomical data. Standing in a US$32-million library building opened last year at Johns Hopkins University in Baltimore, Maryland, Choudhury faces a 2-metre-by-4-metre 'visualization wall' of television screens. Pointing with his arm, he selects a picture of the Ring Nebula out of 40 images from the Hubble Space Telescope. Choudhury spreads his hands in a welcoming gesture and the nebula's rim of glowing orange gas fills the frame. This wall is the brainchild of computer scientist Greg Hager and Choudhury, who directs digital research and curation at the library. For $30,000, they and their team patched together monitors, processors and the Microsoft Kinect system that recognizes arm and body gestures. They placed the wall in the library last October as an experiment, allowing students and researchers to explore a few of the university's data sets, from star systems to illustrated medieval manuscripts. \u201cAs we create more and more digital content, there's a question of how do you get people to even realize we have it and then interact with it in new ways,\u201d says Choudhury, who thinks that the wall is starting to catch on. One chemical engineer wants to use it to visualize and manipulate molecules, and astronomers hope that it could help to train students in categorizing galaxies. By providing alternative ways to explore and share data, says Choudhury, the wall \u201cis a new form of publishing\u201d. Around the world, university libraries are racing to reinvent themselves to keep up with rapid transformations in twenty-first-century scholarship. They still do a brisk business in purchasing books, licensing access to academic journals and providing study spaces and research training for students. And libraries are increasingly helping teachers to develop courses and adopt new technologies. But for working scientists, who can now browse scientific literature online without leaving their desks, much of this activity goes unseen. For many, libraries seem to be relics that no longer serve their needs. \n               The new data wranglers \n             That could soon change. At Johns Hopkins and many other top universities, libraries are aiming to become more active partners in the research enterprise \u2014 altering the way scientists conduct and publish their work. Libraries are looking to assist with all stages of research, by offering guidance and tools for collecting, exploring, visualizing, labelling and sharing data. \u201cI see us moving up the food chain and being co-contributors to the creation of new knowledge,\u201d says Sarah Thomas, the head of libraries at the University of Oxford, UK. It is not yet clear how successful this reinvention will be, especially given the tight budgets facing both libraries and researchers. And as they step into the data-curation business, libraries are entering a crowded market of commercial publishers, information-storage companies and discipline-specific data repositories such as GenBank, which archives DNA sequences. But many say that libraries have a natural role in the data world, and that their importance will only grow with the push to unlock the products of research. Last month, US President Barack Obama's administration ordered granting agencies to ensure that the public can access publications and data generated by federally funded research. \u201cThis is going to have significant repercussions and result in much greater appreciation and support for the need to preserve data and make it available for scientific use,\u201d says William Michener, an information scientist at the University of New Mexico libraries in Albuquerque. \u201cLibraries are really critical stakeholders in this new landscape. They're the first line of defence when faculty members have a problem with managing their data.\u201d The research team at the Hopkins' Sheridan Libraries is one of the leaders in planning for this transformation, thanks in part to more than a decade of experience managing data from the Sloan Digital Sky Survey, which has mapped nearly one million galaxies. Choudhury is also principal investigator on a $9.4-million grant from the US National Science Foundation (NSF) to develop the Data Conservancy, a multi-institution programme that is researching and building tools for data curation. That grant enabled Johns Hopkins to launch a fee-based service in 2011 to help researchers manage their data. University scientists were not thrilled with the idea at first. During a 2011 meeting to describe the efforts, some rebelled against what seemed to be a mandatory data-management charge \u2014 \u201csort of like a tax on proposals\u201d, says Noah Cowan, a mechanical engineer at Johns Hopkins. It did not help that Cowan and his colleagues were already dealing with new data-management obligations: earlier that year, the NSF had started requiring that grant-seeking researchers make clear how they would disseminate and share their data. Choudhury had to work hard to explain that Johns Hopkins' data service would be voluntary and useful. \u201cPreservation is not a big selling point for researchers,\u201d says Betsy Gunia, a data-management consultant at the university. \u201cSome of it is lack of awareness.\u201d Cowan, who studies animal biomechanics, eventually stepped forward as one of the service's first customers, thinking that he could improve on his usual practices. In his office, he pulls up an example of the kind of data he generates: high-speed videos of a knifefish swimming in a simulated current. His team records the fish's fin movements and his neuroscientist colleagues measure nerve signals to study how the animal controls its position in the water. Although the science is cutting edge, Cowan's approach to data is relatively old-school. When a study is complete, he stores the video and analyses on a hard drive on his shelf. And like many other researchers, he shares his data on a case-by-case basis in response to requests. Those methods have generally sufficed, but when a graduate student wanted to reanalyse some 7-year-old studies last summer, it took a few months to make sense of the data. They were kept separately from the analysis code and there were multiple versions of code to sort through. The poor quality of the metadata \u2014 information that describes the data \u2014 \u201cmade it a treasure hunt\u201d, says Cowan. So when he started drafting a new NSF proposal to explore the neural activity of singing birds, he sat down with Gunia and another information specialist. Together, they developed a plan to organize the project's data and make some available to outside researchers. If the NSF funds the project, the Johns Hopkins service will curate and store Cowan's data for five years under a renewable contract. The curation process is much more involved than simply storing data online through a service such as Dropbox. Cowan will supply 'readme' files of metadata as well as any scripts used to collect or process the data. The service will also help him to label the data with a unique and permanent reference, such as the digital object identifier (DOI) numbers used by publishers \u2014 a vast improvement over web links to a data set, which can break, resulting in the all-too-familiar '404 error' message. And with a persistent identifier, the data become directly citable by others. The Johns Hopkins data-management team also agrees to safeguard against problems such as the storage media degrading, files getting corrupted and data formats becoming obsolete \u2014 protections not offered by many existing university repositories, which primarily house digital documents. \u201cI don't have to suffer the problem of bit rot,\u201d says Cowan. If the grant is funded, the services will amount to roughly 2% of direct costs, but Cowan doesn't mind paying. \u201cMy time is better spent mentoring students and collecting and analysing data than running my own long-term data archive,\u201d he says. \n               Concern over curation \n             Many scientists are too busy or lack the knowledge to tackle data-management on their own. In a survey of 1,300 scientists completed in 2010, more than 80% said that they would use other researchers' data sets if they were easy to reach, but only 36% said that others could access their data easily (see 'The data gap') (C. Tenopir  et al .  PLoS ONE   6 , e21101; 2011 ). Scientists who do their own data curation can take advantage of many new options, such as DataONE, an international network for preserving and sharing data that was developed through a $20-million grant from the NSF, which Michener is leading. Another non-profit option is Dryad, a repository that helps researchers to identify data sets, store them and connect them with publications. Data services are also available from a range of companies, including figshare in London (owned by Macmillan Publishers, the parent company of  Nature ) and the Data Citation Index, launched last year by Thomson Reuters in New York. However, libraries are particularly well positioned to fill the data-management gap, says Michener. They have long experience helping faculty members and they are not likely to disappear, he says. \u201cLibraries have a significant amount of trust capital.\u201d And many are interested in entering the data-management game. Carol Tenopir, an information scientist at the University of Tennessee in Knoxville who ran the 2010 survey of scientists' data habits, also conducted an as-yet unpublished study of libraries at more than 100 US research universities in 2011\u201312. Although fewer than 20% offered data services at the time, nearly 40% had plans to help scientists to curate and store their data within two years, says Tenopir. Oxford's Bodleian Libraries are already developing a fee-based approach with different tiers of storage, from completely closed levels for sensitive information such as patient data to more open tiers for publicly accessible data and metadata. A survey of Oxford researchers last year indicated that they collectively would have about 3 petabytes (3 million gigabytes) of data to deposit in the repository's first year \u2014 roughly double what is currently stored in Oxford's central file system. But they will probably not deposit the whole amount, says Wolfram Horstmann, who is developing the digital services for the Bodleian. The survey did not mention that researchers would have to document their data and pay for storage, at a rate of about \u00a35,000 (US$7,500) per terabyte (1,000 gigabytes). Other universities are exploring different approaches for curating research data. Stanford University in California, for example, is piloting a data-management service and a repository in which researchers can deposit their own data, with no cost for small and simple items. Yet many universities lack the resources to house their own repositories and instead will help researchers to find appropriate existing ones. \u201cIt's just the very top tier of research institutions that are going to be able to be data repositories,\u201d says Tenopir. The push to share data is accelerating in several countries. The Australian government has invested AU$75.5 million (US$78.3 million) to establish the Australian National Data Service, run from Monash University in Melbourne. Monash librarian Cathrine Harboe-Ree says that the service is helping Australian universities to identify and publish all kinds of information \u2014 \u201ceverything from the petabytes of data from a synchrotron to the quite modest material from an oral research project\u201d. The data-curation efforts at Monash, Johns Hopkins and other libraries dovetail with what many people say is a revolution in scientific publishing: the move away from narrative, text-based papers as the major output of research. \u201cThe classical scholarly publishing model has reached its limit because it mostly hasn't changed in the past 300 years,\u201d says Jan Brase, an information researcher at the German National Library of Science and Technology in Hanover and managing director of DataCite, an international organization focused on describing and citing data sets. In the future, scientific output could be measured using all types of data, from spreadsheets of observations to algorithms and analysis tools. \u201cUntil recently, data have been considered a second-class citizen in the science and publishing world, and that's all about to change,\u201d says Michener. \n               From papers to products \n             A step in that direction came this year, when the NSF altered its proposal guidelines by allowing researchers to list 'products' that they have created, such as data sets and software, instead of just publications. At a congressional hearing this month, Choudhury and others made the case that public access to scientific data is a matter of national competitiveness. And proponents of open data argue that it will help to expose fraud and mistakes in research. In negotiating all these changes, libraries face huge challenges, including shrinking budgets and the rising cost of journal subscriptions. \u201cThe combination of the paradigm shift and fiscal pressure has led some to use the word crisis,\u201d says Brian Schottlaender, chief librarian at the University of California, San Diego, whose budget shrank by 21% between 2008 and 2012. Schottlaender himself takes a less dire view, saying that libraries are at a \u201ccrossroads\u201d. It is too early to say whether libraries will succeed at redefining themselves in the digital age, says Eefke Smit, director of standards and technology at the International Association of Scientific, Technical & Medical Publishers in Amsterdam, the Netherlands. But \u201cit definitely seems that some are successful in setting up new things\u201d. If the emerging complex data environment is any predictor, libraries will one day be part of a vast ecosystem dealing with information curation. But everybody hopes that the borders between repositories will be seamless, so that a researcher running a query from his or her desk can pull up data from around the world. Many portray the new focus on data as a big change for libraries, but Thomas says that it is not a huge departure from what they have been doing for centuries: organizing information, preserving it and making it available to scholars. Scientific data sets are more complicated, she says, \u201cbut in some respects they're no different from a page in a medieval manuscript\u201d. \n                     The future of publishing: A new page 2013-Mar-27 \n                   \n                     Open your minds and share your results 2012-Jun-27 \n                   \n                     Post-publication sharing of data and tools 2009-Sep-09 \n                   \n                     Prepublication data sharing 2009-Sep-09 \n                   \n                     Data sharing: Empty archives 2009-Sep-09 \n                   \n                     Nature special: The future of publishing \n                   \n                     Nature special: Big data \n                   \n                     Nature special: Data sharing \n                   \n                     Johns Hopkins Data Management Services \n                   \n                     Johns Hopkins visualization wall \n                   \n                     DataONE network \n                   \n                     DataCite \n                   \n                     Dryad digital repository \n                   Reprints and Permissions"},
{"file_id": "495426a", "url": "https://www.nature.com/articles/495426a", "year": 2013, "authors": [{"name": "Richard Van Noorden"}], "parsed_as_year": "2006_or_before", "body": "Cheap open-access journals raise questions about the value publishers add for their money. Michael Eisen doesn't hold back when invited to vent. \u201cIt's still ludicrous how much it costs to publish research \u2014 let alone what we pay,\u201d he declares. The biggest travesty, he says, is that the scientific community carries out peer review \u2014 a major part of scholarly publishing \u2014 for free, yet subscription-journal publishers charge billions of dollars per year, all told, for scientists to read the final product. \u201cIt's a ridiculous transaction,\u201d he says. Eisen, a molecular biologist at the University of California, Berkeley, argues that scientists can get much better value by publishing in open-access journals, which make articles free for everyone to read and which recoup their costs by charging authors or funders. Among the best-known examples are journals published by the Public Library of Science (PLoS), which Eisen co-founded in 2000. \u201cThe costs of research publishing can be much lower than people think,\u201d agrees Peter Binfield, co-founder of one of the newest open-access journals,  PeerJ , and formerly a publisher at PLoS. But publishers of subscription journals insist that such views are misguided \u2014 born of a failure to appreciate the value they add to the papers they publish, and to the research community as a whole. They say that their commercial operations are in fact quite efficient, so that if a switch to open-access publishing led scientists to drive down fees by choosing cheaper journals, it would undermine important values such as editorial quality. These charges and counter-charges have been volleyed back and forth since the open-access idea emerged in the 1990s, but because the industry's finances are largely mysterious, evidence to back up either side has been lacking. Although journal list prices have been rising faster than inflation, the prices that campus libraries actually pay to buy journals are generally hidden by the non-disclosure agreements that they sign. And the true costs that publishers incur to produce their journals are not widely known. The past few years have seen a change, however. The number of open-access journals has risen steadily, in part because of funders' views that papers based on publicly funded research should be free for anyone to read. By 2011, 11% of the world's articles were being published in fully open-access journals 1  (see 'The rise of open access'). Suddenly, scientists can compare between different publishing prices. A paper that costs US$5,000 for an author to publish in  Cell Reports , for example, might cost just $1,350 to publish in  PLoS ONE  \u2014 whereas  PeerJ  offers to publish an unlimited number of papers per author for a one-time fee of $299. \u201cFor the first time, the author can evaluate the service that they're getting for the fee they're paying,\u201d says Heather Joseph, executive director of the Scholarly Publishing and Academic Resources Coalition in Washington DC. The variance in prices is leading everyone involved to question the academic publishing establishment as never before. For researchers and funders, the issue is how much of their scant resources need to be spent on publishing, and what form that publishing will take. For publishers, it is whether their current business models are sustainable \u2014 and whether highly selective, expensive journals can survive and prosper in an open-access world. \n               The cost of publishing \n             Data from the consulting firm Outsell in Burlingame, California, suggest that the science-publishing industry generated $9.4 billion in revenue in 2011 and published around 1.8 million English-language articles \u2014 an average revenue per article of roughly $5,000. Analysts estimate profit margins at 20\u201330% for the industry, so the average cost to the publisher of producing an article is likely to be around $3,500\u20134,000. Most open-access publishers charge fees that are much lower than the industry's average revenue, although there is a wide scatter between journals. The largest open-access publishers \u2014 BioMed Central and PLoS \u2014 charge $1,350\u20132,250 to publish peer-reviewed articles in many of their journals, although their most selective offerings charge $2,700\u20132,900. In a survey published last year 2 , economist Bo-Christer Bj\u00f6rk of the Hanken School of Economics in Helsinki and psychologist David Solomon of Michigan State University in East Lansing looked at 100,697 articles published in 1,370 fee-charging open-access journals active in 2010 (about 40% of the fully open-access articles in that year), and found that charges ranged from $8 to $3,900. Higher charges tend to be found in 'hybrid' journals, in which publishers offer to make individual articles free in a publication that is otherwise paywalled (see 'Price of prestige'). Outsell estimates that the average per-article charge for open-access publishers in 2011 was $660. Although these fees seem refreshingly transparent, they are not the only way that open-access publishers can make money. As Outsell notes, the $660 average, for example, does not represent the real revenue collected per paper: it includes papers published at discounted or waived fees, and does not count cash from the membership schemes that some open-access publishers run in addition to charging for articles. Frequently, small open-access publishers are also subsidized, with universities or societies covering the costs of server hosting, computers and building space. That explains why many journals say that they can offer open access for nothing. One example is  Acta Palaeontologica Polonica , a respected open-access palaeontology journal, the costs of which are mostly covered by government subsidies to the Institute of Paleobiology of the Polish Academy of Sciences in Warsaw; it charges nothing for papers under 10 pages. Another is  eLife , which is covered by grants from the Wellcome Trust in London; the Max Planck Society in Munich, Germany; and the Howard Hughes Medical Institute in Chevy Chase, Maryland. And some publishers use sets of journals to cross-subsidize each other: for example,  PLoS Biology  and  PLoS Medicine  receive subsidy from  PLoS ONE , says Damian Pattinson, editorial director at  PLoS ONE . Neither PLoS nor BioMed Central would discuss actual costs (although both organizations are profitable as a whole), but some emerging players who did reveal them for this article say that their real internal costs are extremely low. Paul Peters, president of the Open Access Scholarly Publishing Association and chief strategy officer at the open-access publisher Hindawi in Cairo, says that last year, his group published 22,000 articles at a cost of $290 per article. Brian Hole, founder and director of the researcher-led Ubiquity Press in London, says that average costs are \u00a3200 (US$300). And Binfield says that  PeerJ 's costs are in the \u201clow hundreds of dollars\u201d per article. The picture is also mixed for subscription publishers, many of which generate revenue from a variety of sources \u2014 libraries, advertisers, commercial subscribers, author charges, reprint orders and cross-subsidies from more profitable journals. But they are even less transparent about their costs than their open-access counterparts. Most declined to reveal prices or costs when interviewed for this article. The few numbers that are available show that costs vary widely in this sector, too. For example, Diane Sullenberger, executive editor for  Proceedings of the National Academy of Sciences  in Washington DC, says that the journal would need to charge about $3,700 per paper to cover costs if it went open-access. But Philip Campbell, editor-in-chief of  Nature , estimates his journal's internal costs at \u00a320,000\u201330,000 ($30,000\u201340,000) per paper. Many publishers say they cannot estimate what their per-paper costs are because article publishing is entangled with other activities. ( Science , for example, says that it cannot break down its per-paper costs; and that subscriptions also pay for activities of the journal's society, the American Association for the Advancement of Science in Washington DC.) Scientists pondering why some publishers run more expensive outfits than others often point to profit margins. Reliable numbers are hard to come by: Wiley, for example, used to report 40% in profits from its scientific, technical and medical (STM) publishing division before tax, but its 2013 accounts noted that allocating to science publishing a proportion of 'shared services' \u2014 costs of distribution, technology, building rents and electricity rates \u2014 would halve the reported profits. Elsevier's reported margins are 37%, but financial analysts estimate them at 40\u201350% for the STM publishing division before tax. ( Nature  says that it will not disclose information on margins.) Profits can be made on the open-access side too: Hindawi made 50% profit on the articles it published last year, says Peters. Commercial publishers are widely acknowledged to make larger profits than organizations run by academic institutions. A 2008 study by London-based Cambridge Economic Policy Associates estimated margins at 20% for society publishers, 25% for university publishers and 35% for commercial publishers 3 . This is an irritant for many researchers, says Deborah Shorley, scholarly communications adviser at Imperial College London \u2014 not so much because commercial profits are larger, but because the money goes to shareholders rather than being ploughed back into science or education. But the difference in profit margins explains only a small part of the variance in per-paper prices. One reason that open-access publishers have lower costs is simply that they are newer, and publish entirely online, so they don't have to do print runs or set up subscription paywalls (see 'How costs break down'). Whereas small start-ups can come up with fresh workflows using the latest electronic tools, some established publishers are still dealing with antiquated workflows for arranging peer review, typesetting, file-format conversion and other chores. Still, most older publishers are investing heavily in technology, and should catch up eventually. \n               Costly functions \n             The publishers of expensive journals give two other explanations for their high costs, although both have come under heavy fire from advocates of cheaper business models: they do more and they tend to be more selective. The more effort a publisher invests in each paper, and the more articles a journal rejects after peer review, the more costly is each accepted article to publish. Publishers may administer the peer-review process, which includes activities such as finding peer reviewers, evaluating the assessments and checking manuscripts for plagiarism. They may edit the articles, which includes proofreading, typesetting, adding graphics, turning the file into standard formats such as XML and adding metadata to agreed industry standards. And they may distribute print copies and host journals online. Some subscription journals have a large staff of full-time editors, designers and computer specialists. But not every publisher ticks all the boxes on this list, puts in the same effort or hires costly professional staff for all these activities. For example, most of  PLoS ONE 's editors are working scientists, and the journal does not perform functions such as copy-editing. Some journals, including  Nature , also generate additional content for readers, such as editorials, commentary articles and journalism (including the article you are reading). \u201cWe get positive feedback about our editorial process, so in our experience, many scientists do understand and appreciate the value that this adds to their paper,\u201d says David Hoole, marketing director at Nature Publishing Group. The key question is whether the extra effort adds useful value, says Timothy Gowers, a mathematician at the University of Cambridge, UK, who last year led a revolt against Elsevier (see  Nature   http://doi.org/kwd ; 2012 ). Would scientists' appreciation for subscription journals hold up if costs were paid for by the authors, rather than spread among subscribers? \u201cIf you see it from the perspective of the publisher, you may feel quite hurt,\u201d says Gowers. \u201cYou may feel that a lot of work you put in is not really appreciated by scientists. The real question is whether that work is needed, and that's much less obvious.\u201d Many researchers in fields such as mathematics, high-energy physics and computer science do not think it is. They post pre- and post-reviewed versions of their work on servers such as arXiv \u2014 an operation that costs some $800,000 a year to keep going, or about $10 per article. Under a scheme of free open-access 'Episciences' journals proposed by some mathematicians this January, researchers would organize their own system of community peer review and host research on arXiv, making it open for all at minimal cost (see  Nature   http://doi.org/kwg ; 2013 ). These approaches suit communities that have a culture of sharing preprints, and that either produce theoretical work or see high scrutiny of their experimental work \u2014 so it is effectively peer reviewed before it even gets submitted to a publisher. But they find less support elsewhere \u2014 in the highly competitive biomedical fields, for instance, researchers tend not to publish preprints for fear of being scooped and they place more value on formal (journal-based) peer review. \u201cIf we have learned anything in the open-access movement, it's that not all scientific communities are created the same: one size doesn't fit all,\u201d says Joseph. \n               The value of rejection \n             Tied into the varying costs of journals is the number of articles that they reject.  PLoS ONE  (which charges authors $1,350) publishes 70% of submitted articles, whereas  Physical Review Letters  (a hybrid journal that has an optional open-access charge of $2,700) publishes fewer than 35%;  Nature  published just 8% in 2011. The connection between price and selectivity reflects the fact that journals have functions that go beyond just publishing articles, points out John Houghton, an economist at Victoria University in Melbourne, Australia. By rejecting papers at the peer-review stage on grounds other than scientific validity, and so guiding the papers into the most appropriate journals, publishers filter the literature and provide signals of prestige to guide readers' attention. Such guidance is essential for researchers struggling to identify which of the millions of articles published each year are worth looking at, publishers argue \u2014 and the cost includes this service. A more-expensive, more-selective journal should, in principle, generate greater prestige and impact. Yet in the open-access world, the higher-charging journals don't reliably command the greatest citation-based influence, argues Jevin West, a biologist at the University of Washington in Seattle. Earlier this year, West released a free tool that researchers can use to evaluate the cost-effectiveness of open-access journals (see  Nature   http://doi.org/kwh ; 2013 ). And to Eisen, the idea that research is filtered into branded journals before it is published is not a feature but a bug: a wasteful hangover from the days of print. Rather than guiding articles into journal 'buckets', he suggests, they could be filtered after publication using metrics such as downloads and citations, which focus not on the antiquated journal, but on the article itself (see page 437). Alicia Wise, from Elsevier, doubts that this could replace the current system: \u201cI don't think it's appropriate to say that filtering and selection should only be done by the research community after publication,\u201d she says. She argues that the brands, and accompanying filters, that publishers create by selective peer review add real value, and would be missed if removed entirely. PLoS ONE  supporters have a ready answer: start by making any core text that passes peer review for scientific validity alone open to everyone; if scientists do miss the guidance of selective peer review, then they can use recommendation tools and filters (perhaps even commercial ones) to organize the literature \u2014 but at least the costs will not be baked into pre-publication charges. These arguments, Houghton says, are a reminder that publishers, researchers, libraries and funders exist in a complex, interdependent system. His analyses, and those by Cambridge Economic Policy Associates, suggest that converting the entire publishing system to open access would be worthwhile even if per-article-costs remained the same \u2014 simply because of the time that researchers would save when trying to access or read papers that were no longer lodged behind paywalls. \n               The path to open access \n             But a total conversion will be slow in coming, because scientists still have every economic incentive to submit their papers to high-prestige subscription journals. The subscriptions tend to be paid for by campus libraries, and few individual scientists see the costs directly. From their perspective, publication is effectively free. Of course, many researchers have been swayed by the ethical argument, made so forcefully by open-access advocates, that publicly funded research should be freely available to everyone. Another important reason that open-access journals have made headway is that libraries are maxed out on their budgets, says Mark McCabe, an economist at the University of Michigan in Ann Arbor. With no more library cash available to spend on subscriptions, adopting an open-access model was the only way for fresh journals to break into the market. New funding-agency mandates for immediate open access could speed the progress of open-access journals. But even then the economics of the industry remain unclear. Low article charges are likely to rise if more-selective journals choose to go open access. And some publishers warn that shifting the entire system to open access would also increase prices because journals would need to claim all their revenue from upfront payments, rather than from a variety of sources, such as secondary rights. \u201cI've worked with medical journals where the revenue stream from secondary rights varies from less than 1% to as much as one-third of total revenue,\u201d says David Crotty of Oxford University Press, UK. Some publishers may manage to lock in higher prices for their premium products, or, following the successful example of PLoS, large open-access publishers may try to cross-subsidize high-prestige, selective, costly journals with cheaper, high-throughput journals. Publishers who put out a small number of articles in a few mid-range journals may be in trouble under the open-access model if they cannot quickly reduce costs. \u201cIn the end,\u201d says Wim van der Stelt, executive vice president at Springer in Doetinchem, the Netherlands, \u201cthe price is set by what the market wants to pay for it.\u201d In theory, an open-access market could drive down costs by encouraging authors to weigh the value of what they get against what they pay. But that might not happen: instead, funders and libraries may end up paying the costs of open-access publication in place of scientists \u2014 to simplify the accounting and maintain freedom of choice for academics. Joseph says that some institutional libraries are already joining publisher membership schemes in which they buy a number of free or discounted articles for their researchers. She worries that such behaviour might reduce the author's awareness of the price being paid to publish \u2014 and thus the incentive to bring costs down. And although many see a switch to open access as inevitable, the transition will be gradual. In the United Kingdom, portions of grant money are being spent on open access, but libraries still need to pay for research published in subscription journals. In the meantime, some scientists are urging their colleagues to deposit any manuscripts they publish in subscription journals in free online repositories. More than 60% of journals already allow authors to self-archive content that has been peer-reviewed and accepted for publication, says Stevan Harnad, a veteran open-access campaigner and cognitive scientist at the University of Quebec in Montreal, Canada. Most of the others ask authors to wait for a time (say, a year), before they archive their papers. However, the vast majority of authors don't self-archive their manuscripts unless prompted by university or funder mandates. As that lack of enthusiasm demonstrates, the fundamental force driving the speed of the move towards full open access is what researchers \u2014 and research funders \u2014 want. Eisen says that although PLoS has become a success story \u2014 publishing 26,000 papers last year \u2014 it didn't catalyse the industry to change in the way that he had hoped. \u201cI didn't expect publishers to give up their profits, but my frustration lies primarily with leaders of the science community for not recognizing that open access is a perfectly viable way to do publishing,\u201d he says. \n                 Tweet \n                 Follow @NatureNews \n               \n                     The future of publishing: A new page 2013-Mar-27 \n                   \n                     Gold on hold 2013-Feb-26 \n                   \n                     Price doesn't always buy prestige in open access 2013-Jan-22 \n                   \n                     Britain aims for broad open access 2012-Jun-19 \n                   \n                     Open access comes of age 2011-Jun-21 \n                   \n                     Nature special: The future of publishing \n                   \n                     Cost-effectiveness for open-access journals \n                   \n                     A study of open-access journals using article-processing charges \n                   \n                     Economic implications of alternative scholarly publishing models: exploring the costs and benefits \n                   Reprints and Permissions"},
{"file_id": "496020a", "url": "https://www.nature.com/articles/496020a", "year": 2013, "authors": [{"name": "Zeeya Merali"}], "parsed_as_year": "2006_or_before", "body": "Will an astronaut who falls into a black hole be crushed or burned to a crisp? In March 2012, Joseph Polchinski began to contemplate suicide \u2014 at least in mathematical form. A string theorist at the Kavli Institute for Theoretical Physics in Santa Barbara, California, Polchinski was pondering what would happen to an astronaut who dived into a black hole. Obviously, he would die. But how? According to the then-accepted account, he wouldn\u2019t feel anything special at first, even when his fall took him through the black hole\u2019s event horizon: the invisible boundary beyond which nothing can escape. But eventually \u2014 after hours, days or even weeks if the black hole was big enough \u2014 he would begin to notice that gravity was tugging at his feet more strongly than at his head. As his plunge carried him inexorably downwards, the difference in forces would quickly increase and rip him apart, before finally crushing his remnants into the black hole\u2019s infinitely dense core. But Polchinski\u2019s calculations, carried out with two of his students \u2014 Ahmed Almheiri and James Sully \u2014 and fellow string theorist Donald Marolf at the University of California, Santa Barbara (UCSB), were telling a different story 1 . In their account, quantum effects would turn the event horizon into a seething maelstrom of particles. Anyone who fell into it would hit a wall of fire and be burned to a crisp in an instant. Zeeya Merali talks about what would happen if she fell into a black hole The team\u2019s verdict, published in July 2012, shocked the physics community. Such firewalls would violate a foundational tenet of physics that was first articulated almost a century ago by Albert Einstein, who used it as the basis of general relativity, his theory of gravity. Known as the equivalence principle, it states in part that an observer falling in a gravitational field \u2014 even the powerful one inside a black hole \u2014 will see exactly the same phenomena as an observer floating in empty space. Without this principle, Einstein\u2019s framework crumbles. Well aware of the implications of their claim, Polchinski and his co-authors offered an alternative plot ending in which a firewall does not form. But this solution came with a huge price. Physicists would have to sacrifice the other great pillar of their science: quantum mechanics, the theory governing the interactions between subatomic particles. The result has been a flurry of research papers about firewalls, all struggling to resolve the impasse, none succeeding to everyone\u2019s satisfaction. Steve Giddings, a quantum physicist at the UCSB, describes the situation as \u201ca crisis in the foundations of physics that may need a revolution to resolve\u201d. With that thought in mind, black-hole experts came together last month at CERN, Europe\u2019s particle-physics laboratory near Geneva, Switzerland, to grapple with the issue face to face. They hoped to reveal the path towards a unified theory of \u2018quantum gravity\u2019 that brings all the fundamental forces of nature under one umbrella \u2014 a prize that has eluded physicists for decades. The firewall idea \u201cshakes the foundations of what most of us believed about black holes\u201d, said Raphael Bousso, a string theorist at the University of California, Berkeley, as he opened his talk at the meeting. \u201cIt essentially pits quantum mechanics against general relativity, without giving us any clues as to which direction to go next.\u201d \n               Fiery origins \n             The roots of the firewall crisis go back to 1974, when physicist Stephen Hawking at the University of Cambridge, UK, showed that quantum effects cause black holes to run a temperature 2 . Left in isolation, the holes will slowly spew out thermal radiation \u2014 photons and other particles \u2014 and gradually lose mass until they evaporate away entirely (see \u2018The information paradox\u2019). These particles aren\u2019t the firewall, however; the subtleties of relativity guarantee that an astronaut falling through the event horizon will not notice this radiation. But Hawking\u2019s result was still startling \u2014 not least because the equations of general relativity say that black holes can only swallow mass and grow, not evaporate. Hawking\u2019s argument basically comes down to the observation that in the quantum realm, \u2018empty\u2019 space isn\u2019t empty. Down at this sub-sub-microscopic level, it is in constant turmoil, with pairs of particles and their corresponding antiparticles continually popping into existence before rapidly recombining and vanishing. Only in very delicate laboratory experiments does this submicroscopic frenzy have any observable consequences. But when a particle\u2013antiparticle pair appears just outside a black hole\u2019s event horizon, Hawking realized, one member could fall in before the two recombined, leaving the surviving partner to fly outwards as radiation. The doomed particle would balance the positive energy of the outgoing particle by carrying negative energy inwards \u2014 something allowed by quantum rules. That negative energy would then get subtracted from the black hole\u2019s mass, causing the hole to shrink. Hawking\u2019s original analysis has since been refined and extended by many researchers, and his conclusion is now accepted almost universally. But with it came the disturbing realization that black-hole radiation leads to a paradox that challenges quantum theory. Quantum mechanics says that information cannot be destroyed. In principle, it should be possible to recover everything there is to know about the objects that fell in a black hole by measuring the quantum state of the radiation coming out. But Hawking showed that it was not that simple: the radiation coming out is random. Toss in a kilogram of rock or a kilogram of computer chips and the result will be the same. Watch the black hole even until it dies, and there would still be no way to tell how it was formed or what fell in it. This problem, dubbed the black-hole information paradox, divided physicists into two camps. Some, like Hawking, argued that the information truly vanishes when the black hole dies. If that contradicted quantum laws, then better laws needed to be found. Others, like John Preskill, a quantum physicist at the California Institute of Technology in Pasadena, stuck by quantum mechanics. \u201cFor a time, I did seriously try to build an alternative theory that included information loss,\u201d he says. \u201cBut I couldn\u2019t find one that made any sense \u2014 nobody could.\u201d The stalemate continued for the next two decades, finding its most famous expression in 1997, when Preskill publicly bet Hawking that information was not being lost, with the winner to receive an encyclopaedia of his choice. But that same year, the deadlock was broken by a discovery made by Juan Maldacena, a physicist then at Harvard University in Cambridge. Maldacena\u2019s insight built on an earlier proposal that any three-dimensional (3D) region of our Universe can be described by information encoded on its two-dimensional (2D) boundary 3 , 4 , 5 , in much the same way that laser light can encode a 3D scene on a 2D hologram. \u201cWe used the word \u2018hologram\u2019 as a metaphor,\u201d says Leonard Susskind, a string theorist at Stanford University in California, and one of those who came up with the proposal 4 . \u201cBut after doing more mathematics, it seemed to make literal sense that the Universe is a projection of information on the boundary.\u201d What Maldacena came up with was a concrete mathematical formulation 6  of the hologram idea that made use of ideas from superstring theory, which posits that elementary particles are composed of tiny vibrating loops of energy. His model envisages a 3D universe containing strings and black holes that are governed only by gravity, bounded by a 2D surface on which elementary particles and fields obey ordinary quantum laws without gravity. Hypothetical residents of the 3D space would never see this boundary because it is infinitely far away. But that wouldn\u2019t matter: anything happening in the 3D universe could be described equally well by equations in the 2D universe, and vice versa. \u201cI found that there\u2019s a mathematical dictionary that allows you to go back and forth between the languages of these two worlds,\u201d Maldacena explains. This meant that even 3D black-hole evaporation could be described in the 2D world, where there is no gravity, where quantum laws reign supreme and where information can never be lost. And if information is preserved there, then it must also be preserved in the 3D world. Somehow, information must be escaping from the black holes. \n               One for all \n             A few years later, Marolf showed that every model of quantum gravity will obey the same rules, whether or not it is built from string theory 7 . \u201cIt was a combination of Maldacena and Marolf\u2019s work that turned me around,\u201d explains a long-term proponent of information loss, Ted Jacobson, a quantum physicist at the University of Maryland in College Park. In 2004, Hawking publicly admitted that he had been wrong, and gave Preskill a baseball encyclopaedia to make good on their bet. Such was the strength of Maldacena\u2019s discovery that most physicists believed that the paradox had been settled \u2014 even though nobody had yet explained how Hawking radiation smuggles information out of the black hole. \u201cI guess we just all assumed there would be a straightforward answer,\u201d says Polchinski. There wasn\u2019t. When Polchinski and his team set themselves the task of clearing up that loose end in early 2012, they soon stumbled on yet another paradox \u2014 the one that eventually led them to the fatal firewall. Hawking had shown that the quantum state of any one particle escaping from the black hole is random, so the particle cannot be carrying any useful information. But in the mid-1990s, Susskind and others realized that information could be encoded in the quantum state of the radiation as a whole if the particles could somehow have their states \u2018entangled\u2019 \u2014 intertwined in such a way that measurements carried out on one will immediately influence its partner, no matter how far apart they are. But how could that be, wondered the Polchinski\u2019s team? For a particle to be emitted at all, it has to be entangled with the twin that is sacrificed to the black hole. And if Susskind and others were right, it also had to be entangled with all the Hawking radiation emitted before it. Yet a rigorous result of quantum mechanics dubbed \u2018the monogamy of entanglement\u2019 says that one quantum system cannot be fully entangled with two independent systems at once. To escape this paradox, Polchinski and his co-workers realized, one of the entanglement relationships had to be severed. Reluctant to abandon the one required to encode information in the Hawking radiation, they decided to snip the link binding an escaping Hawking particle to its infalling twin. But there was a cost. \u201cIt\u2019s a violent process, like breaking the bonds of a molecule, and it releases energy,\u201d says Polchinski. The energy generated by severing lots of twins would be enormous. \u201cThe event horizon would literally be a ring of fire that burns anyone falling through,\u201d he says. And that, in turn, violates the equivalence principle and its assertion that free-fall should feel the same as floating in empty space \u2014 impossible when the former ends in incineration. So they posted a paper on the preprint server, arXiv, presenting physicists with a stark choice: either accept that firewalls exist and that general relativity breaks down, or accept that information is lost in black holes and quantum mechanics is wrong 1 . \u201cFor us, firewalls seem like the least crazy option, given that choice,\u201d says Marolf. The paper rocked the physics community. \u201cIt was outrageous to claim that giving up Einstein\u2019s equivalence principle is the best option,\u201d says Jacobson. Bousso agrees, adding: \u201cA firewall simply can\u2019t appear in empty space, any more than a brick wall can suddenly appear in an empty field and smack you in the face.\u201d If Einstein\u2019s theory doesn\u2019t apply at the event horizon, cosmologists would have to question whether it fully applies anywhere. Polchinski admits that he thought they could have made a silly mistake. So he turned to Susskind, one of the fathers of holography, to find it. \u201cMy first reaction was that they were wrong,\u201d says Susskind. He posted a paper stating as much 8 , before quickly retracting it, after further thought. \u201cMy second reaction was that they were right, my third was that they were wrong again, my fourth was that they were right,\u201d he laughs. \u201cIt\u2019s earned me the nickname, \u2018the yo-yo,\u2019 but my reaction is pretty much the same as most physicists\u2019.\u201d Since then, more than 40 papers have been posted on the topic in arXiv, but as yet, nobody has found a flaw in the team\u2019s logic. \u201cIt\u2019s a really beautiful argument proving that there\u2019s something inconsistent in our thinking about black holes,\u201d says Don Page, a collaborator of Hawking\u2019s during the 1970s who is now at the University of Alberta in Edmonton, Canada. A number of inventive solutions have been offered, however. \n               Real-world implications \n             One of the most promising resolutions, according to Susskind, has come from Daniel Harlow, a quantum physicist at Princeton University in New Jersey, and Patrick Hayden, a computer scientist at McGill University in Montreal, Canada. They considered whether an astronaut could ever detect the paradox with a real-world measurement. To do so, he or she would first have to decode a significant portion of the outgoing Hawking radiation, then dive into the black hole to examine the infalling particles. The pair\u2019s calculations show that the radiation is so tough to decode that the black hole would evaporate before the astronaut was ready to jump in 9 . \u201cThere\u2019s no fundamental law preventing someone from measuring the paradox,\u201d says Harlow. \u201cBut in practice, it\u2019s impossible.\u201d Giddings, however, argues that the firewall paradox requires a radical solution. He has calculated that if the entanglement between the outgoing Hawking radiation and its infalling twin is not broken until the escaping particle has travelled a short distance away from the event horizon, then the energy released would be much less ferocious, and no firewall would be generated 10 . This protects the equivalence principle, but requires some quantum laws to be modified. At the CERN meeting, participants were tantalized by the possibility that Giddings\u2019 model could be tested: it predicts that when two black holes merge, they may produce distinctive ripples in space-time that can be detected by gravitational-wave observatories on Earth.\u00a0 There is another option that would save the equivalence principle, but it is so controversial that few dare to champion it: maybe Hawking was right all those years ago and information is lost in black holes. Ironically, it is Preskill, the man who bet against Hawking\u2019s claim, who raised this alternative, at a workshop on firewalls at Stanford at the end of last year. \u201cIt\u2019s surprising that people are not seriously thinking about this possibility because it doesn\u2019t seem any crazier than firewalls,\u201d he says \u2014 although he adds that his instinct is still that information survives. The reluctance to revisit Hawking\u2019s old argument is a sign of the immense respect that physicists have for Maldacena\u2019s dictionary relating gravity to quantum theory, which seemingly proved that information cannot be lost. \u201cThis is the deepest ever insight into gravity because it links it to quantum fields,\u201d says Polchinski, who compares Maldacena\u2019s result \u2014 which has now accumulated close to 9,000 citations \u2014 to the nineteenth-century discovery that a single theory connects light, electricity and magnetism. \u201cIf the firewall argument had been made in the early 1990s, I think it would have been a powerful argument for information loss,\u201d says Bousso. \u201cBut now nobody wants to entertain the possibility that Maldacena is wrong.\u201d Maldacena is flattered that most physicists would back him in a straight-out fight against Einstein, although he believes it won\u2019t come to that. \u201cTo completely understand the firewall paradox, we may need to flesh out that dictionary,\u201d he says, \u201cbut we won\u2019t need to throw it out.\u201d The only consensus so far is that this problem will not go away any time soon. During his talk, Polchinski fielded all proposed strategies for mitigating the firewall, carefully highlighting what he sees as their weaknesses. \u201cI\u2019m sorry that no one has gotten rid of the firewall,\u201d he concludes. \u201cBut please keep trying.\u201d See Editorial  page 5 \n                     Against the law 2013-Apr-03 \n                   \n                     Astrophysics: Going supernova 2013-Feb-06 \n                   \n                     Astrophysics: Two black holes found in a star cluster 2012-Oct-03 \n                   \n                     Collaborative physics: String theory finds a bench mate 2011-Oct-19 \n                   \n                     Can information get back out of black holes? 2008-May-16 \n                   \n                     Hawking changes his mind about black holes 2004-Jul-15 \n                   \n                     Blogpost: NuSTAR spies black holes in galactic web \n                   \n                     Blogpost: Record-breaking black holes fill a cosmic gap \n                   \n                     Joseph Polchinski \n                   \n                     Don Marolf \n                   \n                     Leonard Susskind \n                   \n                     Juan Maldacena \n                   \n                     Daniel Harlow \n                   Reprints and Permissions"},
{"file_id": "495433a", "url": "https://www.nature.com/articles/495433a", "year": 2013, "authors": [{"name": "Declan Butler"}], "parsed_as_year": "2006_or_before", "body": "The explosion in open-access publishing has fuelled the rise of questionable operators. Spam e-mails changed the life of Jeffrey Beall. It was 2008, and Beall, an academic librarian and a researcher at the University of Colorado in Denver, started to notice an increasing flow of messages from new journals soliciting him to submit articles or join their editorial boards. \u201cI immediately became fascinated because most of the e-mails contained numerous grammatical errors,\u201d Beall says. He started browsing the journals' websites, and was soon convinced that many of the journals and their publishers were not quite what they claimed. The names often sounded grand \u2014 adjectives such as 'world', 'global' and 'international' were common \u2014 but some sites looked amateurish or gave little information about the organization behind them. Since then, Beall has become a relentless watchdog for what he describes as \u201cpotential, possible or probable predatory scholarly open-access publishers\u201d, listing and scrutinizing them on his blog, Scholarly Open Access. Open-access publishers often collect fees from authors to pay for peer review, editing and website maintenance. Beall asserts that the goal of predatory open-access publishers is to exploit this model by charging the fee without providing all the expected publishing services. These publishers, Beall says, typically display \u201can intention to deceive authors and readers, and a lack of transparency in their operations and processes\u201d. Beall says that he regularly receives e-mails from researchers unhappy about their experiences with some open-access journals. Some say that they thought their papers had been poorly peer reviewed or not peer reviewed at all, or that they found themselves listed as members of editorial boards they had not agreed to serve on. Others feel they were not informed clearly, when submitting papers to publishers, that publication would entail a fee \u2014 only to face an invoice after the paper had been accepted. According to Beall, whose list now includes more than 300 publishers, collectively issuing thousands of journals, the problem is getting worse. \u201c2012 was basically the year of the predatory publisher; that was when they really exploded,\u201d says Beall. He estimates that such outfits publish 5\u201310% of all open-access articles. Beall's list and blog are widely read by librarians, researchers and open-access advocates, many of whom applaud his efforts to reveal shady publishing practices \u2014 ones that, they worry, could taint the entire open-access movement. \u201cI think Beall has taken a brave and principled stand in publishing this, at no small risk to himself,\u201d says Douglas Sipp, an expert in science policy and ethics at the RIKEN Center for Developmental Biology in Kobe, Japan, who studies the open-access movement in Asia. Beall says that he has been the target of vicious online comments, and last December he was the subject of an online campaign to create the false impression that he was extorting fees from publishers to re-evaluate their status on his list. The Canadian Center of Science and Education, a company based in Toronto that publishes many open-access journals and is on Beall's list, is now threatening to sue him for alleged defamation and libel. But even some experts in scholarly publishing are uncomfortable with Beall's blacklist, arguing that it runs the risk of lumping publishers that are questionable together with those that could be bona fide start-ups simply lacking experience in the publishing industry. Matthew Cockerill, managing director of BioMed Central, an open-access publisher based in London, says that Beall's list \u201cidentifies publishers which Beall has concerns about. These concerns may or may not be justified.\u201d \n               Rising tide \n             As a research librarian, Beall has been in prime position to watch the dramatic changes that have taken place in scientific publishing since the rise of the open-access movement about a decade ago. In the conventional subscription-based model, journals bring in revenue largely through selling print or web subscriptions and keeping most online content locked behind a paywall. But in the most popular model of open access, publishers charge an upfront 'author fee' to cover costs \u2014 and to turn a profit, in the case of commercial publishers \u2014 then make the papers freely available online, immediately on publication. The open-access movement has spawned many successful, well-respected operations.  PLOS ONE , for example, which charges a fee of US$1,350 for authors in middle- and high-income countries, has seen the number of articles it publishes leap from 138 in 2006 to 23,464 last year, making it the world's largest scientific journal. The movement has also garnered growing political support. In the past year, the UK and US governments, as well as the European Commission, have thrown their weight behind some form of open-access publishing. And scarcely a week goes by without the appearance of new author-pays, open-access publishers, launching single journals or large fleets of them. Many new open-access publishers are trustworthy. But not all. Anyone with a spare afternoon and a little computing savvy can launch an impressive-looking journal website and e-mail invitations to scientists to join editorial boards or submit papers for a fee. The challenge for researchers, and for Beall, is to work out when those websites or e-mail blasts signal a credible publisher and when they come from operations that can range from the outright criminal to the merely amateurish. In one e-mail that Beall received and shared with  Nature , a dental researcher wrote that she had submitted a paper to an open-access journal after she \u201cwas won over by the logos of affiliated databases on the home page and seemingly prestigious editorial board\u201d. But the researcher, who prefers to remain anonymous, says that she became concerned about the peer-review process when the article was accepted within days and she was not sent any reviewers' comments. She says that last week \u2014 several months after her original submission \u2014 she was sent page proofs that match the submitted manuscript, and that she still has not seen reviewers' comments. Complaints like this prompted Beall to coin the term predatory publisher and to compile his first list of them, which he published in 2010. He now estimates that his zeal for investigating publishers takes up 20\u201325 hours a week, squeezed in around his day job. Beall says that he is motivated partly by his sense of duty, as an academic librarian, to evaluate online resources and to help patrons to \u201crecognize scholarly publishing scams and avoid them\u201d, and partly by the \u201cprivate and very positive feedback\u201d he receives from researchers and librarians. But Beall's critics assert that he often relies heavily on analysis of publishers' websites rather than detailed discussions with publishers, and that this might lead to incorrect or premature conclusions. \u201cOne of the major weaknesses of Jeffrey Beall's methodology is that he does not typically engage in direct communication with the journals that he has classified as predatory,\u201d says Paul Peters, chief strategy officer at Hindawi Publishing Corporation, based in Cairo, and president of the Open Access Scholarly Publishers Association (OASPA), based in The Hague, the Netherlands. A set of Hindawi's journals appeared on a version of Beall's list because he had concerns about their editorial process, but has since been removed. \u201cI reanalysed it and determined that it did not belong on the list,\u201d he says. \u201cIt was always a borderline case.\u201d Another concern, say Beall's critics, is that he risks throwing undue suspicion on start-up publishers. \u201cAlthough rapid launches of many journals may well correlate negatively with journal quality, it is certainly not enough in and of itself to warrant describing a publisher as predatory,\u201d says Cockerill, who is also a board member of the OASPA. \u201cSimilarly, some publishers identified on Beall's list are guilty of poor copy-editing and user-interface design on their websites,\u201d he says. \u201cAgain, this is, at best, circumstantial evidence for problems with the scholarly standard of the material they publish.\u201d OMICS Group, based in Hyderabad, India, is on Beall's list. One researcher complained in an e-mail to Beall that she had submitted a paper to an OMICS journal after receiving an e-mail solicitation \u2014 but learned that she had to pay a fee to publish it only from a message sent by the journal after the paper had been accepted. \u201cTo my horror, I opened the file to find an invoice for $2,700!\u201d she wrote. \u201cThis fee was not mentioned anywhere obvious at the time I submitted my manuscript.\u201d ( Nature  was unable to contact this researcher.) Beall says that OMICS journals do not show their author fees prominently enough on their journal websites or in e-mails that they send to authors to solicit manuscript submissions. Srinubabu Gedela, director of OMICS Group, says that article-handling fees are displayed clearly on the 'Instructions for Authors' web page for each OMICS journal. Gedela adds that he would assume researchers would be aware that such open-access journals charge author fees. He says that OMICS Group is \u201cnot predatory\u201d and that its staff and editors are acting in \u201cgood faith and confidence\u201d to promote open-access publishing. Publishers in developing countries and emerging economies are at particular risk of being unfairly tarred by Beall's brush, critics say. Many open-access publishers are springing up in India and China, for example, where swelling researcher ranks are creating large publishing markets. Pressure to publish is often intense in developing countries, and vanity presses could attract unscrupulous researchers keen to pad out their CVs. But respectable domestic publishers could have an important role by helping to address local science issues, such as those related to crops, diseases or environmental problems. \u201cIt is important that criteria for evaluating publishers and journals do not discriminate [against] publishers and journals from other parts of the world,\u201d says Lars Bj\u00f8rnshauge, managing director of the Directory of Open Access Journals (DOAJ), based in Copenhagen, which lists open-access journals that have been reviewed for quality. New publishing outfits may legitimately use aggressive marketing tactics to recruit authors, and they may have yet to polish their websites, editorial boards and peer-review procedures. \u201cSome are embarrassingly, toe-cringingly amateurish, but predatory is a term that, I think, implies intent to deceive,\u201d says Jan Velterop, a former science publisher at Nature Publishing Group and elsewhere who is now working with several start-ups to promote innovative ways to publish science data. Damage could be done if \u201ca damning verdict is given to otherwise honest, though perhaps amateurish, attempts to enter the publishing market\u201d, he says. \n               Questioning the verdict \n             For researchers involved in journals whose publishers have appeared on Beall's blacklist, the verdict can be unsettling. David Warhurst, a malaria researcher at the London School of Hygiene and Tropical Medicine, is the unpaid editor-in-chief of  Malaria Chemotherapy, Control & Elimination , an open-access, peer-reviewed journal owned by Ashdin Publishing, a company that has offices in Cairo and Honnelles, Belgium, and that is on Beall's list. Warhurst says that he was initially reluctant to take up the invitation he received two years ago to become the journal's editor-in-chief, because he found that the publishers did not have a journal registered with the publication index PubMed. But \u201ccertainly I do not believe that this is a toxic journal\u201d, he says. The journal is still in its launch phase, and refereeing of the papers so far has entailed extensive corrections but has not been \u201cexceptional\u201d compared with his experiences at other journals, Warhurst says. The papers \u201chad new findings or findings useful in their geographical context, but needed help with presentation \u2014 mainly language and analysis.\u201d Ashry Aly, director of Ashdin Publishing, says that the company is not a predatory publisher. Beall accepts that the publishers on his list fall along a spectrum, with some being worse than others, but he strongly defends his methods. He denies that he doesn't make sufficient efforts to contact publishers, arguing that many of them \u2014 who often can be contacted only through a web form \u2014 never respond. When it comes to publishers in developing countries, he says: \u201cLook, when I discover a new publisher from Nigeria, I admit I am more suspicious than I would be were the publisher from, for example, the Vatican.\u201d But, he says, \u201cI try to be as fair and honest as I can be when I am making judgements\u201d. Beall says that he usually gives blog posts a 'cooling-off' period between writing and publishing them. Last month, he announced an appeals process in which a three-person advisory board will conduct a blinded review and then recommend whether the publisher or journal should stay on the list. And to improve transparency, Beall last August posted a set of criteria that he says he uses to assess publishers, including an evaluation of their content and practices based on standards established by organizations such as the OASPA. Rick Anderson, a library dean at the University of Utah in Salt Lake City, says that Beall's criteria \u201cmake a lot of sense\u201d and also allow for distinctions between truly exploitative publishers and those that are just sloppy. Bj\u00f8rnshauge feels that the entire problem needs to be kept in perspective. He estimates that questionable publishing probably accounts for fewer than 1% of all author-pays, open-access papers \u2014 a proportion far lower than Beall's estimate of 5\u201310%. Instead of relying on blacklists, Bj\u00f8rnshauge argues, open-access associations such as the DOAJ and the OASPA should adopt more responsibility for policing publishers. He says that they should lay out a set of criteria that publishers and journals must comply with to win a place on a 'white list' indicating that they are trustworthy. The DOAJ, he says, is now compiling a list of new, more stringent criteria. To help clean up practices, he adds, research funders should pay author fees only to such white-listed publishers. Meanwhile, he urges researchers to be as cautious when shopping online for publishers as when shopping for anything else (see \u2018Buyer beware\u2019). \u201cExamine the company you are about to deal with,\u201d he says. \n               boxed-text \n             Beall says that he supports such efforts. \u201cIf someone can figure out a better way, that would be great, and I will defer to them,\u201d he says. \u201cI wish them success.\u201d But he is sceptical about whether a white list would be able to keep up with the surge of new publishers, and believes that his blacklist provides more immediate warning. That, however, depends on whether he can keep up. \u201cI did not expect and was unprepared for the exponential growth of questionable publishers that has occurred in the past two years,\u201d he says. \n                     US science to be open to all 2013-Feb-26 \n                   \n                     Price doesn't always buy prestige in open access 2013-Jan-22 \n                   \n                     Predatory publishers are corrupting open access 2012-Sep-12 \n                   \n                     Europe joins UK open-access bid 2012-Jul-17 \n                   \n                     Open access comes of age 2011-Jun-21 \n                   \n                     Two new journals copy the old 2010-Jan-13 \n                   \n                     Editor to quit over hoax open-access paper 2009-Jun-17 \n                   \n                     Nature special: The future of publishing \n                   \n                     Jeffrey Beall \n                   \n                     Open Access Scholarly Publishers Association \n                   \n                     Directory of Open Access Journals \n                   Reprints and Permissions"},
{"file_id": "496152a", "url": "https://www.nature.com/articles/496152a", "year": 2013, "authors": [{"name": "Stephen S. Hall"}], "parsed_as_year": "2006_or_before", "body": "A mutation that gives people rock-bottom cholesterol levels has led geneticists to what could be the next blockbuster heart drug. When Sharlayne Tracy showed up at the clinical suite in the University of Texas (UT) Southwestern Medical Center in Dallas last January, the bandage wrapped around her left wrist was the only sign of anything medically amiss. The bandage covered a minor injury from a cheerleading practice led by Tracy, a 40-year-old African American who is an aerobics instructor, a mother of two and a college student pursuing a degree in business. \u201cI feel like I'm healthy as a horse,\u201d she said. Indeed, Tracy's well-being has been inspiring to doctors, geneticists and now pharmaceutical companies precisely because she is so normal. Using every tool in the modern diagnostic arsenal \u2014 from brain scans and kidney sonograms to 24-hour blood-pressure monitors and cognitive tests \u2014 researchers at the Texas medical centre have diagnostically sliced and diced Tracy to make sure that the two highly unusual genetic mutations she has carried for her entire life have produced nothing more startling than an incredibly low level of cholesterol in her blood. At a time when the target for low-density lipoprotein (LDL) cholesterol, more commonly called 'bad cholesterol', in Americans' blood is less than 100 milligrams per decilitre (a level many people fail to achieve), Tracy's level is just 14. A compact woman with wide-eyed energy, Tracy (not her real name) is one of a handful of African Americans whose genetics have enabled scientists to uncover one of the most promising compounds for controlling cholesterol since the first statin drug was approved by the US Food and Drug Administration in 1987. Seven years ago, researchers Helen Hobbs and Jonathan Cohen at UT-Southwestern reported 1  that Tracy had inherited two mutations, one from her father and the other from her mother, in a gene called  PCSK9 , effectively eliminating a protein in the blood that has a fundamental role in controlling the levels of LDL cholesterol. African Americans with similar mutations have a nearly 90% reduced risk of heart disease. \u201cShe's our girl, our main girl,\u201d says Barbara Gilbert, a nurse who has drawn some 8,000 blood samples as part of Cohen and Hobbs' project to find genes important to cholesterol metabolism. Of all the intriguing DNA sequences spat out by the Human Genome Project and its ancillary studies, perhaps none is a more promising candidate to have a rapid, large-scale impact on human health than  PCSK9 . Elias Zerhouni, former director of the US National Institutes of Health (NIH) in Bethesda, Maryland, calls  PCSK9  an \u201ciconic example\u201d of translational medicine in the genomics era. Preliminary clinical trials have already shown that drugs that inhibit the PCSK9 protein \u2014 used with or without statins \u2014 produce dramatic reductions in LDL cholesterol (more than 70% in some patients). Half-a-dozen pharmaceutical companies \u2014 all aiming for a share of the global market for cholesterol-reducing drugs that could reach US$25 billion in the next five years according to some estimates \u2014 are racing to the market with drugs that mimic the effect of Tracy's paired mutations. Stephen Hall talks about Sharlayne\u2019s unusual condition and whether similar cases might lead to a new line of drugs. Zerhouni, now an in-house champion of this class of drug as an executive at drug firm Sanofi, headquartered in Paris, calls the discovery and development of  PCSK9  a \u201cbeautiful story\u201d in which researchers combined detailed physical information about patients with shrewd genetics to identify a medically important gene that has made \u201csuper-fast\u201d progress to the clinic. \u201cOnce you have it, boy, everything just lines up,\u201d he says. And although the end of the  PCSK9  story has yet to be written \u2014 the advanced clinical trials now under way could still be derailed by unexpected side effects \u2014 it holds a valuable lesson for genomic research. The key discovery about  PCSK9 's medical potential was made by researchers working not only apart from the prevailing scientific strategy of genome research over the past decade, but with an almost entirely different approach. As for Tracy, who lives in the southern part of Dallas County, the implications of her special genetic status have become clear. \u201cI really didn't understand at first,\u201d she admits. \u201cBut now I'm watching ads on TV [for cholesterol-lowering drugs], and it's like, 'Wow, I don't have  that  problem'.\u201d \n               A heart problem \n             Cardiovascular disease is \u2014 and will be for the foreseeable future, according to the World Health Organization \u2014 the leading cause of death in the world, and its development is intimately linked to elevated levels of cholesterol in the blood. Since their introduction, statin drugs have been widely used to lower cholesterol levels. But Jan Breslow, a physician and geneticist at Rockefeller University in New York, points out that up to 20% of patients cannot tolerate statins' side effects, which include muscle pain and even forgetfulness. And in many others, the drugs simply don't control cholesterol levels well enough. The search for better treatments for heart disease gained fresh impetus after scientists published the draft sequence of the human genome in 2001. In an effort to identify the genetic basis of common ailments such as heart disease and diabetes, geneticists settled on a strategy based on the 'common variant hypothesis'. The idea was that a handful of disease-related versions (or variants) of genes for each disease would be common enough \u2014 at a frequency of roughly 5% or so \u2014 to be detected by powerful analyses of the whole genome. Massive surveys known as genome-wide association studies compared the genomes of thousands of people with heart disease, for example, with those of healthy controls. By 2009, however, many scientists were lamenting the fact that although the strategy had identified many common variants, each made only a small contribution to the disease. The results for cardiovascular disease have been \u201cpretty disappointing\u201d, says Daniel Steinberg, a lipoprotein expert at the University of California, San Diego. More than a decade earlier, in Texas, Hobbs and Cohen had taken the opposite tack. They had backgrounds in Mendelian, or single-gene, disorders, in which an extremely rare variant can have a big \u2014 often fatal \u2014 effect. They also knew that people with a particular Mendelian disorder didn't share a single common mutation in the affected gene, but rather had a lot of different, rare mutations. They hypothesized that in complex disorders, many different rare variants were also likely to have a big effect, whereas common variants would have relatively minor effects (otherwise natural selection would have weeded them out). \u201cJonathan and I did not see any reason why it couldn't be that rare variants cumulatively contribute to disease,\u201d Hobbs says. To find these rare variants, the pair needed to compile detailed physiological profiles, or phenotypes, of a large general population. Cohen spoke of the need to \u201cMendelize\u201d people \u2014 to compartmentalize them by physiological traits, such as extremely high or low cholesterol levels, and then look in the extreme groups for variations in candidate genes known to be related to the trait. The pair make a scientific odd couple. Hobbs, who trained as an MD, is gregarious, voluble and driven. Cohen, a soft-spoken geneticist from South Africa, has a laid-back, droll manner and a knack for quantitative thinking. In 1999, they set out to design a population-based study that focused on physical measurements related to heart disease. Organized with Ronald Victor, an expert on high blood pressure also at UT Southwestern, and funded by the Donald W. Reynolds Foundation in Las Vegas, Nevada, the Dallas Heart Study assembled exquisitely detailed physiological profiles on a population of roughly 3,500 Dallas residents 2 . Crucially, around half of the participants in the study were African Americans, because the researchers wanted to probe racial differences in heart disease and high blood pressure. The team measured blood pressure, body mass index, heart physiology and body-fat distribution, along with a battery of blood factors related to cholesterol metabolism \u2014 triglycerides, high-density lipoprotein (HDL) cholesterol and LDL cholesterol. In the samples of blood, of course, they also had DNA from each and every participant. As soon as the database was completed in 2002, Hobbs and Cohen tested their rare-variant theory by looking at levels of HDL cholesterol. They identified the people with the highest (95th percentile) and lowest (5th percentile) levels, and then sequenced the DNA of three genes known to be key to metabolism of HDL cholesterol. What they found, both in Dallas and in an independent population of Canadians, was that the number of mutations was five times higher in the low HDL group than in the high group 3 . This made sense, Cohen says, because most human mutations interfere with the function of genes, which would lead to the low HDL numbers. Published in 2004, the results confirmed that rare, medically important mutations could be found in a population subdivided into extreme phenotypes. Armed with their extensive database of cardiovascular traits, Hobbs and Cohen could now dive back into the Dallas Heart Study whenever they had a new hypothesis about heart disease and, as Cohen put it, \u201cinterrogate the DNA\u201d. It wasn't long before they had an especially intriguing piece of DNA at which to look. \n               The missing link \n             In February 2003, Nabil Seidah, a biochemist at the Clinical Research Institute of Montreal in Canada, and his colleagues reported the discovery of an enigmatic protein 4 . Seidah had been working on a class of enzymes known collectively as proprotein convertases, and the researchers had identified what looked like a new member of the family, called NARC-1: neural apoptosis-regulated convertase 1. \u201cWe didn't know what it was doing, of course,\u201d Seidah says. But the group established that the gene coding the enzyme showed activity in the liver, kidney and intestines as well as in the developing brain. The team also knew that in humans the gene mapped to a precise genetic neighbourhood on the short arm of chromosome 1. That last bit of geographical information pointed Seidah to a group led by Catherine Boileau at the Necker Hospital in Paris. Her team had been following families with a genetic form of extremely high levels of LDL cholesterol known as familial hypercholesterolaemia, which leads to severe coronary artery disease and, often, premature death. Group member Marianne Abifadel had spent five fruitless years searching a region on the short arm of chromosome 1 for a gene linked to the condition. When Seidah contacted Boileau and told her that he thought  NARC-1  might be the gene she was looking for, she told him, \u201cYou're crazy\u201d, Seidah recalls. Seidah bet her a bottle of champagne that he was correct; within two weeks, Boileau called back, saying: \u201cI owe you three bottles.\u201d In 2003, the Paris and Montreal groups reported that the French families with hypercholesterolaemia had one of two mutations in this newly discovered gene, and speculated that this might cause increased production of the enzyme 5 . Despite Seidah's protests, the journal editors gave both the gene and its protein product a new name that fit with standard nomenclature: proprotein convertase subtilisin/kexin type 9, or  PCSK9 . At around the same time, Kara Maxwell in Breslow's group at Rockefeller University 6  and Jay Horton, a gastroenterologist at UT-Southwestern 7  also independently identified the  PCSK9  gene in mice and revealed its role in a previously unknown pathway regulating cholesterol 8 . The dramatic phenotype of the French families told Hobbs that \u201cthis is an important gene\u201d. She also realized that in genetics, mutations that knock out a function are much more common than ones that amplify function, as seemed to be the case with the French families. \u201cSo immediately I'm thinking, a loss-of-function mutation should manifest as a low LDL level,\u201d she says. \u201cLet's go and see if that's true.\u201d \n               Going to extremes \n             Hobbs and Cohen had no further to look than in the extreme margins of people in the Dallas Heart Study. In quick order, they identified the highest and lowest LDL readings in four groups: black women, black men, white women and white men. They then resequenced the  PCSK9  gene in the low-cholesterol groups, looking for mutations that changed the make-up of the protein. They found seven African Americans with one of two distinct 'nonsense' mutations in  PCSK9  \u2014 mutations that essentially aborted production of the protein. Then they went back and looked for the same mutations in the entire population. Just 2% of all black people in the Dallas study had either of the two  PCSK9  mutations \u2014 and those mutations were each associated with a 40% reduction of LDL cholesterol in the blood 9 . (The team later detected a 'missense mutation' in 3% of white people, which impaired but did not entirely block production of the protein.) The frequency of the mutations was so low, Hobbs says, that they would never have shown up in a search for common variants. When Hobbs and Cohen published their findings in 2005, they suggested that  PCSK9  played a crucial part in regulating bad cholesterol, but said nothing about whether the mutations had any effect on heart disease. That evidence came later that year, when they teamed up with Eric Boerwinkle, a geneticist at the University of Texas Health Science Center in Houston, to look for  PCSK9  mutations in the Atherosclerosis Risk in Communities (ARIC) study, a large prospective study of heart disease that had been running since 1987. To experts such as Steinberg, the results 10  \u2014 published in early 2006 \u2014 were \u201cmind-blowing\u201d. African Americans in ARIC who had mutations in  PCSK9  had 28% less LDL cholesterol and an 88% lower risk of developing heart disease than people without the mutations. White people with the less severe mutation in the gene had a 15% reduction in LDL and a 47% reduced risk of heart disease. How did the gene exert such profound effects on LDL cholesterol levels? As researchers went on to determine 11 , the PCSK9 protein normally circulates in the bloodstream and binds to the LDL receptor, a protein on the surface of liver cells that captures LDL cholesterol and removes it from the blood. After binding with the receptor, PCSK9 escorts it into the interior of the cell, where it is eventually degraded. When there is a lot of PCSK9 (as in the French families), there are fewer LDL receptors remaining to trap and remove bad cholesterol from the blood. When there is little or no PCSK9 (as in the black people with mutations), there are more free LDL receptors, which in turn remove more LDL cholesterol. The UT-Southwestern group, meanwhile, went back into the community looking for family members who might carry additional  PCSK9  mutations. In September 2004, Gilbert, the nurse known as 'the cholesterol lady' in south Dallas because of her frequent visits, knocked on the door of Sharlayne Tracy's mother, an original member of the Dallas Heart Study. Gilbert tested Tracy, as well as her sister, brother and father. \u201cThey tested all of us, and I was the lowest,\u201d Tracy says. Zahid Ahmad, a doctor working with Hobbs at UT-Southwestern, was one of the first to look at Tracy's lab results. \u201cDr Zahid was in awe,\u201d Tracy recalled. \u201cHe said, 'You're not supposed to be so healthy!'.\u201d It wasn't just that her LDL cholesterol measured 14. As a person with two dysfunctional copies of the gene \u2014 including a new type of mutation \u2014 Tracy was effectively a human version of a knockout mouse. The gene had been functionally erased from her genome, and PCSK9 was undetectable in her blood without any obvious untoward effects. The genomics community might have been a little slow to understand the significance, Hobbs says, \u201cbut the pharmaceutical companies got it right away\u201d. \n               The next statin? \n             This being biology, however, the road to the clinic was not completely smooth. The particular biology of PCSK9 has so far thwarted efforts to find a small molecule that would interrupt its interaction with the LDL receptor and that could be packaged in a pill. But the fact that the molecule operates outside cells means that it is vulnerable to attack by monoclonal antibodies \u2014 one of the most successful (albeit most expensive) forms of biological medicine. The results of early clinical trials have caused a stir. Regeneron Pharmaceuticals of Tarrytown, New York, collaborating with Sanofi, published phase II clinical-trial results 12  last October showing that patients with high LDL cholesterol levels who had injections every two weeks of an anti-PCSK9 monoclonal antibody paired with a high-dose statin saw their LDL cholesterol levels fall by 73%; by comparison, patients taking high-dose statins alone had a decrease of just 17%. Last November, Regeneron and Sanofi began to recruit 18,000 patients for phase III trials that will test the ability of their therapy to cut cardiovascular events, including heart attacks and stroke. Amgen of Thousand Oaks, California, has also launched several phase III trials of its own monoclonal antibody after it reported similarly promising results 13 . Among other companies working on PCSK9-based therapies are Pfizer headquartered in New York, Roche based in Basel, Switzerland, and Alnylam Pharmaceuticals of Cambridge, Massachusetts. (Hobbs previously consulted for Regeneron and Pfizer, and now sits on the corporate board of Pfizer.) Not everyone is convinced that a huge market awaits this class of cholesterol-lowering drugs. Tony Butler, a financial analyst at Barclays Capital in New York, acknowledges the \u201cbeautiful biology\u201d of the  PCSK9  story, but wonders if the expense of monoclonal drugs \u2014 and a natural reluctance of both patients and doctors to use injectable medicines \u2014 will constrain potential sales. \u201cI have no idea what the size of the market may be,\u201d he says. \u201cEverything hinges on the phase III side effects,\u201d says Steinberg. So far, the main side effects reported have been minor, such as reactions at the injection site, diarrhoea and headaches. But animal experiments have raised potential red flags: the Montreal lab reported in 2006 that knocking out the gene in zebrafish is lethal to embryos 14 . That is why the case of Tracy was \u201cvery, very helpful\u201d to drug companies, says Hobbs. Although her twin mutations have essentially deprived her of PCSK9 throughout her life, doctors have found nothing abnormal about her. That last point may revive a debate in the cardiology community: should drug therapy to lower cholesterol levels, including statins and the anti-PCSK9 medicines, if they pan out, be started much earlier in patients than their 40s or 50s? That was the message Steinberg took from the people with  PCSK9  mutations in the ARIC study \u2014 once he got over his shock at the remarkable health effects. \u201cMy first reaction was, 'This must be wrong. How could that be?'And then it hit me \u2014 these people had low LDL from the day they were born, and that makes all the difference.\u201d Steinberg argues that cardiologists \u201cshould get off our bums\u201d and reach a consensus about beginning people on cholesterol-lowering therapy in their early thirties. But Breslow, a former president of the American Heart Association, cautions against being too aggressive too soon. \u201cLet's start out with the high-risk individuals and see how they do,\u201d he says. Not long after Hobbs and Cohen published their paper in 2006, they began to get invited to give keynote talks at major cardiology meetings. Soon after, the genetics community began to acknowledge the strength of their approach. In autumn 2007, then-NIH director Zerhouni organized a discussion at the annual meeting of the institutes' directors to raise the profile of the rare-variant approach and contrast it with genome-wide studies. \u201cObviously, the two approaches are opposed to each other, and the question was, what was the relative value of each?\u201d says Zerhouni. \u201cI thought the  PCSK9  story was a terrific example of an up-and-coming pattern of translational research\u201d \u2014 indeed, he adds, \u201ca harbinger of things to come\u201d. Hobbs and Cohen might not have found their gene if they had not had a hunch about where to look, but improved sequencing technology and decreasing costs now allow genomicists to incorporate the rare variant approach and to mount large-scale sweeps in search of such variants. \u201cGene sequencing is getting cheap enough that if there's another gene like  PCSK9  out there, you could probably find it genome-wide,\u201d says Jonathan Pritchard, a population biologist at the University of Chicago, Illinois. \u201cWhat was amazing to us,\u201d says Hobbs, \u201cwas that the genome project was spending all this time, energy, effort sequencing people, and they weren't phenotyped, so there was no potential for discovery. We didn't understand, and couldn't understand, why everybody wasn't doing what we were doing. Particularly when we started making discoveries.\u201d \n                     Cholesterol limits lose their lustre 2013-Feb-26 \n                   \n                     ENCODE: The human encyclopaedia 2012-Sep-05 \n                   \n                     Humans riddled with rare genetic variants 2012-May-17 \n                   \n                     Human genome at ten: Life is complicated 2010-Mar-31 \n                   \n                     Hiding place for missing heritability uncovered 2010-Jan-25 \n                   \n                     Personal genomes: The case of the missing heritability 2008-Nov-05 \n                   \n                     Nature speical: ENCODE \n                   \n                     Nature special: Human genome at 10 \n                   \n                     Nature human genome issue, 2001 \n                   \n                     Hobbs-Cohen lab \n                   Reprints and Permissions"},
{"file_id": "496156a", "url": "https://www.nature.com/articles/496156a", "year": 2013, "authors": [{"name": "Quirin Schiermeier"}], "parsed_as_year": "2006_or_before", "body": "An ambitious plan to slash greenhouse-gas emissions must clear some high technical and economic hurdles. In an industrial warehouse on the outskirts of Stuttgart, the sooty past is shaking hands with Germany's green-energy future. In one corner sits a relic destined for a museum: a cast-iron engine as big as a bus, which was used until the early 1970s to compress coal gas for lighting, cooking and heating. Nearby, a gleaming network of stainless-steel tanks uses electricity to create methane out of water and carbon dioxide. This power-to-gas (P2G) pilot plant is the largest of its kind in the world, and the research behind it could help to propel Germany to the front of the race for cleaner energy. Developers of the \u20ac3.5-million (US$4.5-million) project say that the P2G technology is an ideal way to cope with the unreliable nature of solar and wind power. During sunny or breezy days, excess electricity can be used to make methane, which can be stored and then burned to generate power when the winds fail or the days turn dark. The quest to develop P2G storage is part of Germany's ambitious  Energiewende , or energy transition, a long-term plan to clean up the country's energy systems. Enshrined in law, the scheme aims to slash CO 2  emissions by replacing fossil fuels with renewable sources of energy. Germany hopes to generate at least 35% of its electricity from green sources by 2020; by 2050, the share is expected to surpass 80% (see 'Green growth'). The  Energiewende  \u2014 the world's most extensive embrace of wind and solar power as well as other forms of renewable energy \u2014 enjoys the support of all Germany's political parties and most of the population. It will probably continue whatever the results of the national election in September. Other nations are watching keenly to see how the experiment proceeds, and whether they should follow the German lead. \u201cGermany's  Energiewende  can mobilize a global energy revolution,\u201d says Harry Lehmann, executive chairman of the World Council for Renewable Energies, who is based in Dessau, Germany. To reach its goal, Germany is currently investing more than \u20ac1.5 billion per year in energy research. One of its chief aims is to improve and build more storage systems, such as the Stuttgart P2G plant. Another is extending and strengthening the electricity grid to wire up remote wind turbines and countless small photovoltaic installations. The research programme also seeks to improve the efficiency of energy production from sunlight, wind and biomass, and to encourage people to reduce energy consumption. Most experts in Germany agree that the technical hurdles are surmountable. \u201cWe don't need technological leaps to accomplish the  Energiewende ,\u201d says Eberhard Umbach, president of the Karlsruhe Institute of Technology, who oversees the \u20ac500-million energy-research activities of Germany's national research centres. But the economic challenges are daunting, with the total costs of the  Energiewende  estimated to top \u20ac1 trillion. Europe's deep financial crisis looms large over a project of that scale, warns Roger Pielke Jr, an environmental-policy researcher at the University of Colorado Boulder. \u201cThe German public has so far shown great willingness to pay for the transformation, but there will be limits to that willingness, especially if the economic climate gets rougher.\u201d \n               Money and power \n             The  Energiewende  is already visible to anyone travelling the German countryside. Expensive solar panels cover more than one million houses, farms and warehouses, thanks to generous subsidies. Along the motorways, clumps of wind turbines sprout on ridges, particularly in the breezy northern regions. The country's green-energy portfolio is growing at a ferocious rate; last year, renewables supplied more than one-quarter of Germany's gross electricity needs. The primary motivation for the  Energiewende  is to combat climate change. By 2020, Germany aims to cut its greenhouse-gas emissions by 40% below 1990 production levels, and it hopes to achieve a reduction of at least 80% by 2050. The government set those goals in 2005, but the targets became much more ambitious in the aftermath of the Fukushima nuclear accident in Japan in March 2011. That crisis spurred German Chancellor Angela Merkel to speed up a planned move away from nuclear power, which provided 25% of the country's primary power in 2010. She closed eight nuclear plants immediately and pledged to shut the remaining nine by 2022. Suddenly, Germany needed to pick up the pace of its move towards renewable energy. For German consumers, the costs of that shift are apparent in their monthly electricity bills. The statements include a litany of 'shared costs' that are split by all households to fund the  Energiewende  \u2014 and result in some of the highest electricity prices in Europe. (Heavy industries are currently exempt from paying the surcharge.) The shared costs are a mechanism for promoting green forms of energy, which are more expensive to produce than electricity from coal and natural gas. Germany's Renewable Energy Act (EEG), the legal force behind the  Energiewende , allows owners of solar panels and wind turbines to sell their electricity to the grid at a fixed, elevated price. Renewable-power producers cashed in an estimated \u20ac20 billion last year for electricity that was actually worth a mere \u20ac3 billion on the wholesale electricity market. The difference came out of the pockets of consumers. The EEG, first passed in 2000, has helped Germany to install many more wind and solar power systems than most other developed nations. But the subsidies are causing some grotesque distortions in the electricity market \u2014 to the point at which energy companies are sometimes forced to sell conventional power at a loss. The effects of Germany's policies are also reverberating through the European energy market. One of Europe's biggest energy providers, E.ON based in D\u00fcsseldorf, announced in January that it plans to close several gas-fired power stations across Europe that were operating at a loss, even though they are far less polluting than coal-fired plants. The \u201cunmanaged growth\u201d of renewable-energy sources is forcing gas-fired plants to spend too much time idle, E.ON boss Johannes Teyssen told shareholders. At the same time, Germany is subject to the vagaries of outside forces, such as the rapid expansion of natural-gas production in the United States, which has curbed domestic demand for coal. Excess US coal is now heading to Europe, helping to fuel a resurgence of coal use in the United Kingdom and Germany, among other countries. With Germany's imports of low-cost coal rising, the country's greenhouse-gas emissions increased by almost 2% in 2012 \u2014 bucking a long-term decline. Last August, federal minister of the environment Peter Altmaier made clear that coal-fired plants will be needed \u201cfor decades to come\u201d to ensure energy supplies. Germany is currently building some 11 gigawatts of coal-fired plants and its existing capacity of around 55 GW will not shrink as quickly as the country had planned. The coal resurgence makes it unlikely that Germany will meet its 2020 emissions targets, says Pielke Jr. \u201cYou must accept the logic,\u201d he says. \u201cIf you opt out of nuclear power, your near-term emissions will go up.\u201d But German officials say that even if emissions rise temporarily, that trend will turn around. State-of-the-art coal-powered plants, such as a new 2.2-GW facility near Cologne that burns lignite, or low-grade coal, will replace several older, less efficient plants. The net effect will be to reduce CO 2  emissions, says Altmaier. To sustain that trend, however, the costs of green energy need to come down. \u201cSubsidies have helped to get the renewable thing started, but sooner or later renewable energy must become economically self-sustaining,\u201d says Brigitte Knopf, head of German and European energy strategies at the Potsdam Institute for Climate Impact Research. That is where Germany is aiming many of its energy-research efforts, which will receive in excess of \u20ac3.5 billion from the government between 2011 and 2014. About \u20ac200 million is going towards developing and improving storage technologies. \u201cRenewable energy is a wonderful thing \u2014 provided you are able to store it at a large scale and distribute it efficiently,\u201d says Frithjof Stai\u00df, managing director of the Center for Solar Energy and Hydrogen Research (ZSW) in Stuttgart, which operates the P2G plant. By making renewable energy more manageable and marketable, advanced storage technologies can ultimately help to reduce the cost of wind and solar power, he says. Germany currently stores excess electricity by using pumps to push water uphill into reservoirs. When electricity demand goes up, the water is released through turbines to generate hydroelectric power. But there are limits to expanding this type of storage. With 30 pump facilities already in operation, Germany has few suitable sites left for building more; the country plans to increase pump-storage capacity by just 20% by 2020. P2G, however, could provide a vast amount of new storage capacity and Germany is leading the way. The plant in Stuttgart has 250 kilowatts of electrolysis stacks, which use electricity from renewables to produce hydrogen from water. To make methane, the hydrogen is reacted with CO 2  from decomposing sewage and agricultural waste at a nearby biogas plant. Other P2G plants could scrub CO 2  from the air. But P2G is still an immature technology, with high upfront costs and an efficiency of only about 50% in converting electricity to methane. Synthetic methane plants have also struggled with the purity of their product. At the ZSW facility, the main goal is to routinely produce gas with low oxygen and hydrogen content. Several other government-funded research groups are working to improve the membranes used in electrolysis reactions to produce the hydrogen for P2G plants. Advanced proton-exchange membranes, for example, can respond to electricity fluctuations within milliseconds, which could produce the very pure hydrogen needed for P2G and other clean-energy technologies. Once P2G plants work out such issues, they could scale up, and synthetic gas could go directly into the network of pipelines and storage tanks currently used for natural gas. It could then be burned in power plants to produce electricity; by some estimates, this technology will be able to store up to 500,000 gigawatt-hours of electricity, enough to power Germany for more than six months. The synthetic gas could be also used for heating or as a vehicle fuel. This prospect has piqued the interest of the car-maker Audi, which is currently building the first commercial P2G test facility in Werlte, northern Germany. The facility is expected to produce up to 4,000 cubic metres of synthetic methane per day \u2014 enough to fuel about 1,500 cars. Audi is also set to introduce a hybrid natural-gas-powered sedan later this year. If Germans embrace this kind of vehicle, it could help to cut carbon emissions from the transport sector, which has lagged behind others in implementing the  Energiewende . The government intends to have one million battery-powered cars on German roads by 2020, but experts view that goal as unrealistic. The industry lacks the capacity to produce that many electric cars, and motor-loving Germans have not shown much desire for them. The new Audi and similar models may turn more heads, because consumers can switch to normal fuel if they are running low on natural gas and cannot find a specialized refuelling station. That option is not available to owners of batter-powered vehicles that run low on charge. \n               Supply and demand \n             Although transportation has been slow to change, the electricity sector has the opposite problem. The rapid rise in wind and solar power has created a nightmare scenario for grid operators, who face power surges when the wind blows and the Sun shines, and shortages when they don't. In 2011, more than 200,000 blackouts exceeding three minutes were reported \u2014 and experts warn of a growing risk of major power failures. For the  Energiewende  to succeed, the grid must be able to accommodate millions of extra small solar installations and wind turbines, as well as autonomous sub-grids such as those that connect offshore wind farms, which intermittently send floods of power into the onshore grid. In January, the government put out a \u20ac150-million call for research proposals for improving the electricity network. The government also announced last year that it would install almost 4,000 kilometres of high- and low-voltage power lines, with a total transmission capacity of 10 GW. The \u20ac20-billion project would help to carry energy to the south of Germany from wind farms in the north. And Germany may look even farther afield. Having distant power sources outside the nation would facilitate the  Energiewende  because a calm, dark day in Germany could be balanced by energy from other countries, says Robert Socolow, an environmental economist at Princeton University in New Jersey. Some companies have floated plans to build large thermal solar plants in the Sahara Desert, which gets enough sunshine to meet most of Europe's electricity demand. But this scheme, the multibillion-euro DESERTEC initiative, lost momentum late last year, when two major industry partners \u2014 Siemens and Bosch \u2014 backed out (see  Nature   491 , 16\u201317; 2012 ). Energy analysts, moreover, doubt that Germany or any other European country would be willing to rely on substantial electricity imports from a politically unstable region. Some question the need to ship power over long distances. \u201cThat the  Energiewende  depends on a huge expansion of the grid is a myth \u2014 and a very expensive myth, too,\u201d says Mathias Willenbacher, co-founder and managing director of juwi in W\u00f6rrstadt, a company that has successfully built small-scale renewable-energy projects. Willenbacher argues that it would be better to plough money into energy-storage options. \u201cIf you solve the storage issue, there is no need any more to transmit massive amounts of wind power from the North Sea to the Alps,\u201d he says. However Germany sources its electricity, the  Energiewende  will not succeed unless the country can convince ordinary people to use energy more efficiently, says Umbach. Researchers from the Karlsruhe Institute of Technology are currently running a field study to see whether they can alter the behaviours of 1,000 private citizens and commercial electricity customers in the city of G\u00f6ppingen and the rural municipality of Freiamt. The trial, supported by the energy company EnBW in Karlsruhe, gives consumers smart-grid meters that provide information on their minute-to-minute energy use as well as the cost of power, which fluctuates throughout the day. The researchers aim to study whether people use the information to reduce their energy consumption. How Germans will get by in the brave new energy world remains to be seen, says Umbach. \u201cWill a computer tell them when to wash and cook?\u201d he asks. \u201cOr will they still want to fry their schnitzel when they feel like it?\u201d Surveying all the impediments to the  Energiewende , Umbach is not sure that the transformation will come off as soon as planned. But he is convinced that Germany is the right country to try the great experiment. \u201cIf it fails it will be bad for Germany,\u201d he says. \u201cBut if it succeeds the whole world will profit.\u201d \n                 See Editorial \n                 page 137 \n               \n                     The Kyoto Protocol: Hot air 2012-Nov-28 \n                   \n                     Germany: Renewables revolution 2011-Dec-07 \n                   \n                     How green is my future? 2011-May-11 \n                   \n                     Smart grids: The energy storage problem 2010-Jan-06 \n                   \n                     Energy alternatives: Electricity without carbon 2008-Aug-13 \n                   \n                     Nature Insight: Chemistry and Energy \n                   \n                     Center for Solar Energy and Hydrogen Research \n                   \n                     Fraunhofer Energy Alliance \n                   \n                     Helmholtz Association of National Research Centres \u2014 Energy Research \n                   Reprints and Permissions"},
{"file_id": "495425a", "url": "https://www.nature.com/articles/495425a", "year": 2013, "authors": [], "parsed_as_year": "2006_or_before", "body": "A special issue of  Nature  looks at the transformation taking place in scientific publishing. After 350 years in the slow-moving world of print, scientific publishing has been thrust into a fast-paced online realm of cloud computing and ubiquitous sharing. The result has been an era of ferment, as established practices are challenged by new ones \u2014 most notably, the open-access model in which the author pays publication fees upfront. Last month, US President Barack Obama's administration declared that government-funded research papers should be made freely available within 12 months of publication (see  Nature   494 , 414\u2013415; 2013 ). And from 1 April, research councils in the United Kingdom will require the results of government-funded research to be open access on publication. In this special issue,  Nature  explores the changing landscape. A News Feature weighs claims that online, author-pays publishing can drastically cut costs (see  page 426 ). Several authors discuss the nuts and bolts of making open-access publishing work well \u2014 including copyright pioneer John Wilbanks on open licensing agreements (see  pages 440  and  442 ). A report explores the dark side of open access: publishers whose tactics lead authors to feel disgruntled or duped (see  page 433 ). And a Careers Feature offers advice for researchers trying to balance prestige, cost and career implications in deciding where to submit manuscripts (see  page 539 ). The special also looks at broader aspects of publishing. Information scientist Jason Priem describes how the concepts of journal and article are being superseded by algorithms that filter, rate and disseminate scholarship as it happens (see  page 437 ). A News Feature investigates how some university libraries are reinventing themselves to help scientists to archive and make accessible a new kind of publication: data sets (see  page 430 ). And Robert Darnton, director of the library at Harvard University in Cambridge, Massachusetts, talks about the soon-to-be launched Digital Public Library of America, which could ultimately hold 5 million books (see  page 447 ). Science itself is changing rapidly; the means by which it is shared must keep up. \n                     Europe joins UK open-access bid 2012-Jul-17 \n                   \n                     Britain aims for broad open access 2012-Jun-19 \n                   \n                     Open access comes of age 2011-Jun-21 \n                   \n                     US seeks to make science free for all 2010-Apr-07 \n                   \n                     Data sharing: Empty archives 2009-Sep-09 \n                   \n                     Nature special: The future of publishing \n                   \n                     Nature special: Big data \n                   \n                     Nature special: Data sharing \n                   Reprints and Permissions"},
{"file_id": "497024a", "url": "https://www.nature.com/articles/497024a", "year": 2013, "authors": [{"name": "Natasha Gilbert"}], "parsed_as_year": "2006_or_before", "body": "Superweeds? Suicides? Stealthy genes? The true, the false and the still unknown about transgenic crops. In the pitched debate over genetically modified (GM) foods and crops, it can be hard to see where scientific evidence ends and dogma and speculation begin. In the nearly 20\u00a0years since they were first commercialized, GM crop technologies have seen dramatic uptake. Advocates say that they have increased agricultural production by more than US$98 billion and saved an estimated 473 million kilograms of pesticides from being sprayed. But critics question their environmental, social and economic impacts. Researchers, farmers, activists and GM seed companies all stridently promote their views, but the scientific data are often inconclusive or contradictory. Complicated truths have long been obscured by the fierce rhetoric. \u201cI find it frustrating that the debate has not moved on,\u201d says Dominic Glover, an agricultural socioeconomist at Wageningen University and Research Centre in the Netherlands. \u201cThe two sides speak different languages and have different opinions on what evidence and issues matter,\u201d he says. Here,  Nature  takes a look at three pressing questions: are GM crops fuelling the rise of herbicide-resistant \u2018superweeds\u2019? Are they driving farmers in India to suicide? And are the foreign transgenes in GM crops spreading into other plants? These controversial case studies show how blame shifts, myths are spread and cultural insensitivities can inflame debate. \n               GM crops have bred superweeds: True \n             Jay Holder, a farming consultant in Ashburn, Georgia, first noticed Palmer amaranth ( Amaranthus palmeri ) in a client\u2019s transgenic cotton fields about five years ago. Palmer amaranth is a particular pain for farmers in the southeastern United States, where it outcompetes cotton for moisture, light and soil nutrients and can quickly take over fields. Case studies reveal the complex truths behind GM crop myths. Since the late 1990s, US farmers had widely adopted GM cotton engineered to tolerate the herbicide glyphosate, which is marketed as Roundup by Monsanto in St Louis, Missouri. The herbicide\u2013crop combination worked spectacularly well \u2014 until it didn\u2019t. In 2004, herbicide-resistant amaranth was found in one county in Georgia; by 2011, it had spread to 76. \u201cIt got to the point where some farmers were losing half their cotton fields to the weed,\u201d says Holder. Some scientists and anti-GM groups warned that GM crops, by encouraging liberal use of glyphosate, were spurring the evolution of herbicide resistance in many weeds. Twenty-four glyphosate-resistant weed species have been identified since Roundup-tolerant crops were introduced in 1996. But herbicide resistance is a problem for farmers regardless of whether they plant GM crops. Some 64 weed species are resistant to the herbicide atrazine, for example, and no crops have been genetically modified to withstand it (see \u2018The rise of superweeds\u2019). Still, glyphosate-tolerant plants could be considered victims of their own success. Farmers had historically used multiple herbicides, which slowed the development of resistance. They also controlled weeds through ploughing and tilling \u2014 practices that deplete topsoil and release carbon dioxide, but do not encourage resistance. The GM crops allowed growers to rely almost entirely on glyphosate, which is less toxic than many other chemicals and kills a broad range of weeds without ploughing. Farmers planted them year after year without rotating crop types or varying chemicals to deter resistance. This strategy was supported by claims from Monsanto that glyphosate resistance was unlikely to develop naturally in weeds when the herbicide was used properly. As late as 2004, the company was publicizing a multi-year study suggesting that rotating crops and chemicals does not help to avert resistance. When applied at Monsanto\u2019s recommended doses, glyphosate killed weeds effectively, and \u201cwe know that dead weeds will not become resistant\u201d, said Rick Cole, now Monsanto\u2019s technical lead of weed management, in a trade-journal advertisement at the time. The study, published in 2007 (ref.  1 ), was criticized by scientists for using plots so small that the chances of resistance developing were very low, no matter what the practice. Glyphosate-resistant weeds have now been found in 18 countries worldwide, with significant impacts in Brazil, Australia, Argentina and Paraguay, says Ian Heap, director of the International Survey of Herbicide Resistant Weeds, based in Corvallis, Oregon. And Monsanto has changed its stance on glyphosate use, now recommending that farmers use a mix of chemical products and ploughing. But the company stops short of acknowledging a role in creating the problem. \u201cOver-confidence in the system combined with economic drivers led to reduced diversity in herbicide use,\u201d Cole tells  Nature . On balance, herbicide-resistant GM crops are less damaging to the environment than conventional crops grown at industrial scale. A study by PG Economics, a consulting firm in Dorchester, UK, found that the introduction of herbicide-tolerant cotton saved 15.5\u00a0million kilograms of herbicide between 1996 and 2011, a 6.1% reduction from what would have been used on conventional cotton 2 . And GM crop technology delivered an 8.9% improvement to the environmental impact quotient \u2014 a measure that considers factors such as pesticide toxicity to wildlife \u2014 says Graham Brookes, co-director of PG Economics and a co-author of the industry-funded study, which many scientists consider to be among the field\u2019s most extensive and authoritative assessments of environmental impacts. The question is how much longer those benefits will last. So far, farmers have dealt with the proliferation of resistant weeds by using more glyphosate, supplementing it with other herbicides and ploughing. A study by David Mortensen, a plant ecologist at Pennsylvania State University in University Park, predicts that total herbicide use in the United States will rise from around 1.5 kilograms per hectare in 2013 to more than 3.5 kilograms per hectare in 2025 as a direct result of GM\u00a0crop use 3 . To offer farmers new weed-control strategies, Monsanto and other biotechnology companies, such as Dow AgroSciences, based in Indianapolis, Indiana, are developing new herbicide-resistant crops that work with different chemicals, which they expect to commercialize within a few years. Mortensen says that the new technologies will lose their effectiveness as well. But abandoning chemical herbicides completely is not a viable solution, says Jonathan Gressel, a weed scientist at the Weizmann Institute of Science in Rehovot, Israel. Using chemicals to control weeds is still more efficient than ploughing and tilling the soil, and is less environmentally damaging. \u201cWhen farmers start to use more sustainable farming practices together with mixtures of herbicides they will have fewer problems,\u201d he says. \n               GM cotton has driven farmers to suicide: False \n             During an interview in March, Vandana Shiva, an environmental and feminist activist from India, repeated an alarming statistic: \u201c270,000 Indian farmers have committed suicide since Monsanto entered the Indian seed market,\u201d she said. \u201cIt\u2019s a genocide.\u201d The claim, based on an increase in total suicide rates across the country in the late 1990s, has become an oft-repeated story of corporate exploitation since Monsanto began selling GM seed in India in 2002. Bt  cotton, which contains a gene from the bacterium  Bacillus thuringiensis  to ward off certain insects, had a rough start. Seeds initially cost five times more than local hybrid varieties, spurring local traders to sell packets containing a mix of  Bt  and conventional cotton at lower prices. The sham seeds and misinformation about how to use the product resulted in crop and financial losses. This no doubt added strain to rural farmers, who had long been under the pressures of a tight credit system that forced them to borrow from local lenders. But, says Glover, \u201cit is nonsense to attribute farmer suicides solely to  Bt  cotton\u201d. Although financial hardship is a driving factor in suicide among Indian farmers, there has been essentially no change in the suicide rate for farmers since the introduction of  Bt  cotton. That was shown by researchers at the International Food Policy Research Institute in Washington DC, who scoured government data, academic articles and media reports about  Bt  cotton and suicide in India. Their findings, published in 2008 (ref.  4 ) and updated in 2011 (ref.  5 ), show that the total number of suicides per year in the Indian population rose from just under 100,000 in 1997 to more than 120,000 in 2007. But the number of suicides among farmers hovered at around 20,000 per year over the same period. And since its rocky beginnings,  Bt  cotton has benefited farmers, says Matin Qaim, an agricultural economist at Georg August University in G\u00f6ttingen, Germany, who has been studying the social and financial impacts of  Bt  cotton in India for the past 10 years. In a study of 533\u00a0cotton-farming households in central and southern India, Qaim found that yields grew by 24% per acre between 2002 and 2008, owing to reduced losses from pest attacks 6 . Farmers\u2019 profits rose by an average of 50% over the same period, owing mainly to yield gains (see \u2018A steady rate of tragedy\u2019). Given the profits, Qaim says, it is not surprising that more than 90% of the cotton now grown in India is transgenic. Glenn Stone, an environmental anthropologist at Washington University in St Louis, says that the empirical evidence for yield increases with  Bt  cotton is lacking. He has conducted original field studies 7  and analysed the research literature 8  on  Bt  cotton yields in India, and says that most peer-reviewed studies reporting yield increases with  Bt  cotton have focused on short time periods, often in the early years after the technology came online. This, he says, introduced biases: farmers who adopted the technology first tended to be wealthier and more educated, and their farms were already producing higher-than-average yields of conventional cotton. They achieved high yields of  Bt  cotton partly because they lavished the expensive GM seeds with care and attention. The problem now is that there are hardly any conventional cotton farms left in India to compare GM yields and profits against, says Stone. Qaim agrees that many studies showing financial gains focus on short-term impacts, but his study, published in 2012, controlled for these biases and still found continued benefits. Bt  cotton did not cause suicide rates to spike, says Glover, but neither is it the sole reason for the yield improvements. \u201cBlanket conclusions that the technology is a success or failure lack the right level of nuance,\u201d he says. \u201cIt\u2019s an evolving story in India, and we have not yet reached a definitive conclusion.\u201d \n               Transgenes spread to wild crops in Mexico: Unknown \n             In 2000, some rural farmers in the mountains of Oaxaca, Mexico, wanted to gain organic certification for the maize (corn) they grew and sold in the hope of generating extra income. David Quist, then a microbial ecologist at the University of California, Berkeley, agreed to help in exchange for access to their lands for a research project. But Quist\u2019s genetic analyses uncovered a surprise: the locally produced maize contained a segment of the DNA used to spur expression of transgenes in Monsanto\u2019s glyphosate-tolerant and insect-resistant maize 9 . GM crops are not approved for commercial production in Mexico. So the transgenes probably came from GM crops imported from the United States for consumption and planted by local farmers who probably didn\u2019t know that the seeds were transgenic. Quist speculated at the time that the local maize probably cross-bred with these GM varieties, thereby picking up the transgenic DNA. When the discovery was published in  Nature , a media and political circus descended on Oaxaca. Many vilified Monsanto for contaminating maize at its historic origin\u00a0\u2014\u00a0a place where the crop was considered sacred. And Quist\u2019s study came under fire for technical deficiencies, including problems with the methods used to detect the transgenes and the authors\u2019 conclusion that transgenes can fragment and scatter throughout the genome 10 .  Nature  eventually withdrew support for the paper but stopped short of retracting it. \u201cThe evidence available is not sufficient to justify the publication of the original paper,\u201d read an editorial footnote to a critique 10  of the research published in 2002. Since then, few rigorous studies of transgene flow into Mexican maize have been published, owing mainly to a dearth of research funding, and they show mixed results. In 2003\u201304, Allison Snow, a plant ecologist at Ohio State University in Columbus, sampled 870 plants taken from 125 fields in Oaxaca and found no transgenic sequences in maize seeds 11 . But in 2009, a study 12  led by Elena Alvarez-Buylla, a molecular ecologist at the National Autonomous University of Mexico in Mexico City, and Alma Pi\u00f1eyro-Nelson, a plant molecular geneticist now at the University of California, Berkeley, found the same transgenes as Quist in three samples taken from 23 sites in Oaxaca in 2001, and in two samples taken from those sites in 2004. In another study, Alvarez-Buylla and her co-authors found evidence of transgenes in a small percentage of seeds from 1,765 households across Mexico 13 . Other studies conducted within local communities have found transgenes more consistently, but few have been published 14 . Snow and Alvarez-Buylla agree that differences in sampling methods can lead to discrepancies in transgene detection. \u201cWe sampled different fields,\u201d says Snow. \u201cThey found them but we didn\u2019t.\u201d The scientific community remains split on whether transgenes have infiltrated maize populations in Mexico, even as the country grapples with whether to approve commercialization of  Bt  maize. \u201cIt seems inevitable that there will be a movement of transgenes into local maize crops,\u201d says Snow. \u201cThere is some proof that it is happening, but it is very difficult to say how common it is or what are the consequences.\u201d Alvarez-Buylla argues that the spread of transgenes will harm the health of Mexican maize and change characteristics, such as a variety\u2019s look and taste, that are important to rural farmers. Once the transgenes are present, it will be very difficult, if not impossible, to get rid of them, she says. Critics speculate that GM traits that accumulate in the genomes of local maize populations over time could eventually affect plant fitness by using up energy and resources or by disrupting metabolic processes, for example. Snow says that there is no evidence so far for negative effects. And she expects that if the transgenes now in use drift to other plants, they will have neutral or beneficial effects on plant growth. In 2003, Snow and her colleagues showed that when  Bt  sunflowers ( Helianthus annuus ) were bred with their wild counterparts, transgenic offspring still required the same kind of close care as its cultivated parent but were less vulnerable to insects and produced more seeds than non-transgenic plants 15 . Few similar studies have been conducted, says Snow, because the companies that own the rights to the technology are generally unwilling to let academic researchers perform the experiments. In Mexico, the story goes beyond potential environmental impacts. Kevin Pixley, a crop scientist and the director of the genetic resources programme at the International Maize and Wheat Improvement Centre in El Batan, Mexico, says that scientists arguing on behalf of GM technologies in the country have missed a crucial point. \u201cMost of the scientific community doesn\u2019t understand the depth of the emotional and cultural affiliation maize has for the Mexican population,\u201d he says. Tidy stories, in favour of or against GM crops, will always miss the bigger picture, which is nuanced, equivocal and undeniably messy. Transgenic crops will not solve all the agricultural challenges facing the developing or developed world, says Qaim: \u201cIt is not a silver bullet.\u201d But vilification is not appropriate either. The truth is somewhere in the middle. \n                     Transgenic grass skirts regulators 2011-Jul-20 \n                   \n                     Food: Inside the hothouses of industry 2010-Jul-28 \n                   \n                     GM crops: Battlefield 2009-Sep-02 \n                   \n                     Nature  special: GM crops \n                   \n                     International Survey of Herbicide Resistant Weeds \n                   \n                     Monsanto \n                   \n                     Dow Agricultural \n                   \n                     The Union of Concerned Scientists \n                   \n                     International Service for the Acquisition of Agri-Biotech Applications \n                   Reprints and Permissions"},
{"file_id": "497027a", "url": "https://www.nature.com/articles/497027a", "year": 2013, "authors": [{"name": "Daniel Cressey"}], "parsed_as_year": "2006_or_before", "body": "The next wave of genetically modified crops is making its way to market \u2014 and might just ease concerns over 'Frankenfoods'. When the first genetically modified (GM) organisms were being developed for the farm, says Anastasia Bodnar, \u201cwe were promised rocket jet packs\u201d \u2014 futuristic, ultra-nutritious crops that would bring exotic produce to the supermarket and help to feed a hungry world. Yet so far, she says, the technology has bestowed most of its benefits on agribusiness \u2014 almost always through crops modified to withstand weed-killing chemicals or resist insect pests. This has allowed farmers to increase yields and spray less pesticide than they might have otherwise. At best, such advances have been almost invisible to ordinary consumers, says Bodnar, a biotechnologist with Biology Fortified, a non-profit GM-organism advocacy organization in Middleton, Wisconsin. And at worst, they have helped to fuel the rage of opponents of genetic modification, who say that transgenic crops have concentrated power and profits in the hands of a few large corporations, and are a prime example of scientists meddling in nature, heedless of the dangers (see page 24). But that could soon change, thanks to a whole new generation of GM crops now making their way from laboratory to market. Some of these crops will tackle new problems, from apples that stave off discolouration to 'Golden Rice' and bright-orange bananas fortified with nutrients to improve the diets of people in the poorest countries. Other next-generation crops will be created using advanced genetic-manipulation techniques that allow high-precision editing of the plant's own genome. Such approaches could reduce the need to modify commercial crops with genes imported from other species \u2014 one of the practices that most disturbs critics of genetic modification. And that, in turn, could conceivably reduce the public disquiet over GM foods. Or maybe not. Whatever promise these crops may show in the laboratory, they will still have to demonstrate their benefits in painstaking, expensive and detailed field trials; jump through multiple regulatory hoops; and reassure an often sceptical public. That last part will not be easy, says Philip Bereano, who studies the political and social aspects of new technologies at the University of Washington, Seattle. He points out that the arguments over GM organisms run the gamut from concerns about safety and labelling to ethical issues with the patenting of life. \u201cPeople are concerned about what they're feeding their kids,\u201d he says, \u201cand that is not going to change.\u201d Nevertheless, most GM-organism researchers seem convinced that the worst of the technology's problems are over, and that its future is bright. If you are looking for the jet-pack era of GM organisms, says Bodnar, \u201cit is happening now.\u201d The first wave of GM crops was marketed mainly to farmers, with the goal of making their jobs easier, more productive and more profitable. In 1996, for example, biotechnology firm Monsanto of St Louis, Missouri, introduced the first of its popular 'Roundup Ready' products: a soya bean equipped with a bacterial gene that allows it to tolerate a Monsanto-made glyphosphate herbicide known as Roundup. This meant that farmers could kill off the majority of weeds with one herbicide rather than several, without damaging the crop. Other GM crops soon followed, including Monsanto's  Bt  cotton: a plant modified to produce a bacterial toxin that discourages destructive bollworms and cuts down on the need for pesticides. Farmers will continue to be a core market for the coming generation of GM organisms. At Rothamsted Research in Harpenden, UK, for example, scientists are working on GM plants that will need even less pesticide than Bt cotton, and maybe none at all. The key is an 'alarm pheromone' that some species of wild plant have evolved to mimic the chemical warning signals put out by aphids \u2014 a major crop pest in the temperate zones \u2014 when they are under attack. Putting the genes for this defence into wheat has created a crop that could trick the insects into thinking that they are in peril and drive them away. Unlike Bt cotton and other existing GM organisms, such a crop would need no insect-killing chemical for protection from pests. Field trials are currently under way, says Maurice Moloney, director and chief executive of the Rothamsted centre. \u201cIn the greenhouse it's been very successful,\u201d he says. \u201cIf we can get it to work in the field, we'll be able to optimize it to make it a robust trait\u201d suitable for large-scale deployment. From there, says Maloney, the team hopes to expand its efforts, searching for naturally evolved protections and deterrents in other crops, and working out how these might be enhanced or modified to fight particular pests. \u201cFor example, you could have a volatile chemical that also is a deterrent for caterpillars, stem borers and the like,\u201d says Maloney. \u201cPotentially, if we can get this to work, the range of applications is phenomenal.\u201d \n               Local concerns \n             Many GM-organism researchers are pushing work on crops sometimes neglected by the big agricultural companies. In the plant biotechnology group at the Swiss Federal Institute of Technology in Zurich, for example, Herve Vanderschuren leads a team working on cassava ( Manihot esculenta ), a tropical shrub with a tuber that is a staple food in the developing world. \u201cThere is not major investment in breeding or improvement of this crop,\u201d he says. Vanderschuren and his team are genetically engineering cassava to be resistant to two particularly damaging viruses, by starting with a variety that is naturally resistant to cassava mosaic virus, and then inserting genes that confer resistance to cassava brown streak virus. The naturally resistant strain was already tailored to local needs and markets. That kind of local adaptation is a \u201cvery important part of the research we do here\u201d, says Vanderschuren \u2014 and something that is rarely embraced by huge agribusinesses that want to sell products worldwide. Vanderschuren and his team have successfully made the plants, and are now collaborating with colleagues in Africa to arrange tests to confirm that the cassava can be grown in the field. Much of the work on crops in developing nations focuses on nutritional enhancement. The most famous example of this effort is Golden Rice, a modified version of the staple food of half the world. Its distinct yellow hue comes from the addition of \u03b2-carotene, a precursor to vitamin A that is deficient in many East Asian diets. After much painstaking development and many objections from opponents of GM organisms \u2014 the original version of Golden Rice was announced in 2000 \u2014 the crop is currently undergoing field trials in the Philippines (see I. Potrykus  Nature   466 , 561; 2010 ). It could clear the final regulatory hurdles and reach farmers by 2014. Others have followed in its wake. James Dale, director of the Centre for Tropical Crops and Biocommodities at Queensland University of Technology in Brisbane, Australia, for example, is trying to equip bananas with resistance to Panama disease, a fungal wilt that can devastate crops, as well as increased \u03b2-carotene and a suite of other nutrients including iron. \u201cLevels of micronutrient deficiencies are really very high\u201d in Uganda and all across Africa, he explains, and bananas are a staple of the diet. Field trials have already been conducted in Australia. Although most next-generation GM organisms are aimed at farmers, some target the next step in the chain: industrial food processors. For example, Chris Dardick, a molecular plant biologist at the US Agricultural Research Service's Appalachian Fruit Research Station in Kearneysville, West Virginia, explains that it is difficult to get plums into processed foods, because removing their hard, woody cores leaves shards behind. But starting with genes from a mostly stoneless, conventionally bred plum, Dardick and his team are in the early stages of engineering a fruit with no stone at all. \u201cOur biggest concern was how such a thing would be embraced by industry and consumers. Most of the feedback we've gotten has been quite positive,\u201d he says. And then there are GM organisms designed to appeal directly to the final consumers. One of the first will be the Arctic Apple, which does not brown rapidly after it is cut or bitten into. This is thanks to the insertion of genes from other apple varieties that produce lower than usual levels of polyphenol oxidase, a key enzyme in the chain of biochemical events that cause browning. \u201cMy wife and I are apple growers ourselves. We were concerned because apple consumption has been declining,\u201d says Neal Carter, president of Okanagan Specialty Fruits in Summerland, British Columbia, the developer of the Arctic Apple. Carter says that apples are losing ground in the supermarket to carrots and other fresh produce that is sold in bags, cleaned, sliced and ready to eat. Making apples that could be processed in such a way without browning could be a real boon for the industry. And if the apples are received well, says Carter, Arctic avocados, pears and even lettuce could be next. \n               Advanced techniques \n             Much of the genetic-modification work so far has been achieved with relatively crude but established techniques, such as a 'gene gun' that fires gold nanopellets coated with DNA from other organisms into the cells of the target plant, which incorporate the DNA at random sites in the genome. But new tools offer unparalleled precision in editing genes. For example, enzymes called transcription activator-like effector nucleases (TALENs) and zinc-finger nucleases (ZFNs) can cut DNA at specific points chosen by the experimenter. By controlling how this break is repaired, it is possible to introduce mutations, single-nucleotide changes or even whole genes at precise sites, says Dan Voytas, who works with such techniques at the University of Minnesota in St Paul. \u201cWe can do precise insertion so we know where in the chromosome the foreign gene resides.\u201d This allows researchers to put the new gene in a spot in the genome where its expression is optimal, and reduces the risk of disrupting the plant's genome in undesirable ways. Voytas's group has already shown that tobacco plants can be modified with ZFNs to introduce herbicide resistance 1 . Other groups have added herbicide resistance to maize (corn) with ZFNs 2  or have used TALENs to snip out the gene in rice that confers susceptibility to bacterial blight 3 . But Voytas says the \u201creal power\u201d of these techniques lies in the ability to confer new traits by modifying native plant genes. For example, rather than engineering plants to withstand dry conditions by incorporating genes from drought-tolerant bacteria (see  Nature   466 , 548\u2013551; 2010 ), researchers could adjust the multiple native genes that help plants to survive drought. \u201cReally, the next stage of the development of the technology is to go in and to tweak multiple genes,\u201d says Voytas. Derek Jantz, co-founder of Precision BioSciences, a biotechnology company based in Durham, North Carolina, is also excited about working with a plant's own genes. For example, all plants have an analogue of the bacterial  EPSPS  gene that is inserted into Monsanto's Roundup Ready crops. It should be possible to create similar herbicide resistance by editing a plant's own version, rather than bringing in an external gene 4 . Like other researchers in the genetic-modification industry, Jantz declines to talk about specific research projects because of commercial confidentiality. But in general terms, he says, \u201cwhat we're trying to do is take advantage of the wealth of functional genomics data that is becoming available\u201d. \n               A breed apart \n             Some researchers are using genetic modification to accelerate conventional breeding techniques. Ralph Scorza, a plant scientist at the Appalachian Fruit Research Station, leads a team that has genetically modified plum trees. The modified trees can survive only in greenhouses. But thanks to the insertion of a gene from poplar trees, they begin to flower much earlier in their lifetimes than conventional varieties do, and then continuously thereafter. This means that researchers can breed the trees throughout the year, using selection, cross-breeding and other traditional techniques to develop traits such as disease resistance in just a few years, as opposed to the decade or more that conventional breeding might require. When the desired traits have been bred in, the transgenes that drive flowering can be bred out, leaving a modified but non-GM plant. Scorza and his colleagues are using this 'FasTrack' breeding strategy in an effort to generate resistance to the plum pox virus, and to increase the sugar content of the fruit. Researchers elsewhere are applying it to crops such as citrus. US regulators have already suggested that organisms modified with the newer techniques such that they contain no DNA from other species will be treated differently from conventional GM organisms. That might also alleviate public concerns. \u201cWe can overcome hopefully at least some of the opposition to the genetic modification,\u201d says Alan McHughen, a molecular geneticist at the University of California, Riverside. Besides, notes Bodnar, there may be no stopping GM organisms. She points out that genetic engineering now has a relatively low bar to entry. 'Biohackers' working with bacteria are already conducting genetic modification experiments in their garages and spare bedrooms, and there is nothing to stop them from applying their skills to plants \u2014 or animals \u2014 in the future. \u201cIt's becoming easier all the time. I think people are hungry for this kind of thing,\u201d says Bodnar. \u201cThe jet packs that everybody wanted \u2014 I think it's time for them to come out. If the marketplace isn't providing that from the top down, you may see it from the bottom up.\u201d \n                 See News Feature \n                 page 21 \n               \n                     Hyped GM maize study faces growing scrutiny 2012-Oct-10 \n                   \n                     Food: Inside the hothouses of industry 2010-Jul-28 \n                   \n                     Nature special: GM crops \n                   \n                     Arctic Apples \n                   \n                     Dan Voytas \n                   \n                     Appalachian Fruit Research Station \n                   \n                     Precision BioSciences \n                   Reprints and Permissions"},
{"file_id": "497172a", "url": "https://www.nature.com/articles/497172a", "year": 2013, "authors": [{"name": "Erika Check Hayden"}], "parsed_as_year": "2006_or_before", "body": "Yaniv Erlich shows how research participants can be identified from 'anonymous' DNA. Late at night, a video camera captures a man striding up to the locked door of the information-technology department of a major Israeli bank. At this hour, access can be granted only by a fingerprint reader \u2014 but instead of using the machine, the man pushes a button on the intercom to ring the receptionist's phone. As it rings, he holds his mobile phone up to the intercom and presses the number 8. The sound of the keypad tone is enough to unlock the door. As he opens it, the man looks back to the camera with a shrug: that was easy. Yaniv Erlich \u2014 the star of this 2006 video \u2014 considers this one of his favourite hacks. Technically a \u201cpenetration exercise\u201d conducted to expose the bank's vulnerabilities, it was one of several projects that Erlich worked on during a two-year stint with a security firm based near Tel Aviv. Since then, the 33-year-old computational biologist has been bringing his hacker ethos to biology. Now at the Whitehead Institute for Biomedical Research in Cambridge, Massachusetts, he is using genome data in new ways, and in the process exposing vulnerabilities in databases that hold sensitive information on thousands of individuals around the world. In a study published in January 1 , Erlich's lab showed that it is possible to discover the identities of people who participate in genetic research studies by cross-referencing their data with publicly available information. Previous studies had shown that people listed in anonymous genetic data stores could be unmasked by matching their data to a sample of their DNA. But Erlich showed that all it requires is an Internet connection. Erlich's work has exposed a pressing ethical quandary. As researchers increasingly combine patient data with other types of information \u2014 everything from social-media posts to entries on genealogy websites \u2014 protecting anonymity becomes next to impossible. Studying these linked data has its benefits, but it may also reveal genetic and medical information that researchers had promised to keep private \u2014 and that, if made public, might hurt people's employability, insurability or even personal relationships. Such revelations may make the scientific community uncomfortable and undermine the public's trust in medical research. But Erlich and his colleagues see their work as a way to alert the world about flawed systems, keep researchers honest and ultimately strengthen science. In March, for instance, the European Molecular Biology Laboratory (EMBL) in Heidelberg, Germany, claimed that the genome sequence that it had published for the HeLa cell line would not reveal anything about Henrietta Lacks \u2014 the source of the cells \u2014 or her descendants. Erlich issued a tart response: \u201cNice lie EMBL!\u201d he tweeted. The sequence was later pulled from public databases, and the EMBL admitted that it would indeed be possible to glean information about the Lacks family from it, even though much of the HeLa genetic data had already been published as part of other studies. \u201cMost scientists would not go anywhere close to these questions, out of a sense of what it might mean for the field, or for them personally,\u201d says David Page, director of the Whitehead Institute, who has advised Erlich about his research. \u201cBut this is not about publicity-seeking \u2014 this is about fearlessness, and a kind of interest in how all the parts of the Universe fit together that mark all of Yaniv's work.\u201d \n               Gaming the system \n             Erlich was inspired to teach himself programming as a child in Israel after seeing the 1983 film  WarGames , in which a teenager accidentally hacks into government computer systems and nearly launches \u201cglobal thermonuclear war\u201d. Erlich thought that he would study maths and physics at university, but after a friend told him that there was a lot of maths in biology, he decided to major in computational neuroscience. In 2006, following his graduation, Erlich moved to the United States to earn his PhD in genetics at Cold Spring Harbor Laboratory in New York. Under his adviser, molecular biologist Greg Hannon, Erlich devised what he called \u201cDNA Sudoku\u201d: a sequencing method that could be used on tens of thousands of specimens analysed simultaneously. It allowed scientists to use computational techniques to find a gene carrying a rare mutation from this mixed batch of DNA and assign it to the right specimen 2 . Erlich is now using the technique to find disease-causing mutations in young Ashkenazi Jews to inform their decisions about potential marriage partners. In 2011, as Erlich was setting up his first independent lab as a Whitehead Fellow, he met a Colorado-based woman, Wendy Kramer, whose son had managed to track down his father \u2014 an anonymous sperm donor \u2014 by searching a consumer-focused genetic-genealogy database for people with DNA similar to his own. Erlich wondered whether a computer program that he had been working on with an undergraduate student, Melissa Gymrek, might enable a similar trick using de-identified genome data from human research studies. The software, called lobSTR, scours sequences and generates a profile of repetitive genetic markers called short tandem repeats (STRs), which are often used in genealogy to identify people. Could Erlich extract STRs from the anonymous data, and then hunt through public genealogy databases for a match and a name? \u201cI had my background in security, and I had lobSTR in hand, and I thought, 'Is this going to affect personal genomes?'\u201d Erlich and his team tested the idea on a man's full genome that had been published in 2007 (ref.  3 ). They used lobSTR to determine the STR profile of the man's Y chromosome, and then searched a consumer genealogy database called Ysearch until they had matches with a few likely surnames. Public records on one of these surnames linked it to a man fitting the geographic location and age listed in the paper: the genomics pioneer J. Craig Venter. Venter had, in fact, already revealed himself as the donor \u2014 one reason Erlich chose that genome was that he thought he could do no harm in revealing Venter's identity. But there was no reason to believe that this process would not work for others. \n               Proof of principle \n             When Erlich submitted his paper to  Science , the reviewers wanted proof that a completely anonymous donor could be identified. So his team extended its analysis to men whose genomes had been sequenced as part of the international 1000 Genomes Project. Extensive information about these men, including their ages and detailed family pedigrees, was available on the website of the Coriell Institute for Medical Research in Camden, New Jersey, which distributes cell lines made from their tissues to researchers. Erlich's team used lobSTR to infer the men's STRs from their 1000 Genomes data, and then searched Y-chromosome databases to find linked last names. After that, it was relatively easy to search public records databases to find men with those last names who were the right age, came from the right place and had similar family trees. The team identified nearly 50 people, including DNA donors and their relatives. When he first saw the results, Erlich said later, he was so shocked at how easily the method worked that he had to go outside and take a walk. Geneticists elsewhere had already revealed security flaws in anonymized genetic data. In 2008, for instance, David Craig, a computational biologist at the Translational Genomics Research Institute in Phoenix, Arizona, reported that he could use information from an individual's DNA sample to confirm whether that person had contributed to a genome-wide association study (GWAS), even if the study reported only summary statistics on hundreds or thousands of participants 4 . This and other studies prompted policy-makers at the US National Institutes of Health (NIH) to pull GWAS data from public databases, and to require investigators to obtain permission to access it. Many researchers resent this move, because it makes it difficult to pool data from different studies. Erlich's study upped the stakes, because it showed that it was possible to identify people from their genetic data by linking not to other sources of research data, but to information freely available on the Internet. He realized that publishing these results might stoke public anger, so he consulted lots of other researchers and ethicists first. \u201cPeople were concerned that the NIH would shut down its databases or that the public would stop donating their material,\u201d says Erlich. He contacted NIH officials about his findings, and met some of them in Bethesda, Maryland, last December. The NIH's National Institute of General Medical Sciences, which funds the Coriell repository, decided to remove the ages of participants from public view. \n               Information withheld \n             When Erlich published the results of his work in January 1 , he revealed no research participants' names. Neither did he spell out all the steps he had taken to find their identities: \u201cThere is an obvious tension, because as a scientist you want to tell everything about how you did the work. On the other hand, you can't do that, because it will expose people's identities to the world,\u201d says Erlich. The question remains of how to handle privacy in future. Removing information after loopholes are revealed \u2014 what some call the whack-a-mole response 5  \u2014 does not seem to satisfy anyone. Some geneticists argue that the public is becoming more accustomed to sharing personal information, and that no harm has ever been done to anyone identified from genetic studies. But many, including Brad Malin a privacy researcher at Vanderbilt University in Nashville, Tennessee, consider that a weak argument. \u201cA lot of people say that because information flows much more freely today than it did 10 years ago, that privacy is dead, and this is certainly not the case,\u201d he says. People still expect some information \u2014 especially health and medical data \u2014 to be private, says Malin. And so far, none of the people identified from anonymous genetic data sets has been named publicly, so it is perhaps too early to say that they are not at risk. Eric Green, director of the US National Human Genome Research Institute (NGHRI) in Bethesda, says that the NIH is trying to balance access and privacy. \u201cOne value is to make the data as widely available and unencumbered as possible, but then you're trading that off against concerns about how data is being used, and maintaining privacy and confidentiality,\u201d he says. \u201cWe're constantly exploring models that put us between those two extremes.\u201d \n               Careful scrutiny \n             Currently, anyone with an Internet connection can access data from the 1000 Genomes Project. Researchers must apply for access to genetic data from most other studies, and must usually submit a new access request for information from each one. That makes it onerous to analyse data from different sources together. Many large data-holders around the world take this approach; the EMBL's European Bioinformatics Institute in Hinxton, UK, for instance, relies on data-access committees to determine what uses of data are appropriate given the consent terms of any particular study. \u201cIt's difficult to imagine how else one would do it, since most of these studies are built around consent agreements,\u201d says Paul Flicek, head of DNA resources at the institute. Some researchers say that genetic data should be deposited with central data-hosting agencies that then grant broad access to trusted users. This would mean that the data would be off-limits to the public, but researchers would not have to ask for permission to access every data set. Laura Rodriguez, director of the NGHRI's division of policy, communications and education, says that NIH committees on data use have concerns about this idea: \u201cWe've seen investigators request access to large swathes of data, and it's clear from their proposed-research statement that they haven't read the use limitations of the data they're requesting.\u201d Erlich argues that genetic data should be broadly available, but that scientists should be more honest about the difficulty of guaranteeing anonymity. Amy McGuire, a lawyer and ethicist at Baylor College of Medicine in Houston, Texas, with whom Erlich consulted on his publication, agrees. But she is not sure that informing people of the risk of re-identification is enough. It may be difficult for someone signing up for a research study to understand all the ways in which their data might be used in the future, let alone to weigh the risks when researchers themselves do not necessarily know them. \u201cThere are challenges to putting so much weight on informed consent,\u201d she says. Scientists should explore further ways to protect research participants, says Erlich, such as encrypting the data before they are deposited, allowing researchers who possess the decryption key to work with them freely without jeopardizing privacy. But Green is concerned that researchers might not be able to work as freely with encrypted data as they can with unencrypted data. There are no simple answers, but researchers give Erlich credit for forcing these issues onto the public stage. Page warns that this could be a double-edged sword for a young scientist: \u201cThis piece of work represents only a slice of Yaniv's broader interests, and the danger could be the risk of being completely consumed by this debate,\u201d he says. Erlich seems happy to be consumed. In a new project that he calls Genetic Epidemiology 2.0, for example, he is working with Daniel MacArthur, a geneticist at Massachusetts General Hospital and Alkes Price, a biostatistician at Harvard School of Public Health, both in Boston, to mine social networks for information that might yield insight into the genetic basis for complex human traits. The project focuses on genealogy-based social networks on which members post extensive family trees \u2014 a potentially rich source of information about inherited traits. Erlich is aware of the ethical complexities of such a study. To start with, he is focusing on public information about deceased people, to minimize the risk that anyone will be harmed by the work. But if the project succeeds, he may go on to ask members of the networks whether they want to upload other types of information \u2014 such as medical records, which could yield insight into a wider range of disease traits. It is a project that plays to Erlich's strengths, says Hannon. \u201cWhen Yaniv says, 'What data is out there?' he doesn't think, 'What data is out there in the literature?' He thinks about what data is out there holistically.\u201d If the technique works, it would use information in the public domain to tackle one of the most difficult problems facing genetic researchers: how to assemble the enormous groups of related individuals needed to illuminate the complex genetic underpinnings of human biology. \u201cYaniv believes nothing is impossible,\u201d says Hannon. Of course, it could expose all kinds of new vulnerabilities. That may not be such a bad thing, says Erlich, harking back to his penetration testing on banks. \u201cAs a client of a US bank, I'm sure you are happy that they undergo these tests. You wouldn't want to say, 'Let's not find something we won't like.'\u201d \n                     Guidance issued for US Internet research 2013-Apr-24 \n                   \n                     HeLa publication brews bioethical storm 2013-Mar-27 \n                   \n                     Gene-analysis firms reach for the cloud 2013-Mar-19 \n                   \n                     Genetic privacy 2013-Jan-17 \n                   \n                     Privacy loophole found in genetic databases 2013-Jan-17 \n                   \n                     Erlich lab \n                   Reprints and Permissions"},
{"file_id": "497022a", "url": "https://www.nature.com/articles/497022a", "year": 2013, "authors": [], "parsed_as_year": "2006_or_before", "body": "In the nearly two decades since they were first commercialized, genetically modified (GM) crops have gained ground on their conventional counterparts. The vast majority are grown in five countries. Four crops feature, with two main traits: herbicide tolerance and insect resistance. See slideshow (below). \n                     Food: The global farm 2010-Jul-28 \n                   \n                     Nature special: GM crops \n                   \n                     Blogpost: Indian parliamentary panel slams GM crops \n                   \n                     Blogpost: BASF abandons GM crop market in Europe \n                   \n                     The International Service for the Acquisition of Agri-biotech Applications \n                   Reprints and Permissions"},
{"file_id": "497176a", "url": "https://www.nature.com/articles/497176a", "year": 2013, "authors": [{"name": "Roberta Kwok"}], "parsed_as_year": "2006_or_before", "body": "Prosthetic arms are getting ever more sophisticated. Now they just need a sense of touch. Sitting motionless in her wheelchair, paralysed from the neck down by a stroke, Cathy Hutchinson seems to take no notice of the cable rising from the top of her head through her curly dark hair. Instead, she stares intently at a bottle sitting on the table in front of her, a straw protruding from the top. Her gaze never wavers as she mentally guides a robot arm beside her to reach across the table, close its grippers around the bottle, then slowly lift the vessel towards her mouth. Only when she finally manages to take a sip does her face relax into a luminous smile. This video of 58-year-old Hutchinson illustrates the strides being taken in brain-controlled prosthetics 1 . Over the past 15 years, researchers have shown that a rat can make a robotic arm push a lever 2 , a monkey can play a video game 3  and a person with quadriplegia \u2014 Hutchinson \u2014 can sip from a bottle of coffee 1 , all by simply thinking about the action. Improvements in prosthetic limbs have been equally dramatic, with devices now able to move individual fingers and bend at more than two dozen joints. But Hutchinson's focused stare in that video also illustrates the one crucial feature still missing from prosthetics. Her eyes could tell her where the arm was, but she could not feel what it was doing. Nor could she sense when the grippers touched the bottle, or whether it was slipping out of their grasp. Without this type of sensory feedback, even the simplest actions can be slow and clumsy, as Igor Spetic of Madison, Ohio, knows well. Fitted with a prosthetic after his right hand was crushed in an industrial accident in 2010, Spetic describes breaking dishes, grabbing fruit too hard and bruising it and dropping a can when trying to pick it up at the local shop. Having a sense of touch would be \u201ctremendous\u201d, he says. \u201cIt'd be one step closer to having the hand back.\u201d Prosthetics researchers are now trying to grant him that wish by creating prosthetics that can 'feel' more like the real thing. It is a daunting task: researchers have managed to read signals from the brain; now they must write information into the nervous system. Touch encompasses a complicated mix of information \u2014 everything from the soft prickliness of a woollen top to the slipping of a sweaty soft-drink can. The research is still in its infancy, with approaches that range from stimulating nerves in a stump and re-routing nerves to other parts of the body, to tapping directly into the brain (see 'Closing the loop'). But \u201cit's probably the next big thing that has to happen\u201d, says Robert Kirsch, a biomedical engineer at Case Western Reserve University in Cleveland, Ohio. \n               Alternative senses \n             Conventional prosthetics are not devoid of feedback. The widely used 'split-hook' hand replacement, for instance, typically has a harness that lets the user open and close the device by moving another part of the body, such as the opposite shoulder; patients then feel resistance in the harness when they grab something. Likewise, users of motorized prosthetics \u2014 which are controlled by electrical signals from muscles in the stump \u2014 will feel pressure on the stump when they push something, or may hear subtle changes in the motor's sound when grabbing an object. Researchers have even tried to introduce such feedback deliberately, through vibrations, air pressure and electrical stimulation. But none of these sensations feels natural \u2014 which may be one reason that many people reject prosthetic limbs: the replacement just doesn't seem like part of the body. Recreating life-like sensation is a tall order. The sensations arise from a host of receptors in the skin, which detect texture, vibration, pain, temperature and shape, as well as from receptors in the muscles, joints and tendons that contribute to 'proprioception' \u2014 the sense of where a limb is in space. Prosthetics are being outfitted with sensors that can gather many of these sensations, but the challenge is to get the resulting signals flowing to the correct part of the brain. For people who, like Spetic, have had limbs amputated, the obvious way to achieve that is to route the signals into the remaining nerves in the limb's stump. Researchers including Ken Horch, a neuroprosthetics researcher at the University of Utah in Salt Lake City, have done just that by threading electrodes into the nerves in stumps then stimulating them with a tiny current, so that patients felt like their fingers were moving or being touched 4 . The technique can even allow patients to distinguish basic features of objects: a man who had lost his lower arms was able to determine the difference between blocks made of wood or foam rubber by using a sensor-equipped prosthetic hand. He correctly identified the objects' size and softness more than twice as often as would have been expected by chance 5 . Information about force and finger position was delivered from the prosthetic to a computer, which prompted stimulation of electrodes implanted in his upper-arm nerves. Researchers at Florida International University in Miami are now working to build an implantable device using the technique. But some researchers worry that implanting electrodes directly into a nerve could damage it. Dustin Tyler, a biomedical engineer at Case Western Reserve University, and his colleagues have therefore developed a cuff-like electrode that encircles the nerve. \u201cWe want to get access to as much of the nerve as we can without actually penetrating into it,\u201d says Tyler. The researchers showed that by running current to the cuffs in cats, they could precisely activate nerves and make the animals move their feet in specific directions 6 . They are now trying to stimulate nerves that carry sensory information. Their first patient \u2014 Spetic \u2014 had the cuffs implanted in his forearm last May, says Tyler, and can feel \u201cvery natural sensation\u201d at multiple spots. The team is now testing the electrodes in a second patient. \n               Complex sensation \n             As promising as such results are, researchers will probably need to stimulate hundreds or thousands of nerve fibres to create complex sensations, and they will need to keep the devices working for many years if they are to minimize the number of surgeries required to replace them. So some researchers are instead trying to give patients sensory feedback by touching their skin. This technique was discovered by accident in 2002, when a group led by Todd Kuiken, director of the Center for Bionic Medicine at the Rehabilitation Institute of Chicago in Illinois, was testing a way to improve patients' control of their prosthetic limbs. The idea was to rewire arm nerves that used to serve the hand, for example, to muscles in other parts of the body. When the patient thought about closing his or her hand, the newly targeted muscle would contract and generate an electric signal, driving movement of the prosthetic. The first patient to receive this 'targeted reinnervation' therapy was Jesse Sullivan, a power-line engineer who had lost both arms from electrical burns. After his arm nerves were re-routed to his chest muscles, Sullivan could operate a prosthetic hand just by thinking about the actions. But to everyone's surprise, he also began to feel as though his missing hand was being touched when his chest was touched. It turned out that the re-routed nerves had grown into the chest skin, and his brain was interpreting the sensory signals as coming from his hand. Some parts of his chest felt like his palm, whereas others felt like his fingers or forearm 7 . The results raised the possibility that sensory information from a prosthetic could be delivered to a device that pushes different parts of the skin. For people who have had targeted reinnervation surgery, pressure on the newly wired skin would then trigger sensations of touch in the missing hand. The technique isn't perfect: the parts of the hand don't map neatly onto the reinnervated skin, and each patient has a different map. And delivering detailed sensory information from a prosthetic can be challenging, as the area for stimulation is limited to a small patch of skin. But even so, one of Kuiken's former colleagues is working with the company HDT Robotics in Evanston, Illinois, to make such a device, and Kuiken plans to develop one as well. \n               Direct hit \n             None of these techniques will work for people who, like Hutchinson, have had a stroke, or received spinal-cord injuries that severed the nerve pathways from the limbs to the brain. So some researchers are skipping directly to the brain. In principle, this should be straightforward. Because signals from specific parts of the body go to specific parts of the brain, scientists should be able to create sensations of touch or proprioception in the limb by directly activating the neurons that normally receive those signals. This is immensely difficult to do, however, because scientists still have an incomplete knowledge of which neurons those are. Researchers therefore have two options: identify and mimic the natural signals, or make the brain learn a new set. A team led by Sliman Bensmaia, a neuroscientist at the University of Chicago, is taking the first approach. In one study, the researchers repeatedly poked monkeys in two spots on the hand and trained the animals to move their eyes left or right, depending on whether the second poke was to the left or right of the first one. They then placed electrodes in the monkeys' brains and mapped which parts of the brain responded as they touched the different spots. Next, the researchers simulated a poke by sending current to the neurons that had been activated when the monkey was touched on the little finger. The animals moved their eyes as if their finger had actually been poked, says Bensmaia, who reported the results at the Society for Neuroscience's 2012 meeting in New Orleans, Louisiana. Miguel Nicolelis, a neuroscientist at Duke University School of Medicine in Durham, North Carolina, favours the second approach. He and his colleagues trained monkeys to direct a virtual hand around a computer screen \u2014 and to touch on-screen objects \u2014 using their thoughts alone 8 . When the hand rubbed a 'rough' object, the team sent low-frequency electrical pulses to the monkey's brain; for a 'smooth' object, it sent a high-frequency signal. Over time, the monkey learned to pick the right object from the frequency of the signal its brain received, and could essentially 'feel' the objects on screen. Nicolelis hopes that he can use the same tactic in people with prosthetics. But neither he nor Bensmaia know whether the monkeys they tested felt poking, roughness or some other sensation such as tingling. \u201cThey are feeling something for sure,\u201d Nicolelis says, \u201cbut what exactly they feel is inside their heads.\u201d Irrespective of which signals are used, scientists will need a more finely tuned technique to deliver them. With electrical stimulation, all neurons close to the electrode's tip are activated indiscriminately, so \u201ceven if I had the sharpest needle in the Universe\u201d, that could create unintended effects, says Arto Nurmikko, a neuroengineer at Brown University in Providence, Rhode Island. For example, an attempt to create sensation in one finger might produce sensation in other parts of the hand as well, he says. Nurmikko and other researchers are therefore using light, in place of electricity, to activate highly specific groups of neurons and recreate a sense of touch. They first used a technique called optogenetics to express genes for light-sensitive proteins in parts of a monkey's brain that receive tactile information from the hand. They then trained the monkey to remove its hand from a pad when the device vibrated. When the team then stimulated the brain with a light source implanted in the animal's skull, the monkey lifted its hand off the pad about 90% of the time, according to results reported at the Society for Neuroscience meeting. The use of such techniques in humans is still probably 10\u201320 years away, says Bensmaia, but it is a promising strategy. \n               Approximations \n             Even if such techniques can be made to work, it is unclear how closely they will approximate natural sensations. Tingles, pokes and vibrations are still a far cry from the complicated sensations that we feel when closing a hand over an apple, or running a finger along a table's edge. But patients don't need a perfect sense of touch, says Douglas Weber, a bioengineer at the University of Pittsburgh in Pennsylvania. Simply having enough feedback to improve their control of grasp could help people to perform tasks such as picking up a glass of water, he explains. Patients who wear cochlear implants, for example, are often happy to regain enough hearing to hold a phone conversation, even if they are still unable to distinguish musical subtleties. One of the most sophisticated devices to include sensory feedback is a prosthetic arm developed by researchers at the Johns Hopkins University Applied Physics Laboratory in Laurel, Maryland. Built as part of a US Department of Defense research programme that has spent US$144 million since 2006 to improve prosthetics for injured soldiers returning from Iraq and Afghanistan, the arm is equipped with more than 100 sensors that detect sensations ranging from pressure to temperature. Scientists at the University of Pittsburgh and the California Institute of Technology in Pasadena are seeking regulatory approval to use brain stimulation to deliver sensory feedback from the prosthetic limb to patients. Spetic, for one, can hardly wait to get hold of a prosthetic hand with a sense of touch. \u201cI'd probably lay everything on the countertop and just start grabbing stuff,\u201d he says. \u201cI'd be so excited I wouldn't even know where to begin.\u201d \n                     FDA Approves First Retinal Implant 2013-Feb-15 \n                   \n                     Mind-controlled robot arms show promise 2012-May-16 \n                   \n                     Monkey brains 'feel' virtual objects 2011-Oct-05 \n                   \n                     Artificial skins detect the gentlest touch 2010-Sep-12 \n                   \n                     Neuroprosthetics: In search of the sixth sense 2006-Jul-12 \n                   \n                     Biotechnology@nature.com \n                   \n                     DARPA's Revolutionizing prosthetics \n                   \n                     Targeted reinnervation \n                   \n                     Johns Hopkins University Applied Physics Laboratory's modular prosthetic limb \n                   Reprints and Permissions"},
{"file_id": "497021a", "url": "https://www.nature.com/articles/497021a", "year": 2013, "authors": [], "parsed_as_year": "2006_or_before", "body": "Genetically modified crops generate hype and hatred. A special section of  Nature  cuts through the drama. Foreign genes were successfully introduced into plants for the first time 30 years ago (see  page 40 ). Ever since, genetically modified (GM) crops have promised to deliver a second green revolution: a wealth of enhanced foods, fuels and fibres that would feed the starving, deliver profits to farmers and promote a greener environment. In many ways, that revolution has arrived. Crops engineered to carry useful traits now grow on 170 million hectares in at least 28 countries (see  page 22 ). But to many, GM crops have been a failure. The market is dominated by just a few insect-resistant and herbicide-tolerant crops. The environmental benefits are disputed, and activists question the safety of GM foods. Politicized and polarized, the war of words that surrounds GM crops ignores the complex truths. In this special issue,  Nature  explores the messy middle ground. A News Feature weighs the evidence behind some of the most controversial claims about the effects of GM crops (see  page 24 ). Christopher Whitty, chief scientific adviser at the UK Department for International Development, and his colleagues argue that the negative attitudes towards GM crops in the developed world undermine the technology's potential in the developing one (see  page 31 ). Such sentiments have helped to delay the approval of the first genetically modified animal for human consumption, a fast-growing salmon (see  page 17 ). The next generation of GM crops might benefit from these hard lessons. Fusuo Zhang, director of the Center for Resources, Environment and Food Security at China Agricultural University in Beijing, thinks that his country \u2014 now the sixth-largest adopter of GM crops \u2014 will serve as a hothouse for agricultural technologies (see  page 33 ). A Perspective article reviews research on membrane transporters in plants that could lead to traits such as stress resistance and increased nutrient content (see  page 60 ). And a second News Feature explores the genetic engineering technologies that are giving rise to the next generation of GM crops (see  page 27 ). The battles are by no means over, but the hope is that science and reasoned debate can inform the future of these technologies. \n                     Hyped GM maize study faces growing scrutiny 2012-Oct-10 \n                   \n                     Transgenic grass skirts regulators 2011-Jul-20 \n                   \n                     Grants aim to fight malnutrition 2011-Apr-14 \n                   \n                     Food: Inside the hothouses of industry 2010-Jul-28 \n                   \n                     Nature special: GM crops \n                   Reprints and Permissions"},
{"file_id": "494166a", "url": "https://www.nature.com/articles/494166a", "year": 2013, "authors": [{"name": "David Cyranoski"}], "parsed_as_year": "2006_or_before", "body": "By offering unproven therapies, a Texas biotechnology firm has sparked a bitter debate about how stem cells should be regulated. Ann McFarlane is losing faith. In the first half of 2012, the Houston resident received four infusions of adult stem cells grown from her own fat. McFarlane has multiple sclerosis (MS), and had heard that others with the inflammatory disease had experienced improvements in mobility and balance after treatment. The infusions \u2014 which have cost her about US$32,000 so far \u2014 didn't help, but she knew that there were no guarantees. It is McFarlane's experience with Celltex Therapeutics, the company that administered the cells, that bothers her. She was told that she had been enrolled in a study to test the cells' efficacy, but received almost no information about it. And although it wasn't exactly a secret that the treatment had not been approved by the US Food and Drug Administration (FDA), Celltex, based in Houston, Texas, assured its clients that it was within its rights to provide it. But Celltex was forced to halt treatments in October, and in November a legal battle broke out over who owned the cells still being stored by the company. For weeks, McFarlane was uncertain whether her cells were being grown and stored properly. Although Celltex has told its customers that it has settled the dispute, McFarlane has her doubts. \u201cI am not confident that the cells are viable and safe,\u201d she says. \u201cI probably will not feel comfortable using these cells.\u201d For the past decade, people such as McFarlane have searched far and wide for clinics offering to deliver on the promise of adult stem cells. Unlike embryonic stem cells, their use does not require the controversial destruction of an embryo. Yet although adult stem cells are claimed to ameliorate a wide range of disorders, they have not yet been shown to do so conclusively in clinical trials in the United States. Relying on customer testimonies and company promises, patients have travelled to clinics in places such as China, Costa Rica, Mexico and Japan to receive them from unregulated, often unaccredited, laboratories, driving a boom in stem-cell tourism. According to Leigh Turner, a bioethicist at the University of Minnesota in Minneapolis, at least ten clinics offer treatments in the United States. Turner and others have questioned the quality of the cells that these firms provide, and several outlets have been forced to stop providing treatments. CellTex has been one of the most visible. Established in 2011 (see \u2018Texas throwdown\u2019), it offered therapies for conditions as varied as arthritis, back pain, MS and Parkinson's disease. It produced its cells in a 1,400-square-metre, state-of-the-art facility in Sugar Land, Texas, that was registered with the FDA, and \u2014 the company claimed \u2014 had strict quality control. Although the company flouted federal regulations, which deem such cell therapies to be biological drugs, it adhered to state rules, which had recently been tailored to bolster the stem-cell industry. But even with the support of Texas governor Rick Perry \u2014 Celltex's first patient \u2014 the company had to stop offering treatments in the United States. \n               boxed-text \n             Throughout the ordeal, Celltex has stirred up a polarizing public debate. Channelling the state's ethos of rugged individualism, Stanley Jones, the orthopaedic and cosmetic surgeon who founded Celltex, argued that the federal government was overstepping its bounds. The company's therapies, he said, are not biological drugs, but tissue transplants, which typically do not require clinical trials to prove that they work. The drama raised and dashed the hopes of patients, many of whom rallied to support Celltex. Now, some in the industry worry that the turmoil will tar the entire field. \u201cIt doesn't serve the interests of patients or the medical community at large,\u201d says Gil Van Bokkelen, head of Athersys, a biotech firm in Cleveland, Ohio, that has been cooperating with the FDA to test stem-cell treatments in clinical trials. Celltex has declined to comment on its business decisions, but it has been promising its patients a reboot, saying that it will publish the results of preliminary studies; start the FDA drug approval process so that it can resume treatments in the United States; and make its stem-cell treatments available in Mexico (see Nature http://doi.org/kdd;2013 ). Although many former patients continue to support the company's endeavours, some, such as McFarlane, see the firm as an unfortunate distraction. \u201cAny bad stem-cell PR is detrimental to the overall picture, which is positive,\u201d she says. \n               Home-grown therapy \n             In an earnest Southern drawl, Jones recounted how he co-founded Celltex. He was one of the headline speakers at the first Houston Stem Cell Summit at the Houstonian Hotel, a plush resort surrounded by 7 hectares of woods. Jones said that he experienced the miracle of stem cells first-hand in 2010 when he travelled to Japan to receive infusions to treat his crippling arthritis. His cells came from RNL Bio, a stem-cell company based in Seoul. Within three months, he said, he could do pilates and work as hard as he had in his youth. But it irked Jones that he and others had to board a plane to get better. So, with the help of David Eller, a former executive of DuPont in Europe and a friend of governor Perry, he gathered the $30 million he needed to gain exclusive rights from RNL to use its stem-cell processing and banking technology in the United States. David Cyranoski discusses the state of Celltex in the aftermath of an FDA inspection. With no background in stem cells, \u201cI had to start from scratch\u201d, Jones recalled. \u201cI knew that I was the only game in town. So I learned to do liposuction, and I started doing liposuction on anybody that wanted stem cells.\u201d Perry, a charismatic politician and one-time presidential hopeful, became Jones' first patient in July 2011, seeking treatment for his recurring back pain. Jones said that he tried to maintain a low profile for the company \u2014 not even establishing a website for months, relying instead on personal recommendations \u2014 but attention soon followed. Operating in a state that has historically bristled under big government, Celltex appealed not only to desperate patients but also to the ideals of the political and religious right and their opposition to big government and embryonic stem-cell research. It campaigned for its cause at intimate 'educational forums' at the Houstonian, arguing that it could already provide ethically acceptable treatments with adult stem cells. Patients spread the word at churches, and bloggers praised the company for asserting patients' rights. When prospective clients showed up in his office, Jones would tell them of his experience; at times, he reportedly wept. By October 2012, 233 people had been treated with Celltex stem cells. Celltex offered treatments with 'mesenchymal stem cells' (MSCs), which are found in bone marrow, muscle, fat and other tissue. In the procedure licensed from RNL, the cells are extracted from just five grams or so of fatty tissue, then cultured, expanded and banked. Doctors could then administer the cultured stem-cell concoction intravenously. RNL developed its method for isolating and culturing MSCs in 2006, and within four years, 7,000 patients had received its cells \u2014 for disorders including stroke, renal failure, asthma, psoriasis and MS. The recoveries \u2014 recounted in anecdotes on its website and in the book  The Grace of Stem Cells  by RNL chairman Jeong-Chan Ra (Tate Publishing, 2012) \u2014 are often complete and miraculous. Some patients have the treatment for fatigue, to remove wrinkles or for \u201crestoring the skin's elasticity\u201d, Ra writes in his book. But even in South Korea, which seems to have approved more adult-stem-cell treatments than any other country, none of RNL's treatments has been approved. That is why Jones and others had to travel to Japan or China to receive them. MSCs do have therapeutic potential. Research over the past ten years has suggested that they home in on damaged or inflamed tissues and have various roles in repair, releasing molecules that suppress an overactive immune system, stabilize newly forming blood vessels or prevent cells from dying 1 . Initial fears that they might take hold and become cancerous subsided with evidence that they disappear after a short time in the body 2 . There have been reports of complications, such as \u201ccatastrophic demyelinating encephalomyelitis\u201d, a life-threatening inflammation of the brain 3 . But there is a growing consensus that MSCs are safe 4 . Arnold Caplan from Case Western Reserve University in Cleveland, Ohio, did pioneering work on MSCs and refers to their action as \u201chit and run\u201d healing. More than 250 MSC trials are ongoing or completed, he told the audience at the October summit. And even though most of these studies are small and none has led to regulatory approval for widespread use in the United States or Europe, Caplan says that he would use the cells. \u201cIf I had MS, I would be getting this therapy. I'd probably go offshore.\u201d That is not a view endorsed by the International Society for Stem Cell Research in Skokie, Illinois, however, which urges prospective patients to be cautious about stem-cell tourism. Safety also depends on how cells are handled, and the FDA has voiced concerns about Celltex's operations. In April 2012, an FDA inspection found a laundry list of problems \u2014 79 in all \u2014 at the Sugar Land plant that the firm ran with RNL. The list included failure to confirm that equipment was sterile, and operating table-top centrifuges and incubators on the floor. Labels on one bottle said different things in Korean and English and one product used in the culture media was marked \u201cnot intended for human or animal diagnostic or therapeutic uses\u201d. Celltex blamed a \u201clanguage barrier\u201d. In a formal statement, it said it was maintaining an \u201copen line of communication\u201d and a \u201ccooperative relationship\u201d with the FDA and that it would address the problems. \u201cCelltex and its partner RNL Bio process stem cells in a safe, sterile laboratory with procedures that ensure cell viability and integrity,\u201d the company said. But the FDA also found fault with the therapy itself \u2014 in particular, it challenged Jones's claim that the procedure qualifies as a treatment with a patient's own cells and therefore falls outside FDA jurisdiction. To qualify as such, the cells would have to be \u201cminimally manipulated\u201d and be implanted for \u201chomologous use\u201d, meaning that they carry out the same functions in the treated tissue as they do in the tissue from which they are extracted. On 24 September, the FDA sent a warning letter to Celltex saying that because the firm's processing \u201calters the original relevant characteristics\u201d of the fat tissue, the cells are not considered minimally manipulated. And because the fat-derived MSCs were used to treat problems in other tissue, such as nerve tissue in MS, it does not count as homologous use. The FDA demanded that Celltex comply and show that it had fixed all the manufacturing problems and that it had approval to use the stem cells, or face regulatory action, which could include seizure of the facility or an injunction. Celltex responded to the FDA on 16 October, again arguing that the preparation \u201cdoes not alter the relevant biological characteristics of the MSCs\u201d. It also claimed that MSCs are known to spur the growth of blood vessels, regulate the immune response and dampen inflammation, and that those are presumably the roles the cells carry out in both the transplant and the original fat tissue. Peter Connick of the University of Cambridge, UK, who is running clinical studies with MSCs, says that he suspects that the FDA is correct that MSCs function differently in different tissue. \u201cThe ability of MSCs to instruct neural stem-cell fate decisions, for example, is by definition not something that they could do in their bone-marrow niche,\u201d he says. On 25 October, Celltex notified its customers that it had to halt treatments while it prepared to start an FDA-approved clinical trial. The message was apologetic, but cast the blame on the FDA. \u201cI know that many of you are deeply hurting due to the FDA's decision to regulate your own stem cells as a drug,\u201d it read. That sentiment was reflected at the October summit, which occurred ten days later. Sponsored by Celltex, the conference had attracted representatives from many of the country's most promising stem-cell companies and banks \u2014 including those that have been working through the FDA approval process. Perry spoke, praising the \u201cwildcat spirit\u201d in Texas and bemoaning the FDA's \u201ccrippling bureaucratic infrastructure\u201d. Lawyers and ethicists debated the need for regulations on stem cells alongside patients, some of whom were wheelchair-bound. \u201cThe FDA is trying to screw you,\u201d one participant told Eller during a coffee break. Yet many in the industry back the FDA's position. Van Bokkelen of Athersys was the previous head of the Alliance for Regenerative Medicine in Washington DC, which represents the interests of 100 or so US companies, many of which, like Athersys, have stem-cell treatments undergoing FDA-approved clinical trials. He says that stem-cell treatments may require changes in existing regulations, but that he hopes to work these out collaboratively with the FDA. \u201cRather than coming at them with pitchforks, I wanted to engage in dialogue to come up with ways to explore areas of uncertainty,\u201d he says. \n               Frustrated patients \n             Mary Pat Moyer, the founder of INCELL, a life-science services company based in San Antonio, Texas, started her presentation at the October summit with a slide that criticized the \u201cstem-cell cowboy\u201d mentality that Celltex has espoused. Frustrated Celltex customers had contacted her about storing their cells in her repository (she refused), and she says that she was surprised to hear how little data the company had collected on patients from ongoing tests. \u201cIt's frustrating. If they have so many patients, they should provide some guidance\u201d in terms of what dosing works best for each disease. Despite being part of a study, McFarlane notes that Celltex didn't ask her about her progress on her treatment until January this year. She had hounded the company for more information about her treatments. Celltex showed, she says, a \u201ctotal lack of follow-up or investigation\u201d. Many researchers are pursuing more rigorous trials. For example, Connick is leading a clinical trial of stem cells to treat MS in which he is using various neurological tests as well as techniques to show improvement in the neurons. Connick says that MSC therapy seems to be \u201cfeasible, safe and may slow the neuroaxonal loss\u201d that leads to the progressive disability in MS 5 . He does concede, however, that \u201cquestions remain about the long-term safety and long-term efficacy\u201d, and that he would not advise patients to try the treatment outside of clinical trials. In November 2012, Celltex's troubles deepened. RNL Bio and Celltex sued each other over how much money Celltex needed to pay for the services that RNL was providing at the Sugar Land plant. Celltex had to file a restraining order to get access to the cells. But in the months since the company halted treatments, Celltex has said that it is making overtures to the FDA, and that it will now collect information from its clinical studies. In January, it told patients that a clinical research organization was evaluating data and that patients should expect a follow-up call about their conditions. It also told its patients that it would start approved clinical trials \u201cshortly after\u201d a meeting with the FDA in March. And it announced that it would be providing treatments through physicians in Mexico. Most patients, meanwhile, are frustrated by the delays. Debbie Bertrand, who has MS, said that, before going to Celltex, she was turned away from five approved clinical trials because her symptoms weren't severe enough to qualify. \u201cI don't want to wait till I'm bad enough,\u201d she says. And many are mistrustful of the FDA, suspecting that it is in cahoots with big drug companies to peddle ineffective treatments. \u201cI have tried all of the FDA-approved MS drugs, which had horrible life-threatening side effects,\u201d says Sammy Jo Wilkinson, an MS patient and Celltex client who gave a moving account of her ordeal at the October summit. Other companies now seem poised to fill the void left by Celltex, albeit less brazenly. According to Turner, who has followed closely the proliferation of companies providing such treatments in the United States, \u201cbusinesses are becoming more cautious about the language they use on their websites. Many of them now emphasize that they engage in no more than minimal manipulation of stem cells before administering them.\u201d He adds, \u201cIn my opinion, the reason the FDA's letter to Celltex does not seem to have had a major deterrent effect on other companies is that marketing and administering autologous stem cells is so profitable.\u201d McFarlane, for one, still hopes to find a stem-cell therapy that works for her. \u201cEven though I'm disenchanted with Celltex, I still hold out great hope for adult stem-cell treatment in general. I personally have not been helped \u2026 but I have seen some amazing results for others.\u201d \n                 See Editorial \n                 page 147 \n               \n                     Preventive therapy 2013-Feb-13 \n                   \n                     Controversial stem-cell company moves treatment out of the United States 2013-Jan-30 \n                   \n                     Korea okays stem cell therapies despite limited peer-reviewed data 2012-Mar-06 \n                   \n                     Controversial bioethicist quits stem-cell company 2012-Mar-01 \n                   \n                     Stem-cell therapy takes off in Texas 2012-Feb-29 \n                   \n                     Texas prepares to fight for stem cells 2011-Sep-20 \n                   \n                     Stem-cell scientists grapple with clinics 2011-Jun-28 \n                   \n                     Korean deaths spark inquiry 2010-Nov-23 \n                   \n                     Blogpost: Texas stem-cell provider under FDA gun \n                   \n                     Blogpost: US drug regulator audits Texas stem-cell company \n                   \n                     A closer look at stem-cell treatments from the ISSCR \n                   Reprints and Permissions"},
{"file_id": "495156a", "url": "https://www.nature.com/articles/495156a", "year": 2013, "authors": [{"name": "Eric Hand"}], "parsed_as_year": "2006_or_before", "body": "After years of delays and cost overruns, an international collaboration is finally inaugurating the world's highest-altitude radio telescope. The car toils upwards along the sinuous road, its engine tuned for the thin air. The clumps of cactus and grass along the road soon give way to bone-dry lifelessness. By the time the car reaches 4,000 metres above sea level, Pierre Cox has a bit of a headache. By the time it reaches the 5,000-metre-high Chajnantor plateau \u2014 one of the highest, driest places on Earth, and one of the best for astronomy \u2014 the altitude is affecting his bladder. Cox, the incoming director of the Atacama Large Millimeter/submillimeter Array (ALMA) in Chile, is about to glimpse the giant telescope dishes he will soon be responsible for. But first he must find a toilet. Cox slides out of the car and staggers into ALMA's glass and steel operations centre. The current director, Thijs de Graauw, a trim 71-year-old Dutchman, follows Cox inside and sits down. For him, journeys like this occur weekly \u2014 if not daily \u2014 but he knows that they are no joke. First-timers get a mandatory medical screening before being allowed up to the plateau, and regular shift workers pad around the building with tubes in their noses and oxygen tanks on their backs. \u201cEveryone okay?\u201d De Graauw asks the group of astronomers who have accompanied Cox to ALMA on this December day. \u201cNo victims yet?\u201d Cox re-emerges from the toilet, puts on wraparound sunglasses and, slightly dizzy, heads outside with the group. Scattered across the surrounding plain of brown volcanic soil are dozens of huge white radio antennas, looking as out of place as the stone statues on Easter Island. High on this cold and lonely plateau, they are gathering photons from the cold and lonely parts of the Universe \u2014 the dimly glowing clouds of dust and gas where stars are born. Their signals are then combined into images that have a resolution better than that of the Hubble Space Telescope. The stillness of the tableau breaks as the dishes begin to tilt and swivel in unison. \u201cMy goodness,\u201d says Cox, hushed by the sight of so much metal moving so quickly and quietly. But the choreography is not quite uniform. Clustered tightly in the middle of the array are 12 dishes, each 7 metres across, and four 12-metre dishes, from Japan. Spaced farther out are 25 dishes, each 12 metres across and fitted together like pie slices, from the United States. And scattered among those are the first of 25 dishes from Europe, each 12 metres across \u2014 top-of-the-line carbon-fibre devices pivoting on silky-smooth gearing. The last of those European antennas will not be installed until the end of 2013, when ALMA will finally reach its full complement of 66 dishes. Rather than wait until then, however, the project held a formal inauguration ceremony on 13 March to celebrate the collaboration that made it all possible. A total of 19 countries have contributed to ALMA, through three primary partners: the European Southern Observatory (ESO); the National Astronomical Observatory of Japan; and the US National Radio Astronomy Observatory (NRAO) in Charlottesville, Virginia, funded by the US National Science Foundation (NSF). Geoff Brumfiel talks to Eric Hand about his 5,000-metre ascent to ALMA. Less celebrated have been the difficulties of keeping this unwieldy confederation on track \u2014 with power shared among three independent organizations that have different cultures and norms. Nor is anyone likely to cheer about how the lack of unity caused the US$1.4-billion project to come in several years late, well over cost and downsized from its original ambitions. Its successive directors have had to be diplomats and negotiators as much as scientists. But ALMA is not unique in that respect. International mega-projects are becoming increasingly common in astronomy. Witness the Square Kilometre Array, a proposal to build 3,000 radio dishes with a total collecting area approaching 1 square kilometre in Australia and South Africa (see  Nature   484 , 154; 2012 ). As costs for such ambitious projects cross the billion-dollar threshold, nations are finding that they cannot go it alone \u2014 a situation for which ALMA might serve as a valuable object lesson. \u201cI think it's the largest science project ever where nobody was in charge,\u201d says Ethan Schreier, president of Associated Universities Incorporated (AUI), a radio-astronomy research-management company based in Washington DC, which operates the NRAO. \u201cBut we have made it work.\u201d \n               Family ties \n             Each of the three primary partners came to Chile in its own way, with pilot projects dating from the 1980s. Of particular interest for Europe was the infrared glow from the dust that shrouds many of the Universe's first galaxies. This glow can be used to estimate the size, brightness and number of stars hidden within \u2014 key questions for astronomers trying to piece together the history of galaxy formation. Shifted to longer wavelengths by the expansion of the Universe, this glow reaches telescopes on Earth as millimetre-wave radiation \u2014 and can be detected day or night, as long as there isn't much atmospheric water vapour in the way. To get at the earliest (and thus most distant and faint) of these dusty galaxies, European astronomers needed a large collecting area. They proposed an array of 16-metre dishes on the salt flats of the Atacama Desert, more than 1 kilometre lower than the Chajnantor plateau. US astronomers were more interested in star formation within our Galaxy, and wanted the better image quality that would come with an array of 8-metre dishes placed more closely together. They also wanted to push into the shorter wavelengths of the unexplored submillimetre band, where they could study chemical-emission lines from molecules in interstellar gas clouds. They pushed hard for Chajnantor, which was high and dry enough for submillimetre observations, and flat enough for the dishes to be moved into various configurations. Pooling resources was an obvious move for the two projects, and in 1997, ESO director Riccardo Giacconi and NRAO director Paul Vanden Bout signed a joint resolution to pursue a compromise \u2014 a facility of 64 dishes, each 12 metres across. \u201cRiccardo and I signed this document with no authority whatsoever,\u201d says Vanden Bout. The official backing from the ESO and the NSF wouldn't come for another six years. Japan joined the partnership in 2004 and committed to building 16 dishes in the centre of the array. The more widely spaced US and European dishes would provide high-resolution detail in a narrow field of view. But the compact array would give a more complete view of large objects such as the Galactic Centre or the sprawling, dusty clouds where the Milky Way forms its new stars. The patchworked nature of ALMA's creation is reflected in its organizational structure. Partner agencies have been loath to relinquish control over budgets (or anything else), so the coordinating body that manages array operations \u2014 the Joint ALMA Observatory (JAO) \u2014 has no formal authority. For example, when Chile created a science preserve on the Chajnantor plateau (in return for 10% of ALMA's observing time), officials signed the lease with AUI and the ESO, not the JAO. \n               Cultural sensitivity \n             ALMA directors quickly learn that management works best through persuasion, not proclamation. \u201cYou have to seduce,\u201d de Graauw says. Cultural sensitivity is also required. On conference calls, de Graauw says, his Japanese colleagues would say nothing until he solicited them directly for comments. Alison Peck, deputy project scientist for ALMA, learned a similar lesson about setting deadlines. \u201cIn Japan, it's really not okay to miss a deadline,\u201d she says. \u201cIn the United States, you can usually make reasonable excuses and ask for an extension. In Europe they worry about it even less.\u201d ALMA's motley nature is apparent even in the 12-metre telescope dishes, the array's biggest single cost. From the beginning, the technical requirements were \u201ctruly daunting\u201d, says Tony Beasley, a former ALMA project manager and current head of the NRAO. Each dish needed a motor that could accurately point at celestial targets to within 0.6 arcseconds (about the same apparent size as a bacterium at arm's length); a reflecting surface with an accuracy of 25 micrometres (about one-quarter of the width of a human hair); and structural materials that could maintain that precision in the face of Chajnantor's wicked winds and subzero temperatures. The cheapest way to meet those requirements would have been for the ESO and the NRAO to share a single design and a single contractor. But the NRAO went with a small US firm \u2014 Vertex, which was later bought by General Dynamics of Falls Church, Virginia \u2014 and the ESO held out for a European consortium led by Thales Alenia Space, based in Paris. The delays associated with going to separate contracts came just as prices for commodities such as steel were rocketing because of demand in China, leading to a dramatic escalation in ALMA's cost. As a result, in 2005, the project was 'descoped' \u2014 the NRAO and the ESO would each contribute only 25 antennas rather than 32, resulting in a loss in array sensitivity (see  Nature   439 , 526\u2013528; 2006 ). Even with the descope, US and European contributions to ALMA would grow from $650 million to $1 billion. Japan, meanwhile, had contracted its dishes to Mitsubishi Electric, based in Tokyo. The three companies maintained separate assembly sites at the ALMA operations support facility (OSF), a cluster of buildings where most staff members live and work. (The OSF was built at 2,900 metres, in part because it costs less to hire Chilean workers for altitudes lower than 3,000 metres.) It is too early to tell whether one design will outperform the others. The ESO's carbon-fibre dishes change pointing position with fewer errors, but it is uncertain how well the advanced internal gearing will hold up to weather over time. So far, all the antennas are performing to specifications. But having three different designs will saddle ALMA with extra operations costs far into the future, says Neal Evans, an astronomer at the University of Texas at Austin and chair of the ALMA board. \u201cYou'll need different spare parts, and you'll need people that know how to maintain each of the designs,\u201d he says. \n               Ambitious targets \n             Despite all the headaches, antennas are steadily accumulating on the plateau. In 2007, the JAO team raised glasses of water to celebrate the first linking of two dishes using the correlator \u2014 a computer that connects dish signals to create a composite view of the sky. (Why no champagne? The altitude impairs judgement, even at 2,900 metres, so ALMA has a strict no-alcohol policy; workers are subject to random breathalyser tests on the buses connecting local towns to the OSF.) In September 2011, with 16 dishes in place, ALMA began its inaugural observing period with the 100 or so projects that had risen to the top of its 'cycle 0' proposal competition. Most of the observation targets were relatively nearby objects in our Galaxy. Results ranged from the detection of sugar-related molecules in a nearby star system to an exceptionally sharp image of the gas clumps that will collapse into giant stars (see  Nature   492 , 319\u2013320; 2012 ). But the targets will soon become more ambitious. The mathematics of radio arrays implies an inverse relationship between antenna spacing and image resolution: the longer an array's 'baselines' (the distances between pairs of antennas), the smaller its field of view and the higher its resolution. The number of baselines, which determines how 'filled in' an ALMA image is, has grown simply through the addition of antennas. But the observatory can also change baselines by moving the antennas around the plateau \u2014 with the help of two German-built transporters nicknamed Otto and Lore (see 'ALMA, small and large'). In January, ALMA began its cycle 1 observations with 32 of the 12-metre dishes working at baselines of up to 1 kilometre, combined for the first time with some of the smaller Japanese dishes in the centre of the array. By the time cycle 2 begins, in early 2014, ALMA astronomers hope to have 40 dishes working at baselines of up to 2 kilometres. The resulting high resolution will help astronomers to understand star formation in distant galaxies seen very early in their lives, when the Universe was young and its chemical composition was different. \u201cALMA could very well open up a whole new field of star formation,\u201d says Linda Tacconi, an astronomer at the Max Planck Institute for Extraterrestrial Physics in Garching, Germany. ALMA will also be able to pinpoint how far away, and therefore how old, an object is. Usually, that measurement is a two-step process. Researchers first need time at a radio-astronomy facility to locate the object \u2014 a distant galaxy, say \u2014 then must spend many hours on an optical telescope to split the faint light up into its spectral components and identify emission or absorption lines caused by the presence of various elements and molecules. Measuring how far those wavelengths have been stretched by cosmic expansion allows observers to estimate how far away the object is. ALMA can do all of the above within minutes. Already, ALMA observations have shown that strangely bright early galaxies were in fact multiple smaller galaxies that had been lumped together by an earlier optical survey (A. Karim  et al . Preprint at  http://arxiv.org/abs/1210.0249 ; 2013 ). The discovery was a relief to theorists, who had been unable to work out how such bright, huge galaxies could have formed so early in the Universe. Once ALMA reaches baselines of 10 or more kilometres, astronomers will be able to turn their attention to stars forming in our Galaxy. The observatory has already detected gas flows in the disk surrounding a newborn star, crossing a gap that indicates the presence of a giant planet (S. Casassus  et al .  Nature   493 , 191\u2013194; 2013 ). Eventually, for some of the star systems closest to Earth, ALMA astronomers could have a shot at seeing the whirlpools of gas in which planets themselves are coalescing. \n               The deep unknown \n             But most of these projects will have been preordained \u2014 interesting stars, clouds or galaxies already seen in different parts of the spectrum by other telescopes. Many astronomers think that ALMA needs to forge a new path. They are calling for a 'deep-field survey' \u2014 a time exposure of a patch of sky for hundreds of hours, long enough to image extremely faint objects in that field and, possibly, to glimpse the formation of the Universe's first galaxies. \u201cI think it's something that has to be done,\u201d says Leonardo Testi, ALMA project scientist for the ESO. \u201cIf you only follow up on something else, then you are only looking after things you already know.\u201d The question is whether the JAO is strong enough to marshal ALMA's partners to do a deep-field survey. The Hubble Space Telescope has done several such surveys over the past two decades \u2014 but only because Hubble directors have allocated large chunks of discretionary time to the projects, thereby circumventing the fierce competition for observation slots. ALMA directors have very little discretionary time \u2014 almost every data-taking moment has been allocated. To do a deep-field survey, the partners would have to donate the time \u2014 a tough sell for a facility now receiving around six proposals for each available slot. The problem highlights a complaint common among JAO staff: none of the partners can call the shots. Europe and the United States have equal shares, both larger than Japan's, but no one has a majority. \u201cThere's no tiebreaker,\u201d says Al Wootten, an astronomer at the NRAO. Yet Beasley doubts that the process would have been any smoother if Europe or the United States had taken the lead. For smaller projects, he says, with stakes on the order of millions of dollars, minority funding partners might accept some decisions that run counter to their interests. But with a project the size of ALMA, in which even a minority stake is hundreds of millions of dollars, funding agencies will fight to protect their interests. \u201cNo one is going to lose any significant decisions at that point,\u201d he says. Beasley says that it would be better to create a strong central authority at the outset, and persuade funding agencies to grant it budgetary powers. There are precedents, particularly the treaty-governed European research institutes such as CERN, a particle-physics facility in Geneva, Switzerland, and the ESO itself, whose member states pay dues each year. And in 2011, the Square Kilometre Array created the SKA Organisation \u2014 a non-profit company based at the Jodrell Bank Observatory near Manchester, UK \u2014 which might give it the authority missing from the JAO. But it is hard to imagine a funding agency such as the NSF \u2014 which answers to the US Congress \u2014 ceding control. So in the near term, big astronomy is likely to be governed by loose confederations, and the success of future mega-projects will depend on the savvy and sweat of the people within. Anyone who has served as ALMA director would know something about this. Each charmed rather than shouted his way to success. De Graauw had his courtliness; Massimo Tarenghi, director from 2003 to 2008, a certain puckishness. Cox's weapon might be positivity: the new director seems always to be grinning. Tarenghi hopes that those smiles will stay after Cox takes the helm in April. \u201cThe person that suffers most is the poor director,\u201d he says wryly. \n               Coming down the mountain \n             By the end of the one-hour tour of ALMA, Pierre Cox is in fact suffering \u2014 from oxygen deprivation. Yet he still seems to be on cloud nine. \u201cI'm infinitely grateful. I'm honoured. I'm thrilled,\u201d he says. \u201cThis is one of the coolest places I've ever been.\u201d He gets in the car for the downhill journey and slips an oxygen saturation meter over his finger. First it reads 70%, then 76%. Not good. The driver, who has already put oxygen tubes in his own nose, calmly hands Cox a pressurized can of oxygen. Cox takes a squirt in his mouth and checks his numbers again. More than 90%. Much better. Maybe it's the rush of oxygen to the brain, but Cox becomes an enthusiastic chatterbox. The high-redshift Universe will be just the beginning, he says. He won't be completely satisfied following up on the objects others have already spotted. \u201cThere will always be surprises,\u201d he says. The car passes a vicu\u00f1a (a relative of the llama) standing sentinel at the lip of a gully. ALMA's dishes have vanished behind the edge of the plateau. The OSF appears in the distance below, white rooftops shimmering as the desert heats up for the day. By 4,000 metres, the air is getting thicker. The oxygen has a soporific effect. De Graauw's head begins to nod. Cox yawns loudly. Inexorably, his eyelids close. Behind him, on an isolated plain at the top of the world, the eyes of ALMA remain open, alert to the earliest glimmers of the Universe. \n                     Antenna decision makes waves 2011-Feb-01 \n                   \n                     First antenna switches on in the Atacama 2008-Dec-31 \n                   \n                     Radio astronomy: High and dry 2006-Feb-01 \n                   \n                     ALMA Observatory \n                   Reprints and Permissions"},
{"file_id": "494296a", "url": "https://www.nature.com/articles/494296a", "year": 2013, "authors": [{"name": "Stephen S. Hall"}], "parsed_as_year": "2006_or_before", "body": "With the help of a tiny worm, Cornelia Bargmann is unpicking the neural circuits that drive eating, socializing and sex. Male sexual dysfunction is never pretty, even in nematodes. In normal roundworm courtship, a slender male will sidle up to a plump hermaphrodite, make contact, and then initiate a set of steps leading up to insemination: a sinuous backwards motion as he searches for the sexual cleft, a pause to probe, and finally the transfer of sperm. The whole business is usually over in a couple of minutes. \u201cIt's very slithery, and affectionate,\u201d says Cornelia Bargmann, who has been observing the behaviour of this particular worm,  Caenorhabditis elegans , for 25 years. Last October, scientists in Bargmann's laboratory at the Rockefeller University, New York, reported the discovery of a gene that seems to be crucial to successful mating. Disrupting the action of this gene causes male sexual confusion of almost epic pathos: nematodes with certain mutations poke tentatively at an inert hermaphrodite, making confused, fruitless curlicues around the potential mate. Occasionally the mutant male succeeds, but often he literally falls off the job and begins the search anew for a mate. Jennifer Garrison, a postdoc of Bargmann's who tracked the behaviour of these males, just shakes her head as she replays the scene on her computer screen. \u201cReally sad,\u201d she says. There are two punchlines to this story of thwarted invertebrate mating. One is the charming squeamishness with which Bargmann describes it, hesitating at words such as \u201cvulva\u201d and \u201cspicule\u201d and other anatomical gewgaws of roundworm reproduction. \u201cAs a well-brought-up Southern girl,\u201d she says with a laugh, \u201cit's still difficult to talk about this!\u201d The other is scientific, supporting Bargmann's long-standing conviction that studying these deaf, part-blind, transparent creatures, which resemble nothing so much as wriggling specks of lint, could nonetheless yield enormous insight into how a nervous system creates behaviour. Since the 1980s, Bargmann and her colleagues have systematically explained the means by which worms taste and smell, exhibit social behaviours such as feeding in groups and explore their surroundings. She and her colleagues have parsed these behaviours down to the genes and circuitry of the neural connections that drive them. Just as studies of the fruitfly laid bare the basic principles of development and studies of yeast revealed the rules of the cell cycle, Bargmann believes that the simple nematode is revealing basic secrets about how animal nervous systems \u2014 including those of humans \u2014 translate sensory information into fundamental behaviours. \u201cWhat are the most basic behaviours that every animal has to show and every animal has to solve?\u201d she asks. \u201cYou can basically say that the three would be hunger, fear and reproduction. None of those things got invented last Saturday night!\u201d The evolutionary strength of that argument grew last year, when Bargmann and her colleagues published the experiments with the mutant males 1 . The mutation, they reported, is in a roundworm gene that they dubbed nematocin, which codes for a small peptide that influences multiple neurons and is a biochemical cousin to oxytocin and vasopressin, two hormones that play key parts in mammalian reproductive behaviour. Put another, Darwinian, way, the sexual confusion in mutant nematode males is tied to a molecule that seems to have been conserved in the nervous systems of animals at least since worms separated from vertebrates an estimated 600 million years ago. \u201cOxytocin and vasopressin are kind of at the top of the hierarchy of human neuropeptides in the brain,\u201d says biologist Scott Emmons of Albert Einstein College of Medicine in New York. \u201cAnd you look in the worm and, lo and behold, you see the same thing, which is quite striking.\u201d Although there had been some initial experimental forays into the behaviour of worms, says Paul Sternberg, who studies the nematodes at the California Institute of Technology in Pasadena, Bargmann took a more systematic, rational approach to dissecting its circuitry. \u201cShe made a major impact by taking that risk, and going broad and deep. She committed her life to this, and it's worked out beautifully.\u201d \n               Speaking the language \n             Bargmann, now 51, grew up in Athens, Georgia, but steeped in European culture. Both her parents were born in Germany. Her father spoke to her in English; her mother in German. \u201cApparently, until I was 4 or 5 years old, I didn't realize that each of us spoke a different language when we were talking to each other,\u201d she says. Her fluency threw open a world of German-language books, including semi-popular accounts of animal behaviour written by Konrad Lorenz 2  and Karl von Frisch 3 . \u201cMy mother had these books,\u201d Bargmann recalls. Decades later, some of these pioneering ethology works are almost compulsory reading in the Bargmann lab. Bargmann says that her true interest in science, however, dates back to an adolescent prank in school. \u201cWe were taking this Earth-science class, and the teacher told us that sodium was a metal and that if you placed it in water, it would burn,\u201d she says. \u201cWe were just electrified.\u201d With several classmates, Bargmann conspired to steal all the sodium from the school lab and test the hypothesis. \u201cThe sodium was flushed down the [boys'] toilet, which was blown off the wall!\u201d \u201cThis was literally the first time that science really struck me as something fun and exciting,\u201d Bargmann recalls. But she wasn't there to witness the explosion. \u201cThe ethics of the eighth grade is that you could be involved in stealing the sodium and you could be involved in planning the event,\u201d she says. \u201cBut there was no way that a girl was going to go into the boys' locker room.\u201d After a pause, she adds, with a smile, \u201cIt was a moment of weakness that I regret to this day.\u201d Bargmann has put that regret to good use, crashing through one door after another ever since. She went on to become a biochemistry major (and class valedictorian) at the University of Georgia in Athens, before heading for graduate studies to the laboratory of Robert Weinberg, a molecular biologist at the Massachusetts Institute of Technology (MIT) in Cambridge. \u201cStill waters run deep,\u201d says Weinberg, recalling the arrival of the soft-spoken Bargmann in 1981. \u201cIn the beginning, she didn't say much and was generally quiet, but I soon realized that she had a superior brain.\u201d At the time, the Weinberg lab was using the newly minted tools of molecular biology to tease apart the mechanics of oncogenes \u2014 genes that, when mutated or hyperactive, trigger the unbridled cellular proliferation that drives cancer. Bargmann isolated and sequenced an oncogene called  neu  from a rat tumour 4 . Later, researchers would discover that some malignancies, notably breast cancer, express the same gene, known in humans as  HER2 . In the 1990s, the Californian biotechnology company Genentech in South San Francisco developed a drug to target breast cancers that overexpress this gene, and the resulting monoclonal antibody, trastuzumab (Herceptin) has since been used to treat nearly one million people with cancer. Bargmann says that she derives \u201cimmense personal satisfaction\u201d, but no significant royalties, from this early work. Despite the heady start in a hot and highly visible field, Bargmann felt intimidated by the intellectual firepower of the people already in it. \u201cIt just wasn't so clear to me what I would do that was so different from what people like Weinberg and Harold Varmus were already doing,\u201d she says. So after receiving her PhD in 1987, she made a strategic \u2014 and quietly ambitious \u2014 decision to switch her area of research. Bargmann had always been fascinated by the neuroscience of behaviour, and the moment seemed right to bring molecular biology into the mix. After deciding against fruitflies (not enough neuroanatomy was known) and mice (the tools for genetic manipulation were still being developed), she settled on  C. elegans . The nematode had been earning a growing scientific constituency 5  ever since the mid-1960s, when British biologist Sydney Brenner proposed it as a model system. The organism was sufficiently complex to share basic biological functions with more advanced organisms, but was also experimentally tractable, with just 959 cells in the hermaphrodite marbled along its 1-millimetre length. Work by Brenner and others has established that all of the nematode's fundamental behaviours \u2014 navigating, foraging and mating \u2014 are governed by a neural system that contains just 302 neurons in hermaphrodites and about 8,000 synaptic connections. (By contrast, the human nervous system has 80 billion to 100 billion neurons and perhaps 100 trillion synapses.) To Bargmann, the combination of a blueprint for an entire nervous system and the powerful new genetic techniques that she had just learned offered \u201calluring\u201d possibilities to map molecules related to behaviour onto a well-established neuroanatomy. She opted to stay at MIT and do a postdoc in the laboratory of H. Robert Horvitz, a leading  C. elegans  researcher. Despite their relative simplicity, nematodes posed a daunting challenge to anyone interested in dissecting complex behaviours: it wasn't clear that they actually had any. Most behavioural studies in  C. elegans  had investigated very simple responses, or reflex actions. But Bargmann, reasoning that nematodes need a way to find food (usually bacteria grazing in foul-smelling, decomposing matter), decided to tackle their sensory behaviour: specifically, how they detect attractive or noxious chemicals around them, process that information and then use it to navigate towards or away from the source. Bargmann first had to work through \u201csome rather awful smelling substances\u201d, recalls Horvitz, to figure out which ones worms prefer. Then she used a technique called laser ablation to obliterate individual nerve cells. She identified a series of neurons that the soil-dwelling C. elegans uses to detect chemicals in its immediate environment6, and then went on to find neurons that responded to volatile odours \u2014 in effect demonstrating for the first time that nematodes had a sense of smell7. Bargmann's systematic approach was \u201cinspirational\u201d to other worm biologists, Sternberg says. Bargmann continued to explore worm olfaction after joining the faculty of the University of California, San Francisco, in 1991. In one study, her lab identified a receptor molecule, odr-10, in a pair of sensory neurons that detect diacetyl, an attractive odour associated with decaying food 8 . The lab correlated this and other mechanisms for sensing chemicals and heat with distinct nematode movements such as \u201cpirouettes\u201d and \u201comega turns\u201d, gradually assembling the neural circuitry of nematode navigation and exploration 9  \u2014 a behaviour she sometimes calls, in casual conversation, \u201ccuriosity\u201d. Soon, the lab began to uncover neural mechanisms that paralleled complex behaviour in other organisms. Bargmann knew that normal strains of roundworm vary in their feeding behaviour; some are solitary eaters, whereas others forage together in clumps of up to several hundred worms. Bargmann's lab showed that solitary eaters could be transformed into social eaters by inserting a slightly different version of the  npr-1  gene, which in worms encodes a receptor with biochemical cousins throughout the animal kingdom, known as neuropeptide Y receptor 10 . In other animals, neuropeptide Y regulates food consumption, mood and anxiety, among other things. In a sense, the  npr-1  story primed Bargmann for the nematocin discovery more than a decade later, which ultimately brought her back to the German ethologists \u2014 and to a new hypothesis about the evolution of behaviour. \n               Due credit \n             Bargmann's eighth-floor office in a research tower at Rockefeller is a homespun museum to the breadth of her intellectual interests and the depth of her personal attachments. A framed, red, heart-shaped piece of art on one wall depicts the anatomical outline of a nematode superimposed on the wiring pattern of the mouse olfactory system \u2014 a wedding gift celebrating her 2007 marriage to neuroscientist Richard Axel, whose research on mammalian olfaction at Columbia University in New York won Nobel recognition in 2004. A row of 21 empty champagne bottles lines the window sill, each uncorked to celebrate the thesis defence of a graduate student. Bargmann's speech oscillates between quiet scientific precision and a generous compulsion to acknowledge the contributions of every student, colleague, mentor and scientific ancestor, so that her conversation sometimes seems like an extended, erudite, magnanimous footnote. One such conversation leads to the tale of the sexually confused males. Molecules related to vasopressin and oxytocin had previously been identified in other branches of the animal kingdom that have very deep evolutionary roots, including octopuses and annelids, suggesting that the molecules had an ancient and conserved role in animal behaviour. Around 2004, Evan Macosko, a PhD student in Bargmann's lab, began scouring the  C. elegans  genome for the nematode version of oxytocin, but neuropeptides are very short and their genes are often hard to identify. He finally found a promising candidate in 2005, and the group went on to identify two nematocin receptors that were clearly related to the mammalian oxytocin receptor. Garrison's subsequent experiments with nematocin mutants dramatically confirmed that the peptide drives a basic behaviour 1 . Other research groups had defined, in exquisite detail, the series of discrete behavioural steps that male worms have to complete to succeed in mating (searching for a mate, contact, reverse turns, prodding for the vulva, insertion of spicule, transfer of sperm) as well as the motor neurons and muscles that rapidly fire and contract to drive these steps. But when Bargmann and her team analysed how the absence of nematocin affected each of these steps, they realized that each one remained intact. \u201cIt's not that he can't turn. It's not that he can't do the backing movement. It's not that he can't transfer sperm. It's that he doesn't know when to do them,\u201d she says. The neuropeptide, in essence, had a \u201cglobal organizing role\u201d and gave reproductive behaviour a forward drive. \u201cThere's something that's a much slower input that says something more like 'continue' or 'move forwards', sort of providing momentum that's superimposed on it. So the nervous system is doing both fast and slow information processing, in parallel, to drive the behaviour.\u201d This two-pronged neural processing reminded Bargmann of observations she had encountered decades earlier in the books in her mother's library. The pre-war ethologists \u2014 not only Lorenz and von Frisch, but also the Dutch scientist Niko Tinbergen \u2014 used astute field observations of fish, birds, insects and mammals to begin to assemble a fundamental grammar of behaviour. \u201c[They] were the first to really express the idea that there were basic rules governing animal behaviours,\u201d she says, \u201cand that you could recognize some of the same elements of those roles across very different animals.\u201d A \u201cfixed action response\u201d, for example, is a swift reaction to an environmental cue or threat. Tinbergen famously noted that the male stickleback fish in his lab aquaria flashed aggressive territorial behaviour when the local postal truck, painted red, rumbled by. He realized that the glimpse of red triggered a fixed, fast, hard-wired behavioural response because male sticklebacks display red bellies during mating season 11 . At the same time, the ethologists described an \u201cinnate releasing mechanism\u201d, a slower orchestration of these fast responses that increases the probability that a fundamental behaviour such as mating will occur. Tinbergen and his peers \u201cwere trying to relate what they saw in different animals to a common logic\u201d, Bargmann says, and she believes that logic is at least partly explained by neuropeptides such as nematocin. According to this model, the ritual movements of mating are the fixed-action patterns and the neuropeptides are the innate releasing mechanism. \u201cThose [peptides], in ways that at this point we're still trying to work out, change the properties of the neurons involved in those behaviours to help organize the outcome,\u201d she says. Bargmann suspects that this broad picture of nervous-system organization sends a counter-intuitive message about the evolution of behaviour: that the sensory apparatus in each species is evolving rapidly and is highly divergent, creating a different set of behavioural cues and responses for different animals, whereas the overarching behavioural coordination exerted by neuropeptides remains largely evolutionarily conserved. \u201cThis is not the way we [usually] think of things in neuroscience,\u201d says Bargmann. \u201cWe always think the simplest part will be the sensory part, and maybe that will be the most conserved part. But in fact the sensory periphery is crazy unconserved between different animals.\u201d This picture assumes that nematocin and its cousins have similar roles across species, but some biologists are not yet convinced. \u201cIt is pleasing to find these evolutionary connections,\u201d says Sternberg, \u201cbut we need more to know if the hypothesis is true.\u201d The idea that peptides can have a global influence on what neurons in a network do is central to a debate roiling in the neuroscience community at the moment. A number of biologists are pushing to create the 'connectome' \u2014 a definitive wiring diagram that would map all the cells, synapses and neuromodulators in mammals and other complex organisms, and describe how those components interact to produce behaviour. It would be a massive, expensive undertaking. Bargmann is all for connectivity maps, but she is not sure that, with current knowledge, they would explain as much as proponents hope. In an essay published last March 12 , she warned that \u201cit will not be possible to read a wiring diagram as if it were a set of instructions\u201d. Because neuropeptides can alter the excitability of neurons, the strength of synapses and even the overall function of a circuit, having a connectome is like having a street map without knowing how traffic flows through it, Bargmann says. With nematocin in hand, the Bargmann lab is now trying to figure out what triggers its release, and how the peptide changes the activity of the neurons it targets. The team is also using nematocin to try to understand how social behaviours can differ within and between species. \u201cMating behaviours change rapidly over evolution compared to other behaviours,\u201d Bargmann notes. \u201cHow does that happen at a mechanistic level?\u201d Understanding the mechanics of the roundworm's simple nervous system, says Bargmann, \u201cmay be the only chance we have of figuring out a more complex system. I'm open to the possibility that the logic is different in other animals. I just see no evidence that, at a deep level, it's true.\u201d \n                     Mapping brain networks: Fish-bowl neuroscience 2013-Jan-23 \n                   \n                     Neuroscience: Making connections 2012-Mar-21 \n                   \n                     Long life passed down through generations 2011-Oct-19 \n                   \n                     Cornelia Bargmann\u2019s lab \n                   Reprints and Permissions"},
{"file_id": "494162a", "url": "https://www.nature.com/articles/494162a", "year": 2013, "authors": [{"name": "Jeff Tollefson"}], "parsed_as_year": "2006_or_before", "body": "In the wake of Hurricane Sandy, scientists and officials are trying to protect the largest US city from future floods. Joe Leader's heart sank as he descended into the South Ferry subway station at the southern tip of Manhattan in New York. It was 8 p.m. on 29 October, and Hurricane Sandy had just made landfall some 150 kilometres south in New Jersey. As chief maintenance officer for the New York city subway system, Leader was out on patrol. He had hoped that the South Ferry station would be a refuge from the storm. Instead, he was greeted by wailing smoke alarms and the roar of gushing water. Three-quarters of the way down the final set of stairs, he pointed his flashlight into the darkness: seawater had already submerged the train platform and was rising a step every minute or two. \u201cUp until that moment,\u201d Leader recalls, standing on the very same steps, \u201cI thought we were going to be fine.\u201d Opened in 2009 at a cost of US$545 million, the South Ferry station is now a mess of peeling paint, broken escalators and corroded electrical equipment. Much of Manhattan has returned to normal, but this station, just blocks from one of the world's main financial hubs, could be out of service for 2\u20133 years. It is just one remnant of a coastal catastrophe wrought by the largest storm in New York's recorded history. Sandy represents the most significant test yet of the city's claim to be an international leader on the climate front. Working with scientists over the past decade, New York has sought to gird itself against extreme weather and swelling seas and to curb emissions of greenhouse gases \u2014 a long-term planning process that few other cities have attempted. But Sandy laid bare the city's vulnerabilities, killing 43 people, leaving thousands homeless, causing an estimated $19 billion in public and private losses and paralysing the financial district. The New York Stock Exchange closed for the first time since 1888, when it was shut down by a massive blizzard. As the humbled city begins to rebuild, scientists and engineers are trying to assess what happened during Sandy and what problems New York is likely to face in a warmer future. But in a dilemma that echoes wider debates about climate change, there is no consensus about the magnitude of the potential threats \u2014 and no agreement about how much the city should spend on coastal defences to reduce them. On 6 December, during his first major public address after the storm, New York mayor Michael Bloomberg promised to reinvest wisely and to pursue long-term sustainability. But he warned: \u201cWe have to live in the real world and make tough decisions based on the costs and benefits.\u201d And he noted that climate change poses threats not just from flooding but also from drought and heat waves. The city must be mindful, he said, \u201cnot to fight the last war and miss the new one ahead\u201d. \n               Calculated risks \n             In the immediate aftermath of Sandy, lower Manhattan looked like a war zone. Each night, streams of refugees wielding flashlights wandered north out of the blackout zone, where flood waters had knocked out an electrical substation. The storm devastated several other parts of the city as well. In Staten Island, pounding waves destroyed hundreds of homes, and one neighbourhood in Queens burned to ashes after water sparked an electrical fire. Power outages lasted for more than two weeks in parts of the city. Chastened by the flooding and acutely aware that Hurricane Irene, in 2011, was a near miss, the city is now wondering what comes next. \u201cIs there a new normal?\u201d asks John Gilbert, chief operating officer of Rudin Management, which manages several office buildings in downtown New York. \u201cAnd if so, what is it?\u201d Gilbert says that the company is already taking action. At one of its buildings, which took on some 19 million litres of water, the company is moving electrical systems to the second floor. \u201cYou have to think that as it has happened, it could happen again,\u201d he says. \u201cAnd it could be worse.\u201d At Battery Park, near the South Ferry station, the storm surge from Sandy rose 2.75 metres above the mean high-water level \u2014 the highest since gauges were installed there in 1923. In a study published last week in  Risk Analysis , researchers working with data from simulated storms concluded that a surge of that magnitude would be expected to hit Battery Park about once every 500 years in the current climate ( J.C.J.H.Aertsetal.RiskAnal.http://dx.doi.org/10.1111/risa.12008;2013 ). But the study authors and other scientists say that the real risks may be higher. The study used flooding at Battery Park as a measure of hurricane severity, yet it also showed that some storms could cause less damage there and still hammer the city elsewhere. Factoring in those storms could drive up the probability estimates of major hurricane damage to New York. The 1-in-500 estimate also does not take into account the unusual nature of Sandy. Dubbed a Frankenstorm, Sandy was a marriage of a tropical cyclone and a powerful winter snowstorm, and it veered into the New Jersey coast along with the high tide of a full Moon. \u201cIt was a hybrid storm,\u201d says Kerry Emanuel, a hurricane researcher at the Massachusetts Institute of Technology (MIT) in Cambridge and one of the study's co-authors. \u201cWe need to understand how to assess the risks from hybrid events, and I'm not convinced that we do.\u201d The risks will only increase as the world warms. The New York City Panel on Climate Change's 2010 assessment suggests that local sea level could rise by 0.3\u20131.4 metres by 2080. Last year, Emanuel and his colleagues found that floods that occur once every 100 years in the current climate could happen every 3\u201320 years by the end of this century if sea level rises by 1 metre. What is classified as a '500-year' event today could come every 25\u2013240 years (N. Lin  et al .  Nature Clim. Change   2 , 462\u2013467; 2012 ). For city planners, the challenge is to rebuild and protect the city in the face of scientific uncertainty. A few scientists have said for more than a decade that the city should armour New York's harbour with a storm-surge barrier similar to the Thames barrier in London. In Sandy's wake, that idea has gained renewed interest, and a New York state panel last month called for a formal assessment of it. \n               Bridges and barriers \n             Malcolm Bowman, who heads the storm-surge modelling laboratory at the State University of New York at Stony Brook, has spearheaded the drive for barriers. He imagines a structure roughly 8 kilometres wide and 6 metres high at the entrance to the harbour, and a second barrier where the East River drains into the Long Island Sound. The state panel's cost estimates for such a system range from $7 billion to $29 billion, depending on the design. The harbour barrier could also serve as a bridge for trains and vehicles to the city's airports, suggests Bowman. \u201cMy viewpoint is not that we should start pouring concrete next week, but I do think we need to do the studies,\u201d he says. But whether Sandy will push the city to build major defences, Bowman says, \u201cI don't know.\u201d Disasters have spurred costly action in the past. The 1888 blizzard helped to drive New York to put its elevated commuter trains underground. And in 2012, the US Army Corps of Engineers completed a $1.1-billion surge barrier in New Orleans, Louisiana, as part of a $14.6-billion effort to protect the city after it was battered by hurricanes Katrina and Rita in 2005. But the New York metropolitan area is bigger and more complex than New Orleans, and protecting it will require a multi-pronged approach. Several hundred thousand city residents live along more than 800 kilometres of coastline, and a barrier would not protect much of coastal Long Island, where Sandy wrought considerable damage. Moreover, the barrier would work only against occasional storm surges. It would not hold back the slowly rising sea or protect against flooding caused by rain. \u201cA storm-surge barrier may be appropriate, but it's never one thing that is going to protect you,\u201d says Adam Freed, a programme director at the Nature Conservancy in New York, who until late last year was deputy director of the city's office of long-term planning and sustainability. \u201cIt's going to be a holistic approach, including a lot of unsexy things like elevating electrical equipment out of the basement and providing more back-up generators.\u201d As part of that holistic effort, officials are exploring options for expanding the remaining bits of wetlands that once surrounded the city and buffered it from storms. In his address, Bloomberg called wetlands \u201cperhaps the best natural barriers against storms that we have\u201d. But most of the city's wetlands have become prime real estate in recent decades, and Sandy made clear the consequences of developing those areas, says Marit Larson, director of wetlands and riparian restoration for the New York parks department. A few weeks after the storm, Larson parks her car near the beach on Staten Island and looks out at a field of  Phragmites australis , a common marsh reed. The field is part of Staten Island's 'Bluebelt' programme, initiated in the late 1980s to promote wetlands and better manage storm-water runoff. But the patch of wetlands here is smaller than a football pitch, and Sandy's surge rolled over it, damaging the nearby row houses. \u201cIf you look at the historical maps,\u201d says Larson, \u201ceverything that used to be a wetland got wet.\u201d \n               Up to code \n             New York is now moving to strengthen its network of existing wetlands, which cover some 2,300\u20134,000 hectares. The mayor's budget plan for 2013\u201317 includes more than $200 million to restore wetlands as part of an effort to protect and redesign coastal developments. Sandy also showed how proper construction can help to reduce risks from future storms. In one Staten Island neighbourhood, a battered roof rests on the ground, marking the spot where an ageing bungalow once stood. Next door, a newer house still stands, with no apparent damage apart from a flooded garage \u2014 sturdy proof of the value of modern building codes. In New York, newer buildings constructed in 100-year-flood zones, which are defined by the US Federal Emergency Management Agency (FEMA), cannot have any living spaces or major equipment, such as heating units, below the projected flood level (see 'Danger zone'). The city's zoning provisions could not protect against a storm like Sandy: officials estimate that two-thirds of the homes damaged by the storm were outside the 100-year-flood area. But scientists say that the FEMA flood maps were out of date, so even century-scale storms could cause damage well beyond the designated areas. Last month, FEMA began releasing new flood maps for the New York region that substantially expand this zone. In their latest study, Emanuel and his colleagues estimate the average annual flood risk for New York as only $59 million to $129 million in direct damages. But costs could reach $5 billion for 100-year storms and $11 billion for 500-year storms. These figures do not include lost productivity or damage to major infrastructure, such as subways. Bowman and other researchers argue that the city should commit to protecting all areas to a 500-year-flood standard, but not all the solutions are physical. A growing chorus of academics and government officials stress that the city must also bolster its response capacity and shore up the basic social services that help people to rebuild and recover. Most importantly, the city and surrounding region need to develop a comprehensive strategy for defending the coastline, says Jeroen Aerts, a co-author of the  Risk Analysis  assessment who studies coastal-risk management at VU University in Amsterdam. Aerts is working with New York officials to analyse proposals for the barrier system and a suite of changes in urban planning, zoning and insurance. \u201cYou need a master plan,\u201d he says. Seth Pinsky is working towards that goal. As president of the New York City Economic Development Corporation, he was tapped by Bloomberg to develop a comprehensive recovery plan that will make neighbourhoods and infrastructure safer. He points out that some newer waterfront parks and residential developments along the coast fared well during the storm. For example, at Arverne by the Sea, a housing complex in Queens, Pinsky says that units survived because they are elevated and set back from the water, with some protection from dunes. The buildings suffered little damage compared with surrounding areas. \n               Intelligent design \n             The cost of strengthening the city will be astronomical. In January, Congress approved some $60 billion to fund Sandy recovery efforts, with around $33 billion for longer-term investments, including infrastructure repair and construction by the Army Corps of Engineers. Pinsky says that he does not yet know how much of that money will go to New York, but he is sure it will not be enough. The city will define its budget in June, after his group has made its official recommendations. The rebuilding endeavour will probably necessitate a \u201ccreative\u201d mix of public and private financing, he says. \u201cIt will probably require calling on a combination of almost every tactic that has been tried around the world.\u201d Even as he calls for more intelligent development, Pinsky says that New York is unlikely to take a drastic approach to dealing with storm surge and sea-level rise. \u201cRetreating from the coastline of New York city both will not be necessary and is not really possible,\u201d he says. Given the sheer scale of development along the coast, it is hard to argue with Pinsky's assessment. But many climate scientists fear that bolstering coastal developments only delays the eventual reckoning and increases the likelihood of future disasters. The oceans will rise well into the future, they say, so cities will eventually be forced to accommodate the water. \u201cI don't see anything yet that looks towards long-term solutions,\u201d says Klaus Jacob, a geoscientist at Columbia University's Lamont-Doherty Earth Observatory in Palisades, New York. But Jacob admits that he is as guilty as anyone. In 2003, he and his wife bought a home in a low-lying area on the Hudson River in Piermont, New York. Although it went against his professional principles, he agreed to the purchase with the assumption that he could elevate the house. But height-restriction laws prevented him from doing so, and Sandy flooded the house. The couple are now rebuilding. \u201cIn a way, I think I was in denial about the risk,\u201d Jacob says. He hopes that a new application to raise the house will be approved, but he still fears that the neighbourhood will not survive sea-level rise at the end of the century. New Yorkers and coastal residents everywhere would be wise to learn that lesson. \u201cUltimately,\u201d Jacob says, \u201cwe all have to move together to higher ground.\u201d \n                 See Editorial \n                 p.148 \n               \n                     Damage control 2013-Feb-13 \n                   \n                     Hurricane sweeps US into climate-adaptation debate 2012-Nov-06 \n                   \n                     Hurricane Sandy spins up climate discussion 2012-Oct-30 \n                   \n                     Climate and weather: Extreme measures 2011-Sep-07 \n                   \n                     Nature special: Rio+20 \n                   \n                     SUNY storm surge modelling lab \n                   \n                     New York city planning document \n                   \n                     Mayor Bloomberg's Sandy address \n                   Reprints and Permissions"},
{"file_id": "493470a", "url": "https://www.nature.com/articles/493470a", "year": 2013, "authors": [{"name": "Dan Jones"}], "parsed_as_year": "2006_or_before", "body": "Praying, fighting, dancing, chanting \u2014 human rituals could illuminate the growth of community and the origins of civilization. By July 2011, when Brian McQuinn made the 18-hour boat trip from Malta to the Libyan port of Misrata, the bloody uprising against Libyan dictator Muammar Gaddafi had already been under way for five months. \u201cThe whole city was under siege, with Gaddafi forces on all sides,\u201d recalls Canadian-born McQuinn. He was no stranger to such situations, having spent the previous decade working for peace-building organizations in countries including Rwanda and Bosnia. But this time, as a doctoral student in anthropology at the University of Oxford, UK, he was taking the risk for the sake of research. His plan was to make contact with rebel groups and travel with them as they fought, studying how they used ritual to create solidarity and loyalty amid constant violence. It worked: McQuinn stayed with the rebels for seven months, compiling a strikingly close and personal case study of how rituals evolved through combat and eventual victory. And his work was just one part of a much bigger project: a \u00a33.2-million (US$5-million) investigation into ritual, community and conflict, which is funded until 2016 by the UK Economic and Social Research Council (ESRC) and headed by McQuinn's supervisor, Oxford anthropologist Harvey Whitehouse. Harvey Whitehouse talks about the project to catalogue the world\u2019s rituals. Rituals are a human universal \u2014 \u201cthe glue that holds social groups together\u201d, explains Whitehouse, who leads the team of anthropologists, psychologists, historians, economists and archaeologists from 12 universities in the United Kingdom, the United States and Canada. Rituals can vary enormously, from the recitation of prayers in church, to the sometimes violent and humiliating initiations of US college fraternity pledges, to the bleeding of a young man's penis with bamboo razors and pig incisors in purity rituals among the Ilahita Arapesh of New Guinea. But beneath that diversity, Whitehouse believes, rituals are always about building community \u2014 which arguably makes them central to understanding how civilization itself began. To explore these possibilities, and to tease apart how this social glue works, Whitehouse's project will combine fieldwork such as McQuinn's with archaeological digs and laboratory studies around the world, from Vancouver, Canada, to the island archipelago of Vanuatu in the south Pacific Ocean. \u201cThis is the most wide-ranging scientific project on rituals attempted to date,\u201d says Scott Atran, director of anthropological research at the CNRS, the French national research organization, in Paris, and an adviser to the project. \n               Human rites \n             A major aim of the investigation is to test Whitehouse's theory that rituals come in two broad types, which have different effects on group bonding. Routine actions such as prayers at church, mosque or synagogue, or the daily pledge of allegiance recited in many US elementary schools, are rituals operating in what Whitehouse calls the 'doctrinal mode'. He argues that these rituals, which are easily transmitted to children and strangers, are well suited to forging religions, tribes, cities and nations \u2014 broad-based communities that do not depend on face-to-face contact. Rare, traumatic activities such as beating, scarring or self-mutilation, by contrast, are rituals operating in what Whitehouse calls the 'imagistic mode'. \u201cTraumatic rituals create strong bonds among those who experience them together,\u201d he says, which makes them especially suited to creating small, intensely committed groups such as cults, military platoons or terrorist cells. \u201cWith the imagistic mode, we never find groups of the same kind of scale, uniformity, centralization or hierarchical structure that typifies the doctrinal mode,\u201d he says. \n               Rebel yell \n             Whitehouse has been developing this theory of 'divergent modes of ritual and religion' since the late 1980s, based on his field work in Papua New Guinea and elsewhere 1 . His ideas have attracted the attention of psychologists, archaeologists and historians. Until recently, however, the theory was largely based on selected ethnographic and historical case studies, leaving it open to the charge of cherry-picking. The current rituals project is an effort by Whitehouse and his colleagues to answer that charge with deeper, more systematic data. The pursuit of such data sent McQuinn to Libya. His strategy was to look at how the defining features of the imagistic and doctrinal modes \u2014 emotionally intense experiences shared among a small number of people, compared with routine, daily practices that large numbers of people engage in \u2014 fed into the evolution of rebel fighting groups from small bands to large brigades. At first, says McQuinn, neighbourhood friends formed small groups comprising \u201cthe number of people you could fit in a car\u201d. Later, fighters began living together in groups of 25\u201340 in disused buildings and the mansions of rich supporters. Finally, after Gaddafi's forces were pushed out of Misrata, much larger and hierarchically organized brigades emerged that patrolled long stretches of the defensive border of the city. There was even a Misratan Union of Revolutionaries, which by November 2011 had registered 236 rebel brigades. McQuinn interviewed more than 300 fighters from 21 of these rebel groups, which varied in size from 12 to just over 1,000 members 2 . He found that the early, smaller brigades tended to form around pre-existing personal ties, and became more cohesive and the members more committed to each other as they collectively experienced the fear and excitement of fighting a civil war on the streets of Misrata. But six of the groups evolved into super-brigades of more than 750 fighters, becoming \u201csomething more like a corporate entity with their own organizational rituals\u201d, says McQuinn. A number of the group leaders had run successful businesses, and would bring everyone together each day for collective training, briefings and to reiterate their moral codes of conduct \u2014 the kinds of routine group activities characteristic of the doctrinal mode. \u201cThese daily practices moved people from being 'our little group' to 'everyone training here is part of our group',\u201d says McQuinn. McQuinn and Whitehouse's work with Libyan fighters underscores how small groups can be tightly fused by the shared trauma of war, just as imagistic rituals induce terror to achieve the same effect. Whitehouse says that he is finding the same thing in as-yet-unpublished studies of the scary, painful and humiliating 'hazing' rituals of fraternity and sorority houses on US campuses, as well as in surveys of Vietnam veterans showing how shared trauma shaped loyalty to their fellow soldiers. To gain a more global perspective on ritual practices, Whitehouse and Quentin Atkinson, a psychologist at the University of Auckland, New Zealand, and a member of the project, used a previously developed database containing information on world cultures to explore the connections between frequency, peak levels of emotional arousal, and average community size for 645 rituals across 74 cultures 3 . As predicted, the rituals fell into two clusters: low-frequency but high-arousal imagistic varieties that were more common in societies with a smaller average community size, and high-frequency, low-arousal doctrinal rituals that were more established in societies in which communities are larger. Given these data from contemporary cultures, it is hard not to speculate about ritual's role in history: did the transition from imagistic mode to doctrinal mode, with its emphasis on a common identity buttressed by daily activities and rituals, play a part in the emergence of large, complex societies 10,000 years ago? \n               The birth of civilization? \n             To address that question, Whitehouse, Atkinsonand Camilla Mazzucato, also based at the University of Oxford, are looking at archaeological data from \u00c7atalh\u00f6y\u00fck, one of the largest and best-preserved Neolithic towns known. Located in the Anatolian plains of northwestern Turkey, \u00c7atalh\u00f6y\u00fck was founded during the dawn of agriculture roughly 9,500 years ago, and housed more than 8,000 people at its peak. The town's early layers show that residents frequently buried their kin under the floors of their houses, sometimes with their heads severed. Wall paintings also depict the town's residents getting together to tease and kill enormous wild bulls for feasting. \u201cThe whole process of baiting and killing these animals would have been extremely intense, and have had a major emotional impact,\u201d says excavation director Ian Hodder, an archaeologist at Stanford University in California. These occasional feasts were also memorialized by mounting the skulls and horns of bulls inside houses, and burying the rest of the bones to commemorate the founding or abandonment of a house, which Hodder says were also highly ritualistic events. Evidence for such imagistic-style rituals declines in the later layers of \u00c7atalh\u00f6y\u00fck. Wild-bull rituals and bull-horn installations become less common as the herding of domesticated sheep, goats and cattle intensified, says Hodder. Human burials within houses fade out, and standardized symbolic artefacts, such as painted pottery and seal stamps, become more common. Whitehouse and Hodder believe that these changes represent a shift to a more doctrinal mode of ritual as people united into a larger, more cooperative community devoted to agriculture and animal herding. Although speculative, this interpretation is consistent with Whitehouse and Atkinson's cross-cultural survey, which found that in contemporary societies the doctrinal mode is more established where agriculture is practised most intensively. Looking beyond \u00c7atalh\u00f6y\u00fck, Whitehouse, Atkinson and Mazzucato are building a regional database chronicling similar changes in ritual at 60 other sites across the Middle East, from the end of the Palaeolithic around 10,000 years ago until the early Bronze Age around 7,000 years ago. This database will dovetail with another one that covers the entire world over the past 5,000 years 4 . That resource codifies information about the culture, religion and ritual practices of people worldwide, and combines this with measures of social complexity \u2014 for example, how many levels of administration a society's government has, or the number of distinct professions \u2014 as well as data on the intensity of warfare. The plan is to use this database to explore the links between ritual and social life, as well as the roles of war and competition between societies in nurturing certain kinds of ritual and driving increases in social complexity. Members of the ESRC project are also probing people's beliefs about how rituals work. For example, Cristine Legare at the University of Texas at Austin has studied Brazilian rituals called  simpatias , which are used to solve everyday problems ranging from bad luck to asthma and depression 5 . A  simpatia  for getting a good job says that during the full Moon the jobseeker must take the jobs page out of a newspaper, fold it four times, and then place it on the floor with a small white candle surrounded by honey and cinnamon, imagining themself in a new job with good pay. The candle stub and the paper should be buried with a plant and watered daily, and the dream job will soon emerge. \n               The ritual mind \n             Legare presented Brazilians with a variety of  simpatias , and found that people judged them as more effective when they involved a large number of repetitive procedural steps that must be performed at a specific time and in the presence of religious icons. \u201cWe're built to learn from others,\u201d she says, which leads us to repeat actions that seemed to work for someone else \u2014 \u201ceven if we don't understand how they produce the desired outcomes\u201d. Meanwhile, psychologist Ryan McKay at Royal Holloway, University of London, and Jonathan Lanman, a cognitive anthropologist at Queen's University, Belfast, are exploring how rituals can be broken down into their component parts and how each part influences behaviour. One such component is synchronized physical action \u2014 for example, the ritualized goose-stepping of military units \u2014 which social psychologists have shown 6  promotes a sense of connection and trust between individuals. This work builds on research by Richard Sosis, an anthropologist at the University of Connecticut, who has shown that immersion in collective rituals, such as communal prayer, in Israeli kibbutzim increases cooperative behaviour in economic games 7  \u2014 but only with other kibbutz members 8 . Ritual also has its darker side. Surveys by Ara Norenzyan, a psychologist at the University of British Columbia in Vancouver who has an advisory role on the project, suggest that support for suicide terrorism among Palestinians is more strongly tied to communal ritual attendance than to religious devotion, as measured by the frequency of private prayer 9 . Atran thinks that rituals could also feed conflict by turning the opinions and preferences of groups into 'sacred values' \u2014 absolute and non-negotiable beliefs that cannot be traded against material benefits such as money. For many Israelis, for example, one such value is the right to occupy the West Bank, whereas for many Palestinians it is the right to return to the villages from which they were expelled. In fact, Atran has found that financial offers to compromise on these sacred values makes them even more entrenched 10 . As an example of how rituals can cause values and preferences to become sacralized, Atran points to his studies showing that, in the United States, people who attend church more frequently are more likely to consider the right to bear arms a sacred value 11 . \u201cEmotionally intense rituals have bound us together and pitted us against our enemies throughout the history of our species,\u201d says Whitehouse. \u201cIt was only when nomadic foragers began to settle down did we discover the possibilities for establishing much larger societies based on frequently repeated creeds and rituals.\u201d The big question, he says, is whether this kind of unity can be extended to humanity at large. For Whitehouse, understanding the ways that rituals shape group behaviour is the first step towards finding out how they can be harnessed to dampen down conflict between groups. He hopes that such insights could help policy-makers to \u201cestablish new forms of peaceful cooperation, as well as bringing down dictators\u201d. \n                     Human cycles: History as science 2012-Aug-01 \n                   \n                     Ritual, Community and Conflict Project \n                   \n                     Evolution of Religion blog \n                   Reprints and Permissions"},
{"file_id": "494300a", "url": "https://www.nature.com/articles/494300a", "year": 2013, "authors": [{"name": "Ivan Amato"}], "parsed_as_year": "2006_or_before", "body": "Cement manufacturing is a major source of greenhouse gases. But cutting emissions means mastering one of the most complex materials known. If this year's expected global output of cement were somehow poured across Manhattan island, the 3.4-billion-tonne mass would solidify into a monolith about 14 metres high. If the monolith were created next year, it would probably be even bigger, given the construction boom now under way in developing nations such as China and India. Cement is a crucial raw material for civilization, holding together artefacts ranging from the 2,000-year-old Pantheon in Rome to modern skyscrapers and highways. Unfortunately for Earth's climate, however, the most widely used form of that material today \u2014 'portland' cement \u2014 is made by roasting limestone and clay in giant kilns in a process that sends nearly a tonne of carbon dioxide skywards for every tonne of final product. The manufacture of portland cement accounts for roughly 5% of all human-generated greenhouse-gas emissions. Worse, for researchers looking for ways to reduce emissions, cement is not just a commonplace, high-volume commodity; it is also one of the most complex substances known in materials science. From its structure and composition to the reactions that ensue when it is mixed with water and poured into a mould to set, \u201cwe still have some of the most basic questions about cement unanswered\u201d, says Hamlin Jennings, director of the Concrete Sustainability Hub (CSHub) at the Massachusetts Institute of Technology in Cambridge. \u201cThe details of what is happening once water touches cement powder are a matter of lively debate,\u201d adds Kenneth Snyder, a cement expert at the US National Institute of Standards and Technology in Gaithersburg, Maryland. \u201cThere are almost religious wars over this.\u201d Nonetheless, the prospect of carbon taxes and cap-and-trade markets has led industry groups around the world to adopt green or sustainable cement initiatives. Their approaches range from supporting basic research to pushing to reform international building codes, and, if successful, could eventually cut the cement industry's carbon dioxide footprint by half. The CSHub is one of the field's largest academic-research centres. Founded in 2009 with funds from industry sponsors totalling US$10 million over five years, the CSHub now comprises about a dozen principal investigators seeking to understand cement \u2014 from its function in various structures to its quantum-mechanical properties. It is a struggle, says Jennings, for reasons that become apparent when one considers what happens on the molecular scale when cement is made. \n               Strange brew \n             The cement-making process (see 'Turning up the heat') begins with a mix of limestone and aluminosilicate clay, he says, \u201ceach with its own chemistry and impurities\u201d, which react in myriad ways as they are roasted together in a kiln at about 1,500 \u00b0C. What emerges are greyish, marble-sized chunks known as 'clinker'. Clinker contains silicon, iron and aluminium oxides (derived mostly from the clay) and calcium oxide, which forms when heat drives carbon dioxide out of the limestone's calcium carbonate. This carbon dioxide is one major source of emissions from the process; the fuel used for heating the kiln is the other. Once the clinker has cooled, it is combined with gypsum (the amount of which controls how fast the cement will set), milled into a powder having the consistency of flour and distributed to 'batch plants'. There, cement powder is mixed with water to form a paste, the consistency of which depends on its intended use \u2014 in a bridge piling, say, or a pavement. Most often, the paste is mixed with sand, gravel or larger stones to form concrete. The concrete slurry is then trucked to the construction site and poured into a mould, where it cures in a process that begins quickly but can take months to complete. \u201cOne of the miracles, and the subject of intense research,\u201d says Jennings, \u201cis that the mix stays fluid for the first few hours, after which a furious set of simultaneous chemical reactions starts to produce the products that lead to the hardening process.\u201d Most important to the final material are the hydration reactions that turn the water and powdered clinker into artificial stone: a matrix of calcium silicate hydrate (CaO\u2013SiO 2 \u2013H 2 O, or C\u2013S\u2013H). \u201cAll construction on this planet relies on this liquid-to-stone transition,\u201d says Roland Pellenq, a physical chemist at the CSHub. But C\u2013S\u2013H is a maddeningly imprecise formula, Pellenq says. Its components have no set proportions, and the reaction products in a given sample of curing concrete depend on the initial ingredients, the amount of water used and the ratio of calcium to silicon, as well as additives, contaminants, temperature and humidity. And, of course, concrete is opaque, adding to the difficulty of analysing C\u2013S\u2013H as it forms. \n               Tweaking the recipe \n             Despite these challenges, says Pellenq, he and his colleagues at the CSHub are making progress on the carbon-emissions problem. One promising line of attack involves finding ways to reduce the roasting temperature and thereby burn less fuel. The major targets are alite and belite, two of the primary minerals in clinker that give rise to C\u2013S\u2013H. Alite (Ca 3 SiO 5 ) is the more reactive of the two \u2014 it begins to cure within hours after the addition of water, giving concrete its initial strength. But alite requires the full 1,500 \u00b0C to form, whereas belite (Ca 2 SiO 4 ) forms at about 1,200 \u00b0C. Belite is ultimately stronger, but takes days and even months to begin hardening \u2014 too long to be used on its own in construction projects. Pellenq and his colleagues are investigating whether some belite crystal structures might be as reactive as alite yet still form at lower kiln temperatures, saving fuel. Because the answer to that question depends on atomic-scale details such as the distribution of electrons in the crystal, the researchers carried out quantum-mechanical calculations of how the structure of C\u2013S\u2013H is affected by aluminium, magnesium and other impurities (K. Van Vliet  et al .  MRS Bull.   37 , 395\u2013402; 2012 ). As Pellenq puts it, \u201cto do quantum clinker engineering, you need to know where the electrons are\u201d. The CSHub researchers found that alite crystals always contain one plane that dissolves more easily in water than the others, whereas in belite crystals, all faces are similar \u2014 and the crystal is less reactive with water (E. Durgun  et al .  Chem. Mater.   24 , 1262\u20131267; 2012 ). This is why belite is slower to cure than alite. But Pellenq says that the results also suggest that certain impurities, such as magnesium, could help to make belite more soluble in water. This might allow it to cure fast enough to be used as the primary ingredient in cement for construction. A move towards low-temperature belite could raise new problems, however. Franz-Josef Ulm, a mechanical engineer at the CSHub, and his team have found that it takes four to nine times more energy to grind belite into powder than alite, which could diminish the emissions benefits of using belite-rich clinker. Others, such as Ceratech, a cement company in Alexandria, Virginia, are seeking solutions in alternatives to conventional clinker. The company has found inspiration in the cement used 2,000 years ago by the engineers of ancient Rome. The key ingredient was pozzolana, a type of volcanic ash that reacts with water to make cement \u2014 thus functioning as natural clinker. Ceratech is exploiting an industrial version of pozzolana: fly ash, the fine particles filtered out of the combustion gases from coal-burning electricity plants. US plants produce roughly 70 million tonnes of fly ash every year, most of which is stored or disposed of in landfills. Ceratech converts the ash into cement powder by combining it with several proprietary liquid additives. Because the process does not require heat, the company says that its fly-ash cements are carbon neutral. Although batch plants have been making blends containing up to 15% fly ash for years, Ceratech's formulations are 95% fly ash and 5% liquid ingredients, says Mark Wasilko, the company's executive vice-president. In addition, he says, concrete made from fly-ash cement is stronger than the conventional variety, so designers can use less of it. The company says that in a typical three-storey, 4,600-square-metre building, the use of its fly-ash cement would reduce the total volume of concrete by 183 cubic metres and the total mass of steel reinforcing bars by about 34 tonnes; it would also divert 374 tonnes of fly ash from landfills and reduce carbon dioxide emissions by 320 tonnes. At present, Wasilko says, Ceratech is a bit player in the cement industry, and its approach to cutting carbon emissions amounts to a mere clink in a multibillion-tonne batch. The big carbon reductions will come only when next-generation cements are embraced by the construction industry's thousands of independent producers, engineers, architects, city planners and building inspectors. And that means lowering the perceived risk of choosing greener cements over their time-tested, conventional counterparts. The concern seems to be: \u201cIf this road doesn't work, my boss will hammer me,\u201d says Snyder. That attitude might change if more countries adopt taxes or cap-and-trade schemes that make the cost of emitting carbon much higher than it is now. But a more tangible, near-term way to overcome the reluctance is to build demonstration structures, such as bridges, roads and buildings, to prove the real-world viability of the new cement and concrete materials. Wasilko says that he hopes the several dozen projects that the company works on each year, such as dock structures in the Port of Savannah, Georgia, and chemical-handling basins for Gulf Sulphur Services in Galveston, Texas, will serve that purpose. There is reason enough to get going. In the eight minutes or so that it took to read this article, cement-makers dumped another 30,000 tonnes of carbon dioxide into the atmosphere. \n                     Granular physics: A concrete theory 2009-Jun-26 \n                   \n                     Global change: China at the carbon crossroads 2009-Apr-22 \n                   \n                     Waste concrete could help to lock up carbon 2008-Apr-02 \n                   \n                     Composition and density of nanoscale calcium\u2013silicate\u2013hydrate in cement 2007-Mar-25 \n                   \n                     Blogpost: From electrons to slabs \n                   \n                     Concrete Sustainabilty Hub \n                   \n                     Ceratech \n                   \n                     Kenneth Snyder \n                   \n                     Portland Cement Association \n                   \n                     US concrete industry's Strategic Development Council \n                   Reprints and Permissions"},
{"file_id": "493466a", "url": "https://www.nature.com/articles/493466a", "year": 2013, "authors": [{"name": "Virginia Hughes"}], "parsed_as_year": "2006_or_before", "body": "Tiny fish trapped in a virtual world provide a window into complex brain connections. A recently hatched zebrafish is swimming upriver for the first time. Its big round eyes, bulging on the front of its eyelash-sized body, scan the surroundings. Suddenly, it sees the scenery flying forwards as a gentle current pushes it backwards. The fish flicks its tail to try to stay in place. Or so it thinks. In reality, the baby fish is paralysed and suspended in a water-filled Petri dish by glass pipettes. The dish sits on the stage of a US$100,000 microscope in the corner of a darkened, cluttered laboratory. A film, projected from below, has transported the fish to a virtual world in which moving bands of light and dark simulate passing underwater scenery. Although the fish doesn't move, the motor neurons that control its tail are firing away, just as if it were swimming. And when fed into a computer, those signals can control the video display, giving the fish nearly every sign that it is swimming normally. All the while, Florian Engert's microscope peers deep into the fish's tiny, translucent brain to watch neurons glow green as they fire (see \u2018A river of deceit\u2019). Engert, the neuroscientist who developed the set-up, often jokes that the fish is just like Neo, a leading character from the 1999 sci-fi thriller  The Matrix , in which humans have been enslaved by machines but are fed a virtual reality that leads them to believe that they are free. Engert's team at Harvard University in Cambridge, Massachusetts, hopes that the fish in this aquatic matrix will help to answer the biggest question in neuroscience: how a doughy mass of neurons in the brain gives rise to an exquisite suite of behaviours, absorbing information from the outside world and generating responses. Since the late nineteenth century, when Spanish anatomist Santiago Ram\u00f3n y Cajal pinpointed the neuron as the fundamental unit of the brain, most neuroscientists have focused on recording the electrical buzzing of individual cells. That has tended to mean sticking electrodes into the brains of cats, rabbits, rats, mice, sea slugs, squid, monkeys and even people. The approach reveals a lot about how neurons respond to inputs \u2014 such as a chemical messenger, a sound or a colour \u2014 and produce individual firing patterns, which the brain decodes to drive behaviour. But how these cells work together to translate and integrate complex, real-world sensory inputs \u2014 such as moving scenery, smells, sounds or an approaching predator \u2014 \u201cis something that's still a big mystery\u201d, Engert says. \u201cThat's probably the main challenge for the next decade.\u201d Virginia Hughes reports from Florian Engert\u2019s lab on the big questions that zebrafish can help to answer. Larval zebrafish ( Danio rerio ) have been a workhorse model organism in developmental biology labs for about 30 years because they are cheap and relatively amenable to genetic manipulation, and have transparent tissues, allowing researchers to see inside them. Engert and a small group of neuroscientists have been looking to capitalize on these qualities to study how the brain encodes vision, hearing, movement and even fear, things that are impossible to do in the brains of more complex model organisms. And techniques such as Engert's matrix are allowing them to monitor the zebrafish's 300,000 or so neurons and to track activity in vast swathes of them simultaneously in living brains. Such innovations mean that grant reviewers and top-tier journals \u2014 which overwhelmingly favour neuroscience research in mammals \u2014 are now giving fish a chance. \u201cThere is a sort of perfect convergence of model and methodology with zebrafish that is peaking right about now,\u201d says Joseph Fetcho, a neurobiologist at Cornell University in Ithaca, New York, who pioneered brain-circuit research in the fish. This impressive array of tools, he adds, makes him wonder \u201cwhy one would use any other model for basic questions about circuits and behaviour\u201d. \n               Spotlight aquarium \n             During his early career at Stony Brook University in New York, Fetcho worked on goldfish brains until he got frustrated recording from only a couple of cells at a time. He switched to zebrafish in the mid-1990s, after two strikes of serendipity. First, at a zoology conference, Fetcho stumbled into a workshop on using zebrafish in high-school biology classes. He realized how easy it was to watch their translucent embryonic cells divide and, over the course of just a few days, develop into organs and limbs. Second, he came across a paper describing how to fill neurons with a green dye that is sensitive to calcium 1 . Because neuronal firing requires an influx of calcium ions, this method provided a way to view cells in action. The paper used cells isolated from chick spinal cords, but Fetcho thought the same approach could light up the neurons of live zebrafish. He went to a local pet shop and bought a mating pair. The next day, he had fertilized eggs to experiment with. In Fetcho's first zebrafish paper 2 , published in 1995, he and his colleague Donald O'Malley used the calcium-sensitive green dye to track the activity of motor neurons during a predator-escape reflex, which is triggered by poking the fish in the head. His team went on to show that neurons in different segments of the hindbrain encode how the fish turns its body to escape from a predator 3 . The researchers then created the first transgenic line of fish to express a calcium indicator, similar to the green dye, in all neurons, so that the dye no longer had to be injected 4 . Neuroscientists also started to use calcium indicators to label neural circuits in other animals. In a landmark study in 2001, for example, researchers mounted miniature two-photon microscopes \u2014 which can probe more than a millimetre deep into tissues \u2014 on the heads of scampering rats to reveal the firing patterns of many individual neurons at once 5 . Another group observed the brain of a fruitfly that was secured under a microscope while its legs walked freely on top of a polystyrene ball 6 . But when using rodents or flies, researchers must first cut windows in the animals' heads to expose the part of the brain they want to image. And even then, the microscopes can probe only superficial layers of these opaque brain tissues. Only two popular model organisms have small, transparent brains and can be easily genetically engineered: the larval zebrafish and the soil nematode  Caenorhabditis elegans , which has just 302 neurons. But to zebrafish researchers,  C. elegans  is too small and its circuits are too simple. Its brain is also covered in a tight cuticle and its neurons are tiny, making it difficult to make recordings with conventional electrodes. Besides, the worms don't show the same variety of behaviours \u2014 notably, those that require sophisticated vision \u2014 that a fish does. \u201cWe could probably do similar things in  C. elegans ,\u201d says Engert, \u201cbut I'd be worried that the answers aren't as interesting.\u201d When Engert launched his Harvard lab in January 2002, he was determined to focus on the circuitry of larval zebrafish. His former adviser's father had been close friends with Nobel-prizewinning developmental biologist Christiane N\u00fcsslein-Volhard, who helped to pioneer the zebrafish as a model for embryonic development. Still, Engert had never seen one in the flesh. \u201cI was quite shocked when I saw one for the first time; they're so small,\u201d he says. It was an audacious choice, but it fit Engert's personality and penchant for risk. He's known around the Harvard biology building for never wearing shirtsleeves, rollerblading into lectures and riding his motorbike without a helmet. A native German, he did his Harvard tenure talk in lederhosen (he earned tenure in 2009). Last year, he was nearly trapped under an avalanche while skiing off piste in Austria \u2014 shirtless. At Harvard, it took Engert a couple of years to set up his lab experiments for zebrafish. At first, he tried imaging neurons while the fish were swimming freely. But \u201ctheir whole brain wiggles\u201d, he says, making imaging impossible. That's when he started to build the virtual environment. For a study published last May 7 , Engert's postdocs Ruben Portugues and Misha Ahrens built a simple virtual world consisting of red and black stripes that moved under the fish. This visual stimulus, crude as it is, was enough to make the animals feel as though they were being swept backwards by a rushing river, and send out muscle commands to push forwards. With the stroke of a few computer keys, the researchers can manipulate the scene, making the stripes go slower or faster. The tweaks make it seem, to the fish, that its movements have been either too weak or too strong, so it makes adjustments to stay in place. This behaviour is called motor adaptation, and it is akin to what people do when they're walking and suddenly slide on a patch of ice, for instance. The brain takes in the new environmental information and adjusts movements to prevent a fall. Studies in monkeys had revealed that specific neuronal populations were involved in motor adaptation. \u201cIf something happens that's unexpected, it needs to get processed completely differently from if it's expected,\u201d Engert says. The processing \u201ceither tells me that something is happening in the world outside, independent of my motion, or it tells me there's something wrong with my body\u201d. His fish study implicated the same populations of cells, but also showed something new. Certain neurons within these populations encode high-feedback gains \u2014 that is, visual feedback telling the fish that its muscles are stronger than expected \u2014 whereas others respond to feedback that its muscles are weaker than expected. This is the sort of nitty-gritty detail that neuroscientists, hoping to look at the individual neurons within circuits, relish. In the past few years, other labs have revealed similar details. In 2010, for example, researchers in Japan pinpointed specific neurons in the habenula \u2014 a deep brain region that is difficult to study in mammals \u2014 as crucial players in the zebrafish's response to fear 8 . In 2011, Fetcho showed that neurons in the fish's hindbrain are neatly stacked during development in a way that allows the oldest cells to drive the fastest movements and younger cells to control more refined motions 9 . But researchers are less excited by Engert's results than by his technology, which allows them to view every neuron in an entire living, working brain. \u201cYou can't do it in any other animal,\u201d says Martin Meyer, a neuroscientist at King's College London who used calcium imaging to show how different layers of cells in the zebrafish brain respond to objects moving in specific directions 10 . \u201cThere's more or less endless scope, once you have that set up.\u201d Neuroscientists who work on other animals also applaud Engert's technique, although they have some reservations. \u201cIt doesn't give you everything that there is to know,\u201d says Rex Kerr, who studies nematode brains at the Howard Hughes Medical Institute's Janelia Farm Research Campus in Ashburn, Virginia. Kerr notes that the two-photon microscope can't actually image all 300,000 neurons at once. Instead, Engert's group systematically monitors 1,000 or so neurons in 300 subregions, from a total of 32 fish, and then uses a computer model to merge the activity onto a reference brain. For some behaviours, this averaged neural activity can mask interesting activity patterns in individual neurons, Kerr says. Still, a lot of interesting activity is determined by populations of many cells. Zebrafish are \u201cextremely valuable\u201d for looking at those ensembles, he says. \n               Fish futures \n             There are practical reasons why more neuroscientists haven't worked on zebrafish. Many sophisticated behaviours \u2014 such as communication, social interaction and complex emotions \u2014 aren't displayed by the animal at all. And scientists have yet to develop ways to study even some basic reflexes in the fish. That means it is not yet clear what other behaviours Engert's technique will be able to test. Jason Rihel, who is setting up a lab at University College London, would like to use a similar approach to study neurons that produce hypocretin, which are involved in sleep and wakefulness. \u201cIf we could watch the whole brain while we're tickling the hypocretin neurons, or inhibiting them, then we might be able to map out every neuron in the brain that has altered activity,\u201d Rihel says. He is a bit worried, however, that the immobilization will affect the fishes' slumber. Engert, in typical fashion, has ambitious plans. He's got each of his five postdocs and about eight graduate students working on a different zebrafish experiment, from uncomfortably warm baths that test fear-learning to alcohol-rich waters to look at the effects of positive rewards. He is also working on a \u201cside project\u201d that is likely to get a lot of attention later this year: the zebrafish connectome. Engert's team is doing whole-brain functional imaging of live, baby fish while they look at moving bars, and then passing those brains on to Harvard colleague Jeff Lichtman, who will use an electron microscope to trace the anatomical connections. \u201cWe'll have a wiring diagram of the whole brain that can relate structure to function,\u201d Engert says. With these types of resource, Engert and his zebrafish might even find what neuroscientists have been searching for since Cajal: a fundamental principle that describes how circuits interact with one another. \u201cMy life will not be a failure if it doesn't happen, but I'd love to find it,\u201d Engert says. \u201cAnd it's ten times more likely to happen in fish than mice.\u201d \n                     Neuroscience: Making connections 2012-Mar-21 \n                   \n                     Computer modelling: Brain in a box 2012-Feb-22 \n                   \n                     Allen Institute aims to crack neural code 2011-Mar-29 \n                   \n                     Neuroscience: Illuminating the brain 2010-May-05 \n                   \n                     Engert Lab \n                   Reprints and Permissions"},
{"file_id": "493597a", "url": "https://www.nature.com/articles/493597a", "year": 2013, "authors": [{"name": "Brendan Borrell"}], "parsed_as_year": "2006_or_before", "body": "Size limits have been a part of fisheries management for decades, but some fear that they are doing more harm than good. One April day, a fisherman named Johan Norman reeled in a female cod near the Norwegian village of Moskenes, where snow-capped mountains rise straight from the sea. He measured the fish: 82 centimetres from the tip of its snout to the tip of its tail. Then he pulled out his knife and sliced off several scales, placing them in a small envelope to deposit at the Institute of Marine Research in Bergen, Norway. The year was 1913. Over the next century, as those scales sat in a repository, radical changes took place in the world's oceans. The small sailing vessels of Norway and other fishing nations were replaced with industrial bottom trawlers. In 1968, the North Atlantic cod harvest started a precipitous decline, as did other stocks, including salmon, sole and lobster. Then, in the early 1980s, biologists began to report another worrying phenomenon. Fish in some areas were growing more slowly, maturing earlier and laying fewer eggs than before 1 . Not only was this an ominous sign for the sustainability of these fisheries, but smaller fish are less valuable than larger ones because they yield smaller fillets. Explanations for the shrinking fish have ranged from changes in seawater temperatures to a decline in food resources 2 . But the real culprit could be the practices devised to protect the fisheries. As mandated by various laws and treaties, most trawlers' nets sport a large mesh that allows small, young fish to wriggle free. The reasoning is simple: harvest only the oldest, fattest members of the population and let young fish live to spawn and contribute to the next generation. Fisheries scientists and conservationists support size restrictions because they are thought to protect populations, and fishermen are happy to concentrate on large, high-value fish. But what if the underlying theory is wrong? Over the past five decades, scientists have come up with little evidence that reducing the catch of juveniles or small fish has improved the annual harvest. Instead, a small chorus of researchers is now arguing, fish are adapting to size restrictions by investing their energy into reaching sexual maturity earlier instead of growing large (see 'Shrinking fish'). And as a result of their small size, they produce fewer eggs. Although these scientists do not deny that overfishing is the greatest threat to fisheries, they say that this evolutionary pressure will have a pernicious impact that will be hard to reverse. \u201cYou can safely ignore it for a couple of years, but it's accumulative, so the problem keeps growing,\u201d says Mikko Heino, a biologist at the University of Bergen. The theory is controversial, and many scientists are unconvinced. So last year, Heino turned to Norman's 100-year-old preserved cod scales for help. He extracted DNA from them and is piecing together the whole genome sequence of this fish and others in a hunt for changes in growth and development genes that might explain the species' shrinking size. But even if the evolution idea is true, there is some disagreement over what to do about it. Only \u201ca shrinking minority of fools\u201d think that increasing fishing pressure on juveniles is smart or sustainable, says Carl Walters of the University of British Columbia in Vancouver, Canada. The theory of fisheries-induced evolution can be traced back to 1981, when the Canadian fisheries scientist William Ricker suggested that coho salmon ( Oncorhynchus kisutch ) and pink salmon ( Oncorhynchus gorbuscha ) were maturing at a smaller size because Japanese gill-net fishermen were targeting only the largest fish on the high seas 1 . By the 1990s, researchers had begun to notice the phenomenon in other species too. But for many years, the consensus was that environmental factors such as climate change and pollution were at play, not genetics. Then, in 2002, David Conover and Stephan Munch at the State University of New York in Stony Brook published a contentious experiment 3 . They caught Atlantic silverside ( Menidia menidia ) off the coast of Long Island and established six captive populations of around 1,000 individuals each. After 190 days, they removed 90% of the fish from each population. In the first two populations, they took only the largest fish; in the second two they took only the smallest fish; and in the final two they took individuals of random size. They then stimulated the remaining 10% to breed. After four generations, the fish in the large-harvested populations were about one-third the average weight of those in the random-catch group. But critics called the experiment unrealistic. The stimulated breeding essentially created a population with a fixed age at sexual maturity, so it was no surprise that removing larger fish favoured those that matured at a smaller size. By contrast, in a natural population, the size at maturity is relatively stable, but age at maturity varies. Slower-growing fish mature later, and faster-growing fish mature earlier. Thus, size limits could select for faster growth, a possibility that Conover and Munch's experiment did not allow. \u201cI was outraged,\u201d recalls Walters. \u201cThey did an experiment that could only give one result.\u201d \n               Precocious cod \n             The dispute intrigued Heino, a theoretical biologist, who had begun working on his own approach to studying the life history of fish. In the past, researchers would chart a population's maturation reaction norm \u2014 the size and age at which fish typically become sexually mature. But Heino realized that comparisons of maturation reaction norms between populations could be misleading if they didn't take into account the variation in growth rates caused by food availability, climate or other environmental factors. So Heino developed a probabilistic approach that considers growth-rate variations. Using this technique, he showed in a 2004 paper in  Nature 4  that northern cod ( Gadus morhua ) born in 1987 were maturing at a younger age and a smaller size than those born in 1980, and these changes preceded a dramatic collapse of the species off the coast of Canada in the late 1980s and early 1990s (see 'A shift in maturity'). \u201cIt's the most famous fisheries collapse in recent times,\u201d says Heino, \u201cYou would expect the potential for rapid evolution.\u201d Heavy fishing was the main cause of these changes, Heino says, but size-selective fishing compounded the problem. Critics point out that the trend coincided with colder water, heavy sea-ice cover and other factors 2 . Nevertheless, Heino's technique opened up a new field, called Darwinian fisheries management, and evolutionary biologists were soon trying to measure the impacts of size restrictions on other wild populations. A 2009 study 5  used Heino's method to conclude that, of 37 commercial fish stocks, the majority were maturing earlier and at a smaller size than in the past, and that these effects were strongest in heavily fished populations. Jeff Hard, a geneticist with the US National Oceanographic and Atmospheric Administration Fisheries Service in Seattle, Washington, says that in 1976 the largest class of female salmon \u2014 those greater than 100 centimetres in length \u2014 accounted for more than 20% of the fish spawning in one Alaskan river. Today, that number is less than 4%, and the number of eggs that females are producing has declined by 16%. But without genetic data from this and other populations, the findings can always be attributed to environmental changes. \u201cIt's almost impossible to prove these things,\u201d says Andrew Hendry, an evolutionary ecologist at McGill University in Montreal, Canada. That is why Heino and others are looking to the DNA from historical samples of cod and other species for help. Filip Volckaert of the Dutch-language Catholic University Leuven in Belgium, for example, is sequencing DNA from otoliths, or ear bones, of yellowfin sole ( Limanda aspera ) from every decade back to the 1950s to identify genetic changes that might be linked to growth. And Heino is complementing the genetic work with his own brand of lab experiment. Inside a special room at his university, he now has nine populations of guppies, and harvests between one-quarter and one-half of the population on the basis of size. To make the experiment more natural than that of Conover and Munch, he allows the guppies to reproduce freely at any age. And, as in nature, the breeding populations contain a wider range of ages and sizes. He expects the experiment, which he started in 2009, to run until 2014. But it will take a lot to convince the sceptics. \u201cFisheries-induced evolution is an interesting side issue, but it's been greatly overblown,\u201d says Ray Hilborn, a fisheries scientist at the University of Washington in Seattle. There is no question that fished populations are evolving, he says, but some traits, such as earlier age of maturation, may make some fish populations more productive, not less so. The data suggesting that growth rates are slowing are also not yet convincing, he says. The best way to preserve fish populations is simply to fish less, he says. Heino agrees, but wants to see other changes in marine policy. For example, he does not think that marine reserves should protect only spawning grounds \u2014 a common conservation strategy \u2014 because that gives another advantage to early-maturing fish, which return to the spawning grounds to breed sooner than late-maturing fish. Second, he says that it is time to abandon most size limits. Support is growing for these views. Last year, an international group of fisheries experts published a policy paper in  Science 6  rejecting size limits for a wide range of reasons, including evolutionary issues. Jeppe Kolding of the University of Bergen studies small-scale fishing in Africa, and has found that areas where fishermen use illegal nets that catch large and small fish alike tend to have food webs that are diverse, intact and resemble unharvested areas, only with lower biomass. When fishing pressure is spread across species and sizes, he argues, fishermen can net more fish, yet the risk of wiping out individual populations is lower. \u201cHow can you tell me this is a bad fishing method?\u201d he asks. Heino knows that overturning entrenched fishing practices could take decades, and for now he is focusing just on the data. \u201cIt requires patience,\u201d he says. \u201cThe practical implications are something that will keep developing for a long time.\u201d \n                     Ocean conservation: Uncertain sanctuary 2011-Dec-07 \n                   \n                     Overfishing hits all creatures great and small 2011-May-03 \n                   \n                     Newsmaker of the year: In the eye of the storm 2010-Dec-19 \n                   \n                     Fisheries: What's the catch? 2010-Jun-02 \n                   \n                     Hatcheries may not save endangered fish 2007-Oct-04 \n                   \n                     Evolutionary Fisheries Ecology \n                   Reprints and Permissions"},
{"file_id": "494022a", "url": "https://www.nature.com/articles/494022a", "year": 2013, "authors": [{"name": "Hillary Rosner"}], "parsed_as_year": "2006_or_before", "body": "Many tropical species never experience extreme heat or cold. That may doom them in a warming world. Up in the foothills of the Rockies last summer, researchers from Colorado State University in Fort Collins fanned out along the banks of a stream. Some took the water's temperature and measured its speed and chemistry. Others waded in to catch insects using flat-bottomed nets. Aquatic ecologist LeRoy Poff combed through the haul, dropping the live stoneflies, caddis flies and mayflies into separate bins in a tackle box. Back at the lab, these would be further sorted under a microscope and then genetically barcoded \u2014 a way to catalogue the species. Over the past year and a half, the team has run through the same procedure at 25 other streams in Colorado, and 26 in the Ecuadoran Andes, studying how variation in air temperature and water flow have affected insects in the two regions. The effort is part of a five-year project, called EVOTRAC (Evolutionary and Ecological Variability in Organismal Trait Response with Altitude and Climate), which seeks to understand how the evolutionary history of animals will influence their chances of survival in a world altered by global warming and other environmental changes. The project was designed, in part, to test an influential hypothesis proposed in 1967 by ecologist Daniel Janzen. Janzen posited that tropical species have a hard time crossing mountains to expand into new territories because they are adapted to a fairly steady climate that never gets extremely hot or cold. At the time, global warming wasn't on the radar. But now Janzen's ideas have resurfaced as a guide to how climate change might affect biodiversity. Although Janzen's hypothesis is widely accepted, much of it has never been tested directly. The EVOTRAC team, and other biologists, are now trying to remedy that \u2014 and to extend his idea about temperature to other environmental factors that are affected by climate, including stream conditions. If the hypothesis holds, it implies that tropical species may be particularly vulnerable to climate change, even though temperate and polar regions will warm much more than the tropics. The hypothesis could also help researchers to develop conservation plans for dealing with climate change. \n               Just right \n             Janzen's pivotal idea came to him in the mid-1960s, when the young ecologist at the University of Kansas in Lawrence was travelling around Costa Rica with 20 US students and a Costa Rican assistant. The trip began in the assistant's home city of San Jos\u00e9, which had a 'bland' climate that was, Janzen says, \u201cnot too hot, too cold, too wet or too dry\u201d. From there, they travelled to a dry tropical forest at sea level, where the weather was hotter. During a lecture, Janzen saw sweat pouring off his assistant. \u201cAnd I looked at the students and none of them was sweating,\u201d says Janzen. Later, when the group moved to a field station at 3,000 metres elevation in the cold, wet cloud forest, Janzen saw his assistant \u201csitting with piles of blankets on him. Everyone else was sitting there in khakis. I realized this guy has spent his life in San Jos\u00e9, which is like sitting in a climate-controlled cabinet.\u201d \u201cThe thought hit me,\u201d says Janzen, \u201cthat all these tropical animals and plants around me are living in whatever their temperature regime is. And it stays that way.\u201d He reasoned that because tropical species are not exposed to seasonal extremes, they are locked into relatively narrow temperature ranges at specific elevations in the mountains. That prevents most of the creatures from spreading over mountains into adjacent valleys, which limits the flow of genes and increases the overall biodiversity in the tropics, he said. When Janzen published his ideas 1 , funders were not supporting the kind of big, interdisciplinary studies that would be required to test them. But when Cameron Ghalambor read Janzen's paper as a graduate student at the University of Montana in Missoula in the late 1990s, \u201cit was one of the best papers I ever read\u201d, he says. He was struck by the idea that climate could shape so many aspects of a species' history and life cycle, and that this in turn could shape patterns of biodiversity. He told his adviser he wanted to pursue some of Janzen's ideas, but was shot down. \u201cThat's not something you can do in a PhD project,\u201d he recalls being told. \u201cThat's something you do over a lifetime.\u201d But other biologists were already thinking along the same lines. \u201cJanzen's theory is unique in that it couples climate variability, physiology and ecological and evolutionary principles,\u201d says Christy McCain, an evolutionary biologist at the University of Colorado Boulder. Inspired by Janzen's ideas, McCain spent nearly a decade compiling some 80 years of data on the ranges of 16,500 species that live on 170 mountains 2 . \u201cI looked at whether in fact range sizes were smaller on the low-latitude mountains,\u201d she says \u2014 the first part of what has come to be known as Janzen's climate variability hypothesis. Her results were published in 2009 (ref.  2 ). \u201cBats, birds, frogs, salamanders, lizards and snakes all show the trend. It's a linear decrease as you go from high to low latitude.\u201d Strangely, however, McCain found that the opposite was true with rodents, a fact she still struggles to explain. Ghalambor, who is now at Colorado State, had reached a similar conclusion about Janzen's hypothesis in 2006 (ref.  3 ), when he and scientists from the University of Washington in Seattle analysed published studies of vertebrate ectotherms, such as reptiles, which are particularly sensitive to temperature. The team found, for instance, that the altitudinal ranges of tropical reptiles were more limited than those of temperate species. Ghalambor's group wondered whether Janzen's hypothesis could be used to test which species were likely to be most vulnerable to climate change. In 2008, Ghalambor joined up with Raymond Huey, a co-author on the 2006 paper, and other researchers at the University of Washington to examine the physiological tolerance of lizards and other ectotherms across different latitudes. The group found \u2014 just as Janzen's hypothesis suggests \u2014 that the optimum temperatures for tropical species are very close to what they currently experience, which means that the animals may not fare well as temperatures rise 4 . \u201cIf you go to tropical lowland forests, the lizards that occur there are the least heat-tolerant lizards in the world,\u201d Huey says. The temperatures in a tropical forest never get much above 32 \u00b0C and lizards there go into heat stress at 36 \u00b0C or 37 \u00b0C, he says. The implication for a lizard, says Ghalambor, is that \u201ca little bit of warming in the tropics quickly puts you outside your optimum\u201d. \n               Revisiting reptiles \n             To put that idea to the test, Huey has now reunited with a group of scientists he has known since graduate school, who all studied lizards in Puerto Rico in the 1970s. Using their old data as a guide, the researchers are going back to make new measurements to determine whether the lizards on the island are adapting to warming temperatures or showing signs of stress. Back in Colorado, Ghalambor began talking to Chris Funk, an evolutionary biologist who studies amphibians, about ways to test Janzen's hypothesis. They mentioned their interest to Poff, who is an expert on aquatic insects. Poff was intrigued \u2014 and saw a parallel research opportunity to test other factors that influence species' ability to adapt or move. He had spent his career working on streams, which change from one season to the next because of variations in water flow. Extrapolating from Janzen's work, Poff wondered whether insects inhabiting streams that fluctuate strongly have evolved to tolerate much more variation than those living in more constant environments. So he teamed up with Ghalambor and Funk to test whether Janzen's hypothesis applies in the complex stream environment, where climate change is altering both temperature and water-flow patterns. A year and a half into the EVOTRAC project, the scientists have yet to answer that question, but they have discovered that the streams in Colorado and Ecuador seem to contain many cryptic species \u2014 those that look the same but are genetically distinct. If scientists have previously underestimated biodiversity in those spots, and the cryptic species are adapted to very specific ranges of temperature and water flow, then both locales could be more vulnerable to species loss than anyone has thought. After a few hours collecting in the Colorado stream last summer, the EVOTRAC team packed up its collection of flies and headed back home. The two-hour drive wound through Poudre Canyon, 64 kilometres of forest that had been blackened by a massive wildfire just a few weeks earlier. There is a sense of urgency to the work. Record-breaking droughts and fires are taking a toll on the American West, and rapidly melting glaciers in Ecuador are altering rivers and streams at lower elevations. Climate change is already making its mark in the forests of both regions, even as researchers race to forecast how ecosystems will fare in the warming world. \n                     Letters of Alfred Russel Wallace go online 2013-Jan-24 \n                   \n                     Ecology: Bleak future for amphibians 2011-Dec-21 \n                   \n                     Ecology: Moving farther and faster 2011-Oct-27 \n                   \n                     Molecular evolution: Hidden diversity sparks adaptation 2011-Jun-01 \n                   \n                     No safe haven for amphibians 2011-May-31 \n                   \n                     Nature special: Darwin 200 \n                   \n                     The EVOTRAC project \n                   Reprints and Permissions"},
{"file_id": "493290a", "url": "https://www.nature.com/articles/493290a", "year": 2013, "authors": [{"name": "Geoff Brumfiel"}], "parsed_as_year": "2006_or_before", "body": "After the Fukushima nuclear disaster, Japan kept people safe from the physical effects of radiation\u00a0\u2014\u00a0but not from the psychological impacts. The first thing Kenichi Togawa does when he comes home from work is switch on his video-game console. The 39-year-old father of three spends hours each evening playing video games and drinking  shochu , a strong Japanese liquor. He often falls asleep in front of the television, then wakes up shivering and crawls into bed with his wife, Yuka. For nearly two years, Kenichi and his family have been refugees from the worst nuclear disaster in 25\u00a0years. On 11\u00a0March\u00a02011, a giant earthquake struck off the northeast coast of Japan, sending a 13-metre-high wall of water into the Fukushima Daiichi nuclear power station and triggering meltdowns in three of the six reactors. The next day, just hours before the Unit 1 reactor exploded, the Togawa family fled their home 10\u00a0kilometres from the plant. Today, they live in a tiny flat outside the evacuation zone\u00a0\u2014\u00a0one of dozens in a series of slate-grey temporary buildings in the northeast section of Fukushima prefecture. The five Togawas are bundled into three rooms totalling just 30\u00a0square metres, with windows poorly insulated against the winter winds. The past 18\u00a0months have taken a mental toll on the family. Kenichi, who had worked at the nuclear plant, was once a keen judo fighter who went out often with friends, but the radioactivity has scattered his martial-arts club. These days, he exercises less and rarely socializes. He drinks more and has put on weight. Yuka is prone to public outbursts of anger, unusual among Japanese women in the relatively traditional Fukushima prefecture. She is happy when she thinks about day-to-day life, but when her mind turns to the long term, as it inevitably does, she feels depressed. \u201cThis is temporary,\u201d she says. \u201cWe leave our house in the morning and we come home and it\u2019s temporary. It\u2019s like floating in the air.\u201d Other people they know are struggling even more. Many of their current neighbours are out of work and stay at home all day. Some of Kenichi\u2019s former colleagues sent their wives and children away, from fear of radioactive contamination, while they stayed to work. In the immediate aftermath of the nuclear accident, public-health experts worried about the possible risk from radiation. Subsequent analyses have shown that the prompt, if frantic, evacuation of areas around the reactors probably limited the public\u2019s exposure to a relatively safe level (see \u2018The evacuation zones\u2019). But uncertainty, isolation and fears about radioactivity\u2019s invisible threat are jeopardizing the mental health of the 210,000\u00a0residents who fled from the nuclear disaster. Researchers and clinicians are trying to assess and mitigate the problems, but it is unclear whether the Japanese government has the will, or the money, to provide the necessary support. Nor is it certain that the evacuees will accept any help, given their distrust of the government and their reluctance to discuss mental problems. This combination, researchers fear, could drive up rates of anxiety, substance abuse and depression. The nuclear evacuees face a more difficult future than the survivors of the tsunami, which left nearly 20,000\u00a0dead or missing and caused billions of dollars in damage. \u201cThe tsunami-area people seem to be improving; they have more positive attitudes about the future,\u201d says Hirooki Yabe, a neuropsychiatrist at Fukushima Medical University, who has been working with both groups. Nuclear evacuees \u201care becoming more depressed day by day\u201d. \n               Escape \n             Fukushima prefecture is a patchwork of orchards, rice paddies and fishing villages. In the 1970s and 80s, coastal residents welcomed nuclear power and two plants were built to supply electricity to Tokyo. Kenichi started working at Fukushima Daiichi in 1994, and at the time of the accident was a contract maintenance engineer. Yuka worked as a hospital nurse. The Togawas and their children, now aged 9, 12 and 15, lived in a four-room flat in Namie, a small, close-knit seaside town. The family\u2019s life was upended at 14:46 on 11\u00a0March 2011. Kenichi was in the smoking room at the plant when he felt the ground shudder for several minutes. He ran back to his office, weaving around scattered desks and downed ceiling panels, and grabbed his driving licence and car keys. But he quickly found that traffic had clogged the routes out of the plant because the quake and tsunami had destroyed bridges and roads. Kenichi ditched his car and walked the remaining 8\u00a0kilometres home. There he found that all his family members were safe, but he worried about the plant. In his job, he had overseen maintenance of the systems meant to cool the reactor in an emergency. If those had failed, he knew that a meltdown would soon follow, spreading radiation to nearby towns. That night, as aftershocks rocked the house, his family slept fitfully with the lights and television on. Kenichi was right to be alarmed. The tsunami had knocked out the generators that pumped cooling water into the reactor cores. As temperatures rose, the slender fuel rods full of uranium pellets began to warp. The meltdowns had begun. Early the next morning, a siren wailed across Namie, signalling an evacuation. The Togawas were told to move to Tsushima, 30\u00a0kilometres to the northwest. After retrieving their car, the family set out, but the roads were choked with panicked residents and the Togawas ended up at a different evacuation centre. When Kenichi learned that the emergency diesel generators at the plant had also failed, he bundled the family into the car once more, hoping to reach Tsushima. \u201cWe have to run away,\u201d he remembers thinking in a panic. On the way, Kenichi received a text message from a friend who worked at the power company\u2019s offices in Tokyo. Unit 1 had exploded, and radioactivity was spreading across Fukushima. The family drove from one full evacuation centre to the next, until they reached a dark, cramped gymnasium in Kawamata, around 40\u00a0kilometres northwest of the plant. There they were given a small patch of hardwood floor to call home. But they were still deeply worried about the radiation. \u201cWe didn\u2019t know much about radiation\u2019s effects, and we didn\u2019t know if Kawamata was safe or not,\u201d says Yuka. Japan is used to natural disasters, and immediately after the tsunami hit, the country\u2019s emergency services sprang into action. Groups of doctors and emergency workers from around the nation struck out for the northeast coast to begin search and rescue operations and to administer care. The medical university in Fukushima City became a hub. In the days and weeks after the accident, the university hospital took in seriously ill patients from the coast. It also found itself on the front line of the nuclear emergency: doctors used Geiger counters to screen evacuees\u2019 thyroid glands, which are particularly sensitive to radiation, and treated several workers from the nuclear plant, who had suffered high radiation exposures. \n               First responders \n             Mental-health experts were among the first responders, reflecting an ongoing change in Japan\u2019s attitudes towards mental health. For many years, Japan\u2019s modest but modern mental health services were geared to help only the most severely mentally ill. The society has traditionally paid little attention to more routine disorders such as depression. In recent years, however, the Japan Medical Association has started educating doctors about depression and suicide, and the national government has conducted public suicide-prevention campaigns. Still, the quality of care remains patchy, and even before the accident, Fukushima prefecture was not a bright spot. Mental health was a not a priority for the rural, conservative region or its taciturn citizens. As a result, the tsunami and nuclear disaster strained the region\u2019s mental-health services to near breaking, says Yabe. In the wake of the accident, most of the prefecture\u2019s resources were devoted to helping those with established mental disorders. Yabe, for example, packed his car with antipsychotic and anticonvulsive medication and made runs to Soma City, where many evacuees had ended up. Mental-health professionals visited the cramped shelters elsewhere, but they tended to treat only the most severe cases of delirium and post-traumatic stress disorder. The Togawas were among thousands of people left to their own devices by the overwhelmed doctors and counsellors. The family\u2019s first days in the cramped shelter are difficult to recall now, says Yuka, but what she can remember isn\u2019t pleasant: ill and elderly patients lying on the floor; ongoing fear about radiation; evacuees jumping queues and snatching food. \u201cWe were like dogs and cats without chains,\u201d she says. With little guidance from the outside, the shelter\u2019s residents tried to organize themselves. Yuka volunteered her nursing skills, but after working for three days, she was filled with anger: why should she, a victim, have to spend all her time helping others, she wondered. Yuka locked herself in the family\u2019s car outside the shelter, \u201cand just exploded and screamed and shouted and cried\u201d. \n               Subtle damage \n             As the evacuees struggled to adjust, so too did the doctors and psychologists at Fukushima Medical University. By May, the emergency response was mostly over and the hospital had a new job\u00a0\u2014 to assess the public\u2019s radiation dose. The task has proved tricky, says Shunichi Yamashita, a radiation health expert at Nagasaki University, who was brought in to head the Fukushima Health Management Survey. The radiation monitors around Daiichi were damaged or destroyed by the earthquake and tsunami, and the chaotic nature of the evacuation makes it difficult to assess how long and severely each person was exposed. The few attempts made so far, however, have generally shown minimal risk. The health survey\u2019s latest assessment suggests that the dose for nearly all the evacuees was very low, with a maximum of only 25\u00a0millisieverts (mSv), well below the 100-mSv exposure that has been linked to an increased risk of cancer in survivors of the atomic bombs dropped on Hiroshima and Nagasaki in 1945. The World Health Organization also issued a reassuring report in May, saying that most evacuees from places like Namie received estimated doses between 10 and 50\u00a0mSv. It did note, however, that infants might have received a dose that could increase the risk of cancer in their still-developing thyroids. Radiation specialists say that it is difficult to predict the health effects from such low doses. \u201cI think it\u2019s likely that there will be increased cancer risks, but they will be very, very small,\u201d says Dale Preston, an independent statistician who has studied atomic-bomb survivors. \u201cIf you did a large study, I think your chance of observing a statistically significant radiation-associated risk would be pretty low.\u201d With that in mind, the health survey decided against following a fixed cohort to study the incidence of disease. Instead, it provides thyroid screening and other health checks to any evacuees who desire them. The hope is that the screenings themselves, along with the data collected, will help to reassure the public that the risks are low, says Yamashita. Mental health has been a major component of the survey. In January 2012, researchers sent out questionnaires to all 210,000\u00a0evacuees to assess their stress and anxiety. The levels tabulated among the more than 91,000\u00a0respondents were \u201cquite high\u201d, says Yuriko Suzuki, a psychiatrist at the National Institute of Mental Health in Tokyo. Roughly 15% of adults showed signs of extreme stress, five times the normal rate, and one in five showed signs of mental trauma\u00a0\u2014 a rate similar to that in first responders to the attacks of 11\u00a0September\u00a02001 in the United States. A survey of children, filled out by their parents, showed stress levels about double the Japanese average. The stress has pushed some evacuees to breaking point. On a crisp day last November, Kenji Ookubo wandered through Iitate, a village 40\u00a0kilometres northwest of the plant, practising his golf swings in the empty streets. The town had been evacuated after the accident because it lay in the path of the plume of radiation blowing away from the plant. But Ookubo couldn\u2019t stand the temporary housing, where he had started drinking and suffered from stomach aches. After renting a room in Kawamata, he began squatting in his parents\u2019 abandoned home. \u201cI came back just to run away from the stress,\u201d he says. With no job, and no prospects, \u201cI can\u2019t see the future,\u201d he says. It is a pattern seen frequently after major catastrophes, says Ronald Kessler, a professor of health-care policy at Harvard Medical School in Boston, Massachusetts. \u201cIn the short term, people get energized,\u201d he says. But when extensive damage or health problems prevent them from getting back to their old lives, depression and anxiety set in. \u201cWhen something this big happens, it\u2019s just ridiculously daunting,\u201d he says. \u201cAt a certain point you just get worn down.\u201d His own surveys of people evacuated after Hurricane Katrina, which struck the United States in 2005, show 1  that property loss and health concerns were the main causes of anxiety. Whereas many survivors of the Japanese tsunami have seen their homes rebuilt and lives restored, nuclear refugees are still dealing with both of those problems. Above all, the fear of radioactivity takes a unique toll. \u201cIt\u2019s something you don\u2019t feel; you don\u2019t notice what happened, and yet you understand that there are these long-term risks,\u201d says Preston. \u201cIt\u2019s scary.\u201d Little is known about the long-term effects of that fear, in part because nuclear accidents are so rare. But the 1986 disaster at the Chernobyl nuclear power plant in Ukraine suggests that fear of radiation can cause lasting psychological harm. Two decades after the accident, those who had evacuated as children complained of physical ailments more often than their peers, even though there was no difference in health 2 . And the mothers of those children suffered from post-traumatic stress disorder at about twice the rate of the general population, says Evelyn Bromet, a psychiatrist at the State University of New York in Stony Brook. Other studies of Chernobyl\u2019s aftermath found that evacuees had elevated rates of depression 3  and that a subset of clean-up workers committed suicide at a rate about 1.5 times that of the general population 4 . For Fukushima evacuees, says Bromet, \u201cThere\u2019s going to be a tremendous amount of health-related anxiety and it\u2019s not going to go away easily.\u201d \n               Fear factor \n             Yabe says that \u201cradiophobia\u201d remains a major problem among the Japanese refugees. A poll published last year by the Pew Research Center in Washington DC, for example, found that 76% of Japanese people believed that food from Fukushima was not safe, despite government and scientific assurances to the contrary. And many do not trust the government health surveys that found very few cases of significant radiation exposure among evacuees. Yuka shares some of those concerns. She and Kenichi have educated themselves, and they have gained some reassurance from regular health checks and thyroid screenings. The children carry dosimeters provided by the health survey to collect radiation data and to calm public concerns. But Yuka wonders whether they will one day develop cancer. At the moment, however, the family is preoccupied with practical concerns. The government has said that the Togawas can remain in their small flat until August 2014, but after that, Yuka says, they don\u2019t know what will happen. \u201cThe government officials say that they are working on it and that they are trying to construct public housing for those people who had to evacuate. But where? Nothing is clear.\u201d Whenever she and Kenichi think about the long term, they start to feel depressed. The scientists involved with the Fukushima Health Management Survey have assigned a team of psychiatrists and nurses to make follow-up phone calls to individuals who had high scores for distress on the mental-health questionnaire. But only about 40% of adults responded to the questionnaires, and the researchers suspect thatthe most severely affected people did not participate. Even when the psychiatrists can connect, the evacuees usually don\u2019t stay on the phone for more than five to ten minutes. \u201cNorthern people are a very closed people, they don\u2019t really talk about their personal things, especially to somebody they\u2019ve never met before,\u201d says Yabe. Even when the psychiatrists identify problems, it is unclear what to do about them. Most evacuees, like the Togawas, are suffering from sub-clinical problems\u00a0\u2014 mental anxiety and stress that affects their everyday life but does not require hospitalization or extensive therapy. There is no established treatment regime for such survivors from large disasters, says Suzuki. Yabe suggests that walk-in clinics specializing in mental health could be set up throughout Fukushima prefecture to engage communities and help families. Suzuki says that involving large segments of the population in group-therapy sessions might be the way forward. Many say that it would help for evacuees to develop a sense of community \u2014 but the government has not fostered that. Temporary houses are \u201cstrung out like a railroad\u201d, says Bromet. The government could have built them \u201cin a circle with a playground in the middle, or some obvious place for people to meet, but they didn\u2019t\u201d, she says. Kessler says that unlike the tsunami survivors, whose grief will lessen over time, the nuclear evacuees could experience growing anxiety, particularly about radiation. \u201cWhen everything has settled down, that will be a huge, rife issue,\u201d he predicts. Now is the best time to try to get ahead of these problems, he says. \u201cThere\u2019s a window of opportunity.\u201d But the health survey lacks the funding for a more ambitious programme. The national government has given it just \u00a53\u00a0billion (US$34\u00a0million) a year, but it is currently consuming about twice that amount, so the survey is under enormous financial pressure, says Seiji Yasumura, one of its leaders and an epidemiologist at Fukushima Medical University. So far, only 100\u00a0of the 210,000\u00a0evacuees have been interviewed face-to-face by mental-health experts. Little by little, things are getting better for the Togawas. The children seem happy in their new school, and in September 2011 Kenichi found a job with the local government, clearing contaminated soil from the homes of neighbours. \u201cHe\u2019s worked so much overtime that his company is saying he has to have a break,\u201d boasts Yuka. She has found part-time work as a nurse in a local clinic. Her occasional outbursts sometimes cause tension with co-workers, but she enjoys speaking her mind: \u201cI say what I want to say.\u201d After filling out one of the health-survey questionnaires last year, Yuka got a flyer in the post inviting her to talk to someone over the phone. She thought about it but decided not to. \u201cI don\u2019t feel like phoning. It\u2019s been nearly two years,\u201d she says. \u201cI don\u2019t know what to say.\u201d See Editorial  page 271 \n                     Fukushima\u2019s doses tallied 2012-May-23 \n                   \n                     Japan's nuclear crisis: Fukushima's legacy of fear 2012-Mar-07 \n                   \n                     Chernobyl's legacy 2011-Mar-28 \n                   \n                     Nature special: Japan earthquake and nuclear crisis \n                   \n                     Fukushima Health Management Survey \n                   \n                     Tokyo Electric Power Company \n                   Reprints and Permissions"},
{"file_id": "493592a", "url": "https://www.nature.com/articles/493592a", "year": 2013, "authors": [{"name": "Maggie McKee"}], "parsed_as_year": "2006_or_before", "body": "We may be seeing some of the Solar System's most striking objects during rare moments of glory. Ever since Copernicus evicted Earth from its privileged spot at the centre of the Solar System, researchers have embraced the idea that there is nothing special about our time and place in the Universe. What observers see now, they presume, has been going on for billions of years \u2014 and will continue for eons to come. But observations of the distant reaches of the Solar System made in the past few years are challenging that concept. The most active bodies out there \u2014 Jupiter's moon Io and Saturn's moons Enceladus and Titan \u2014 may be putting on limited-run shows that humans are lucky to witness. Saturn's brilliant rings, too, might have appeared relatively recently, and could grow dingy over time. Some such proposals make planetary researchers uncomfortable, because it is statistically unlikely that humans would catch any one object engaged in unusual activity \u2014 let alone several. Maggie McKee discusses the activity in the Solar System. The proposals also go against the grain of one of geology's founding principles: uniformitarianism, which states that planets are shaped by gradual, ongoing processes. \u201cGeologists like things to be the same as they ever were,\u201d says Jeff Moore, a planetary scientist at the NASA Ames Research Center in Moffett Field, California. The unchanging world is \u201cphilosophically comforting because you don't have to assume you're living in special times\u201d, he says. But on occasion, the available evidence forces researchers out of their comfort zone. Here,  Nature  looks at some of the frozen worlds that may be putting on an unusual spectacle. \n               Saturn's rings \n             Researchers have long thought that Saturn acquired its dazzling adornments early in its life, some 4 billion years ago. The rings could be the glistening remnants of a shattered moon or a comet pulled apart by the giant planet's gravity. But some planetary scientists say that the rings' resplendence is hard to reconcile with a lifetime lasting billions of years 1 . The rings' particles are 90% water ice and should darken over time as they are struck by carbonaceous dust shed from comets and asteroids. \u201cIf you look at the rings of all the other planets \u2014 Jupiter, Uranus and Neptune \u2014 those rings are all very dark,\u201d says Jeff Cuzzi, a planetary scientist at Ames. \u201cThat's kind of what you'd expect from heavily polluted material.\u201d According to Cuzzi, the sparkle of Saturn's rings suggests that something \u2014 perhaps an icy interloper from beyond Neptune or a large moon of Saturn itself \u2014 might have broken apart near the planet and formed the rings within the past few hundred million years, less than 10% of the planet's life so far. The brilliance would be fleeting, because the rings would \u201cget duller and duller\u201d over time, says Cuzzi. But the idea of young rings presents its own puzzle. Large bodies of the kind that could have formed the bands flew helter-skelter through the Solar System during its first 700 million years or so, but they have grown much rarer since then. There is only a minuscule chance that such a large object whizzed past Saturn in the past one billion years, says Cuzzi. Likewise, he adds, it would be difficult to explain how a moon large enough to form the rings could have fallen close enough to the planet in that time frame. Another possibility is that the rings formed billions of years ago, but somehow retained their youthful glow. That could be the case if they are at least ten times more massive than previously thought, so the dust has so far had little effect. \u201cIf you have a thimbleful of black paint, and you drop it into a gallon of white paint, you'll make it pretty dark,\u201d says Cuzzi. \u201cBut if you drop it into a swimming pool, you won't.\u201d That explanation appeals to Robin Canup, associate vice-president of the planetary-science directorate at the Southwest Research Institute in Boulder, Colorado. \u201cI know of no way to form the rings recently with any reasonable probability,\u201d she says. There is no evidence of any missing mass yet. But it could be hiding in the biggest ring, dubbed the B ring, which is so opaque that researchers cannot study its contents by measuring how light passes through it. The solution to this puzzle could come soon from the Cassini spacecraft, which has been orbiting Saturn since 2004. In 2017, at the end of Cassini's planned lifetime, mission controllers will send it between the planet and the innermost D ring. Comparing the spacecraft's motion at different orbital distances will reveal the rings' mass with unprecedented precision, says Cuzzi. But Canup warns that \u201cif the Cassini results point to a low mass for the rings, it will be a real mystery\u201d. \n               Enceladus \n             Enceladus is a fairy moon. As it orbits Saturn, it sprinkles a glittering trail of ice \u2014 the E ring \u2014 thanks to watery geysers that shoot from its south pole. But researchers have struggled to explain how it can sustain such activity. Enceladus seems to be giving off 16 gigawatts of heat: ten times as much as theorists think it should be able to produce from the decay of radioactive elements in its interior and from the simplest models of tidal heating, the kneading and flexing of the moon caused by Saturn's powerful gravity. Several explanations have been put forward to account for this furious release of heat, but all rely on arguments that researchers are viewing the moon at a special time. One such proposal, advanced by planetary scientists Craig O'Neill of Macquarie University in Sydney, Australia, and Francis Nimmo of the University of California, Santa Cruz, suggests that over the course of between 100 million and 1 billion years, the internal stresses and strains from tidal forces could build up enough heat to crack the moon's crust, releasing energy and water vapour into space 2 . Such activity would last for only about 10 million years before the crust cooled and the geysers died. Then the heat-storage cycle would start anew. \u201cIt seems like special pleading \u2014 we just happened to catch it in the act,\u201d says O'Neill, echoing criticisms that he has heard when presenting the model at conferences. But he points out that the cycle would be just like those of the geysers in Yellowstone National Park in the United States, except on a longer timescale. Episodic tectonic activity could also explain another discrepancy: why parts of the moon appear to be different ages, with some areas heavily pockmarked by craters and other, fresh-faced regions that have presumably been plastered over by newer crust. A similar patchwork of surfaces is seen on a few other moons, including Jupiter's giant Ganymede and Uranus's small moon Miranda. If these have also gone through cycles of activity, it would make Enceladus less of an outlier. At any given time, there would be a good chance that at least one of them would be passing through a lively period, says O'Neill. The mystery, then, is why Saturn's moon Mimas, which lies closer to the giant planet than Enceladus and therefore experiences greater tidal forces, shows no sign of tectonic activity. Nimmo says that Mimas may have a different internal composition, making it too rigid to deform, but he acknowledges that this is just one possibility. \u201cMimas should be producing more heat than Enceladus and it doesn't, and we don't really understand why,\u201d he says. Cassini will collect more clues when it snaps images of Enceladus's south pole between 2015 and 2017, gathering measurements that could refine estimates of the geysers' heat output. \n               Io \n             In terms of heat, Enceladus is a firefly in comparison with the furnace of Jupiter's moon Io. The most volcanically active body in the Solar System, Io harbours hundreds of volcanic features, some of which spew plumes of sulphur and sulphur dioxide 500 kilometres into space \u2014 a distance that from Earth would reach further than the International Space Station. But the 90,000 gigawatts of heat released by Io is several times more than would be expected from the simplest models of tidal interactions between the moon and Jupiter. That mismatch suggests that \u201cIo is more volcanically active in some periods than others\u201d, says David Stevenson, a planetary scientist at the California Institute of Technology in Pasadena. One possible explanation is that the shape of Io's orbit changes periodically. Io currently takes a slightly elongated, or eccentric, path around Jupiter, thanks to the gravitational influence of two other moons, Europa and Ganymede. Every time Io makes a circuit of Jupiter, the other moons give it a push, \u201cjust like a child on a swing\u201d, says Stevenson, preventing Jupiter's gravity from pulling Io into a perfectly circular orbit. The eccentric path intensifies the tidal warping, which deforms Io's surface by about 10 metres on each orbit. The frictional heat from all that warping gets released through volcanic eruptions. But the same process steals energy from the orbit, so that Io might not be able to swing as far away from Jupiter on subsequent rounds. Ultimately, as energy is drained into internal heating, Io's path could become more circular, causing the tidal forces to weaken and the moon to cool. Then, over the course of millions of years, Europa and Ganymede could push Io into a more eccentric orbit \u2014 one with several times its current eccentricity, says Stevenson \u2014 and the process could begin again. Val\u00e9ry Lainey, a planetary scientist at the Paris Observatory, agrees that there may be cyclical variations in Io's orbit. Some support for that hypothesis comes from observations of Io over more than a century, which show that its orbit may be growing more circular 3 . If so, the moon's raging volcanic activity might be on the wane. Such orbital transformations \u201cwould satisfy the data\u201d, says Stevenson. But even though cyclical patterns abound in nature, he says, Io's behaviour, like that of Enceladus, seems so strikingly variable \u201cthat it's possible we simply don't understand them\u201d. \n               Titan \n             When Cassini dropped its Huygens probe through the haze-shrouded atmosphere of Saturn's biggest moon in 2005, it revealed a landscape of sinuous river channels that seems much like Earth's except for one big twist: The liquid that sculpts much of the surface is methane that rains down from hydrocarbon clouds. Yet the atmospheric methane \u2014 and its effects on the landscape \u2014 ought to be short-lived. Sunlight degrades methane, driving reactions that turn it into heavier hydrocarbons, which should deplete Titan's atmospheric reservoir in a few tens of millions of years. Either researchers are witnessing Titan at a rare moment, not long after a massive release of methane into the atmosphere, or \u2014 as many believe \u2014 something is replenishing what sunlight destroys. Cassini revealed a number of what might be ice volcanoes that pump methane up from the moon's interior. That plumbing process could be driven by heat from the decay of radioactive elements inside the moon and from tidal tugs from Saturn. One of these candidate volcanoes is Titan's highest known mountain, Doom Mons, which lies beside the moon's deepest known pit in a region called Sotra Facula. Rosaly Lopes, a planetary scientist at NASA's Jet Propulsion Laboratory in Pasadena, suggests that deposits in that area were formed from methane-rich slush that erupted from the mountain, causing the nearby terrain to collapse. Moore takes a different view, arguing that other processes, such as impacts and erosion by methane rivers, could have created the supposed volcanic features 4 . He thinks that researchers are seeing Titan at a unique and geologically fleeting time. In his view, methane and nitrogen \u2014 the main component of Titan's atmosphere \u2014 were frozen on the moon's surface until a few tens or hundreds of millions of years ago. At that point, the Sun, which has been growing warmer over its 4.6-billion-year life, vaporized these ices, forming a methane-rich atmosphere within a million years or so. Methane then condensed from the atmosphere and \u201crained like hell\u201d over the moon, creating the landscape features, says Moore. Gradually, sunlight turned the methane into heavier hydrocarbons, and the rain tapered off. In another 40 million years or so, says Moore, the methane could completely disappear, and Titan could revert to a nearly unchanging tableau, with blue, nitrogen-filled skies rising above a reddish, hydrocarbon-covered surface. Ralph Lorenz of the Johns Hopkins University Applied Physics Laboratory in Laurel, Maryland, argues that Moore's picture is too simplistic. Some evidence suggests, he says, that it would have taken billions of years for the destruction of atmospheric methane to form the hydrocarbon-filled dunes that cover 20% of Titan's surface. If that is so, the liquid-methane cycle has persisted for much of the moon's history. Continuing observations by Cassini will reveal how much Titan's surface changes on timescales of a few years \u2014 allowing researchers to better estimate how long methane rain has been sculpting it. \u201cI think we have to have a much more nuanced view of Titan through time,\u201d says Lorenz. \u201cTitan is bloody complicated.\u201d \n                     Planetary science: Landscapes on Titan 2012-Dec-21 \n                   \n                     Origin of Saturn\u2019s rings and inner moons by mass removal from a lost Titan-sized satellite 2010-Dec-12 \n                   \n                     The role of episodic overturn in generating the surface geology and heat flow on Enceladus 2010-Jan-10 \n                   \n                     Strong tidal dissipation in Io and Jupiter from astrometric observations 2009-Jun-18 \n                   \n                     Nature special: Titan \n                   \n                     Blogpost: Building blocks of proteins discovered in Titan's atmosphere \n                   \n                     Blogpost: Enceladus spewing boulders? \n                   \n                     Cassini at the European Space Agency \n                   \n                     NASA on Saturn's rings \n                   \n                     Uniformitarianism at the University of California Museum of Paleontology \n                   Reprints and Permissions"},
{"file_id": "493286a", "url": "https://www.nature.com/articles/493286a", "year": 2013, "authors": [{"name": "Ed Yong"}], "parsed_as_year": "2006_or_before", "body": "Bob Paine fathered an idea \u2014 and an academic family \u2014 that changed ecology. Bob Paine is nearly 2 metres tall and has a powerful grip. The ochre sea star, however, has five sucker-lined arms and can span half a metre. So when Paine tried to prise the creatures off the rocks along the Pacific coast, he found that his brute strength simply wasn't enough. In the end, he resorted to a crowbar. Then, once he had levered the animals up, he hurled them out to sea as hard as he could. \u201cYou get pretty good at throwing starfish into deeper water,\u201d he says. It was a ritual that began in 1963, on an 8-metre stretch of shore in Makah Bay, Washington. The bay's rocky intertidal zone normally hosts a thriving community of mussels, barnacles, limpets, anemones and algae. But it changed completely after Paine banished the starfish. The barnacles that the sea star ( Pisaster ochraceus ) usually ate advanced through the predator-free zone, and were later replaced by mussels. These invaders crowded out the algae and limpets, which fled for less competitive pastures. Within a year, the total number of species had halved: a diverse tidal wonderland became a black monoculture of mussels 1 . By re-engineering the coastline in this way 1 , Paine dealt a serious blow to the dominant view in ecology of the time: that ecosystems are stable dramas if they have a diverse cast of species. Instead, he showed that individual species such as  Pisaster  are prima donnas, whose absence can warp the entire production into something blander and unrecognizable. He described these crucial creatures, whose influence far exceeds their abundance, as keystone species, after the central stone that prevents an arch from crumbling. Their loss can initiate what Paine would later call trophic cascades \u2014 the rise and fall of connected species throughout the food web. The terms stuck, and 'keystone' would go on to be applied to species from sea otters to wolves, grey whales and spotted bass. Today, ecology students take these concepts for granted \u2014 but they shook the field when Paine first articulated them in the 1960s. \u201cHe's been one of the most influential ecologists in the last half century,\u201d says Simon Levin, a mathematical ecologist at Princeton University in New Jersey, and one of Paine's closest friends. The revelation that not all species are equal was as disruptive to ecology as the loss of  Pisaster  was to Makah Bay. So was Paine's insistence on tinkering with nature \u2014 what some have called kick-it-and-see ecology \u2014 at a time when most ecologists simply observed it. But Paine \u2014 an organism whose disproportionate influence equals that of any starfish or sea otter \u2014 has also changed the ecosystem of scientists. In his five-decade career, he has trained a thriving dynasty of around 40 students and postdocs, many of whom are now leading ecologists themselves and who consider their time with Paine formative. They include Paul Dayton at the Scripps Institution of Oceanography in La Jolla, California, who has shaped understanding of rocky shores, kelp forests and Antarctica's sea floor; Bruce Menge at Oregon State University in Corvallis, who expanded Paine's research to coasts worldwide; Jane Lubchenco, who heads the US National Oceanic and Atmospheric Administration (NOAA) in Washington DC; and Steve Palumbi at Stanford University in California, who used genetics to track the illegal trade of whale and dolphin meat. \u201cThere are other ecologists as famous as Bob, but if you look at their list of students, there aren't nearly as many whom you know by reputation,\u201d says Chris Harley at the University of British Columbia in Vancouver, Canada, who was one of Paine's most recent PhD students. Once Paine's students' students are taken into account, his academic family easily stretches into the hundreds. \u201cEveryone is tied to Paine,\u201d says Craig McClain, a deep-sea biologist at the National Evolutionary Synthesis Center in Durham, North Carolina, who is three links removed \u2014 his postdoctoral adviser was Dayton's student. \n               Family values \n             Science hosts many such dynasties: successions of academic leaders related not by blood, but by mentorship. Each generation inherits attitudes, philosophies and technical skills from the one before. Some, like Paine's, are particularly fertile, sprouting lush branches on the academic tree and driving a field in a new direction. But Paine's dynasty is remarkable not just for its scientific influence, but for its dedicated, tight-knit nature. Thanks to Paine's original \u2014 and widely applicable \u2014 ideas, his emphasis on independent thought by his prot\u00e9g\u00e9s and his fun, irreverent nature, almost every member has stayed in science, and specifically in ecology or marine biology. \u201cIt's a surprising list of superstars \u2014 great mentors of graduate students, who have published interesting work,\u201d says Paine, who retired in 1998 but is still active in the field. These days, Paine can be spotted at ecological meetings by the swarm of academic descendants milling around him. Perhaps in this rich family, there are lessons about why some scientific dynasties flourish and grow, whereas others never bud. Paine's name is synonymous with coastal life, but his introduction to natural history began on terra firma. As a child in Massachusetts, he went on regular birdwatching walks with a neighbour, who insisted that he record everything he saw. \u201cThat was extraordinarily good training,\u201d says Paine; it instilled an appreciation for nature and careful observation. After studies at Harvard University in Cambridge, Massachusetts, and two years' military service, he got his PhD in zoology from the University of Michigan in Ann Arbor, under the late Fred E. Smith. A qualified ecologist in search of an ecosystem, Paine started his own group at the University of Washington in Seattle in 1962 and embarked on his seminal work in Makah Bay. But despite its abundance of starfish, the bay was plagued with marauding beachcombers, who would tamper with Paine's experiments. He craved isolation. He found it in 1967, after a salmon-fishing trip in the Pacific Ocean, when on a whim he landed on a small island called Tatoosh, just off Washington's Olympic Peninsula. \u201cI just sat around and gawked,\u201d he says. \u201cThe place was a wonderland of pattern.\u201d He saw hundreds of species jostling for space and dominance. He saw starfish prising open mussels, and predatory sea snails drilling into barnacles. He saw whales and sea lions swimming offshore, while raptors and seabirds hovered overhead. And he saw \u2026 no one else. With the island protected by the local Makah tribe, there was no one around to ruin his experiments. \u201cI said: this is where I'm going to work.\u201d \n               Island retreat \n             Paine repeated his starfish-clearing experiment on Tatoosh, and saw the same events unfold on a bigger scale: the loss of  Pisaster  triggered a black landslide of mussels that crushed its way down 40 metres of coast. Elsewhere, in a zone dominated by brown algae, Paine systematically added or excluded seven grazing animals by building rings of paint and putty 2 . Two of the species \u2014 a sea urchin and a mollusc called a chiton \u2014 would annihilate the algae when present in high numbers. The others, all molluscs, had no effect. \u201cIt was a colossal effort,\u201d says Paine, and the first time that anyone had quantified the influence of so many species in a community. It showed that most are weak interactors, whose absence goes unnoticed. Only a few \u2014 including keystone species \u2014 are strong interactors that can radically reshape their world. \u201cIt was a starting place for untangling the complexity of interactions,\u201d says Paine. \u201cIf all species were created equal, you wouldn't know where to start.\u201d Paine's views on experimental ecology rapidly percolated through the field, and attracted waves of eager students. \u201cIt was a pioneering stage in the field,\u201d says Menge. \u201cWe felt that we were really the first ones to be doing these sorts of experiments.\u201d Tatoosh gave them a place to experiment, and most of Paine's prot\u00e9g\u00e9s have done tours on its weather-beaten terrain. They camped in garage-sized buildings, abandoned from the island's days as a coastguard outpost. There was no running water and, until a decade ago, no electricity. \u201cIt was a brutal environment, and I was out there suffering with them,\u201d says Paine. He kept the atmosphere spartan, splashing out only on heavy-duty clothing and boots \u2014 the \u201cPaine lab uniform\u201d. Unlike many principal investigators, who dole out predefined projects to new recruits, Paine encouraged freedom and individuality. \u201cHe was hands-off until it was necessary to be hands-on,\u201d says Menge. Students walked to disparate corners of the island to explore their own passions. One tagged larval rockfish; another studied barnacle dynamics. In the evenings, the group traded data over campfires. \u201cYou'd have a day of working the shoreline, you'd trudge back up to the island, and he invariably asked: What did you learn today?\u201d says Tim Wootton from the University of Chicago, Illinois, who studied under Paine in the 1980s. Paine set a high intellectual bar, and was gruff and challenging. \u201cHe's very brusque and abrupt, and he kind of scowls a lot,\u201d says Lubchenco. But he also encouraged irreverence and mischief \u2014 he once signed the name of his friend, Peter Kareiva, on Christmas cards full of lascivious limericks and posted them to the world's top ecologists.  All my students were smarter than me but just less knowledgeable.  Paine treated his students as peers, supporting their endeavours without directing them. (Smith had treated Paine the same way.) When they published, he kept his name off their papers unless he had had a heavy hands-on role in the research \u2014 an ethos that seems unthinkable today. \u201cIt hurt him a bit,\u201d says Kareiva, now chief scientist for the Nature Conservancy, an environmental organization based in Arlington, Virginia. \u201cHis presence in the literature would be ten-fold if he hadn't done that.\u201d Harley adds: \u201cIf Bob Paine were reborn into the system as a junior professor right now, he would turn out excellent graduate students but I don't know how many grants he'd get.\u201d Soon, Paine's students were growing up and embarking on careers of their own. Few have spawned as rich a legacy as Jane Lubchenco and Bruce Menge. They met as graduate students in Paine's lab in 1969, married two years later and began a partnership that has generated more than 31 students and 19 postdocs. After the pair left Paine's lab, they took his experimental approach to the US east coast; she focused on plants and herbivores, while he concentrated on predators. By enclosing, excluding and removing species at different points along the New England shore, they showed 3  that fierce waves can keep predators such as starfish at bay, allowing mussels to dominate. But in sheltered areas, predators kept mussels under control, allowing Irish moss ( Chondrus crispus ), a type of red alga, to take over. The work revealed how the environment can control interactions between species. \n               Growing ecosystem \n             After divvying up the New England food web between them, Lubchenco and Menge also split a tenure-track faculty position when they returned west to Oregon State University in 1976. This move, unprecedented at the time, allowed them to further their careers while spending time with their growing family. (One of their children, Duncan Menge, accompanied the Paine clan on field trips and this year is setting up his own ecology lab at Columbia University in New York \u2014 a blood child of the academic family.) The students of the cheerfully described \u201cLubmengo lab\u201d benefited from the yin-yang qualities of their mentors. Menge was impulsive and had an open-door policy. Lubchenco was deliberate and required appointments. Today, he is laid-back and wears Hawaiian shirts; she is intense and sharp-suited. \u201cThe combination of those two was really powerful,\u201d says Steve Gaines from the University of California, Santa Barbara, who was one of their star students. \u201cYou got an educational experience that would be hard for any individual to give you.\u201d Lubchenco and Menge wanted to probe other environmental impacts on coastal ecosystems, and so began a decades-long world tour that took Paine's experimental approaches to Chile, South Africa and New Zealand. Their studies showed 4 , for example, that the intermittent upwelling of nutrient-rich water can intensify competition, predation and other interactions between marine species. But they also moved \u2014 to Paine's disapproval \u2014 from his solitary style to large teams. Their grandest venture is the Partnership for Interdisciplinary Studies of Coastal Oceans (PISCO) \u2014 a 13-lab collaboration that began in 1999. The participants run regimented studies along a 1,900-kilometre stretch of coastline from Alaska to Mexico, including a census of local marine life and measurements of water conditions. There are some Paine-style manipulations, but PISCO is more about collecting data to reveal how the oceans are changing. When Oregon's beaches started filling with dead crabs and fish in 2002, PISCO's data quickly revealed the cause \u2014 an oxygen-deprived dead zone that had been mysteriously growing in the water off the coast (see  Nature   466 , 812\u2013814; 2010 ). \u201cThe problems we're trying to solve in ecology are way beyond the local scale,\u201d says Menge. \u201cMy dream is for PISCO-like entities in all the marine ecosystems around the world.\u201d But that is not Paine's dream: big science is an anathema to him. \u201cHe was always grousing about how this massively expensive work was really not much more than stamp-collecting,\u201d says former student Richard Palmer, now at the University of Alberta in Edmonton. Paine fears that PISCO's approach will yield broad, unclear trends rather than detailed insights. Worse, it risks robbing students of the playful creativity that he tried to instil. \u201cThey're so involved with making the same measurements up and down the same kilometres of coast,\u201d he laments. \u201cMy loosey-goosey attitude to Tatoosh was to get brilliant people to do what they want to do.\u201d But Lubchenco and Menge argue that ecology's challenges, from global warming to ocean acidification, are outpacing the rhythms of solo experiments. \u201cBob fought that a lot,\u201d says Lubchenco. \u201cPeople had to rebel against their parent.\u201d Lubchenco's move into policy was similarly rebellious. She served as president of the American Association for the Advancement of Science in 1997 and of the Ecological Society of America between 1992 and 1993. And as administrator of NOAA from 2009, she has influenced the US government's response to the Deepwater Horizon oil spill, and its strategy to manage the oceans sustainably. \u201cThe culture we grew up with was very anti being relevant,\u201d says Lubchenco, who will leave NOAA at the end of February to return to research. \u201cI knew Bob didn't approve and I did it anyhow. It was really painful to rebel.\u201d Any such disapproval has long since vanished, and Paine now speaks of Lubchenco's influence with deep respect. He has reluctantly become a key player in conservation himself, leading panels of scientists who have assessed the decline of the Steller's sea lion ( Eumetopias jubatus ) and the recovery of Alaskan waters following the 1989  Exxon Valdez  oil spill. \u201cMy role was simply to oversee the mob,\u201d he says. \u201cI was bullied into it.\u201d \n               Next generation \n             Lubchenco's academic children \u2014 Paine's grandchildren \u2014 have been steeped in 'relevance' from the start. \u201cWe were all surrounded by this environment where it was encouraged to think about steps connecting the science to policy,\u201d says Heather Leslie at Brown University in Providence, Rhode Island, a former Lubchenco student. Gaines epitomizes this approach. The ultimate blend of the Paine and Lubmengo heritage, he is an experimental ecologist who uses basic science to influence policy, is not shy of collaborations and has won a national award for mentoring. \u201cSteve sits right at that sweet spot in many different ways,\u201d says Lubchenco. Whereas Paine studies how species interact on individual shores, Gaines looks at connections across entire oceans. He has shown 5  that the larvae of fish and other marine life sometimes drift for hundreds of kilometres on ocean currents, even if the adults are fixed in place. \u201cWhen Bob removed the sea star and got an explosion of mussels, those mussels were coming from somewhere else,\u201d he says. These planktonic drifters connect disparate parts of the ocean, with huge implications for marine reserves, where fishing is forbidden. Rather than reducing fish catches, Gaines showed 6  in 2005 that these reserves have the potential to make nearby fisheries more productive as their larvae disperse and replenish the stocks. Gaines' own students, in the style of the entire family, have branched out into new areas. Kate Smith, who did her PhD with him and is now at Brown, applied Gaines' ideas on dispersing larvae to understand how infectious diseases disperse. Her ecological interests are the same as those of her academic parents and grandparents \u2014 where organisms are, why they are there and how they influence each other \u2014 but applied to microbes and continents, rather than invertebrates and tidal pools. \u201cIt's all related to connectivity,\u201d says Gaines. There are other ways than Paine's to spawn a dynasty. Take, for example, the chain of biological luminaries that emerged in the 1940s, beginning with physiologist James Shannon at the US National Institutes of Health. His descendants \u2014 including Steve Brodie, who did pioneering work in drug metabolism and Julius Axelrod, who was awarded a Nobel prize for his work on the release of neurotransmitters \u2014 helped to revolutionize human pharmacology and neuroscience. \u201cTheir approach was: Don't feel you have to have all your i's dotted and t's crossed. Just go and do it,\u201d says Robert Kanigel, a writer who chronicled the dynasty in his book  Apprentice to Genius  (Macmillan, 1986). But the Shannon lineage was radically different from Paine's. There was no premium on independence; instead, students served as apprentices to their masters. \u201cIn many of the key experiments, the younger person was doing the work of the mentor, and that sometimes led to resentments,\u201d says Kanigel. Such relationships can breed envy and mistrust, he says. \u201cPeople wonder if they'll get the credit they deserve.\u201d Paine proves that the opposite strategy works. \u201cTreat your graduate students as human beings and be accessible to them. That sense of social equality is very important,\u201d he says. \u201cAll my students were smarter than me but just less knowledgeable.\u201d This attitude selected for self-propelled, passionate students who could find their own way. \u201cIt helped in getting the experience to implement your own research programme,\u201d says Wootton. Now aged 79, Paine is still conducting research and inspiring students. He joined a crop of them last summer for a week in Patagonia. \u201cHe'd be in the intertidal on his hands and knees and asking questions,\u201d says Kareiva, who was also on the trip. Paine also makes regular excursions to Tatoosh, where research is now run by Wootton and his wife, Cathy Pfister, who is also at the University of Chicago. Paine's mind is as sharp as ever, but having lost good binocular vision, his steps are less sure. \u201cI hire my very athletic daughter to haul me around the rocks,\u201d he says. \u201cIt's not sufficient, but I can continue to keep track of my long-term experiments.\u201d In 1995, Paine finally allowed starfish to return to the Tatoosh shore after 25 years of continuous exile. \u201cI thought, I'm not going to live forever, so let's see what happens if I let the starfish back.\u201d Their numbers have rebounded, the mussels are losing ground, and the shore is returning to the state it was in before Paine's interference. The same cannot be said for ecology. It will never be the same after Paine. Reprints and Permissions"},
{"file_id": "494024a", "url": "https://www.nature.com/articles/494024a", "year": 2013, "authors": [{"name": "Meredith Wadman"}], "parsed_as_year": "2006_or_before", "body": "As director of the NIH's bold new translational research centre, Christopher Austin has to show that he can jump-start a tortuous drug-discovery process. In his last role two years ago with the Opera Vivente in Baltimore, Maryland, Christopher Austin played the Calvinist chaplain in Gaetano Donizetti's  Lucia di Lammermoor . The story does not lack for drama: the heroine pulls out a knife in her wedding bed and stabs to death the husband who has been forced on her in place of her true love. On the heels of the murder, the chaplain \u201cis the guy who is trying to bring order to chaos\u201d, says Austin, a bass-baritone who once considered a full-time career in opera. Austin's most recent stage part has a certain resonance with his new day job. In September, he was appointed as director of the fledgling National Center for Advancing Translational Sciences (NCATS) at the US National Institutes of Health (NIH) in Bethesda, Maryland. In existence since December 2011, the centre has an ambitious \u2014 some say audacious \u2014 agenda that channels the central passion of both Austin and his boss, NIH director Francis Collins: to get more successful medicines into more patients, more quickly. That means forcing the agonizingly slow, failure-prone process of 'translational research' \u2014 the term of art for moving promising discoveries from the lab to the clinic \u2014 into a higher gear. Passion runs high among the sceptics, too. Researchers both inside and outside the agency fear that NCATS \u2014 the first new centre at the NIH in more than a decade, funded at US$575 million last year \u2014 will encroach on a finite pot of money that they say would be better spent probing the mechanisms of basic biology and disease. Others question the scale of its mission. \u201cWith the available resources, how are you going to achieve this?\u201d asks Thomas Caskey, a molecular geneticist at Baylor College of Medicine in Houston, Texas. \u201cTo me, you cannot just take this money and be another biotechnology company and you certainly don't have enough money to be a pharmaceutical company.\u201d NCATS will be neither, Austin responds. What will set it apart, he says, is a focus on overcoming obstacles on the road to drug development, from inadequate toxicology methods to inefficient clinical-trial recruitment, rather than actually producing the drugs. In an era in which more than 95% of drug candidates fail, and a novel drug takes 13 years and more than $1 billion to develop, \u201cNCATS has to be focused on logarithmic improvements in the process\u201d, says Austin. \u201cYou can't do this in a brute-force way. You have to do it differently. You have to drive the technology development.\u201d Austin's fans say that if anyone has a shot at making this work, it is him. \u201cThis guy has got clinical training, industry training and scientific training. If you wanted me to pick a quarterback, this is the quarterback I'd pick,\u201d says Lee Nadler, director of Harvard Catalyst, the NCATS-funded clinical and translational science centre based at Harvard University in Boston, Massachusetts. But whether quarterback or maestro, Austin has now to give the performance of his career. The biggest risk he faces lies in \u201cnot delivering something concrete within 12\u201324 months\u201d, says Nadler. \u201cEverybody is watching him.\u201d \n               Losing a life \n             Austin learned early, and at first-hand, about the tragic shortcomings of medicine. One night in 1989, when he was a neurology resident on call at Massachusetts General Hospital in Boston, an ambulance brought in a middle-aged man with end-stage amyotrophic lateral sclerosis (ALS), a disease that slowly destroys muscle power but leaves brain function intact. Patients usually die when their breathing muscles give out. The man had a 'do not resuscitate' order, but, because of a miscommunication, he had been revived by the paramedics. Furious that he had not been allowed to die at home, he demanded that his ventilator be turned off. Austin complied. Watched by his family and Austin, the man died slowly over three hours, in the end turning blue before his heart monitor flatlined. \u201cIt was like sitting through the crucifixion,\u201d Austin recalls. \u201cAnd I just said: 'I can't do this. There has got to be a better way.'\u201d Convinced that he had to do more, Austin began a postdoc in the lab of Connie Cepko, a geneticist at Harvard Medical School in Boston. There, he dived into developmental neurology, using new tracing techniques to reveal the migration of neural progenitor cells in the budding mouse cortex ( C. P. Austin and C. L. Cepko  Development   110,  713\u2013732; 1990 ). \u201cHe was just really driven. He absolutely loves research,\u201d says Cepko. She recalls the day that Austin's wife went into labour with the couple's first child at the Brigham and Women's Hospital, around the corner. \u201cI went to the lab and there was Chris sitting as his bench, pipetting away. I said, 'Chris, aren't you supposed to be in the delivery room?' He said: 'It'll be a couple hours [yet]'.\u201d Despite all the time he logged in the lab, Austin did not stop seeing patients; at one point, he did a stint as the lone doctor in a hospital in rural Swaziland. But the distance from the elegant experiments of Cepko's lab to the clinic increasingly bothered him. \u201cThat gulf was so wide,\u201d says Austin. In 1995, when Edward Scolnick, research chief for the pharmaceutical company Merck, visited Harvard and announced that his firm was launching a genetics-based research operation that would redefine how it developed therapeutics, Austin immediately applied.  He's a guy with a sense of humour, which God knows to do that job you need.  He spent the next seven years at Merck Research Laboratories in West Point, Pennsylvania, using the sequence that was beginning to come out of the Human Genome Project to seek targets for treating schizophrenia, bipolar disorder and Alzheimer's disease. Austin's know-how in identifying drug targets was \u201crevered\u201d says Caskey, who was his boss at Merck, although he \u201cnever really did\u201d the downstream drug development during which so many potential drugs founder. And Austin also learned up close the disappointments of drug development: the Merck compounds that arose from his considerable work on the Alzheimer's target \u03b3-secretase were dropped by the company several years ago because of their side effects. Eventually, Austin grew frustrated with the constraints of working for a huge drug company, where the need for profit made chasing cures for rare diseases such as ALS a non-starter. He had already crossed paths with Collins, then the director of the NIH's National Human Genome Research Institute. In August 2002, after Austin gave a talk on the NIH campus, Collins asked him to \u201c'come down here and help us figure out what to do with the genome'. Those were his exact words,\u201d Austin recalls. By November, Austin was in place as Collins's senior adviser for translational research. \n               Thinking big \n             Austin's most prominent early project was the launch of the Molecular Libraries Program (MLP), a multi-centre effort to identify small molecules that academics could use to probe potential drug targets, and that sometimes formed the basis for drugs themselves. Many NIH officials envisioned a fairly modest effort for screening and tweaking molecules on the NIH campus. But Austin, with his commercial experience, thought on a different scale. He shopped high and low for the latest in high-throughput robotics systems and landed a deal with Kalypsys, a biotechnology firm based in San Diego, California. The company built him a one-of-a-kind, fully automated system that could, as an NIH YouTube video claimed, \u201cboldly go where no robot had gone before\u201d. In the space of five days, it could screen seven concentrations each of 400,000 compounds to test their activity against genes, proteins or cellular pathways implicated in a panoply of diseases. Before long, the robot was hosting a steady string of industry visitors who wanted to understand Austin's technological leap. In 2009, Austin launched a programme in which the NIH partners with companies, non-profit organizations and academics to try to move into clinical testing compounds that show promise against neglected diseases \u2014 including some from the MLP. The Therapeutics for Rare and Neglected Diseases programme, which is now part of NCATS, has already proved its worth, says Austin, with four compounds moving into clinical trials in the past 15 months. The trials include one launched last month, which deploys cyclodextrin against a rare, fatal disorder of cholesterol metabolism, Niemann\u2013Pick disease type C. Austin's supporters say that he is no grey bureaucrat buried in the bowels of the NIH. He has proved, says Collins, \u201cexceptionally effective\u201d in building collaborations, whether with academics, industry veterans or earnest disease advocates. \u201cHe's a guy with a sense of humour, which God knows to do that job you need,\u201d adds Garret FitzGerald, director of the NCATS-supported Institute for Translational Medicine and Therapeutics at the University of Pennsylvania in Philadelphia. Last month, while speaking at the J.P. Morgan Healthcare Conference in San Francisco, California, Austin was asked whether it is true that he is an opera singer and whether, if so, he could sing a C for the audience. \u201cYes, on both counts!\u201d he sang loudly into the Colonial Room at the Westin St. Francis Hotel. \n               Rough start \n             In December 2010, Collins, early in his second year as NIH director, announced his intention to form a translational-medicine centre from existing components of the NIH, and to do so within a year \u2014 a veritable burst of speed in the government world. The reorganization would mean the dissolution of the NIH's National Center for Research Resources, an entrenched institute with heavy investment in translational science and many constituents in basic research (see   Nature   471 , 15\u201316; 2011). The next month,  The New York Times  ran a front-page story declaring that NIH officials \u201chave decided to start a billion-dollar government drug development center to help create medicines\u201d and that to do so Collins was willing to \u201ccannibalize\u201d other parts of the NIH. The reaction was fierce. Congressional Republicans, drug-industry executives and NIH-funded basic researchers blasted the agency for treading on private-sector prerogatives, for neglecting its basic-research mandate and for presuming that it could succeed where industry had been failing. Perhaps the most damaging jab came from Roy Vagelos, former chief executive of Merck, at a congressional hearing last March. \u201cDoes anyone in the audience believe that there is something that NCATS is going to do that the industry thinks is critical and that they are not doing?\u201d he asked. \u201cThat is incredible to think that. If you believe that, you believe in fairies.\u201d Collins rushed to defend the nascent centre's mission. NCATS, he explained, would \u201ccomplement and not compete with\u201d industry, by taking on thorny problems in the technology of drug development to smooth the road to the clinic for all concerned. And, he assured his constituents, NCATS would not eat into the NIH's basic-science dollars. Congress, at least, set aside its doubts, and funded NCATS in the dying days of 2011. The search for a director took nine months, and at least one other candidate was offered the job. But Austin, who had created many of the programmes that comprised NCATS, was a natural fit to head the centre. Besides, says Nadler, \u201cFrancis loves this guy\u201d. During a recent interview, Austin spontaneously countered the now-famous \u201cfairies\u201d criticism. On the NCATS to-do list, \u201care there things pharma hasn't thought of doing? Maybe, but for the most part, no,\u201d he says. However, he adds: \u201cThat's not the right question. The right question is: what can they do within the confines of \u2026 a profit-making organization? There's a lot of things you just can't do even if you want to.\u201d To underscore what sets NCATS apart from industry, Austin has been showcasing the centre's first new programme, which makes available to NIH-funded scientists 58 drugs tested in humans but abandoned by big drug firms for business reasons or because they didn't work against the conditions that the companies had tested them on. The goal is to put those candidates to other uses. The programme has become a useful flagship for NCATS not least because it \u2014 unlike others in the opaque field of translational research \u2014 is easily explained to the public. Austin also likes to talk about NCATS's bid to overcome one of the biggest hurdles in the quest for new drugs: the discovery of harmful side effects when a compound is well into development. \u201cThis is a classic problem for NCATS to work on,\u201d he says. His solution, in part, is a programme in which NCATS is working with the Defense Advanced Research Projects Agency and the US Food and Drug Administration to put ten human tissues, from heart to brain and gut, on a chip that could then be used to screen potential drugs rapidly and efficiently for toxic effects. Another attack on toxicity can be found in the Tox21 programme, a collaboration between NCATS, the National Toxicology Program at the NIH's National Institute of Environmental Health Sciences and the Environmental Protection Agency. It began in December 2011 to screen 10,000 environmental chemicals and approved drugs against every known human signalling pathway, to identify which molecules might have toxic effects. In the longer term, one of Austin's major goals is to find a better way to use NCATS's biggest programme: the $461-million Clinical and Translational Science Awards (CTSAs), which fund around 60 translational-medicine centres, each operating independently. The awards aim to train the next generation of translational researchers and to improve the full spectrum of that research, from discovering drug targets to answering health-delivery questions such as: do asymptomatic women really need routine manual pelvic exams? Since September, Austin has visited nine of the CTSA centres \u2014 he has planned six more visits \u2014 to talk to principal investigators and other staff members. While praising the centres' work, he says that they have so far mostly operated \u201cwithout particular encouragement or direction from the NIH, and thus in a disjointed and uncoordinated fashion\u201d. Austin wants to see a \u201cCTSA 2.0\u201d that will apply the consortium collectively to problems including clinical-trial recruitment, the development of better biomarkers and the rational use of electronic medical records in research. This can best be done \u201cacross a nationwide network focused on solving systematic problems\u201d, says Austin. Austin has not sung in an opera since the launch of NCATS. It is a fact of his current life that he regrets. \u201cWhen I got this job,\u201d he says, \u201cI got more than one congratulatory note saying: 'Now you really need to do the music. Because this is the only way that you're gonna maintain your sanity.'\u201d Working out how to make that happen may be easier said than done; Opera Vivente folded during the economic meltdown, and Austin is logging 12\u201315-hour workdays. But he still sees the pursuit as intimately related to his chosen career. \u201cIf you look at what NCATS is trying to do, and why I got into medicine in the first place \u2014 you are trying to understand the human condition. Fundamentally, that's what opera does, that's what it tries to explore. What makes people tick? And then, sometimes, how do you fix it?\u201d \n                     New cures sought from old drugs 2012-Oct-03 \n                   \n                     National prescription for drug development 2012-Apr-10 \n                   \n                     US translational-science centre gets under way 2012-Jan-10 \n                   \n                     Embrace change 2011-Mar-09 \n                   \n                     NIH revamp rushes ahead 2011-Mar-01 \n                   \n                     Blogpost: NIH insider will head translational medicine centre \n                   \n                     Blogpost: NIH director grilled over translational research centre \n                   \n                     National Center for Advancing Translational Sciences \n                   Reprints and Permissions"},
{"file_id": "494420a", "url": "https://www.nature.com/articles/494420a", "year": 2013, "authors": [{"name": "Eugenie Samuel Reich"}], "parsed_as_year": "2006_or_before", "body": "With the ear of politicians and the respect of researchers, Norman Augustine is the most influential non-scientist in US science. Of the many requests for advice that stream in from the scientific community, Norman Ralph Augustine says he responds to those that meet one of three criteria: \u201cThey're a cause you believe in; they're for a friend; or they'll be fun.\u201d Augustine, 77, says that he got the line from a friend who was advising him on what to do on his retirement as chairman of the aerospace firm Lockheed Martin, back in 1998. He never expected that following the advice would lead him to a new career, as an unpaid adviser to the scientific community. Yet the role meets all three criteria. Augustine enjoys science. He has a lot of friends in science. And it is unquestionably a cause that he believes in. Science as a cause may sound like a contradiction in terms, and in many ways it is. But the contradiction lies at the heart of the Augustine enigma. The man who has advised NASA on its space programme, three US presidents on science and technology policy and the US Congress on the urgent need for more science and education funding (see 'Linked in') has largely avoided the critical attention that the scientific community normally turns on its movers and shakers. It helps that Augustine is not just a tireless advocate for science, but also disarmingly and naturally folksy. In 2010, while testifying to Congress about a study he had led five years earlier on the importance of science and engineering, Augustine suggested off the cuff that cutting science funding to help the economy was like saving an overloaded aeroplane by removing an engine. US President Barack Obama's team liked the analogy so much that the president used the same line to raise a laugh from Congress when he made his own pitch for increasing science funding in his 2011 State of the Union address. Today, that 2005 study, known as  Rising Above the Gathering Storm  ( RAGS ), remains one of the most influential science-policy reports in a generation, and it continues to help guide US science spending on science education and research. It also propelled Augustine into an ever-expanding role at the intersection of science and government. Among his many current duties, he leads a panel that reviews science management at the National Institutes of Health (NIH), and one that will appoint a new director for the Fermi National Accelerator Laboratory (Fermilab) in Batavia, Illinois. From health to energy, Augustine \u201cis the go-to person for a fair judgement on a technical matter\u201d, says Neal Lane, a physicist at Rice University in Houston, Texas, and former head of the US National Science Foundation in Arlington, Virginia. Augustine has his critics. Some say that he sometimes advances policies \u2014 for example, easing visa limits on foreign postdocs \u2014 that benefit US universities and businesses but harm individual American scientists. \u201cHe represents a particular set of views: of the chief executive and scientific establishment,\u201d says Ron Hira, a labour economist at Rochester Institute of Technology in New York. But with the success of  RAGS  and other Augustine studies, it is not hard to understand why leaders in the US scientific community are increasingly turning to him to head their panels, says David Goldston, a former staff member on the congressional committee that implemented some of the recommendations of  RAGS  and a policy expert at the National Resources Defense Council in Washington DC. \u201cIt's like a popular television show. You want to do the sequel.\u201d \n               Rising above \n             Although he has earned the respect of scientific leaders, Augustine has never worked in science, does not have a PhD and has limited personal experience doing research. After getting a master's degree in aeronautical engineering at Princeton University in New Jersey in 1959, he briefly studied gas flows at Douglas Aircraft Company in Long Beach, California, before moving into management. In the 1960s and 1970s, Augustine bounced back and forth between jobs in the defence industry and increasingly senior positions in the US Department of Defense, reaching undersecretary of the Army in 1975 under then-president Gerald Ford. In 1987, he became chief executive of aerospace company Martin Marietta, and, in that position, helped to broker what was at the time the largest merger of defence aerospace companies in US history, to form Lockheed Martin in Bethesda, Maryland. From his first research experience, Augustine says he had been aware of science as the source of US industry's power to innovate, but it wasn't until he became chief executive of Lockheed Martin that he took that conviction into the political sphere. In 1996, Augustine gathered the leaders of 20 other corporations, including Hewlett Packard, Ford Motor Company and Motorola, to co-author a letter to then-president Bill Clinton about the importance of government spending on basic research. They sent the letter to  The New York Times , Augustine says, but the newspaper rejected it. When other papers did likewise, Augustine bought a full-page advertisement in several papers for his letter. He also began to spend time on Capitol Hill promoting basic research. Through his stints on government advisory panels, Augustine became friendly with Burton Richter, a physics Nobel laureate at Stanford University in California. They paired up, and found that the idea of a joint briefing on the future of the US economy from a former executive and a Nobel laureate proved irresistible even to very busy congressional staffers. \u201cNorm was more of the star,\u201d says Richter. \u201cI was more of the person who could provide specifics \u2014 like Batman and Robin.\u201d In the beginning, Augustine says, many of their victories came in forestalling threatened cuts to science funding. But the pair began to have success asking for more money in 2005, even though President George W. Bush and the Republicans in Congress were pledging to rein in spending. Augustine's extensive business background may have helped him to win over some of the Republican lawmakers. That year, he, Richter and others garnered bipartisan support for increasing science funding from the two Democratic and two Republican leaders on the Senate and House science committees. The leaders asked the US National Academies to produce a report on the top ten things that Congress could do to strengthen science and engineering with a view to improving national competitiveness. Sherwood Boehlert, Republican chairman of the House Committee on Science at the time, says that he was delighted when Augustine was picked to chair the National Academy of Sciences panel that put together the  RAGS  report. \u201cI couldn't have been happier, it was like something from heaven,\u201d he says. \u201cHe's a national treasure.\u201d Of the 20 recommended actions in the report, the most notable were calls to recruit 10,000 science and mathematics schoolteachers and to increase federal support for basic research, especially in the physical sciences, by 10% per year for 7 years. Both recommendations were welcomed by senior scientists, who saw them as obvious and long overdue. But some workforce experts found plenty to dislike in the report, including calls to increase the number of visas for scientific workers. Labour economists have found that temporary visas can lead to the offshoring of jobs because businesses can train foreigners in the United States, then keep them on the payroll when they return home. Hira, who was a reviewer of the report, says that these concerns were given short shrift on the panel, which included eminent scientists and university and corporate leaders, but no representatives of US workers, junior working scientists or economists studying their situations. Hira and others say that those are striking omissions for a panel tasked with discussing the technical workforce. The first version of the report also ended up including at least one major exaggeration: that China graduated nearly ten times more engineers than the United States (600,000 versus 70,000) \u2014 a comparison used to argue for increasing the number of scientists and engineers in the United States. But the Chinese data probably included two-year technical degrees whereas the US figure did not. The error \u201ccontributed to the alarm quality of the report\u201d, says Michael Teitelbaum, an economist at the Alfred P. Sloan Foundation in New York, who says that he questioned the numbers in his role as a reviewer. \u201cI don't know of any serious analyst with an open mind who has concluded there are shortages in the science and technology workforce,\u201d he says. In fact, many US scientists and engineers were struggling to find high-quality jobs in academia and industry, a trend that continues today. The  RAGS  committee had been given an incredibly tight deadline of just 13 weeks. National Academies spokesman Bill Kearney says that the panel operated with much the same rigour as other academy committees. But participants recall some differences from standard procedures. Charles Vest, who served on the  RAGS  panel and is now president of the National Academy of Engineering, says that some of the most compelling facts in the report about the United States being out-competed by other countries came not from careful reviews of economic and science-policy research literature, but from newspaper clippings that Augustine had collected over the years. Not all of those items were fact-checked by academy staff. The final version of  RAGS , released in 2007, corrected the Chinese figures \u2014 but by that point the report had already made its mark. The conclusions were roundly welcomed by academic leaders, whose institutions would benefit from the increased funding and influx of foreign students and postdocs. And it had a much wider impact than that. In his 2006 State of the Union address, Bush announced what he called the \u201cAmerican Competitiveness Initiative\u201d, which took up many of the  RAGS  recommendations as White House policy. A year later, Congress passed legislation called America Creating Opportunities to Meaningfully Promote Excellence in Technology, Education, and Science (COMPETES), which set a goal of increasing research investment in science funding agencies by 10% per year, just as  RAGS  had recommended. Mike Quear, who helped to draft the America COMPETES legislation as a Democratic congressional staff member, says that the rationale of  RAGS  was understood by Congress and the White House precisely because it had been communicated by Augustine in clear, common-sense terms, without using ideas from economics that could be disputed and without pandering to constituencies such as postdoc unions. \u201cThat was Norm's effectiveness,\u201d Quear says. \u201cHe was like a big stone rolling down a hill. No one wanted to get in his way.\u201d When Augustine came to testify on the report, his unscripted remarks were a hit on Capitol Hill. Augustine also cemented his testimony with political friendships. Boehlert says that when Augustine realized that the congressman was a self-described \u201cbaseball nut\u201d, he began to quiz him about sports trivia. Augustine didn't quit after the legislative victory. When Congress failed to provide funding for elements of the COMPETES act or implement some of his recommendations on school education, Augustine prepared another report, in 2010, just as America COMPETES came up for reauthorization, that called attention to the shortcomings. Those reports continue to carry weight on Capitol Hill, where science agencies have generally fared better in terms of funding than other agencies. The success of  RAGS  has brought Augustine invitations to join a wide variety of science advisory boards. Lane says that community leaders who want to get something done turn to Augustine because they know that any report he writes will be read by the right people in Congress. \u201cThe chair has to be politically tuned-in,\u201d he says \u2014 and Augustine is. Yet Augustine has also been accused of being used by establishment figures to provide external validation for their plans. For example, he leads the NIH Scientific Management Review Board, which was asked in 2010 to assess plans for a new translational research centre. Francis Collins, the NIH chief, was pushing hard for the centre \u2014 but many researchers were worried about Collins's proposal to dismantle the National Center for Research Resources (NCRR), which oversees important elements of US research infrastructure, to make way for his brainchild. Jeremy Berg, director of the National Institute of General Medical Sciences and a member of the review board at the time, e-mailed Augustine, warning that the reputation of the board would be damaged if it didn't assert its independence from Collins and take a hard look at the implications of abolishing the NCRR. Three days later, however, the board voted through Collins's plan with little examination of what might happen to the NCRR. \u201cI never heard from Norm,\u201d says Berg, the only person on the board who opposed the creation of the translational medicine institute. Augustine says that the board heard from a number of stakeholders before reaching its decision. But the unease that some researchers felt about the swiftness of that reorganization points to another concern about Augustine's advisory work. His background leads him to view scientific organizations from a business model, a perspective that can conflict with the way that science actually works. Research is often economically unproductive, for example, and the aims of university and government leaders do not always align with the needs of working scientists. Some organizations seeking Augustine's advice have therefore brought in other experts to complement his outlook. Last year, Augustine and Teitelbaum co-chaired a National Academy of Sciences panel on the defence science workforce to produce a more nuanced report than the discussion in  RAGS . Yet nothing has swayed Augustine from the same basic conclusion he reached long ago: \u201cIf America wants to compete, it needs to double the research budget \u2014 in which case more scientists will be needed.\u201d To those who say that he simply supports the scientific establishment, Augustine counters that he is independent enough to say what he thinks. And he has demonstrated that before. In 2009, a panel that Augustine chaired for NASA concluded that the agency could not complete the Constellation human-spaceflight programme, which upset NASA leaders and possibly his former colleagues at Lockheed Martin, a major space contractor. More recently, public accountability watchdogs complained that Augustine should not have been chosen to review a controversial fracking study at the University of Texas at Austin because he had been on the board of the oil and gas company ConocoPhillips in Houston, Texas. Yet in December, Augustine's panel issued a bruising critique of the study. Indeed, it is difficult to predict from Augustine's private interests how he will vote on any given topic. But friends say that there is one thing they can predict: Augustine's recommendations will be motivated by a strong sense of patriotism. His love of science stems from his devotion to country, from his sense of science's place in the nation and in the global economy, which is precisely why his words have traction with politicians. \u201cIt's incorrect to refer to Norm as an advocate for science,\u201d says Vest. \u201cHe's an advocate for what he thinks the United States needs.\u201d \n                     'Gathering Storm' back on the radar 2010-Sep-23 \n                   \n                     Climate politics: Beyond Bush 2007-Nov-14 \n                   \n                     Advice from the in-crowd 1990-Aug-09 \n                   \n                     Norman Augustine AAAS Award \n                   \n                     Rising Above the Gathering Storm \n                   \n                     America COMPETES Act \n                   \n                     Obama 2006 State of the Union address \n                   Reprints and Permissions"},
{"file_id": "494416a", "url": "https://www.nature.com/articles/494416a", "year": 2013, "authors": [{"name": "Monya Baker"}], "parsed_as_year": "2006_or_before", "body": "Where once there was the genome, now there are thousands of \u2019omes.  Nature  goes in search of the ones that matter. \u2019Omics bashing is in fashion. In the past year,  The New York Times  and  The Wall Street Journal  have run pieces poking fun at the proliferation of scientific words ending in -ome, which now number in the thousands. One scientist has created a bad\u00adomics generator, which randomly adds the suffix to a list of biological terms and generates eerily plausible titles for scientific papers (example: \u2018Sequencing the bacteriostaticome reveals insights into evolution and the environment\u2019). Jonathan Eisen, a microbiologist at the University of California, Davis, regularly announces awards for unnecessary additions to the scientific vocabulary on his blog (recent winner: CircadiOmics, for genes involved in daily circadian rhythms). Botanist Hans Winkler had no idea what he was starting back in 1920, when he proposed the term \u2018genome\u2019 to refer to a set of chromosomes. Other \u2019omes existed even then, such as biome (collection of living things) and rhizome (system of roots), many of them based on the Greek suffix \u2018-ome\u2019 \u2014 meaning, roughly, \u2018having the nature of\u2019. But it was the glamorization of \u2018genome\u2019 by megabuck initiatives such as the Human Genome Project that really set the trend in motion, says Alexa McCray, a linguist and medical informatician at Harvard Medical School in Boston, Massachusetts. \u201cBy virtue of that suffix, you are saying that you are part of a brand new exciting science.\u201d Researchers also recognize the marketing potential of an inspirational syllable, says Eisen. \u201cPeople are saying that it\u2019s its own field and that it deserves its own funding agency,\u201d he says. But although some \u2019omes raise an eyebrow \u2014 museomics (sequencing projects on archived samples) and the tongue-in-cheek ciliomics (study of the wriggling hairlike projections on some cells) \u2014 scientists insist that at least some \u2019omes serve a good purpose. \u201cMost of them will not make sense and some will make sense, so a balance should be in place,\u201d says Eugene Kolker, chief data officer at Seattle Children\u2019s Hospital in Washington, and founding editor of the journal  Omics . \u201cIf we just laugh about different new terms, that\u2019s not good.\u201d Ideally, branding an area as an \u2019ome helps to encourage big ideas, define research questions and inspire analytical approaches to tackle them (see \u2018Hot or not\u2019). \u201cI think -ome is a very important suffix. It\u2019s the clarion call of genomics,\u201d says Mark Gerstein, a computational biologist at Yale University in New Haven, Connecticut. \u201cIt\u2019s the idea of everything, it\u2019s the thing we find inspiring.\u201d Here,  Nature  takes a look at five up-and-coming \u2019omes that represent new vistas in science. \n               boxed-text \n             How many \u2019omes can you name? Kerri Smith challenges Monya Baker to an \u2019ome-off. A study last year 2  revealed the extent of the dilemma. It polled 16 genetic specialists about mutations implicated in 99 common genetic conditions that might show up in large-scale sequencing, whether or not a doctor was looking for them. For some 21\u00a0conditions or genes, including well-known sequence variants associated with certain cancers and a heart irregularity, all 16 specialists recommended informing adult patients. But only ten would do the same for Huntington\u2019s disease \u2014 an untreatable, fatal condition \u2014 and there was relatively little consensus on more obscure mutations, or what to tell parents when the variant showed up in a child\u2019s sequence. The biggest problem with the incidentalome is that no one knows what most sequence variants \u2014 and there are more than 3 million in every human genome \u2014 mean for health. Wendy Chung, a clinical geneticist at Columbia University in New York, is developing ways to help research participants and patients to choose which genetic results they want to learn. She is also measuring the behavioural and psychosocial impacts of the information. \u201cIf you ask people what they want to know about their DNA sequences, everyone initially either says everything, or nothing,\u201d says Chung. \u201cWhen people are thoughtful, there are shades of grey.\u201d As clinical sequencing gains popularity, the definition and scale of the incidentalome is blurring. Geneticists should expect these hard-to-handle results, says Holly Tabor, a bioethicist at Seattle Children\u2019s Hospital. \u201cIt\u2019s somewhat misleading to say that there are incidental results from a genome study. You know that they will be there.\u201d \n               Phenome \n             Human genomes are now easy to come by. What\u2019s missing are phenomes: thorough, exact descriptions of a person\u2019s every physical and behavioural characteristic. Researchers most want to know about the portion of the human phenome related to disease: facial abnormalities, limb deformities, whether and how people were diagnosed with depression. And they want those descriptions in a form that computers can read \u2014 the better to see how such phenotypic traits might relate to genomes. \u201cI do not know of another word or phrase with which we can say this better,\u201d says Peter Robinson, a computational biologist at the Charity University Hospital in Berlin, who is working to standardize such physical descriptions. Phenome projects are already under way for mice, rats, yeast, zebrafish and the plant  Arabidopsis thaliana . In the most systematic efforts, scientists knock out genes one by one, then carefully put organisms through a battery of measurements and physical tests to find out how genes shape physical form, metabolism and behaviour. Such comprehensive data cannot be had for human genes, but some clinical researchers hope to pull together a partial resource by carefully collecting patient data. Even for \u2018Mendelian\u2019 diseases, known to be caused by a single mutated gene, matching up disease and gene is challenging. Of more than 6,000\u00a0rare, heritable disorders, fewer than half have been pinned to a genetic cause. One of the hardest parts is finding enough patients with such conditions, which may occur in fewer than one person in one million. \u201cWe could probably solve the majority of Mendelian disorders with an unknown cause if we had access to enough well-phenotyped cases,\u201d says Michael Bamshad, a geneticist at the University of Washington in Seattle. But how to compile those cases? Many research and disease communities already have their own long-standing informatics tools and vocabularies to describe fine phenotypic details of various disorders. The challenge lies in getting these resources to work together. If one clinician enters \u2018stomach ache\u2019 and another \u2018gastroenteritis\u2019, patients with very similar symptoms may not get grouped together, explains Richard Cotton, a geneticist at the University of Melbourne in Australia. In November last year, Cotton was among the scores of interested parties who came together in San Francisco, California, for a meeting called \u2018Getting ready for the Human Phenome Project\u2019. The major aim of the meeting was to make the exchange of phenotypic data easier. A consortium that focuses on rare diseases, called Orphanet, is leading efforts to get clinicians and scientists to agree on 1,000\u20132,000 standard terms \u2014 such as \u2018short stature\u2019, which may also be categorized as \u2018decreased body height\u2019, \u2018height less than 3rd percentile\u2019 and \u2018small stature\u2019. \u201cIf you agree on the terms, no matter what form you have, we can all be talking about apples and apples and apples,\u201d says Ada Hamosh, a clinical geneticist at Johns Hopkins University School of Medicine in Baltimore, Maryland. Other researchers are trying to unlock the often idio\u00adsyncratic information in electronic medical records so that computer algorithms can comb them and classify common phenotypes automatically. \u201cThe data are ugly and sparse, and the magic \u2014 the science \u2014 is turning that dross into gold,\u201d says Kohane. \n               Interactome \n             Biology\u2019s central dogma is essentially a parts list. DNA codes for RNA, which codes for protein. That may give you three basic \u2019omes (genome, transcriptome and proteome), but life happens only because these parts work together. A neuron fires and a cell divides or dies because molecules interact. The interactome describes all of those molecular interactions. And in terms of complexity, it is a king of the \u2019omes. Just considering one-on-one interactions for 20,000 or so proteins generates 200 million possibilities. That scope is not daunting to researchers such as Marc Vidal. Before he retires, the 50-year-old systems biologist at the Dana-Farber Cancer Institute in Boston hopes to see a first, rough draft of all the interactions that the genome encodes. Actually, he would be happy with a subset, a catalogue of all the proteins that come together in pairs. \u201cThat\u2019s what we\u2019ve been doing for the past 20\u00a0years, and we\u2019re almost there now,\u201d he says. By \u2018almost there\u2019 Vidal means that his and a few other labs have observed 10\u201315% of human protein\u2013protein interactions, based on studies of cells genetically engineered to generate a signal when a pair of proteins comes together. Other researchers have been pursuing the same goal by plucking proteins from crushed cells and seeing which others come along for the ride, scouring the literature and making computational predictions based on protein shapes and the behaviour of related molecules. It has helped that, more than a decade after the first large-scale interactome study 3 , researchers are finally starting to get a handle on which observed interactions are real and which are artefacts. Making that distinction requires hunting for the same inter\u00adaction using multiple techniques 4 . But lists do not need to be complete to be useful \u2014 and biologists are already beginning to consult the interactome. Haiyuan Yu, a systems biologist at Cornell University in Ithaca, New York, tested about 18 millionpotential protein pairs and combed established databases for interactions, eventually identifying 20,614\u00a0interactions between 7,401\u00a0human proteins. For around one-fifth of these interactions, the team also got a good sense of what parts of these proteins made contact 5 . Yu and his colleagues showed that disease-causing mutations are more likely to be at these points of contact than elsewhere in the proteins. For example, the blood disorder Wiskott\u2013Aldrich syndrome is caused by mutations in a protein called WASP \u2014 but only by mutations located in an area that interacts with a second protein called VASP. Patterns that make no sense in terms of genes, says Yu, can become clear when considered in terms of interactions. Vidal believes that increasingly sophisticated information can be layered into the interactome. First will come fleshed-out basic networks: lists of proteins and their binding partners, ideally annotated by cell types. Next will come descriptive data, such as how long interactions last, the conditions necessary for them and the parts of proteins that make contact. Vidal imagines a day when clinicians diagnosing a patient will consider not only their genome, but the consequences of all their sequence variants on the interactome \u2014 not to mention the influences of the interactome on the phenome. Genomes, after all, are generally static, says Trey Ideker, a systems biologist at the University of California, San Diego. \u201cThe sequence is not perturbed by drugs, tissues or other conditions. Interactomes are.\u201d \n               Toxome \n             Thomas Hartung wants to learn all the ways a small molecule can hurt you. To do so, he has organized the Human Toxome Project, funded with US$6 million over five years from the US National Institutes of Health, plus extra support from the Environmental Protection Agency and the Food and Drug Administration. The -ome suffix, Hartung says, suited the scale of his goal: a description of the entire set of cellular processes responsible for toxicity. \u201cThe toxome is very similar to the Human Genome Project because it establishes a point of reference,\u201d says Hartung, a toxicologist at Johns Hopkins Bloomberg School of Public Health in Baltimore. Toxicity testing in animal studies costs millions of dollars for every compound that enters human trials, yet animal tests sometimes fail to predict toxicity in humans. More than one in six drugs are pulled for safety problems that are discovered during human trials. Hartung says that the toxome could help to lay out a series of straightforward cell-based assays that could replace animal tests \u2014 and perhaps improve on them. Knowing which toxicity-related processes a compound triggers could also help scientists to tweak promising new drugs or industrial molecules into less-harmful versions. To start with, Hartung wants to expose cells to toxic chemicalsand then monitor their metabolomes (the set of all small molecules in the cell) and their transcriptomes. He hopes to piece together the details of pathways in human cells that disrupt hormone signals, poison liver cells, break the heart\u2019s rhythm or otherwise endanger people\u2019s health. The total number of pathways, Hartung believes, will be perhaps a couple of hundred \u2014 a manageable amount for testing toxicities. The project is still in its early days \u2014 making sure that the same assay yields the same results in different labs. Eventually, however, those pathways could be used in cell-based assays to serve as bellwethers of toxicity. \u201cWe\u2019d know if we triggered one of those pathways that something bad would happen, and we\u2019d know what that adverse event would be,\u201d says David Jacobson-Kram, who evaluates ways to predict toxicity at the Food and Drug Administration in Silver Spring, Maryland. He warns that a molecule that seemed harmless to cells in culture might behave differently in the body \u2014 for example if the liver converted it to a toxin. Nonetheless, he says, the toxome project could save time, money and animals. \u201cDo I think this paradigm has promise?\u201d he asks. \u201cAbsolutely.\u201d \n               Integrome \n             The key to unravelling biology\u2019s greatest mysteries depends less on inventing new \u2019omes, says Kolker, than on combining those that are already there. \u201cOne approach won\u2019t solve it,\u201d he says. Enter the integrome: information from all the \u2019omes thrown into one pot for an integrated analysis, along with any other relevant data for good measure. \u201cThat\u2019s the real deal, it\u2019s going to be more and more important,\u201d says Kolker. Consider Google Maps. Separate lists of petrol stations, restaurants and street names are far less useful than one map showing that a particular petrol station is on the same street as a particular restaurant. But many conventional \u2019omics studies stop at list-making \u2014 genes, proteins or RNA transcripts. These can ignore networks and so may not reveal, for example, that changes in disparate genes actually converge on the same pathway. Ideker has shown that it is possible to analyse disparate \u2019omics data automatically 6 . He created software that interrogated four collections of such data for patterns, and then used the results to work out independently what the relevant genes were doing. Not only did the software recapitulate parts of existing genome resources (for instance, identifying components of cellular machinery that help to dispose of spent proteins), but it started filling in gaps by finding similar patterns of organization for genes with unknown functions. \u201cWe trolled the transcriptome and inter\u00adactome data and inferred the entire hierarchical structure of the components in a cell,\u201d says Ideker. \u201cI\u2019m more excited about this technology than I\u2019ve been about anything in a long, long time.\u201d Such algorithms will not supplant human data curators, but they can pick up patterns that would be missed by humans or text-mining software that extracts relationships from published papers, he says. \u201cCells don\u2019t speak English; they speak data.\u201d Last year, Michael Snyder, a geneticist at Stanford University in California, published his personal integrome 7  (although he called it an \u201cintegrative personal omics profile\u201d \u2014 and others dubbed it the narcissome), combining data for his genome, transcriptome, proteome and metabolome (see Nature http://doi.org/hrq;2012 ). The genomic profile revealed that Snyder had a risk variant for diabetes; during the study he was diagnosed with the disease and fought off two viral infections, which were reflected in increased activity of genes associated with inflammation. The \u2019omes also revealed changes in pathways not previously associated with diabetes or infection, says Snyder. \u201cHad you only followed transcriptome or proteome, you would have only got part of the picture.\u201d \n               boxed-text \n             \n                 Click here for the crossword answers \n               \n                     Data barriers limit genetic diagnosis 2013-Feb-13 \n                   \n                     Daily dose of toxics to be tracked 2012-Nov-27 \n                   \n                     One-stop shop for disease genes 2012-Nov-07 \n                   \n                     Open-data project aims to ease the way for genomic research 2012-Apr-25 \n                   \n                     Proteomics: The interaction map 2012-Apr-11 \n                   \n                     Epidemiology: Every bite you take 2011-Feb-16 \n                   \n                     Badomics generator \n                   \n                     Badomics awards \n                   \n                     Getting ready for the Human Phenome Project (PDF) \n                   \n                     Thomas Hartung \n                   \n                     Marc Vidal \n                   \n                     Mark Gerstein \n                   \n                     Trey Ideker \n                   Reprints and Permissions"},
{"file_id": "495028a", "url": "https://www.nature.com/articles/495028a", "year": 2013, "authors": [{"name": "Heidi Ledford"}, {"name": "Anna Petherick"}, {"name": "Alison Abbott"}, {"name": "Linda Nordling"}], "parsed_as_year": "2006_or_before", "body": "What's being female got to do with anything, ask the scientists who are starting labs and having kids. \n               Kay Tye: Power mover \n             The neuroscientist break-dancing down the tenure track . Being five months pregnant comes with a series of concessions: no booze, no sushi, no double-shot espressos. Less appreciated, perhaps, is the havoc it can wreak on a breakdancer's moves. \u201cMy dancing is definitely limited now,\u201d says Kay Tye, neurobiologist, award-winning b-girl and assistant professor at the Picower Institute for Learning and Memory at the Massachusetts Institute of Technology (MIT) in Cambridge. \u201cI can't do windmills \u2014 I can't do anything that might cause me to fall. Which is, like, everything.\u201d It is one of the few limitations that Tye, 31, has been willing to accept. Striving to make her mark in optogenetics, one of the hottest fields in neuroscience, Tye thought nothing of working past midnight, getting by on four or five hours sleep a night and maintaining a constant, transcontinental travel schedule. She has had to dial back a little in recent weeks, and she knows that life may change further once her daughter is born. But she is ready. \u201cI've been preparing for this my entire life,\u201d she says. \u201cI chose a career path that's family friendly.\u201d An assistant professorship at MIT, where the tenure rate hovers at around 50% and the faculty is still about 80% male, may not strike many as particularly family friendly. But Tye, the daughter of a theoretical-physicist father and a biochemist mother, grew up in her mother's lab, where she was paid 25 cents per box to rack pipette tips. With her mother as a role model, Tye says that she was in her teens before it occurred to her that her gender could hold back her career. \u201cAnd by then, my brain was already fully formed,\u201d she says with a smile. Even so, Tye wasn't sure that science was for her. After graduating from MIT, where she first took up break-dancing, she travelled to Australia to live on a cattle farm, in a yoga ashram and finally in a beach tent in an art commune. Her goal was to live moment-to-moment and write a novel based on her experiences. But Tye found her new lifestyle unfulfilling \u2014 and, she adds, the novel wasn't very good. She flew back home and started graduate school at the University of California, San Francisco. After rotating through the usual three labs without finding a suitable home, she begged neurobiologist Patricia Janak for the chance to do one last placement in her lab. \u201cIf you don't let me rotate, I'm going to drop out,\u201d Tye tearfully told her. Tye got the place and a new mentor: Janak, a successful female scientist with two children. And Janak watched Tye bloom. \u201cHer insecurities rapidly disappeared,\u201d she says. \u201cShe started to get amazing results.\u201d In Janak's lab, Tye published her first  Nature  paper after finding that in rats learning to associate a cue with a reward, there was a boost in the activity and synaptic strength of neurons in the amygdala, a brain region that in humans is associated with processing emotions (K. M. Tye  et al .  Nature   453 , 1253\u20131257; 2008 ). But Tye wasn't satisfied: she wanted to be able to switch neurons on or off directly. That led her to optogenetics, a way to activate or inhibit specific neurons in rodent brains using light. After a two-year postdoc learning the technique in Karl Deisseroth's lab at Stanford University in California, Tye landed at the Picower. She plans to use the approach to map the neural circuits that govern whether an animal forms a positive or a negative association with a given environmental cue. Ultimately, she hopes that her studies can be used to devise ways to treat disorders such as anxiety, depression and addiction. Over the past five years, the Picower has recruited a number of young female faculty members, several of whom have since started families. (MIT opened a day-care facility across the street from Tye's office in 2004 and uses it as a recruitment tool.) It definitely helps to know they have paved the way, says Tye. Since her return to MIT a year ago, Tye has recruited four graduate students and four postdocs, applied for 13 grants, extended her list of top-tier papers and begun to prepare herself for the impact of motherhood. Some decisions are easy: the exercise bike in her office will be replaced with \u201ca crib, or playpen, or whatever\u201d for the times that her daughter accompanies her to work. Some are more difficult, like a trip to Tokyo to speak at a conference a month after the baby is due. Tye can't say no, not yet. And tenure remains near the top of her list. \u201cI never thought that my life had to be limited to anything,\u201d she says. \u201cAnd I want to set that example for my daughter.\u201d \n               Keity Souza Santos: Venom detective \n             An immunologist who studies allergic shock receives a shock of her own . What should have been an ordinary Thursday for Keity Souza Santos turned out to be anything but. It was 4 a.m. when she woke up on 22 November 2012, tired but alert. She had been meaning to take a pregnancy test for days; now she decided she couldn't put it off any longer, and headed to the bathroom. Later, at work at the University of S\u00e3o Paulo Medical School's allergy and immunology department in Brazil, Santos, 33, told none of her colleagues why she had felt like screaming for joy hours earlier. She kept her secret even when one of them called to tell Santos that she had won the prestigious Young Investigator Award from the S\u00e3o Paulo Research Foundation. That meant that she would be starting her own lab at about the same time as her baby was due. Only it will not be just one baby; Santos is expecting twins. Santos studies life-threatening allergens in foods and insects, a serious threat in Brazil. Well known for its stunning biodiversity, the country has more than 400 species of wasp compared with the Northern Hemisphere's 30-odd. One species,  Polybia paulista , causes hundreds of hospitalizations in Brazil every year. But doctors often have trouble pinpointing the cause of the allergic reactions. \u201cSometimes patients even bring the wasp to the hospital, but even then we cannot treat them properly because we don't know what allergens are in the sting,\u201d Santos says. During a PhD at the University of S\u00e3o Paulo, Santos worked on an antivenom against the sting of the Africanized honeybee or 'killer bee' ( Apis mellifera  L.). As a postdoc, she studied the proteins responsible for anaphylactic reactions to cassava ( Manihot esculenta ), a staple food in north Brazil, and to the sting of several wasps. From  P. Paulista  alone, she and her colleagues separated out and identified 84 venom proteins \u2014 including some that had previously been found only in snake venom \u2014 and showed how they can trigger devastating tissue damage (L. D. dos Santos  et al .  J. Proteome Res.   9 , 3867\u20133877; 2010 ). Now she is trying to identify the offending proteins in other insect venoms. To learn the mass spectrometry and other molecular techniques required for the task, Santos spent months in labs in the United States and Austria. While abroad, she heard tales of sexism, something she says she did not encounter when growing up in Brazil. Santos says that her family was shocked when she announced (after reading about the cloning of Dolly the sheep) that she wanted to become a biologist, but not because of her gender. No one in her family was a scientist, and such a career was different from the world they knew. Now working largely independently, Santos's goal is to create kits that will help doctors to quickly identify the allergens to which a person has been exposed and how to detoxify them. But first she is focused on the challenges that this summer will bring. \u201cMy boss is a little bit worried,\u201d she says. \u201cBut I already have a PhD student and a technician. We can Skype a lot [during my maternity leave].\u201d \u201cI think she will manage,\u201d says immunologist Jorge Kalil, Santos's head of department, before adding after a pause, \u201cbut they are twins\u201d. Santos has no such qualms. \u201cI want to increase my group of students and collaborators,\u201d she says. \u201cWhy would I give up my scientific career now?\u201d \n               M\u00f3nica Bettencourt-Dias: Cell mechanic \n             A biologist who explores and shares the intricacies of the cell . M\u00f3nica Bettencourt-Dias grew up surrounded by role models. Despite being relatively poor, Portugal has an excellent record within Europe for appointing women to top positions in academia and other professions. Some think that the situation traces back to the 1960s and 1970s, when educated young men were sent to fight in Angola and Mozambique, leaving room to promote women and spawning a gender blindness in academia. On top of that, Bettencourt-Dias was raised by a supportive mathematician father and social-scientist mother, and she came of scientific age just as her country was introducing an innovative, government-paid doctoral programme, for which she was selected in 1996. \u201cWe had some of the world's best scientists teaching us,\u201d she recalls, \u201cand I was able to learn that my destiny was cell biology and development.\u201d Bettencourt-Dias travelled to University College London to study the regenerative properties of salamanders as part of her PhD. Later, as a postdoc at the University of Cambridge, UK, she discovered a master regulator of the centrosome, an organelle that organizes some of the key structures and machinery involved in the cell cycle, bagging her first  Nature  paper (M. Bettencourt-Dias  et al .  Nature   432 , 980\u2013987; 2004 ). She returned to Portugal in 2006 to start her own laboratory at the Gulbenkian Institute of Science in Oeiras and the money has flowed generously ever since \u2014 including a prestigious \u20ac1.5-million (US$2-million) Starting Grant from the European Research Council. Now 39, Bettencourt-Dias's life changed a few months ago with the long-awaited arrival of her adopted one-year-old daughter. But Portugal has abundant professional child-care places, and family members tend to be close by to help out. Bettencourt-Dias's husband, also a scientist, does his share of the child-raising, and the couple has hired help for their domestic chores. In the lab, Bettencourt-Dias still focuses on the tight communication and organization imposed by cellular-signalling pathways and centrosomes. There are parallels in her own life. Being well-organized has been essential to her career, she says, and she developed a drive for communicating science to the public that has led to regular participation in workshops in Portugal's former colonies. Last year, she organized a molecular biology workshop in the west African island nation of Cape Verde, which in 2008 became the first country in Africa to have a government comprising a majority of women. The workshop had a similar number of men and women, Bettencourt-Dias says, and the attendees \u201ctold me they wanted to learn science to help their country \u2014 you don't hear this in Western countries\u201d. Sharing is a life philosophy for Bettencourt-Dias; her discoveries are recalled in those terms. One of her first such moments came in Cambridge when she and her first PhD student showed that an enzyme called PLK4 is important for the structure of fly centrosomes (M. Bettencourt-Dias  et al .  Curr. Biol.   15 , 2199\u20132207; 2005 ). Together, they watched scores of tiny centrosomes forming under the microscope. \u201cIt is beautiful to share that moment with someone you are teaching,\u201d she says. \n               Amanda Weltman: Driving force \n             A cosmologist who probes dark energy and ignores stereotypes . When Amanda Weltman discovered physics as an undergraduate at the University of Cape Town in South Africa, she thought that \u201cunderstanding the way the Universe worked was just about the coolest job anyone could have\u201d. Weltman was just 24 when she shot to fame with a proposal about how the Universe works at the grandest scales. Her breakthrough paper, ' Chameleon Cosmology ' (J. Khoury and A. Weltman  Phys. Rev. D   69 , 044026; 2004 ), published when she was graduate student at Columbia University in New York, gave rise to a popular theory to explain the phenomenon of dark energy \u2014 the mysterious force that is hypothesized to be speeding up the expansion of the Universe. Weltman and her colleague Justin Khoury suggested that a new force that changes according to its environment could explain many observations about the Universe's expansion. This 'chameleon' force would be weak when particles are packed together, such as on Earth or in the early Universe. But as galaxies fly apart the force would grow, and accelerate the growth. What makes their theory popular is its testability: it predicts that a photon will sometimes decay into a chameleon 'particle' when travelling through a strong magnetic field. Experimental physicists have begun looking for this effect, but haven't yet found anything conclusive (see  Nature   http://doi.org/b96z3f ; 2009 ). In 2009, after finishing her PhD at Columbia and a postdoc at the University of Cambridge, UK, Weltman moved back to South Africa. This enabled her to start a life with her husband, string theorist Jeff Murugan, whom she had met a decade earlier. Until that point, their courtship had been a typical case of academia's 'two-body problem' \u2014 mostly conducted at great distances. Their return to South Africa was also driven by idealism. After years learning from the best in their disciplines, they wanted to bring that expertise home. \u201cWe thought we could be better put to use here to grow the country's science and knowledge,\u201d Weltman says. Back at the University of Cape Town, she is part of a large research group, but is also building her own \u2014 she has one student and one postdoc so far \u2014 to extend and test the chameleon theory. Last year she received a 'P' rating from the country's National Research Foundation, a distinction given to a handful of young researchers who are on their way to becoming international leaders in their field. Weltman thinks that early barriers for women \u2014 the expectations that girls are better at soft sciences than hard ones, or that mathematics is more for boys than girls \u2014 are the most harmful. She was raised in a family in which such stereotypes did not exist, she says, and is grateful to have had role models, many of whom were men. \u201cI don't think girls necessarily need girl role models, but I think they need good role models,\u201d she says. Having her husband down the hall was handy after the arrival of their two children, now 2 years old and 8 months old. Weltman, who is now 33, kept her research going through her four-month bouts of maternity leave, which in practice were only a leave from teaching. She admits that it was tough at times. Her husband, she says, \u201clooked after the children as much as possible, so I could work. Together we make it work by finding the cracks in the day\u201d. They go to conferences as a family and take turns looking after the children. Academia offers flexibility, but it can still be a daunting place to start a family, Weltman says. \u201cWhen I was pregnant, I felt a little bit defensive and guilty, like I was admitting that my personal life was important to me by having a child,\u201d she says. \u201cIn physics, you are supposed to be life, blood, flesh, dedicated 100% to your research.\u201d \n                     Nature\u2019s sexism 2012-Nov-21 \n                   \n                     Throw off the cloak of invisibility 2012-Oct-22 \n                   \n                     Prions and chaperones: Outside the fold 2012-Feb-15 \n                   \n                     Neuroscience: One hundred years of Rita 2009-Apr-01 \n                   \n                     Nature special: Women in science \n                   Reprints and Permissions"},
{"file_id": "495025a", "url": "https://www.nature.com/articles/495025a", "year": 2013, "authors": [{"name": "Alison McCook"}], "parsed_as_year": "2006_or_before", "body": "The number of women in scientific research is going up \u2014 but where academia crosses into industry, men still rule. Nancy Hopkins started Googling her colleagues in spring 2012. She mentally scanned the hallways of her institution at the Massachusetts Institute of Technology (MIT) in Cambridge \u2014 along with the campuses of other elite institutions \u2014 for the offices of men she knew who had founded companies. Then she clicked on the websites of their firms, and counted the number of men and women serving on their scientific advisory boards (SABs), a prestigious position for researchers who steer the company's scientific direction. It was an informal exercise, rather than a systematic survey. But Hopkins, a molecular biologist at MIT and a long-time campaigner for women in science, found the results shocking. A sample of 12 of the companies she examined had a total of 129 SAB members; only 6 were women. \u201cI was completely stunned,\u201d says Hopkins. \u201cAnd it made me sad. I thought, 'gee, why don't these men want to work with [MIT] women?' We have such incredible women faculty.\u201d The proportion of women in industrial and academic science has shot up over the past 20 years. According to the US National Science Foundation, women make up 25% of tenured academics in science and engineering and more than 25% of industry scientists in research and development. But when it comes to academics engaging in commercial work \u2014 patenting their discoveries, starting biotech companies or serving on SABs \u2014 the picture is less progressive. Studies have confirmed Hopkins' impression that even leading female scientists are often absent from these roles. \u201cThe secret club [of men] used to be going to the lab and conferences,\u201d says Fiona Murray, who studies life-sciences entrepreneurship at MIT. \u201cThat world has changed a lot, but we have a new venue where it is still difficult for women to play a similar role.\u201d Experts in industry and academia speculate that the disparity could reflect the small numbers of women in certain specialized fields; the demands of family life; or a residual male clubbiness. Whatever the reasons, this stubborn gender gap hurts everyone, says Bonnie Bassler, a molecular biologist at Princeton University in New Jersey. \u201cI think the companies would do better science by having the best people on their board. And I think these women, who are great scientists, would do better science in their labs by having access to these ideas.\u201d \u201cEverybody's losing,\u201d says Bassler. \n               Hidden problem \n             For much of the 1980s and 1990s, there were more than 11 men for every one woman in the science faculty at MIT. Things started to change 20 years ago, when Hopkins, as the first chair of MIT's Committee on Women Faculty in the School of Science, and her team drove through major increases in the hiring of women. By 2006, one out of every five biology faculty members on the MIT campus was a woman. At a dinner last April to honour these achievements and mark her retirement from the lab, Hopkins spoke about the work still to be done. She talked about a list she had been given by a graduate of Harvard Business School in Boston, Massachusetts, showing the names of scientists in the area who had received funding from a local venture-capitalist firm. Among 100 names, only one was a woman. The list would not have surprised Hopkins more than 30 years ago, when she had been told by a colleague that \u201cwomen aren't allowed\u201d to found biotech companies. But to see such a dearth of academic women in modern biotechnology was upsetting. Around that time, Hopkins embarked on her Google search. She was particularly interested in SABs because they consist mainly of working scientists who are often invited by the company's academic founders \u2014 a social process that could reveal conscious or unconscious biases against female academics. And membership in advisory boards comes with advantages: it can tip members off to promising tools and areas of research, and lead to other lucrative prospects, such as consulting. Plus, for a few meetings per year, board members are paid a sometimes-substantial fee, given stock options, or both. The first name Hopkins looked up was Eric Lander, founding director of the Broad Institute of MIT and Harvard. She typed \u201cEric Lander companies\u201d into the search engine. Scrolling through the results, she came upon Verastem, a cancer stem-cell company founded in 2010 by Lander and others, including Robert Weinberg, a cancer researcher at the Whitehead Institute in Cambridge. She counted 14 people on Verastem's SAB; all were men. Entering \u201cPhil Sharp companies\u201d brought up Alnylam Pharmaceuticals, a Cambridge-based firm co-founded by the Nobel prizewinning molecular biologist at MIT in 2002. The company, which is developing therapies based on RNA interference, had one woman on its 11-person SAB. \u201cBob Langer companies\u201d yielded a handful of the 20-plus firms that the MIT bioengineer has helped to launch, including Taris Biomedical in Lexington, Massachusetts, which focuses on genitourinary conditions, and the biopharmaceutical company Blend Therapeutics in Watertown, Massachusetts. Neither SAB included any women. (Weinberg and Lander say that they were not involved in selecting the SABs at Verastem, and Langer that he was not involved with the process at Blend or Taris. Sharp says that at Alnylam, choosing the SAB required \u201cagreement between\u201d the founders, chief executive, venture capitalists and other people already brought into the company.) Hopkins included in her search a few scientists from other institutions, such as Harvard University in Cambridge and Memorial Sloan-Kettering Cancer Center in New York. Overall, among the full-time professors affiliated with a sample of 14 companies she reviewed, only 5% of founders or SAB members were women. Although boards change over time, that fraction was much the same as of last month. Last July, Hopkins began circulating her results to a handful of faculty members at MIT and to scientists further afield. Vicki Sato, a professor of biology and management at Harvard with a long career in the biotechnology industry, says she could not believe what she was seeing. \u201cI was stunned by the sampling she had done, and told her she had to be wrong,\u201d says Sato. \u201cBut I knew deep down she was right.\u201d \n               Global concern \n             More rigorous studies have reached similar conclusions. In a paper published last October 1 , Murray, Toby Stuart at the University of California, Berkeley, and Waverly Ding at the University of Maryland in College Park reviewed all publicly available lists of US biotech SABs, starting in the 1970s and including about 500 companies. Although women represented between 12% and 30% of academically active PhD holders over that time period, the percentage of women on SABs never exceeded 10.2% (see 'Inequality on board'). Even when the researchers compared male and female faculty members with similar levels of achievement, measured by factors such as publication and citation counts, male scientists were roughly twice as likely to join SABs as female ones. SABs are not the only commercial forum in which academic women seem to be disadvantaged. US women also receive patents about 40% as often as men 2 , start businesses half as often 1  and receive significantly less funding for the start-ups that they do launch 3 . This is not just a US problem: a study released in April 2012 by the Royal Society of Edinburgh found that women are underrepresented on the boards of UK science, technology, engineering and mathematics companies 4 . That is despite the fact that including women seems to be beneficial: a 2012 report from Credit Suisse in Zurich, Switzerland, found that worldwide, companies with women on the board have higher share prices than those with all-male boards 5 . \n               Invitation only \n             So what is going on? For SABs, Hopkins thinks that the answer is simple: women are not asked. When she noticed the stark patterns in board memberships, Hopkins asked some of her female colleagues \u2014 including one she believed was an \u201cabsolute star\u201d \u2014 if they had ever been invited to serve on boards. All of them said no. \u201cIn the end, these stories are very sad,\u201d says Hopkins. \u201cPeople know they're excluded, and it's costly professionally. They're embarrassed to talk about it. It's like not being asked to dance.\u201d But the picture is not so simple, says Paul Schimmel, a former colleague of Hopkins who is now based at the Scripps Research Institute in La Jolla, California, and is a co-founder of Alnylam. He says that he has tried to ensure equal gender participation in his lab and his companies for the past 20 years. \u201cThere's no lack of effort, I tell you,\u201d says Schimmel. But serving on a board \u201ccan be a lot of work\u201d \u2014 conference calls, e-mails, travel several times a year and thick documents to review \u2014 and women often bear the majority of domestic work and child care. At least one woman has turned down Schimmel's invitation to serve on an SAB because of family responsibilities, he says. Indeed, research has shown that female academics with children are less likely than those without to patent their discoveries 6 . Some prominent female scientists disagree. Carolyn Bertozzi, a chemical biologist at the University of California, Berkeley, who has two young children and one on the way, says that she is always willing to make time to serve on the research advisory board at GlaxoSmithKline, which entails attending two-day meetings twice a year for \u201cgenerous\u201d compensation. The meetings teach her about what it takes to make a drug, including medicinal chemistry, regulatory issues and intellectual property; that helps with her start-up, Redwood Bioscience in Emeryville, California, which has two female SAB members out of four. Bertozzi acknowledges that her situation is unusual: her female partner is a stay-at-home mother. But Bassler, too, says that the work involved in SABs is worth the sacrifices. \u201cIf I were asked to serve on a board, I wouldn't do something else,\u201d she says. Bassler has been invited to serve on two SABs in her career, but \u201cof course\u201d would accept another invitation if it arose. Research seems to support the idea that it is a lack of invitations \u2014 not a lack of time \u2014 that reduces female membership in biotech SABs. Murray, Stuart and Ding found that both men and women tend to join SABs on average around the 20th year after completing their PhDs 1  \u2014 often a time when the major strain of child rearing is over. This suggests that family obligations are not holding back women more than men. And in interviews at a leading institution that Murray declined to name, women consistently reported they had rarely been invited to serve on their colleagues' SABs \u2014 which was not the case in a matched sample of men 7 . Stuart says that the disparity is most likely to be a result of social connections and unconscious bias among men. \u201cIf you're male, you're slightly more comfortable shooting the shit with your male colleagues, and they're who come first to mind when you're putting these boards together. You may assume \u2014 'oh, she's got two kids, she's not going to be interested' \u2014 and then not invite her.\u201d But companies say that they can have difficulty finding women with the right experience, because there are fewer women than men in academia overall. At Alnylam, says Schimmel, the type of science and the diseases it hopes to treat \u201cconsiderably narrow the size of the pool of highly qualified senior investigators, regardless of gender\u201d. (A statement from the company notes that women represent \u201cnearly 30%\u201d of Alnylam's management team.) At Taris, says Langer, the SAB had to include mostly clinical experts in urology, who are generally men. And Verastem found that there were few prominent female biologists who focus on cancer stem cells, says chief medical officer Joanna Horobin. At least one woman declined the offer to join the SAB, Horobin says, because she was already working with a competing company. The academics and biotech companies interviewed for this story say that they hope the situation will change. At Alnylam, people have \u201cdiscussed openly the issue of gender and the SAB\u201d, says Schimmel. \u201cAll of us support strongly the idea of addressing the 'gender problem' in a thoughtful way and are actively working on it.\u201d In Lander's opinion, more important than the make-up of the SAB is the selection of the company's board of directors \u2014 who \u201ccontrol the entire company\u201d. Two out of seven directors at Verastem are women. Women can also make the first move, says Helen Blau, a stem-cell biologist at Stanford University in California, who has served on the advisory boards for several start-ups. She broke into commercialization by patenting discoveries and talking to companies at conferences about her work. The effort paid off: companies have licensed at least a dozen of her patents, which helped Blau to get consulting jobs, board invitations and now her own start-up, Didimi in Berkeley, California. Hopkins, meanwhile, has not let the issue lie. After she discussed her data with MIT colleagues, the group decided to forward the findings to the university's provost, Chris Kaiser. It turned out that Lydia Snover, director of institutional research at MIT, had already started mining faculty CVs across the entire institution for information about activities such as patenting, technology licensing and participation in SABs. If MIT finds gender differences and can help to do something about them, it will, says Snover. \u201cWe want all [faculty members] to be involved in the same way.\u201d Hopkins wants to see all institutions follow MIT's example. In academia, people used to believe that \u201ctime would fix things naturally\u201d, and that women would eventually move up the ranks, she says \u2014 and this attitude may still exist when it comes to academics moving into industry. \u201cI think [the gender disparity in SABs] is what universities would look like if we hadn't stopped, analysed what was going on, and changed it. If you don't put attention to it, it doesn't happen.\u201d \n                     Women in science: Women\u2019s work 2013-Mar-06 \n                   \n                     Nature\u2019s sexism 2012-Nov-21 \n                   \n                     Throw off the cloak of invisibility 2012-Oct-22 \n                   \n                     A passion for science without barriers 2012-Jan-04 \n                   \n                     Women in business: Finding a way in 2012-Jan-04 \n                   \n                     Nature special: Women in science \n                   Reprints and Permissions"},
{"file_id": "495021a", "url": "https://www.nature.com/articles/495021a", "year": 2013, "authors": [], "parsed_as_year": "2006_or_before", "body": "A special section of  Nature  finds that there is still much to do to achieve gender equality in science Science remains institutionally sexist. Despite some progress, women scientists are still paid less, promoted less,win fewer grants and are more likely to leave research than similarly qualified men. The reasons range from overt and covert discrimination to the unavoidable coincidence of the productive and reproductive years. In this special issue,  Nature  takes a hard look at the gender gap and at what is being done to close it. A survey of the data (see  page 22 ) reveals where progress has been made and where inequalities still lie, from salary to tenure. A News Feature (see  page 25 ) reveals a particular dearth of women in some commercial spheres, such as on the scientific advisory boards of biotechnology firms, and an article by historian Patricia Fara (see  page 43 ) traces the wearying stereotypes perpetuated by the biographers of women scientists. A series of Comment articles looks at possible solutions. Neuroscientist Jennifer Raymond (see  page 33 ) calls on both sexes to recognize and reduce their biases against women in science, and eight researchers from around the world offer their prescriptions (see  page 35 ), from equalizing the retirement age in China, to liberalizing travel restrictions in Saudi Arabia, to boycotting conferences that lack female speakers. We catalogue some of the ambitious moves being made in Europe to get more women into top positions (see  page 40 ) and explore some surprising statistics about mandatory quotas (see  page 39 ). Finally, profiles of four successful 30-something women (see  page 28 ) show how ambition and talent can trump obstacles. This special issue is dedicated to the memory of  Maxine Clarke . In the 28 years that Maxine spent championing the highest scientific standards as an editor at  Nature , she was all too often the only one to ask, \u201cWhere are the women?\u201d \n                     Nature special: Women in science \n                   Reprints and Permissions"},
{"file_id": "493014a", "url": "https://www.nature.com/articles/493014a", "year": 2013, "authors": [{"name": "Leigh Phillips"}], "parsed_as_year": "2006_or_before", "body": "The world is starting to win the war against tuberculosis, but drug-resistant forms pose a new threat. If there was any doubt that tuberculosis (TB) was fighting back, it was dispelled in 2005, at the Church of Scotland Hospital in the village of Tugela Ferry, South Africa. Doctors at the hospital, in a rough, remote corner of KwaZulu-Natal province, were hardened to people dying from gunshots and AIDS. But even they were puzzled and frightened when patients with HIV who were responding well to antiretroviral drugs began dying \u2014 rapidly \u2014 from TB. With ordinary TB, patients start to feel better after a few weeks or months on a selection of four mainstay antibiotics. But of the 542 people with TB at the hospital in 2005 and early 2006, 221 (41%) had a multi-drug-resistant (MDR) form, against which these therapies are mostly powerless. Worse, 53 of them did not even respond to the few antibiotics that form a second line of defence. Eventually, doctors had nothing left to try: all but one of the 53 died, half of them within 16 days of diagnosis. It was the first major outbreak of what became known as extensively drug-resistant (XDR) TB \u2014 and a wake-up call to the world that TB had taken a turn for the worse 1 . In the early 1980s, TB cases had dropped to such low rates that Western policy-makers frequently talked of eradication of the disease. Then came the HIV epidemic, which triggered a resurgence of TB in the late 1990s. But the latest report on TB from the World Health Organization (WHO), published in October, revealed signs of progress against normal \u2014 or drug-sensitive \u2014 cases of the bacterial disease. New infections have fallen and the mortality rate has dropped by 41% since 1990. But, the report warned, \u201cdrug-resistant TB threatens global TB control\u201d. Some 3.7% of new cases and 20% of previously treated cases are MDR-TB. And whereas in 2000 the highest incidence of MDR-TB was 14%, in Estonia; in 2010 that figure had jumped to 35%, in Russia's Arkhangelsk province. An estimated 9% of drug-resistant cases are XDR-TB, which has now been reported in 84 countries. It is a tale of two TBs. Once detected, drug-sensitive TB is almost always treatable, as long as the appropriate drugs are provided and taken. Simple practices \u2014 such as checking that patients take their medicine \u2014 can be transformative. But in some countries, particularly in eastern Europe, Asia and Africa, the weakening or collapse of health-care systems over the past two decades has meant that patients do not always finish their drugs, or they take the wrong ones, allowing highly transmissible, drug-resistant strains to emerge and spread. Drug-resistant TB is harder, more expensive and more time-consuming to treat. New tools are needed \u2014 but there have been no new anti-TB drugs in more than 50 years, and the current vaccine is largely ineffective. The most common diagnostic technique \u2014 analysing sputum samples under a microscope \u2014 can determine that  Mycobacterium tuberculosis  bacteria are present but not whether they are drug resistant. Meanwhile, researchers have lacked interest in developing drugs and tests, and drug companies have lacked market incentives to do so. The growth of multi-drug resistance is an \u201cescalating public-health emergency\u201d, says Grania Brigden, TB adviser for M\u00e9decins Sans Fronti\u00e8res (Doctors Without Borders) in Geneva, Switzerland: \u201cWith barely 1 in 20 TB patients being tested for drug resistance, we're just seeing the tip of the iceberg.\u201d But scientists are careful to temper their alarm. In the past decade, researchers and policy-makers have fought for and won a reversal in funding and attention for TB. Several new drugs are in development, and progress is being made towards an effective vaccine. \u201cI do worry when people stand up at conferences and talk about MDR-TB and say it's a big disaster and the whole world is going to collapse. It's not that severe yet,\u201d says Tim McHugh, head of the Centre for Clinical Microbiology at University College London, who leads a team that is trialling one of the two most advanced candidates for new TB drugs. \u201cThe big anxiety is that if we don't act now, it will easily run away from us.\u201d \n               Rise and fall \n             TB is one of the world's leading killers, stealing 1.4 million lives and causing 8.7 million new and relapse infections in 2011. One-third of the world's population carries the bacterium, but most will never develop the active form of the disease. The first modern TB epidemic took off in the late 1700s, during the Industrial Revolution. Rural workers in Europe and North America moved in droves to cities, where poverty \u2014 and related malnutrition and overcrowding \u2014 created an ideal environment for the disease's spread. But as hygiene, nutrition and medicine improved, what was known as the Great White Plague began to ebb. \u201cBy the 1940 and 50s, things looked quite bright,\u201d says McHugh, who seems almost as interested in the history of TB as in its microbiology. The Bacillus Calmette\u2013Gu\u00e9rin (BCG) vaccine, first used in the 1920s, helped. But BCG is now effective mainly against childhood TB, which is not infectious, rather than the adult form. What really broke TB's back was the introduction of isoniazid, in 1952, and then rifampicin, in the 1970s. \u201cIf you look at a graph of TB from the 1950s onward, [infection rates] just collapsed,\u201d McHugh says. Then, in the 1980s and 1990s, HIV hit. \u201cYou can't underestimate the importance of HIV,\u201d McHugh says. A co-infection of TB and HIV produces a powerful biological synergy, accelerating the breakdown of the body's immune defences; latent TB infection is 20\u201330 times more likely to become active in people who have HIV. In 1993, the WHO declared TB a global emergency. Worldwide, TB is now the leading cause of death among people with HIV. The resurgence of ordinary TB set the stage for drug-resistant forms. Resistance develops when people do not stick to their drug regimens \u2014 which typically last six months for drug-sensitive TB and 20 months for MDR-TB \u2014 allowing naturally occurring resistant mutants to grow and evolve. MDR-TB, which grew more threatening during the 1990s, is resistant to isoniazid and rifampicin. People with this form require second-line drugs \u2014 broad-spectrum antibiotics called fluoroquinolones or injectable agents (amikacin, capreomycin and kanamycin). These treatments are less effective, more toxic and take many more months to work than first-line therapies. An infection is classified as XDR-TB if it is also resistant to fluoroquinolones and at least one of the injectables. The XDR-TB outbreak at Tugela Ferry, when it was reported in 2006, rattled the TB research and policy community. Experts agree that the biggest driver for the growth in drug-resistant TB has been the deterioration in some countries' health-care infrastructures, including TB programmes, since the 1990s \u2014 particularly in the former Soviet bloc. This decline has meant that patients are not diagnosed and treated; and in some countries, over-the-counter availability of anti-TB drugs also encourages people to take inappropriate second-line therapies, accelerating the growth of drug resistance. In a stroke of bad luck, the virulent and often drug-resistant 'Beijing' strain of TB, identified in 1995 in China, swept through Russia and eastern Europe just as the region's public-health provision was being dismantled. \u201cThere was a confluence of the biology of the organism and its progress into Russia, where lots of people had a health-care system that was collapsing around their ears,\u201d says McHugh. (In 2010, the Beijing strain was found in around 13% of active TB infections worldwide.) All of this helps to explain why the recent WHO report shows the highest burden of MDR TB in Russia's Arkhangelsk province and in Belarus, Estonia, Kazakhstan, Kirghizia and Moldova (see 'Two faces of TB'). \n               Fighting back \n             Over the past decade or so, the paths of drug-sensitive and drug-resistant TBs have diverged. The solution for drug-sensitive TB is simply to deliver the drugs and diagnostics to patients \u2014 and the drive to do so has grown. One of the United Nations Millennium Development Goals set in 2000 was to halt and begin to reverse the incidence of TB by 2015; in 2001, the international Stop TB Partnership was established, bringing together government programmes, researchers, charitable foundations, non-governmental organizations (NGOs) and the private sector. One major result of these and other efforts was a global expansion of 'directly observed treatment, short course' (DOTS), a strategy promoted by the WHO to combat drug-sensitive TB. Once diagnosed, the disease is treated with a supply of first-line drugs, taken under the close observation of health-care workers to ensure that people finish the course. Thanks in large part to such efforts, the WHO says that the world is on track to halve TB mortality from 1990 levels by 2015. Tackling drug-resistant TB, however, will require not just the rebuilding of health-care infrastructure, but also new weapons, such as diagnostics, drugs and vaccines. The private sector has had little incentive to invest in basic research whose eventual products, if any emerge, would largely be sold at low cost in poor countries. \u201cThey have to look at the bottom line,\u201d says Anthony Fauci, head of the US National Institute of Allergy and Infectious Diseases in Bethesda, Maryland. In the past decade or so, global TB programmes have pumped money into research. One major development came in 1998, when researchers at the Wellcome Trust Sanger Institute in Hinxton, UK, published the genome sequence of  M. tuberculosis , allowing researchers to identify and study genes that underlie the bacterium's virulence and ability to evade the immune system 2 . In 2012, the US National Institutes of Health in Bethesda started a bigger genome-sequencing project that aims to uncover the genetic roots of drug resistance. \u201cWe'll use next-generation sequencing technologies to sequence 1,000 TB clinical isolates from around the world \u2014 South Africa, Korea, Russia, Uganda \u2014 anywhere drug-resistant TB is heavily present,\u201d says Fauci. \n               War chest \n             There are now ten TB drugs in clinical trials. The aim is to find compounds that are effective against resistant strains and that work faster and have fewer side effects, so that patients will be more likely to finish the course. McHugh and his team, for example, are running a clinical trial at sites across Africa and Asia to test the antibiotic moxifloxacin, which is commonly used for pneumonia and skin infections. (They expect to release preliminary results in 2013.) The researchers are also working to speed up the screening process for potential drugs by using mycobacterial species that are less pathogenic and more fecund than  M. tuberculosis , which is slow-growing, finicky and poses a biosecurity risk. \u201cPreviously, what you had was a chemist who says 'I've got this molecule that will kill  Escherichia coli . I'm fairly sure it should kill TB. But there's nowhere I can see if it does,'\u201d McHugh says. Accurate and fast diagnostic tests for drug-resistant strains are also a key part of the fight, and a number of tests have come online in the past five years. One, called GeneXpert, takes 90 minutes to complete and is based on a gene-amplification technique that detects DNA sequences specific to  M. tuberculosis  and to rifampicin resistance. The system has been endorsed by the WHO and subsidized by a coalition of organizations, but researchers are still seeking simpler, cheaper options. Only better vaccines will solve the problem for good. \u201cWe must invest in vaccine research if our ultimate goal is to be able to prevent the disease rather than forever chase growing drug resistance,\u201d says Helen McShane, a vaccine researcher at the University of Oxford, UK. In 2008, the European Commission pushed for the creation of the TB Vaccine Initiative, which draws funding from European countries, NGOs and private funders. These and other efforts have helped to boost the number of vaccine candidates from 0 to 12 since 2000. McShane and her team are on the cusp of the first efficacy results for MVA85A, one of the most clinically advanced TB vaccines in the pipeline at present. The shot, which McShane helped to develop as a PhD student 15 years ago, contains a virus designed to ramp up the activity of T cells that have already been primed by BCG. In 2009, in partnership with the South African Tuberculosis Vaccine Initiative, McShane launched a major phase II clinical trial on nearly 3,000 BCG-vaccinated babies in South Africa; early results are expected in the first quarter of 2013. In parallel, she and her colleagues are also testing the vaccine's efficacy in HIV-infected adults in South Africa and Senegal. But are these efforts enough? \u201cUnfortunately not,\u201d concludes Karin Weyer, coordinator of laboratories, diagnostics and drug resistance at the WHO Stop TB Department in Geneva. Annual funding for TB diagnosis and treatment is expected to reach some US$4.8 billion in 2013 \u2014 but TB care and control are expected to demand up to $8 billion a year by 2015. The $600 million contributed to TB research in 2010 also falls well short of the $2 billion the WHO estimates will be needed annually \u2014 and the economic crisis has slowed financing across the board. \u201cI need to be and want to be optimistic,\u201d says Weyer. \u201cBut we're still working with shoestring budgets compared to HIV.\u201d Meanwhile, the bacterium is not resting. In December last year, clinicians in Mumbai, India, reported 3  the identification of 12 patients with what they termed totally drug-resistant TB, or TDR-TB. Similar claims had been made a few years earlier in Italy and Iran, but this time the WHO took it seriously enough to investigate. In March 2012, 40 experts convened by the WHO concluded that there was not enough evidence to say that TDR-TB was substantially different from XDR-TB. McHugh agrees. But he does not need further evidence to act. In the face of marching drug resistance, it is the responsibility of researchers to speak out, he says. \u201cI think we can no longer be scientists in our labs doing fascinating stuff and think we're doing good work. We have to evangelize a little bit too.\u201d \n                     India moves to tackle antibiotic resistance 2012-Sep-11 \n                   \n                     The TB test you can do at home 2012-Sep-02 \n                   \n                     Resistance to backup tuberculosis drugs increases 2012-Aug-30 \n                   \n                     TB drugs chalk up rare win 2012-Jul-24 \n                   \n                     Totally drug-resistant TB emerges in India 2012-Jan-13 \n                   \n                     The challenge of new drug discovery for tuberculosis 2011-Jan-26 \n                   \n                     Focus on Vaccines \n                   \n                     Special on TB \n                   \n                     WHO Global Tuberculosis report 2012 \n                   \n                     WHO Tuberculosis \n                   \n                     Stop TB strategy \n                   Reprints and Permissions"},
{"file_id": "493154a", "url": "https://www.nature.com/articles/493154a", "year": 2013, "authors": [{"name": "Nicola Jones"}], "parsed_as_year": "2006_or_before", "body": "Catastrophes from the past will strike again \u2014 we just do not know when. One hundred thousand years ago, a massive chunk of the Mauna Loa volcano cracked away from Hawaii and slid into the sea, launching a wave that rose as high as the Eiffel tower up the slopes of a nearby island. That mega-tsunami was not an isolated incident: the past 40,000 years have seen at least ten gigantic landslides of more than 100 cubic kilometres in the North Atlantic ocean alone, each capable of producing waves tens to hundreds of metres high. Another is bound to happen sometime \u2014 although whether it will strike tomorrow or 10,000 years from now is anyone's guess. This week, the World Economic Forum published its 2013 global risks report, which includes a section, produced in collaboration with  Nature , on X factors: low-probability, high-impact risks resulting mainly from human activity (see  go.nature.com/outhzr ). But the natural world holds unpredictable threats as well. The geologic record is peppered with evidence of rare, monstrous disasters, ranging from asteroid impacts to supervolcanoes to \u03b3-ray bursts.  Nature  looks into some of the life-shattering events that Earth and the broader Universe could throw our way. \n               Death by volcano \n             Earth is now in the middle of a flare-up of supervolcanic activity 1 . Over the past 13.5 million years, no fewer than 19 giant eruptions have each spewed more than 1,000 cubic kilometres of rock \u2014 enough to coat an entire continent in a few centimetres of ash and push the planet into 'nuclear winter'. One of the most recent such eruptions, of Toba in Indonesia 74,000 years ago, was such a catastrophic event that some scientists have blamed it for starting the last ice age and slashing the human population to about 10,000 people. One estimate 1  suggests that there is a 1% chance of a super-eruption in the next 460\u20137,200 years. Nicola Jones runs us through the various ways our planet could try to extinguish us. The four youngest, most active supervolcanic systems in the world are Toba, Campi Flegrei in Italy, Yellowstone in the northwestern United States and Taupo in New Zealand. All four systems are being monitored for groundswell and seismic swarms \u2014 clusters of small earthquakes that can signal moving magma \u2014 and all occasionally show these warning signs. But no one knows whether the result of each flare-up will be a small squirt of steam or \u2014 much more hazardous \u2014 a mega-eruption of lava. \u201cIf something were brewing, we would get warning hours, days and months ahead,\u201d says Shan de Silva, a volcanologist at Oregon State University in Corvallis. \u201cBut how big it's going to be, we don't have a handle on.\u201d To help answer these questions, scientists are now drilling into the heart of one of the top contenders for the next blow-up: the Campi Flegrei caldera, a crater that is 13 kilometres wide and includes the city of Naples. Since 1969, the ground at Campi Flegrei has bulged upwards by as much as 3.5 metres, and researchers are eager to find out whether the culprit is underground steam or a pool of magma. Previous bouts of volcanic activity in the caldera came after the ground surface had swelled up by several metres or more 2 , and researchers think that major activity could occur within the next few decades or centuries. To investigate the risk, scientists at Campi Flegrei plan to drill more than 3 kilometres into the crater, despite concerns from some researchers that the drilling could trigger earthquakes or an explosion. One goal is to look at the magma pool beneath the crater: the shallower and more molten it is, the greater the chances of a super-eruption. Characterizing such pools through seismic studies is hard, and the range of error is huge. \u201cWe really are groping in the dark,\u201d says de Silva. Scientists estimate that 10\u201330% of the magma under Yellowstone, for example, is liquid \u2014 shy of the 50% thought to be needed for super-eruption. But pockets of molten magma in the chamber could still cause eruptions several-fold larger than the 1980 blast from Mount St Helens in Washington state, warns Jacob Lowenstern, head of the Yellowstone Volcano Observatory for the US Geological Survey in Menlo Park, California. The effort to drill into Campi Flegrei and measure features such as temperature and rock permeability should help researchers to interpret seismic-imaging studies of magma pools, says Lowenstern. \u201cIf we want to be able to successfully image Earth, we occasionally need to make a few strategic incisions into the patient,\u201d he says. As for the dangers of drilling, Lowenstern is convinced that the project will have minimal impact. \u201cIt's like a pinprick on an elephant,\u201d he says. The Campi Flegrei team finished an initial 500-metre test well in December 2012 without incident. And seismologists safely drilled a hole of similar size into the Long Valley caldera in California \u2014 a supervolcano site that erupted 760,000 years ago and holds the same killer potential as Yellowstone. Until more is learned about these systems, societies must accept that the threat of a super-eruption is real, yet remote. Lowenstern says that although the chances of one happening this year are tiny, \u201cit is theoretically possible\u201d. \n               Death by fungus \n             Although viruses and bacteria grab more attention, fungi are the planet's biggest killers. Of all the pathogens being tracked, fungi have caused more than 70% of the recorded global and regional extinctions 3 , and now threaten amphibians, bats and bees. The Irish potato famine in the 1840s showed just how devastating such pathogens can be.  Phytophthora infestans  (an organism similar to, and often grouped with, fungi) wiped out as much as three-quarters of the potato crop in Ireland and led to the death of one million people. Potato blight is still a threat: 13_A2, a highly aggressive strain of  P. infestans , is now rampant in Europe and North Africa. Across the globe,  Phytophthora  causes some US$6.7 billion in annual damages, according to a 2009 estimate 4 . Sarah Gurr, a plant pathologist at the University of Oxford, UK, estimates that the worst theoretical potato infestation would deprive 1.3 billion people of food each year. Other major staple crops face similar threats, such as rice blast ( Magnaporthe oryzae ), corn smut ( Ustilago maydis ), soya bean rust ( Phakopsora pachyrhizi ) and wheat stem rust ( Puccinia graminis ). The stem-rust superstrain Ug99 has in recent years slashed yields in parts of Africa by as much as 80%. If all five crop staples were hit with fungal outbreaks at the same time, more than 60% of the world's population could go hungry, says Gurr. \u201cThat's apocalyptic\u201d, but unlikely, she says \u2014 \u201cmore of a James Bond movie\u201d. David Hughes, a zoologist at Pennsylvania State University in University Park, adds that terrorists could use fungi to wreak havoc by targeting economically important crops. In the 1980s, for example, a possibly deliberate infection wiped out cacao crops in northern Brazil, changing the country's demographics and ecology as people moved from unproductive farms to the cities and cleared more rainforest. \u201cIf you wanted to destabilize the world, you could easily introduce rubber blight into southeast Asia,\u201d he says, which would trigger a chain reaction of economic and political effects. Modern agriculture has exacerbated societies' vulnerability by encouraging farmers to plant the same strains of high-yield crops, limiting the variety of resistance genes among the plants, says Gurr. \u201cWe've skewed the arms race in favour of the pathogen,\u201d she says. \u201cThat's why we're on the brink of disaster.\u201d Researchers estimate that there are 1.5 million to 5 million species of fungi in the world, but only 100,000 have been identified. Reports of new types of fungal infection in plants and animals have risen nearly tenfold since 1995 (ref.  3 ). Gurr suggests that climate change might be a culprit. Humans have cause for concern as well. In the past decade, a tropical fungus called  Cryptococcus gattii  has adapted to thrive in cooler climes and invaded the forests of North America's Pacific Northwest. By 2010, it had infected some 280 people, dozens of whom died. Although fungi are not spread as easily from person to person as viruses, for example, and anti-fungal agents can effectively tackle most infections, there are still reasons to worry. Fungi continue to evolve, and once they are established in an ecosystem, they can be almost impossible to wipe out. Given these trends, experts say that fungi have not received enough attention from researchers and governments. \u201cI'd be very surprised if an abrupt fungal infection killed a large swathe of people. But it's not impossible,\u201d says Matthew Fisher, an emerging-disease researcher at Imperial College London. \u201cComplacency is not a recommended course of action.\u201d \n               Death from above \n             The heavens hold plenty of threats. The Sun occasionally launches outsize solar flares, which fry electricity grids by generating intense currents in wires. The most recent solar megastorm, in 1859, sparked fires in telegraph offices; today, a similarly sized storm would knock out satellites and shut down power grids for months or longer. That could cause trillions of dollars in economic damage. A solar flare some 20 times larger than that may have hit Earth in 774, according to Adrian Melott, a cosmologist at the University of Kansas in Lawrence, and Brian Thomas, an astrophysicist at Washburn University in Topeka, Kansas. \u201cThat's not an extinction event,\u201d says Melott, \u201cbut for a technological civilization, it could kill hundreds of millions of people and set us back 150 years.\u201d Fortunately, there are ways to mitigate this worst-case scenario should it occur: engineers can protect the grid with fail-safes or by turning off the power in the face of an incoming blast. Next up the scale of disaster magnitude is a large comet or asteroid strike. Sixty-five million years ago, an asteroid 10 kilometres wide hit Earth and triggered the end-Cretaceous mass extinction; 2-kilometre rocks, thought to be capable of causing extinctions on a smaller scale, smack the planet once or twice every million years. Astronomers are hard at work tallying and tracking asteroids in Earth's vicinity, and scientists are investigating ways to divert any real threats that might materialize. A far rarer danger \u2014 and one that could not be avoided \u2014 is the blast of radiation from a nearby \u03b3-ray burst. Perhaps the most frightening of these celestial explosions is the 'short-hard' \u03b3-ray burst, caused by the violent merger of two black holes, two neutron stars or a combination. If one such blast were directed at Earth from within 200 parsecs away (less than 1% of the distance across the Milky Way), it would zap the globe with enough high-energy photons to wipe out 30% of the atmosphere's protective ozone layer for nearly a decade 5 . That sort of event \u2014 expected once every 300 million years or so \u2014 would double the amount of ultraviolet (UV) light reaching the ground and scorch phytoplankton, which make up the base of the ocean's food web. Astronomers have no way of knowing whether such a rare event is imminent. Neutron stars are small and dark, so there is no catalogue of those within striking distance. \u201cWe wouldn't see it coming,\u201d says Thomas. In as-yet-unpublished work, he estimates that such an event could cause a 60% increase in UV damage to crops, with up to 60% reduction in crop yields. From a distance of about 2,000 parsecs, 'long-soft' \u03b3-ray bursts \u2014 which result from the collapse of massive stars \u2014 could also cause extinctions. But these events are rarer than short-hard bursts, and easier to spot in advance because they come from larger, brighter stars. The two-star system WR 104 is some 2,500 parsecs away from Earth, and is far enough along in its life cycle that it is expected to explode some time in the next few hundred thousand years \u2014 although the beam from the burst is unlikely to hit Earth. It is possible that a \u03b3-ray blast has hit the planet before. Melott, Thomas and their colleagues have suggested that the mass extinction at the end of the Ordovician period, 440 million years ago, could have been triggered by a \u03b3-ray blast that wiped out some species through UV exposure and killed off others by creating a sunlight-blocking haze of nitrogen dioxide 6 . This would explain why some species went extinct before the globe cooled during that period, and it fits the extinction pattern, which shows that among marine organisms, the greatest toll was on plankton and other life in the upper part of the ocean. Thomas says that none of these potential disasters is keeping him up at night. He does, however, \u201chave some canned food in the basement\u201d \u2014 a prudent backup in the event of any disaster. \n               Death by water \n             Eight thousand years ago, sediments covering an underwater area the size of Scotland slipped from their moorings off the west coast of Norway and raced along the sea floor. The Storegga slide triggered a tsunami that ran at least 20 metres up the nearby Shetland Islands, and probably wiped out some coastal tribes as it clobbered shores around northern Europe. The scar it left on the ocean floor stretches nearly 300 kilometres. \u201cIt's absolutely enormous, and I'm not using the word 'enormous' lightly,\u201d says Peter Talling, a sedimentologist at the University of Southampton, UK, who is leading a project to assess the country's risk of similar slides. The United Kingdom is not the only country concerned about giant submarine landslides. \u201cThere are definitely areas that have potential,\u201d says Uri ten Brink, a geophysicist at the US Geological Survey in Woods Hole, Massachusetts, who conducted a 2008 study of possible sources of tsunamis on the US east coast, where some nuclear power plants are within striking distance of such waves. \u201cThere are far larger piles of sediment around today than Storegga ever was,\u201d ten Brink says, including deposits along the coast of southern Alaska and off the Amazon, Niger and Nile river deltas. Smaller slides are more probable and can still have a huge local impact \u2014 and they often strike without warning. In 1998, a relatively small (magnitude-7) earthquake triggered an underwater slide that launched a 15-metre-high tsunami into Papua New Guinea, killing 2,200 people. Researchers say that it is hard to quantify the threat of marine slides, particularly the giant ones. \u201cThere is so little information about events that happen so rarely,\u201d says ten Brink. \u201cWe just have to learn as much as we can.\u201d \n                 See Editorial \n                 page 134 \n               \n                     Tipping points: From patterns to predictions 2013-Jan-09 \n                   \n                     Planetary disasters: It could happen one night 2013-Jan-08 \n                   \n                     Realities of risk 2013-Jan-08 \n                   \n                     Emerging fungal threats to animal, plant and ecosystem health 2012-Apr-11 \n                   \n                     Science in Africa: The wheat stalker 2011-Jun-29 \n                   \n                     Questions loom over US nuclear safety 2011-Mar-22 \n                   \n                     Volcano borehole prompts safety doubts 2010-Sep-20 \n                   \n                     'Death Star' found pointing at Earth 2008-Mar-06 \n                   \n                     \n                         Global Risks \n                       \n                   \n                     Supervolcanoes \n                   \n                     Kansas University astrobiophysics project \n                   \n                     Giant Hawaiian underwater landslides \n                   Reprints and Permissions"},
{"file_id": "inequality-quantified-mind-the-gender-gap-1.12550", "url": "https://www.nature.com/news/inequality-quantified-mind-the-gender-gap-1.12550", "year": 2013, "authors": [], "parsed_as_year": "2011_2015", "body": "As an aspiring engineer in the early 1970s, Lynne Kiorpes was easy to spot in her undergraduate classes. Among a sea of men, she and a handful of other women made easy targets for a particular professor at Northeastern University in Boston, Massachusetts. On the first day of class, \u201che looked around and said 'I see women in the classroom. I don't believe women have any business in engineering, and I'm going to personally see to it that you all fail'.\u201d He wasn't bluffing. All but one of the women in the class ultimately left engineering; Kiorpes went on to major in psychology. Such blatant sexism is almost unthinkable today, says Kiorpes, now a neuroscientist at New York University. But Kiorpes, who runs several mentoring programmes for female students and postdoctoral fellows, says that subtle bias persists at most universities. And it drives some women out of science careers. By almost any metric, women have made great gains in closing the scientific gender gap, but female scientists around the world continue to face major challenges. According to the US National Science Foundation, women earn about half the doctorates in science and engineering in the United States but comprise only 21% of full science professors and 5% of full engineering professors. And on average, they earn just 82% of what male scientists make in the United States \u2014 even less in Europe. Scientific leaders say that they continue to struggle with ways to level the playing field and entice more women to enter and stay in science. \u201cWe are not drawing from our entire intellectual capital,\u201d says Hannah Valantine, dean of leadership and diversity at the Stanford School of Medicine in California. \u201cWe've got to put on the accelerator to evoke social change.\u201d One of the most persistent problems is that a disproportionate fraction of qualified women drop out of science careers in the very early stages (see  'Women in science' ). A 2006 survey of chemistry doctoral students by the Royal Society of Chemistry in London, for example, found that more than 70% of first-year female students said that they planned a career in research; by their third year, only 37% had that goal, compared with 59% of males 1 . Many experts say that a big factor driving this trend is the lack of role models in the upper divisions of academia, which have been slow to change. The Royal Society of Chemistry has found, for instance, that female chemistry students are more likely than males to express low self-confidence and to report dissatisfaction with mentorship 2 . Female students \u201cconclude consciously and unconsciously that these careers are not for them because they don't see people like them\u201d, suggests Valantine. \u201cThat effect is very, very powerful \u2014 this sense of not belonging.\u201d The attrition continues at later stages. In biology, for example, women comprised 36% of assistant professors and only 27% of tenure candidates in a 2010 study by the US National Research Council 3 . \u201cWe're not talking about a lack of talent here. Part of the story is that women leave earlier. In a sense, they give up on an academic career,\u201d says  Curt Rice , vice-president of research and development at the University of Troms\u00f8 in Norway, who has studied gender equality in US and European universities. Many of the UK chemistry students viewed research as an all-consuming endeavour that was incompatible with raising a family. Meeting the demanding schedule of academic research can seem daunting for both mothers and fathers. But family choices seem to weigh more heavily on the career goals of women. Law professor Mary Ann Mason at the University of California, Berkeley, and her colleagues have found 4  that male and female postdocs without children are equally likely to decide against research careers, each leaving at a rate of about 20%. But female postdocs who become parents or plan to have children abandon research careers up to twice as often as men in similar circumstances. \u201cThe plan to have children in the future, or already having them, is responsible for an enormous drop-off in the women who apply for tenure-track jobs,\u201d says Wendy Williams, a psychologist at Cornell University in Ithaca, New York. Furthermore, women who do become faculty members in astronomy, physics and biology tend to have fewer children than their male colleagues \u2014 1.2 versus 1.5, on average \u2014 and also have fewer children than they desire 5 . In response to these concerns, many universities have taken steps to establish family-friendly policies such as providing child-care assistance and extending tenure clocks for new parents. Shirley Tilghman, president of Princeton University in New Jersey, believes that such initiatives provide crucial support for women, but that other solutions are still needed. \u201cI don't think there's a single obstacle,\u201d she says. \u201cI think there's a whole series of phenomena that add up.\u201d At Yale University in New Haven, Connecticut, microbiologist Jo Handelsman is one of many researchers who think that gender discrimination continues to be a significant part of the problem. In a much-talked-about experiment last year 6 , her team showed that science faculty members of both sexes exhibit unconscious biases against women. Handelsman's group asked 127 professors of biology, chemistry and physics at 6 US universities to evaluate the CVs of two fictitious college students for a job as a laboratory manager. The professors said they would offer the student named Jennifer US$3,730 less per year than the one named John, even though the CVs were identical. The scientists also reported a greater willingness to mentor John than Jennifer. \u201cIf you extrapolate that to all the interactions that faculty have with students, it becomes very frightening,\u201d says Handelsman. Her findings match well with the results of a survey 7  done in 2010 by the American Association for the Advancement of Science. Of the 1,300 or so people who responded, 52% of women said that they had encountered gender bias during their careers, compared with just 2% of men. Still, other concrete evidence of bias is hard to find. Some measures show female scientists outperforming male rivals in landing interviews and job offers early in their careers. The National Research Council study 3  showed that women accounted for 19% of the interview pool and received 32% of job offers for tenure-track electrical-engineering positions. Women fared just as well as men in tenure evaluations, but female assistant professors in many disciplines seemed less likely to reach tenure consideration compared with men. Women face even more daunting odds in Spain. Men are 2.5 times more likely to rise to the rank of full professor than female colleagues with comparable age, experience and publication records 8 . Disparities can also be found in grant funding in some countries. In one frequently cited study 9 , Christine Wenner\u00e5s and Agnes Wold at the University of Gothenburg in Sweden found in 1997 that female applicants for postdoctoral fellowships had to score 2.5 times higher on an index of publication impact to be judged the same as men. Several groups, such as the UK Medical Research Council and biomedical research charity the Wellcome Trust, have since investigated their grant programmes and found negligible or very subtle effects of gender 10 . The Canadian Medical Research Council found no differences in success rate in most of its research grant programmes, but reported lower success rates for women in some training grants 11 . In the United States, women are slightly more successful than men in obtaining grants from the National Science Foundation, but the trend is reversed for the National Institutes of Health (NIH). The NIH also gives women smaller awards on average (see  'The funding gap' ). Information provided to  Nature  by the NIH through a Freedom of Information Act request indicates that the percentage of women on review panels has improved marginally over the past decade, from 25% in 2003 to 30% in 2012. Those figures roughly parallel the percentage of women applying for and receiving grants in that time. The inequalities also extend to salaries. In the European Union, female scientists earned on average between 25% and 40% less than male scientists in the public sector in 2006 (ref.  12 ). Although the average pay gap is smaller in the United States, the disparity is particularly large in physics and astronomy, where women earn 40% less than men. For young academic scientists, however, those differences may be fading. The National Research Council found an 8% pay gap at the level of full science and engineering professors but no significant differences among junior faculty members 3 . Some experts argue, however, that the salary gap may reflect other continued trends, such as the fact that a disproportionate share of women move into non-tenure positions or faculty jobs at lower-status universities. Tilghman says that Princeton and many other universities have grown increasingly conscious of the need to track and rectify gender gaps in salary and other institutional support. \u201cAbsolutely, it needs eternal vigilance,\u201d she says. \u201cBut we're in a much better place.\u201d"},
{"file_id": "495160a", "url": "https://www.nature.com/articles/495160a", "year": 2013, "authors": [{"name": "M. Mitchell Waldrop"}], "parsed_as_year": "2006_or_before", "body": "Massive open online courses are transforming higher education \u2014 and providing fodder for scientific research. When campus president Wallace Loh walked into Juan Uriagereka's office last August, he got right to the point. \u201cWe need courses for this thing \u2014 yesterday!\u201d Uriagereka, associate provost for faculty affairs at the University of Maryland in College Park, knew exactly what his boss meant. Campus administrators around the world had been buzzing for months about massive open online courses, or MOOCs: Internet-based teaching programmes designed to handle thousands of students simultaneously, in part using the tactics of social-networking websites. To supplement video lectures, much of the learning comes from online comments, questions and discussions. Participants even mark one another's tests. MOOCs had exploded into the academic consciousness in summer 2011, when a free artificial-intelligence course offered by Stanford University in California attracted 160,000 students from around the world \u2014 23,000 of whom finished it. Now, Coursera in Mountain View, California \u2014 one of the three researcher-led start-up companies actively developing MOOCs \u2014 was inviting the University of Maryland to submit up to five courses for broadcast on its software platform. Loh wanted in. \u201cHe was very clear,\u201d says Uriagereka. \u201cWe needed to be a part of this.\u201d Similar conversations have been taking place at major universities around the world, as dozens \u2014 74, at the last count \u2014 rush to sign up. Science, engineering and technology courses have been in the vanguard of the movement, but offerings in management, humanities and the arts are growing in popularity (see 'MOOCs rising'). \u201cIn 25 years of observing higher education, I've never seen anything move this fast,\u201d says Mitchell Stevens, a sociologist at Stanford and one of the leaders of an ongoing, campus-wide discussion series known as Education's Digital Future. The ferment is attributable in part to MOOCs hitting at exactly the right time. Bricks-and-mortar campuses are unlikely to keep up with the demand for advanced education: according to one widely quoted calculation, the world would have to construct more than four new 30,000-student universities per week to accommodate the children who will reach enrolment age by 2025 (see  go.nature.com/mjuzhu ), let alone the millions of adults looking for further education or career training. Colleges and universities are also under tremendous financial pressure, especially in the United States, where rocketing tuition fees and ever-expanding student debt have resulted in a backlash from politicians, parents and students demanding to know what their money is going towards. When MOOCs came along, says Chris Dede, who studies educational technologies at Harvard University in Cambridge, Massachusetts, they promised to solve these problems by radically expanding the reach of existing campuses while streamlining the workload for educators \u2014 and universities seized on them as the next big thing. There is reason to hope that this is a positive development, says Roy Pea, who heads a Stanford centre that studies how people use technology. MOOCs, which have incorporated decades of research on how students learn best, could free faculty members from the drudgery of repetitive introductory lectures. What's more, they can record online students' every mouse click, an ability that promises to transform education research by generating data that could improve teaching in the future. \u201cWe can have microanalytics on every paper, every test, right down to what media each student prefers,\u201d says Pea. MOOC companies still face challenges, such as dealing with low course-completion rates and proving that they can make profit. And they have a lot of convincing to do among faculty members, says Uriagereka. \u201cSome salivate and can't wait to be a part of it,\u201d he says, noting that his university had 20 volunteers for its 5 inaugural MOOCs. \u201cOthers say, 'Wait a minute. How do we preserve quality? How do we connect with students?'\u201d \n               Large-scale pedagogy \n             MOOCs are largely a product of one corridor in the Stanford computer-science department, where the offices of Andrew Ng, Daphne Koller and Sebastian Thrun are just a few steps apart. But they are also the fruit of research dating back to at least the 1990s, when the explosive worldwide growth of the Internet inspired a multitude of efforts to exploit it for education. Campus administrators tended to regard such projects as a sideshow \u2014 the higher-education financial crunch was not quite as serious back then \u2014 so most experiments were the work of committed individuals, departments or research centres. But with the relentless advance of technologies such as broadband, social networking and smart phones, researchers' interest continued to grow. Ng got involved in 2007 because he wanted to bring Stanford-quality teaching to \u201cthe people who would never be able to come to Stanford\u201d, he says. Following a path blazed by the open-source software movement, and by earlier open-source education initiatives, he started a project to post online free lecture videos and handouts for ten of Stanford's most popular engineering courses. His approach was fairly crude, he admits: just record the lectures, put them online and hope for the best. But to his astonishment, strangers started coming up to him and saying, \u201cAre you Professor Ng? I've been taking machine learning with you!\u201d He began to grasp how far online courses could reach, and started working on a scaled-up version of his system. \u201cWhen one professor can teach 50,000 people,\u201d he says, \u201cit alters the economics of education.\u201d One of the many people he talked to about his work was Koller, who began developing her own online-education system in 2009. Whereas Ng looked outwards, Koller wanted to look inwards and reform Stanford's teaching on-campus. She particularly wanted to promote 'flipping', a decade-old innovation in which students listen to lectures at home and do their 'homework' in class with their teachers, focusing on the most difficult aspects or discussing a concept's wider implications. This lets the instructors concentrate on the parts of teaching most of them enjoy \u2014 interacting with the students \u2014 and relieves them of the repetitive lecturing that they often dislike. Koller also wanted to incorporate insights from the many studies showing that passively listening to a lecture is a terrible way to learn (F. I. M. Craik and R. S. Lockhart  J. Verb. Learn. Verb. Behav.   11 , 671\u2013684; 1972 ). Following an approach pioneered by other online developers over the previous decade, Koller broke each video into 8\u201310-minute segments separated by pauses in which students have to answer questions or solve a problem. The idea was to get them to think about what they had learned; the deeper their engagement, studies showed, the better their retention. Finally, to encourage greater interaction among the students themselves, Koller took a cue from social-networking sites such as Facebook and gave her system an online discussion forum. As Ng explains, the idea was to extend what happens in a face-to-face study group: \u201cStudents sit with their best friends, they work on problems together, they critique each others' solutions \u2014 lots of pedagogical studies show that these more interactive modes of student engagement result in better student learning.\u201d Koller and Ng eventually realized that they could achieve both their goals \u2014 outreach and on-campus reform \u2014 by pooling their efforts. In late 2010, they started work on a software platform that would support discussion forums, video feeds and all the other basic services of an online course, so that an instructor only had to provide the content. But making social interaction work on a large scale turned out to be a research project of its own, says Ng. For example, standard online discussion forums are a fine way to bring communities together \u2014 for 100 or so users. \u201cWith 100,000 it gets more complicated,\u201d he says. Hundreds of students might end up asking the same question. So the developers implemented a real-time search algorithm that would display related questions and potential answers before a student could finish typing. Ng and Koller also let students vote items up or down, much like on the link-sharing website Reddit, so that the most insightful questions would rise to the top rather than being lost in the chatter. The two researchers even set the system up so that students could mark one another's homework for essay questions, which computers can't yet handle. Not only is such a system essential to scaling up learning, says Koller, but it also turns out to be a valuable learning experience. And experiments have shown that if the criteria are spelled out clearly, grades given by the students correlate strongly with those given by the teacher ( R. Robinson  Am. Biol. Teach.   63,  474\u2013480; 2001 ). By early 2011, Ng and Koller were planning to demonstrate the platform on campus, and other faculty members were paying attention. Among them was Thrun, a robotics researcher who was splitting his time between Stanford and Google in Mountain View, where he worked on the development of driverless cars. It was Thrun's idea to go big, using a platform of his own based in part on Ng and Koller's ideas. He says that he was scheduled to teach an artificial-intelligence course that autumn, along with Peter Norvig, Google's director of research, \u201cand I thought it was a social responsibility to take it online, so we could reach more than the 200 students we would get at Stanford\u201d. But even he hadn't imagined how big it would get. This was the course that registered 160,000 people from 195 countries after just one public announcement, a post to an artificial-intelligence mailing list. \u201cIt shocked everybody,\u201d he says. In response, Ng took his machine-learning course public using the platform he and Koller had developed, while department chair Jennifer Widom did the same with a database course. Each attracted roughly 100,000 students. With those numbers, venture-capital funding quickly followed. Thrun announced his company Udacity in January 2012. Arguing that most professors don't have a clue about how to exploit the online medium, he and his colleagues elected to develop their courses in-house, working with education experts to make the pedagogy as effective as possible. Ng and Koller announced Coursera in April 2012, and took the opposite tack. They partnered with big-name universities \u2014 Stanford and three others, to start \u2014 and let them provide the content while Coursera provided the hosting and software platform. Anant Agarwal, former head of the computer science and artificial-intelligence laboratory at MIT, had been experimenting with online learning for a decade, developing an electric-circuit simulation package called WebSim that tried to give online students an effective substitute for hands-on laboratory experience. In December 2011, inspired by goings on at Stanford, he launched MITx: an independent, not-for-profit company that would offer massive online courses from MIT on an open-source basis. It became edX in May 2012, when Harvard joined. At the same time, the term MOOCs, which had been circulating quietly in educational circles since it was coined in 2008, took off. Media accounts boomed, and company principals were soon giving talks at the popular Technology, Entertainment and Design (TED) conferences and the annual meeting of the World Economic Forum in Davos, Switzerland. As Koller told one interviewer: \u201cI can't believe my life!\u201d \n               Learning curve \n             The MOOC companies can point to plenty of success stories. For example, the 7,200 students who completed Agarwal's electric-circuits MOOC in spring 2012 included an 81-year-old man, a single mother with two children, and a 15-year-old prodigy from Mongolia who got a perfect score on the final exam. Udacity's Introduction to Computer Science MOOC, currently its most popular, has enrolled more than 270,000 students. But MOOCs have also had some teething problems. \u201cMany people have no idea what they're in for when they commit to put a course online,\u201d says John Mitchell, a computer scientist and Stanford's first vice-provost of online learning. \u201cRestructuring even one lecture into short, self-contained segments takes a fair amount of thinking.\u201d So does coming up with good, compelling questions to engage the students between the segments. Then there is the push for high-quality production, he says. \u201cIt takes many hours to produce one hour of quality video.\u201d More worrisome are the MOOCs' dismal completion rates, which rarely rise above 15%. Completion has been a problem for distance learning ever since the first correspondence courses in the nineteenth century, says Dede. Only a small fraction of students have the drive and the perseverance to learn on their own, he says, and most people need help: \u201csocial support from their fellow students to help them keep going, and intellectual support from their professors and fellow students to help them figure out the material\u201d. At the moment, says Dede, the MOOC companies' peer-to-peer communication tools don't do nearly enough to provide that kind of help. \u201cThey're just kind of hoping that people will figure out from the bottom up how to support each other,\u201d he says. The companies acknowledge that completion rates are a concern and that their platforms are still works in progress. \u201cMy aspiration isn't to reach the 1% of the world that is self-motivating,\u201d says Thrun, \u201cit's to reach the other 99%.\u201d The companies are already working on enhanced social tools such as live video and text chat, for example. And to observers such as David Krakauer, that is as it should be. \u201cThere are two ways to make something new,\u201d says Krakauer, a biologist who directs the Institute for Discovery at the University of Wisconsin\u2013Madison. \u201cYou can design something that's perfect on paper, and then try to build it. Or you can start with a system that's rubbish, experiment and build a better one with feedback. That's the Silicon Valley style \u2014 but it's also the scientific way.\u201d \n               Silicon valley style \n             A Silicon Valley sensibility permeates the three big MOOC firms. For example, they all subscribe to the open-source ideal. \u201cCharging for content would be a tragedy,\u201d says Ng. But they also see plenty of opportunities to make money using the 'freemium' model followed by Google and many other technology companies: give away the basic product to draw users, and then charge for premium add-ons. One obvious add-on might be certification, says Ng. \u201cYou would get a certificate that verifies you took the course for a small fee like US$10\u2013$30\u201d \u2014 a potentially substantial revenue stream when enrolments are in six figures. In the future, the companies might also offer full university course credits for a fee; they are already working with accreditation agencies to arrange that. Other possibilities include profiting from in-course mentoring services, career counselling \u2014 and charging universities for licensing. In October 2012, for example, edX licensed a circuit-theory MOOC designed by Agarwal to San Jose State University in California, where it was used as the online component of a flipped classroom experience. In return for the licensing fee, \u201cthe professors can offer the course on campus, tweak the course however they please, get access to students' grades and online activity, and all the analytics a teacher would want to see\u201d, says Agarwal. In this particular experiment, he adds, the San Jose course's usual 40% failure rate fell to 9%. Analytics are another example of the Silicon Valley style, potentially allowing the MOOC companies to do for education what Internet giants such as Google or Amazon have done for marketing. In Coursera's case, says Koller, the platform monitors the students' every mouse click \u2014 \u201cquiz submissions, forum posts, when and where a student pauses a lecture video, or rewinds, or moves to 1.5 speed\u201d. The company is constantly using these data as feedback, says Koller, both for refining the platform's user interface and for improving the course content. If 90% of the students start stumbling over the review exercises for a certain lecture, for example, then maybe it is time to revise that lecture. \u201cBut anything we do is just the tip of the iceberg,\u201d says Koller. When data from individual students are multiplied by tens or hundreds of thousands of students per course, they reach a scale big enough to launch a whole new field of learning informatics \u2014 \u201cbig-data science for education\u201d, Pea calls it. Learning informatics could provide an unprecedented level of feedback for colleges and universities, says Stevens: \u201cWe haven't measured learning in higher education very often, very consistently or very well \u2014 ever.\u201d Academics have endlessly studied factors that are associated with university enrolment and success, such as race, parental income and school achievement. They have also studied what happens after graduation: the higher earnings and other benefits that college confers, on average, over a lifetime. \u201cWhat we don't know is how college performs this magic,\u201d says Stevens. \u201cWe certainly don't know the extent to which digitally mediated college experiences will deliver the same returns as a four-year residential experience.\u201d Now, however, he and his colleagues can begin to see what education science will look like as it merges with data analytics. Instead of looking at aggregate data about students on average, for example, researchers can finally \u2014 with appropriate permissions and privacy safeguards \u2014 follow individual students throughout their university careers, measuring exactly how specific experiences and interactions affect their learning. \u201cIt's thrilling,\u201d he says, \u201ca huge intellectual frontier.\u201d What remains to be seen is how higher education will change in response to the new technology. Maybe not much, says Dede. Yes, the major universities will extend their courses beyond their own campuses; the MOOCs have already shown them that they can do so with relatively little effort and potentially large profits. But the MOOC founders' other goal \u2014 fundamental reform in on-campus teaching \u2014 is a much tougher proposition. \u201cUniversities think of themselves as being in the university business, not the learning business,\u201d explains Dede. That is, they mostly take their existing structures and practices as given, and look to MOOCs and other online technologies as a way to do things more cheaply. But experience with earlier innovations such as personal computing shows the limits of that approach, he says: real gains in the productivity and effectiveness of learning will not come until universities radically reshape those structures and practices to take full advantage of the technology. No one knows exactly where that restructuring might end up. Lectures becoming a rarity, for example? Vast numbers of students getting their degrees entirely online? But the revolution has already begun, says Stevens. Major universities such as Stanford are taking the lead, \u201ctrying to integrate and embed digital learning into the fabric of the entire university\u201d \u2014 and trying to master the new technology before it masters them. Virtually everyone participating in this upheaval agrees on one thing. Colleges and universities will change \u2014 perhaps dramatically \u2014 but they will not disappear. \u201cNo one says that all education has to be online,\u201d says Thrun. \u201cSometimes, a classroom is better.\u201d Especially in communal endeavours such as science, \u201ceducation is more than just knowledge\u201d, says Dede. \u201cIt's abilities like leadership and collaboration, and traits like tenacity\u201d, all of which are best learned face to face. An unspoken irony weaves through almost every discussion about MOOCs: thanks to innovations such as flipping, online technology's most profound effect on education may be to make human interaction more important than ever. As Krakauer puts it, \u201cwhat's absolutely clear is that the very large lecture hall can be completely replaced: there's no value added over watching it at home on an iPad screen with a cup of tea. But there is also no substitute for a conversation.\u201d \n                     Developing world: Educating India 2011-Apr-06 \n                   \n                     Clicking on a new chapter 2009-Apr-01 \n                   \n                     The learning revolution 2009-Jan-07 \n                   \n                     Science education: Spare me the lecture 2003-Sep-18 \n                   \n                     Coursera \n                   \n                     Udacity \n                   \n                     edX \n                   \n                     Education's Digital Future \n                   \n                     Daphne Koller at TED \n                   Reprints and Permissions"},
{"file_id": "493150a", "url": "https://www.nature.com/articles/493150a", "year": 2013, "authors": [{"name": "Ewen Callaway"}], "parsed_as_year": "2006_or_before", "body": "Brian Butterworth is on a crusade to understand the number deficit called dyscalculia \u2014 and to help those who have it. In the mid-1980s, Paul Moorcraft, then a war correspondent, journeyed with a film crew into Afghanistan to produce a documentary about the fifth anniversary of the Soviet invasion. The trip took them behind Soviet lines. \u201cWe were attacked every fucking day by the Russians,\u201d says the colourful Welshman. But the real trouble started later, when Moorcraft tried to tally his expenses, such as horses and local garb for his crew. Even with a calculator, the simple sums took him ten times longer than they should have. \u201cIt was an absolute nightmare. I spent days and days and days.\u201d When he finally sent the bill to an accountant, he had not realized that after adding a zero he was claiming millions of pounds for a trip that had cost a couple of hundred thousand. \u201cHe knew I was an honest guy and assumed that it was just a typo.\u201d Such mistakes were part of a lifelong pattern for Moorcraft, now director of the Centre for Foreign Policy Analysis in London and the author of more than a dozen books. He hasn't changed his phone number or PIN in years for fear that he would never remember new ones, and when working for Britain's Ministry of Defence he put subordinates in charge of remembering safe codes. In 2003, a mistaken phone number \u2014 one of hundreds before it \u2014 lost him a girlfriend who was convinced he was out gallivanting. That finally convinced him to seek an explanation. Ewen Callaway meets the scientist hoping to treat dyscalculia with mild electrical brain stimulation. At the suggestion of a friend who teaches children with learning disabilities, Moorcraft contacted Brian Butterworth, a cognitive neuroscientist at University College London who studies numerical cognition. After conducting some tests, Butterworth concluded that Moorcraft was \u201ca disaster at arithmetic\u201d and diagnosed him with dyscalculia, a little-known learning disability sometimes called number blindness and likened to dyslexia for maths. Researchers estimate that as much as 7% of the population has dyscalculia, which is marked by severe difficulties in dealing with numbers despite otherwise normal (or, in Moorcraft's case, probably well above normal) intelligence. That combination has attracted neuroscientists such as Butterworth, who believe that the disorder illuminates the inner workings of the brain's number sense \u2014 the ability to understand and manipulate quantities. This sense is every bit as innate as vision or hearing, yet scientists disagree over its cognitive and neural basis, a debate that dyscalculics may help to settle. For Butterworth, scientific curiosity eventually gave way to advocacy. \u201cI thought, it's not enough to just try to identify the cause,\u201d he says. In the past decade, he has crusaded to get dyscalculia recognized \u2014 by parents, teachers, politicians and anyone who will listen. And he is using his scientific insights into the condition to help dyscalculic children. \u201cWhat's the point of telling someone they have dyscalculia if you can't help them?\u201d he says. \n               Finding the number \n             Christopher, a chatty nine-year-old in a rumpled blue sweatshirt and white polo shirt, sits beside Patricia Babtie, a teacher who specializes in dyscalculia and tutors children across Greater London. On a sturdy-looking laptop, Christopher (not his real name) is navigating  Number Sense , a suite of educational computer games designed by Butterworth and his colleague Diana Laurillard at the Institute of Education in London. By developing treatments for dyscalculia, Butterworth hoped to test competing theories about the cognitive basis of numeracy. If, as he believes, dyscalculia is at heart a deficiency of basic number sense and not of memory, attention or language, as others have proposed, then nurturing the roots of number sense should help dyscalculics such as Christopher. \u201cIt may be the case that what these kids need is just much more practice than the rest of us,\u201d Butterworth says. Christopher's school is one of several in London working with the software, and students in Cuba, Singapore and elsewhere will also soon start using it. Christopher starts with a game involving a number line \u2014 a spatial representation that scientists believe is key to number sense. \u201cWhat is the number that is right in the middle between 200 and 800? Do you know it?\u201d Babtie asks. Christopher shrugs. \u201cThink of any number that is bigger than 200 and smaller than 800 and put it in this box. It could be 201,\u201d she says. He enters 200, and Babtie reminds him that the number needs to be greater than 200. He selects 210, probably mistaking it for 201. A classic sign of dyscalculia is difficulty in grasping the place-value system, according to Babtie. \u201cThat will do fine,\u201d she says. A soft computer voice tells Christopher to \u201cfind the number and click it\u201d. The game involves zooming in and zooming out to rescale the number line, and Christopher talks through each move \u2014 a strategy that Babtie encourages \u2014 but it takes him more than a minute to locate 210. His classmates, meanwhile, are learning to multiply two-digit numbers. Some children at Christopher's school have more profound numeracy problems. One nine-year-old classmate says that she doesn't know if 50 is greater or less than 100; another the same age confuses four dots for five and routinely tots up small sums on his fingers, a common strategy for dyscalculics. \u201cOK, time to stop. We'll do some more of this another day,\u201d Babtie says to Christopher, after 20 frustrating minutes. It is clear he would rather be back in class than here in this room practising a skill his classmates learned years ago. \n               How many are there? \n             Butterworth, now 69, straddles the academic and public spheres. A fellow of the British Academy, the United Kingdom's national body for the humanities and social sciences, he made his name probing obscure speech and language disorders and has appeared in the British media for many years. In a  Sunday Times  article in 1984, for example, Butterworth claimed that the speech patterns of former US president Ronald Reagan indicated Alzheimer's disease. Reagan was diagnosed with the condition a decade later. In the late 1980s Butterworth studied a stroke patient who would change the course of his professional life. The woman, a 59-year-old former hotel manager from Italy, fared about average on verbal IQ tests and had a good memory, but when Butterworth's Italian colleagues asked her to count, she would start, \u201c uno, due, tre, quattro ,\u201d and then stall. \u201c Miei matematica finisce alle quattro \u201d \u2014 my mathematics stops at four \u2014 the woman, known as CG, would tell them 1 . Neurologists had presented case studies of 'acalculic' patients such as CG from the early twentieth century onwards, if not before, but \u201cpeople hadn't thought a lot about the specific brain areas involved in calculation\u201d, says Butterworth. Brain scans of CG revealed a lesion in the parietal lobe, a part of the brain just above the ears. Later, Butterworth found another patient with the opposite pattern of disability: neurodegeneration had robbed him of speech, language and much of his knowledge, save for the ability to do intricate calculations. Butterworth grew more certain that numerical abilities relied on specialized brain networks, and not only on those supporting general intelligence, as many scientists believed at the time. Genetics and the vagaries of brain development disrupted these networks in dyscalculics, Butterworth proposed. And Moorcraft was one of Butterworth's most revelatory subjects because of the great disparity in his abilities in different domains. Butterworth and his colleagues also tested 31 eight- and nine-year-old children who were near the bottom of their class in mathematics but did well enough in other subjects. Compared with normal children and those with dyslexia, the dyscalculic children struggled on almost every numerical task, yet were average on tests of reading comprehension, memory and IQ 1 . The study confirmed for Butterworth that developmental forms of dyscalculia are the result of basic problems in comprehending numbers and not in other cognitive faculties. But determining exactly what those problems are would prove challenging. Like nearly all human cognitive abilities, number sense is evolutionarily ancient \u2014 tens if not hundreds of millions of years old. Studies of chimpanzees, monkeys, newborn chicks, salamanders and even honeybees point to two parallel systems for representing quantities. One, called the approximate number sense, distinguishes larger quantities from smaller ones, be they dots flashing on a screen or fruits in a tree. Studies on monkeys reveal that certain neurons in a specific fold of the parietal lobe fire more vigorously in response to increasingly higher numbers 2 . A second ancient number system allows humans and many other animals to instantly and precisely recognize small quantities, up to four. Primate studies show that individual neurons within the same fold, called the intraparietal sulcus, seem tuned to particular quantities, such that when a monkey is performing a task that involves numbers, one neuron will fire for the number 1, a different one will fire for 2 and so on 3 . People who are poor at distinguishing approximate quantities do badly in maths, suggesting that the approximate-number system is crucial 4 . And some work shows that dyscalculics are poor at recognizing small numbers, suggesting that this ability is also fundamental to numeracy 5 . Moreover, scans of people with dyscalculia suggest that their intraparietal sulci are less active when processing numbers 6  and less connected with the rest of the brain 7  compared with numerate children and adults. Yet Butterworth views such results as consequences, not causes, of the poor numerical abilities that characterize dyscalculia. He argues that another cognitive capacity is even more fundamental to number sense. He calls this 'numerosity coding': the understanding that things have a precise quantity associated with them, and that adding or taking things away alters that quantity. But Stanislas Dehaene, a cognitive neuroscientist who studies numerical cognition at INSERM, France's national institute for research on medicine and health, near Paris, sees number sense as being supported by a broader set of cognitive features. Approximation and a sense of small numbers, while critical, are not enough for humans to precisely grasp large numbers, he says. Language, he argues, empowers humans to integrate the two number systems \u2014 giving them the ability to intuitively distinguish, say, 11,437 from 11,436. Butterworth's concept of numerosity coding may be an important part of number sense, says Dehaene, but there is still much to learn about it \u2014 for instance, whether it is present in other animals or in children from a very early age. One of Butterworth's favourite papers is titled 'Six does not just mean a lot: preschoolers see number words as specific' 8 . In it, the developmental psychologist Barbara Sarnecka, now at the University of California, Irvine, and Susan Gelman, at the University of Michigan in Ann Arbor, showed that young children who could not yet count past two nonetheless understood that adding pennies to a bowl containing six somehow altered its number, even if the children couldn't say exactly how. If numerosity coding is fundamental, it predicts that dyscalculics such as Moorcraft or Christopher struggle to enumerate and manipulate all numbers, large and small. Butterworth hopes that, by honing this ability, the Number Sense games will help support his research ideas. Three months on, Christopher seems to be faring better at the number-line game, going so quickly that Babtie asks him to slow down and explain his reasoning for each move. Babtie says that dyscalculic children tend to learn much more quickly when they talk through what they do. She also believes that Christopher's maths anxiety, a near-universal trait of child and adult dyscalculics, is fading. He moves on to a Tetris-like game called Numberbonds, in which bars of different lengths fall down the screen, and he is asked to select a block of the correct size to fill out a row. This emphasizes spatial relationships, which some dyscalculics also struggle with. The blocks move too quickly at first, frustrating Christopher, but he soon gets the hang of it, and when Babtie suggests he stop for the day, he begs for ten more minutes. The Number Sense games, including a snazzy-looking iPhone version of Numberbonds, are intended to nurture the abilities that, Butterworth contends, are the root of numerical cognition and the core deficit of dyscalculia \u2014 manipulating precise quantities. In a game called Dots to Track, for example, children must ascribe an Arabic numeral to a pattern of dots, similar to those on dice. When they enter the wrong value \u2014 and they often do \u2014 the game asks the children to add or remove dots to achieve the correct answer. As the summer holidays approach, Babtie is worried that Christopher and the other students she has been working with won't practise the games at home, returning in the autumn the worse for it. But in early October when school is back, Christopher announces that he will challenge himself with a number line that stretches from 950 to 9,000, \u201cif you'll allow me\u201d, he adds. At first he flounders, but quickly starts to understand the game and locates a string of four-digit numbers, beaming with each correct response. Other students are improving more slowly, but it is not easy to say why. Dyslexia, attention deficit hyperactivity disorder and autism spectrum disorder are common among dyscalculics, and it can be difficult to untangle these problems, says Babtie. The nine-year-old who counted on his fingers nine months ago can now deal with numbers below 6, but still struggles to distinguish 9 from 10. Yet with the right practice and attention from teachers and parents, dyscalculic children can thrive, says Babtie, who emphasizes that computer games are a supplement, not a replacement, for one-on-one tutoring. Butterworth knows that it will take a controlled evaluation of Number Sense before he can say if the game genuinely improves numeracy in dyscalculic children. Small studies of other computer-based interventions hint that they might help. Dehaene reported in 2009 that Number Race, a game his group developed, modestly improved the ability of 15 dyscalculic kindergarten children to discern the larger of two numbers, but that it had no effect on their arithmetic or counting 9 . Meanwhile, a Swiss team reported in 2011 that a game that involves placing a spaceship on a number line helped eight- to ten-year-old dyscalculics with arithmetic. The researchers also studied the children in an fMRI scanner during a task that involved arranging numbers. They found that one month after training, the children showed increased activation in the intraparietal sulcus and reduced neural activation elsewhere in the parietal lobes \u2014 a hint that their improvements in arithmetic were related to changes involving brain areas that respond to number 10 . Butterworth hopes to monitor the brains of students such as Christopher as they practise Number Sense, to see if their parietal lobes are indeed changing. But he has been turned down by every funding source he has applied to. Although dyscalculia, like other learning disabilities, takes a toll on productivity (one report estimated that low numeracy costs the United Kingdom \u00a32.4 billion (US$4 billion) per year, mostly in lost wages) it doesn't attract much attention or money. In the United States, for example, the National Institutes of Health spent $2 million studying dyscalculia between 2000 and 2011, compared with more than $107 million on dyslexia. Butterworth's team now has tentative plans to evaluate its software with researchers at the Cuban Neurosciences Center and the University of Pedagogical Sciences in Havana next year, and the group is also placing the game in other countries, including China and Singapore. \u201cThe Cubans, curiously, are putting money into this, even though they've got very little,\u201d Butterworth says, commending the strength of the country's education system. Although an emeritus professor, technically retired, Butterworth continues to research the neurodevelopmental roots of number sense, recently showing that guppies 11 , like humans, possess approximate and precise number systems, and that dyscalculic adults have no more trouble telling the time than numerate people 12 . He hopes that Number Sense \u2014 if it can improve dyscalculia \u2014 will help him in the academic debate over the cognitive basis of numeracy. But Dehaene, probably his most fervent opponent in that debate, isn't counting on classroom computer games to resolve it. His Number Race game and its successor,  Number Catcher , incorporate a multitude of numerical skills, so even if the game works, it won't address the theoretical differences about which skills are most essential to number sense or most compromised in dyscalculics. \u201cI quickly realized that the interest of the children was to have a fun game full of ideas and variety, and that was not very compatible with an analytic approach,\u201d he says. Butterworth, too, says that he is ultimately more motivated by helping children. In the course of his studies, he was struck that children \u201cwere very, very distressed by being bad at maths. So every day they would go to school, every day there's a maths class, every day they're shown up to be incompetent in a way other kids in their class are not\u201d, he says. Moorcraft can commiserate. When he occasionally meets dyscalculic children, he tells them that he, too, counts with his fingers under the table, that they have nothing to be embarrassed about and that, with the practice that he never got, they can get up to speed. Moorcraft is also completing a book on dyscalculia with one of Butterworth's postdocs. \u201cI have written an introduction,\u201d he says. \u201cI just hope the chapters are in the right order.\u201d \n                     Brain's 'reading centres' are culturally universal 2012-Nov-26 \n                   \n                     Brain connectivity predicts reading skills 2012-Oct-08 \n                   \n                     Treating schizophrenia: Game on 2012-Feb-29 \n                   \n                     Dyslexic diversity 2008-Apr-07 \n                   \n                     Number Sense \n                   \n                     The Number Catcher \n                   Reprints and Permissions"}
]