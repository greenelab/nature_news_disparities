[
{"file_id": "462676a", "url": "https://www.nature.com/articles/462676a", "year": 2009, "authors": [], "parsed_as_year": "2006_or_before", "body": "In 1873, German physicist Ernst Abbe proposed that diffraction fundamentally limited the resolution that any microscope could achieve to around half the wavelength of light. And, despite many advances, microscopes didn't threaten to challenge this law of physics for more than a century. Stefan Hell, now a director of the Max Planck Institute for Biophysical Chemistry in G\u00f6ttingen, Germany, was the first to show that the diffraction limit could be beaten. Hell, while a postdoc at the University of Turku in Finland in the 1990s, thought that, with the right lasers, he could activate a fluorescent spot and then shrink it by superimposing a larger, hollow beam of light to deplete all the light emission except for that at the centre of the spot. He called the technique stimulated emission depletion (STED) microscopy. Although many physicists were initially sceptical of Hell's ideas, by 2000 he had used STED to produce the first nanoscale fluorescence images 10 . Super-resolution microscopy has blossomed since, allowing researchers to see cellular processes unfolding at nanometre scales. \u201cThis is something that the field has desired since people first started looking through light microscopes,\u201d says Jan Liphardt, a biophysicist at the Lawrence Berkeley National Laboratory in California. Since Hell's work, the field has been boosted by other groups, including those of Eric Betzig, a physicist at the Howard Hughes Medical Institute's Janelia Farm Research Campus in Virginia and Jennifer Lippincott-Schwartz, a cell biologist at the National Institutes of Health in Bethesda, Maryland. In 2006, the groups reported that they had increased resolution by harnessing single-molecule photoactivatable fluorescent proteins and compiling images of thousands to millions of them 11 . They called the approach photo-activated localization microscopy (PALM). At Harvard University in Cambridge, Massachusetts, physicist Xiaowei Zhuang has developed three-dimensional (3D) stochastic optical reconstruction microscopy (STORM), which uses photoswitchable probes to temporally separate the overlapping images of individual molecules and so boost resolution to ten times better than the diffraction limit. Yet another approach \u2014 fluorescence PALM (FPALM) \u2014 was developed in 2006 by Samuel Hess, a physicist at the University of Maine in Orono. His group's technique involves looking at thousands of fluorophores at once, and localizing on small numbers at a time. These methods have already begun to demostrate their utility. For example, in 2007, Hess's group showed that FPALM could be used to detect proteins clustering in lipid rafts in living cells 12 . In 2008, Lippincott-Schwartz's group combined PALM with single-particle tracking to detect the movement of membrane proteins in live cells 13 . And Zhuang's group used 3D STORM to image microtubules and other molecular structures within monkey kidney cells 14 , later extending the method to multicolour 3D imaging of whole cells 15 . Hell's group, early in 2008, used the STED method to show the movement of synaptic vesicles inside living neurons at video rate 16 . But the field is just warming up. \u201cTo people like me who were trained in physics or optics in the 1990s, it's just unbelievable that one can image below the resolution of light,\u201d says Bernardo Sabatini, a neurobiologist at Harvard Medical School in Boston, Massachusetts. \u201cThe major revolution for the next 5 or 10 years is getting these advances to answer biological questions. \n               K.R.C. \n             Reprints and Permissions"},
{"file_id": "457618a", "url": "https://www.nature.com/articles/457618a", "year": 2009, "authors": [], "parsed_as_year": "2006_or_before", "body": "In recent years, glycan arrays have been used to identify the cell-surface sugars bound to by pathogens such as the flu virus. In parallel, companies such as CombiMatrix in Mukilteo, Washington, have developed specialized diagnostic instruments to identify pathogens, including tools to differentiate flu strains. Now researchers at the MITRE Corporation in Bedford, Massachusetts, and the University of California, San Diego (UCSD), have developed a method that could blur the line between these tow types of tool. The device was described in the December issue of  IEEE Sensors Journal  by MITRE researchers Grace Hwang and Elaine Mullen and UCSD researchers Lin Pang and Y. Shaya Fainman 4 . It features an array developed at the UCSD with a gold surface that is perforated with nanometre-wide holes. A glycoprotein is attached to the gold surface inside the hole and the pathogen or carbohydrate-binding lectin is added. The instrument detects binding events through surface plasmon resonance (SPR), measuring fluctuations in electron density at the boundary between the metal and a dielectric surface. Hwang and Mullen use microfluidic delivery channels to place the glycoproteins on the gold surface. When the proteins attach to the gold there is a detectable change in plasmon resonance. Once this reaches equilibrium, the pathogen or lectin is introduced using the delivery channel and any binding that takes place further changes the plasmon resonance. The device is reusable as acids can be used to break the glycan\u2013lectin bonds and clean the array. \u201cThe reason we wanted to use a plasmonic device was because plasmons are very sensitive to perturbations at the metal\u2013dielectric interface,\u201d says Hwang. For studying pathogens such as the flu virus, sensitivity can be an issue. The binding of the virus to different oligosaccharides occurs in the low millimolar range says Ian Wilson, a structural biologist at the Scripps Research Institute in La Jolla, California. As a result, he notes, glycan arrays often need to amplify the fluorescence signal, which requires additional antibodies.  Hwang and Mullen's system avoids this problem, as plasmon detection does not require fluorescence to measure binding interactions \u2014 potentially opening the instrument up to a wide range of sensitive interactions. At the moment, however, the researchers are still working to improve the device's sensitivity for detecting flu viruses \u2014 their calculations suggest that it should be possible to identify up to one million influenza particles per millilitre. Unlike other glycan arrays, the SPR system doesn't need printing or linkers to attach sugar targets to the gold surface. \u201cThe disulphide bonds in the glycoproteins will typically break and then bind to gold spontaneously,\u201d says Mullen. The is helpful because when the sulphide bonds form with the gold surface, the oligosaccharides of the glycoproteins are oriented properly with their bioactive sugars projecting towards the medium. Nevertheless, using glycoproteins in this way means that Hwang and Mullen have to choose carefully and be confident in the glycosylation patterns of the glycoproteins they use as their target. To help them, Mullen and her colleagues built a database called SugarBindDB ( http://sugarbinddb.mitre.org ). \u201cWe know which glycoproteins to choose by looking at our own database of pathogens and their specific sugar sequences,\u201d says Mullen. \u201cThen we go to the GlycoSuite, a database of oligosaccharides, to determine which glycoprotein it was attached to when it was isolated and what organisms it came from.\u201d Hwang acknowledges that it is challenging to identify potential glycoproteins that present only the sugars required for selective pathogen sensing. If the glycoprotein displays a mixture of sugars, then it could bind to non-pathogens. It is even more difficult to identify potential pathogen targets displayed on glycoproteins in human tissue, but she and Mullen think this is a challenge not just for their device, but for glycobiologists in general. \u201cI realized from discussions with other researchers that predictive tools to compute binding affinities between sugars and lectins do not exist today,\u201d Hwang says, noting that this is a gap in glycan research tools that does not exist for nucleic acids and proteins. But she thinks in time, as more biophysical information is gained about glycan structures and properties, glycan arrays will catch up. \n               N.B. \n             Reprints and Permissions"},
{"file_id": "457617a", "url": "https://www.nature.com/articles/457617a", "year": 2009, "authors": [{"name": "Nathan Blow"}], "parsed_as_year": "2006_or_before", "body": "Carbohydrates are important in many biological processes, but the full extent of their distribution and function remains unclear. Advances in technology are now reveal those secrets. Nathan Blow reports.  Sugars occur in a variety of forms and locations throughout the human body. From those that are attached to proteins during glycosylation to the carbohydrates that decorate the surfaces of cells lining the lungs and digestive tract, the range of possible sugar conformations and glycoforms is tremendous. As a result, analysing carbohydrates is a tricky business for anyone interested in glycobiology. The term glycobiology was coined in 1988 by biochemist Raymond Dwek at the University of Oxford, UK. Dwek used the phrase simply to emphasize the importance of relating sugars back to basic biology rather than just isolating and examining them outside of their biological context. Instead, he named a field that is thriving in its own right. Today glycobiology is intertwined with fields such as immunology, virology, reproductive biology and drug discovery. \u201cMore people are starting to realize that sugars are not just there for protecting surfaces from proteolysis, but they have some functional role to play,\u201d says Ian Wilson, a structural biologist at the Scripps Research Institute in La Jolla, California.  But even as more researchers accept the importance of sugars in basic biology, many glycobiologists worry that the barrier to entry into their field remains too high, potentially delaying or hampering discovery and innovation. \u201cThe technical difficulty is so great now that many scientists are turned away,\u201d says Peter Seeberger, a chemist at the Swiss Federal Institute of Technology in Zurich. The solution, he adds, is to \u201clower the hurdle by providing access to technology more easily\u201d. To those ends, Seeberger is trying to develop user-friendly automated solutions for complicated procedures such as the synthesis of complex carbohydrates. Seeberger is not alone. Pauline Rudd, a professor of glycobiology at University College Dublin in Ireland and a principal investigator at the National Institute for Bioprocessing Research and Training, spent the better part of ten years refining chromatography approaches for glycosylation analysis. Now she and her colleagues are taking their approach to the next level, using high-performance liquid chromatography (HPLC) as the basis for a high-throughput pipeline for analysing glycans. Glycoproteins featuring N-linked glycans are first immobilized either in gels or on membranes, and the glycans are then released using an enzyme that cleaves the sugars from the proteins. The system examines the patterns of the glycans on the proteins by attaching fluorescent labels to the sugars, which Rudd says offers highly sensitive results during chromatography. \n               Sweet analysis \n             The labelled sugars are run on a normal phase HPLC column and the resulting peaks are correlated to a pre-run dextran ladder, thereby assigning a 'glucose unit value' to each of the peaks. \u201cWe have a database that is automatically interrogated to give us a list of sugars that could have these particular glucose units,\u201d says Rudd. Using this information, a series of exoglycosidase digestions is performed and those data are fed back into the computer program to assign final structures. The researchers recently installed an automated liquid-handling platform from Hamilton Robotics of Reno, Nevada, so that they could do their glycan analyses in 96-well plates. \u201cOne analysis will take about eight hours, so the aim is to get it done by the end of the shift,\u201d says Rudd. Speed is important, Rudd notes, because the drug industry increasingly wants to monitor the glycosylation patterns of proteins. \u201cWhen people want to achieve quality by design, they need to determine the optimal culture conditions and time for harvesting monoclonal antibodies,\u201d she says. \u201cTherefore, they need to understand how the glycosylation changes over the course of production.\u201d Rudd says that her pipeline can test samples every hour, over a number of days or at different pH conditions to find those optimal points. Elizabeth Higgins, chief executive and founder of GlycoSolutions in Worcester, Massachusetts, feels that a different factor is driving the drug industry's interest in glycan analysis. GlycoSolutions offers glycomics services and analyses, and last year worked on 20 different glycosylation analysis projects for various pharmaceutical companies. Higgins says that the analyses were largely done to meet regulatory requirements. \u201cMost companies we work with are driven by getting data for the Food and Drug Administration,\u201d she says. Another company working on high-throughput tools for analysing glycosylation patterns to aid drug development is Procognia, in Ashdod, Israel. Because many different glycoforms can exist, an extensive knowledge of glycosylation patterns and how they change during drug manufacturing is important for he development of biosimilar drugs, says Ilana Belzer, Procognia's vice-president of research and development. To tackle this issue, the firm has developed GlycoScope, a high-throughput workflow for glycosylation analysis platform that uses lectin (carbohydrate-binding protein) arrays and informatics tools to provide glycosylation fingerprints and glycan structures for glycoproteins. By providing values of molecular weight that can be used to deduce initial structures, mass spectrometry (MS) is yet another approach to glycan analysis. \u201cMS is very good at defining the sugar profiles of cell surfaces,\u201d says James Paulson, a glycobiologist at the Scripps Research Institute who studies glycan binding proteins that mediate cellular communication in the immune system. With additional isolation and fragmentation using either matrix-assisted laser desorption/ionization or electrospray ionization followed by tandem MS (MS/MS), researchers can deconvolute the configuration of sugars, a process very similar to protein sequencing. The past few years have seen the arrival of many commercial programs and algorithms that assign glycan structures based on MS spectra. PREMIER Biosoft based in Palo Alto, California, sells SimGlycan, which uses MS/MS data to query a database of more than 8,000 theoretical glycan fragmentation patterns to generate a list of probable structures. Developers at the Palo Alto Research Center have designed new software packages that identify and annotate glycopeptides from a combination of single and tandem MS data. MS analysis works well early on, says Higgins, but can be dangerous when it comes to working out the exact sugar structure because researchers often make assumptions based on mass alone. But the main challenge in using mass spectrometry for glycan analysis is figuring out the linkages between sugars. This is complex because the system needs consistently to fragment the sugars at the correct point to show one sugar is linked to a certain position on another sugar, says Paulson. He adds that such consistency remains an issue. Rudd says that the HPLC approach along with enzyme digests can identify specific sugar linkages. HPLC columns can resolve the sugars on the basis of their conformations, and each monosaccharide contributes a specific incremental value to the retention time of an oligosaccharide. The pools of released sugars are treated with enzyme arrays in which each enzyme is highly specific for a particular monosaccharide in a particular linkage. The sequence and the linkage between sugars can be determined simultaneously for all the sugars in the pool. To help researchers interested in using this approach, Rudd's group recently made available the database GlycoBase and the analytical tool AutoGU to aid in the assignment of provisional structures based on HPLC profiles 1 . \n               Glycan arrays and bird flu \n              The hunt for specific binding partners to various branched sugars or sugar-binding proteins called lectins requires a higher-throughput system. This can be achieved using glycan arrays. First described in 2002, these arrays feature different oligosaccharides or polysaccharides printed on slides or held in wells on a plate. \u201cI think that glycan arrays have been a spectacular success over the past few years,\u201d says Paulson. Initially, the arrays contained relatively small numbers of glycans and were designed mainly to study the specificity of antibodies and carbohydrate-binding proteins. But Wilson is one of a number of researchers who realized that some of these arrays would prove useful for diverse applications relevant to their own research. He uses glycan arrays for studying how the influenza virus binds to cells.  Some viruses, such as flu and HIV, attach themselves to host cells during the early stages of infection by binding to sugars on the cells' surface. Paulson, in fact, discovered in the 1980s that avian flu viruses recognize different sugar receptors from their human virus counterparts. For Wilson, glycan arrays offered a way to look in detail at the specificity of different flu strains for various sugars, especially the H5N1 strain of bird flu that emerged in 1997 as a worldwide health concern, as well as the strain that caused the human pandemic in 1918. \u201cWe have analysed 50 to 60 or maybe even more influenza haemagglutinin mutants on the array to look for how the 1918 and H5N1 influenza strains can convert from human-to-avian or avian-to-human receptor specificity,\u201d says Wilson. Work has gone far in explaining how mutations can change the sugars to which influenza strains bind, thereby interconverting the receptor-binding characteristics of avian strains and human strains. Wilson thinks that glycobiology is brought to the attention of a much wider audience when researchers use tools such as glycan arrays to work on well-known microorganisms such as the flu virus (see  'Surface sensing' ). \u201cThere are a lot of other uses for these arrays, but everyone understands flu and the risks of bird flu,\u201d he says. The Consortium for Functional Glycomics (CFG), an effort funded by the US National Institute of General Medical Sciences, aims to provide unique resources for glycobiology research. Headed by Paulson, the consortium has expanded the number of glycans available for arrays. \u201cThere are now 480 glycans in the consortium library,\u201d says Paulson, \u201cand they all have amino-terminal linkers that allow them to be printed on slides using standard robotics.\u201d The ease of generating and analysing these arrays is opening the field to ever more researchers who can now submit samples to the CFG for rapid analysis. Commercial developers also make high-content arrays with both glycans and carbohydrate-binding lectins attached to the surface. Robotic Labware Designs in Encinitas, California, offers printing services for glycan arrays as well as a series of preprinted glycan arrays. QIAGEN, headquartered in Hilden, Germany, provides the Qproteome GlycoArray kit for glycosylation analysis. This array and analysis software, developed by Procognia, contains a series of specific lectins that bind different monosaccharides, which allows researchers to determine the pattern and relative abundance of specific glycosylation epitopes in a glycoprotein. Although 480 glycans on one array might not seem impressive compared with DNA microarrays, which can contain over a million features, Paulson is unperturbed. For carbohydrate-binding proteins, which usually recognize and interact with the tips of glycans, 480 represents a reasonable approximation of the options. \u201cIf you only consider the tips, or the last six or seven sugars, then it is a very finite number of structures, in the order of 500,\u201d he says.  The problem for glycobiologists is how quickly carbohydrate diversity can grow when those 500 structures are attached to different branches on a single N-linked glycan. \u201cIf you allow any one of those structures to occur on any one of the four branches, you have this huge number of structures that could theoretically exist,\u201d says Paulson. And this is where glycan arrays run into a wall \u2014 researchers want this level of diversity on their arrays to help them understand how proteins and pathogens bind sugars, but generating such a diversity of glycans can be difficult. Although many groups still try to isolate sugars from natural sources to use in their research, most agree that improving synthesis methods and technology is essential to obtaining large quantities of diverse carbohydrates. \u201cI think that up to now, carbohydrate synthesis has been restricted to a relatively small group of experts who bring considerable technical knowledge to the table,\u201d says Seeberger. Even for experts, such synthesis can take a long time \u2014 weeks or even years when it comes to making complex carbohydrates or glycoconjugates. Seeberger and his group, along with a handful of other labs around the world, have been working to improve carbohydrate synthesis methods. Ultimately they hope to develop automated instruments that can synthesize carbohydrates much like DNA synthesizers currently produce nucleic acids. There are currently two main approaches to carbohydrate synthesis: solid-phase or one-pot synthesis. In 2001, Seeberger and his colleagues described an automated system that uses solid-phase synthesis for carbohydrates 2 . A programmable one-pot synthesis approach, meanwhile, has been advanced by Chi-Huey Wong, from the Scripps Research Institute and Academia Sinica in Taipei, Taiwan, and his colleagues. \n               Cooking up sugars \n             In solid-phase synthesis, sugar building blocks are attached to a surface or a bead, which can be moved during the synthesis process to allow other monosaccharides to be added. The one-pot approach uses a computer program to determine which monosaccharides to place in a flask; the next reagent is added and the mixture stirred. This process is repeated until an oligosaccharide is obtained. \u201cWhat you save is the different work-up steps that often take much more time to achieve than the actual synthesis,\u201d says Seeberger of the one-pot approach. He adds that in this sense, both approaches cut down on the purification and separation steps in carbohydrate synthesis. Seeberger sees the building blocks as a big issue for both approaches. Unlike DNA, which has four nucleotide bases, or the 20 amino acids that comprise peptides, there are 10 common monosaccharides in humans and many more in bacterial systems. Even more vexing when it comes to synthesis is the potential for branching of sugars. For example, glucose can link to another sugar at two points in its structure \u2014 a 1\u20136 linkage or a 1\u20134 linkage. This means that two different building blocks must be available for synthesis, which adds another level of complexity. The synthesis of monosaccharide building blocks was advanced recently when Shang-Cheng Hung in Taiwan and his colleagues reported a selective one-pot synthesis approach for the synthesis of highly functionalized, differentially protected monosaccharides 3 . Some commercial companies are producing monosaccharide building blocks for chemical syntheses. Dextra Laboratories in Reading, UK, offers monosaccharides as well as various glycoconjugates and more complex N-linked oligosaccharides. And other companies such as Omicron Biochemicals of South Bend, Indiana, and GLYCOTEAM in Hamburg, Germany, offer carbohydrate chemical synthesis services. Chemical synthesis is not the only route to obtaining synthetic carbohydrates \u2014 researchers can also take advantage of nature's methods. \u201cEnzymatic synthesis is one approach the CFG uses and that has enormously accelerated the rate at which you can synthesize complex natural sugars,\u201d says Paulson. But the approach is limited by the number of glycosyltransferase enzymes needed to synthesize all the carbohydrates researchers may be interested in. The number of glycosyltransferases needed for synthesis can be almost as daunting as the number of monosaccharide building blocks in chemical approaches. For example, GlycoGene, a company based in Ibaraki, Japan, offers enzymatic synthesis services to researchers through the use of more than 180 different glycosyltransferases. For this reason, the CFG has merged enzymology and chemistry in the production of many of the sugars on its glycan array.  Although he is keen to see automated chemical synthesis up and running, Paulson sees gaps when it comes to the carbohydrates that can be synthesized with existing methods. \u201cYou cannot make everything you want now, although you can make some carbohydrates quickly and easily,\u201d he says. \u201cThe gaps are the key things and these might be what people are really interested in looking at.\u201d Despite this, Seeberger still sees access to tools as the greatest challenge in glycobiology at the moment. \u201cWhen you think about genomics and proteomics, you can sequence and you can synthesize,\u201d he says. \u201cBut those two things are still not generally possible in glycobiology.\u201d The field of glycobiology is still finding its way 20 years after the word was first printed. Although advances in the analysis and synthesis of carbohydrates are leading to fresh insights, much remains to be discovered. But Dwek can sit back and take comfort in the knowledge that his word has blossomed into a field that continues to grow. \u201cI think the future of glycobiology is very exciting,\u201d he says. Reprints and Permissions"},
{"file_id": "457621a", "url": "https://www.nature.com/articles/457621a", "year": 2009, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "458240a", "url": "https://www.nature.com/articles/458240a", "year": 2009, "authors": [], "parsed_as_year": "2006_or_before", "body": "When the complete sequence of human chromosome 22 was first published in 1999 (ref. 4), John Rinn, an assistant professor at Beth Israel Deaconess Medical Center and an associate member of the Broad Institute in Cambridge, Massachusetts, got very excited. He was not interested in looking at the map of known protein-coding genes on the chromosome, but rather everything else. \u201cWe wanted to see if we could find biologically active molecules in the human genome that no one previously knew about,\u201d he says. Armed with the sequence of an entire chromosome \u2014 and a year later the whole human genome \u2014 researchers and developers began to create genome-wide tiling microarrays. \u201cBy probing these tiling arrays we found out that there are tonnes of biologically active regions by proxy of RNA being made,\u201d says Rinn \u2014 results he and his colleagues reported in 2003 (ref. 5). Since then, Rinn has focused his efforts on understanding a collection of these RNAs known as large intervening non-coding RNAs (lincRNAs). \u201cInitially many people thought that this had to be an artefact of the technology: how could there be so many RNA molecules that we have never seen before?\u201d says Rinn. Arguments against a true biological purpose for lincRNAs came largely from the lack of evolutionary conservation within their sequences \u2014 conservation implies function, whereas lack of conservation can often imply noise. As so few functional lincRNAs had been described, Rinn and his colleagues set out to find more. In 2007 they reported the identification of a new 2.2-kilobase large non-coding RNA, which they called HOTAIR. It played a role in the guiding of chromatin complexes within the cell 6 . Although only a single new functional lincRNA \u2014 and still only one of four known to be functional at the time \u2014 the discovery gave Rinn an idea on how to enrich for functional lincRNAs from the genome. \u201cWhat we did next was to go after things that looked like HOTAIR,\u201d he explains. Instead of using an RNA-based approach, the group decided to look at chromatin structure. Histones have clear indications of where active genes start and stop. Using high-throughput chromatin immunoprecipitation (ChIP) sequencing on the Illumina Genome Analyzer to look for these marks, Rinn and his colleagues at the Broad Institute developed genome-wide chromatin state maps. Then, just as with his analysis of chromosome 22 almost ten years ago, Rinn says he threw out the known protein-coding genes and looked at what was left. He identified 1,600 other RNAs located by themselves in the middle of nowhere in the genome that look just like HOTAIR 7 . To determine if some of their newly discovered RNAs were functional, the team took a 'guilt by association' approach, using microarrays to profile a number of the newly identified lincRNAs in 21 different tissue samples while at the same time profiling protein-coding genes in the same tissue samples. Then they asked the question: which RNAs had similar profiles to protein-coding genes of known function? Their initial analysis was followed by further validation using independent systems. \u201cThis has turbo-charged the field, as not only can we identify these things now but we can get a good idea of what they might be doing to test functional relationships,\u201d says Rinn. For Rinn and his colleagues it is now time to muster all the force they can to explore these RNAs. \u201cWe are going to throw the Broad kitchen sink at them,\u201d says Rinn, who is teaming up with a number of scientific platforms at the Broad Institute to look at the effects of knocking down each newly discovered lincRNA. \n               N.B. \n             Reprints and Permissions"},
{"file_id": "458925a", "url": "https://www.nature.com/articles/458925a", "year": 2009, "authors": [{"name": "Nathan Blow"}], "parsed_as_year": "2006_or_before", "body": "Advances in magnetic resonance imaging are helping scientists learn more about the structure and function of the brain. Nathan Blow looks at how far the technology has developed and where it could go.  A wave of enthusiasm seems to overcome Joy Hirsch, a neuroscientist at Columbia University in New York, when she talks about recent developments in functional neuroscience. \"I just love the new area of neuroeconomics,\" she says. Although not her primary field of study \u2014 her group focuses on how the brain uses cognitive control in decision-making processes \u2014 she says that understanding the neurophysiology behind financial decisions could make it possible to one day predict a person's financial habits by imaging the neural activity in their brain. And it is advances in cognitive research such as this that could eventually change the way we think about the brain\u2013mind interface, says Hirsch. The emergence of these areas in functional neuroscience can be traced in part to recent developments in brain imaging technology. \"Magnetic resonance imaging used to be only in the hands of radiologists, but today more and more cognitive neuroscientists are using it,\" says Alan Koretsky, scientific director of the National Institute of Neurological Disorders and Stroke at the National Institutes of Health (NIH) in Bethesda, Maryland. And by taking advantage of magnetic resonance imaging (MRI), sometimes in combination with other methods such as electroencephalography (EEG) or positron emission tomography (PET), neuroscientists are exploring the structure of the human brain in greater detail than ever before and identifying specific regions that are active when a person is making a decision or feeling an emotion such as fear or happiness. \n               Size is important \n             With its high sensitivity and non-invasive nature, MRI is at the core of neuroimaging today. To create detailed anatomical and functional images, MRI systems take advantage of the ability of very large cylindrical magnets, ranging in strength from 1.5 to 15 tesla, to align the protons found in water throughout the body. Smaller, localized radio-frequency electromagnetic fields are then generated to push those protons out of alignment. The displaced protons generate signals that are detected by the MRI instrument and translated into an image. Whether built for imaging humans or small animals, MRI instruments are defined by the field strengths of their magnets \u2014 and here bigger is better. The signal from magnetic resonance is inherently weak and it can be difficult to detect, which tends to limit resolution. So developers are constantly trying to increase the field strengths of their magnets and boost the signal. \"Small-animal imaging is based on making systems with magnets of smaller diameter but much higher field strength than you would get in clinical practice,\" says Roy Gordon, vice-president of imaging at Bruker Biospin, a scientific-instrumentation company in Billerica, Massachusetts. Most animal-imaging magnets range in field strength from 4.7 T to 15 T, with 7 T being the \"workhorse field strength in animal imaging\", according to Gordon. Human MRI instruments, on the other hand, use 0.5\u20133 T magnets for clinical applications and up to 9.4 T for research applications. Part of the challenge in moving human scanners up in field strength to match their small-animal counterparts comes from our body sizes. \"Maintaining field strength homogenously over a large volume becomes more and more challenging as the field strength increases,\" says Vibhas Deshpande, a research and development scientist at Siemens Medical Solutions in Malvern, Pennsylvania. For this reason, he says, the human scanners that are operating at ultra-high field strengths are mostly using smaller diameter magnets, similar to animal scanners, to image small human samples such as tissues rather than performing whole-body scans. \n               Image artefacts \n             Maintaining homogenous field strength is not the only issue that magnet builders face in their quest to boost the signal. \"Another challenge for all systems is that as you go up in field strength, you must address issues related to magnetic susceptibility,\" says Gordon. At higher field strengths, a magnetic field gradient can occur at the interface of materials with very different magnetic susceptibilities, such as tissue, bone and a void (in the sinuses). This can lead to artefacts in the images that must be accounted for during image analysis, says Gordon.  Currently, 3 T MRI systems are the standard for high-end human neuroimaging. But research-grade 7 T MRI instruments for human studies have come a long way in recent years with several companies, including Siemens Medical Solutions, GE Healthcare in Piscataway, New Jersey, and Philips in Andover, Massachusetts, now supplying second-generation versions of these systems. \"The first generation of 7 T systems were monsters that needed 400 tonnes of shielding, but the second-generation systems are actively shielded so now in principle you don't need any iron to shield the magnet,\" says Gordon. But many researchers say that, even with the advances, 7 T systems still need more engineering work. \n               The 'best' field strength? \n             \"The 7 T scanner is still a bit of a specialized device and, in my opinion, has yet to reach its full potential,\" says Larry Wald, director of the NMR core facility at the Athinoula A. Martinos Center for Biomedical Imaging in Charlestown, Massachusetts. Deshpande agrees: \"There will be some ramp-up time with the 7 T in terms of applications and research. We need to find out how far we can push the instrument.\" Whether 7 T will eventually become a clinically robust field strength for human MRI is not clear. \"Ever since the beginning of MRI there has always been discussion about what field one should work at,\" says Koretsky. First there was the decision between using 0.5 T and 1.5 T, and then between 3 T and 4 T magnets. Now, he says, a 7 T debate might start. \"Some of the images from the brain at 7 T are truly amazing,\" says Deshpande. \"We are seeing things that we have never seen at either 1.5 T or 3 T.\" Even as new 7 T human systems and 11.7 T and higher animal systems from companies such as Bruker Biospin and Varian in Palo Alto, California, expand in use within the neuroscience community, bigger magnets are being designed for cutting-edge animal and human imaging. In autumn 2008, the Martinos Center installed a 15 T magnet designed by Varian and Magnex Scientific in Walnut Creek, California, for imaging mice and rats, which Wald says is now at field strength and should be generating its first images in the coming weeks. The machine, used in conjunction with knockout mice with genes that have been 'turned off', should allow scientists to understand neural disease progression more effectively and even test potential drug therapies, says Wald. Field strengths for human MRI magnets may reach double digits in the coming years. Both the NIH and NeuroSpin, a centre for ultra-high-field MRI in Saclay, France, have announced projects to construct 11.7 T MRI magnets for imaging human subjects. Although Koretsky says that it will be several years before these new magnets are installed and operational, he thinks the images will be worth the wait. \"In the end you would love an anatomical picture that looks like a histological slice,\" he says, adding that these new magnets will get them closer than ever to making this a reality. High-strength magnets come with big technical challenges and hefty price tags. \"These are demanding instruments. We can manufacture 16 T magnets for small-animal imaging but they are exceptionally expensive,\" says Gordon. With a complete imaging system based on a 16 T magnet for small-animal imaging costing tens of millions of dollars and magnets for imaging humans often costing similar amounts, Gordon and others think that advances in radio-frequency-coil design could provide a more economical route to higher sensitivity.  \"It is easy to forget, but if you dust off the old birdcage detectors that we used to use for the human head and compare those to what is shipping on a modern scanner these days \u2014 the sensitivity difference is pretty astonishing,\" says Wald. His group at the Martinos Center has been developing the coils for many years, but it might be the group's contributions to the development of parallel imaging that have had the biggest impact for MRI. \n               Parallel evolution \n              Radio-frequency coils are used as both transmitters to oscillate protons and as detectors to receive the signal. Ten years ago, single-channel coils were used for this purpose. But today, most instruments have multi-channel coils, allowing the parallel acquisition of data during a scan. The benefits of parallel imaging can be seen in the 32-channel coil for head scans that Wald's group developed for brain imaging on Siemens' research-grade 7 T human MRI system. \"We have a protocol that used to run as two 8.5-minute scans, but now we can do that in a single 3.5-minute scan,\" he says. Faster scans mean less time in the instrument for subjects, which presents new opportunities for the researchers at the larger imaging centres who routinely scan thousands of people a year. \"There are now a lot of scientists interested in standardizing a fast morphometric protocol that would enable large-scale genotype/phenotype studies of neurological diseases,\" says Wald. Wald's group has also worked at getting a better signal from its scans at lower field strengths with the development of a 96-channel head coil for use on Siemens' clinical 3 T MAGNETOM system. Although using more channels should mean better signal detection, this is not always the case. \"In principle, if it is a 96-channel coil you can get a 96 factor acceleration in scan speed, but we do not come anywhere close to that,\" says Wald, a fact that is leading coil groups around the world to work on further enhancing the sensitivity of multi-channel coils. The developments are leading to advances in commercially available coils. With its total imaging matrix (TIM) technology for scanning the whole body, Siemens offers multiple coils that can connect at the same time, allowing the system to decide on which of the coils to use depending on the section of body being scanned. In this way larger sections can initially be scanned at low resolution, followed by more focused higher-resolution scans of sections of interest, all without having to change the coil setups or move the patient out of the scanner to reposition the coil array. Philips now offers up to 33-element coils for neuroimaging on its 3 T Achieva TX system and GE Healthcare offers a series of coils designed for its Signa Infinity 1.5 T system. \n               Divide and conquer \n             As magnet field strengths increase, coil developers will have to keep improving sensitivity. \"The detection side of it will continue to be hard, but we know what to do and how to do it now. The transmit side on the other hand is potentially most challenging,\" says Koretsky. For radio-frequency transmitting, the technical challenges increase with field strength because the human body starts to distort the radio frequency at high field strengths affecting the radio-frequency transmission. Some progress has been made on this front already, with researchers working on the idea of parallel transmission using multiple radio-frequency transmitters that are simultaneously excited. By using parallel transmission it is possible to eliminate the homogeneity problems caused by the shorter wavelengths generated at higher frequencies. \"Parallel transmit breaks it down into chunks, which can be put back together to get a uniform transmit in the end,\" says Wald. \"A sort of 'divide and conquer' approach helps to get around this.\"  But another recent development might further change how transmission is done. In February 2009, Klaas Pruessmann's group at the Institute for Biomedical Engineering in Zurich, Switzerland, described a new transmission and detection technique called travelling-wave nuclear magnetic resonance (D. O. Brunner   et al .  Nature   457 , 994\u2013998; 2009). In this approach, the radio-frequency wave travels down the bore of the magnet, which acts like a waveguide. Although this means that the bore has to be big enough compared with the wavelength for the wave to travel, many researchers are encouraged by the potential of this approach to solve some of the radio-frequency transmission issues. \"The most exciting thing with the travelling wave approach is that you not only get the apparatus for transmit out of the bore of the magnet, but it also offers the potential to have a more homogenous excitation,\" says Wald. Although the approach will only work for 7 T-and-higher strength systems, Wald says that this is where this type of technology is most needed, given the transmission problem and the decreasing bore sizes that come with larger magnets. \n               Adding function to the form \n              Signal in MRI is much like currency, says Deshpande. By boosting the signal with stronger magnets and multi-channel coils, researchers can \"spread their currency around to get shorter scan times and higher resolution when doing functional imaging\". Functional MRI (fMRI) is a technique that looks for changes in the levels of deoxyhaemoglobin \u2014 used as an internal contrast agent (see 'Changing the colour of MRI',  page 926 ) \u2014 as an indicator of blood flow in the brain due to neural activity. By observing neural activity in subjects performing different tasks, fMRI is helping cognitive neuroscientists gain a better handle on how the brain works. \"It is that intimate relationship between structure and function through the melding of behaviour science and experimental paradigms along with the ability to image the brain while doing these experiments that grows the field,\" says Hirsch. Although the field continues to grow through technology development and improving experimental design, mainstream applications of fMRI remain limited. Neurosurgeons are using the technique to map patients' brains before surgery, but at the moment this is the only application of fMRI for which physicians can be reimbursed through insurance companies in the United States. But this is not stopping researchers from moving forwards with new fMRI studies. Omneuron, a life-sciences company in Menlo Park, California, is currently recruiting volunteers for a clinical trial on the neural basis of chronic pain. Using fMRI, researchers at Omneuron are mapping the brain activities of participants performing specific mental tasks. Other companies, such as the Applied fMRI Institute in San Diego, California, with its 3 T Siemens Tim MAGNETOM Trio scanner, provide both fMRI and anatomical MRI services, and a host of companies, including Compumedics Neuroscan in Charlotte, North Carolina, Visage Imaging in Andover, Massachusetts, and Prism Clinical Imaging in Wauwatosa, Wisconsin, are providing software to integrate the functional information of fMRI with other modalities including EEG and PET. The biggest changes in fMRI might come as the technology becomes more personalized. \"One of the signs of brain imaging advancement is how well can we apply the benefits of neuroimaging to personalized patient care,\" Hirsch says. But even more sensitivity and accuracy will probably be required for this, because fMRI studies often rely on multiple subjects. By improving understanding of why we make certain decisions and how our brains differ in shape, MRI is changing the way we view the brain. And many researchers think more insights are on the way as the technology continues to advance. \"MRI is going to move from a tool for identification to one that helps us understand mechanisms,\" says Koretsky. \"From being able to say there is a tumour there to understanding how that tumour formed and grew.\" Reprints and Permissions"},
{"file_id": "458239a", "url": "https://www.nature.com/articles/458239a", "year": 2009, "authors": [{"name": "Nathan Blow"}], "parsed_as_year": "2006_or_before", "body": "Next-generation sequencing is pushing gene-expression profiling further into the digital age. But analog methods still have plenty of wind left. Nathan Blow looks at the looming battle over the cell's transcriptome. Could it be only a matter of time before all gene-expression analysis goes digital? The answer is less straightforward than you might think. When it comes to studying a cell's transcriptome \u2014 the collection of all RNA transcripts produced at a specific time \u2014 next-generation sequencing promises insights into the way genes are expressed and regulated in cells. Compared with analog methods such as microarrays and real-time PCR, the technology is still in its infancy. But with availability increasing and costs declining, more researchers are turning a curious eye to sequencing's high-throughput digital readouts. The future of analog techniques \u2014 and their possible obsolescence \u2014 was one of the questions tackled in a conference panel session held before a packed audience at last year's conference on issues in parallel sequencing at Harvard University Medical School in Boston, Massachusetts. The panel explored the changed landscape of gene-expression analysis since the arrival of next-generation sequencing, and feelings on the subject in both panel and audience were mixed. \u201cSequencing allows you to ask many different types of questions and look at transcription from new angles,\u201d says Shirley Liu, an associate professor at the Harvard School of Public Health and Dana-Farber/Harvard Cancer Center in Boston, who organized the conference and led the panel. But she thinks sequencing has a long way to go before it reaches the level of adoption that microarrays have among researchers. \u201cMy feeling is that if people have never done this kind of work and are trying things for the first time or are asking traditional questions, they are better off with arrays now,\u201d she says. \n               By the numbers \n             \u201cThese days researchers are stepping into a new world when looking at sequencing,\u201d says Shawn Baker, a senior product manager for RNA analysis at Illumina in San Diego, California. Although the 'big splash' for most new next-generation sequencing systems comes from deciphering eukaryotic genomes at a breakneck pace, Baker says that digital gene expression and RNA profiling are increasingly popular. He suggests that the company's Genome Analyzer is used for this type of work 30\u201340% of the time \u2014 although he adds that when the major genome centres are subtracted from the user mix that percentage is likely to be much higher. Four companies now provide next-generation sequencing platforms that support, or plan to support, digital gene-expression applications, but that number is widely predicted to rise in the coming years as developers work on 'next-next-generation' single-molecule sequencing systems. For now, Applied Biosystems (part of Life Technologies) in Foster City, California, recently released a whole transcriptome-expression kit along with a small RNA expression kit for use on its SOLiD 3 platform. Illumina has developed a digital gene-expression kit for RNA analysis and profiling for use on its Genome Analyzer; and 454 Life Sciences, a Roche company located in Branford, Connecticut, uses a serial analysis of gene expression (SAGE) tag-based approach for expression profiling on its GS FLX system. This level of development and interest among researchers should come as no surprise: gene-expression analysis is an application that seems ideally suited to next-generation sequencing. \u201cFor digital gene expression you are just counting the number of times you hit a gene and then assuming that that represents the number of copies of the transcript that you have in your population,\u201d says Chad Nusbaum, co-director of the genome sequencing and analysis programme at the Broad Institute and Harvard University, both in Cambridge, Massachusetts. So the more you can count \u2014 and next-generation systems can count a lot \u2014 the better the measure of copy number for even those rare transcripts in a population. \u201cPeople are sequencing anything from 10 million to 40 million reads in a run,\u201d says Baker, \u201cand they are getting a phenomenal level of data.\u201d Last year, short-read high-throughput sequencing, or 'RNA-seq', was used to obtain global views of gene expression in human embryonic kidney and B cells 1 , to profile transcription in mouse embryonic stem cells 2  and to quantify whole mouse-cell transcriptomes 3 . Those studies also highlight the other benefit that comes with a sequencing-based approach. \u201cSequencing is the best way to truly profile all aspects of sequence variation,\u201d says Roland Wicki, director of Applied Biosystems' SOLiD application development. In addition to looking at RNA expression patterns, RNA-seq can allow researchers to discover new classes of RNA, detect point mutations in expressed transcripts, identify fusion transcripts or uncover new alternative splicing events \u2014 discovery applications not possible with other technologies. Using the SOLiD 3 platform, Nicole Cloonan of the Institute for Molecular Bioscience at the University of Queensland in St Lucia, Australia, and her colleagues identified more than 2,000 expressed single nucleotide polymorphisms (SNPs) from embryonic stem cells and determined that the RNA-seq approach could detect 25% more genes than analog microarrays. Ali Mortazavi of the California Institute of Technology, Pasadena, and his colleagues used Illumina's Genome Analyzer to find 3,500 genes that showed one or more alternative splice forms. Wicki says that when it comes to the identification of point mutations in transcripts or analysis of allele-specific expression, next-generation sequencing platforms such as the SOLiD 3 system, which uses a unique ligation-based sequencing approach that allows for two-base encoding, tend to be highly accurate. Still, the real value of such systems lies in the ever-increasing numbers of sequencing reads they can generate. \n               Enriching discovery \n             Sequencing today is allowing researchers to increase the dynamic range of their investigations simply by increasing the number of sequencing reads analysed. There is a cost to this, though. \u201cHigh-throughput sequencing of RNA does take a lot of reads to get depth,\u201d says John Rinn, an assistant professor at Beth Israel Deaconess Medical Center in Boston, Massachusetts. \u201cSo you end up sequencing highly abundant RNAs over and over again before getting to low-abundance stuff.\u201d Although this is necessary when it comes to profiling gene-expression patterns, it has led several researchers, including Rinn's group, to develop new methods to enrich for specific populations of RNAs from the transcriptome (see  'Rethinking junk DNA' ) when it comes to discovery applications. Several new enrichment procedures focus on eliminating the abundant amounts of ribosomal RNA from a pool of total RNA before the library-generation step of RNA-seq. Invitrogen, part of Life Technologies in Carlsbad, California, recently introduced the RiboMinus kit for RNA-seq applications. The kit depletes ribosomal RNA from a sample by binding the ribosomal RNA to probes containing locked nucleic acids, which are then separated from the sample using magnetic beads. Another enrichment approach was developed by Evrogen of Moscow, Russia. This uses a duplex-specific nuclease for normalization of RNA transcript levels. Following complementary DNA generation, the templates are denatured and a duplex-specific nuclease is added to the reaction. Although abundant transcripts find matches and become double-stranded, thereby acting as targets for the nuclease, less abundant transcripts take longer to find their partners and so are degraded less frequently. With or without enrichment for specific RNA populations, the analysis of tens of millions of sequence reads can be a daunting task for most researchers, especially as analysis tools in the digital world are not as advanced as their analog counterparts. \u201cHigh-throughput sequencing data analysis is totally different from using arrays,\u201d says Liu, adding that at the moment there are few standard tools for analysing the digital gene-expression data sets generated with next-generation sequencing platforms. \u201cI think in the middle of last year we realized that there was a shift coming from the analog to the digital platform,\u201d says Roald Forsberg, director of scientific software solutions at CLC bio in Aarhus, Denmark, which specializes in analysis tools for high-throughput sequencing data sets. This led CLC bio to update its main and genomics software packages to support the analysis of both digital and analog gene-expression data sets. Although Forsberg suspects the shift to digital will take time, he thinks analysis tools with the ability to interrogate both sequencing and microarray data sets will remain critical to researchers for even longer. \u201cThere has been much investment in microarrays in both the academic and pharma worlds, using unique tissue samples in a lot of cases, which would be a shame to ignore or just throw away,\u201d he says. \n               Higher-level complications \n             Currently, the biggest challenge for researchers looking at next-generation sequencing approaches is probably the sheer volume of data. \u201cFor serial analysis of gene-expression experiments there are some tools, but a lot of them are not well equipped to deal with the amount of tags that come out from high-throughput sequencing,\u201d says Liu. This creates the need to perform most data analysis on more powerful computer clusters. \u201cIt is not something where you can download a program to run on your laptop,\u201d says Liu. Then there is the challenge of determining where those millions of tags or short sequences come from in the genome. \u201cIt is not uncommon for a sequencing platform to generate data and only 40\u201350% of the data are mappable,\u201d says Liu \u2014 meaning that the remaining could not be mapped at all or mapped to regions of the genome once considered 'junk'. And for Liu and others the question then becomes, are these biological relevant sequences \u2014 for instance, unannotated genes or antisense transcripts \u2014 or merely artefacts of the sequencing process? \u201cI think it is hard to say, but it is probably going to end up being a bit of both,\u201d says Jay Shendure, an assistant professor in the department of genome sciences at the University of Washington in Seattle. Shendure says that his group has performed runs on the Illumina Genome Analyzer using genomic DNA where 95% of the reads were mappable, implying that for human DNA libraries with similarly sized read-lengths, the technical artefacts are not so great as to result in only 50% mappable reads. He cautions, however, that there are additional steps in making a sequencing library from RNA that could introduce some artefacts. Developers at CLC bio have also experienced problems in mapping short-read RNA-seq data sets. Forsberg hopes that the future use of 'paired-end' reads \u2014 in which sequencing information is obtained from both ends of a DNA fragment \u2014 might help when mapping back to the genome. He has noticed reservations among researchers when it comes to generating either longer reads or using paired-end protocols for RNA-seq, because this increases the time and cost of sequencing. \n               Economic advantage \n             Although next-generation sequencing provides a high-throughput option to look at gene-expression profiles from a small number of samples, it turns out that in the analog world microarrays provide their own high-throughput advantage to researchers. \u201cWith microarrays you can profile RNA from a hundred different samples, which would be incredibly expensive to do  de novo  with sequencing,\u201d says Rinn. Lower costs alongside increasing probe density on whole-genome tiling arrays for transcript-mapping applications are keeping microarrays from being lost in the blur of sequencing advances. \u201cI think the array platform is well suited to screening studies for a quick look at the transcriptome or the general clustering of samples,\u201d says Baker. He is quick to add that the per-sample cost for microarrays has dropped significantly in recent years, with some genome-wide tiling arrays now costing less than $100 per sample. \u201cResolution is essential for tiling arrays to achieve sensitive and reproducible data, and the more closely spaced the probes are, the more likely they will detect small exons and small RNAs,\u201d says Rohaizah James, expression product manager at Roche NimbleGen in Madison, Wisconsin. High density seems to be the order of the day. Roche NimbleGen now offers a 2.1-million-feature whole-genome tiling array to explore both coding and non-coding parts of the genome. Affymetrix of Santa Clara, California, provides tiling arrays with 6.4 million features consisting of 25 base-pair probes spaced 10 bases apart across the entire human genome for transcript mapping. And Agilent Technologies, also in Santa Clara, California, offers custom tiling arrays with several hundred thousand features. James says that tiling arrays can be used for quick and accurate empirical annotations of transcriptomes, information that can then be applied to an expanding area of interest among researchers: building custom differential-expression arrays with more comprehensive coverage. \u201cWe receive many interesting requests for custom array designs,\u201d says James, such as multi-organism arrays that are useful for studying the transcriptomes of pathogenic organisms in the host background. Exon and exon-junction arrays have also become increasingly popular among researchers exploring the transcriptome, says Joel Fellis, associate product manager at Affymetrix. \u201cScientists are increasingly using whole-transcript assays and arrays to measure changes at the exon level, and incorporate alternative splicing analysis into their expression studies.\u201d A number of companies now offer exon-specific arrays in addition to exon-junction arrays for these studies. Having been around for some time now, microarrays also tend to offer researchers the security of a standardized technique. \u201cI think that is a very mature tool pack, where people know what to expect,\u201d says Liu. The tools to analyse microarrays are more widely available and better developed, making analysis less complicated and easier for even novice researchers. Analog measurements come with a downside, however \u2014 one that has hounded microarrays from the start. \u201cThere is a frequent lack of inter-platform reproducibility,\u201d says Shendure, something that digital, direct-counting platforms could overcome. Others argue the real challenge for microarrays in the coming years will be to remain up to date. \u201cOur understanding of the transcriptome is constantly evolving, making it difficult for micro-arrays to stay current,\u201d says Wicki. For this reason a number of researchers expect microarrays to migrate towards more targeted applications in the future, perhaps associated with biomarker validation or other diagnostic applications. \u201cI think in terms of basic science research or mechanistic understanding it could be that sequencing will gradually phase out arrays,\u201d Liu says. Although microarrays continue to be the tool of choice for most researchers for gene-expression analysis, looking at copy-number variations within the genome and genotyping, some scientists see even bigger changes for this technology on the horizon. \u201cI think sequencing will penetrate all those markets with increasing force as the cost drops,\u201d says Shendure. Although he thinks this will lead to a steady decline in the analytical role of microarrays over time, he strongly suspects their use in preparative applications will expand in the coming years. \n               The way ahead \n             \u201cArrays will have a role as a super-cheap way of making DNA,\u201d says Shendure. At the 2009 Advances in Genome Biology Technology meeting in February at Marco Island in Florida, Shendure set up a workshop on methods to selectively capture subsets of the genome or transcriptome for high-throughput sequencing. Three of the four methods he highlighted relied on microarrays to isolate specific fragments of DNA either by hybridization of genomic libraries directly to the arrays, or by solution-phase hybridization to oligonucleotides cleaved from arrays, or by an approach that converts oligonucleotides released from arrays to inversion probes to capture exons. \u201cWe are now able to capture the whole coding genome on a single array,\u201d he says. Developers have recently started supplying arrays for preparative applications as well. Agilent Technologies has partnered with the Broad Institute to provide the SureSelect Target Enrichment System for the Illumina Genome Analyzer, with a version in development for Applied Biosystems' SOLiD platform. The current system uses up to 55,000 biotinylated RNA probes in a single tube to capture particular segments of the genome, but future plans include an array-based partitioning tool for smaller-scale experiments. Last month, Roche NimbleGen introduced a human exome sequence capturing array to prepare samples for sequencing on the GS FLX systems from 454 Life Sciences. In the end, most researchers think advances in next-generation sequencing for digital gene-expression analysis will continue to push microarrays towards more specific applications. Just how soon researchers leave their analog roots and take that digital plunge, however, will depend on how quickly developers can standardize and support a digital lifestyle. Reprints and Permissions"},
{"file_id": "458243a", "url": "https://www.nature.com/articles/458243a", "year": 2009, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "458929a", "url": "https://www.nature.com/articles/458929a", "year": 2009, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "460417a", "url": "https://www.nature.com/articles/460417a", "year": 2009, "authors": [], "parsed_as_year": "2006_or_before", "body": "When researchers at Plectix BioSystems in Somerville, Massachusetts, began to use their new Cellucidate software to model the epidermal growth factor receptor pathway, they calculated that there were 10 33  potential states \u2014 including all protein complexes and phosphorylation states \u2014 for the system. \u201cThis is the kind of complexity that scientists have to grapple with when it comes to cell-signalling networks,\u201d says Gordon Webster, vice-president of biology at Plectix. Although not all these potential states necessarily occur in that pathway, when it comes to creating more manageable models for understanding cell signalling researchers face a difficult question: what interaction data do they use in their models? Although many commercial and public databases still rely heavily on the small-scale protein\u2013protein-interaction studies that appear in peer-reviewed literature, the emergence of high-throughput experimental approaches that generate very large interaction data sets is creating the need for a new set of rules. \u201cIn practice, what comes out of these high-throughput studies is not a yes/no thing \u2014 'these interact, and these don't' \u2014 but in fact they generate a list of interactions and associated probabilities,\u201d says Jack Greenblatt from the University of Toronto in Canada. To generate such probabilities for his mass-spectrometry studies, Greenblatt applied a 'gold standard' for protein interactions \u2014 a set of protein complexes or interactions in which there is a strong amount of confidence according to the literature \u2014 as well as a set of proteins not known to interact with one another as a negative standard. He then tackled the question of whether or not data sets generated by mass spectrometry stacked up against protein-interaction reports seen in peer-reviewed literature. \u201cWhat we did in the end was to use the same gold standard to look at the molecular-biology literature,\u201d says Greenblatt. After adjusting the cut-off point so that the average confidence score from a high-throughput study matched the confidence score of interactions reported in the literature, he says the interaction data from such studies are no better or worse than what is in the literature. Marc Vidal, a geneticist at the Dana\u2013Farber Cancer Institute in Boston, Massachusetts, wants to see a similar approach taken with yeast two-hybrid and other binary screens. \u201cLet's roll up our sleeves and decide on a positive and negative gold standard,\u201d he says. \u201cBut let's also use orthogonal assays to give confidence scores to the interactions.\u201d In January, Vidal and his colleagues published a series of papers 6\u20139  suggesting the use of new binary interaction assays to build confidence in basic networks produced using yeast two-hybrid data sets. \u201cYou say 'OK, this is basic network' and then push that into a framework where all interactions are going to be tested by two or three orthogonal assays. And not only that, but do that under conditions where you have a positive and negative gold standard,\u201d says Vidal, adding that the high-scoring interactions can then serve as hypotheses for researchers to test. Whether or not these efforts and standards will lead researchers to rely more on large-scale data sets and mine them more deeply will only be known in time. For some, even with confidence measures, large-scale data sets lack information often found in smaller studies. \u201cThis is one of the paradoxes that we find when people talk about systems biology. With technology it is very easy to generate spreadsheets of interaction data, but that alone does not represent any knowledge,\u201d says Webster. But for Greenblatt and others, large-scale data sets represent a starting point for further research efforts. \u201cTo me, high-throughput studies are just like the conventional literature,\u201d he says, \u201cproviding a gold mine for people to dig into.\u201d \n               N.B. \n             Reprints and Permissions"},
{"file_id": "4611150a", "url": "https://www.nature.com/articles/4611150a", "year": 2009, "authors": [], "parsed_as_year": "2006_or_before", "body": "Even as 'connectomics' makes its way into the mainstream scientific vocabulary, there is already debate over what \u2014 if anything \u2014 it actually means. \u201cIt's sort of analogous to how 'genome' used to mean the set of genes, but now it means the whole DNA sequence,\u201d explains Sebastian Seung of the Massachusetts Institute of Technology in Cambridge. There is fairly broad agreement that mapping the wiring in mammalian brains is a worthwhile endeavour. The issue is one of scale \u2014 should these be comprehensive reconstructions of neuronal circuitry, or more macro-scale representations of long-range connections between regions of the brain? This is the neuroanatomical equivalent of choosing between creating a road atlas or Google Earth. Arguments can readily be marshalled for and against either approach; most boil down to how best to invest time, money and technology. \u201cDense reconstruction of a cubic millimetre of the cortex is kind of a 'going to the Moon' goal, where we think it's possible but difficult,\u201d says Seung, \u201cbut in solving that problem, all the other problems become trivial.\u201d On the other hand, Partha Mitra of Cold Spring Harbor Laboratory in New York thinks that the tools are already at hand for creating a sparser 'mesoscopic' map of the projections that link functionally discrete brain regions, which some call a projectome \u2014 although you won't catch Mitra using that term. \u201cEverything has an '-ome' added to it, and that's ok if you're in a yoga class,\u201d he jokes. \u201cBut I prefer 'brain architecture' because it conveys structure and function; architects shape space for human use, and evolution shaped our nervous system for appropriate behavioural repertoires and so on.\u201d Mitra and dozens of colleagues recently published a plan for integrating existing tools \u2014 including chemical labels and engineered viral tracers \u2014 into a concerted effort to chart the connections between functionally homogeneous clusters of cells via light microscopy 9 . Larry Swanson at the University of Southern California, Los Angeles, has proposed that 500\u20131,000 such anatomical units exist within the brain, and Mitra thinks that linking these will prove challenging but not insurmountable. \u201cLarry has deep knowledge of the relevant literature, and estimates that only around a third of these possible mesoscopic connections have ever been studied,\u201d Mitra says. \u201cBut when I sat down and thought about the cost to map out those connections, I was shocked to find that it actually shouldn't take that much time, money or effort.\u201d With the recent awarding of a Transformative R01 grant from the US National Institutes of Health, Mitra's team is now taking first steps towards making their Mouse Brain Architecture Project into a reality. At the same time, by deliberately overlooking the highest orders of neuroanatomical complexity, this approach leaves open numerous questions that will probably be answered only by dense mapping. \u201cThese things are just so incredibly tangled and complicated, it's inconceivable that you'll come across two identical brains,\u201d says Stephen Smith of Stanford University in California. \u201cAnd I think it is a wonderful opportunity for those of us who are pursuing connectomes \u2014 to do not one, but many. Neural plasticity is just one of the many interesting questions that will be open for new attack. Scientists in both camps hold up work done by researchers at the Allen Institute for Brain Science in Seattle, Washington, in mapping gene expression in the brain as an example of how good science, careful planning and efficient workflows can yield tremendous pay-offs. Smith and others think that maturation of high-resolution circuit-mapping techniques will ultimately bring high-throughput 'dense' reconstruction within reach. Accordingly, Mitra emphasizes that his group's data \u2014 which it intends to make freely available via an open-access model \u2014 should provide a framework for future reconstructions. \u201cThis is just supposed to be the first generation,\u201d he says. \u201cI have no doubt that if this succeeds \u2014 this whole-brain approach to brain architecture and neuroanatomy \u2014 then we will see successive waves of technology hitting the problem.\u201d \n               M.E. \n             \n               Back to main article \n             Reprints and Permissions"},
{"file_id": "460419a", "url": "https://www.nature.com/articles/460419a", "year": 2009, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "4611153a", "url": "https://www.nature.com/articles/4611153a", "year": 2009, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "4611149a", "url": "https://www.nature.com/articles/4611149a", "year": 2009, "authors": [{"name": "Michael Eisenstein"}], "parsed_as_year": "2006_or_before", "body": "After a long lull, powerful new technologies are putting the charting of brain circuitry back on neuroscientists' agenda. Michael Eisenstein explores the challenge of mapping the mammalian mind. It's hard out there for a neuroanatomist \u2014 or at least for those who are working to reinvigorate a field that has come to be viewed as outdated and relatively 'unsexy'. \u201cVery few people openly use the term 'neuroanatomy' at this stage for the kind of thing we're talking about, but frankly that's what it is,\u201d says Stephen Smith of the Stanford School of Medicine in California. \u201cIt's just a kind of neuroanatomy that was impossible until now.\u201d  Smith and his colleagues are part of a small community of scientists striving to pick up the mantle of Spanish neuroscience pioneer Santiago Ram\u00f3n y Cajal by developing sophisticated methods for brain-wide mapping of synaptic connections and neural circuits. \u201cNeuroscience got off to a very good start with the idea that wiring diagrams [are probably the key to understanding] brain function, but remarkably little has happened with that idea,\u201d says Jeff Lichtman of Harvard University in Cambridge, Massachusetts. This is now changing, and although these researchers may debate whether to call what they do 'connectomics' or 'circuit mapping' \u2014 or even 'neuroanatomy' \u2014 there's no question that ongoing strides in cell biology, imaging and computational analysis are bringing scientists closer to understanding the structural foundations of brain function. \n               Detail-oriented \n             For more than half a century, scientists have recognized the power that electron microscopy's still-unparalleled resolution could bring to the exploration of neural circuitry. Indeed, the successful assembly by Sydney Brenner and colleagues in the 1980s of the 300-neuron wiring diagram 1  of the nematode worm  Caenorhabiditis elegans  was a neuroscience tour de force made possible through reconstruction of transmission electron microscopy (TEM) images from serially collected tissue sections. Sadly, that was pretty much it for the next 25 years, as the labour-intensive reconstruction process \u2014 which consumed more than 10 years' work from Brenner's team \u2014 was simply too demanding to deliver large-scale brain mapping. It wasn't until 2004 that Winfried Denk's team at the Max Planck Institute for Medical Research in Heidelberg, Germany, revitalized electron microscopy as a tool for high-resolution neuroanatomy. In his serial block-face (SBF) imaging method 2 , samples are mounted on an ultramicrotome housed within a scanning electron microscope (SEM), which images the surface of the embedded tissue immediately before the diamond knife shaves a thin slice off the top, exposing the next layer for a subsequent round of imaging. This introduces unprecedented capacity for automation to the imaging workflow, but also overcomes several other issues, including the ability to collect data from ultrathin sections without the distortion that can arise with imaging of slices. Denk's team has since learned how to give the samples a closer shave, boosting the accuracy of reconstruction. \u201cIn 2004, the thinnest slices were 40\u201345 nanometres, but now we're at 25 nanometres,\u201d he says. Another major objective has been to tweak staining to optimize the labelling of neuronal processes. \u201cWe were working very hard on getting a staining technique that selectively stains the surface of cells and gets rid of the insides, so you don't get distracted by things such as mitochondria, nuclei or the endoplasmic reticulum,\u201d says Denk. This method is compatible with instruments from leading manufacturers such as FEI, of Hillsboro, Oregon, and Hitachi High-Technologies in Tokyo, and users can even buy integrated SBF-SEM systems, such as the 3View platform made by Gatan in Pleasanton, California, which is based on the Denk lab's design. \u201cI think Denk's was really a landmark publication,\u201d says Ben Lich, strategic marketing manager at FEI. At the same time, SBF-SEM is still limited in resolution by the amount of energy that can be pumped into samples safely. \u201cSpecimens are typically embedded in a resin, and under the influence of the electron beam they will crosslink and the material becomes harder to cut,\u201d explains Lich. \u201cIf you put a lot of electrons into your material to create that image, you also do a lot of damage in terms of crosslinking and you cannot really cut it reliably. As an alternative, FEI is applying technology initially developed for the semiconductor industry, using a focused ion beam to precisely remove thin layers of tissue. \u201cThe advantage is that we can put a lot more charge into these blocks and create crosslinking,\u201d says Lich, \u201cbecause the focused ion beam can cut silicon or diamond \u2014 basically, we can cut anything with it.\u201d Thus, greater resolution is possible, and FEI's DualBeam instruments can image voxels of \u00d7 4 \u00d7 10 nanometres, relative to the present 20 \u00d7 20 \u00d7 25-nanometre resolution limit of SBF-SEM. However, SBF-SEM can image much greater volumes \u2014 on the scale of two to three orders of magnitude more \u2014 making focused ion beam and SBF complementary rather than competitive tools. \n               A beautiful mind \n             There are alternatives to electron microscopy \u2014 particularly for researchers interested in more than a static snapshot. \u201cAll the things I've studied up until now have been dynamic questions,\u201d says Lichtman. \u201cAnd you just can't do that with electron microscopy \u2014 you've got to kill it to look at it!\u201d Lichtman's solution was the Brainbow transgenic mouse 3 , which uses a site-specific DNA recombination system to randomize expression of multiple fluorescent protein genes in neurons, yielding intermediate colour combinations that distinguish each cell from its neighbours. With a broad portfolio of commercially available fluorescent proteins from which to choose \u2014 including the Living Colors proteins made by Clontech in Mountain View, California, and the TurboColors proteins from Evrogen in Moscow \u2014 Lichtman's group had many options. However, just a handful of colours proved sufficient to generate nearly 100 distinct labels. \u201cAll of the colours of the rainbow that we see are interpreted from three pigments in our retina,\u201d he explains. \u201cSo we just inverted that, thinking that if we could just mix different amounts of three colours in different cells, we should be able to get all the visible colours of the rainbow.  In other cases, more selective labelling is desirable, and scientists since Ram\u00f3n y Cajal have pursued chemical and biological methods for exclusively targeting neurons that are functionally linked via active synapses. One promising method, being pioneered by scientists such as Lynn Enquist at Princeton University in New Jersey and Ed Callaway at the Salk Institute for Biological Studies in La Jolla, California, exploits natural infection patterns of neurotropic viruses for the fluorescent labelling of individual neural circuits 4 . Callaway works with modified rabies virus, a pathogen that spreads so efficiently across mouse neurons that a single particle injected into the brain can prove lethal. His viruses are constrained via deletion of a key glycoprotein gene. \u201cWe preserved the ability to replicate and amplify, but provided a means to control the spread,\u201d he says. \u201cDeleting the glycoprotein gene also allows us to control the initial infection and target specific cell types.\u201d Some investigators are applying viral tracing to trace entire networks of interconnected cells, but Callaway is mostly interested in targeting smaller 'neighbourhoods'. \u201cWhen we get to the point where we can go into a live animal and target one cell and label every single input to that cell, that will be a huge advance,\u201d says Callaway. \u201cBut it's clear we're far from labelling all of them. We're now labelling up to 100 inputs, but it should be 1,000. By definition, such methods lend themselves to 'sparse' mapping of a limited subset of neurons at a lower resolution than 'dense' strategies such as electron microscopy. But many scientists see this as a feature rather than a bug, enabling a more selective type of connectomics ( see 'Whose map is it anyway?' ) that has the capacity to correlate circuit structure with function. For instance, it can combine the maps with either neuronal activity sensors, such as the calcium-sensitive Cameleon indicator from Invitrogen in Carlsbad, California, or light-activated ion-channel proteins, such as the channelrhodopsin and halorhodopsin molecules engineered by Karl Deisseroth's team at Stanford University in California 5 . Indeed, even exquisite resolution may soon no longer be the sole domain of electron microscopy, as optical methods emerge that exploit clever workarounds to overcome the diffraction limit for fluorescence imaging, including stimulated emission depletion, stochastic optical reconstruction microscopy, photoactivation localization microscopy and structured illumination 6 , and Lichtman and Smith are among those exploring the benefits and challenges of using 'super-resolution' imaging to characterize circuits at the molecular level. \n               Filling in the details \n             Another important capability of fluorescence imaging is the ease with which multiple molecular targets can be labelled and readily distinguished \u2014 an essential consideration in building a useful map. \u201cAs soon as people have a black-and-white connectivity diagram,\u201d says Smith, \u201cthey'll realize they're really stumped by not knowing what molecules are at a synapse, how the synapse is going to transmit, what its kinetics are going to be, and what's going to turn it on and off.\u201d Close pairing of electron microscopy and light microscopy represents a potential solution. In the array tomography technique developed by Smith's team 7 , for example, a resin-embedded sample is continually sliced by a diamond knife, with the sections sequentially collected on an adhesive surface, enabling them to be arrayed on a slide. These arrays can then be subjected to multiple rounds of immunofluorescence staining and, ultimately, prepared for SEM imaging. Combining imaging modalities enables data relating to expression of channels and receptors to be overlaid onto high-resolution circuit maps. \u201cThere are tricks that let us routinely work with 10\u201315 labels on individual specimens,\u201d says Smith. \u201cIt's a far cry from the 20,000 genes that we'd have to image to fully unlock the brain, but it's a big step in the right direction.\u201d  Lichtman's team has been following a similar path, as it continues to refine its automated tape-collecting lathe ultramicrotome (ATLUM) method 8 . In ATLUM, an epoxy-embedded tissue sample is rotated continuously on a lathe, grazing against a diamond knife that pares away ultrathin sections, which are automatically collected on a continuous strip of adhesive tape. The resulting strips can be imaged by SEM then retained indefinitely for further study. The next generation of this platform promises to deliver sections as thin as 20\u201325 nanometres along the  z -axis, enabling near-seamless circuit reconstruction. \u201cThere is a diminishingly small amount of ambiguity at 30 nanometres, and I think most of those ambiguities would go away if we went down to 25 nanometres,\u201d he says. Lichtman is also looking to integrate fluorescent imaging with ATLUM, perhaps via electron-microscopy-friendly labelling methods that target the tags used in Brainbow. \u201cA purple axon in Brainbow might have two epitope tags, so that blue and red fluorescent proteins can both be stained by immunofluorescence with gold beads of different sizes,\u201d he says. \u201cSo if you see an axon with equal numbers of big and little gold beads, then you know that it's a purple axon.\u201d  This would prove useful not only for integrating data from both modalities, but also as a way to 'fact-check' circuit traces. \u201cI think that soon we'll be able to look at a cell body and predict what kind of glutamate receptors and kinases we're going to find in the terminals,\u201d says Smith. \u201cSo if you follow your wire for millimetres, it better have a certain marker in the synaptic terminal at the end of that wire. If it doesn't, that means you made a mistake.\u201d \n               Trace elements \n             After all the imaging is done, the fundamental problem remains of turning mountains of data into three-dimensional reconstructions and following the neuronal processes that weave through them.  Numerous commercial tools are available for user-guided neuronal tracing, including Imaris from Bitplane in Zurich, Switzerland, and Neurolucida, from MBF Bioscience in Williston, Vermont. Neurolucida was initially developed more than 20 years ago and is one of the most established tools for the manual charting of neurons from fluorescent or electron-microscopy images, offering a relatively straightforward interface for charting complex neuronal processes. \u201cUsing a motorized microscope stage and video camera, the software lets you map neuronal processes that are far larger than a single field of view,\u201d explains chief scientific applications officer Geoff Greene. \u201cWhen the user reaches the end of a dendrite, the software will automatically take you back to the  x \u2013 y\u2013z  coordinates of the last unfinished branch, so it can travel down the alternate branch and trace it.\u201d Denk's team has also developed an elegant tool, Knossos, for rapidly tracing neurons within their reconstructions, in which users sketch rudimentary 'skeletons' along the path taken by a given neurite. Of course, manual tracing is wholly impractical for large-scale mapping, and the hunt is on for algorithms that can automatically define individual neurons within three-dimensional reconstructions, a process known as segmentation. Neurolucida features a module called AutoNeuron that strives to deliver rapid, machine-assisted tracing, but Greene acknowledges that this is still a work in progress. The fundamental problem is that near-perfect accuracy is required. \u201cIf you lose a wire somewhere along its length, then you lose all the connections downstream from where you lose it,\u201d says Denk. \u201cIf I lose an axon halfway down its length, I could lose 5,000 connections.\u201d The core of the segmentation problem, explains Sebastian Seung of the Massachusetts Institute of Technology in Cambridge, is getting computers to see the world as humans do. \u201cTo be able to trace the neurons, we have to know the boundaries of every object,\u201d he says. \u201cThis is one of the first problems ever attacked in computer vision, back in the late 1960s \u2014 and yet today we still don't have reliable systems that do it.\u201d Rather than trying to establish strict neuron-recognition guidelines, Seung and his colleagues such as Dmitri Chklovskii at the Janelia Farm Research Campus in Ashburn, Virginia, are pursuing 'machine-learning' strategies that teach computers by example. \u201cWe have humans trace the boundaries, and create a training set,\u201d says Seung, \u201cthen we have the computer learn how to imitate the human tracing.\u201d This approach brings with it a number of complex challenges, including the development of metrics that enable unambiguous comparison of different tracing strategies and quantification of overall accuracy. These computer 'students' also need to know the limitations of their organic instructors. \u201cWhen humans trace stuff, there's just a lot of jitter in the way they trace,\u201d says Seung. \u201cSo we devised ways in which computers can be made to imitate humans but not take them too literally.\u201d  Even historically pure 'wet labs' are finding themselves joining the fight. Lichtman's lab routinely churns out hundreds of 16,000 \u00d7 16,000-pixel images, each of which can consume a gigabyte of space; as such, developing tools for data handling is now a daily fact of life. \u201cIn my hiring now, about a third of the people I'm looking at are people who are computer scientists,\u201d he says, \u201cand that would have been unthinkable five years ago.\u201d \n               Thinking big \n             Ultimately, many of the obstacles that constrain large-scale circuit mapping boil down to maximizing throughput; for example, the rate of electron-microscopy imaging. \u201cWe'll either have to parallelize acquisition, or have a microscope that's much faster,\u201d says Denk. \u201cAnd I don't mean by a factor of five, I mean by factors of 100. Consistent quality of sample preparation is also a key problem, as all downstream analysis rests on this step. Accordingly, Smith's main nemesis these days is the dirt that can obliterate fine details from specimens, and his team is learning lessons in cleanliness from their neighbours in Silicon Valley. \u201cI take much heart from the fact that today people can make a microprocessor chip with a billion transistors that each work perfectly for about $20 a chip,\u201d he says. In fact, many cite the semiconductor industry as a model for what will be needed for any large-scale 'Connectome Project': consistent application of an established set of optimized methods. \u201cAt some point we'll have all the tools lined up,\u201d says Denk, \u201cand then we'll decide to spend some real money on this to do the whole brain of some animal.\u201d Reprints and Permissions"},
{"file_id": "460415a", "url": "https://www.nature.com/articles/460415a", "year": 2009, "authors": [{"name": "Nathan Blow"}], "parsed_as_year": "2006_or_before", "body": "Researchers have identified thousands of macromolecular interactions within cells. But, as Nathan Blow finds out, joining them up in networks and figuring out how they work still poses a big challenge. In the spring of 2006, Andrew Emili and Jack Greenblatt from the University of Toronto in Canada and their colleagues published a survey 1  of the global landscape of protein complexes within the yeast  Saccharomyces cerevisiae  in  Nature . In the same issue, another group of researchers from the drug research company Cellzome in Heidelberg, Germany, also reported 2  on  Saccharomyces  protein complexes. \u201cThose two data sets overlapped nicely, but by no means perfectly,\u201d says Mike Tyers, a systems biologist at the University of Edinburgh, UK. \u201cAnd yet it was essentially the same method and same organism.\u201d Greenblatt thinks that the two studies highlight something important that is emerging from the current crop of large-scale protein\u2013protein interaction studies. \u201cIf you combine data sets you have more information than from any one study alone,\u201d he says. This is not to say that one such study is right and the other is wrong: scientists suspect it is more likely that one study often compensates for another's false negatives, revealing true protein interactions that can be missed during a single screen. \u201cI think the interaction space is very large. Part of the issue is that there is a large range of interaction affinities, and as you start to get down into the weaker interactions those are tougher to detect,\u201d says Tyers. He adds that identifying such \u201cmoving targets\u201d is not like sequencing DNA, which can be argued to be a more stable target for researchers to aim at. But the jigsaw pieces are starting to pile up as researchers generate more and more genetic, metabolic and protein-interaction data sets using a diverse array of technologies. This work has been aided in recent years by a number of improved methods and techniques. Add to this recent refinements in computational tools for modelling signalling pathways and it's clear that scientists might be on the cusp of changing the way they look at signalling and information flow in cells. \n               Embracing diversity \n             \u201cI think genetic information lays out the blueprints, whereas proteomics is much closer to what is going on in the cell, a molecular manifestation of a phenotype,\u201d says Mike Snyder, a biologist at Yale University. When it comes to cataloguing proteins and their interactions, researchers are learning to embrace experimental diversity. \u201cEvery approach will usually give an overlapping but distinct set of information,\u201d says Snyder. \u201cThey all have their strengths and weaknesses.\u201d Tyers and Greenblatt are in a growing group of investigators who are advancing the use of affinity-purification chromatography followed by mass spectrometry to uncover protein interactions in different cell types. In this approach, a protein of interest is tagged with a label that can be used for affinity purification. Although some scientists suspect weaker-interacting protein pairs or transient interactions could be lost during purification, Greenblatt \u2014 whose lab relies on tandem affinity purification tags in their purifications \u2014 says this is where the use of mass spectrometry helps out. \u201cMass spectrometry is very sensitive, so even if you lose 90% of the interactor during the affinity purification you can still detect the 10% that is left,\u201d he says. As with the technologies behind protein\u2013protein analysis, researchers are finding that no single labelling tag may be enough to isolate all proteins. Tyers's group recently reinterrogated a section of the yeast proteome using three different tags, each with different properties. \u201cFor a number of baits we queried, it made a difference what tag was on it,\u201d he says. \u201cTags can certainly affect the recovery of interactions, consistent with the well-known genetic effects often caused by different tags.\u201d Once a specific protein or protein complex is purified, it is analysed with mass spectrometry. Electro-spray ionization or matrix-assisted laser desorption ionization (MALDI) volatilizes and ionizes peptides, which are analysed on orthogonal or quadrupole time-of-flight (Q-TOF) instruments to identify ions with high mass-to-charge ratio values. Here researchers have benefited greatly from advances by instrument developers. During the American Society for Mass Spectrometry annual conference in Philadelphia, Pennsylvania, in June, Bruker Daltonics of Billerica, Massachusetts, announced its new ultrafleXtreme MALDI TOF/TOF system, and Thermo Fisher Scientific of Waltham, Massachusetts, introduced the LTQ Velos and LTQ Orbitrap Velos devices. Alongside other hardware, such as the Xevo Q-TOF from Waters in Milford, Massachusetts, and the 6500 series of Q-TOF instruments from Agilent Technologies in Santa Clara, California, these machines have improved both the dynamic range and sensitivity of mass analysis; in many cases they also feature integrated upstream separation technologies and improved databases, all of which is making it easier to define a sample's protein composition. For additional detail in the analysis, protein complexes can also be analysed with tandem mass spectrometry, in which selected precursor ions can be smashed into one another to produce still smaller fragments for analysis. \u201cThe big advantage of using mass spectrometry is that it can be performed in a physiological context,\u201d says Tyers. Unlike other methods for surveying protein\u2013protein interactions, mass spectrometry can be done on cell lines or even tissue samples, so indirect interactions that depend on more than two proteins or on post-translational protein modifications can be uncovered. Still, some researchers suggest that although affinity purification followed by mass spectrometry gives important information on how proteins interact in complexes, the approach does not reveal everything about the nature and mechanics of those interactions. \n               Yeast shows the way \n             Binary approaches, such as the yeast two-hybrid assay, can provide different protein interaction information, according to Marc Vidal, a geneticist at the Dana\u2013Farber Cancer Institute in Boston, Massachusetts. Vidal uses the analogy of two football teams facing each other with referees in the middle of the field to explain the differences between the techniques. \u201cThe pull-down mass-spectrometry approach will show you the players, referees and field, but not who is passing to whom and in what direction the ball is travelling,\u201d he says. \u201cThis is where a binary approach comes in.\u201d The yeast two-hybrid assay is arguably the best-known binary approach. It relies on a split transcription factor in which one portion is placed on each of the two proteins being tested for interaction. If the proteins interact, the transcription factor will be regenerated and a reporter gene transcribed, providing a read-out. The assay allows for more the testing of dynamics of protein\u2013protein interactions, such as dissociation rates. \u201cPhysical interactions are not everything: you need both edges and arrows to know the dissociation rates as well as other logical aspects of the relationships. Pull-down mass spectrometry is a little short when it comes to those interactions,\u201d says Vidal. The other advantage of the yeast two-hybrid approach is that it presents a more high-throughput solution to studying protein interactions. \u201cThe two-hybrid approach is reasonably high-throughput,\u201d says Snyder, noting that with robotics a large number of proteins can be tested for potential interactions in a two-by-two format. Other approaches have also been rising to the surface. \u201cFrom the probing that we have done, we have picked up interactions that you definitely do not see with other methods,\u201d says Snyder of his experience using protein microarrays to explore protein\u2013protein interactions. Protein arrays, which are sold by a number of companies including Invitrogen in Carlsbad, California, RayBiotech in Norcross, Georgia, and R&D Systems in Minneapolis, Minnesota, have not been used as often for large-scale protein-interaction studies as either mass spectrometry or the binary-interaction approaches. \u201cThey have had impact in certain areas. Part of the problem is that they have been somewhat expensive, which might be the reason that they have not caught on as much for large-scale studies,\u201d says Snyder. Given the potential of protein microarrays to identify unique interactions, he hopes that costs will fall, which could increase their use in large-scale interaction studies. An orthogonal approach to the yeast two-hybrid assay for detecting protein\u2013protein interactions is the protein-fragment complementation assay (PCA), in which two proteins of interest are attached to complementary fragments of a reporter protein. If the proteins interact with one another the reporter is regenerated providing a direct read-out that is not dependent on transcription of another gene as in the yeast-two-hybrid assay. Steven Michnick and his colleagues used the PCA approach last year 3  to explore the yeast-protein interactome, identifying nearly 2,800 interactions among 1,124 proteins, many of which had not previously been identified by other approaches. Additional work and tools could be needed to define a complete interaction map for even the most well-characterized organisms. Snyder suspects that in yeast each protein 'sees' about five other proteins on average. But at the moment all of the interactions identified for yeast, which has around 6,000 proteins, add up to far fewer than the potentially 30,000 predicted. \u201cSo, there is still a way to go,\u201d he says. \n               Clear pathways \n             Finding which macromolecules interact is only the first step to figuring out signalling pathways. Researchers also need methods to assemble those interactions into cellular networks, which is where bioinformatics enters the picture. \u201cIt is like building a bicycle \u2014 you have the wheels, a seat and handlebars, but we provide the steps to put the parts together,\u201d says Julie Bryant, vice-president of business development at GeneGo in St Joseph, Michigan, a company specializing in the development of software for cell-signalling and metabolic analysis. GeneGo is not alone here: a growing number of developers are creating tools for the analysis of signalling networks \u2014 from those that build model networks based on existing data to systems that use data sets and models to make predictions about the activity of different signalling networks. \u201cWe can take in any kind of experimental data \u2014 genomic, proteomic, metabolomic \u2014 and overlay them on cell-signalling pathways,\u201d says Bryant, describing GeneGo's MetaCore software. Being able to overlay a variety of different experimental data from different sources requires careful database curation, she says. At the moment, GeneGo employs 50 scientists to manually mine and curate published literature for studies on protein interaction, gene expression, metabolism and drugs to expand and update its internal database, which now contains more than 120,000 multi-step interaction pathways, each averaging 11 steps, with information on direction, mechanism and feedback along the pathways, along with direct links to literature evidence. Literature mining is important for building larger interaction databases, but Bryant says it can be especially difficult if the experimental descriptions underlying the results have not been published. Another problem, according to Vidal, is that researchers sometimes have \u201csociological\u201d biases in terms of which proteins and interactions they will work on and report. \u201cWe have learned a lot about the rules of how macromolecules interact, but when you ask how much of the network we have, or what the size of the interactome of a particular species is, if you only used the literature it would be tough to answer those questions,\u201d he says. Tyers is involved with the publicly funded BioGRID (Biological General Repository for Interaction Datasets) initiative, an internationally curated database of molecular interactions. Three years ago, there was an effort to back-curate all the yeast literature for protein and genetic interactions, but now the database contains protein-interaction data from yeast, worms, flies, plants and even humans along with some genetic-interaction data as well. For Tyers, the goal is to accurately mirror the primary literature and distil it into a format that can be used in network biology. \u201cWe make no judgement calls on the method or even, within reason, the quality of the data themselves,\u201d he says, giving researchers the opportunity to extract the maximum amount of information. A different angle in modelling signalling networks was recently described by Walter Fontana from Harvard University and his colleagues 4 . It uses sets of rules to define relationships between cellular components instead of the more conventional method of defining specific interactions and species using differential equations. Fontana co-founded a company called Plectix BioSystems in Somerville, Massachusetts, which has employed this approach in a web-based system called Cellucidate. \u201cThe system is represented at a very granular level where the participants are allowed to do  in silico  what they would do in real life,\u201d says Paul Edwards, chief executive at Plectix. Imagine the city-building computer game SimCity reworked for complex cellular networks, but here the agents of the cell \u2014 proteins and other molecules \u2014 are the automata instead of colourful animated people. \u201cIn that way the model mirrors the behaviour of the living system it represents: the biology that emerges from our models is the combinatorial expression of all these automata doing their own little thing \u2014 just the way it is in the cell,\u201d says Gordon Webster, vice-president of biology at Plectix. \n               Complexity from simplicity \n             According to Edwards, the advantage of the Cellucidate approach is that a simple set of rules for each agent can result in complex biological behaviour when agents interact during the course of a simulation, unlike modelling in other formats, where the complexity has to be defined before a simulation can be executed. \u201cThe level of granularity also means that rules and agents can be easily recycled from one model to another,\u201d he notes. Like the GeneGo platform and the BioGRID initiative, Plectix relies on literature mining from various sets of experimental data to create the rules for a model system (see  'Playing by the rules', page 417 ). \u201cMapping all interactions is important, but so is understanding the dynamics behind those interactions,\u201d says Snyder. To understand the dynamics of the information flow in cells, researchers not only need more knowledge of protein\u2013protein interaction networks, but they also need to understand protein\u2013DNA interactions, the effects of microRNAs and epigenetic changes on gene expression, and how other macromolecules such as metabolites affect the output of signalling networks. \u201cIt is the whole system together that determines the final output and activity,\u201d says Snyder. Vidal thinks that technological improvements \u2014 especially in nanotechnology, to generate more data, and microscopy, to explore interaction inside cells, along with increased computer power \u2014 are required to push systems biology forward. \u201cCombine all this and you can start to think that maybe some of the information flow can be captured,\u201d he says. But when it comes to figuring out the best way to explore information flow in cells, Tyers jokes that it is like comparing different degrees of infinity. \u201cThe interesting point coming out of all these studies is how complex these systems are \u2014 the different feedback loops and how they cross-regulate each other and adapt to perturbations are only just becoming apparent,\u201d he says. \u201cThe simple pathway models are a gross oversimplification of what is actually happening.\u201d Paul Nurse of Rockefeller University in New York wrote about understanding the cell's information flow last year 5 . He noted that \u201cour past successes have led us to underestimate the complexity of living organisms\u201d, an oversight that is rapidly disappearing within the world of systems biology and will probably never happen again. Reprints and Permissions"},
{"file_id": "462675a", "url": "https://www.nature.com/articles/462675a", "year": 2009, "authors": [{"name": "Kelly Rae Chi"}], "parsed_as_year": "2006_or_before", "body": "Overcoming the limitations of spatial and temporal resolution to image within a cell is no easy feat. Kelly Rae Chi examines the latest diffraction-busting technologies. For many years it was a source of frustration for biologists that the internal components of a cell were practically invisible to them. Researchers believed that the wavelength of light determined a fundamental limit to the resolution of optical microscopes. However, it now seems that the wavelength of light was not such a limiting factor after all. Super-resolution technology allows researchers to see details that are difficult or impossible to image with conventional light microscopes \u2014 at resolutions of 100 nanometres or better. \u201cThere's a huge explosion of interest and progress,\u201d says W. E. Moerner, a professor of chemistry and applied physics at Stanford University in California. \u201cThat makes it very exciting to watch and to participate in.\u201d Although the theory behind super-resolution is advancing, and many different techniques are now validated, commercialization of these new microscopes is to some extent in its infancy \u2014 so much so that many research teams still tackle super-resolution using their own lenses, lasers and algorithms.  \n               Upping commercialization \n             The past year or so has been particularly active in terms of commercialization. Throughout 2008, Applied Precision of Issaquah, Washington, tested prototypes of its DeltaVision OMX system. The final version is available this year and is already installed in 11 laboratories. It uses three-dimensional structured-illumination microscopy, or 3D-SIM, which illuminates a sample with a series of light patterns that look like bar codes. The low-resolution light patterns reflect off the fine structure of the sample to create moir\u00e9 fringes. By applying bar codes in different orientations and processing the reflections using computer algorithms, the microscope generates a high-resolution image of the underlying structure. Carl Zeiss, headquartered in Oberkochen, Germany, is also using the SIM method in its system, ELYRA S.1, to be launched in January. And the company is launching ELYRA P.1, which is based on photoactivated localization microscopy (PALM). This uses photoactivatable proteins that are scattered throughout the sample. Taking multiple frames of the photoactivated dots, and combining them into a high-resolution image, the ELYRA P.1 produces a resolution of around 20 nm. Zeiss also offers a single system, the ELYRA PS.1, that combines laser scanning microscopy, PALM and SIM. Leica Microsystems in Wetzlar, Germany, is using a variation of stimulated emission depletion (STED) microscopy in its newest super-resolution system. The technique uses a laser beam to excite fluorescent dyes, or fluorophores, within the sample. As with a normal laser scanning microscope, the size of the excited spot determines the resolution of the microscope. In STED microscopy, to improve resolution and narrow the focus of the beam, an excitatory laser pulse is immediately followed by a ring-shaped depletion laser pulse to leave molecules in a smaller region in the centre of the ring in an excited state. This allows resolution of structural details in the 100 nm range. In Leica's new system, TCS STED CW, the excitatory and depletion laser beams are continuous and overlayed rather than pulsed. Whereas the TCS STED microscope was priced at around US$1 million, the cost of the TCS STED CW microscope will be closer to the price range of high-end confocal microscopes. The lasers in TCS STED CW microscopy use the intermediate range of the electromagnetic spectrum, which means the system works with common fluorescents such as enhanced yellow fluorescent protein (EYFP), Alexa Fluor 488 and fluorescein isothiocyanate (FITC). \u201cAs long as it's a good confocal sample, it's a good STED sample as well,\u201d says Tanjef Szellas, product manager for super-resolution at Leica in Mannheim in Germany. \u201cBuilding or buying a confocal without a STED option doesn't make much sense,\u201d says Stefan Hell, a physicist at the Max Planck Institute for Biophysical Chemistry in G\u00f6ttingen, Germany, who pioneered the super-resolution movement by creating STED microscopy and other forms of imaging that bypass the limits of diffraction \u2014 about 200 nm (see ' Breaking the light barrier '). \u201cThe commercial hurdles, in terms of costs and system complexity for basic STED microscopy, are gone.\u201d Nikon, headquartered in Tokyo, is using a super-resolution approach called stochastic optical reconstruction microscopy (STORM) \u2014 a similar system to PALM \u2014 developed in the lab of Xiaowei Zhuang, a physicist at Harvard University in Cambridge, Massachusetts. STORM uses photoswitchable fluorescent dyes that are activated and deactivated by different coloured light. By generating overlapping images in different colours and compiling them, the system can produce three-dimensional images that have a spatial resolution ten times better than confocal images in each dimension, Zhuang says. Nikon's instrument, called N-STORM, should be available by May 2010.  \n               Technology tweaks \n             The pioneers of super-resolution microscopy are continuing to improve their methods with better sample preparation, a few strategically placed pieces of hardware and more sophisticated algorithms. One way to achieve greater resolution in fixed samples is to cut thinner slices of tissue. Some groups, including Zhuang's, are collaborating to combine STORM with array tomography, which involves embedding fixed tissue in resin and cutting ultrathin (50\u2013200 nm) slices. \u201cArray tomography is a potential adjunct to all the super-resolution methods,\u201d says Stephen Smith of Stanford University in Palo Alto, California, who first described the method. Using this combination, his group has achieved a 14-fold improvement in axial resolution, and twofold improvements in the each of the two dimensions, compared with confocal imaging. Other combination strategies are possible. For example, two-photon excitation microscopy, an established method that allows researchers to see further into tissues, can be combined with STED microscopy. Earlier this year, Hell's group used the technique on mammalian cell nuclei tagged with fluorescent nanoparticles, obtaining a resolution of less than 70 nm in the focal plane 1 . Bernardo Sabatini, a neurobiologist at Harvard Medical School in Boston, Massachusetts, used the same combination to image slender neuronal projections and dendritic spines in brain slices, which are not visible using conventional methods 2 . Sabatini and his team built the system themselves. They created the ring shape in the depletion laser using a phase plate to split the laser light so that one part served as the central spot and the other part as the ring. The result was a three-fold improvement in spatial resolution over normal two-photon techniques. Advances in resolution have not been restricted to two-dimensional samples. Harald Hess and his colleagues at Howard Hughes Medical Institute's Janelia Farm Research Campus in Virginia have modified PALM to do 3D imaging by adding an interferometer and a second objective lens 3 . Called iPALM, this method can pinpoint fluorescent labels to within 10\u201320 nm in three dimensions. However, the samples must be around 225\u2013250 nm thick, and ensuring that the extra optics needed are perfecty aligned is a time-consuming task. As a result, iPALM might prove challenging to commercialize, Hess says. Groups led by Moerner and Rafael Piestun at the University of Colorado, Boulder, have come up with another 3D technique, called double-helix PALM. This adds two lenses and a spatial light modulator to a standard wide-field microscope. The modulator splits the light into a double-helix shape that, when shined on to a fluorescent tagged molecule, generates two spots of light. The angular orientation of the spots can be used to calculate the depth of the molecule. \u201cThe double helix has a much deeper depth of field over which the objects remain in focus,\u201d says Moerner. \u201cIn current experiments we use a two-micrometre range, but that can be changed depending on how the phase modulator is designed.\u201d In collaboration with Robert Twieg at Kent State University in Ohio, Moerner's team has localized molecules with 20 nm precision in depth and 12 nm precision in length and width 4 , and has used the system to image bacterial proteins whose structures have never before been seen in cells.  \n               Going live \n             SIM is also finding application in 3D systems \u2014 through imaging live cells. Mats Gustafsson and his colleagues at Janelia Farm sped up the bar-code patterns used in two-dimensional SIM to allow it to image live cells, which are constantly moving. They did this with a liquid-crystal spatial light modulator, a mirror-like device, 1.5 centimetres long, that generates patterns of light using thousands of pixels that can be controlled individually. The modulator allows the microscope to generate new patterns of light about 1,000 times faster than the normal SIM equipment. The technique has allowed Gustafsson's team to see proteins moving along individual microtubules within living cells of the fruitfly  Drosophila melanogaster  at 100 nm resolution 5 . SIM will soon become even faster because of advances in camera technology. \u201cYou can already buy cameras that go up to 1,000 frames per second, and that speed can more or less be directly translated into speed increase in structured illumination,\u201d says Rainer Heintzmann, a molecular biologist at King's College London, who has used the system to see the inner folds of a cell's mitochondria. One of the main hurdles to widespread use of super-resolution microscopy in biology is the lack of appropriate probe molecules. Developing new probes is a trial-and-error process of seeing whether each protein variant will work in a variety of biological samples. \u201cThis is where the long, hard work is,\u201d Hess says. \u201cRight now there are few examples of single colours, but making them work in pairs will be the part that will bring a lot of value to the field.\u201d Having two or more colours would help biologists by allowing them to track more than one protein in a cell. Two-colour experiments can work but, at present, the second colour has usually faded by the time the first colour has been completed, Hess says.  A group led by Vladislav Verkhusha, an associate professor of anatomy and structural biology at Albert Einstein College of Medicine at Yeshiva University in New York, has developed a new red probe 6  that works using PALM methods. Called photoactivatable monomeric cherry, or PAmCherry1, the protein becomes red when activated by violet light. It seems to have better pH stability, faster photoactivation and better photostability than other red photoactivatable probes, the group says. Several groups are working on small organic probes that can be used with PALM or STORM. Earlier this year, for example, researchers at Bielefeld University in Germany imaged the inner membranes of mitochondria 7  using commercially available fluorophores. The researchers triggered the fluorophores to light up on cue by altering the chemical environment. And Hell is backing an unlikely material as a potential probe: diamond. His team found that STED microscopy can pick up subtle structural defects in diamonds with 10\u201320 nm resolution 8 . So if the diamonds can be ground into 35-nanometre-sized pieces and attached to antibodies \u2014 which researchers have shown is possible \u2014 they could be used as tags that will never bleach under STED microscopy's lasers. There is a \u201cclear prospect\u201d of using diamonds for cell-biology applications, Hell says. \u201cIt is not a pipe dream. It's farther down the road but is coming.\u201d \n               Applying software \n             An increasingly important aspect of microscopy \u2014 particularly super-resolution systems \u2014 is the software that constructs the final images. Scientists who want to modify the output or improve the accessibility of their super-resolution systems can develop algorithms and write programs for image processing. Many scientists have developed their own software and made it available to the community. \u201cThe design of the microscope and its modification is available to anybody who knows how to program,\u201d says Sabatini, and once you know how the microscope works, it is possible to determine where the photons should be and, from that, how the image can be constructed. Sabatini has helped at least ten other labs to implement super-resolution methods. \u201cBiologists feel comfortable taking the software we wrote and modifying it for their application,\u201d he says. \u201cIn our own lab, a lot of the advances we have week to week are software driven. We get an idea for an experiment, and we just reprogram the code.\u201d That makes experiments much less expensive, and the weekly progress wouldn't be possible using off-the-shelf systems, he adds. Zhuang's group has created software that allows image processing to happen as the data are acquired. Unlike a conventional fluorescence image, a single STORM image is obtained by stacking many frames. \u201cIt used to be that we could only do post-image processing,\u201d she says. \u201cNow we have analysis software that you can use to see the build-up of the high-resolution image as it is being recorded.\u201d This will speed up experiments, she says, because if there is something about the sample you dislike, you can change it before the image construction is finished. \n               One-stop system? \n             Biologists still have a long wish list for super-resolution imaging technology, probes and software. \u201cWe're at least one or two years away from the technology being fully established,\u201d says Jan Liphardt, a biophysicist at the University of California, Berkeley. \u201cMy own preference is to keep building my own hardware.\u201d Although improvements are happening fast, and developers are quickly adding to the pool of 3D and live-cell imaging data, there is still no one system that can readily do everything a biologist wants, says M. Cristina Cardoso, a cell biologist at the Technical University of Darmstadt, Germany. Cardoso and her colleagues used 3D-SIM and spatially modulated illumination to compare the sizes of DNA replication spots \u2014 called replicons \u2014 on chromosomes within a cell nucleus, and how they change over time. Replicons are, on average, 125 nm in diameter. Although these super-resolution techniques allowed her to identify three- to fivefold more replicons than the numbers previously reported 9 , the researchers would have preferred to conduct the experiment in live cells to get better temporal resolution. However, with current systems this would have meant giving up some spatial resolution, so instead they stopped replication and fixed the cells at various points in their cycle.  \n               Rivalling Hubble \n             Live-cell approaches are improving, but there are still challenges. In STORM, PALM and SIM, multiple pictures must be taken and then blended together to reveal all of the detail. \u201cDoes that represent structure or motion of some sort?\u201d Moerner says, adding that it is an ongoing challenge to distinguish the two. The challenges have not stopped large collaborations from harnessing super-resolution microscopes to answer big questions. Liphardt says that the University of California, Berkeley, has seven to ten PALM/STORM systems, built by researchers there and at other labs, that will be used for research into tumour-cell signalling as part of a US$15.7-million grant awarded by the US National Cancer Institute. About a year ago, a consortium formed, which includes Zhuang and Smith, to construct a detailed map of neuronal connections in the brain. This 'connectome' project will combine super-resolution microscopy, including STORM, with array tomography and electron microscopy to see neuronal details at a resolution of 100 nm or better over the entire brain of a mouse, and eventually a human. To get a complete wiring diagram of the brain, says Zhuang, one needs to resolve the long neuronal projections \u2014 axons \u2014 that are invisible under a traditional light microscope. \u201cWe're starting to get pictures out of the mouse brain that rival anything from the Hubble Space Telescope, and we're just getting started,\u201d Smith says. But to make use of that information will require a lot more than looking at pretty pictures: the new imaging data will also push forwards development of software used to reconstruct and analyse connections in three dimensions. \u201cThe tools for comparing and understanding the elaborate circuit diagrams are beyond anything that anyone has thought of today,\u201d he says.  The promise of super-resolution microscopy \u2014 thought for so long to be little more than a dream \u2014 is starting to become a reality. Researchers have taken different approaches and are using tools and techniques borrowed from physics, chemistry and computing technology to bring the nanoscopic world to our macroscopic eyes. Although commercialization is progressing, there is still plenty of room for the do-it-yourself biologist to modify and improve their systems, and produce images of stunning complexity that will rival anything else in science. Reprints and Permissions"},
{"file_id": "462679a", "url": "https://www.nature.com/articles/462679a", "year": 2009, "authors": [], "parsed_as_year": "2006_or_before", "body": "\n               Table 1 \n               Reprints and Permissions"},
{"file_id": "458926a", "url": "https://www.nature.com/articles/458926a", "year": 2009, "authors": [{"name": "Nathan Blow"}], "parsed_as_year": "2006_or_before", "body": "When Alan Koretsky, scientific director of the National Institute of Neurological Disorders and Stroke in Bethesda, Maryland, and Gary Zabow began to think about developing new contrast agents for magnetic resonance imaging (MRI), they took their design cues from the colourful world of molecular imaging. \"It was looking at what existed and then figuring out some way to copy the idea of quantum dots,\" says Zabow, a physicist at the National Institute of Standards and Technology in Boulder, Colorado.  In molecular imaging, a quantum dot can generate a range of possible emission spectra simply by varying the size of the dot's inner core shell. This is a stark contrast to the traditional agents used in MRI, such as gadolinium or iron oxide, which are magnetic materials that alter the signal from the protons in the surrounding water, appearing as either darker or brighter spots on images. \"It is sometimes difficult to tell the different agents apart from one another or from artefacts that make something brighter or darker,\" says Zabow. (At present, colour in MRI scans \u2014 such as those in this article \u2014 is assigned to shades of grey and added during processing.) \"One of the areas of MRI that has exploded over the past 5 or 6 years is the ability to track cells as they move around,\" says Koretsky. Although MRI cannot achieve single-cell resolution, a single cell can have sufficient magnetic-resonance contrast to be detected. But for researchers interested in tracking two or three cells at once, this level of differentiation is not enough.  So Zabow and Koretsky microfabricated specific magnetic shapes that would create different magnetic fields and so shift the nuclear magnetic resonance frequency. \"The existing magnetic particles do not shift the frequency \u2014 they just broaden it out,\" says Zabow. But the very precise shape of these new agents generates a corresponding precise frequency shift, similar to quantum dots, giving Zabow and Koretsky the possibility of creating different colours through different shapes and their specific frequency shifts. The initial work consists of two discs with a gap between them in which the magnetic field can be generated. By varying the thickness or diameter of the discs, or the gap, different fields can be obtained so that when water passes between the discs the magnetic resonance of the water molecules flowing through the gap shifts (G. Zabow   et al .  Nature   453 , 1058\u20131063; 2008). Although the possible range of new colours is still to be determined, that is not the primary focus at the moment, says Zabow. They are working to improve the fabrication process and make the magnetic particles smaller and more robust. \"We are working first on that. The idea of having as many colours as possible falls out from there because in improving the fabrication we are getting the geometry more precise,\" he says. Koretsky sees these new contrasting agents as adding a unique ability to MRI that no other radiological imaging technique possesses. \n               N.B. \n             Reprints and Permissions"}
]